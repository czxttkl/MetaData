<?xml version="1.0" encoding="ISO-8859-1"?>
<!DOCTYPE proceeding SYSTEM "proceeding.dtd">
<proceeding ver="6.0" ts="04/10/2010">
<conference_rec>
	<conference_date>
		<start_date>07/21/2013</start_date>
		<end_date>07/25/2013</end_date>
	</conference_date>
	<conference_loc>
		<city><![CDATA[Anaheim]]></city>
		<state>California</state>
		<country></country>
	</conference_loc>
	<conference_url></conference_url>
</conference_rec>
<series_rec>
	<series_name>
		<series_id>SERIES382</series_id>
		<series_title><![CDATA[International Conference on Computer Graphics and Interactive Techniques]]></series_title>
		<series_vol></series_vol>
	</series_name>
</series_rec>
<proceeding_rec>
	<proc_id>2504435</proc_id>
	<acronym>SIGGRAPH '13</acronym>
	<proc_desc>ACM SIGGRAPH 2013 Courses</proc_desc>
	<conference_number>2013</conference_number>
	<proc_class>conference</proc_class>
	<proc_title></proc_title>
	<proc_subtitle></proc_subtitle>
	<proc_volume_no></proc_volume_no>
	<isbn13>978-1-4503-2339-0</isbn13>
	<issn></issn>
	<eissn></eissn>
	<copyright_year>2013</copyright_year>
	<publication_date>07-21-2013</publication_date>
	<pages>860</pages>
	<plus_pages></plus_pages>
	<price><![CDATA[]]></price>
	<other_source></other_source>
	<abstract>
		<par><![CDATA[<p>In SIGGRAPH 2013 Courses, attendees learn from the experts in the field and gain inside knowledge that is critical to career advancement. Courses are short (1.5 hours) or half-day (3.25 hours) structured sessions that often include elements of interactive demonstration, performance, or other imaginative approaches to teaching.</p> <p>The spectrum of Courses ranges from an introduction to the foundations of computer graphics and interactive techniques for those new to the field to advanced instruction on the most current techniques and topics. Courses include core curricula taught by invited instructors as well as Courses selected from juried proposals.</p>]]></par>
	</abstract>
	<publisher>
		<publisher_id>PUB27</publisher_id>
		<publisher_code>ACMNY</publisher_code>
		<publisher_name>ACM</publisher_name>
		<publisher_address>2 Penn Plaza, Suite 701</publisher_address>
		<publisher_city>New York</publisher_city>
		<publisher_state>NY</publisher_state>
		<publisher_country>USA</publisher_country>
		<publisher_zip_code>10121-0701</publisher_zip_code>
		<publisher_contact>Bernard Rous</publisher_contact>
		<publisher_phone>212 869-7440</publisher_phone>
		<publisher_isbn_prefix></publisher_isbn_prefix>
		<publisher_url>www.acm.org/publications</publisher_url>
	</publisher>
	<sponsor_rec>
		<sponsor>
			<sponsor_id>SP932</sponsor_id>
			<sponsor_name>ACM Special Interest Group on Computer Graphics and Interactive Techniques</sponsor_name>
			<sponsor_abbr>SIGGRAPH</sponsor_abbr>
		</sponsor>
	</sponsor_rec>
	<categories>
		<primary_category>
			<cat_node/>
			<descriptor/>
			<type/>
		</primary_category>
	</categories>
	<ccc>
		<copyright_holder>
			<copyright_holder_name>ACM</copyright_holder_name>
			<copyright_holder_year>2013</copyright_holder_year>
		</copyright_holder>
	</ccc>
</proceeding_rec>
<content>
	<article_rec>
		<article_id>2504436</article_id>
		<sort_key>10</sort_key>
		<display_label>Article No.</display_label>
		<display_no>1</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Introduction to computer graphics]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2504435.2504436</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2504436</url>
		<abstract>
			<par><![CDATA[<p>The best way to get all you can out of your SIGGRAPH week is to start off with a solid understanding of the basics. This course covers the basics of 3D computer graphics in a friendly and visual way, without math or programming. The course is mostly made up of live demonstrations, because computer graphics is a great way to teach new ideas! Topics include the basic principles and language, so that you'll understand what's going on around you and enjoy meaningful conversations during SIGGRAPH 2013.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4193481</person_id>
				<author_profile_id><![CDATA[82458650257]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Andrew]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Glassner]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The Imaginary Institute]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2013</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>2504437</article_id>
		<sort_key>20</sort_key>
		<display_label>Article No.</display_label>
		<display_no>2</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Mobile game creation for everyone]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2504435.2504437</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2504437</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4193482</person_id>
				<author_profile_id><![CDATA[82458701957]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Joel]]></first_name>
				<middle_name><![CDATA[Van]]></middle_name>
				<last_name><![CDATA[Eenwyk]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Havok]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4193483</person_id>
				<author_profile_id><![CDATA[82458817457]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Walter]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Luh]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Corona Labs]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2013</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>2504438</article_id>
		<sort_key>30</sort_key>
		<display_label>Article No.</display_label>
		<pages>110</pages>
		<display_no>3</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[An introduction to OpenGL programming]]></title>
		<page_from>1</page_from>
		<page_to>110</page_to>
		<doi_number>10.1145/2504435.2504438</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2504438</url>
		<abstract>
			<par><![CDATA[<p>OpenGL is the most widely available library for creating interactive, computer graphics applications across all of the major computer operating systems. Its uses range from creating applications for scientific visualizations to computer-aided design, interactive gaming, and entertainment, and with each new version its capabilities reveal the most up-to-date features of modern graphics hardware.</p> <p>This course provides an accelerated introduction to programming OpenGL, emphasizing the most modern methods for using the library. In recent years, OpenGL has undergone numerous updates, which have fundamentally changed how programmers interact with the application programming interface (API) and the skills required for being an effective OpenGL programmer. The most notable of these changes, the introduction of shader-based rendering, has expanded to subsume almost all functionality in OpenGL.</p> <p>While there have been numerous courses on OpenGL in the past, the recent revisions to the API have provided a wealth of new functionality and features to create ever-richer content. This course builds from demonstrating the use of the most fundamental shader-based OpenGL pipeline to introducing numerous techniques that can be implemented using OpenGL.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4193484</person_id>
				<author_profile_id><![CDATA[81100366265]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Edward]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Angel]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of New Mexico]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4193485</person_id>
				<author_profile_id><![CDATA[81322506343]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Dave]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Shreiner]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[ARM, Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 An#Introduction#to#! OpenGL'Programming! Ed#Angel## University!of!New!Mexico# Dave#Shreiner## ARM,!Inc.# 
! Presented!at!SIGGRAPH!2013! Sunday,!July!21st,!2013! Anaheim,!CA,!USA! # # Contents# An Introduction 
to OpenGL Programming .............................................................. What Is OpenGL? 
..................................................................................................... 
Course Ground Rules ................................................................................................ 
 Evolution of the OpenGL Pipeline ............................................................................. 
In the Beginning ................................................................................................... 
Beginnings of The Programmable Pipeline .............................................................. 
An Evolutionary Change........................................................................................... 
The Exclusively Programmable Pipeline .................................................................. 
More Programmability .............................................................................................. 
More Evolution Context Profiles ........................................................................... 
The Latest Pipelines .................................................................................................. 
OpenGL ES and WebGL .......................................................................................... 
 OpenGL Application Development ............................................................................ 
A Simplified Pipeline Model .................................................................................... 
OpenGL Programming in a Nutshell ........................................................................ 
Application Framework Requirements ..................................................................... 
Simplifying Working with OpenGL ......................................................................... 
Representing Geometric Objects .............................................................................. 
OpenGL s Geometric Primitives .............................................................................. 
 A First Program ........................................................................................................... 
Our First Program ..................................................................................................... 
Initializing the Cube s Data ...................................................................................... 
Initializing the Cube s Data (cont d) ........................................................................ 
Cube Data.................................................................................................................. 
Cube Data.................................................................................................................. 
Generating a Cube Face from Vertices ..................................................................... 
Generating the Cube from Faces............................................................................... 
Vertex Array Objects (VAOs) .................................................................................. 
VAOs in Code ........................................................................................................... 
Storing Vertex Attributes .......................................................................................... 
VBOs in Code ........................................................................................................... 
Connecting Vertex Shaders with Geometric Data .................................................... Vertex 
Array Code .................................................................................................... 
Drawing Geometric Primitives ................................................................................. 
 Shaders and GLSL....................................................................................................... 
GLSL Data Types ..................................................................................................... 
Operators ................................................................................................................... 
Components and Swizzling....................................................................................... 
Qualifiers................................................................................................................... 
Functions ................................................................................................................... 
Built-in Variables ...................................................................................................... 
Simple Vertex Shader for Cube Example ................................................................. 
The Simplest Fragment Shader ................................................................................. 
 Getting Your Shaders into OpenGL.......................................................................... 
44 A Simpler Way.......................................................................................................... 
45 Associating Shader Variables and Data .................................................................... 
46 Determining Locations After Linking....................................................................... 
47 Initializing Uniform Variable Values ....................................................................... 
48 Finishing the Cube Program ..................................................................................... 
49 Cube Program s GLUT Callbacks ............................................................................ 
50 Vertex Shader Examples ........................................................................................... 
51 Transformations........................................................................................................... 
52 Camera Analogy ....................................................................................................... 
53 Transformations ........................................................................................................ 
54 Camera Analogy and Transformations ..................................................................... 
55 3D Transformations .................................................................................................. 
56 Specifying What You Can See.................................................................................. 
57 Specifying What You Can See (cont d).................................................................... 
58 Specifying What You Can See (cont d).................................................................... 
59 Viewing Transformations ......................................................................................... 
60 Creating the LookAt Matrix...................................................................................... 
61 Translation ................................................................................................................ 
62 Scale .......................................................................................................................... 
63 Rotation ..................................................................................................................... 
64 Rotation (cont d) ....................................................................................................... 
65 Vertex Shader for Rotation of Cube ......................................................................... 
66 Vertex Shader for Rotation of Cube (cont d)............................................................ 
67 Vertex Shader for Rotation of Cube (cont d)............................................................ 
68 Sending Angles from Application............................................................................. 
69 Lighting ......................................................................................................................... 
70 Lighting Principles .................................................................................................... 
71 Modified Phong Model ............................................................................................. 
72 Surface Normals........................................................................................................ 
73 Material Properties .................................................................................................... 
74 Adding Lighting to Cube .......................................................................................... 
75 Adding Lighting to Cube .......................................................................................... 
76 Adding Lighting to Cube .......................................................................................... 
77 Fragment Shaders ........................................................................................................ 
78 Fragment Shaders...................................................................................................... 
79 Shader Examples ....................................................................................................... 
80 Height Fields ............................................................................................................. 
81 Displaying a Height Field ......................................................................................... 
82 Time Varying Vertex Shader .................................................................................... 
83 Mesh Display ............................................................................................................ 
84 Adding Lighting ........................................................................................................ 
85 Mesh Shader.............................................................................................................. 
86 Mesh Shader (cont d) ................................................................................................ 
87 Shaded Mesh ............................................................................................................. 
88 Texture Mapping ......................................................................................................... 
89 Texture Mapping ....................................................................................................... 
90 Texture Mapping and the OpenGL Pipeline ............................................................. 
91 Applying Textures..................................................................................................... 
92 Texture Objects ......................................................................................................... 
93 Texture Objects (cont'd.)........................................................................................... 
94 Specifying a Texture Image ...................................................................................... 
95 Mapping a Texture .................................................................................................... 
96 Applying the Texture in the Shader .......................................................................... 
97 Applying Texture to Cube......................................................................................... 
98 Creating a Texture Image .......................................................................................... 
99 Texture Object........................................................................................................... 
100 Vertex Shader............................................................................................................ 
101 Fragment Shader ....................................................................................................... 
102 Resources ...................................................................................................................... 
103 Books ........................................................................................................................ 
104 Online Resources ...................................................................................................... 
105  OpenGL is a library of function calls for doing computer graphics. With it, you can create interactive 
applications that render high-quality color images composed of 2D and 3D geometric objects and images. 
Additionally, the OpenGL API is independent of all operating systems, and their associated windowing 
systems. That means that the part of your application that draws can be platform independent. However, 
in order for OpenGL to be able to render, it needs a window to draw into. Generally, this is controlled 
by the windowing system on whatever platform you are working on. While OpenGL has been around for over 
20 years, a lot of changes have occurred since it was created. This course concentrates on the latest 
versions of OpenGL version 4.3, although we don t have time to discuss all the features available. In 
these modern versions of OpenGL (which we define as versions starting with version 3.1), OpenGL applications 
are entirely shader based. In fact, most of this course will discuss shaders and the operations they 
support. If you re familiar with previous versions of OpenGL, or other rasterization­based graphics pipelines 
in general that may have included fixed-function processing, we won t be covering those techniques since 
these functions have been deprecated. Instead, we ll concentrate on showing how we can implement those 
techniques on a modern, shader-based graphics pipeline. In this modern world of OpenGL, all applications 
will need to provide shaders, and as such, providing some perspective on how the pipeline evolved and 
its phases will be illustrative. We ll discuss this next.  The initial version of OpenGL was announced 
in July of 1994. That version of OpenGL implemented what s called a fixed-function pipeline, which means 
that all of the operations that OpenGL supported were fully-defined, and an application could only modify 
their operation by changing a set of input values (like colors or positions). The other point of a fixed-function 
pipeline is that the order of operations was always the same that is, you can t reorder the sequence 
operations occur. This pipeline was the basis of many versions of OpenGL and expanded in many ways, and 
is still available for use. However, modern GPUs and their features have diverged from this pipeline, 
and support of these previous versions of OpenGL are for supporting current applications. If you re developing 
a new application, we strongly recommend using the techniques that we ll discuss. Those techniques can 
be more flexible, and will likely preform better than using one of these early versions of OpenGL since 
they can take advantage of the capabilities of recent Graphics Processing Units (GPUs). While many features 
and improvements were added into the fixed-function OpenGL pipeline, designs of GPUs were exposing more 
features than could be added into OpenGL. To allow applications to gain access to these new GPU features, 
OpenGL version 2.0 officially added programmable shaders into the graphics pipeline. This version of 
the pipeline allowed an application to create small programs, called shaders, that were responsible for 
implementing the features required by the application. In the 2.0 version of the pipeline, two programmable 
stages were made available: vertex shading enabled the application full control over manipulation of 
the 3D geometry provided by the application  fragment shading provided the application capabilities 
for shading pixels (the terms classically used for determining a pixel s color).  OpenGL 2.0 also fully 
supported OpenGL 1.X s pipeline, allowing the application to use both version of the pipeline: fixed-function, 
and programmable. Until OpenGL 3.0, features have only been added (but never removed) from OpenGL, providing 
a lot of application backwards compatibility (up to the use of extensions). OpenGL version 3.0 introduced 
the mechanisms for removing features from OpenGL, called the deprecation model. It defines how the OpenGL 
design committee (the OpenGL Architecture Review Board (ARB) of the Khronos Group) will advertise of 
which and how functionality is removed from OpenGL. You might ask: why remove features from OpenGL? Over 
the 15 years to OpenGL 3.0, GPU features and capabilities expanded and some of the methods used in older 
versions of OpenGL were not as efficient as modern methods. While removing them could break support for 
older applications, it also simplified and optimized the GPUs allowing better performance. Within an 
OpenGL application, OpenGL uses an opaque data structure called a context, which OpenGL uses to store 
shaders and other data. Contexts come in two flavors: full contexts expose all the features of the current 
version of OpenGL, including features that are marked deprecated.  forward-compatible contexts enable 
only the features that will be available in the next version of OpenGL (i.e., deprecated features pretend 
to be  OpenGL version 3.1 was the first version to remove deprecated features, and break backwards 
compatibility with previous versions of OpenGL. The features removed from included the old-style fixed-function 
pipeline, among other lesser features. One major refinement introduced in 3.1 was requiring all data 
to be placed in GPU-resident buffer objects, which help reduce the impacts of various computer system 
architecture limitations related to GPUs. While many features were removed from OpenGL 3.1, the OpenGL 
ARB realized that to make it easy for application developers to transition their products, they introduced 
an OpenGL extensions, GL_ARB_compatibility, that allowed access to the removed features. Until OpenGL 
3.2, the number of shader stages in the OpenGL pipeline remained the same, with only vertex and fragment 
shaders being supported. OpenGL version 3.2 added a new shader stage called geometry shading which allows 
the modification (and generation) of geometry within the OpenGL pipeline. In order to make it easier 
for developers to choose the set of features they want to use in their application, OpenGL 3.2 also introduced 
profiles which allow further selection of OpenGL contexts. The core profile is the modern, trimmed-down 
version of OpenGL that includes the latest features. You can request a core profile for a Full or Forward-compatible 
profile. Conversely, you could request a compatible profile, which includes all functionality (supported 
by the OpenGL driver on your system) in all versions of OpenGL up to, and including, the version you 
ve requested. The OpenGL 4.X pipeline added another pair of shaders (which work in tandem, so we consider 
it a single stage) for supporting dynamic tessellation in the GPU. Tessellation control and tessellation 
evaluation shaders were added to OpenGL version 4.0. The current version of OpenGL is 4.3, which includes 
some additional features over the 4.0 pipeline, but no new shading stages. WebGL is becoming increasingly 
more important because it is supported by all browsers except Internet Explorer (and even that appears 
to be changing). Besides the advantage of being able to run without recompilation across platforms, it 
can easily be integrated with other Web applications and make use of a variety of portable packages available 
over the Web.  To begin, let us introduce a simplified model of the OpenGL pipeline. Generally speaking, 
data flows from your application through the GPU to generate an image in the frame buffer. Your application 
will provide vertices, which are collections of data that are composed to form geometric objects, to 
the OpenGL pipeline. The vertex processing stage uses a vertex shader to process each vertex, doing any 
computations necessary to determine where in the frame buffer each piece of geometry should go. The other 
shading stages we mentioned tessellation and geometry shading are also used for vertex processing, 
but we re trying to keep this simple. After all the vertices for a piece of geometry are processed, the 
rasterizer determines which pixels in the frame buffer are affected by the geometry, and for each pixel, 
the fragment processing stage is employed, where the fragment shader runs to determine the final color 
of the pixel. In your OpenGL applications, you ll usually need to do the following tasks: specify the 
vertices for your geometry  load vertex and fragment shaders (and other shaders, if you re using them 
as well)  issue your geometry to engage the OpenGL pipeline for processing  Of course, OpenGL is capable 
of many other operations as well, many of You ll find that a few techniques for programming with modern 
OpenGL goes a long way. In fact, most programs in terms of OpenGL activity are very repetitive. Differences 
usually occur in how objects are rendered, and that s mostly handled in your shaders. There four steps 
you ll use for rendering a geometric object are as follows: 1. First, you ll load and create OpenGL shader 
programs from shader source programs you create 2. Next, you will need to load the data for your objects 
into OpenGL s memory. You do this by creating buffer objects and loading data into them. 3. Continuing, 
OpenGL needs to be told how to interpret the data in your buffer objects and associate that data with 
variables that you ll use in your shaders. We call this shader plumbing. 4. Finally, with your data 
initialized and shaders set up, you ll render your objects  We ll expand on those steps more through 
the course, but you ll find that most applications will merely iterate through those steps. While OpenGL 
will take care of filling the pixels in your application s output window or image, it has no mechanisms 
for creating that rendering surface. Instead, OpenGL relies on the native windowing system of your operating 
system to create a window, and make it available for OpenGL to render into. For each windowing system 
(like Microsoft Windows, or the X Window System on Linux [and other Unixes]), there s a binding library 
that lets mediates between OpenGL and the native windowing system. Since each windowing system has different 
semantics for creating windows and binding OpenGL to them, discussing each one is outside of the scope 
of this course. Instead, we use an open-source library named freeglut that abstracts each windowing system 
s specifics into a simple library. freeglut is a derivative of an older implementation called GLUT, and 
we ll use those names interchangeably. GLUT will help us in creating windows, dealing with user input 
and input devices, and other window-system activities. You can find out more about freeglut at its website: 
http://freeglut.sourceforge.net< < Both GLUT and freeglut use deprecated functions and should not work 
with a core profile. One alternative is GLFW which runs on Windows, Linux and Mac OS X. Just like window 
systems, operating systems have different ways of working with libraries. In some cases, the library 
you link your application exposes different functions than the library you execute your program with. 
Microsoft Windows is a notable example where you compile your application with a<.lib library, but use 
a .dll at runtime for finding function definitions. As such, your application would generally need to 
use operating-system specific methods to access functions. In general, this is troublesome and a lot 
of work. Fortunately, another open-source library comes to our aid, GLEW, the OpenGL Extension Wrangler 
library. It removes all the complexity of accessing OpenGL functions, and working with OpenGL extensions. 
We use GLEW in our examples to simplify the code. You can find details about GLEW at its website: http://glew.sourceforge.net< 
 In OpenGL, as in other graphics libraries, objects in the scene are composedof geometric primitives, 
which themselves are described by vertices. A vertex in modern OpenGL is a collection of data values 
associated with a location in space. Those data values might include colors, reflectioninformation for 
lighting, or additional coordinates for use in texture mapping.Locations can be specified on 2, 3 or 
4 dimensions but are stored in 4 dimensional homogeneous coordinates. Vertices must be organized in OpenGL 
server-side objects called vertex buffer objects (also known asVBOs), which need to contain all of the 
vertex information for all of the primitives that you want to draw at one time. VBOs can store vertex 
information in almost any format (i.e., an array-of-structures (AoS) each containing a single vertex 
s information, or a structure-of-arrays (SoA) where all of the same type of data for a vertex is stored 
in acontiguous array, and the structure stores arrays for each attribute that a vertex can have). The 
data within a VBO needs to be contiguous in memory,but doesn t need to be tightly packed (i.e., data 
elements may be separatedby any number of bytes, as long as the number of bytes between attributes isconsistent). 
VBOs are further required to be stored in vertex array objects (known as VAOs). Since it may be the case 
that numerous VBOs are associated with a single object, VAOs simplify the management of the collection 
of VBOs. To form 3D geometric objects, you need to decompose them into geometric primitives that OpenGL 
can draw. OpenGL only knows how to draw three things: points, lines, and triangles, but can use collections 
of the same type of primitive to optimize rendering. OpenGL'Primi,ve' Descrip,on' Total'Ver,ces'for'n' 
Primi,ves' GL_POINTS* Render'a'single'point'per'vertex'(points' may'be'larger'than'a'single'pixel)' n' 
 GL_LINES* Connect'each'pair'of'ver,ces'with'a' single'line'segment.' 2n' GL_LINE_STRIP* Connect'each'successive'vertex'to'the' 
previous'one'with'a'line'segment.' n+1' GL_LINE_LOOP* Connect'all'ver,ces'in'a'loop'of'line' segments.' 
n' GL_TRIANGLES* Render'a'triangle'for'each'triple'of' ver,ces.' 3n' GL_TRIANGLE_STRIP* Render'a'triangle'from'the'.rst'three' 
ver,ces'in'the'list,'and'then'create'a' new'triangle'with'the'last'two'rendered' ver,ces,'and'the'new'vertex.' 
n+2' GL_TRIANGLE_FAN* Create'triangles'by'using'the'.rst'vertex' in'the'list,'and'pairs'of'successive' 
ver,ces.' n+2'  The next few slides will introduce our first example program, one which simply displays 
a cube with different colors at each vertex. We aim for simplicity in this example, focusing on the OpenGL 
techniques, and not on optimal performance. In order to simplify our application development, we define 
a few types and constants to make our code more readable and organized. Our cube, like any other cube, 
has six square faces, each of which we ll draw as two triangles. In order to sizes memory arrays to hold 
the necessary vertex data, we define the constant NumVertices." " Addi&#38;onally,"as"we ll"see"in"our".rst"shader,"the"OpenGL"shading"language,"GLSL," 
has"a"built=in"type"called"vec4,"which"represents"a"vector"of"four".oa&#38;ng=point" values.""We"de.ne"a"C++"class"for"our"applica&#38;on"that"has"the"same"seman&#38;cs"as" 
that"GLSL"type.""Addi&#38;onally,"to"logically"associate"a"type"for"our"data"with"what"we" intend"to"do"with"it,"we"leverage"C++""typedefs"to"create"aliases"for"colors"and" 
posi&#38;ons.< In order to provide data for OpenGL to use, we need to stage it so that we can load it 
into the VBOs that our application will use. In your applications, you might load these data from a file, 
or generate them on the fly. For each vertex, we want to use two bits of data vertex attributes in OpenGL 
speak to help process each vertex to draw the cube. In our case, each vertex has a position in space, 
and an associated color. To store those values for later use in our VBOs, we create two arrays to hold 
the per vertex data. Note that we can organize our data in other ways such as with a single array with 
interleaved positions and colors. In our example we ll copy the coordinates of our cube model into a 
VBO for OpenGL to use. Here we set up an array of eight coordinates for the corners of a unit cube centered 
at the origin. You may be asking yourself: Why do we have four coordinates for 3D data? The answer is 
that in computer graphics, it s often useful to include a fourth coordinate to represent three-dimensional 
coordinates, as it allows numerous mathematical techniques that are common operations in graphics to 
be done in the same way. In fact, this four-dimensional coordinate has a proper name, a homogenous coordinate. 
We could also use a point3 type, i.e. point2(-0.5, -0.5, 0.5) which will be stored in 4 dimensions on 
the GPU. Just like our positional data, we ll set up a matching set of colors for each of the model 
s vertices, which we ll later copy into our VBO. Here we set up eight RGBA colors. In OpenGL, colors 
are processed in the pipeline as floating-point values in the range [0.0, 1.0]. Your input data can take 
any for; for example, image data from a digital photograph usually has values between [0, 255]. OpenGL 
will (if you request it), automatically convert those values into [0.0, 1.0], a process called normalizing 
values. As our cube is constructed from square cube faces, we create a small function, quad(), which 
takes the indices into the original vertex color and position arrays, and copies the data into the VBO 
staging arrays. If you were to use this method (and we ll see better ways in a moment), you would need 
to remember to reset the Index value between setting up your VBO arrays. Here we complete the generation 
of our cube s VBO data by specifying the six faces using index values into our original positions and 
colors arrays. It s worth noting that the order that we choose our vertex indices is important, as it 
will affect something called backface culling later. We ll see later that instead of creating the cube 
by copying lots of data, we can use our original vertex data along with just the indices we passed into 
quad() here to accomplish the same effect. That technique is very common, and something you ll use a 
lot. We chose this to introduce the technique in this manner to simplify the OpenGL concepts for loading 
VBO data. Similarly to VBOs, vertex array objects (VAOs) encapsulate all of the VBO data for an object. 
This allows much easier switching of data when rendering multiple objects (provided the data s been set 
up in multiple VAOs). The process for initializing a VAO is similar to that of a VBO, except a little 
less involved. 1. First, generate a name VAO name by calling glGenVertexArrays()" 2. Next,"make"the"VAO" 
current "by"calling"glBindVertexArray().""Similar"to" what"was"described"for"VBOs,"you ll"call"this"every"&#38;me"you"want"to"use"or" 
update"the"VBOs"contained"within"this"VAO.<  The above sequence calls shows how to create and bind 
a VAO. Since all geometric data in OpenGL must be stored in VAOs, you ll use this code idiom often. 
While we ve talked a lot about VBOs, we haven t detailed how one goes about creating them. Vertex buffer 
objects, like all (memory) objects in OpenGL (as compared to geometric objects) are created in the same 
way, using the same set of functions. In fact, you ll see that the pattern of calls we make here are 
similar to other sequences of calls for doing other OpenGL operations. In the case of vertex buffer objects, 
you ll do the following sequence of function calls: 1. Generate a buffer s name by calling glGenBuffers()< 
 2. Next, you ll make that buffer the current buffer, which means it s the selected buffer for reading 
or writing data values by calling  glBindBuffer(),"with"a"type"of"GL_ARRAY_BUFFER.""There"are"di.erent" 
types"of"bu.er"objects,"with"an"array"bu.er"being"the"one"used"for"storing" geometric"data." 3. To initialize 
a buffer, you ll call glBufferData(), which will copy data from your application into the GPU s memory. 
You would do the same operation if you also wanted to update data in the buffer 4. Finally, when it 
comes time to render using the data in the buffer, you ll once again call glBindVertexArray() to make 
it and its VBOs current again. In fact, if you have multiple objects, each with their own VAO, you ll 
likely call glBindVertexArray() once per frame for each object.  The above sequence of calls illustrates 
generating, binding, and initializing a VBO with data. In this example, we use a technique permitting 
data to be loaded into two steps, which we need as our data values are in two separate arrays. It s noteworthy 
to look at the glBufferData() call; in this call, we basically have OpenGL allocate an array sized to 
our needs (the combined size of our point and color arrays), but don t transfer any data with the call, 
which is specified with the NULL value. This is akin to calling malloc()<to create a buffer of uninitialized 
data. We later load that array with our calls to glBufferSubData(), which allows us to replace a subsection 
of our array. This technique is also useful if you need to update data inside of a VBO at some point 
in the execution of your application. The final step in preparing you data for processing by OpenGL 
(i.e., sending it down for rendering) is to specify which vertex attributes you d like issued to the 
graphics pipeline. While this might seem superfluous, it allows you to specify multiple collections of 
data, and choose which ones you d like to use at any given time. Each of the attributes that we enable 
must be associated with an in variable of the currently bound vertex shader. You retrieve vertex attribute 
locations was retrieved from the compiled shader by calling glGetAttribLocation().<<We discuss this call 
in the shader section. To complete the plumbing of associating our vertex data with variables inour 
shader programs, you need to tell OpenGL where in our buffer object tofind the vertex data, and which 
shader variable to pass the data to when we draw. The above code snippet shows that process for our two 
data sources.In our shaders (which we ll discuss in a moment), we have two variables:vPosition, and vColor, 
which we will associate with the data values in our VBOs that we copied form our vertex positions<and 
colors arrays. The calls to glGetAttribLocation() will return a compiler-generated index which we need 
to use to complete the connection from our data to the shaderinputs. We also need to turn the valve on 
our data by enabling its attributearray by calling glEnableVertexAttribArray() with the selected attribute 
location. This is the most flexible approach to this process, but depending on your OpenGL version, you 
may be able to use the layout construct, which allows you to specify the attribute location, as compared 
to having to retrieve it after compiling and linking your shaders. We ll discuss that in our shader section 
later in the course. BUFFER_OFFSET is a simple macro defined to make the code morereadable #define<BUFFER_OFFSET(<offset<)<<<((GLvoid*)<(offset))< 
 In order to initiate the rendering of primitives, you need to issue a drawing routine. While there are 
many routines for this in OpenGL, we ll discuss the most fundamental ones. The simplest routine is glDrawArrays(), 
to which you specify what type of graphics primitive you want to draw (e.g., here we re rending a triangle 
strip), which vertex in the enabled vertex attribute arrays to start with, and how many vertices to send. 
This is the simplest way of rendering geometry in OpenGL Version 3.1. You merely need to store you vertex 
data in sequence, and then glDrawArrays() takes care of the rest. However, in some cases, this won t 
be the most memory efficient method of doing things. Many geometric objects share vertices between geometric 
primitives, and with this method, you need to replicate the data once for each vertex. We ll see a more 
flexible, in terms of memory storage and access in the next slides.  As with any programming language, 
GLSL has types for variables. However, it includes vector-, and matrix-based types to simplify the operations 
that occur often in computer graphics. In addition to numerical types, other types like texture samplers 
are used to enable other OpenGL operations. We ll discuss texture samplers in the texture mapping section. 
 The vector and matrix classes of GLSL are first-class types, with arithmetic and logical operations 
well defined. This helps simplify your code, and prevent errors. Note in the above example, overloading 
ensures that both a*m and m*a are defined although they will not in general produce the same result. 
 For GLSL s vector types, you ll find that often you may also want to access components within the vector, 
as well as operate on all of the vector s components at the same time. To support that, vectors and matrices 
(which are really a vector of vectors), support normal C vector accessing using the square-bracket notation 
(e.g., [i] ), with zero-based indexing. Additionally, vectors (but not matrices) support swizzling, which 
provides a very powerful method for accessing and manipulating vector components. Swizzles allow components 
within a vector to be accessed by name. For example, the first element in a vector element 0 can also 
be referenced by the names x , s , and r . Why all the names to clarify their usage. If you re working 
with a color, for example, it may be clearer in the code to use r to represent the red channel, as compared 
to x , which make more sense as the x-positional coordinate In addition to types, GLSL has numerous qualifiers 
to describe a variable usage. The most common of those are:  in qualifiers that indicate the shader 
variable will receive data flowing into the shader, either from the application, or the previous shader 
stage. out qualifier which tag a variable as data output where data will flow to the next shader stage, 
or to the framebuffer  uniform qualifiers for accessing data that doesn t change across a draw operation 
  GLSL also provides a rich library of functions supporting common operations. While pretty much every 
vector- and matrix-related function available you can think of, along with the most common mathematical 
functions are built into GLSL, there s no support for operations like reading files or printing values. 
Shaders are really data-flow engines with data coming in, being processed, and sent on for further processing. 
 Fundamental to shader processing are a couple of built-in GLSL variable which are the terminus for operations. 
In particular, vertex data, which can be processed by up to four shader stages in OpenGL, are all ended 
by setting a positional value into the built-in variable, gl_Position. Additionally, fragment shaders 
provide a number of built-in variables. For example, gl_FragCoord is a read-only variable, while gl_FragDepth 
is a read-write variable. Later versions of OpenGL allow fragment shaders to output to other variables 
of the user s designation as well. Here s the simple vertex shader we use in our cube rendering example. 
It accepts two vertex attributes as input: the vertex s position and color, and does very little processing 
on them; in fact, it merely copies the input into some output variables (with gl_Position being implicitly 
declared). The results of each vertex shader execution are passed further down the OpenGL pipeline, and 
ultimately end their processing in the fragment shader. Here s the associated fragment shader that we 
use in our cube example. While this shader is as simple as they come merely setting the fragment s color 
to the input color passed in, there s been a lot of processing to this point. In particular, every fragment 
that s shaded was generated by the rasterizer, which is a built-in, non-programmable (i.e., you don t 
write a shader to control its operation). What s magical about this process is that if the colors across 
the geometric primitive (for multi-vertex primitives: lines and triangles) is not the same, the rasterizer 
will interpolate those colors across the primitive, passing each iterated value into our color variable. 
 Shaders need to be compiled in order to be used in your program. As compared to C programs, the compiler 
and linker are implemented in the OpenGL driver, and accessible through function calls from within your 
program. The diagram illustrates the steps required to compile and link each type of shader into your 
shader program. A program can contain either a vertex shader (which replaces the fixed-function vertex 
processing), a fragment shader (which replaces the fragment coloring stages), or both. If a shader isn 
t present for a particular stage, the fixed-function part of the pipeline is used in its place. Just 
a with regular programs, a syntax error from the compilation stage, or a missing symbol from the linker 
stage could prevent the successful generation of an executable program. There are routines for verifying 
the results of the compilation and link stages of the compilation process, but are not shown here. Instead, 
we ve provided a routine that makes this process much simpler, as demonstrated on the next slide. To 
simplify our lives, we created a routine that simplifies loading, compiling, and linking shaders: InitShaders(). 
It implements the shader compilation and linking process shown on the previous slide. It also does full 
error checking, and will terminate your program if there s an error at some stage in the process (production 
applications might choose a less terminal solution to the problem, but it s useful in the classroom). 
InitShaders() accepts two parameters, each a filename to be loaded as source for the vertex and fragment 
shader stages, respectively. The value returned from InitShaders() will be a valid GLSL program id that 
you can pass into glUseProgram(). OpenGL shaders, depending on which stage their associated with, process 
different types of data. Some data for a shader changes for each shader invocation. For example, each 
time a vertex shader executes, it s presented with new data for a single vertex; likewise for fragment, 
and the other shader stages in the pipeline. The number of executions of a particular shader rely on 
how much data was associated with the draw call that started the pipeline if you call glDrawArrays() 
specifiying 100 vertices, your vertex shader will be called 100 times, each time with a different vertex. 
Other data that a shader may use in processing may be constant across a draw call, or even all the drawing 
calls for a frame. GLSL calls those uniform varialbes, since their value is uniform across the execution 
of all shaders for a single draw call. Each of the shader s input data variables (ins and uniforms) needs 
to be connected to a data source in the application. We ve already seen glGetAttribLocation() for retrieving 
information for connecting vertex data in a VBO to shader variable. You will also use the same process 
for uniform variables, as we ll describe shortly. Once you know the names of variables in a shader 
whether they re attributes or uniforms you can determine their location using one of the glGet*Location() 
calls. If you don t know the variables in a shader (if, for instance, you re writing a library that accepts 
shaders), you can find out all of the shader variables by using the glGetActiveAttrib() function. You 
ve already seen how one associates values with attributes by calling glVertexAttribPointer(). To specify 
a uniform s value, we use one of the glUniform*() functions. For setting a vector type, you ll use one 
of the glUniform*() variants, and for matrices you ll use a glUniformMatrix *() form. You ll find that 
many OpenGL programs look very similar, particularly simple examples as we re showing in class. Above 
we demonstrate the basic initialization code for our examples. In our main() routine, you can see our 
use of the freeglut and GLEW libraries. The main() has a number of tasks: Initialize and open a window 
 Initialize the buffers and parameters by calling init()  Specify the callback functions for events 
 Enter an infinite event loop  Although callbacks aren t required by OpenGL, it is the standard method 
for developing interactive applications. A display callback is required by freeglut. It is invoked whenever 
OpenGL determines a window has to be redrawn, i.e. when a window is first opened or the contents of a 
window are changed. In our example we use a keyboard callback to end the program. We begin delving into 
shader specifics by first taking a look at vertex shaders. As you ve probably arrived at, vertex shaders 
are used to process vertices, and have the required responsibility of specifying the vertex s position 
in clip coordinates. This process usually involves numerous vertex transformations, which we ll discuss 
next. Additionally, a vertex shader may be responsible for determine additional information about a vertex 
for use by the rasterizer, including specifying colors. To begin our discussion of vertex transformations, 
we ll first describe the synthetic camera model.  This model has become know as the synthetic camera 
model. Note that both the objects to be viewed and the camera are three­dimensional while the resulting 
image is two dimensional. The processing required for converting a vertex from 3D or 4D space into a 
2D window coordinate is done by the transform stage of the graphics pipeline. The operations in that 
stage are illustrated above. The purple boxes represent a matrix multiplication operation. In graphics, 
all of our matrices are 4×4 matrices (they re homogenous, hence the reason for homogenous coordinates). 
When we want to draw an geometric object, like a chair for instance, we first determine all of the vertices 
that we want to associate with the chair. Next, we determine how those vertices should be grouped to 
form geometric primitives, and the order we re going to send them to the graphics subsystem. This process 
is called modeling. Quite often, we ll model an object in its own little 3D coordinate system. When we 
want to add that object into the scene we re developing, we need to determine its world coordinates. 
We do this by specifying a modeling transformation, which tells the system how to move from one coordinate 
system to another. Modeling transformations, in combination with viewing transforms, which dictate where 
the viewing frustum is in world coordinates, are the first transformation that a vertex goes through. 
Next, the projection transform is applied which maps the vertex into another space called clip coordinates, 
which is where clipping occurs. After clipping, we divide by the w value of the Note that human vision 
and a camera lens have cone-shaped viewing volumes. OpenGL (and almost all computer graphics APIs) describe 
a pyramid-shaped viewing volume. Therefore, the computer will see differently from the natural viewpoints, 
especially along the edges of viewing volumes. This is particularly pronounced for wide-angle fish­eye 
camera lenses.  By using 4×4 matrices, OpenGL can represent all geometric transformations using one 
matrix format. Perspective projections and translations require the 4th row and column. Otherwise, these 
operations would require an vector-addition operation, in addition to the matrix multiplication. While 
OpenGL specifies matrices in column-major order, this is often confusing for C programmers who are used 
to row-major ordering for two-dimensional arrays. OpenGL provides routines for loading both column- and 
row-major matrices. However, for standard OpenGL transformations, there are functions that automatically 
generate the matrices for you, so you don t generally need to be concerned about this until you start 
doing more advanced operations. For operations other than perspective projection, the fourth row is always 
(0, 0, 0, 1) which leaves the w-coordinate unchanged.. Another essential part of the graphics processing 
is setting up how much ofthe world we can see. We construct a viewing frustum, which defines the chunk 
of 3-space that we can see. There are two types of views: a perspective view, which you re familiar with 
as it s how your eye works, is used to generate frames that match your view of reality things farther 
from your appear smaller. This is the type of view used for video games,simulations, and most graphics 
applications in general. The other view, orthographic, is used principally for engineering and design 
situations, where relative lengths and angles need to be preserved. For a perspective, we locate the 
eye at the apex of the frustum pyramid. We can see any objects which are between the two planes perpendicular 
to eye (they re called the near and far clipping planes, respectively). Any vertices between near and 
far, and inside the four planes that connect them will berendered. Otherwise, those vertices are clipped 
out and discarded. In some cases a primitive will be entirely outside of the view, and the system willdiscard 
it for that frame. Other primitives might intersect the frustum, which we clip such that the part of 
them that s outside is discarded and we createnew vertices for the modified primitive. While the system 
can easily determine which primitive are inside the frustum, it s wasteful of system bandwidth to have 
lots of primitives discarded in this manner. We utilize a technique named culling to determine exactly 
whichprimitives need to be sent to the graphics processor, and send only those primitives to maximize 
its efficiency. In OpenGL, the default viewing frusta are always configured in the same manner, which 
defines the orientation of our clip coordinates. Specifically, clip coordinates are defined with the 
eye located at the origin, looking down the z axis. From there, we define two distances: our near and 
far clip distances, which specify the location of our near and far clipping planes. The viewing volume 
is then completely by specifying the positions of the enclosing planes that are parallel to the view 
direction . The images above show the two types of projection transformations that are commonly used 
in computer graphics. The orthographic view preserves angles, and simulates having the viewer at an infinite 
distance from the scene. This mode is commonly used in used in engineering and design where it s important 
to preserve the sizes and angles of objects in relation to each other. Alternatively, the perspective 
view mimics the operation of the eye with objects seeming to shrink in size the farther from the viewer 
they are. The each projection, the matrix that you would need to specify is provided. In those matrices, 
the six values for the positions of the left, right, bottom, top, near and far clipping planes are specified 
by the first letter of the plane s name. The only limitations on the values is for perspective projections, 
where the near and far values must be positive and non-zero, with near greater than far. LookAt() generates 
a viewing matrix based on several points. LookAt() provides natrual semantics for modeling flight application, 
but care must be taken to avoid degenerate numerical situations, where the generated viewing matrix is 
undefined. An alternative is to specify a sequence of rotations and translations that are concatenated 
with an initial identity matrix. Note: that the name modelview matrix is appropriate since moving objects 
in the model front of the camera is equivalent to moving the camera to view a set of objects. Using 
the values passed into the LookAt() call, the above matrix generates the corresponding viewing matrix. 
 Here we show the construction of a translation matrix. Translations really move coordinate systems, 
and not individual objects. Here we show the construction of a scale matrix, which is used to change 
the shape of space, but not move it (or more precisely, the origin). The above illustration has a translation 
to show how space was modified, but a simple scale matrix will not include such a translation. Here 
we show the effects of a rotation matrix on space. Once again, a translation has been applied in the 
image to make it easier to see the rotation s affect. The formula for generating a rotation matrix is 
a bit more complex that for scales and translations. Naming the axis of rotation v, we begin by normalizing 
v and storing the result in the vector u. From there, we create a 3 × 3 matrix M, which is composed of 
the sum of three terms. 1. The outer product of the vector u with its transpose ut 2. The difference 
of the identity matrix, I, with u s outer product, scaled the by the cosine of the input angle . 3. 
Finally, we scale the matrix S which is composed of the elements of the rotation matrix.  The complete 
rotation matrix is formed by composing M as the upper 3 × 3 matrix in R. Here s an example vertex shader 
for rotating our cube. We generate the matrices in the shader (as compared to in the application), based 
on the input angle theta. It s useful to note that we can vectorize numerous computations. For example, 
we can generate a vectors of sines and cosines for the input angle, which we ll use in further computations. 
 Completing our shader, we compose two of three rotation matrices (one around each axis). In generating 
our matrices, we use one of the many matrix constructor functions (in this case, specifying the 16 individual 
elements). It s important to note in this case, that our matrices are column­major, so we need to take 
care in the placement of the values in the constructor. We complete our shader here by generating the 
last rotation matrix, and ) and then use the composition of those matrices to transform the input vertex 
position. We also pass-thru the color values by assigning the input color to an output variable. Finally, 
we merely need to supply the angle values into our shader through our uniform plumbing. In this case, 
we track each of the axes rotation angle, and store them in a vec3 that matches the angle declaration 
in the shader. We also keep track of the uniform s location so we can easily update its value.  Lighting 
is an important technique in computer graphics. Without lighting, objects tend to look like they are 
made out of plastic. OpenGL divides lighting into three parts: material properties, light properties 
and global lighting parameters. While we ll discuss the mathematics of lighting in terms of computing 
illumination in a vertex shader, the almost identical computations can be done in a fragment shader to 
compute the lighting effects per-pixel, which yields much better results. OpenGL can use the shade at 
one vertex to shade an entire polygon (constant shading) or interpolate the shades at the vertices across 
the polygon (smooth shading), the default. The original lighting model that was supported in hardware 
and OpenGL was due to Phong and later modified by Blinn. The lighting normal tells OpenGL how the object 
reflects light around a vertex. If you imagine that there is a small mirror at the vertex, the lighting 
normal describes how the mirror is oriented, and consequently how light is reflected. Material properties 
describe the color and surface properties of a material (dull, shiny, etc). The properties described 
above are components of the Phong lighting model, a simple model that yields reasonable results with 
little computation. Each of the material components would be passed into a vertex shader, for example, 
to be used in the lighting computation along with the vertex s position and lighting normal. Here we 
declare numerous variables that we ll use in computing a color using a simple lighting model. All of 
the uniform values are passed in from the application and describe the material and light properties 
being rendered. In the initial parts of our shader, we generate numerous vector quantities to be used 
in our lighting computation. pos represents the vertex s position in eye coordinates  L represents 
the vector from the vertex to the light  E represents the eye vector, which is the vector from the vertex 
s eye­space position to the origin  H is the half vector which is the normalized vector half-way between 
the light and eye vectors  N is the transformed vertex normal  Note that all of these quantities are 
vec3 s, since we re dealing with vectors, as compared to homogenous coordinates. When we need to convert 
form a homogenous coordinate to a vector, we use a vector swizzle to extract the components we need. 
 Here we complete our lighting computation. The Phong model, which this shader is based on, uses various 
material properties as we described before. Likewise, each light can contribute to those same properties. 
The combination of the material and light properties are represented as our product variables in this 
shader. The products are merely the component­wise products of the light and objects same material propreties. 
These values are computed in the application and passed into the shader. In the Phong model, each material 
product is attenuated by the magnitude of the various vector products. Starting with the most influential 
component of lighting, the diffuse color, we use the dot product of the lighting normal and light vector, 
clamping the value if the dot product is negative (which physically means the light s behind the object). 
We continue by computing the specular component, which is computed as the dot product of the normal and 
the half-vector raised to the shininess value. Finally, if the light is behind the object, we correct 
the specular contribution. Finally, we compose the final vertex color as the sum of the computed ambient, 
diffuse, and specular colors, and update the transformed vertex position.  The final shading stage that 
OpenGL supports is fragment shading which allows an application per-pixel-location control over the color 
that may be written to that location. Fragments, which are on their way to the framebuffer, but still 
need to do some pass some additional processing to become pixels. However, the computational power available 
in shading fragments is a great asset to generating images. In a fragment shader, you can compute lighting 
values similar to what we just discussed in vertex shading per fragment, which gives much better results, 
or add bump mapping, which provides the illusion of greater surface detail. Likewise, we ll apply texture 
maps, which allow us to increase the detail for our models without increasing the geometric complexity. 
 We ll now analyze a few case studies from different applications. The first simple application we ll 
look at is rendering height fields, as you might do when rendering terrain in an outdoor game or flight 
simulator.  We d first like to render a wire-frame version of our mesh, which we ll draw a individual 
line loops. To begin, we build our data set by sampling the function f for a particular time across the 
domain of points. From there, we build our array of points to render. Once we have our data and have 
loaded into our VBOs we render it by drawing the individual wireframe quadrilaterals. There are many 
ways to render a wireframe surface like this give some thought of other methods.  Here s a rendering 
of the mesh we just generated. While the wireframe version is of some interest, we can create better 
looking meshes by adding a few more effects. We ll begin by creating a solid mesh by converting each 
wireframe quadrilateral into a solid quad composed of two separate triangles. Turns out with our pervious 
set of points, we can merely changed our glDrawArrays()<call or more specifically, the geometric primitive 
type to render a solid surface. However, if we don t do some additional modification of one of our shaders, 
we ll get a large back blob. To produce a more useful rendering, we ll add lighting computations into 
our vertex shader, computing a lighting color for each vertex, which will be passed to the fragment shader. 
 Details of lighting model are not important to here. The model includes the standard modified Phong 
diffuse and specular terms without distance. Note that we do the lighting in eye coordinates and therefore 
must compute the eye position in this frame. All the light and material properties are set in the application 
and are available through the OpenGL state.  Here s a rendering of our shaded, solid mesh.  Textures 
are images that can be thought of as continuous and be one, two, three, or four dimensional. By convention, 
the coordinates of the image are s, t, r and q. Thus for the two dimensional image above, a point in 
the image is given by its (s, t) values with (0, 0) in the lower­left corner and (1, 1) in the top-right 
corner. A texture map for a two-dimensional geometric object in (x, y, z) world coordinates maps a point 
in (s, t) space to a corresponding point on the screen. The advantage of texture mapping is that visual 
detail is in the image, not in the geometry. Thus, the complexity of an image does not affect the geometric 
pipeline (transformations, clipping) in OpenGL. Texture is added during rasterization where the geometric 
and pixel pipelines meet. In the simplest approach, we must perform these three steps. Textures reside 
in texture memory. When we assign an image to a texture it is copied from processor memory to texture 
memory where pixels are formatted differently. Texture coordinates are actually part of the state as 
are other vertex attributes such as color and normals. As with colors, OpenGL interpolates texture inside 
geometric objects. Because textures are really discrete and of limited extent, texture mapping is subject 
to aliasing errors that can be controlled through filtering. Texture memory is a limited resource and 
having only a single active texture can lead to inefficient code. The first step in creating texture 
objects is to have OpenGL reserve some indices for your objects. glGenTextures() will request n texture 
ids and return those values back to you in texIds. To begin defining a texture object, you call glBindTexture() 
with the id of the object you want to create. The target is one of GL_TEXTURE_{123}D(). All texturing 
calls become part of the object until the next glBindTexture() is called. To have OpenGL use a particular 
texture object, call glBindTexture() with the target and id of the object you want to be active. To delete 
texture objects, use glDeleteTextures( n, *texIds ), where texIds is an array of texture object identifiers 
to be deleted. After creating a texture object, you ll need to bind to it to initialize or use the texture 
stored in the object. This operation is very similar to what you ve seen when working with VAOs and VBOs. 
 Specifying the texels for a texture is done using the glTexImage{123}D() call. This will transfer the 
texels in CPU memory to OpenGL, where they will be processed and converted into an internal format. The 
level parameter is used for defining how OpenGL should use this image when mapping texels to pixels. 
Generally, you ll set the level to 0, unless you are using a texturing technique called mipmapping, which 
we will discuss in the next section. When you want to map a texture onto a geometric primitive, you 
need to provide texture coordinates. Valid texture coordinates are between 0 and 1, for each texture 
dimension, and usually manifest in shaders as vertex attributes. We ll see how to deal with texture coordinates 
outside the range [0, 1] in a moment. Just like vertex attributes were associated with data in the application, 
so too with textures. In particular, you access a texture defined in your application using a texture 
sampler in your shader. The type of the sampler needs to match the type of the associated texture. For 
example, you would use a sampler2D to work with a two-dimensional texture created with glTexImage2D( 
GL_TEXTURE_2D, ); Within the shader, you use the texture() function to retrieve data values from the 
texture associated with your sampler. To the texture() function, you pass the sampler as well as the 
texture coordinates where you want to pull the data from. Note: the overloaded texture() method was added 
into GLSL version 3.30. Prior to that release, there were special texture functions for each type of 
texture sampler (e.g., there was a texture2D() call for use with the sampler2D). Similar to our first 
cube example, if we want to texture our cube, we need to provide texture coordinates for use in our shaders. 
Following our previous example, we merely add an additional vertex attribute that contains our texture 
coordinates. We do this for each of our vertices. We will also need to update VBOs and shaders to take 
this new attribute into account. The code snippet above demonstrates procedurally generating a two 64 
× 64 texture maps. The above OpenGL commands completely specify a texture object. The code creates a 
texture id by calling glGenTextures(). It then binds the texture using glBindTexture() to open the object 
for use, and loading in the texture by calling glTexImage2D(). After that, numerous sampler characteristics 
are set, including the texture wrap modes, and texel filtering. In order to apply textures to our geometry, 
we need to modify both the vertex shader and the pixel shader. Above, we add some simple logic to pass-thru 
the texture coordinates from an attribute into data for the rasterizer. Continuing to update our shaders, 
we add some simple code to modify our fragment shader to include sampling a texture. How the texture 
is sampled (e.g., coordinate wrap modes, texel filtering, etc.) is configured in the application using 
the glTexParameter*() call.  All the above books except Angel and Shreiner, Interactive Computer Graphics 
(Addison-Wesley), are in the Addison-Wesley Professional series of OpenGL books.  Many example programs, 
a C++ matrix-vector package and the InitShader function are under the Book Support tab at www.cs.unm.edu/~angel 
 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2013</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>2504439</article_id>
		<sort_key>40</sort_key>
		<display_label>Article No.</display_label>
		<pages>5</pages>
		<display_no>4</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[Recent advances in light transport simulation]]></title>
		<subtitle><![CDATA[theory &#38; practice]]></subtitle>
		<page_from>1</page_from>
		<page_to>5</page_to>
		<doi_number>10.1145/2504435.2504439</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2504439</url>
		<abstract>
			<par><![CDATA[<p>Robust and efficient light-transport simulation based on statistical methods is the subject of renewed research interest, propelled by the desire to accurately render general environments with complex materials and light sources, which is often difficult with current solutions. In addition, it has been recognized that advanced methods, which can render many effects in one pass without excessive tweaking, increase artists' productivity and allow them to focus on their creative work. For this reason, the movie industry is shifting away from approximate rendering solutions toward physically based rendering methods, which poses new challenges in terms of strict requirements on high image quality and algorithm robustness.</p> <p>Many of the recent advances in light-transport simulation, such as new Markov chain Monte Carlo methods or robust combination of bidirectional path tracing with photon mapping, are made possible by interpreting light transport as an integral in the space of light paths. However, there is a great deal of confusion among practitioners and researchers alike regarding these path-space methods.</p> <p>The goal of this course is twofold. First, it presents a coherent review of the path-integral formulation of light transport and its applications, including the most recent ones, and it shows that rendering algorithms that may seem complex at first sight are, in fact, naturally derived from this general framework. A significant part of the course is devoted to application of Markov chain Monte Carlo methods for light-transport simulation, such as Metropolis Light Transport and its variants. The second part of the course discusses practical aspects of applying advanced light-transport simulation methods in the movie industry and other application domains, such as architectural and product visualization.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4222314</person_id>
				<author_profile_id><![CDATA[81100294023]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Jaroslav]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[K&#345;iv&#225;nek]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Charles University, Prague]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4193486</person_id>
				<author_profile_id><![CDATA[81436597914]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Iliyan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Georgiev]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Universit&#228;t des Saarlandes]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4193487</person_id>
				<author_profile_id><![CDATA[81456636673]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Anton]]></first_name>
				<middle_name><![CDATA[S.]]></middle_name>
				<last_name><![CDATA[Kaplanyan]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Karlsruher Institut f&#252;r Technologie]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4193488</person_id>
				<author_profile_id><![CDATA[82659100457]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Juan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ca&#241;ada]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Next Limit Technologies]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Recent Advances in Light Transport Simulation: Theory &#38; Practice SIGGRAPH 2013 Course Course materials 
are available from http://cgg.mff.cuni.cz/ jaroslav/papers/2013-ltscourse/index.htm Organizers Jaroslav 
K.riv ´ anek Charles University, Prague Iliyan Georgiev Saarland University Lecturers Anton S. Kaplanyan 
Karlsruhe Institute of Technology Juan Ca nada Next Limit Technologies Abstract We are witnessing a renewed 
research interest in robust and ef.cient light transport simulation based on statistical methods. This 
research effort is propelled by the desire to accurately render general environments with complex materials 
and light sources, which is often dif.cult with the currently employed solutions. In addition, it has 
been recognized that advanced methods, which are able to render many effects in one pass without excessive 
tweaking, increase artists productivity and allow them to focus on their creative work. For this reason, 
the movie industry is shifting away from approxi­mate rendering solutions toward physically-based rendering 
methods, which poses new challenges in terms of strict requirements on high image quality and algorithm 
robust­ness. Many of the recent advances in light transport simulation, such as new Markov chain Monte 
Carlo methods or the robust combination of bidirectional path tracing with pho­ton mapping, are made 
possible by interpreting light transport as an integral in the space of light paths. However, there is 
a great deal of confusion among practitioners and researchers alike regarding these path space methods. 
The goal of this course is twofold. First, we present a coherent review of the path integral formulation 
of light transport and its applications, including the most recent ones. We show that rendering algorithms 
that may seem complex at .rst sight, are in fact naturally derived from this general framework. A signi.cant 
part of the course is devoted to the application of Markov chain Monte Carlo methods for light transport 
simulation, such as Metropolis Light Transport and its variants. We include an exten­sive empirical comparison 
of these MCMC methods. The second part of the course discusses practical aspects of applying advanced 
light transport simulation methods in practical architectural visualization and VFX tasks. Intended audience 
Industry professionals and researchers interested in recent advances in robust light transport simulation 
for realistic rendering with global illumination. Prerequisites Familiarity with rendering and with concepts 
of global illumination computation is expected. Level of dif.culty Intermediate Syllabus 1. Introduction 
&#38; Welcome (K.riv ´anek) (5 min)  2. Path Integral Formulation of Light Transport (K.anek)  riv´ 
(30 min) Rendering and measurement equations  Light transport simulation as an integral over the space 
of light paths  Monte Carlo integration primer  Path sampling methods and path probability density 
 Unidirectional path sampling: Path tracing, Light tracing. Limitations  3. Bidirectional Path Sampling 
Techniques (K.anek) riv´ (20 min) Bidirectional path tracing  Combining different path sampling techniques 
 Virtual point lights  Discussion: advantages &#38; limitations  4. Vertex Connection and Merging 
(Georgiev) (35 min)  (Progressive) photon mapping  Vertex merging: photon mapping as a path sampling 
technique  Combining photon mapping with bidirectional path tracing  Consistency and convergence rate 
 Discussion: advantages &#38; limitations  Break (15 min) 5. Markov Chain and Sequential Monte Carlo 
Methods (Kaplanyan) (25 min)  Markov chains  Metropolis-Hastings algorithm  Metropolis light transport 
 Normalization, start-up bias and strati.cation  Different mutation strategies and their properties 
 Light transport with sequential Monte Carlo  6. Comparison of Advanced Light Transport Methods (Kaplanyan) 
(30 min)  Ordinary Monte Carlo methods  Metropolis light transport with different mutation strategies 
 Energy redistribution path tracing  Replica exchange light transport  Markov chain progressive photon 
mapping  Population Monte Carlo light transport  7. Advanced Light Transport in the VFX/Archiviz industry 
(Ca nada) (30 min) Stage of the Industry -the reasons for accurate light transport in practice.  Current 
problems, solutions, and workarounds.  Whats next?  8. Conclusions / Q &#38; A (all) (5 min) Course 
presenter information Jaroslav K .anek riv ´Charles University, Prague jaroslav.krivanek@mff.cuni.cz 
Jaroslav is an assistant professor at Charles University in Prague. Prior to this ap­pointment, he was 
a Marie Curie post-doctoral research fellow at the Cornell Univer­sity Program of Computer Graphics, 
and a junior researcher and assistant professor at Czech Technical University in Prague. Jaroslav received 
his Ph.D. from IRISA/INRIA Rennes and the Czech Technical University (joint degree) in 2005. In 2003 
and 2004 he was a research associate at the University of Central Florida. His primary research interest 
is realistic rendering and global illumination. Iliyan Georgiev Saarland University georgiev@cs.uni-saarland.de 
Iliyan is a graphics researcher at Saarland University, Germany, pursuing a PhD degree. He received a 
B.Sc. degree in computer science from So.a University, Bulgaria, and a M.Sc. degree in computer science 
from Saarland University. His primary research interests are ray tracing and Monte Carlo methods for 
global illumination rendering. Anton S. Kaplanyan Karlsruhe Institute of Technology anton.kaplanyan@kit.edu 
Anton S. Kaplanyan is a graphics researcher at Karlsruhe Institute of Technology (KIT), Germany. Additionally 
he is pursuing a Ph.D. title. His primary research and recent publications are about advanced light transport 
methods for global illumination. Prior to joining academia Anton had been working at Crytek for three 
years at vari­ous positions from senior R&#38;D graphics engineer to lead researcher. He received his 
M.Sc. in Applied Mathematics at National Research University of Electronic Technol­ogy, Moscow in 2007. 
Juan Ca Next Limit Technologies nada juan.canada@nextlimit.com Juan joined Next Limit in 2004 to work 
in the Real.ow development team and later he moved to the newborn Maxwell research team. Since then Juan 
held several positions in the team, leading it since 2007. He holds a bachelors degree in Mechanical 
Engineering and a degree in Environmental Sciences. 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2013</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>2504440</article_id>
		<sort_key>50</sort_key>
		<display_label>Article No.</display_label>
		<display_no>5</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>5</seq_no>
		<title><![CDATA[The digital production pipeline]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2504435.2504440</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2504440</url>
		<abstract>
			<par><![CDATA[<p>It used to be about getting analogue data in and out of the digital world. Today it's about connecting those various digital worlds into a fluid creative process.</p> <p>While studios look to tighten budgets, minimize risk and broaden films' reach worldwide across platforms, there is no standard operating procedure for studios to produce content. Shorter schedules, and globalization are as disruptive to the digital pipeline today as non-liner editing and digital cameras were to their analogue counterparts.</p> <p>We'll walk through digital pipelines, expose a few of the methods and workflows that haven't changed for far too long, and take a look at trends in production workflows that will allow studios to quickly adapt to these ever changing environments.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4193489</person_id>
				<author_profile_id><![CDATA[82458754257]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Darin]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Grant]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Method Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4193490</person_id>
				<author_profile_id><![CDATA[82458844357]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Kim]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Libreri]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[LucasFilm]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4193491</person_id>
				<author_profile_id><![CDATA[82458895257]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Steve]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lavietes]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Sony Pictures Imageworks]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4193492</person_id>
				<author_profile_id><![CDATA[82459278057]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Jonathan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gibbs]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[PDI/Dreamworks]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4193493</person_id>
				<author_profile_id><![CDATA[82458761357]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Barbara]]></first_name>
				<middle_name><![CDATA[Ford]]></middle_name>
				<last_name><![CDATA[Grant]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[How to Make Good Pictures, LLC]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2013</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>2504441</article_id>
		<sort_key>60</sort_key>
		<display_label>Article No.</display_label>
		<display_no>6</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>6</seq_no>
		<title><![CDATA[Turbulent fluids]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2504435.2504441</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2504441</url>
		<abstract>
			<par><![CDATA[<p>Over the last decade, the visual effects industry has embraced physics simulations as a highly useful tool for creating realistic scenes ranging from a small camp fire to large-scale destruction of whole cities. While fluid simulations are now widely used in the industry, it is still inherently difficult to control large-scale simulations, and there is a constant struggle to increase visual detail.</p> <p>This course approaches these problems using turbulence methods. Turbulent detail is what makes typical fluid simulations look impressive, and the underlying physics motivate a powerful approach for control; they allow for an elegant split of large-scale motion and small-scale turbulent detail. The result is a two-stage work flow that is highly convenient for artists. First, a rough, fast initial simulation is performed, then turbulent effects are added to enhance detail.</p> <p>After reviewing the basics of fluid solvers and the popular wavelet turbulence approach, the course presents several powerful methods for capturing advanced effects such as boundary layers and turbulence with directional preferences. It also explains the difficulties of liquid simulations and presents an approach to liquid turbulence that is based on wave dynamics.</p> <p>Full source code for all of the methods covered in the course is available to attendees. Instructors outline convenient starting points for navigating the code.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4193494</person_id>
				<author_profile_id><![CDATA[81385603331]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Nils]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Thuerey]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[ScanlineVFX GmbH]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4193495</person_id>
				<author_profile_id><![CDATA[82458964557]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Theodore]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kim]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of California, Santa Barbara]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4193496</person_id>
				<author_profile_id><![CDATA[81448598993]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Tobias]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Pfaff]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of California, Berkeley]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2013</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>2504442</article_id>
		<sort_key>70</sort_key>
		<display_label>Article No.</display_label>
		<pages>126</pages>
		<display_no>7</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>7</seq_no>
		<title><![CDATA[Digital geometry processing with discrete exterior calculus]]></title>
		<page_from>1</page_from>
		<page_to>126</page_to>
		<doi_number>10.1145/2504435.2504442</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2504442</url>
		<abstract>
			<par><![CDATA[<p>An introduction to geometry processing using discrete exterior calculus (DEC), which provides a simple, flexible, and efficient framework for building a unified geometry-processing platform. The course provides essential mathematical background as well as a large array of real-world examples. It also provides a short survey of the most relevant recent developments in digital geometry processing and discrete differential geometry. Compared to previous SIGGRAPH courses, this course focuses heavily on practical aspects of DEC, with an emphasis on implementation and applications.</p> <p>The course begins with the core ideas from exterior calculus, in both the smooth and discrete setting. Then it shows how a large number of fundamental geometry-processing tools (smoothing, parameterization, geodesics, mesh optimization, etc.) can be implemented quickly, robustly, and efficiently within this single common framework. It concludes with a discussion of recent extensions of DEC that improve efficiency, accuracy, and versatility.</p> <p>The course notes grew out of the discrete differential geometry course taught over the past five years at the California Institute of Technology, for undergraduates and beginning graduate students in computer science, applied mathematics, and associated fields. The notes also provide guided exercises (both written and coding) that attendees can later use to deepen their understanding of the material.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4193499</person_id>
				<author_profile_id><![CDATA[81315488302]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Keenan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Crane]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[California Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4193498</person_id>
				<author_profile_id><![CDATA[81319490357]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Fernando]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[de Goes]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[California Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4193500</person_id>
				<author_profile_id><![CDATA[81100041821]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Mathieu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Desbrun]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[California Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4193501</person_id>
				<author_profile_id><![CDATA[81100117380]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Peter]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Schr&#246;der]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[California Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>344859</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[{ACOL00} Marc Alexa, Daniel Cohen-Or, and David Levin. As-Rigid-as-Possible Shape Interpolation. In <i>Proc. ACM SIGGRAPH</i>, pages 157--164, 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[{CDS10} Keenan Crane, Mathieu Desbrun, and Peter Schr&#246;der. Trivial Connections on Discrete Surfaces. <i>Comp. Graph. Forum</i>, 29(5):1525--1533, 2010.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[{CWW13} Keenan Crane, Clarisse Weischedel, and Max Wardetzky. Geodesics in Heat: A New Approach to Computing Distance Based on Heat Flow. <i>ACM Trans. Graph</i>., 2013.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[{dGC10} Fernando de Goes and Keenan Crane. Trivial Connections Revisited: A Simplified Algorithm for Simply-Connected Surfaces, 2010.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[{DHLM05} Mathieu Desbrun, Anil Hirani, Melvin Leok, and Jerrold Marsden. Discrete Exterior Calculus. <i>ArXiv e-prints</i>, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[{DKT08} Mathieu Desbrun, Eva Kanso, and Yiying Tong. Discrete Differential Forms for Computational Modeling. In Alexander I. Bobenko, Peter Schr&#246;der, John M. Sullivan, and G&#252;nther M. Ziegler, editors, <i>Discrete Differential Geometry</i>, volume 38 of <i>Oberwolfach Seminars</i>, pages 287--324. Birkh&#228;user Verlag, 2008.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>311576</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[{DMSB99} Mathieu Desbrun, Mark Meyer, Peter Schr&#246;der, and Alan Barr. Implicit Fairing of Irregular Meshes using Diffusion and Curvature Flow. In <i>Proc. ACM SIGGRAPH</i>, pages 317--324, 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>644208</ref_obj_id>
				<ref_obj_pid>644108</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[{Epp03} David Eppstein. Dynamic Generators of Topologically Embedded Graphs. In <i>Proc. ACM-SIAM Symp. Disc. Alg. (SODA)</i>, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1070581</ref_obj_id>
				<ref_obj_pid>1070432</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[{EW05} Jeff Erickson and Kim Whittlesey. Greedy Optimal Homotopy and Homology Generators. In <i>Proc. ACM-SIAM Symp. Disc. Alg. (SODA)</i>, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[{Hir03} Anil Hirani. <i>Discrete Exterior Calculus</i>. PhD thesis, Pasadena, CA, USA, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[{KCPS13} Felix Kn&#246;ppel, Keenan Crane, Ulrich Pinkall, and Peter Schr&#246;der. Globally Optimal Direction Fields. In <i>Proc. ACM SIGGRAPH</i>, 2013.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[{Mac49} Richard MacNeal. <i>The Solution of Partial Differential Equations by means of Electrical Networks</i>. PhD thesis, Caltech, 1949.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1964998</ref_obj_id>
				<ref_obj_pid>1964921</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[{MMdGD11} Patrick Mullen, Pooran Memari, Fernando de Goes, and Mathieu Desbrun. HOT: Hodge-optimized triangulations. In <i>Proc. ACM SIGGRAPH</i>, 2011.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1731335</ref_obj_id>
				<ref_obj_pid>1731309</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[{MTAD08} Patrick Mullen, Yiying Tong, Pierre Alliez, and Mathieu Desbrun. Spectral Conformal Parameterization. <i>Comp. Graph. Forum</i>, 27(5):1487--1494, 2008.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1281983</ref_obj_id>
				<ref_obj_pid>1281957</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[{TACSD06} Y. Tong, P. Alliez, D. Cohen-Steiner, and M. Desbrun. Designing Quadrangulations with Discrete Harmonic Forms. In <i>Proc. Symp. Geom. Proc</i>., 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 DI G I TA L GE O M E T RY PR O C E S S I N G W I T H DI S C R E T E EX T E R I O R CA L C U L U S  
Keenan Crane SIGGRAPH 2013 Course Notes Lecturers: Fernando de Goes Keenan Crane Mathieu Desbrun Peter 
Schr ¨oder Contents Chapter 1. Introduction 3 1.1. Disclaimer 5 1.2. Copyright 5 1.3. Acknowledgements 
5 Chapter 2. A Quick and Dirty Introduction to Differential Geometry 6 2.1. The Geometry of Surfaces 
6 2.2. Derivatives and Tangent Vectors 10 2.3. The Geometry of Curves 13 2.4. Curvature of Surfaces 16 
2.5. Geometry in Coordinates 20 Chapter 3. A Quick and Dirty Introduction to Exterior Calculus 24 3.1. 
Vectors and 1-Forms 24 3.2. Differential Forms and the Wedge Product 28 3.3. Hodge Duality 32 3.4. Differential 
Operators 37 3.5. Integration and Stokes Theorem 43 3.6. Discrete Exterior Calculus 47 Chapter 4. Topological 
Invariants of Discrete Surfaces 54 4.1. Euler Characteristic 54 4.2. Regular Meshes and Average Valence 
55 4.3. Gauss-Bonnet 56 Chapter 5. Normals of Discrete Surfaces 58 5.1. Vector Area 58 5.2. Area Gradient 
61 5.3. Volume Gradient 63 5.4. Other De.nitions 64 Chapter 6. The Laplacian 68 6.1. Basic Properties 
68 6.2. Discretization via FEM 71 6.3. Discretization via DEC 75 6.4. Meshes and Matrices 77 6.5. The 
Poisson Equation 79 6.6. Implicit Mean Curvature Flow 80 Chapter 7. Surface Parameterization 83 7.1. 
Conformal Structure 85 7.2. The Cauchy-Riemann Equation 86 7.3. Differential Forms on a Riemann Surface 
87 1 CONTENTS 7.4. Conformal Parameterization 89 7.5. Eigenvectors, Eigenvalues, and Optimization 92 
Chapter 8. Vector Field Decomposition and Design 98  8.1. Hodge Decomposition 99 8.2. Homology Generators 
and Harmonic Bases 106 8.3. Connections and Parallel Transport 110 8.4. Vector Field Design 116 Chapter 
9. Conclusion 120 Bibliography 121 Appendix A. A Nice Formula for Normal Curvature 123 Appendix B. Why 
Are Principal Directions Orthogonal? 125 CHAPTER 1   Introduction These notes provide an introduction 
to working with real-world geometric data, expressed in the language of discrete exterior calculus (DEC). 
DEC is a simple, .exible, and ef.cient frame­work which provides a uni.ed platform for geometry processing. 
The notes provide essential mathematical background as well as a large array of real-world examples, 
with an emphasis on applications and implementation. The material should be accessible to anyone with 
some exposure to basic linear algebra and vector calculus, though most of the key concepts are reviewed 
as needed. Coding exercises depend on a basic knowledge of C++, though knowledge of any programming language 
is likely suf.cient: we do not make heavy use of paradigms like inheritance, templates, etc. The notes 
also provide guided written exercises that can be used to deepen understanding of the material. Why use 
exterior calculus? There are, after all, many other ways to describe algorithms for mesh processing. 
One reason has to do with language: the exterior calculus of differential forms is, to a large degree, 
the modern language of differential geometry and mathematical physics. By learning to speak this language 
we can draw on a wealth of existing knowledge to develop new algorithms, and better understand current 
algorithms in terms of a well-developed theory. It also allows us to easily write down and implement 
many seemingly disparate algorithms in a single, uni.ed framework. In these notes, for instance, we ll 
see how a large number of basic geometry processing tasks (smoothing, parameterization, vector .eld design, 
etc.) can be expressed in only a few lines of code, typically by solving a simple Poisson equation. There 
is another good reason for taking this approach, beyond simply saying the same thing in a different way. 
By .rst formulating algorithms in the smooth geometric setting, we can ensure that essential structures 
are subsequently preserved at the discrete level. As one elementary example, consider the vertex depicted 
above. If we take the sum of the tip angles .i, we get a number that is (in general) different from 2p. 
On any smooth surface, however, we expect this number to be exactly 2p said in a differential-geometric 
way: the tangent space at any point should consist of a whole circle of directions. Of course, if we 
consider .ner and .ner approximations of 3 1. INTRODUCTION a smooth surface by a triangle mesh, the vertex 
will eventually .atten out and our angle sum will indeed approach 2p as expected. But there is an attractive 
alternative even at the coarse level: we can rede.ne the meaning of angle so that it always yields the 
expected result. In particular, let 2p s := .i .i be the ratio between the angle sum 2p that we anticipate 
in the smooth setting, and the Euclidean angle sum .i .i exhibited by our .nite mesh, and consider the 
augmented angles .i := s.i. In other words, we simply normalize the usual Euclidean angles such that 
they sum to exactly 2p, no matter how coarse our mesh is: . . i = s . .i = 2p. i i From here we can carry 
out all the rest of our calculations as usual, using the augmented or discrete angles . i rather than 
the usual Euclidean angles .i. Conceptually, we can imagine that each vertex has been smoothed out slightly, 
effectively pushing the curvature of our surface into otherwise .at triangles. This particular convention 
may not always (or even often) be useful, but in problems where the tangent space structure of a surface 
is critical it leads to highly effective algorithms for mesh processing (see in particular [KCPS13]). 
This message is one theme we ll encounter frequently in these notes: there is no one right way to discretize 
a given geometric quantity, but rather many different ways, each suited to a particular purpose. The 
hope, then, is that one can discretize a whole theory such that all the pieces .t together nicely. DEC 
is one such theory, which has proven to be highly successful at preserving the homological structure 
of a surface, as we ll discuss in Chapter 8. The remainder of these notes proceeds as follows. We .rst 
give an overview of the differential geometry of surfaces (Chapter 2), using a description that leads 
naturally into a discussion of smooth exterior calculus (Chapter 3) and its discretization via DEC. We 
then study some basic properties of discrete surfaces (Chapter 4) and their normals (Chapter 5), leading 
up to an equation that is central to our applications: the discrete Poisson equation (Chapter 6). The 
remaining chapters investigate various geometry processing applications, introducing essential geometric 
concepts along the way (conformal structure, homology, parallel transport, etc.). Coding exercises refer 
to a supplementary C++ framework, available from https://github.com/dgpdec/course which includes basic 
mesh data structures, linear algebra libraries, and visualization tools any similar framework or library 
would be suitable for completing these exercises. Solutions to written exercises are available upon request. 
Our goal throughout these notes was to describe every concept in terms of a concrete geometric picture 
we have tried as much as possible to avoid abstract algebraic arguments. Likewise, to get the most out 
of the written exercises one should try to make an intuitive geometric argument .rst, and only later 
.ll in the formal details. 1.3. ACKNOWLEDGEMENTS 1.1. Disclaimer  These notes are very much a work in 
progress and there will be errors. As always, your brain is the best tool for determining whether or 
not a statement is actually true! If you do encounter errors, please do not hesitate to contact the author 
(noting the page number and the relevant version of the notes). 1.2. Copyright Images were produced solely 
by the author with the exception of the Stanford Bunny mesh, which is provided courtesy of the Stanford 
Graphics Computer Laboratory. The remaining images are licensed under the Creative Commons Attribution-NonCommercial-NoDerivs 
3.0 Unported License. To view a copy of this license, visit http://creativecommons.org/licenses/ by-nc-nd/3.0/. 
 1.3. Acknowledgements These notes grew out of a Caltech course on discrete differential geometry (DDG) 
over the past few years. Peter Schr ¨ oder, Max Wardetzky, and Clarisse Weischedel provided invaluable 
feedback for the .rst draft of many of these notes; Mathieu Desbrun, Fernando de Goes, and Corentin Wallez 
provided extensive feedback on the SIGGRAPH 2013 revision. Thanks to Mark Pauly s group at EPFL for suffering 
through (very) early versions of these lectures. Thanks also to those who have pointed out errors over 
the years: Mirela Ben-Chen, Nina Amenta, Chris Wojtan, Yuliy Shcwarzburg, Robert Luo, Andrew Butts, Scott 
Livingston, and all the students in CS177 at Caltech, as well as others who I am currently forgetting! 
Most of the algorithms described in these notes appear in previous literature. The method for mean curvature 
.ow appears in [DMSB99]. The conformal parameterization scheme described in Chapter 7 is based on [MTAD08]. 
The approach to discrete Helmholtz-Hodge decomposition described in Chapter 8 is based on the scheme 
described in [DKT08]. The method for computing smooth vector .elds with prescribed singularities is based 
on [CDS10]; the improvement using Helmholtz-Hodge decomposition (Section 8.4.1) is previously unpublished 
and due to Fernando de Goes [dGC10]. More material on DEC itself can be found in a variety of sources 
[Hir03, DHLM05, DKT08]. Finally, the cotan-Laplace operator central to many of these algorithms has a 
long history, dating back at least as far as [Mac49]. CHAPTER 2 A Quick and Dirty Introduction to Differential 
Geometry  2.1. The Geometry of Surfaces There are many ways to think about the geometry of a surface 
(using charts, for instance) but here s a picture that is well-suited to the way we work with surfaces 
in the discrete setting. Consider a little patch of material .oating in space, as depicted below. Its 
geometry can be described via a map f : M . R3 from a region M in the Euclidean plane R2 to a subset 
f (M) of R3: f N df (X) f (M) X M The differential of such a map, denoted by df , tells us how to map 
a vector X in the plane to the corresponding vector df (X) on the surface. Loosely speaking, imagine 
that M is a rubber sheet and X is a little black line segment drawn on M. As we stretch and deform M 
into f (M), the segment X also gets stretched and deformed into a different segment, which we call df 
(X). Later on we can talk about how to explicitly express df (X) in coordinates and so on, but it s important 
to realize that fundamentally there s nothing deeper to know about the differential than the picture 
you see here the differential simply tells you how to stretch out or push forward vectors as you go from 
one space to another. For example, the length of a tangent vector X pushed forward by f can be expressed 
as df (X) · df (X), where · is the standard inner product (a.k.a. dot product or scalar product) on 
R3 . Note that this length is typically different than the length of the vector we started with! To keep 
things clear, we ll use angle brackets to denote the inner product in the plane, e.g., the length of 
the original vector r would be (X, X). More generally, we can measure the inner product between any two 
tangent vectors df (X) and df (Y): g(X, Y) = df (X) · df (Y). The map g is called the metric of the surface, 
or to be more pedantic, the metric induced by f . Note that throughout we will use df (X) interchangeably 
to denote both the pushforward of a single 6  2.1. THE GEOMETRY OF SURFACES vector or an entire vector 
.eld, i.e., a vector at every point of M. In most of the expressions we ll consider this distinction 
won t make a big difference, but it s worth being aware of. So far we ve been talking about tangent vectors, 
i.e., vectors that lay .at along the surface. We re also interested in vectors that are orthogonal to 
the surface. In particular, we say that a vector u . R3 is normal to the surface at a point p if df (X) 
· u = 0 for all tangent vectors X at p. For convenience, we often single out a particular normal vector 
N called the unit normal, which has length one. Of course, at any given point there are two distinct 
unit normal vectors: +N and -N. Which one should we use? If we can pick a consistent direction for N 
then we say that M is orientable. For instance, the circular band on the left is orientable, but the 
M ¨obius band on the right is not: For orientable surfaces, we can actually think of N as a continuous 
map N : M . S2 (called the Gauss map) which associates each point with its unit normal, viewed as a point 
on the unit sphere S2 . R3 . In fact, why not think of N as simply a different geometry for M? Now the 
map d N (called the Weingarten map) tells us about the change in the normal direction as we move from 
one point to the other. For instance, we could look at the change in normal along a particular tangent 
direction X by evaluating df (X) · d N (X) .n(X) = . |df (X)|2 (The factor |df (X)|2 in the denominator 
simply accounts for any stretching out of X that occurs as we go from the plane to the surface.) The 
quantity .n is called the normal curvature we ll have a lot more to say about curvature in the future. 
Overall we end up with the following picture, which captures the most fundamental ideas about the geometry 
of surfaces: 2.1. THE GEOMETRY OF SURFACES 2.1.1. Conformal Coordinates. When working with curves, one 
often introduces the idea of an isometric (a.k.a. arc-length or unit speed) parameterization. The idea 
there is to make certain expressions simpler by assuming that no stretching occurs as we go from the 
domain into R3 . One way to state this is requirement is |df (X)| = |X|, i.e., we ask that the norm of 
any vector X is preserved. For surfaces, an isometric parameterization does not always exist (not even 
locally!). Most of the time you simply have to stretch things out. For instance, you may know that it 
s impossible to .atten the surface of the Earth onto the plane without distortion that s why we end up 
with all sorts of different funky projections of the globe. However, there is a setup that (like arc-length 
parameterization for curves) makes life a lot easier when dealing with certain expressions, namely conformal 
coordinates. Put quite simply, a map f is conformal if it preserves the angle between any two vectors. 
More speci.cally, a conformal map f : R2 . M . R3 satis.es df (X) · df (Y) = a(X, Y)for all tangent vectors 
X, Y, where a is a positive function and (·, ·) is the usual inner product on R2 . In practice, the function 
a is often replaced with eu for some real-valued function u this way, one never has to worry about whether 
the scaling is positive. Notice that vectors can still get stretched out, but the surface never gets 
sheared for instance, orthogonal vectors always stay orthogonal: 2.1. THE GEOMETRY OF SURFACES A key 
fact about conformal maps is that they always exist, as guaranteed by the uniformization theorem. In 
a nutshell, the uniformization theorem says that any disk can be conformally mapped to the plane. So 
if we consider any point p on our surface f (M), we know that we can always .nd a conformal parameterization 
in some small, disk-like neighborhood around p. As with unit-speed curves, it is often enough to simply 
know that a conformal parameterization exists we do not have to construct the map explicitly. And, as 
with arc-length parameterization, we have to keep track of the least possible amount of information about 
how the domain gets stretched out: just a single number at each point (as opposed to, say, an entire 
Jacobian matrix). 2.2. DERIVATIVES AND TANGENT VECTORS 2.2. Derivatives and Tangent Vectors 2.2.1. Derivatives 
on the Real Line. So far we ve been thinking about the differential in a very geometric way: it tells 
us how to stretch out or push forward tangent vectors as we go from one place to another. In fact, we 
can apply this geometric viewpoint to pretty much any situation involving derivatives. For instance, 
think about a good old fashioned real-valued function f(x) on the real line. We typically visualize f 
by plotting its value as a height over the x-axis: f(x) x In this case, the derivative f' can be interpreted 
as the slope of the height function, as suggested by the dashed line in the picture above. Alternatively, 
we can imagine that f stretches out the real line itself, indicated by the change in node spacing in 
this picture: X R f df(X) f(R) Where the derivative is large, nodes are spaced far apart; where the 
derivative is small, nodes are spaced close together. This picture inspires us to write the derivative 
of f in terms of the push-forward df(X) of a unit tangent vector X pointing along the positive x-axis: 
f' = df(X). In other words, the derivative of f is just the stretch factor as we go from one copy of 
R to the other. But wait a minute does this equality even make sense? The thing on the left is a scalar, 
but the thing on the right is a vector! Of course, any tangent vector on the real line can be represented 
as just a single value, quantifying its extent in the positive or negative direction. So this expression 
does make sense as long as we understand that we re identifying tangent vectors on R with real numbers. 
Often this kind of type checking can help verify that formulas and expressions are correct, similar to 
the way you might check for matching units in a physical equation. 2.2. DERIVATIVES AND TANGENT VECTORS 
Here s another question: how is this interpretation of the derivative any different from our usual interpretation 
in terms of height functions? Aren t we also stretching out the real line in that case? Well, yes and 
no certainly the real line still gets stretched out into some other curve. But this curve is now a subset 
of the plane R2 in particular, it s the curve . = (x, f(x)). So for one thing, type checking fails in 
this case: f ' is a scalar, but d.(X) is a 2-vector. But most importantly, the amount of stretching experienced 
by the curve doesn t correspond to our usual notion of the r derivative of f for instance, if we look 
at the magnitude of |d.(X)| we get 1 + (f ')2. (Why is this statement true geometrically? How could you 
write f ' in terms of d.(X)? Can you come up with an expression that recovers the proper sign?) 2.2.2. 
Directional Derivatives. So far so good: we can think of the derivative of a real-valued function on 
R as the pushforward of a (positively-oriented) unit tangent vector X. But what does df(X) mean if f 
is de.ned over some other domain, like the plane R2? This question may stretch your mind a little, but 
if you can understand this example then you re well on your way to understanding derivatives in terms 
of tangent vectors. Let s take a look at the geometry of the problem again, there are two ways we could 
plot f. The usual approach is to draw a height function over the plane: The derivative has something 
to do with the slope of this hill, but in which direction? To answer this question, we can introduce 
the idea of a directional derivative i.e., we pick a vector X and see how quickly we travel uphill (or 
downhill) in that direction. And again we can consider an alternative picture:  2.2. DERIVATIVES AND 
TANGENT VECTORS Since f is a map from R2 to R, we can imagine that it takes a .at sheet of rubber and 
stretches it out into a long, skinny, one-dimensional object along the real line. Therefore if we draw 
an arrow X on the original sheet, then the stretched-out arrow df(X) gives us the rate of change in f 
along the direction X, i.e., the directional derivative. What about type checking? As before, everything 
matches up: df(X) is a tangent vector on R, so it can be represented by a single real number. (What if 
we had continued to work with the height function above? How could we recover the directional derivative 
in this case?) By the way, don t worry if this discussion seems horribly informal! We ll see a more explicit, 
algebraic treatment of these ideas when we start talking about exterior calculus. The important thing 
for now is to build some geometric intuition about derivatives. In particular: a map from any space to 
any other space can be viewed as some kind of bending and twisting and stretching (or possibly tearing!); 
derivatives can be understood in terms of what happens to little arrows along the way. 2.3. THE GEOMETRY 
OF CURVES 2.3. The Geometry of Curves  The picture we looked at for surfaces is actually a nice way 
of thinking about shapes of any dimension. For instance, we can think of a one-dimensional curve as a 
map . : I . R3 from an interval I = [0, T] . R of the real line to R3. Again the differential d. tells 
us how tangent vectors get stretched out by ., and again the induced length of a tangent vector X is 
given by |d.(X)| = d.(X) · d.(X). Working with curves is often easier if . preserves length, i.e., if 
for every tangent vector X we have |d.(X)| = |X|. There are various names for such a parameterization 
( unit speed , arc-length , isometric ) but the idea is simply that the curve doesn t get stretched out 
when we go from R to R3 think of . as a completely relaxed rubber band. This unit-speed view is also 
often the right one for the discrete setting where we have no notion of a base domain I from the very 
beginning, the curve is given to us as a subset of R3 and all we can do is assume that it sits there 
in a relaxed state. 2.3.1. The Frenet Frame. Suppose we have a unit-speed curve . and a positively-oriented 
unit vector X on the interval I. Then T = d.(X) is a unit vector in R3 tangent to the curve. Carrying 
this idea one step further, we can look at the change in tangent direction as we move along .. Since 
T may change at any rate (or not at all!) we 2.3. THE GEOMETRY OF CURVES split up the change into two 
pieces: a unit vector N called the principal normal that expresses the direction of change, and a scalar 
. . R called the curvature that expresses the magnitude of change: dT(X) = .N. One thing to realize 
is that T and N are always orthogonal. Why? Because if the change in T were parallel to T, then it would 
cease to have unit length! (This argument is a good one to keep in mind any time you work with unit vector 
.elds.) Together with a third vector B = T × N called the binormal, we end up with a very natural orthonormal 
coordinate frame called the Frenet frame. How does this frame change as we move along the curve? The 
answer is given by the Frenet-Serret formula: . .. ... T' 0 . 0 T . B' . = . -. 0 t . . N . . N' 0 -t 
0 B . .. . . .. . . .. . Q'.R3 A.R3 Q.R3 Here T, N, and B are interpreted as row vectors, and a prime 
indicates the change in a quantity as we move along the curve at unit speed. For instance, T' = dT (X), 
where X is a positively-oriented unit vector on I. The quantity t is called the torsion, and describes 
the way the normal and binormal twist around the curve. A concise proof of this formula was given by 
Cartan. First, since the vectors T, N, and B are mutually orthogonal, one can easily verify that QQT 
= I, i.e., Q is an orthogonal matrix. Differenti­ating this relationship in time, the identity vanishes 
and we re left with Q'QT = -(Q'QT )T, i.e., the matrix Q'QT is skew-symmetric. But since A = Q'QT , A 
must also be skew-symmetric. Skew symmetry implies that the diagonal of A is zero (why?) and moreover, 
we already know what the top row (and hence the left column) looks like from our de.nition of . and N. 
The remaining value A23 = -A32 is not constrained in any way, so we simply give it a name: t . R. What 
do you think about this proof? On the one hand it s easy to verify; on the other hand, it provides little 
geometric understanding. For instance, why does N change in the direction of both T and B, but B changes 
only in the direction of N? Can you come up with more geometric arguments? 2.3.2. Visualizing Curvature. 
What s the curvature of a circle S? Well, if S has radius r then it takes time 2pr to go all the way 
around the circle at unit speed. During this time the tangent turns around by an angle 2p. Of course, 
since T has unit length the instantaneous change in T is described exclusively by the instantaneous change 
in angle. So we end up with . = |.N| = |dT (X)| = 2p/2pr = 1/r. In other words, the curvature of a circle 
is simply the reciprocal of its radius. This fact should make some intuitive sense: if we watch a circle 
grow bigger and bigger, it eventually looks just like a straight line with zero curvature: limr.8 1/r 
= 0. Similarly, if we watch a circle get smaller and smaller it eventually looks like a single point 
with in.nite curvature: limr.0 1/r = 8. 2.3. THE GEOMETRY OF CURVES Now consider a smooth curve . in 
the plane. At any point p . . there is a circle S called the osculating circle that best approximates 
., meaning that it has the same tangent direction T and curvature vector .N. In other words, the circle 
and the curve agree up to second order. (The phrase agree up to nth order is just shorthand for saying 
that the .rst n derivatives are equal.) How do we know such a circle exists? Easy: we can always construct 
a circle with the appropriate curvature by setting r = 1/.; moreover every circle has some tangent pointing 
in the direction T. Alternatively, we can consider the a circle passing through p and two other points: 
one approaching from the left, another approaching from the right. Since these three points are shared 
by both . and S, the .rst and second derivatives will agree in the limit (consider that these points 
can be used to obtain consistent .nite difference approximations of T and .N). The radius and center 
of the osculating circle are often referred to as the radius of curvature and center of curvature, respectively. 
We can tell this same story for any curve in R3 by considering the osculating plane T × N, since this 
plane contains both the tangent and the curvature vector. For curves it makes little difference whether 
we express curvature in terms of a change in the tangent vector or a change in the (principal) normal, 
since the two vectors are the same up to a quarter-rotation in the osculating plane. For surfaces, however, 
it will often make more sense to think of curvature as the change in the normal vector, since we typically 
don t have a distinguished tangent vector to work with. 2.4. CURVATURE OF SURFACES   2.4. Curvature 
of Surfaces Let s take a more in-depth look at the curvature of surfaces. The word curvature really 
corresponds to our everyday understanding of what it means for something to be curved: eggshells, donuts, 
and cavatappi pasta have a lot of curvature; .oors, ceilings, and cardboard boxes do not. But what about 
something like a beer bottle? Along one direction the bottle quickly curves around in a circle; along 
another direction it s completely .at and travels along a straight line: This way of looking at curvature 
in terms of curves traveling along the surface is often how we treat curvature in general. In particular, 
let X be a unit tangent direction at some distinguished point on the surface, and consider a plane containing 
both df (X) and the corresponding normal N. This plane intersects the surface in a curve, and the curvature 
.n of this curve is called the normal curvature in the direction X: Remember the Frenet-Serret formulas? 
They tell us that the change in the normal along a curve is given by d N = -.T + tB. We can therefore 
get the normal curvature along X by extracting the tangential part of d N: -df (X) · d N (X) .n(X) = 
. |df (X)|2 2.4. CURVATURE OF SURFACES The factor |df (X)|2 in the denominator simply normalizes any 
stretching out that occurs as we go from the domain M into R3 a derivation of this formula can be found 
in Appendix A. Note that normal curvature is signed, meaning the surface can bend toward the normal or 
away from it. 2.4.1. Principal, Mean, and Gaussian Curvature. At any given point we can ask: along which 
directions does the surface bend the most? The unit vectors X1 and X2 along which we .nd the maximum 
and minimum normal curvatures .1 and .2 are called the principal directions; the curvatures .i are called 
the principal curvatures. For instance, the beer bottle above might have principal curvatures .1 = 1, 
.2 = 0 at the marked point. We can also talk about principal curvature in terms of the shape operator, 
which is the unique map S : T M . T M satisfying df (SX) = d N (X) for all tangent vectors X. The shape 
operator S and the Weingarten map d N essentially represent the same idea: they both tell us how the 
normal changes as we travel along a direction X. The only difference is that S speci.es this change in 
terms of a tangent vector on M, whereas d N gives us the change as a tangent vector in R3. It s worth 
noting that many authors do not make this distinction, and simply assume an isometric identi.cation of 
tangent vectors on M and the corresponding tangent vectors in R3. However, we choose to be more careful 
so that we can explicitly account for the dependence of various quantities on the immersion f this dependence 
becomes particularly important if you actually want to compute something! (By the way, why can we always 
express the change in N in terms of a tangent vector? It s because N is the unit normal, hence it cannot 
grow or shrink in the normal direction.) One important fact about the principal directions and principal 
curvatures is that they corre­spond to eigenvectors and eigenvalues (respectively) of the shape operator: 
SXi = .i Xi. Moreover, the principal directions are orthogonal with respect to the induced metric: g(X1, 
X2) = df (X1) · df (X2) = 0 see Appendix B for a proof of these two facts. The principal curvatures therefore 
tell us everything there is to know about normal curvature at a point, since we can express any tangent 
vector Y as a linear combination of the principal directions X1 and X2. In particular, if 2.4. CURVATURE 
OF SURFACES Y is a unit vector offset from X1 by an angle ., then the associated normal curvature is 
.n(Y) = .1 cos2 . + .2 cos2 ., as you should be able to easily verify using the relationships above. 
Often, however, working directly with principal curvatures is fairly inconvenient especially in the discrete 
setting. On the other hand, two closely related quantities called the mean curvature and the Gaussian 
curvature will show up over and over again (and have some particularly nice interpretations in the discrete 
world). The mean curvature H is the arithmetic mean of principal curvatures: .1 + .2 H = , 2 and the 
Gaussian curvature is the (square of the) geometric mean: K = .1.2. What do the values of H and K imply 
about the shape of the surface? Perhaps the most elementary interpretation is that Gaussian curvature 
is like a logical and (is there curvature along both directions?) whereas mean curvature is more like 
a logical or (is there curvature along at least one direction?) Of course, you have to be a little careful 
here since you can also get zero mean curvature when .1 = -.2. It also helps to see pictures of surfaces 
with zero mean and Gaussian curvature. Zero-curvature surfaces are so well-studied in mathematics that 
they have special names. Surfaces with zero Gaussian curvature are called developable surfaces because 
they can be developed or .attened out into the plane without any stretching or tearing. For instance, 
any piece of a cylinder is developable since one of the principal curvatures is zero: Surfaces with 
zero mean curvature are called minimal surfaces because (as we ll see later) they minimize surface area 
(with respect to certain constraints). Minimal surfaces tend to be saddle-like since principal curvatures 
have equal magnitude but opposite sign:  2.4. CURVATURE OF SURFACES The saddle is also a good example 
of a surface with negative Gaussian curvature. What does a surface with positive Gaussian curvature look 
like? The hemisphere is one example: Note that in this case .1 = .2 and so principal directions are 
not uniquely de.ned maximum (and minimum) curvature is achieved along any direction X. Any such point 
on a surface is called an umbilic point. There are plenty of cute theorems and relationships involving 
curvature, but those are the basic facts: the curvature of a surface is completely characterized by the 
principal curvatures, which are the maximum and minimum normal curvatures. The Gaussian and mean curvature 
are simply averages of the two principal curvatures, but (as we ll see) are often easier to get your 
hands on in practice. 2.4.2. The Fundamental Forms. For historical reasons, there are two objects we 
should proba­bly mention: .rst fundamental form I and the second fundamental form II . I m actually not 
sure what s so fundamental about these forms, since they re nothing more than a mashup of the metric 
g and the shape operator S, which themselves are simple functions of two truly fundamental objects: the 
immersion f and the Gauss map N. In fact, the .rst fundamental form is literally just the induced metric, 
i.e., I(X, Y) := g(X, Y). The second fundamental form looks quite similar to our existing expression 
for normal curvature: II (X, Y) := -g(SX, Y) = -d N (X) · df (Y). The most important thing to realize 
is that I and II do not introduce any new geometric ideas just another way of writing down things we 
ve already seen. 2.5. GEOMETRY IN COORDINATES 2.5. Geometry in Coordinates  So far we ve given fairly 
abstract descriptions of the geometric objects we ve been working with. For instance, we said that the 
differential df of an immersion f : M . R3 tells us how to stretch out tangent vectors as we go from 
the domain M . R2 into the image f (M) . R3. Alluding to the picture above, we can be a bit more precise 
and de.ne df (X) in terms of limits: f (p + hX) - f (p) df p(X) = lim . h.0 h Still, this formula remains 
a bit abstract we may want something more concrete to work with in practice. When we start working with 
discrete surfaces we ll see that df (X) often has an incredibly concrete meaning for instance, it might 
correspond to an edge in our mesh. But in the smooth setting a more typical representation of df is the 
Jacobian matrix . f 1/.x. 1 2 . f 1/.x1 2 J = . . f 2/.xf 2/.x. . 1 2 . f 3/.xf 3/.x Here we pick coordinates 
on R2 and R3, and imagine that 1 111 f (x, x2) = ( f1(x, x2), f2(x, x2), f3(x, x2)) for some triple of 
scalar functions f1, f2, f3 : M . R. So if you wanted to evaluate df (X), you could simply apply J to 
some vector X = [X1 X2]T . 2.5.1. Coordinate Representations Considered Harmful. You can already see 
one drawback of the approach taken above: expressions get a lot longer and more complicated to write 
out. But there are other good reasons to avoid explicit matrix representations. The most profound reason 
is that matrices can be used to represent many different types of objects, and these objects can behave 
in very different ways. For instance, can you guess what the following matrix represents? 0 1 1 0 Give 
up? It s quite clear, actually: it s the adjacency matrix for the complete graph on two vertices. No, 
wait a minute it must be the Pauli matrix sx, representing spin angular momentum along the x-axis. Or 
is it the matrix representation for an element of the dihedral group D4? You get the idea: when working 
with matrices, it s easy to forget where they come from which makes it very easy 2.5. GEOMETRY IN COORDINATES 
to forget which rules they should obey! (Don t you already have enough things to keep track of?) The 
real philosophical point here is that matrices are not objects: they are merely representations of objects! 
Or to paraphrase Plato: matrices are merely shadows on the wall of the cave, which give us nothing more 
than a murky impression of the real objects we wish to illuminate. A more concrete example that often 
shows up in geometry is the distinction between linear operators and bilinear forms. As a reminder, a 
linear operator is a map from one vector space to another, e.g., f : R2 . R2; u . f (u), whereas a bilinear 
form is a map from a pair of vectors to a scalar, e.g., g : R2 × R2 . R; (u, v) . g(u, v). 1 Sticking 
with these two examples let s imagine that we re working in a coordinate system (x, x2), where f and 
g are represented by matrices A, B . R2×2 and their arguments are represented by vectors u, v . R2. In 
other words, we have f (u) = Au and g(u, v) = u TBv. 1 x 2 xu 2 u u 2 u 1 x1 u 1 2 ux 1 Now suppose 
we need to work in a different coordinate system (x , x 2), related to the .rst one by a change of basis 
P . R2×2. For instance, the vectors u and v get transformed via u = Pu, v = Pv. How do we represent the 
maps f and g in this new coordinate system? We can t simply evaluate Au , for instance, since A and u 
are expressed in different bases. What we need to do is evaluate f (u) = Au = AP-1 u and similarly g(u, 
v) = u TBv = (P-1 u)TB(P-1 v) = u T (P-TBP-1)v . 2.5. GEOMETRY IN COORDINATES In other words, linear 
operators transform like A . AP-1 , whereas bilinear forms transform like B . P-TBP-1 . So what we discover 
is that not all matrices transform the same way! But if we re constantly scrawling out little grids of 
numbers, it s very easy to lose track of which transformations should be applied to which objects. 2.5.2. 
Standard Matrices in the Geometry of Surfaces. Admonitions about coordinates aside, it s useful to be 
aware of standard matrix representations for geometric objects because they provide an essential link 
to classical results. We ve already seen a matrix representation for one object: the differential df 
can be encoded as the Jacobian matrix J containing .rst-order derivatives of the immersion f . What about 
the other objects we ve encountered in our study of surfaces? Well, the induced metric g should be pretty 
easy to .gure out since it s just a function of the differential remember that g(u, v) = df (u) · df 
(v). Equivalently, if we use a matrix I . R2×2 to represent g, then we have u TIv = (Ju)T(Jv) which means 
that I = JTJ. We use the letter I to denote the matrix of the induced metric, which was historically 
referred to as the .rst fundamental form fewer authors use this terminology today. In older books on 
differential geometry you may also see people talking about E , F , and G , which refer to particular 
entries of I: E F I = . F G (Is it clear why F appears twice?) One might conjecture that these .fth, 
sixth, and seventh letters of the alphabet have fallen out of fashion precisely because they are so coordinate-dependent 
and hence carry little geometric meaning on their own. Nonetheless, it is useful to be able to recognize 
these critters, because they do show up out there in the wild. Earlier on, we also looked at the shape 
operator, de.ned as the unique map S : T M . T M satisfying d N (X) = df (SX), and the second fundamental 
form, de.ned as II (u, v) = g(Su, v). (Remember that S turned out to be self-adjoint with respect to 
g, and likewise II turned out to be symmetric with respect to its arguments u and v.) If we let S, II 
. R2×2 be the matrix representations of S and II , respectively, then we have u TIIv = u TISv for all 
vectors u, v . R2, or equivalently, II = IS. 2.5. GEOMETRY IN COORDINATES Components of II are classically 
associated with lowercase letters from the Roman alphabet, namely II = e f , f g which in coordinates 
(x, y) are given explicitly by e = N · fxx , f = N · fxy , g = N · fyy , where N is the unit surface 
normal and fxy denotes the second partial derivative along directions x and y. At this point we might 
want to stop and ask: how does a matrix like IS transform with respect to a change of basis? The .rst 
term, I, is a bilinear form, but the second term S is a linear map! As emphasized above, we can t determine 
the answer by just staring at the matrices themselves we need to remember what they represent. In this 
case, we know that IS corresponds to the second fundamental form, so it should transform like any other 
bilinear form: IS . P-T I SP-1 . Finally, we can verify that classical geometric expressions using matrices 
correspond to the expressions we derived earlier using the differential. For instance, the classical 
expression for normal curvature is II (u, u) .n(u) = , I(u, u) which we can rewrite as uTIIu uTISu (Ju)T(JSu) 
df (u) · d N (u) == = . uTIu uTIu (Ju)T(Ju) |df (u)|2 Up to a choice of sign, this expression is the 
same one we obtained earlier by considering a curve embedded in the surface. CHAPTER 3   A Quick and 
Dirty Introduction to Exterior Calculus Many important concepts in differential geometry can be nicely 
expressed in the language of exterior calculus. Initially these concepts will look exactly like objects 
you know and love from vector calculus, and you may question the value of giving them funky new names. 
For instance, scalar .elds are no longer called scalar .elds, but are now called 0-forms! In many ways 
vector and exterior calculus are indeed dual to each-other, but it is precisely this duality that makes 
the language so expressive. In the long run we ll see that exterior calculus also makes it easy to generalize 
certain ideas from vector calculus the primary example being Stokes theorem. Actually, we already started 
using this language in our introduction to the geometry of surfaces, but here s the full story. 3.1. 
Vectors and 1-Forms Once upon a time there was a vector named v: What information does v encode? One 
way to inspect a vector is to determine its extent or length along a given direction. For instance, we 
can pick some arbitrary direction a and record the length of the shadow cast by v along a: v a a(v) 
The result is simply a number, which we can denote a(v). The notation here is meant to emphasize the 
idea that a is a function: in particular, it s a linear function that eats a vector and produces a scalar. 
Any such function is called a 1-form (also known as a covector). Of course, it s clear from the picture 
that the space of all 1-forms looks a lot like the space of all vectors: we just had to pick some direction 
to measure along. But often there is good reason to distinguish between vectors and 1-forms the distinction 
is not unlike the one made between 24 3.1. VECTORS AND 1-FORMS row vectors and column vectors in linear 
algebra. For instance, even though rows and column both represent vectors, we only permit ourselves to 
multiply rows with columns: . .  a1 · · · an .. v1 . . . vn .. . If we wanted to multiply, say, 
two column vectors, we would .rst have to take the transpose of one of them to convert it into a row: 
. . v1  .. .. . T . . . vn v v = v1 · · · vn Same deal with vectors and 1-forms, except that 
now we have two different operations: sharp (.), which converts a 1-form into a vector, and .at (b) which 
converts a vector into a 1-form. For instance, it s perfectly valid to write vb(v) or a(a.), since in 
either case we re feeding a vector to a 1-form. The operations . and b are called the musical isomorphisms. 
All this fuss over 1-forms versus vectors (or even row versus column vectors) may seem like much ado 
about nothing. And indeed, in a .at space like the plane, the difference between the two is pretty super.cial. 
In curved spaces, however, there s an important distinction between vectors and 1-forms in particular, 
we want to make sure that we re taking measurements in the right space. For instance, suppose we want 
to measure the length of a vector v along the direction of another vector u. It s important to remember 
that tangent vectors get stretched out by the map f : R2 . M . R3 that takes us from the plane to some 
surface in R3. Therefore, the operations . and b should satisfy relationships like ub(v) = g(u, v) where 
g is the metric induced by f . This way we re really measuring how things behave in the stretched out 
space rather than the initial domain M. 3.1.1. Coordinates. Until now we ve intentionally avoided the 
use of coordinates in other words, we ve tried to express geometric relationships without reference to 
any particular coordinate system x1, . . . , xn. Why avoid coordinates? Several reasons are often cited 
(people will mumble something about invariance ), but the real reason is quite simply that coordinate-free 
expressions tend to be shorter, sweeter, and easier to extract meaning from. This approach is also particularly 
valuable in geometry processing, because many coordinate-free expressions translate naturally to basic 
operations on meshes. Yet coordinates are still quite valuable in a number of situations. Sometimes there 
s a special coordinate basis that greatly simpli.es analysis recall our discussion of principal curvature 
di­rections, for instance. At other times there s simply no obvious way to prove something without coordinates. 
For now we re going to grind out a few basic facts about exterior calculus in coor­dinates; at the end 
of the day we ll keep whatever nice coordinate-free expressions we .nd and politely forget that coordinates 
ever existed! 3.1. VECTORS AND 1-FORMS Let s setup our coordinate system. For reasons that will become 
clear later, we re going to use . . the symbols .x1 , . . . , to represent an orthonormal basis for vectors 
in Rn, and use dxi , . . . , dxn .xn to denote the corresponding 1-form basis. In other words, any vector 
v can be written as a linear combination . . 1 n v = v· · + v, .x1 + · .xn and any 1-form can be written 
as a linear combination a = a1dx1 + · · · + andxn . . To keep yourself sane at this point, you should 
completely ignore the fact that the symbols .xi and dxi look like derivatives they re simply collections 
of unit-length orthogonal bases, as depicted above. . The two bases dxi and .xi are often referred to 
as dual bases, meaning they satisfy the relationship  . 1, i = j dxi= di = j .xj0, otherwise. . This 
relationship captures precisely the behavior we re looking for: a vector .xi casts a shadow on the 1-form 
dx j only if the two bases point in the same direction. Using this relationship, we can work out that 
 v j . = .xj . aidxi . i . a(v) = aiv i j i i.e., the pairing of a vector and a 1-form looks just 
like the standard Euclidean inner product. 3.1.2. Notation. It s worth saying a few words about notation. 
First, vectors and vector .elds tend to be represented by letters from the end of the Roman alphabet 
(u, v, w or X, Y, Z, repectively), whereas 1-forms are given lowercase letters from the beginning of 
the Greek alphabet (a, ß, ., etc.). Although one often makes a linguistic distinction between a vector 
 (meaning a single arrow) and a vector .eld (meaning an arrow glued to every point of a space), there 
s an unfortunate precedent to use the term 1-form to refer to both ideas sadly, nobody ever says 1-form 
.eld! Scalar .elds or 0-forms are often given letters from the middle of the Roman alphabet ( f , g, 
h) or maybe lowercase Greek letters from somewhere in the middle (f, ., etc.). You may also notice that 
we ve been very particular about the placement of indices: coef.cients vi of vectors have indices up, 
coef.cients ai of 1-forms have indices down. Similarly, vector bases . .xi have indices down (they re 
in the denominator), and 1-form bases dxi have indices up. The reason for being so neurotic is to take 
advantage of Einstein summation notation: any time a pair of 3.1. VECTORS AND 1-FORMS variables is indexed 
by the same letter i in both the up and down position, we interpret this as a sum over all possible values 
of i: aivi = . aivi . i The placement of indices also provides a cute mnemonic for the musical isomorphisms 
and b. In musical notation indicates a half-step increase in pitch, corresponding to an upward movement 
on the staff. For instance, both notes below correspond to a C with the same pitch1: Therefore, to go 
from a 1-form to a vector we raise the indices. For instance, in a .at space we don t have to worry about 
the metric and so a 1-form a = a1dx1 + · · · + andxn becomes a vector a = a1 . · · + an . . .x1 + · .xn 
Similarly, b indicates a decrease in pitch and a downward motion on the staff: and so b lowers the indices 
of a vector to give us a 1-form e.g., . . 1 n v = v· · + v. .x1 + · .xn becomes b v= v1dx1 + · · · + 
vndxn . 1At least on a tempered instrument! 3.2. DIFFERENTIAL FORMS AND THE WEDGE PRODUCT  3.2. Differential 
Forms and the Wedge Product In the last subsection we measured the length of a vector by projecting it 
onto different coordi­nate axes; this measurement process effectively de.ned what we call a 1-form. But 
what happens if we have a collection of vectors? For instance, consider a pair of vectors u, v sitting 
in R3: We can think of these vectors as de.ning a parallelogram, and much like we did with a single 
vector we can measure this parallelogram by measuring the size of the shadow it casts on some plane: 
 u×v ß v . u a For instance, suppose we represent this plane via a pair of unit orthogonal 1-forms a 
and ß. Then the projected vectors have components u ' = (a(u), ß(u)), v ' = (a(v), ß(v)), hence the (signed) 
projected area is given by the cross product ' ' u × v = a(u)ß(v) - a(v)ß(u). Since we want to measure 
a lot of projected volumes in the future, we ll give this operation the special name a . ß : a . ß(u, 
v) := a(u)ß(v) - a(v)ß(u). As you may have already guessed, a . ß is what we call a 2-form. Ultimately 
we ll interpret the symbol . (pronounced wedge ) as a binary operation on differential forms called the 
wedge product. Algebraic properties of the wedge product follow directly from the way signed volumes 
behave. For instance, notice that if we reverse the order of our axes a, ß the sign of the area changes. 
In other words, the wedge product is antisymmetric: a . ß = -ß . a. 3.2. DIFFERENTIAL FORMS AND THE WEDGE 
PRODUCT An important consequence of antisymmetry is that the wedge of any 1-form with itself is zero: 
 a . a = -a . a . a . a = 0. But don t let this statement become a purely algebraic fact! Geometrically, 
why should the wedge of two 1-forms be zero? Quite simply because it represents projection onto a plane 
of zero area! (I.e., the plane spanned by a and a.) Next, consider the projection onto two different 
planes spanned by a, ß and a, .. The sum of the projected areas can be written as a . ß(u, v) + a . .(u, 
v) = a(u)ß(v) - a(v)ß(u) + a(u).(v) - a(v).(u) = a(u)(ß(v) + .(v)) - a(v)(ß(u) + .(u)) =: (a . (ß + .))(u, 
v), or in other words . distributes over +: a . (ß + .) = a . ß + a . .. Finally, consider three vectors 
u, v, w that span a volume in R3: We d like to consider the projection of this volume onto the volume 
spanned by three 1-forms a, ß, and ., but the projection of one volume onto another is a bit dif.cult 
to visualize! For now you can just cheat and imagine that a = dx1 , ß = dx2, and . = dx3 so that the 
mental picture for the projected volume looks just like the volume depicted above. One way to write the 
projected volume is as the determinant of the projected vectors u ' , v ', and w ' : .. .. a(u) a(v) 
a(w) a . ß . .(u, v, w) := detu ' v ' w '= det .. ß(u) ß(v) ß(w) .. . .(u) .(v) .(w) (Did you notice 
that the determinant of the upper-left 2x2 submatrix also gives us the wedge product of two 1-forms?) 
Alternatively, we could express the volume as the area of one of the faces times the length of the remaining 
edge: 3.2. DIFFERENTIAL FORMS AND THE WEDGE PRODUCT Thinking about things this way, we might come up 
with an alternative de.nition of the wedge product in terms of the triple product: a . ß . .(u, v, w) 
= (u ' × v ' ) · w ' = (v ' × w ' ) · u ' = (w ' × u ' ) · v ' The important thing to notice here is 
that order is not important we always get the same volume, regardless of which face we pick (though we 
still have to be a bit careful about sign). A more algebraic way of saying this is that the wedge product 
is associative: (a . ß) . . = a . (ß . .). In summary, the wedge product of k 1-forms gives us a k-form, 
which measures the projected volume of a collection of k vectors. As a result, the wedge product has 
the following properties for any k-form a, l-form ß, and m-form .: Antisymmetry: a . ß = (-1)kl ß . 
a  Associativity: a . (ß . .) = (a . ß) . .  and in the case where ß and . have the same degree (i.e., 
l = m) we have Distributivity: a . (ß + .) = a . ß + a . . A separate fact is that a k-form is antisymmetric 
in its arguments in other words, swapping the relative order of two input vectors changes only the sign 
of the volume. For instance, if a is a 2-form then a(u, v) = -a(v, u). In general, an even number of 
swaps will preserve the sign; an odd number of swaps will negate it. (One way to convince yourself is 
to consider what happens to the determinant of a matrix when you exchange two of its columns.) Finally, 
you ll often hear people say that k-forms are multilinear all this means is that if you keep all but 
one of the vectors .xed, then a k-form looks like a linear map. Geometrically this makes sense: k-forms 
are built up from k linear measurements of length (essentially just k different dot products). 3.2.1. 
Vector-Valued Forms. Up to this point we ve considered only real-valued k-forms for instance, a(u) represents 
the length of the vector u along the direction a, which can be expressed as a single real number. In 
general, however, a k-form can spit out all kinds of different values. For instance, we might want to 
deal with quantities that are described by complex numbers (C) or vectors in some larger vector space 
(e.g., Rn). 3.2. DIFFERENTIAL FORMS AND THE WEDGE PRODUCT A good example of a vector-valued k-form is 
our map f : M . R3 which represents the geometry of a surface. In the language of exterior calculus, 
f is an R3-valued 0-form: at each point p of M, it takes zero vectors as input and produces a point f 
(p) in R3 as output. Similarly, the differential df is an R3-valued 1-form: it takes one vector (some 
direction u in the plane) and maps it to a value df (u) in R3 (representing the stretched out version 
of u). More generally, if E is a vector space then an E-valued k-form takes k vectors to a single value 
in E. However, we have to be a bit careful here. For instance, think about our de.nition of a 2-form: 
a . ß(u, v) := a(u)ß(v) - a(v)ß(u). If a and ß are both E-valued 1-forms, then a(u) and ß(v) are both 
vectors in E. But how do you multiply two vectors? In general there may be no good answer: not every 
vector space comes with a natural notion of multiplication. However, there are plenty of spaces that 
do come with a well-de.ned product for instance, the product of two complex numbers a + bi and c + di 
is given by (ac - bd) + (ad + bc)i, so we have no trouble explicitly evaluating the expression above. 
In other cases we simply have to say which product we want to use in R3 for instance we could use the 
cross product ×, in which case an R3-valued 2-form looks like this: a . ß(u, v) := a(u) × ß(v) - a(v) 
× ß(u). 3.3. HODGE DUALITY  3.3. Hodge Duality u×v ß v . u a Previously we saw that a k-form measures 
the (signed) projected volume of a k-dimensional parallelpiped. For instance, a 2-form measures the area 
of a parallelogram projected onto some plane, as depicted above. But here s a nice observation: a plane 
in R3 can be described either by a pair of basis directions (a, ß), or by a normal direction .. So rather 
than measuring projected area, we could instead measure how well the normal of a parallelogram (u, v) 
lines up with the normal of our plane. In other words, we could look for a 1-form . such that .(u × v) 
= a . ß(u, v). This observation captures the idea behind Hodge duality: a k-dimensional volume in an 
n-dimensional space can be speci.ed either by k directions or by a complementary set of (n - k) directions. 
There should therefore be some kind of natural correspondence between k-forms and (n - k)-forms. 3.3.1. 
The Hodge Star. Let s investigate this idea further by constructing an explicit basis for the space of 
0-forms, 1-forms, 2-forms, etc. to keep things manageable we ll work with R3 and its 1 2 standard coordinate 
system (x, x, x3). 0-forms are easy: any 0-form can be thought of as some function times the constant 
0-form, which we ll denote 1. We ve already seen the 1-form basis dx1 , dx2 , dx3, which looks like the 
standard orthonormal basis of a vector space: dx3 dx2 dx1 3.3. HODGE DUALITY What about 2-forms? Well, 
consider that any 2-form can be expressed as the wedge of two 1-forms: a . ß = (aidxi) . (ßjdx j) = ai 
ßjdxi . dx j. In other words, any 2-form looks like some linear combination of the basis 2-forms dxi 
. dx j. How many of these bases are there? Initially it looks like there are a bunch of possibilities: 
dx1 . dx1 dx1 . dx2 dx1 . dx3 dx2 . dx1 dx2 . dx2 dx2 . dx3 dx3 . dx1 dx3 . dx2 dx3 . dx3 But of course, 
not all of these guys are distinct: remember that the wedge product is antisym­metric (a . ß = -ß . a), 
which has the important consequence a . a = 0. So really our table looks more like this: 0 dx1 . dx2 
-dx3 . dx1 -dx1 . dx2 0 dx2 . dx3 dx3 . dx1 -dx2 . dx3 0 and we re left with only three distinct bases: 
dx2 . dx3 , dx3 . dx1, and dx1 . dx2. Geometrically all we ve said is that there are three linearly-independent 
planes in R3: How about 3-form bases? We certainly have at least one: dx1 . dx2 . dx3 . Are there any 
others? Again the antisymmetry of . comes into play: many potential bases are just permutations of this 
.rst one: dx2 . dx3 . dx1 = -dx2 . dx1 . dx3 = dx1 . dx2 . dx3 , and the rest vanish due to the appearance 
of repeated 1-forms: dx2 . dx1 . dx2 = -dx2 . dx2 . dx1 = 0 . dx1 = 0. 3.3. HODGE DUALITY In general 
there is only one basis n-form dx1 . · · · . dxn, which measures the usual Euclidean volume of a parallelpiped: 
 Finally, what about 4-forms on R3? At this point it s probably pretty easy to see that there are none, 
since we d need to pick four distinct 1-form bases from a collection of only three. Geo­metrically: there 
are no four-dimensional volumes contained in R3! (Or volumes of any greater dimension, for that matter.) 
The complete list of k-form bases on R3 is then 0-form bases: 1  1-form bases: dx1 , dx2 , dx3  2-form 
bases: dx2 . dx3 , dx3 . dx1 , dx1 . dx2  3-form bases: dx1 . dx2 . dx3 ,  which means the number of 
bases is 1, 3, 3, 1. In fact you may see a more general pattern here: the number of k-form bases on an 
n-dimensional space is given by the binomial coef.cient n n! = k k!(n - k)! (i.e., n choose k ), since 
we want to pick k distinct 1-form bases and don t care about the order. An important identity here is 
n n = , k n - k which, as anticipated, means that we have a one-to-one relationship between k-forms and 
(n - k)­forms. In particular, we can identify any k-form with its complement. For example, on R3 we have 
 3.3. HODGE DUALITY * 1 = dx1 . dx2 . dx3 * dx1 = dx2 . dx3 * dx2 = dx3 . dx1 * dx3 = dx1 . dx2 * 
dx1 . dx2 = dx3 * dx2 . dx3 = dx1 * dx3 . dx1 = dx2 * dx1 . dx2 . dx3 = 1  The map * (pronounced 
star ) is called the Hodge star and captures this idea that planes can be identi.ed with their normals 
and so forth. More generally, on any .at space we have * dxi1 . dxi2 . · · · . dxik = dxik+1 . dxik+2 
. · · · . dxin , where (i1, i2, . . . , in) is any even permutation of (1, 2, . . . , n).  3.3.2. The 
Volume Form. f df (v) v df (u) u So far we ve been talking about measuring volumes in .at spaces like 
Rn . But how do we take measurements in a curved space? Let s think about our usual example of a surface 
f : R2 . M . R3. If we specify a region on our surface via a pair of unit orthogonal vectors u, v . R2, 
it s clear that we don t want the area dx1 . dx2(u, v) = 1 since that just gives us the area in the plane. 
Instead, we want to know what a unit area looks like after it s been stretched-out by the map f . In 
particular, we said that the length of a vector df (u) can be expressed in terms of the metric g: |df 
(u)| = df (u) · df (u) = g(u, u). r So the area we re really interested in is the product of the lengths 
|df (u)||df (v)| = g(u, u)g(v, v). When u and v are orthonormal the quantity det(g) := g(u, u)g(v, v) 
- 2g(u, v) is called the determi­ r nant of the metric, and can be used to de.ne a 2-form det(g)dx1 . 
dx2 that measures any area on our surface. More generally, the n-form . := det(g)dx1 . · · · . dxn is 
called the volume form, and will play a key role when we talk about integration. On curved spaces, we 
d also like the Hodge star to capture the fact that volumes have been stretched out. For instance, it 
makes a certain amount of sense to identify the constant function 1 with the volume form ., since . really 
represents unit volume on the curved space: *1 = . 3.3. HODGE DUALITY 3.3.3. The Inner Product on k-Forms. 
More generally we ll ask that any n-form constructed from a pair of k-forms a and ß satis.es a . *ß = 
((a, ß))., where ((a, ß)) = .i ai ßi is the inner product on k-forms. In fact, some authors use this 
relationship as the de.nition of the wedge product in other words, they ll start with something like, 
the wedge product is the unique binary operation on k-forms such that a . *ß = ((a, ß))., and from there 
derive all the properties we ve established above. This treatment is a bit abstract, and makes it far 
too easy to forget that the wedge product has an extraordinarily concrete geometric meaning. (It s certainly 
not the way Hermann Grassmann thought about it when he invented exterior algebra!). In practice, however, 
this identity is quite useful. For instance, if u and v are vectors in R3, then we can write b u · v 
= *ub . *v, i.e., on a .at space we can express the usual Euclidean inner product via the wedge product. 
Is it clear geometrically that this identity is true? Think about what it says: the Hodge star turns 
v into a plane with v as a normal. We then build a volume by extruding this plane along the direction 
u. If u and v are nearly parallel the volume will be fairly large; if they re nearly orthogonal the volume 
will be quite shallow. (But to be sure we really got it right, you should try verifying this identity 
in coordinates!) Similarly, we can express the Euclidean cross product as just u × v = *(ub . vb) , i.e., 
we can create a plane with normal u × v by wedging together the two basis vectors u and v. (Again, working 
this fact out in coordinates may help soothe your paranoia.) 3.4. DIFFERENTIAL OPERATORS  3.4. Differential 
Operators Originally we set out to develop exterior calculus. The objects we ve looked at so far k-forms, 
the wedge product . and the Hodge star * actually describe a more general structure called an exterior 
algebra. To turn our algebra into a calculus, we also need to know how quantities change, as well as 
how to measure quantities. In other words, we need some tools for differentiation and integration. Let 
s start with differentiation. In our discussion of surfaces we brie.y looked at the differential df of 
a surface f : M . R3 , which tells us something about the way tangent vectors get stretched out as we 
move from the domain M to a curved surface sitting in R3. More generally d is called the exterior derivative 
and is responsible for building up many of the differential operators in exterior calculus. The basic 
idea is that d tells us how quickly a k-form changes along every possible direction. But how exactly 
is it de.ned? So far we ve seen only a high-level geometric description. 3.4.1. Div, Grad, and Curl. 
Before jumping into the exterior derivative, it s worth reviewing what the basic vector derivatives div, 
grad, and curl do, and more importantly, what they look like. The key player here is the operator \ (pronounced 
nabla ) which can be expressed in coordinates as the vector of all partial derivatives: . . \ := .x1,..., 
. .xn For instance, applying \ to a scalar function f : Rn . R yields the gradient . f . f \f = .x1,..., 
, .xn which can be visualized as the direction of steepest ascent on some terrain: We can also apply 
\ to a vector .eld X in two different ways. The dot product gives us the divergence .X1 .Xn \ · X = + 
· · · + .x1 .xn 3.4. DIFFERENTIAL OPERATORS which measures how quickly the vector .eld is spreading out 
, and on R3 the cross product gives us the curl \ × X = .X3 .x2 - .X2 .x3 , .X1 .x3 - .X3 .x1 , .X2 .x1 
- .X1 .x2 , which indicates how much a vector .eld is spinning around. For instance, here s a pair of 
vector .elds with a lot of divergence and a lot of curl, respectively: (Note that in this case one .eld 
is just a 90-degree rotation of the other!) On a typical day it s a lot more useful to think of div, 
grad and curl in terms of these kinds of pictures rather than the ugly expressions above. 3.4.2. Think 
Differential. Not surprisingly, we can express similar notions using exterior calculus. However, these 
notions will be a bit easier to generalize (for instance, what does curl mean for a vector .eld in R4, 
where no cross product is de.ned?). Let s .rst take a look at the exterior derivative of 0-forms (i.e., 
functions), which is often just called the differential. To keep things simple, we ll start with real-valued 
functions f : Rn . R. In coordinates, the differential is de.ned as .f .f df := dx1 + · · · + dxn . .x1 
.xn It s important to note that the terms .f actually correspond to partial derivatives of our function 
f, .xi whereas the terms dxi simply denote an orthonormal basis for Rn. In other words, you can think 
of df as just a list of all the partial derivatives of f. Of course, this object looks a lot like the 
gradient \f we saw just a moment ago. And indeed the two are closely related, except for the fact that 
\f is a vector .eld and df is a 1-form. More precisely, \f = (df) . 3.4.3. Directional Derivatives. Another 
way to investigate the behavior of the exterior deriv­ative is to see what happens when we stick a vector 
u into the 1-form df . In coordinates we get something that looks like a dot product between the gradient 
of f and the vector u: . f . f df (u) = 1 + · · · + un . .x1 u.xn 3.4. DIFFERENTIAL OPERATORS . f For 
instance, in R2 we could stick in the unit vector u = (1, 0) to get the partial derivative .x1 along 
the .rst coordinate axis: (Compare this picture to the picture of the gradient we saw above.) In general, 
df (u) represents the directional derivative of f along the direction u. In other words, it tells us 
how quickly f changes if we take a short walk in the direction u. Returning again to vector calculus 
notation, we have df (u) = u · \ f . 3.4.4. Properties of the Exterior Derivative. How do derivatives 
of arbitrary k-forms behave? For one thing, we expect d to be linear after all, a derivative is just 
the limit of a difference, and differences are certainly linear! What about the derivative of a wedge 
of two forms? Harken­ing back to good old-fashioned calculus, here s a picture that explains the typical 
product rule . ( f (x)g(x)) = f ' (x)g(x) + f (x)g ' (x): .x g(x + h) O(h) O(h2) g(x) O(h) f (x) f (x 
+ h) The dark region represents the value of f g at x; the light blue region represents the change in 
this value as we move x some small distance h. As h gets smaller and smaller, the contribution of the 
upper-right quadrant becomes negligible and we can write the derivative as the change in 3.4. DIFFERENTIAL 
OPERATORS f times g plus the change in g times f . (Can you make this argument more rigorous?) Since 
a k-form also measures a (signed) volume, this intuition also carries over to the exterior derivative 
of a wedge product. In particular, if a is a k-form then d obeys the rule d(a . ß) = da . ß + (-1)ka 
. dß. which says that the rate of change of the overall volume can be expressed in terms of changes in 
the constituent volumes, exactly as in the picture above. 3.4.5. Exterior Derivative of 1-Forms. To be 
a little more concrete, let s see what happens when we differentiate a 1-form on R3. Working things out 
in coordinates turns out to be a total mess, but in the end you may be pleasantly surprised with the 
simplicity of the outcome! (Later on we ll see that these ideas can also be expressed quite nicely without 
coordinates using Stokes theorem, which paves the way to differentiation in the discrete setting.) Applying 
the linearity of d, we have da = d(a1dx1 + a2dx2 + a3dx3) = d(a1dx1) + d(a2dx2) + d(a3dx3). Each term 
ajdx j can really be thought of a wedge product aj . dx j between a 0-form aj and the corresponding basis 
1-form dx j. Applying the exterior derivative to one of these terms we get .ajd(aj . dx j) = (daj) . 
dx j + aj . (ddx j) = .xi dxi . dx j. =0 To keep things short we used the Einstein summation convention 
here, but let s really write out all the terms: .a1 .a1 .a1 da = .x1 dx1 . dx1 + .x2 dx2 . dx1 + .x3 
dx3 . dx1 .a2 .a2 .a2 .x1 dx1 . dx2 + .x2 dx2 . dx2 + .x3 dx3 . dx2 .a3 .a3 .a3 .x1 dx1 . dx3 + .x2 dx2 
. dx3 + .x3 dx3 . dx3 . Using the fact that a . ß = -ß . a, we get a much simpler expression ( .a3 - 
.a2 da = .x3 )dx2 . dx3 .x2 ( .a1 - .a3 .x1 )dx3 . dx1 .x3 ( .a2 - .a1 .x2 )dx1 . dx2 . .x1 Does this 
expression look familiar? If you look again at our review of vector derivatives, you ll recognize that 
da basically looks like the curl of a , except that it s expressed as a 2-form instead of a vector .eld. 
Also remember (from our discussion of Hodge duality) that a 2-form and a 1-form are not so different 
here geometrically they both specify some direction in R3. Therefore, we can express the curl of any 
vector .eld X as \ × X = *dXb . 3.4. DIFFERENTIAL OPERATORS It s worth stepping through the sequence 
of operations here to check that everything makes sense: b converts the vector .eld X into a 1-form Xb; 
d computes something that looks like the curl, but expressed as a 2-form dXb; * turns this 2-form into 
a 1-form *dXb; and .nally converts this 1-form back into the vector .eld *dXb . The take-home message 
here, though, is that the exterior derivative of a 1-form looks like the curl of a vector .eld. So far 
we know how to express the gradient and the curl using d. What about our other favorite vector derivative, 
the divergence? Instead of grinding through another tedious derivation, let s make a simple geometric 
observation: in R2 at least, we can determine the divergence of a vector .eld by rotating it by 90 degrees 
and computing its curl (consider the example we saw earlier). Moreover, in R2 the Hodge star * represents 
a rotation by 90 degrees, since it identi.es any line with the direction orthogonal to that line: *a 
a Therefore, we might suspect that divergence can be computed by .rst applying the Hodge star, then applying 
the exterior derivative: \ · X = *d * Xb . The leftmost Hodge star accounts for the fact that d * Xb 
is an n-form instead of a 0-form in vector calculus divergence is viewed as a scalar quantity. Does this 
de.nition really work? Let s give it a try in coordinates on R3. First, we have *Xb = *(X1dx1 + X2dx2 
+ X3dx3) = X1dx2 . dx3 + X2dx3 . dx1 + X3dx1 . dx2 . Differentiating we get .X1 d * Xb = .x1 dx1 . dx2 
. dx3+ .X2 .x2 dx2 . dx3 . dx1+ .X3 .x3 dx3 . dx1 . dx2 , but of course we can rearrange these wedge 
products to simply .X1 .X2 .X3 d * Xb = ++ dx1 . dx2 . dx3 . .x1 .x2 .x3 A .nal application of the Hodge 
star gives us the divergence .X1 .X2 .X3 *d * Xb = + + .x1 .x2 .x3 as desired.  3.4. DIFFERENTIAL OPERATORS 
In summary, for any scalar .eld f and vector .eld X we have \f = (df) \ × X = *dXb \ · X = *d * Xb One 
cute thing to notice here is that (in R3) grad, curl, and div are more or less just d applied to a 0-, 
1- and 2- form, respectively. 3.4.6. The Laplacian. Another key differential operator from vector calculus 
is the scalar Laplacian which (confusingly!) is often denoted by . or \2, and is de.ned as . := \ · \, 
i.e., the divergence of the gradient. Although the Laplacian may seem like just yet another in a long 
list of derivatives, it deserves your utmost respect: the Laplacian is central to the most fundamental 
of physical laws (any diffusion process and all forms of wave propagation, including the Schr ¨ odinger 
equation); its eigenvalues capture almost everything there is to know about a given piece of geometry 
(can you hear the shape of a drum?). Heavy tomes and entire lives have been devoted to the Laplacian, 
and in the discrete setting we ll see that this one simple operator can be applied to a diverse array 
of tasks (surface parameterization, surface smoothing, vector .eld design and decomposition, distance 
computation, .uid simulation... you name it, we got it!). Fortunately, now that we know how to write 
div, grad and curl using exterior calculus, express­ing the scalar Laplacian is straightforward: . = 
*d * d. More generally, the k-form Laplacian is given by . := *d * d + d * d * . The name Laplace-Beltrami 
is used merely to indicate that the domain may have some amount of curvature (encapsulated by the Hodge 
star). Some people like to de.ne the operator d := *d*, called the codifferential, and write the Laplacian 
as . = dd + dd. One question you might ask is: why is the Laplacian for 0-forms different from the general 
k-form Laplacian? Actually, it s not consider what happens when we apply the term d * d* to a 0-form 
f: *f is an n-form, and so d * f must be an (n + 1)-form. But there are no (n + 1)-forms on an n-dimensional 
space! So this term is often omitted when writing the scalar Laplacian. 3.5. INTEGRATION AND STOKES THEOREM 
  3.5. Integration and Stokes Theorem In the previous section we talked about how to differentiate 
k-forms using the exterior derivative d. We d also like some way to integrate forms. Actually, there 
s surprisingly little to say about integration given the setup we already have. Suppose we want to compute 
the total area AO of a region O in the plane: If you remember back to calculus class, the basic idea 
was to break up the domain into a bunch of little pieces that are easy to measure (like squares) and 
add up their areas: AO . Ai. i As these squares get smaller and smaller we get a better and better approximation, 
ultimately achieving the true area AO =d A. O Alternatively, we could write the individual areas using 
differential forms in particular, Ai = dx1 . dx2(u, v). Therefore, the area element d A is really nothing 
more than the standard volume form dx1 . dx2 on R2 . (Not too surprising, since the whole point of k-forms 
was to measure volume!) To make things more interesting, let s say that the contribution of each little 
square is weighted by some scalar function f. In this case we get the quantity f d A =f dx1 . dx2 . 
O O Again the integrand f dx1 . dx2 can be thought of as a 2-form. In other words, you ve been working 
with differential forms your whole life, even if you didn t realize it! More generally, integrands on 
an n-dimensional space are always n-forms, since we need to plug in n orthogonal vectors representing 
the local volume. For now, however, looking at surfaces (i.e., 2-manifolds) will give us all the intuition 
we need. 3.5. INTEGRATION AND STOKES THEOREM  3.5.1. Integration on Surfaces.  f df (v) v df (u) u 
If you think back to our discussion of the Hodge star, you ll remember the volume form . = det(g)dx1 
. dx2 , r which measures the area of little parallelograms on our surface. The factor det(g) reminds 
us that we can t simply measure the volume in the domain M we also have to take into account any stretching 
induced by the map f : M . R2. Of course, when we integrate a function on a surface, we should also take 
this stretching into account. For instance, to integrate a function f : M . R, we would write f. = f 
det(g) dx1 . dx2 . O O r In the case of a conformal parameterization things become even simpler since 
det(g) = a we have just fa dx1 . dx2 , O where a : M . R is the scaling factor. In other words, we scale 
the value of f up or down depending on the amount by which the surface locally in.ates or de.ates. In 
fact, this whole story gives a nice geometric interpretation to good old-fashioned integrals: you can 
imagine that s O f d A represents the area of some suitably deformed version of the initially planar 
region O. 3.5.2. Stokes Theorem. The main reason for studying integration on manifolds is to take advantage 
of the world s most powerful tool: Stokes theorem. Without further ado, Stokes theorem says that da = 
a, O .O where a is any n - 1-form on an n-dimensional domain O. In other words, integrating a differential 
form over the boundary of a manifold is the same as integrating its derivative over the entire domain. 
If this trick sounds familiar to you, it s probably because you ve seen it time and again in different 
contexts and under different names: the divergence theorem, Green s theorem, the fundamental theorem 
of calculus, Cauchy s integral formula, etc. Picking apart these special cases will really help us understand 
the more general meaning of Stokes theorem. 3.5.3. Divergence Theorem. Let s start with the divergence 
theorem from vector calculus, which says that \ · Xd A = N · X d£, O .O 3.5. INTEGRATION AND STOKES THEOREM 
where X is a vector .eld on O and N represents the (outward-pointing) unit normals on the boundary of 
O. A better name for this theorem might have been the what goes in must come out theorem , because if 
you think about X as the .ow of water throughout the domain O then it s clear that the amount of water 
being pumped into O (via pipes in the ground) must be the same as the amount .owing out of its boundary 
at any moment in time: Let s try writing this theorem using exterior calculus. First, remember that 
we can write the divergence of X as \ · X = *d * Xb. It s a bit harder to see how to write the right-hand 
side of the divergence theorem, but think about what integration does here: it takes tangents to the 
boundary s and sticks them into a 1-form. For instance, Xb adds up the tangential components of X. To 
get O the normal component we could rotate Xb by a quarter turn, which conveniently enough is achieved 
by hitting it with the Hodge star. Overall we get d * Xb = *Xb , O .O which, as promised, is just a special 
case of Stokes theorem. Alternatively, we can use Stokes theo­rem to provide a more geometric interpretation 
of the divergence operator itself: when integrated over any region O no matter how small the divergence 
operator gives the total .ux through the region boundary. In the discrete case we ll see that this boundary 
.ux interpretation is the only notion of divergence in other words, there s no concept of divergence 
at a single point. By the way, why does d * Xb appear on the left-hand side instead of *d * Xb? The reason 
is that *d * Xb is a 0-form, so we have to hit it with another Hodge star to turn it into an object that 
measures areas (i.e., a 2-form). Applying this transformation is no different from appending d A to \ 
· X we re specifying how volume should be measured on our domain. 3.5.4. Fundamental Theorem of Calculus. 
The fundamental theorem of calculus is in fact so fundamental that you may not even remember what it 
is. It basically says that for a real-valued function f : R . R on the real line b .f dx = f(b) - f(a). 
a .x In other words, the total change over an interval [a, b] is (as you might expect) how much you end 
up with minus how much you started with. But soft, behold! All we ve done is written Stokes  3.5. INTEGRATION 
AND STOKES THEOREM theorem once again: df = f, [a,b] .[a,b] since the boundary of the interval [a, b] 
consists only of the two endpoints a and b. Hopefully these two examples give you a good feel for what 
Stokes theorem says. In the end, it reads almost like a Zen k ¯ oan: what happens on the outside is purely 
a function of the change within. (Perhaps it is Stokes that deserves the name, fundamental theorem of 
calculus! ) 3.6. DISCRETE EXTERIOR CALCULUS 3.6. Discrete Exterior Calculus  So far we ve been exploring 
exterior calculus purely in the smooth setting. Unfortunately this theory was developed by some old-timers 
who did not know anything about computers, hence it cannot be used directly by machines that store only 
a .nite amount of information. For instance, if we have a smooth vector .eld or a smooth 1-form we can 
t possibly store the direction of every little arrow at each point there are far too many of them! Instead, 
we need to keep track of a discrete (or really, .nite) number of pieces of information that capture the 
essential behavior of the objects we re working with; we call this scheme discrete exterior calculus 
(or DEC for short). The big secret about DEC is that it s literally nothing more than the good-old fashioned 
(continuous) exterior calculus we ve been learning about, except that we integrate differential forms 
over elements of our mesh. 3.6.1. Discrete Differential Forms. One way to encode a 1-form might be to 
store a .nite collection of arrows associated with some subset of points. Instead, we re going to do 
some­thing a bit different: we re going to integrate our 1-form over each edge of a mesh, and store the 
resulting numbers (remember that the integral of an n-form always spits out a single number) on the corresponding 
edges. In other words, if a is a 1-form and e is an edge, then we ll associate the number a e := a e 
with e, where the use of the hat ( ) is supposed to suggest a discrete quantity (not to be confused with 
a unit-length vector). Does this procedure seem a bit abstract to you? It shouldn t! Think about what 
this integral represents: it tells us how strongly the 1-form a .ows along the edge e on average. More 
speci.cally, remember how integration of a 1-form works: at each point along the edge we take the vector 
tangent to the edge, stick it into the 1-form a, and sum up the resulting values each value tells us 
something about how well a lines up with the direction of the edge. For instance, we could approximate 
the integral via the sum N 1 a |e| . api (e ) , e N i=1 where |e| denotes the length of the edge, {pi} 
is a sequence of points along the edge, and e := e/|e|is a unit vector tangent to the edge: 3.6. DISCRETE 
EXTERIOR CALCULUS Of course, this quantity tells us absolutely nothing about the strength of the .ow 
orthogonal to the edge: it could be zero, it could be enormous! We don t really know, because we didn 
t take any measurements along the orthogonal direction. However, the hope is that some of this information 
will still be captured by nearby edges (which are most likely not parallel to e). More generally, a k-form 
that has been integrated over each k-dimensional cell (edges in 1D, faces in 2D, etc.) is called a discrete 
differential k-form. (If you ever .nd the distinction confusing, you might .nd it helpful to substitute 
the word integrated for the word discrete. ) In practice, however, not every discrete differential form 
has to originate from a continuous one for instance, a bunch of arbitrary values assigned to each edge 
of a mesh is a perfectly good discrete 1-form. 3.6.2. Orientation. One thing you may have noticed in 
all of our illustrations so far is that each edge is marked with a little arrow. Why? Well, one thing 
to remember is that direction matters when you integrate. For instance, the fundamental theorem of calculus 
(and common sense) tells us that the total change as you go from a to b is the opposite of the total 
change as you go from b to a: b a .f .f dx = f(b) - f(a) = -(f(a) - f(b)) = - dx. a .x b .x Said in a 
much less fancy way: the elevation gain as you go from Pasadena to Altadena is 151 meters, so the elevation 
gain in the other direction must be -151 meters! Just keeping track of the number 151 does you little 
good you have to say what that quantity represents. Altadena Altadena 151m -151m Pasadena Pasadena Therefore, 
when we store a discrete differential form it s not enough to just store a number: we also have to specify 
a canonical orientation for each element of our mesh, corresponding to the orientation we used during 
integration. For an edge we ve already seen that we can think about orientation as a little arrow pointing 
from one vertex to another we could also just think of an edge as an ordered pair (i, j), meaning that 
we always integrate from i to j. More generally, suppose that each element of our mesh is an oriented 
k-simplex s, i.e., a collection of k + 1 vertices pi . Rn given in some .xed order (p1, . . . , pk+1). 
The geometry associated with s 3.6. DISCRETE EXTERIOR CALCULUS is the convex combination of these points: 
k+1 k+1 . .i pi . .i = 1 . Rn i=1 i=1 (Convince yourself that a 0-simplex is a vertex, a 1-simplex 
is an edge, a 2-simplex is a triangle, and a 3-simplex is a tetrahedron.) Two oriented k-simplices have 
the same orientation if and only if the vertices of one are an even permutation of the vertices of another. 
For instance, the triangles (p1, p2, p3) and (p2, p3, p1) have the same orientation; (p1, p2, p3) and 
(p2, p1, p3) have opposite orientation. p3 p3 p1 p2 p1 p2 If a simplex s1 is a (not necessarily proper) 
subset of another simplex s2, then we say that s1 is a face of s2. For instance, every vertex, edge, 
and triangle of a tetrahedron s is a face of s; as is s itself! Moreover, the orientation of a simplex 
agrees with the orientation of one of its faces as long as we see an even permutation on the shared vertices. 
For instance, the orientations of the edge (p2, p1) and the triangle (p1, p3, p2) agree. Geometrically 
all we re saying is that the two point in the same direction (as depicted above). To keep yourself sane 
while working with meshes, the most important thing is to pick and orientation and stick with it! So 
in general, how do we integrate a k-form over an oriented k-simplex? Remember that a k-form is going 
to eat k vectors at each point and spit out a number a good canonical choice is to take the ordered collection 
of edge vectors (p2 - p1, . . . , pk+1 - p1) and orthogonalize them (using, say the Gram-Schmidt algorithm) 
to get vectors (u1, . . . , un). This way the sign of the integrand changes whenever the orientation 
changes. Numerically, we can then approximate the integral via a sum N |s| a . api (u1, . . . , un) 
s N i=1 3.6. DISCRETE EXTERIOR CALCULUS where {pi} is a (usually carefully-chosen) collection of sample 
points. (Can you see why the orientation of s affects the sign of the integrand?) Sounds like a lot of 
work, but in practice one rarely constructs discrete differential forms via integration: more often, 
discrete forms are constructed via input data that is already discrete (e.g., vertex positions in a triangle 
mesh). By the way, what s a discrete 0-form? Give up? Well, it must be a 0-form (i.e., a function) that 
s been integrated over every 0-simplex (i.e., vertex) of a mesh: f i = f vi By convention, the integral 
of a function over a zero-dimensional set is simply the value of the function at that point: f i = f(vi). 
In other words, in the case of 0-forms there is no difference between storing point samples and storing 
integrated quantities: the two coincide. It s also important to remember that differential forms don 
t have to be real-valued. For instance, we can think of a map f : M . R3 that encodes the geometry of 
a surface as an R3-valued 0-form; its differential df is then an R3-valued 1-form, etc. Likewise, when 
we say that a discrete differential form is a number stored on every mesh element, the word number is 
used in a fairly loose sense: a number could be a real value, a vector, a complex number, a quaternion, 
etc. For instance, the collection of (x, y, z) vertex coordinates of a mesh can be viewed as an R3-valued 
discrete 0-form (namely, one that discretizes the map f ). The only requirement, of course, is that we 
store the same type of number on each mesh element. 3.6.3. The Discrete Exterior Derivative. One of the 
main advantages of working with in­tegrated (i.e., discrete ) differential forms instead of point samples 
is that we can easily take advantage of Stokes theorem. Remember that Stokes theorem says da = a, O .O 
for any k-form a and k + 1-dimensional domain O. In other words, we can integrate the derivative of a 
differential form as long as we know its integral along the boundary. But that s exactly the kind of 
information encoded by a discrete differential form! For instance, if a is a discrete 1-form stored on 
the three edges of a triangle s, then we have 3 3 da = a = . a = . a i. s .s ei i=1 i=1 In other words, 
we can exactly evaluate the integral on the left by just adding up three numbers. Pretty cool! In fact, 
the thing on the left is also a discrete differential form: it s the 2-form da integrated over the only 
triangle in our mesh. So for convenience, we ll call this guy d a , and we ll call the operation d the 
discrete exterior derivative. (In the future we will drop the hats from our notation when the meaning 
is clear from context.) In other words, the discrete exterior derivative 3.6. DISCRETE EXTERIOR CALCULUS 
takes a k-form that has already been integrated over each k-simplex and applies Stokes theorem to get 
the integral of the derivative over each k + 1-simplex. In practice (i.e., in code) you can see how this 
operation might be implemented by simply taking local sums over the appropriate mesh elements. However, 
in the example above we made life particularly easy on ourselves by giving each edge an orientation that 
agrees with the orientation of the triangle. Unfortunately assigning a consistent orientation to every 
simplex is not always possible, and in general we need to be more careful about sign when adding up our 
piecewise integrals. For instance, in the example below we d have (d a )1 = a 1 + a 2 + a 3 and (d a 
)2 = a 4 + a 5 - a 2. e1 e4 s1 s2 e2 e3 e5 3.6.4. Discrete Hodge Star. As hinted at above, a discrete 
k-form captures the behavior of a continuous k-form along k directions, but not along the remaining n 
- k directions for instance, a discrete 1-form in 2D captures the .ow along edges but not in the orthogonal 
direction. If you paid close attention to the discussion of Hodge duality, this story starts to sound 
familiar! To capture Hodge duality in the discrete setting, we ll need to de.ne a dual mesh. In general, 
the dual of an n-dimensional simplicial mesh identi.es every k-simplex in the primal (i.e., original) 
mesh with a unique (n - k)-cell in the dual mesh. In a two-dimensional simplicial mesh, for instance, 
primal vertices are identi.ed with dual faces, primal edges are identi.ed with dual edges, and primal 
faces are identi.ed with dual vertices. Note, however, that the dual cells are not always simplices! 
(See above.) A dual mesh is an orthogonal dual if, conceptually, dual elements are contained in orthogonal 
linear subspaces. For instance, on a planar triangle mesh a dual edge would make a right angle with the 
corresponding 3.6. DISCRETE EXTERIOR CALCULUS primal edge. For curved domains, we ask only that primal 
and dual elements be orthogonal intrinsically, e.g., if one rigidly unfolds a pair of neighboring triangles 
into the plane, the primal and dual edges should again be orthogonal. The fact that dual mesh elements 
are contained in orthogonal linear subspaces leads naturally to a notion of Hodge duality in the discrete 
setting. In particular, the discrete Hodge dual of a (discrete) k-form on the primal mesh is an (n - 
k)-form on the dual mesh. Similarly, the Hodge dual of an k-form on the dual mesh is an (n - k)-form 
on the primal mesh. Discrete forms on the primal mesh are called primal forms and discrete forms on the 
dual mesh are called dual forms. Given a discrete form a (whether primal or dual), we ll write its Hodge 
dual as * a . Unlike continuous forms, discrete primal and dual forms live in different places (so for 
instance, discrete primal k-forms and dual k-forms cannot be added to each other). In fact, primal and 
dual forms often have different physical interpretations. For instance, a primal 1-form might represent 
the total circulation along edges of the primal mesh, whereas in the same context a dual 1-form might 
represent the total .ux through the corresponding dual edges (see illustration above). Of course, these 
two quantities (.ux and circulation) are closely related, and naturally leads into one de.nition for 
a discrete Hodge star called the diagonal Hodge star. Consider a primal k-form a. If a i is the value 
of a on the k-simplex si, then the diagonal Hodge star is de.ned by |si *| * a i = a i |si| for all i, 
where |s| indicates the (unsigned) volume of s (which by convention equals one for a vertex!) and |s*| 
is the volume of the corresponding dual cell. In other words, to compute the dual form we simply multiply 
the scalar value stored on each cell by the ratio of corresponding dual and primal volumes. If we remember 
that a discrete form can be thought of as a continuous form integrated over each cell, this de.nition 
for the Hodge star makes perfect sense: the primal and dual quantities should have the same density, 
but we need to account for the fact that they are integrated over cells of different volume. We therefore 
normalize by a ratio of volumes when mapping between primal and dual. This particular Hodge star is called 
diagonal since the ith element of the dual differential 3.6. DISCRETE EXTERIOR CALCULUS form depends 
only on the ith element of the primal differential form. It s not hard to see, then, that Hodge star 
taking dual forms to primal forms (the dual Hodge star) is the inverse of the one that takes primal to 
dual (the primal Hodge star). 3.6.5. That s All, Folks! Hey, wait a minute, what about our other operations, 
like the wedge product (.)? These operations can certainly be de.ned in the discrete setting, but we 
won t go into detail here the basic recipe is to integrate, integrate, integrate. Actually, even in continuous 
exterior calculus we omitted a couple operations like the Lie derivative (LX) and the interior product 
(ia). Coming up with a complete discrete calculus where the whole cast of characters d, ., *, LX, ia, 
etc., plays well together is an active and ongoing area of research. CHAPTER 4   Topological Invariants 
of Discrete Surfaces 4.1. Euler Characteristic A topological disk is, roughly speaking, any shape you 
can get by deforming the unit disk in the plane without tearing it, puncturing it, or gluing its edges 
together. Some examples of shapes that are disks include a .ag, a leaf, and a glove. Some examples of 
shapes that are not disks include a circle (i.e., a disk without its interior), a ball, a sphere, a donut, 
a washer, and a teapot. A polygonal disk is any topological disk constructed out of simple polygons. 
Similarly, a topological sphere is any shape resembling the standard sphere, and a polyhedron is a sphere 
made of polygons. More generally, a piecewise linear surface is any surface made by gluing together polygons 
along their edges; a simplicial surface is a special case of a piecewise linear surface where all the 
faces are triangles. The boundary of a piecewise linear surface is the set of edges that are contained 
in only a single face (all other edges are shared by exactly two faces). For example, a disk has a boundary 
whereas a polyhedron does not. You may assume that surfaces have no boundary unless otherwise stated. 
 EX E R C I S E 1. Polyhedral Formula. Show that for any polygonal disk with V vertices, E edges, and 
F faces, the following relationship holds: V - E + F = 1 and conclude that for any polyhedron V - E + 
F = 2. Hint: use induction. Note that induction is generally easier if you start with a given object 
and decompose it into smaller pieces rather than trying to make it larger, because there are fewer cases 
to think about. 54 4.2. REGULAR MESHES AND AVERAGE VALENCE Clearly not all surfaces look like disks 
or spheres. Some surfaces have additional handles that distinguish them topologically; the number of 
handles g is known as the genus of the surface (see illustration above for examples). In fact, among 
all surfaces that have no boundary and are connected (meaning a single piece), compact (meaning closed 
and contained in a ball of .nite size), and orientable (having two distinct sides), the genus is the 
only thing that distinguishes two surfaces. A more general formula applies to such surfaces, namely V 
- E + F = 2 - 2g, which is known as the Euler-Poincar´e; formula. 4.2. Regular Meshes and Average Valence 
 The valence of a vertex in a piecewise linear surface is the number of faces that contain that vertex. 
A vertex of a simplicial surface is said to be regular when its valence equals six. EX E R C I S E 2. 
Regular Valence. Show that the only (connected, orientable) simplicial surface for which every vertex 
has regular valence is a torus (g = 1). You may assume that the surface has .nitely many faces. Hint: 
apply the Euler-Poincar´e; formula. EX E R C I S E 3. Show that the minimum possible number of irregular 
valence vertices in a (con­nected, orientable) simplicial surface K of genus g is given by m(K) = . 
.. .. 4, g = 0 0, g = 1 1, g = 2, assuming that all vertices have valence at least three and that 
there are .nitely many faces. 4.3. GAUSS-BONNET EX E R CI S E 4. Mean Valence. Show that the mean valence 
approaches six as the number of vertices in a (connected, orientable) simplicial surface goes to in.nity, 
and that the ratio of vertices to edges to faces hence approaches V : E : F = 1 : 3 : 2.  4.3. Gauss-Bonnet 
EX E R CI S E 5. Area of a Spherical Triangle. Show that the area of a spherical triangle on the unit 
sphere with interior angles a1, a2, a3 is A = a1 + a2 + a3 - p. Hint: consider the areas A1, A2, A3 of 
the three shaded regions (called diangles ) pictured below. a2 a1 a3 A A1 A2 A3 EX E R CI S E 6. Area 
of a Spherical Polygon. Show that the area of a spherical polygon with consecutive interior angles ß1, 
. . . , ßn is n A = (2 - n)p + . ßi. i=1 Hint: use the expression for the area of a spherical triangle 
you just derived! EX E R C I S E 7. Angle Defect. Recall that for a discrete planar curve we can de.ne 
the curvature at a vertex as the distance on the unit circle between the two adjacent normals. For a 
discrete surface we can de.ne (Gaussian) curvature at a vertex v as the area on the unit sphere bounded 
by a spherical polygon whose vertices are the unit normals of the faces around v. Show that this area 
is equal to the angle defect d(v) = 2p - . .f (v) f .Fv where Fv is the set of faces containing v and 
.f (v) is the interior angle of the face f at vertex v. 4.3. GAUSS-BONNET Hint: consider planes that 
contain two consecutive normals and their intersection with the unit sphere. v N N3 N2 N2 N4 N1 N3 N1 
N5 N4 N5 EX E R C I S E 8. Discrete Gauss-Bonnet Theorem. Consider a (connected, orientable) simplicial 
surface K with .nitely many vertices V, edges E and faces F. Show that a discrete analog of the Gauss-Bonnet 
theorem holds for simplicial surfaces, namely . d(v) = 2p. v.V where . = |V| - |E| + |F| is the Euler 
characteristic of the surface. CHAPTER 5  Normals of Discrete Surfaces For a smooth surface in R3, 
the normal direction is easy to de.ne: it is the unique direction orthogonal to all tangent vectors in 
other words, it s the direction sticking straight out of the surface. For discrete surfaces the story 
is not so simple. If a mesh has planar faces (all vertices lie in a common plane) then of course the 
normal is well-de.ned: it is simply the normal of the plane. But if the polygon is nonplanar, or if we 
ask for the normal at a vertex, then it is not as clear how the normal should be de.ned. In practice 
there are a number of different possibilities, which arise from different ways of looking at the smooth 
geometry. But before jumping in, let s establish a few basic geometric facts. 5.1. Vector Area Here s 
a simple question: how do you compute the area of a polygon in the plane? Suppose our polygon has vertices 
p1, p2, . . . , pn. One way to compute the area is to stick another point q in the middle and sum up 
the areas of triangles q, pi, pi+1 as done on the left: q pi+1 pi+1 q pi pi 58 5.1. VECTOR AREA A cute 
fact is that if we place q anywhere and sum up the signed triangle areas, we still recover the polygon 
area! (Signed area just means negative if our vertices are oriented clockwise; positive if they re counter-clockwise.) 
You can get an idea of why this happens just by looking at the picture: positive triangles that cover 
too much area get accounted for by negative triangles. The proof is an application of Stokes theorem 
consider a different expression for the area A of a planar polygon P: A = dx . dy. P Noting that dx . 
dy = d(x . dy) = -d(y . dx), we can also express the area as 1 1 A = d(x . dy) - d(y . dx) = x . dy - 
y . dx, 2 P 2 .P where we ve applied Stokes theorem in the .nal step to convert our integral over the 
entire surface into an integral over just the boundary. Now suppose that our polygon vertices have coordinates 
pi = (xi, yi). From here we can explicitly work out the boundary integral by summing up the integrals 
over each edge ei j : x . dy - y . dx = . x . dy - y . dx. .P ei j Since the coordinate functions x and 
y are linear along each edge (and their differentials dx and dy are therefore constant), we can write 
these integrals as s xi +xj yi +yj . x . dy - y . dx = . (yj - yi) - (xj - xi) ei j 2 2 = 2 1 .(pi + 
pj) × (pj - pi) = 2 1 . pi × pj - pi × pi - pj × pj - pj × pi = . pi × pj. In short, we ve shown that 
the area of a polygon can be written as simply 1 A = . pi × pj. 2 i EX E R C I S E 9. Complete the proof 
by showing that for any point q the signed areas of triangles (q, pi, pi+1) sum to precisely the expression 
above. A more general version of the situation we just looked at with polygon areas is the vector area 
of a surface patch f : M . R3, which is de.ned as the integral of the surface normal over the entire 
domain: NV := N d A. M 5.1. VECTOR AREA A very nice property of the vector area is that it depends only 
on the shape of the boundary .M (as you will demonstrate in the next exercise). As a result, two surfaces 
that look very different (such as the ones above) can still have the same vector area the physical intuition 
here is that the vector area measures the total .ux through the boundary curve. For a .at region the 
normal is constant over the surface and we get just the usual area times the unit normal vector. Things 
get more interesting when the surface is not .at for instance, the vector area of a circular band is 
zero since opposing normals cancel each-other out: EX E R C I S E 10. Using Stokes theorem, show that 
the vector area can be written as 1 NV = f . df , 2 .M where the product of two vectors in R3 is given 
by the usual cross product ×. p u. u Here s another fairly basic question: consider a triangle sitting 
in R3, and imagine that we re allowed to pull on one of its vertices p. What s the quickest way to increase 
its area A? In other words, what s the gradient of A with respect to p? EX E R C I S E 11. Show that 
the area gradient is given by 1 . \p As = u 2 where u. is the edge vector across from p rotated by an 
angle p/2 in the plane of the triangle (such that it points toward p). You should require only a few 
very simple geometric arguments there s no need to write things out in coordinates, etc. 5.2. AREA GRADIENT 
  5.2. Area Gradient With these facts out of the way let s take a look at some different ways to de.ne 
vertex normals. There are essentially only two de.nitions that arise naturally from the smooth picture: 
the area gradient and the volume gradient; we ll start with the former. The area gradient asks, which 
direction should we push the surface in order to increase its total area A as quickly as possible? Sliding 
all points tangentially along the surface clearly doesn t change anything: we just end up with the same 
surface. In fact, the only thing we can do to increase surface area is move the surface in the normal 
direction. The idea, then, is to de.ne the vertex normal as the gradient of area with respect to a given 
vertex. Since we already know how to express the area gradient for a single triangle s, we can easily 
express the area gradient for the entire surface: \p A = . \As. s Of course, a given vertex p in.uences 
only the areas of the triangles touching p. So we can just sum up the area gradients over this small 
collection of triangles. EX E R C I S E 12. Show that the gradient of surface area with respect to vertex 
pi can be expressed as \pi A = 1 .(cot aj + cot ßj)( pj - pi) 2 j where pj is the coordinate of the jth 
neighbor of pi and aj and ßj are the angles across from edge (pi, pj). 5.2.1. Mean Curvature Vector. 
The expression for the area gradient derived in the last exercise shows up all over discrete differential 
geometry, and is often referred to as the cotan formula. Interestingly enough this same expression appears 
when taking a completely different approach to de.ning vertex normals, by way of the mean curvature vector 
H N. In particular, for a smooth surface f : M . R3 we have . f = 2H N where H is the mean curvature, 
N is the unit surface normal (which we d like to compute), and . is the Laplace-Beltrami operator (see 
below). Therefore, another way to de.ne vertex normals for a 5.2. AREA GRADIENT discrete surface is to 
simply apply a discrete Laplace operator to the vertex positions and normalize the resulting vector. 
The question now becomes, how do you discretize the Laplacian? We ll take a closer look at this question 
in the future, but the remarkable fact is that the most straightforward discretization of . leads us 
right back to the cotan formula! In other words, the vertex normals we get from the mean curvature vector 
are precisely the same as the ones we get from the area gradient. This whole story also helps us get 
better intuition for the Laplace-Beltrami operator . itself. Unfortunately, there s no really nice way 
to write . the standard coordinate formula you ll .nd in r v1 . i j . a textbook on differential geometry 
is .f = .xi ( |g|g.xj f), where g is the metric. However, |g|this obfuscated expression provides little 
intuition about what . really does, and is damn-near useless when it comes to discretization since for 
a triangle mesh we never have a coordinate representation of g! Later, using exterior calculus, we ll 
see that the (0-form) Laplacian can be expressed as . = *d * d, which leads to a fairly straightforward 
discretization. But for now, we ll make use of a nice tool we learned about earlier: conformal parameterization. 
Remember that if f is a conformal map, then lengths on M and lengths on f (M) are related by a positive 
scaling eu . In other words, |df (X)| = eu|X| for some real-valued function u on M. Moreover, a conformal 
parameterization always exists in other words, we don t have to make any special assumptions about our 
geometry in order to use conformal coordinates in proofs or other calculations. The reason conformal 
coordinates are useful when talking about Laplace-Beltrami is that we can write . as simply a rescaling 
of the standard Laplacian in the plane, i.e., as the sum of second partial derivatives divided by the 
metric scaling factor e2u: d(df(X))(X) + d(df(Y))(Y) .f = , e2u where X and Y are any pair of unit, orthogonal 
directions. What s the geometric meaning here? Remember that for a good old-fashioned function f : R 
. R in 1D, second derivatives basically tell us about the curvature of a function, e.g., is it concave 
or convex? Well, since . is a sum of second derivatives, it s no surprise that it tells us something 
about the mean curvature! EX E R C I S E 13. Show that the relationship . f = 2H N holds. 5.3. VOLUME 
GRADIENT  5.3. Volume Gradient An alternative way to come up with normals is to look at the volume 
gradient. Suppose that our surface encloses some region of space with total volume V. As before, we know 
that sliding the surface along itself tangentially doesn t really change anything: we end up with the 
same surface, which encloses the same region of space. Therefore, the quickest way to increase V is to 
again move the surface in the normal direction. A somewhat surprising fact is that, in the discrete case, 
the volume gradient actually yields a different de.nition for vertex normals than the one we got from 
the area gradient. To express this gradient, we ll use three-dimensional versions of of our basic facts 
from above. First, much like we broke the area of a polygon into triangles, we re going to decompose 
the volume enclosed by our surface into a collection of tetrahedra. Each tetrahedron includes exactly 
one face of our discrete surface, along with a new point q. For instance, here s what the volume might 
look like in the vicinity of a vertex p: p q Just as in the polygon case the location of q makes no 
difference, as long as we work with the signed volume of the tetrahedra. (Can you prove it?) Next, what 
s the volume gradient for a single tetrahedron? One way to write the volume of a tet is as 1 V = Ah, 
3 where A is the area of the base triangle and h is the height. Then using the same kind of geometric 
reasoning as in the triangle case, we know that 1 \pV = A N, 3 where N is the unit normal to the base. 
 5.4. OTHER DEFINITIONS To express the gradient of the enclosed volume with respect to a given vertex 
p, we simply sum up the gradients for the tetrahedra containing p: \pV = . Vi = 1 . Ai Ni. 3 i i At .rst 
glance this sum does not lead to a nice expression for .pV for instance, it uses the normals Ni of faces 
that have little to do with our surface geometry. However, remember that we can place q anywhere we please 
and still get the same expression for volume. In particular, if we put q directly on top of p, then the 
Ni and Ai coincide with the normals and areas (respectively) of the faces containing p from our original 
surface: p = qp Ni Ni q EX E R C I S E 14. Show that the volume gradient points in the same direction 
as the vector area NV (i.e., show that they are the same up to a constant factor). 5.4. Other De.nitions 
So far we ve only looked at de.nitions for vertex normals that arise from some smooth de.nition. This 
way of thinking captures the essential spirit of discrete differential geometry: relationships from the 
smooth setting should persist unperturbed in the discrete setting (e.g., . f = 2H N should be true independent 
of whether ., H, and N are smooth objects or discrete ones). Nonetheless, there are a number of common 
de.nitions for vertex normals that do not have a known origin in the smooth world. (Perhaps you can .nd 
one?) 5.4. OTHER DEFINITIONS 5.4.1. Uniform Weighting.  Perhaps the simplest way to get vertex normals 
is to just add up the neighboring face normals: NU := . i Ni The main drawback to this approach is that 
two different tessellations of the same geometry can produce very different vertex normals, as illustrated 
above. 5.4.2. Tip-Angle Weights. .i Ni A simple way to reduce dependence on the tessellation is to weigh 
face normals by their corresponding tip angles ., i.e., the interior angles incident on the vertex of 
interest: N. := . .i Ni i 5.4. OTHER DEFINITIONS 5.4.3. Sphere-Inscribed Polytope.  Here s another 
interesting approach to vertex normals: consider the sphere S2 consisting of all points unit distance 
from the origin in R3. A nice fact about the sphere is that the unit normal N at a point x . S2 is simply 
the point itself! I.e., N(x) = x. So if we start out with a polytope whose vertices all sit on the sphere, 
one reasonable way to de.ne vertex normals is to simply use the vertex positions. In fact, it s not too 
hard to show that the direction of the normal at a vertex pi can be expressed purely in terms of the 
edge vectors ej = pj - pi, where pj are the immediate neighbors of pi. In particular, we have 1 n-1 ej 
× ej+1 NS = c . |ej|2|ej+1|2 j=0 where the constant c . R can be ignored since we re only interested 
in the direction of the normal. (For a detailed derivation of this expression, see Max, Weights for Computing 
Vertex Normals from Facet Normals. ) Of course, since this expression depends only on the edge vectors 
it can be evaluated on any mesh (not just those inscribed in a sphere). Implement the following methods: 
Vertex::normalEquallyWeighted() Purpose: returns unit vertex normal using uniform weights NU Vertex::normalAreaWeighted() 
Purpose: returns unit vertex normal using face area weights NV Vertex::normalAngleWeighted() Purpose: 
returns unit vertex normal using tip angle weights N. Vertex::normalMeanCurvature() Purpose: returns 
unit mean curvature normal . f Vertex::normalSphereInscribed() Purpose: returns unit sphere-inscribed 
normal NS (The de.nitions for these methods can be found in Vertex.cpp.) Once you ve successfully implemented 
these methods, test them out on the provided meshes. For convenience you can .ip through the different 
methods using the keys [1]-[5]. You can also see what the mesh looks like by viewing it in wireframe 
mode. Do you notice that some de.nitions work better than others? When? Why? The only thing you need 
to submit for the 5.4. OTHER DEFINITIONS coding assignment (via email) is your modi.ed version of Vertex.cpp. 
(Of course, if you modify other .les you should submit these too! For instance, you might .nd it convenient 
to add a method HalfEdge::cotan() that computes the cotangent of the angle across from a given half edge.) 
CHAPTER 6   The Laplacian Earlier we mentioned that the Laplace-Beltrami operator (commonly abbreviated 
as just the Laplacian) plays a fundamental role in a variety of geometric and physical equations. In 
this chapter we ll put the Laplacian to work by coming up with a discrete version of the Poisson equation 
for triangulated surfaces. As in the chapter on vertex normals, we ll see that the same discrete expression 
for the Laplacian (via the cotan formula) arises from two very different ways of looking at the problem: 
using test functions (often known as Galerkin projection), or by integrating differential forms (often 
called discrete exterior calculus). 6.1. Basic Properties Before we start talking about discretization, 
let s establish a few basic facts about the Laplace operator . and the standard Poisson problem .f = 
.. Poisson equations show up all over the place for instance, in physics . might represent a mass density 
in which case the solution f would (up to suitable constants) give the corresponding gravi­tational potential. 
Similarly, if . describes an charge density then f gives the corresponding electric potential (you ll 
get to play around with these equations in the code portion of this assignment). In geometry processing 
a surprising number of things can be done by solving a Poisson equation (e.g., smoothing a surface, computing 
a vector .eld with prescribed singularities, or even computing the geodesic distance on a surface). Often 
we ll be interested in solving Poisson equations on a compact surface M without bound­ary. EX E R C I 
S E 15. A twice-differentiable function f : M . R is called harmonic if it sits in the kernel of the 
Laplacian, i.e., .f = 0. Argue that the only harmonic functions on a compact domain without boundary 
are the constant functions. Your argument does not have to be incredibly formal there are just a couple 
simple observa­tions that capture the main idea. This fact is quite important because it implies that 
we can add a constant to any solution to a Poisson equation. In other words, if f satis.es .f = ., then 
so does f + c since .(f + c) = .f + .c = .f + 0 = .. EX E R C I S E 16. A separate fact is that on a 
compact domain without boundary, constant functions are not in the image of .. In other words, there 
is no function f such that .f = c. Why? This fact is also important because it tells us when a given 
Poisson equation admits a solution. In particular, if . has a constant component then the problem is 
not well-posed. In some situations, 68 6.1. BASIC PROPERTIES however, it may make sense to simply remove 
the constant component. I.e., instead of trying to s solve .f = . one can solve .f = . - .¯, where .¯:= 
M . dV /|M| and |M| is the total volume of M. However, you must be certain that this trick makes sense 
in the context of your particular problem! When working with PDEs like the Poisson equation, it s often 
useful to have an inner product between functions. An extremely common inner product is the L2 inner 
product (·, ·), which takes the integral of the pointwise product of two functions over the entire domain 
O: ( f , g) := f (x)g(x)dx. O In spirit, this operation is similar to the usual dot product on Rn: it 
measures the degree to which two functions line up. For instance, the top two functions have a large 
inner product; the bottom two have a smaller inner product (as indicated by the dark blue regions): 
Similarly, for two vector .elds X and Y we can de.ne an L2 inner product (X, Y) := X(x) · Y(x)dx O which 
measures how much the two .elds line up at each point. Using the L2 inner product we can express an important 
relationship known as Green s .rst identity. Green s identity says that for any suf.ciently differentiable 
functions f and g (. f , g) = -(\ f , \g) + (N · \ f , g)., where (·, ·). denotes the inner product on 
the boundary and N is the outward unit normal. EX E R CI S E 17. Using exterior calculus, show that Green 
s identity holds. Hint: apply Stokes theorem to the 1-form gdf . One last key fact about the Laplacian 
is that it is positive-semide.nite, i.e., . satis.es (.f, f) = 0 for all functions f. (By the way, why 
isn t this quantity strictly greater than zero?) Words cannot express the importance of (semi)de.niteness. 
Let s think about a very simple example: functions of the form f(x, y) = ax2 + bxy + cy2 in the plane. 
Any such function can also be expressed in matrix form: a b/2 x f(x, y) = x y = ax2 + bxy + cy2 , b/2 
c y xT A x 6.1. BASIC PROPERTIES and we can likewise de.ne positive-semide.niteness for A. But what 
does it actually look like? As depicted below, positive-de.nite matrices (xT Ax > 0) look like a bowl, 
positive-semide.nite matrices (xT Ax = 0) look like a half-cylinder, and inde.nite matrices (xT Ax might 
be positive or negative depending on x) look like a saddle: Now suppose you re a back country skiier 
riding down this kind of terrain in the middle of a howling blizzard. You re cold and exhausted, and 
you know you parked your truck in a place where the ground levels out, but where exactly is it? The snow 
is blowing hard and visibility is low all you can do is keep your .ngers crossed and follow the slope 
of the mountain as you make your descent. (Trust me: this is really how one feels when doing numerical 
optimization!) If you were smart and went skiing in Pos Def Valley then you can just keep heading down 
and will soon arrive safely back at the truck. But maybe you were feeling a bit more adventurous that 
day and took a trip to Semi Def Valley. In that case you ll still get to the bottom, but may have to 
hike back and forth along the length of the valley before you .nd your car. Finally, if your motto is 
safety second then you threw caution to the wind and took a wild ride in Indef Valley. In this case you 
may never make it home! In short: positive-de.nite matrices are nice because it s easy to .nd the minimum 
of the quadratic functions they describe many tools in numerical linear algebra are based on this idea. 
Same goes for positive de.nite linear operators like the Laplacian, which can often be thought of as 
sort of in.nite-dimensional matrices (if you take some time to read about the spectral theorem, you ll 
.nd that this analogy runs even deeper, especially on compact domains). Given the ubiquity of Poisson 
equations in geometry and physics, it s a damn good thing . is positive-semide.nite! EX E R C I S E 18. 
Using Green s .rst identity, show that . is negative-semide.nite on any compact surface M without boundary. 
From a practical perspective, why are negative semi-de.nite operators just as good as positive semi-de.nite 
ones? 6.2. DISCRETIZATION VIA FEM 6.2. Discretization via FEM  The solution to a geometric or physical 
problem is often described by a function: the temperature at each point on the Earth, the curvature at 
each point on a surface, the amount of light hitting each point of your retina, etc. Yet the space of 
all possible functions is mind-bogglingly large too large to be represented on a computer. The basic 
idea behind the .nite element method (FEM) is to pick a smaller space of functions and try to .nd the 
best possible solution from within this space. More speci.cally, if u is the true solution to a problem 
and {fi} is a collection of basis functions, then we seek the linear combination of these functions u 
= . xifi, xi . R i such that the difference || u - u|| is as small as possible with respect to some norm. 
(Above we see a detailed curve u and its best approximation u by a collection of bump-like basis functions 
fi.) Let s start out with a very simple question: suppose we have a vector v . R3, and want to .nd the 
best approximation v within a plane spanned by two basis vectors e1, e2 . R3: v e2 v e1 Since v is forced 
to stay in the plane, the best we can do is make sure there s error only in the normal direction. In 
other words, we want the error v - v to be orthogonal to both basis vectors e1 and e2: (v - v) · e1 = 
0, (v - v) · e2 = 0. 6.2. DISCRETIZATION VIA FEM In this case we get a system of two linear equations 
for two unknowns, and can easily compute the optimal vector v . Now a harder question: suppose we want 
to solve a standard Poisson problem .u = f . How do we check whether a given function u is the best possible 
solution? The basic picture still applies, except that our bases are now functions f instead of .nite-dimensional 
vectors ei, and the simple vector dot product · gets replaced by the L2 inner product. Unfortunately, 
when trying to solve a Poisson equation we don t know what the correct solution u looks like (otherwise 
we d be done already!). So instead of the error u - u, we ll have to look at the residual .u - f , which 
measures how closely u satis.es our original equation. In particular, we want to test that the residual 
vanishes along each basis direction fj: (.u - f , fj) = 0, again resulting in a system of linear equations. 
This condition ensures that the solution behaves just as the true solution would over a large collection 
of possible measurements. Next, let s work out the details of this system for a triangulated surface. 
The most natural choice of basis functions are the piecewise linear hat functions fi, which equal one 
at their associated vertex and zero at all other vertices: At this point you might object: if all our 
functions are piecewise linear, and . is a second derivative, aren t we just going to get zero every 
time we evaluate .u? Fortunately we re saved by Green s identity let s see what happens if we apply this 
identity to our triangle mesh, by breaking up the integral into a sum over individual triangles s: (.u, 
fj) = .k(.u, fj)sk = .k(\u, \fj)sk + .k(N · \u, fj).sk . If the mesh has no boundary then this .nal sum 
will disappear since the normals of adjacent triangles are oppositely oriented, hence the boundary integrals 
along shared edges cancel each-other out: 6.2. DISCRETIZATION VIA FEM N N sj sj N N si si N N In this 
case, we re left with simply (\u, \fj) in each triangle sk. In other words, we can test .u as long as 
we can compute the gradients of both the candidate solution u and each basis function fj. But remember 
that u is itself a linear combination of the bases fi, so we have (\u, \fj) =\ . xifi, \fj= . xi(\fi, 
\fj). i i The critical thing now becomes the inner product between the gradients of the basis functions 
in each triangle. If we can compute these, then we can simply build the matrix Ai j := (\fi, \fj) and 
solve the problem Ax = b for the coef.cients x, where the entries on the right-hand side are given by 
bi = ( f , fi) (i.e., we just take the same measurements on the right-hand side). EX E R CI S E 19. Show 
that the aspect ratio of a triangle can be expressed as the sum of the cotangents of the interior angles 
at its base, i.e., w = cot a + cot ß. h EX E R C I S E 20. Let e be the edge vector along the base of 
a triangle. Show that the gradient of the hat function f associated with the opposite vertex is given 
by 6.2. DISCRETIZATION VIA FEM e. \f = , 2A where e. is the vector e rotated by a quarter turn in the 
counter-clockwise direction and A is the area of the triangle. EX E R C I S E 21. Show that for any 
hat function f associated with a given vertex 1 (\f, \f) = (cot a + cot ß) 2 within a given triangle, 
where a and ß are the interior angles at the remaining two vertices. EX E R C I S E 22. Show that for 
the hat functions fi and fj associated with vertices i and j (respec­tively) of the same triangle, we 
have 1 (\fi, \fj) = - cot ., 2 where . is the angle between the opposing edge vectors. 6.3. DISCRETIZATION 
VIA DEC Putting all these facts together, we .nd that we can express the Laplacian of u at vertex i via 
the infamous cotan formula 1 (.u)i = .(cot aj + cot ßj)(uj - ui), 2 j where we sum over the immediate 
neighbors of vertex i.   6.3. Discretization via DEC The FEM approach re.ects a fairly standard way 
to discretize partial differential equations. But let s try a different approach, based on discrete exterior 
calculus (DEC). Interestingly enough, although these two approaches are quite different, we end up with 
exactly the same result! Again we want to solve the Poisson equation .u = f , which (if you remember 
our discussion of differential operators) can also be expressed as *d * du = f . We already outlined 
how to discretize this kind of expression in the notes on discrete exterior calculus, but let s walk 
through it step by step. We start out with a 0-form u, which is speci.ed as a number ui at each vertex: 
 Next, we compute the discrete exterior derivative du, which just means that we want to integrate the 
derivative along each edge: (du)i j = du = u = uj - ui. ei j .ei j 6.3. DISCRETIZATION VIA DEC (Note 
that the boundary .ei j of the edge is simply its two endpoints vi and vj.) The Hodge star converts a 
circulation along the edge ei j into the .ux through the corresponding dual edge ei j * . In particular, 
we take the total circulation along the primal edge, divide it by the edge length to get the average 
pointwise circulation, then multiply by the dual edge length to get the total .ux through the dual edge: 
|e * i j | (*du)i j = (uj - ui). ei j Here |ei j | and ei j * denote the length of the primal and dual 
edges, respectively. Next, we take the derivative of *du and integrate over the whole dual cell: (d * 
du)i = d * du = *du = . Ci .Ci j |e * i j | |ei j | (uj - ui). The .nal Hodge star simply divides this 
quantity by the area of Ci to get the average value over the cell, and we end up with a system of linear 
equations 1 (*d * du)i = |Ci| . j |e * i j | |ei j | (uj - ui) = fi where fi is the value of the right-hand 
side at vertex i. In practice, however, it s often preferable to move the area factor |Ci| to the right 
hand side, since the resulting system (*d * du)i . |ei j * | (uj - ui) = |Ci| fi = j |ei j | can be 
represented by a symmetric matrix. (Symmetric matrices are often easier to deal with numerically and 
lead to more ef.cient algorithms.) Another way of looking at this transformation 6.4. MESHES AND MATRICES 
is to imagine that we discretized the system d * du = * f . In other words, we converted an equation 
in terms of 0-forms into an equation in terms of n­forms. When working with surfaces, the operator d 
* d is sometimes referred to as the conformal Laplacian, because it does not change when we subject our 
surface to a conformal transformation. Alternatively, we can think of d * d as simply an operator that 
gives us the value of the Laplacian integrated over each dual cell of the mesh (instead of the pointwise 
value). EX E R C I S E 23. Consider a simplicial surface and suppose we place the vertices of the dual 
mesh at the circumcenters of the triangles (i.e., the center of the unique circle containing all three 
vertices): vi e aj ßj e * vj Demonstrate that the dual edge e * (i.e., the line between the two circumcenters) 
bisects the primal edge orthogonally, and use this fact to show that |ei j * | 1 = (cot aj + cot ßj). 
|ei j | 2 Hence the DEC discretization yields precisely the same cotan-Laplace operator as the Galerkin 
discretization.  6.4. Meshes and Matrices So far we ve been giving a sort of algorithmic description 
of operators like Laplace. For instance, we determined that the Laplacian of a scalar function u at a 
vertex i can be approximated as 1 (.u)i = .(cot aj + cot ßj)(uj - ui), 2 j where the sum is taken over 
the immediate neighbors j of i. In code, this sum could easily be expressed as a loop and evaluated at 
any vertex. However, a key aspect of working with discrete differential operators is building their matrix 
representation. The motivation for encoding an operator as a matrix is so that we can solve systems like 
.u = f using a standard numerical linear algebra package. (To make matters even more complicated, some 
linear solvers are perfectly happy to work with algorithmic representations of operators called callback 
functions in general, however, we ll need a matrix.) 6.4. MESHES AND MATRICES In the case of the Poisson 
equation, we want to construct a matrix L . R|V|×|V| (where |V| is the number of mesh vertices) such 
that for any vector u . R|V| of values at vertices, the expression Lu effectively evaluates the formula 
above. But let s start with something simpler consider an operator B that computes the sum of all neighboring 
values: (Bu)i = . uj j How do we build the matrix representation of this operator? Think of B a machine 
that takes a vector u of input values at each vertex and spits out another vector Bu of output values. 
In order for this story to make sense, we need to know which values correspond to which vertices. In 
other words, we need to assign a unique index to each vertex of our mesh, in the range 1, . . . , |V|: 
 1 2 9 3 4 5 11 10 6 12 7 8 It doesn t matter which numbers we assign to which vertices, so long as there 
s one number for every vertex and one vertex for every number. This mesh has twelve vertices and vertex 
1 is next to vertices 2, 3, 4, 5, and 9. So we could compute the sum of the neighboring values as . . 
 (Bu)1 = 0 1 1 1 1 0 0 0 1 0 0 0 .................. u1 u2 u3 u4 u5 u6 u7 u8 u9 u10 u11 u12 .................. 
 . Here we ve put a 1 in the jth place whenever vertex j is a neighbor of vertex i and a 0 otherwise. 
Since this row gives the output value at the .rst vertex, we ll make it the .rst row of the matrix B. 
The entire matrix looks like this: 6.5. THE POISSON EQUATION . . B = .................. 0 1 1 1 1 
0 0 0 1 0 0 0 1 0 0 1 0 1 0 0 1 1 0 0 1 0 0 0 1 0 1 0 1 0 1 0 1 1 0 0 1 1 0 1 0 0 0 0 1 0 1 1 0 0 1 1 
0 0 0 0 0 1 0 1 0 0 0 1 0 1 0 1 0 0 1 0 1 0 0 1 0 0 1 1 0 0 0 1 1 1 1 0 0 0 0 1 1 1 1 0 0 0 0 0 0 1 1 
0 0 1 0 0 0 1 0 0 1 0 1 1 0 0 1 0 0 0 1 0 1 1 0 1 0 0 0 0 0 1 1 1 0 1 1 0 .................. (You could 
verify that this matrix is correct, or you could go outside and play in the sunshine. Your choice.) In 
practice, fortunately, we don t have to build matrices by hand we can simply start with a matrix of zeros 
and .ll in the nonzero entries by looping over local neighborhoods of our mesh. Finally, one important 
thing to notice here is that many of the entries of B are zero. In fact, for most discrete operators 
the number of zeros far outnumbers the number of nonzeros. For this reason, it s usually a good idea 
to use a sparse matrix, i.e., a data structure that stores only the location and value of nonzero entries 
(rather than explicitly storing every single entry of the matrix). The design of sparse matrix data structures 
is an interesting question all on its own, but conceptually you can imagine that a sparse matrix is simply 
a list of triples (i, j, x) where i, j . N specify the row and column index of a nonzero entry and x 
. R gives its value.  6.5. The Poisson Equation In the .rst part of the coding assignment you ll build 
the cotan-Laplace operator and use it to solve the scalar Poisson equation .f = . on a triangle mesh, 
where . can be thought of as a (mass or charge) density and f can be thought of as a (gravitational or 
electric) potential. Once you ve implemented the methods below, you can visualize the results via the 
Viewer. (If you want to play with the density function ., take a look at the method Viewer::updatePotential.) 
CO D I N G 1. Implement the method Mesh::indexVertices() which assigns a unique ID to each vertex in 
the range 0, . . . , |V| - 1. CO D I N G 2. Derive an expression for the cotangent of a given angle purely 
in terms of the two incident edge vectors and the standard Euclidean dot product (·) and cross product 
(×). Implement the method HalfEdge::cotan(), which computes the cotangent of the angle across from a 
given halfedge. 6.6. IMPLICIT MEAN CURVATURE FLOW 80 CO D I N G 3. Implement the methods Face::area() 
and Vertex::dualArea(). For the dual area of a vertex you can simply use one-third the area of the incident 
faces you do not need to compute the area of the circumcentric dual cell. (This choice of area will not 
affect the order of convergence.) CO D I N G 4. Using the methods you ve written so far, implement the 
method Mesh::buildLaplacian() which builds a sparse matrix representing the cotan-Laplace operator. (Remember 
to initialize the matrix to the correct size!) CO D I N G 5. Implement the method Mesh::solveScalarPoissonProblem() 
which solves the problem .f = . where . is a scalar density on vertices (stored in Vertex::rho). You 
can use the method solve from SparseMatrix.h; . and f should be represented by instances of DenseMatrix 
of the appropriate size. Be careful about appropriately incorporating dual areas into your computations; 
also remember that the right-hand side cannot have a constant component! You should verify that your 
code produces results that look something like these two images (density on the left; corresponding potential 
on the right):  6.6. Implicit Mean Curvature Flow Next, you ll use nearly identical code to smooth 
out geometric detail on a surface mesh (also known as fairing or curvature .ow). The basic idea is captured 
by the heat equation, which describes the way heat diffuses over a domain. For instance, if u is a scalar 
function describing the temperature at every point on the real line, then the heat equation is given 
by .u .2u = .t .x2 . 6.6. IMPLICIT MEAN CURVATURE FLOW Geometrically this equation simply says that 
concave bumps get pushed down and convex bumps get pushed up after a long time the heat distribution 
becomes completely .at. We also .u could have written this equation using the Laplacian: = .u. In fact, 
this equation is exactly .t the one we ll use to smooth out a surface, except that instead of considering 
the evolution of temperature, we consider the .ow of the surface f : M . R3 itself: . f = . f . .t Remember 
from our discussion of vertex normals that . f = 2H N, i.e., the Laplacian of position yields (twice) 
the mean curvature times the unit normal. Therefore, the equation above reads, move the surface in the 
direction of the normal, with strength proportional to mean curvature. In other words, it describes a 
mean curvature .ow. So how do we compute this .ow? We already know how to discretize the term . f just 
use the cotangent discretization of Laplace. But what about the time derivative . f ? There are all sorts 
.t of interesting things to say about discretizing time, but for now let s use a very simple idea: the 
change over time can be approximated by the difference of two consecutive states: . f fh - f0 , .t h 
where f0 is the initial state of our system (here the initial con.guration of our mesh) and fh is the 
con.guration after a mean curvature .ow of some duration h > 0. Our discrete mean curvature .ow then 
becomes fh - f0 = . f . h The only remaining question is: which values of f do we use on the right-hand 
side? One idea is to use f0, which results in the system fh = f0 + h. f0. This scheme, called forward 
Euler, is attractive because it can be evaluated directly using the known data f0 we don t have to solve 
a linear system. Unfortunately, forward Euler is not numerically stable, which means we can take only 
very small time steps h. One attractive alternative is to use fh as the argument to ., leading to the 
system (I - h.) fh = f0, A where I is the identity matrix (try the derivation yourself!) This scheme, 
called backward Euler, is far more stable, though we now have to solve a linear system A f h = f0. Fortunately 
this system is highly sparse, which means it s not too expensive to solve in practice. (Note that this 
system is not much different from the Poisson system.) CO D I N G 6. Implement the method Mesh::buildFlowOperator(), 
which should look very similar to Mesh::buildLaplacian. 6.6. IMPLICIT MEAN CURVATURE FLOW CO D I N G 
7. Implement the method Mesh::computeImplicitMeanCurvatureFlow(). Note that you can treat each of the 
components of f (x, y, and z) as separate scalar quantities. You should verify that your code produces 
results that look something like these two images (original mesh on the left; smoothed mesh on the right): 
 CHAPTER 7  Surface Parameterization In this chapter we re going to look at the problem of surface 
parameterization. The basic idea is that we have a surface sitting in space, and we want to .atten it 
out in the plane. The oldest example, perhaps, is making a map of the Earth: One thing you ll notice 
about maps of the Earth is that they all look distorted in some way: Greenland looks way too big, or 
north doesn t quite make a right angle with east. These phenomena re.ect a general fact about surface 
parameterization: it s usually impossible to .atten a surface while perfectly preserving both lengths 
and angles in other words, not every surface admits an isometric parameterization. It is, however, always 
possible to .nd an angle-preserving or conformal parameterization, which is what we ll do in this assignment. 
83 7. SURFACE PARAMETERIZATION 7.0.1. Two Quarter Turns Make a Flip: A Brief Review of Complex Numbers. 
If you ve ever seen the complex numbers, you ve probably encountered the totally abysmal description 
of the imaginary unit i as the square root of negative one: v i = -1. Since the square of any real number 
is nonnegative, one argues, the number i must be imaginary. Makes sense, right? The problem with this 
story is that it neglects the simple geometric meaning of i, which turns out to be quite real! So, let 
s start over: the symbol i denotes a quarter-turn in the counter-clockwise direction. For instance, if 
z is a vector pointing east, then iz is a vector pointing north: What happens if we apply another quarter 
turn? We get a half turn, of course!  In other words, we have i(iz) = -z. We can abbreviate this statement 
by writing i2z = -z, which means we must have i2 = -1, i.e., two quater turns make a .ip. That s all. 
No square roots, and very little imagination required. Thinking of the imaginary unit i as a 90-degree 
rotation will be essential in our discussion of conformal maps. 7.1. CONFORMAL STRUCTURE 7.1. Conformal 
Structure  For a surface f : M . R3 sitting in space, we also have a simple way to express 90-degree 
rotations. In particular, if df (X) is a tangent vector in R3, we can express a quarter turn in the counter­clockwise 
direction by taking the cross product with the unit normal N: N × df (X), Since the vector N × df (X) 
is also tangent to the immersed surface f (M), there must be some corresponding tangent vector on the 
domain M let s call this vector JX. In other words, df (JX) = N × df (X). The map J is called the conformal 
structure induced by the immersion f . (Some might prefer to call J an almost complex structure or a 
linear complex structure, but for surfaces all of these ideas are essentially the same.) A Riemann surface 
is a surface with a complex structure, i.e., it is a surface where we know how to measure angles between 
tangent vectors (but possibly not their length). The most important thing to remember about the conformal 
structure J is that, like the imaginary unit i, there is nothing strange or mysterious about it: it denotes 
a quarter turn in the counter-clockwise direction. And, as before, two quarter turns make a .ip: J 2 
= -id, where id just denotes the identity. At this point you might be wondering, ok, so why do we bother 
with two different objects, i and J , that do exactly the same thing? This story is especially confusing 
given that the domain M in the picture above looks an awful lot like a piece of the (complex) plane. 
But in general, M does not have to be a piece of the plane it can be any topological surface (a sphere, 
a donut, etc.). And in general, tangent vectors are not complex numbers! Therefore, it doesn t make much 
sense to write iX, nor does it make sense to write J z. But there s clearly a relationship we want to 
capture here, and that relationship is described by our good friends Augustin Cauchy and Bernhard Riemann. 
7.2. THE CAUCHY-RIEMANN EQUATION 7.2. The Cauchy-Riemann Equation immersed surface df (X) df (JX) f 
i C X JX dz(X) z dz(JX) M 1 abstract domain parameterization Remember that, like the cartographers of 
yore, our goal is to parameterize a given surface over the plane. In particular, we want to .nd a map 
that preserves angles. How can we express this condition more explicitly? Well, we know how to express 
90-degree rotations on the surface, using the complex structure J . And we know how to express 90-degree 
rotations in the plane, using the imaginary unit i. Therefore, an angle-preserving or conformal map z 
: M . C must satisfy the Cauchy-Riemann equation dz(JX) = idz(X) for all tangent vectors X on M. In other 
words, rotating a vector by 90-degrees and then mapping it into the plane is no different from mapping 
it into the plane and then rotating it by 90 degrees. To be more precise, z is a holomorphic function, 
meaning that it preserves both angles and orientation (dz(X) × dz(JX) sticks out of the plane). Maps 
that preserve angles but reverse orientation are called antiholomorphic. Note that the meaning of dz 
in the Cauchy-Riemann equation is no different from the meaning of df when we talk about an immersion 
f : it tells us how tangent vectors get stretched out as we go from one space to another. In fact, like 
f , the map z is just another immersion of M this time into C instead of R3. The basic idea of the Cauchy-Riemann 
equation is that both of these immersions should share an identical notion of angle, as emphasized in 
the illustration above. One way to look at this picture is to imagine that we start out with the abstract 
domain M, which doesn t know how to measure the angle between two vectors. By immersing M in three-dimensional 
space (via the map f ), we inherit the usual Euclidean notion of angle. We then look for a map z to the 
complex plane that shares this same notion of angle (but perhaps a different notion of length!). 7.3. 
DIFFERENTIAL FORMS ON A RIEMANN SURFACE  7.3. Differential Forms on a Riemann Surface Half of life 
is knowing what you want. The other half is knowing how to get it. In this case, we know what we want: 
a map z satisfying the Cauchy-Riemann equation. But how do we actually compute it? In order to connect 
this question to our existing computational tools, let s rewrite Cauchy-Riemann in terms of exterior 
calculus. In fact, let s revisit the whole idea of differential forms in the context of surfaces and 
their conformal structure. As we ll see, this way of thinking can lead to some beautifully simple geometric 
expressions. Recall our geometric interpretation of real-valued differential forms: a k-form measures 
some k-dimensional volume (length, area, etc.). One thing to notice is that on an n-manifold, there are 
no n + 1-dimensional volumes to measure. For instance, we can t measure two-dimensional areas on a curve 
just one-dimensional lengths. Likewise, we can t measure three-dimensional volumes on a surface just 
1D lengths and 2D areas. For this reason, differential forms on surfaces become particularly simple to 
understand: 0-forms look like scalar functions,  1-forms look like vector .elds, and  2-forms look 
like scalar multiples of area.  That s where the list ends! There are no 3-forms (or 4-forms, or 5-forms. 
. . ) to worry about. (A more algebraic way to convince yourself of this fact is to consider the antisymmetry 
of the wedge product: a . ß = -ß . a. What happens when you take the wedge of more than two basis 1-forms?) 
The Hodge star is also particularly easy to express on surfaces. Recall the basic idea behind the Hodge 
star: in n-dimensions, we can specify any k-dimensional linear subspace via a complemen­tary (n - k)-dimensional 
subspace. For instance, we can describe a plane in R3 either by two basis vectors, or by a single normal 
vector. On surfaces, the most interesting case is perhaps the Hodge star on 1-forms. Roughly speaking, 
any 1-form a can also be speci.ed via an orthogonal 1-form *a of equal length: Look familiar? At this 
point it becomes clear that, at least on surfaces, the Hodge star on 1-forms is closely connected to 
the conformal structure J . More precisely, if a is a 1-form on a surface M then 7.3. DIFFERENTIAL FORMS 
ON A RIEMANN SURFACE we can de.ne the 1-form Hodge star * via *a(X) := a(JX) for any tangent vector .eld 
X. In other words, applying *a to a vector X is the same as applying a to the rotated vector JX. The 
Hodge star on 2-forms can also be expressed via the conformal structure. In particular, let . be any 
2-form on a surface M, and let X be any unit vector .eld. Then we have *. := .(X, JX). In other words, 
by using our 2-form to measure a little square of unit area, we can determine the associated scale factor, 
which is just a scalar function on the surface (i.e., a 0-form). Notice that we ve adopted a particular 
convention here: there are two equal and opposite directions orthogonal to a, and we could have just 
as easily adopted the convention that *a(X) = -a(JX) (many authors do!). An important thing to be aware 
of is how this choice affects our expression for the inner product. EX E R C I S E 24. Like the inner 
product for vectors, functions, or vector .elds, the inner product on 1-forms captures the notion of 
how well two 1-forms line up. Any such inner product should be positive-de.nite, i.e., we should have 
((a, a)) = 0 for any 1-form a. Show that the inner product ((a, ß)) = *a . ß. M on real-valued 1-forms 
a, ß is positive-de.nite only if we adopt the convention *a(X) = a(JX). Likewise, show that the inner 
product ((a, ß)) = a . *ß. M is positive-de.nite only if we adopt the convention *a(X) = -a(JX). Hint: 
evaluate the expressions *a . a(X, JX) and a . *a(X, JX). Throughout we will adopt the former convention 
(*a(X) := a(JX)), and will use double bars || · || to denote the corresponding norm, i.e., ||a|| := ((a, 
a)) EX E R C I S E 25. Show that the Hodge star preserves the norm of any 1-form a, i.e., || * a|| = 
||a||. What s the geometric intuition? 7.3.1. Complex Differential Forms. In general, a k-form is a multilinear 
map from k vectors to a scalar. However, a scalar does not have to be a real number. For instance, when 
we looked at the area vector, we viewed the map f : M . R3 as an R3-valued 0-form, and its differential 
df : T M . R3 as an R3-valued 1-form. Likewise, we can also talk about complex-valued k-forms, i.e., 
functions taking k vectors to a single complex number. In the complex setting, it becomes dif.cult to 
interpret k-forms as objects measuring k-dimensional volumes (what s a complex volume?), but we still 
retain all the same algebraic properties (multilinearity, antisymmetry, etc.). We also have a few new 
tools at our disposal. EX E R C I S E 26. Let z = a + bi be any complex number. The complex conjugate 
of z is the number z¯:= a - bi. Show that for any two complex numbers u, v . C we have ¯ uv = u · v + 
(u × v)i 7.4. CONFORMAL PARAMETERIZATION where on the right-hand side we interpret u, v as vectors in 
R2 . Hint: expand the left-hand side in components. Remember that i2 = -1! EX E R CI S E 27. In the real 
setting, inner products are symmetric: we can exchange the two arguments without affecting the result. 
In the complex setting, the analogous concept is that an inner product is Hermitian: changing the arguments 
only conjugates the result. In other words, for any Hermitian inner product (·, ·) on complex numbers, 
we have (u, v) = (v, u). Show that the inner product (u, v) := ¯ uv introduced in the previous exercise 
is Hermitian and positive-de.nite. Hint: use the formula you just derived! Just as we can conjugate complex 
numbers, we can conjugate complex-valued 1-forms. As one might expect, this operation simply .ips the 
imaginary part of the result: (a¯)(X) := (a(X)). Similarly, we can de.ne (a . ß)(X, Y) := (a . ß(X, Y)), 
in which case a . ß = a¯. ß ¯ (why?). EX E R C I S E 28. Let a, ß be complex 1-forms on M. Show that 
the inner product ((a, ß)) := *a¯. ß M is Hermitian and positive-de.nite. Hint: evaluate the integrand 
on a basis (X, JX); the second part of your proof should be very similar to the real case.  7.4. Conformal 
Parameterization At long last we have all the tools we need to describe our algorithm for conformal parame­terization. 
Remember that we want to .nd a map z : M . C that satis.es the Cauchy-Riemann equation dz(JX) = idz(X) 
for all tangent vectors X. If we interpret dz as a complex-valued 1-form, we can rewrite this relationship 
as just *dz = idz. Note that the geometric meaning of this statement hasn t changed: the map *dz rotates 
its argument by 90 degrees before mapping it to the plane; idz rotates vectors by 90-degrees after mapping 
them into the plane. Ultimately, angles are preserved. We can measure the failure of a map to be conformal 
by measuring the total difference between the expression on the left and the expression on the right: 
1 EC(z) := 4 || * dz - idz||2 . The quantity E(z) is called the conformal energy. To compute a conformal 
map, then, we just need to solve a simple convex quadratic optimization problem min EC(z), z subject 
to appropriate constraints. First, however, we re going to rewrite this energy in terms of familiar objects 
like the Laplacian this formulation will make it particularly simple to setup and solve our optimization 
problem in the discrete setting. 7.4. CONFORMAL PARAMETERIZATION EX E R CI S E 29. Let u, v be complex 
functions on M. Assuming that the normal derivative of either function vanishes along the boundary, show 
the .rst Green s identity ((du, dv)) = ((.u, v)) where . = - * d * d is the Laplace-Beltrami operator 
on 0-forms. Hint: you already proved this fact in the real case! EX E R C I S E 30. Let z be a map from 
a topological disk M to the complex plane C. Show that the total signed area A(z) of the region z(M) 
. C can be expressed as i A(z) = - dz¯. dz. 2 M EX E R CI S E 31. Assuming that z has vanishing normal 
derivatives along the boundary, show that the conformal energy EC(z) can be expressed as EC(z) = ED(z) 
- A(z), 1 where the .rst term is the Dirichlet energy ED(z) := 2 ((.z, z)). Hint: use the results of 
the last two exercises! EX E R C I S E 32. Suppose z is a piecewise linear map on a simplicial disk, 
i.e., we have one value of z per vertex. Starting with the formula you derived in Exercise 30, show that 
the signed area of the image z(M) can be expressed as the sum A(z) = - i . z¯izj - z¯jzi. 4 eij.E. where 
E. denotes the set of oriented boundary edges. Hint: .rst, use Stokes theorem. Next, break up the integral 
into an integral over each edge. Finally, think of dz as the pushforward of the edge vectors. CO D I 
N G 8. Implement the method ConformalParameterization::buildEnergy(), which builds a |V| × |V| matrix 
corresponding to the conformal energy EC. For the Dirichlet energy, you can reuse the expression you 
previously derived for the discrete Laplace operator (i.e., the cotan formula) the only difference is 
that these values are now the entries of a complex matrix rather than a real one (SparseMatrix<Complex>). 
For the area term, subtract the expression your derived in Exercise 32 from the Laplacian. You may .nd 
it easiest to simply iterate over the edges of the virtual boundary face (Mesh::boundaries.begin()). 
Great. So to compute a conformal map we just have to know how to discretize the Laplacian . (which already 
did while studying the Poisson equation) and the signed area A. However, let s take another look at our 
optimization problem originally we said we want to solve min || * dz - idz||2 . z There s a glaring problem 
with this formulation, namely that any constant map z(p) = z0 . C is a global minimizer. In other words, 
if we map the whole surface M to a single point z0 in the complex plane then the conformal energy is 
zero. (Why? Because the derivative dz is zero everywhere!) Intuitively, we can imagine that we re trying 
to stretch out a tiny sheet of elastic material (like a small piece of a rubber balloon) over a large 
region in the plane. If we don t nail this sheet down in enough places, it will simply collapse into 
a point: 7.4. CONFORMAL PARAMETERIZATION We therefore need to add some additional constraints to make 
the solution more interesting. But which constraints should we use? If we use too few constraints, the 
solution may still be uninteresting for instance, if we just nail our elastic sheet to a single point, 
it can still collapse around that point. If we use too many constraints, there may be no solution at 
all in other words, there may be no perfectly conformal map (*dz = idz) that satis.es all our constraints 
simultaneously. To understand this situation better, let s take another look at harmonic functions and 
how they relate to holomorphic maps. EX E R CI S E 33. Recall that a function is harmonic if it sits 
in the kernel of the Laplace-Beltrami operator .. Show that any holomorphic map z : M . C is harmonic. 
Hint: use the Cauchy-Riemann equation and the expression for Laplace-Beltrami you derived in the homework 
on vertex normals. Another way to investigate the relationship between harmonic and holomorphic functions 
is to consider our optimization problem min EC(z) = 1 2 ((.z, z)) - A(z). z What does the minimizer look 
like? Well, to make things a bit easier to analyze, let s imagine that the map z is prescribed along 
.M, i.e., we nail down all the points along the boundary. From our discussion of vertex normals, you 
may recall that the signed area is also now .xed, since it can be expressed as a boundary integral. In 
other words, if we pin down the boundary then A(z) evaluates to the same constant for all maps z, and 
so we need only consider the map that minimizes the Dirichlet energy ED(z) = 1 2 ((.z, z)). In particular, 
since ED is positive and quadratic, its minimum will be found wherever its gradient vanishes, i.e., wherever 
.z = 0. In conclusion: the minimizer of conformal energy subject to .xed boundary conditions is harmonic. 
Is it also holomorphic? In other words, does it preserve angles? Sadly, no: even though every conformal 
map is harmonic, not every harmonic map is conformal. EX E R C I S E 34. Let M be a topological disk 
and let . : M . C be a harmonic function (.. = 0) with zero imaginary part, i.e., Im(z) = 0. Give a simple 
geometric reason for why . is not a holomorphic map. (You answer should involve prose only no direct 
calculations!) 7.5. EIGENVECTORS, EIGENVALUES, AND OPTIMIZATION From a practical perspective, this observation 
means that we can t just haphazardly nail down pieces of our rubber sheet and hope for the best in general, 
a harmonic map will not preserve angles (as illustrated above, where we pin the boundary to a given rectangle). 
Instead, let s consider the following optimization problem: min z EC(z) s.t. ||z|| = 1, ((z, 1)) = 0, 
(1) where 1 denotes the constant function z(p) = 1, i.e., the function equal to 1 at every point. What 
do these constraints mean geometrically? Well, suppose A is the total area of the surface f (M). Then 
the second constraint is equivalent to 1 zd A = 0,A M i.e., the average value of the solution is zero. 
Equivalently: the solution must be centered around the origin. The .rst constraint makes sure that the 
solution doesn t collapse around the origin, i.e., in order for the norm to be nonzero, there must be 
at least one nonzero point z(p) = 0. Together, these conditions are sort of the weakest things we can 
ask for: we don t know where we want our map to go, but we sure don t want it to collapse!  7.5. Eigenvectors, 
Eigenvalues, and Optimization Ok, next question: how do we actually solve an optimization problem like 
Equation 1? At .rst glance it might look rather nasty and nonlinear, but in fact problems of this form 
turn out to be some of the nicest ones we can hope to solve. To study the situation in more detail, let 
s revisit an important topic in linear algebra: eigenvalue problems. Often, eigenvectors and eigenvalues 
are introduced in the context of real, .nite-dimensional matrices A . Rn×n in particular, we say that 
a unit vector e . Rn is an eigenvector of A if Ae = .e 7.5. EIGENVECTORS, EIGENVALUES, AND OPTIMIZATION 
for some constant . . R. As with all things in life, it s better if we can attach some kind of geometric 
meaning to this statement. One way to visualize a linear map is to apply it to a bunch of vectors in 
space and see how they change. For instance, suppose we apply a matrix A . R3×3 to all the unit vectors 
in R3 , i.e., the unit sphere. What happens? For one thing, any point on the sphere corresponding to 
an eigenvector ei will grow or shrink by the associated factor .i, but remain pointing in the same direction. 
What about the rest of the sphere? Well, if A is a symmetric matrix (AT = A), then the three corresponding 
eigenvectors e1, e2, e3 are mutually orthogonal (as you will prove in just a minute!). We can therefore 
visualize a symmetric linear map as a squishing of a sphere along these three axes, where the amount 
of squish is determined by the magnitude of the eigenvalues .i: This picture provides a fairly decent 
mental image not only for real symmetric matrices, but more generally for any self-adjoint linear operator. 
A linear operator is just any map from one vector space to another that is, well, linear! Earlier, for 
instance, we looked at the Laplace operator ., which (roughly speaking) maps functions to the sum of 
their second derivatives. Functions form a vector space (you can add two functions, subtract them, multiply 
them by scalars, etc.), and the Laplacian . is said to be linear since it preserves basic vector space 
operations, e.g., .(af + b.) = a.f + b.. for any pair of functions f, . and scalars a, b . R. In the 
context of general linear operators, the idea of eigenvectors and eigenvalues is essentially unchanged: 
an eigenfunction of a linear operator is any function that changes only by a scalar multiple when we 
hit it with the operator. For instance, the constant function 1 is an eigenfunction of the Laplacian 
with associated eigenvalue . = 0, since .1 = 0 = 01. In the next few exercises we ll look at some important 
properties of linear operators and their eigenfunctions, which will help us develop algorithms for conformal 
maps (and other geometry processing problems). EX E R C I S E 35. Let A be a linear operator and let 
((·, ·)) be a Hermitian inner product. The adjoint of A, denoted by A* is the unique linear operator 
satisfying (( Ax, y)) = ((x, A* y)) for all vectors x, y. An operator is called self-adjoint if A* = 
A. Show that all the eigenvalues of a self-adjoint operator are real. EX E R C I S E 36. Let A be a self-adjoint 
linear operator. Show that any two eigenfunctions ei, ej of A with distinct eigenvalues .i, .j must be 
orthogonal, i.e., ((ei, ej)) = 0. 7.5. EIGENVECTORS, EIGENVALUES, AND OPTIMIZATION If you ve had much 
experience with real symmetric matrices, these facts should look familiar: eigenvalues are real, and 
eigenvectors are orthogonal. We ve glossed over a lot of important points here for instance, why should 
eigenfunctions always exist? But the main point is that if an operator is nice enough (e.g., the Laplacian 
. on a compact surface) it will indeed behave much like a good old-fashioned matrix. This relationship 
is particularly valuable in geometry processing, where we would ultimately like to approximate continuous, 
in.nite-dimensional linear operators with discrete, .nite-dimensional matrices which we can actually 
store on our computer. For the moment, this way of thinking will help us develop an algorithm for solving 
our optimization problem above. EX E R C I S E 37. Let A . Rn×n be a real symmetric positive-de.nite 
matrix, i.e., xTAx = 0 for all x. Show that a solution to the optimization problem min xTAx x s.t. ||x|| 
= 1 is given by any solution to the eigenvalue problem Ax = .x, where x is a unit eigenfunction and 
. . R is the smallest eigenvalue of A. Moreover, show that the minimal value itself is the eigenvalue 
.. Hint: note that the constraint ||x|| = 1 is equivalent to the constraint ((x, x)) = 1 and use the 
method of Lagrange multipliers. In other words, our optimization problem (Equation 1) reduces to a standard 
eigenvalue problem. So, how do we solve an eigenvalue problem? In general, there are many fascinating 
eigenvalue algorithms with fancy names and very, very complicated descriptions. Fortunately for us, when 
looking for just the extreme eigenvalues of a matrix (i.e., the biggest or the smallest) we can often 
do just as well by using the stupidest algorithm imaginable. That algorithm is called the power method. 
EX E R CI S E 38. The Power Method. Let A . Rn×n be a real symmetric matrix with distinct, nonzero eigenvalues 
0 < .1, . . . , .n, and corresponding eigenvectors x1, . . . , xn. Consider the iterative procedure y 
. Ay, i.e., we just repeatedly hit the vector y with the matrix A. Show that the unit vector y/|y| converges 
to the eigenvector xn corresponding to the largest eigenvalue .n, as long as y is not initially orthogonal 
to xn. Hint: express y as a linear combination of the eigenvectors. To keep things simple, we made the 
assumption that A has distinct, nonzero eigenvalues, but in general the same principle applies: hit a 
vector over and over again with a matrix and you ll end up with a vector parallel to the largest eigenvector, 
i.e., the eigenvector corresponding to the largest eigenvalue. (How far can you generalize your proof?) 
Geometrically, we can imagine that our unit sphere is gets squashed more and more until it ends up looking 
like a thin spindle along the direction of the largest eigenvector: 7.5. EIGENVECTORS, EIGENVALUES, AND 
OPTIMIZATION Notice that this scheme gives us the largest eigenvalue (and its associated eigenvector). 
To .nd the smallest eigenvalue we need only make a slight modi.cation. EX E R CI S E 39. Let e be an 
eigenfunction of an invertible linear operator A, with associated eigenvalue .. Show that A-1 1 e = . 
e, i.e., show that e is also an eigenfunction of the inverse, but has the reciprocal eigenvalue. Explain 
why this relationship makes sense geometrically, in light of the picture at the beginning of this section. 
To get the smallest eigenvalue, then, we can simply apply the power method using the inverse matrix A-1 
instead of the original matrix A. (Why?) In practice, however, we don t want to explicitly compute the 
inverse matrix A-1, for two very important reasons: (1) computing the inverse of a matrix is, in general, 
numerically unstable and, (2) even very sparse matrices can have very dense inverses (e.g., a sparse 
matrix that takes up ~100MB of memory might have an inverse that takes up ~10GB of memory!).  Instead 
of inverting A and iteratively applying it to some initial vector, we can just solve a linear system 
at each step: Algorithm 1: TH E IN V E R S E PO W E R ME T H O D Require: Initial guess y0. 1: while 
Residual(A, yi-1) > do 2: Solve Ayi = yi-1 3: yi . yi/|yi| 4: end while The function Residual measures 
how far our current guess y is from being an eigenvector. Recalling that eTAe = . for any eigenvector 
e and eigenvalue . (Exercise 37), we could write this function as Residual(A, y) := Ay - (y TAy)y, being 
careful to note that y is a unit vector. CO D I N G 9. Implement the routine smallestEig( const SparseMatrix<T>&#38; 
A, const DenseMatrix&#38; x ), 7.5. EIGENVECTORS, EIGENVALUES, AND OPTIMIZATION which can be found in 
src/SparseMatrix.inl. To compute the residual, you may call the subroutine residual(A,y). CO D I N G 
10. Implement the routine residual( const SparseMatrix<T>&#38; A, const DenseMatrix&#38; x ), which can 
be found in src/SparseMatrix.inl. We now have a concrete procedure for solving eigenvalue problems. Can 
we use it to compute a conformal map? Well, remember that our optimization problem (Equation ??) involves 
two constraints: we want our solution to have unit norm ||z|| = 1, and we want it to be orthogonal to 
any constant function (((z, 1)) = 0). The .rst constraint is already enforced by normalizing the iterate 
yi after each step (by the way, why is this projection legitimate?). The second constraint is a simple 
modi.cation to our existing algorithm. CO D I N G 11. Modify the routine smallestEig( const SparseMatrix<T>&#38; 
A, const DenseMatrix&#38; x ) to enforce the constraint (y, 1) = 0 by simply subtracting the mean of 
y after each application of A, prior to normalization. CO D I N G 12. Implement the routine ConformalParameterization::update(), 
which computes a conformal parameterization of a simplicial disk by computing the .rst non­trivial eigenvector 
of the conformal energy. This routine should call your previous two routines, buildEnergy() and smallestEig(). 
It should also copy the resulting values from the eigen­vector to the texture coordinates Vertex::texcoord 
associated with each vertex. (Once this method has been implemented, you should be able to successfully 
.atten meshes from the Viewer.) 7.5.1. (Smallest) Eigenvalue Problems are Easy! As mentioned above, there 
are a lot of eigen­value algorithms out there many of which are far more sophisticated than the simple 
iterative scheme we describe above. Can t we do better by using something more advanced? Believe it or 
not, for many standard geometry processing tasks (such as conformal parameterzation) the answer is, probably 
not! At least, not if we make a slight modi.cation to our implementation of the (inverse) power method. 
In particular, our current implementation of the power method solves a linear system for each iteration. 
However, we can save an enormous amount of work by taking advantage of prefactorization. Roughly speaking, 
prefactorization decomposes our initial matrix A into a product of matrices that allow us to very quickly 
compute the solution to a system of the form Ax = b. (For instance, it is always possible to write a 
square matrix A as the product of a lower triangular matrix L and an upper triangular matrix U, after 
which point we can quickly solve a system by applying forward-and back-substitution.) From a practical 
perspective, a prefactorization allows us to quickly solve a sequence of linear systems where the matrix 
A stays .xed but the right-hand side b changes (Ax = b1, Ax = b2, . . . ). Of course, a sequence of systems 
with a changing right-hand side sounds a lot like the (inverse) power method! 7.5. EIGENVECTORS, EIGENVALUES, 
AND OPTIMIZATION CO D I N G 13. Prefactorization. Modify your implementation of the inverse power method 
to take advantage of prefactorization. In particular, you can create a SparseFactor object and prefactor 
the matrix A via the method SparseFactor::build(). To solve a linear system using this factorization, 
use the method backsolve() in SparseMatrix.h. There are two important rules of thumb about matrix prefactorization 
in the context of geometry processing (at least when it comes to nice sparse operators like cotan-Laplace): 
(1) factoring a matrix costs about as much as solving a single linear system, and (2) the cost of backsubstitution 
is almost negligible compared to factorization.  The main outcome, then, is that (at least for the types 
of matrices we ve considered in these notes) computing the smallest eigenvector via the power method 
costs about as much as solving a single linear system. In fact, in our code framework all linear systems 
are solved by .rst computing a prefactorization, since (at least these days. . . ) prefactored or direct 
linear solvers tend to be the most ef.cient way to solve a sparse linear system especially if you don 
t have a good preconditioner. They also tend to be much more reliable than standard iterative methods 
like conjugate gradient, which may be very slow to converge for, e.g., poor triangulations. An important 
exception is when working with very large meshes, where matrix factors may not be able to .t into memory 
in this case, a simple iterative solver may be your best bet. In general, understanding the tradeoffs 
among different linear solvers (and other numerical tools) can make or break the effectiveness of a given 
geometry processing algorithm know them well! CHAPTER 8   Vector Field Decomposition and Design In 
this chapter we look at several tools for working with tangent vector .elds on surfaces. A tangent vector 
.eld consists of vectors lying .at along the surface for instance, the hair on the back of your cat or 
the direction of the wind on the surface of the Earth. One way to describe a vector .eld is to simply 
specify each vector individually. This process is quite tedious, however, and in practice there are much 
more convenient ways to describe a vector .eld. In this assignment we re going to look at two different 
but closely related descriptions. First, we ll see how any tangent vector .eld can be expressed in terms 
of a scalar potential, vector potential, and a harmonic component, using a tool called Helmholtz-Hodge 
decomposition. Next, we ll see how a vector .eld can instead be expressed purely in terms of its singularities. 
Finally, we ll tie these two perspectives together and show how Helmholtz-Hodge decomposition and singularity-based 
editing can be combined into a highly effective tool for vector .eld design. Our discussion of vector 
.elds is closely related to the discussion of homology that we initiated while studying loops on surfaces. 
Remember that, very roughly speaking, homology is a tool that helps us detect certain interesting features 
of a space for example, the homology generators of a surface were noncontractible loops that wrap around 
each handle. In the context of vector .elds, we ll see a very closely related notion of de Rham cohomology, 
which helps us detect .elds that also wrap around in a similar way. Interestingly enough, these two ideas 
turn out to be nearly identical in the discrete setting (the only difference is a matrix transpose!). 
98 8.1. HODGE DECOMPOSITION 8.1. Hodge Decomposition 8.1.1. Breaking Up is Easy to Do: Short Exact Sequences. 
The most important tools we ll need have very little to do with geometry or topology just good old fashioned 
linear algebra. In particular, we re going to show that any short exact sequence of vector spaces yields 
a decomposition of the space in the middle into three natural pieces. This perspective will ultimately 
help us make sense of objects in both the continuous and discrete setting. First, let s recall some important 
ideas from linear algebra. In particular, let A : U . V be a linear map between vector spaces U and V, 
and let ((, )) be an inner product on V. The adjoint A* of A is the unique linear operator A* : V . U 
such that (( Au, v)) = ((u, A* v)) for all vectors u . U and v . V. For instance, in the case where the 
operator A can be represented by a real matrix, its adjoint A* is the matrix transpose. There are a few 
natural spaces associated with any linear operator. One is the image, consisting of all vectors in V 
that can be obtained by applying A to some vector in U: im(A) := {v . V|Au = v, u . U}. A complementary 
idea is the cokernel, consisting of all vectors in V that cannot be obtained as the image of some vector 
in U: coker(A) := im(A). . Here the symbol . denotes the orthogonal complement, i.e., all vectors in 
V orthogonal to im(A). Finally, we have the kernel, consisting of all vectors u that get mapped to the 
trivial vector 0 . V: ker(A) := {u . U|Au = 0}. EX E R C I S E 40. Show that the cokernel of a linear 
operator is the same as the kernel of its adjoint. In the context of surfaces, we found interesting loops 
(i.e., the noncontractible ones) by looking for subsets that had an empty boundary but were not themselves 
the boundary of a larger set. This basic idea can be applied in lots of other settings in particular, 
consider a sequence of vector spaces U, V, W connected by two maps A : U . V and B : V . W. A common 
shorthand for such a sequence is U A-. V B-. W. We say that this sequence is exact if B . A = 0, i.e., 
if any vector mapped into V by A subsequently gets killed when we apply B. An interesting question we 
can ask, then, is, are there additional elements of V that get killed by B for some other reason? As 
with loops on surfaces, answering this question helps us unearth interesting stuff about the space V. 
EX E R C I S E 41. The intersection n of two linear subspaces is the collection of all vectors common 
to both of them. Show that im(A) n im(B* ) = 0, AB for any short exact sequence U -. V -. W. In other 
words, show that the maps A and B* don t produce any common vectors except for the zero vector. Hint: 
use the exactness condition B . A = 0 and the result of the previous exercise.  8.1. HODGE DECOMPOSITION 
Finally, we get to the punchline. If things are starting to feel a bit abstract at this point, don t 
dismay! The meaning of this decomposition will become quite clear in the next section, when we apply 
it to tangent vector .elds on surfaces. EX E R C I S E 42. Show that any vector v . V can be expressed 
as v = Au + B* w + z for some triple of vectors u . U, w . W and z . V such that A* z = 0 and Bz = 0. 
Equivalently, show that the vector space U can be decomposed into the three orthogonal subspaces im(A), 
im(B*), and Z := ker(B) \ im(A), where the backslash denotes the set of all vectors in ker(B) that are 
not in im(A) (excluding the origin). Hint: start with the two pieces of V you considered in the previous 
exercise, and consider what the rest looks like. It may be helpful to recall some basic facts about unions, 
intersection, and complements, e.g., (V1 . V2). = V. n V. . 1 2 8.1. HODGE DECOMPOSITION 8.1.2. Helmholtz-Hodge 
Decomposition.  In any .ow, certain features naturally catch the eye: wind swirling around the Great 
Red Spot on Jupiter, or water being sucked down into a mysterious abyss through the bathtub drain. Many 
of these features can be given a precise mathematical description using the decomposition we studied 
in the previous section. Consider, for instance, the vector .eld depicted above. Visually, we experience 
three features: a swirling spot, a sucking sink, and a steady .ow around a lazy river, each of which 
is quite distinct. But can we apply the same kind of decomposition to any vector .eld? To answer this 
question, let s look at an exact sequence called the de Rham complex, which shows up again and again 
in exterior calculus. EX E R CI S E 43. Let d be the exterior derivative on k-forms and let d := *d* 
be the associated codifferential, acting on k + 1-forms. Assuming that the domain M has no boundary, 
show that d and d are adjoint, i.e., show that ((da, ß)) = ((a, dß)) for any k-form a and (k + 1)-form 
ß. Hint: Stokes theorem! EX E R CI S E 44. Helmholtz-Hodge Decomposition. Let Ok denote the space of 
real-valued k­forms on a closed compact n-dimensional manifold M, equipped with its usual L2 inner product. 
 8.1. HODGE DECOMPOSITION A de Rham complex is a sequence 1 dd Ok-1 . Ok+1 -. Ok -. Any such sequence 
is exact: if you recall, d . d = 0 was one of the de.ning properties of the exterior derivative d. Show 
that any k-form . can be expressed as . = da + dß + . for some (k - 1)-form a, k + 1-form ß, and k-form 
. . Ok such that d. = 0 and d. = 0. Hint: apply the results of Exercises 42 and 43. The three pieces 
da, dß, and . show up often enough in exterior calculus that they are given special names. In particular, 
a k-form is exact if it can be expressed as the image of the exterior derivative (e.g., da),  coexact 
if it can be expressed as the image of the codifferential (e.g., dß), and  harmonic if it is in the 
kernel of both d and d in other words, a harmonic form is both closed (d. = 0) and co-closed (d. = 0). 
 We can gain some valuable intuition about these conditions by again considering the special case of 
surfaces. Recall that df = (\f)b , i.e., the exterior derivative of a 0-form f looks like its gradient. 
In a similar vein, we have the following fact. EXERCISE 45. Let ß be a 2-form on a surface M. As with 
any volume form, we can think of ß as the Hodge dual of some 0-form f, i.e., ß = *f. Show that dß = ((\u).)b 
, i.e., the codifferential of a 2-form looks like the gradient, rotated by 90 degrees in the counter­clockwise 
direction. Hint: on a surface, what does the Hodge star on 1-forms look like? Remember our discussion 
of conformal structure. Therefore, in the special case of surfaces we can write any vector .eld X as 
the sum of a curl-free \f, a divergence-free part (\u). , and a harmonic part Y which is both curl-and 
divergence-free. In other words, X = \f + (\u). + Y, where f and u are scalar functions and Y is a vector 
.eld. The corresponding de Rham complex can also be expressed in terms of vector calculus, namely \\× 
C8 -. X - . C8 where C8 denotes the space of smooth, real-valued functions on M, and X denotes the space 
of smooth vector .elds. With this interpretation in mind, we can visualize the Hodge decomposition of 
a 1-form on a surface via the two functions a and *ß, which are sometimes called a scalar potential and 
a vector potential, respectively: 1Most people use the term de Rham complex to refer to the entire sequence 
going from 0-forms to n-forms, but for our purposes this little piece will suf.ce. 8.1. HODGE DECOMPOSITION 
 a ß d d + + da . d ß But wait a minute what s the corresponding picture for the harmonic component? 
Actually, the absence of a picture is the whole point! The harmonic component is precisely the piece 
that cannot be expressed in terms of some potential. In this example, for instance, any such potential 
would have to keep increasing monotonically as we walk around the torus. Of course, no periodic function 
can increase inde.nitely, unless your name happens to be M.C. Escher:  8.1. HODGE DECOMPOSITION EX E 
R CI S E 46. Above, we said that a k-form . is harmonic if it is both closed (d. = 0) and co-closed (d. 
= 0). Some authors instead de.ne harmonic k-forms as elements of the kernel of the k-form Laplacian . 
:= dd + dd. Show that these two de.nitions are equivalent, i.e., show that . is harmonic if and only 
if .. = 0. 8.1.3. Computing a Decomposition. So far our discussion of Helmholtz-Hodge decomposi­tion 
has been very ephemeral: there exists a decomposition of any k-form into its three constituent pieces. 
Unfortunately, although compilers are quite good at interpreting if, while, and for each statements, 
they aren t particulary adept at understanding there exists statements! So, how do we actually compute 
a decomposition of real data? Fortunately, we ve already written everything down in terms of exterior 
calculus, which will make the translation to the discrete setting straightforward. EX E R C I S E 47. 
Let A be a linear operator and let A* be its adjoint. Show that im(A) n ker(A* ) = 0, i.e., any vector 
in the image of A is not in the kernel of A* . EX E R CI S E 48. Let . be a real-valued k-form on a closed 
compact domain. From Exercise 44 we know that . can be expressed as . = da + dß + . for some k - 1-form 
a, k + 1-form ß, and harmonic k-form .. Show that a and ß are solutions to the linear equations dda = 
d. and dd ß = d., respectively. Hint: as always, if you get stuck, think about what you did in the previous 
exercise! A practical procedure for decomposing a vector .eld is therefore: Algorithm 2: HE L M H O LT 
Z -HO D G E DE C O M P O S I T I O N 1: Solve dda = d. 2: Solve dd ß = d. 3: . . . - da - dß In other 
words, we compute the two potentials, then see what remains. (By the way, can you apply this procedure 
to any short exact sequence, or do we need some special structure from the de Rham complex?) The nice 
thing about this algorithm is that we can implement it using the discrete differential operators we ve 
already looked at no additional discretization required! To be a bit more explicit, let d0 . R|E|×|V| 
and d1 . R|F|×|E| be the discrete exterior derivatives on 0-and 1-forms, respectively, where V, E, and 
F, denote the set of vertices, edges, and faces in a triangulated surface. Also let *0 . R|V|×|V|, *1 
. R|E|×|E| and *2 . R|F|×|F| denote the diagonal Hodge star matrices computed using the circumcentric 
dual. (At this point you may .nd it useful to review the earlier material on discrete exterior calculus.) 
The two systems above can then be discretized as *-1dT = *-1dT 0 0 *1 d0a 0 0 *1 . 8.1. HODGE DECOMPOSITION 
and d1 *1 dT *2 ß = d1., 1 where a . R|V| and ß . R|F| are encoded as a single value per vertex and per 
face, respectively. Computationally, it is often more ef.cient if we can solve an equivalent symmetric 
system. In the case of the .rst system we can simply remove the factor star-1 from both sides, yielding 
0 dT = dT 0 *1 d0a 0 *1 .. This system is also positive-semide.nite independent of the quality of the 
triangulation since the operator dT *1 d0 on the left-hand side is simply the restriction of the positive-de.nite 
Laplace­ 0 Beltrami operator . to the space of piecewise linear functions (see our discussion of the 
Poisson equation). Therefore, we can always solve for the potential a using a highly ef.cient numerical 
method like sparse Cholesky factorization this method is implemented in our code framework via the routine 
solvePositiveDefinite. As for the second equation, consider the change of variables ß := *2ß. We can 
then solve the symmetric system d1 *1 d1 T ß = d1. and recover the .nal solution by computing ß = *-1ß 
. (This .nal step comes at virtually no 2 additional cost since the matrix *2 is diagonal the inverse 
simply divides each entry of ß by a known constant.) Unlike the .rst equation, this system is not always 
positive-semide.nite. A suf.cient (though not actually necessary) condition for positive-de.niteness 
is that the entries of the 1-form Hodge star *1 are positive. In general, however, this condition will 
not be met, but we can still ef.ciently compute a solution using LU factorization, which is implemented 
in our code framework via the method solveSquare. (An extremely rough rule of thumb is that LU factorization 
is about twice as costly as Cholesky, since it involves two distinct factors.) CO D I N G 14. Implement 
the methods void HodgeStar0Form<T> :: build() void HodgeStar1Form<T> :: build() void HodgeStar1Form<T> 
:: build() in DiscreteExteriorCalculus.inl, which build the diagonal Hodge star matrices *0, *1, and 
*2, respectively. Notice that these methods are templated on a type T, which can be Real, Complex, or 
Quaternion. In all cases, however, the matrices should have real entries (i.e., the imaginary parts should 
be zero). This setup will allow you to solve a variety of different geometry processing problems using 
the same collection of routines. CO D I N G 15. Implement the methods void ExteriorDerivative0Form<T> 
:: build() void ExteriorDerivative1Form<T> :: build() in DiscreteExteriorCalculus.inl, which build the 
discrete exterior derivatives d0 and d1 on 0-forms and 1-forms, respectively. Check that these two matrices 
have been properly built by computing their product and verifying that the resulting matrix is zero. 
CO D I N G 16. Implement the methods HodgeDecomposition :: computeZeroFormPotential() HodgeDecomposition 
:: computeTwoFormPotential() HodgeDecomposition :: extractHarmonicPart() using the matrices you built 
in the last two exercises. You should now be able to visualize the three components of the provided tangent 
vector .elds from the Viewer. Compare the speed of solving the two linear systems with the generic routine 
solve() versus the more specialized routines solvePositiveDefinite() and solveSquare(), respectively. 
8.2. HOMOLOGY GENERATORS AND HARMONIC BASES One .nal question: why should this procedure work in the 
discrete setting? In other words, how do we know that any discrete vector .eld can be decomposed as in 
the smooth case? Well, as you should have veri.ed in Coding 15, the sequence of vector spaces corresponding 
to the discrete exterior derivatives d0 and d1 is exact. Therefore, all of our previous results about 
Hodge decomposition still apply! In other words, it makes no difference (at least not in this case) whether 
we work with in.nite-dimensional function spaces or .nite-dimensional vector spaces. Ideally, this kind 
of behavior is exactly the kind of thing we want to capture in the discrete setting: our discretization 
should preserve the most essential structural properties of a smooth theory, so that we can directly 
apply all the same theorems and results without doing any additional work. 8.2. Homology Generators 
and Harmonic Bases When working with surfaces of nontrivial topology, we often need to be able to compute 
two things: generators for the .rst homology group, and bases for the space of harmonic 1-forms. Loosely 
speaking, homology generators represent all the basic types of nontrivial loops on a surface. For instance, 
on a torus we can .nd two homology generators: a loop going around the inner radius, and a loop going 
around the outer radius more generally, a closed surface of genus g will have 2g generators. Likewise, 
we will have 2g distinct bases for the harmonic 1-forms, each circulating around a different handle in 
a different direction (see for example the harmonic component we extracted above via Helmholtz-Hodge 
decomposition). People have come up with lots of funky schemes for computing these objects on triangle 
meshes; here we describe the two methods that are perhaps simplest and most ef.cient. In fact, these 
methods are closely related: we .rst compute a collection of homology generators, then use these generators 
to construct a basis for harmonic 1-forms. 8.2.1. Homology Generators. The algorithm for .nding generators, 
called tree-cotree decompo­sition [Epp03, EW05], depends on nothing more than the most elementary graph 
algorithms. If you don t remember anything about graphs, here s a quick and dirty reminder. A graph is 
a set of vertices V and a collection of edges E which connect these vertices, i.e., each edge eij . E 
is an unordered pair {vi, vj} of distinct vertices vi, vj . V. For example, the vertices and edges of 
a simplicial surface describe a graph. The faces and dual edges also describe a graph. A subgraph is 
a subset of a graph that is also a graph. A graph is connected if every vertex can be reached from every 
other vertex along some sequence of edges. A tree is a connected graph containing no cycles. If we wanted 
to be a bit more geometric (we are studying geometry, after all), we could say that a tree is a simplicial 
1-complex that is both connected and simply-connected. If most of this stuff sounds unfamiliar to you, 
go read about graphs! They re important. And fun. To .nd a set of generators, we ll need to compute a 
couple spanning trees. A spanning tree is just a tree touching all the vertices of a given graph. How 
do we compute a spanning tree? If you ve studied graphs before, you might vaguely recall someone mumbling 
something like, Prim s algorithm. . . Kruskal s algorithm. . . O(n log n). . . These two algorithms 
compute minimal spanning trees, i.e., spanning trees with minimum total edges weight. We don t care about 
edge weight, ergo, we don t care about Prim or Kruskal (sorry guys). Actually, we can use a much simpler 
linear time algorithm to get a spanning tree: 8.2. HOMOLOGY GENERATORS AND HARMONIC BASES Algorithm 3: 
X-FI R S T SE A R C H Put any vertex in a bag. Until this bag is empty, pull out a vertex, mark it as 
visited, and put all unvisited neighbors in the bag. Every time you put a neighbor in the bag, add the 
corresponding edge to the tree. Pretty easy. A bag here could be a stack, a queue, or. . . whatever. 
In other words, you could do depth-.rst search. Or breadth-.rst search. Or anything-.rst search. The 
point is, we just need to visit all the vertices2. Oh yeah, and we ll call the initial vertex the root. 
Once we know how to compute a spanning tree, .nding a collection of generators on a simplicial surface 
is also easy: Algorithm 4: TR E E -CO T R E E 1: Build a spanning tree T of primal edges. 2: Build a 
spanning tree T* of dual edges that do not cross edges in T. 3: For each dual edge eij * that is neither 
contained in T* nor crossed by T, follow both of its endpoints back to the root of T*. The resulting 
loop is a generator. Overall, the algorithm will produce 2g generating loops. Instead of writing a proof, 
let s just get a sense for how this algorithm works in practice: EX E R C I S E 49. Recall that the fundamental 
polygon of the torus is just a square with opposite sides glued together. Consider the following graph 
on the torus: Run the tree-cotree algorithm by hand, i.e., draw a primal and dual spanning tree on this 
graph. How many generators did you get? Hint: be careful about corners and edges! 2This discussion inspired 
by Jeff. 8.2. HOMOLOGY GENERATORS AND HARMONIC BASES CO D I N G 17. Implement the methods TreeCotree::buildPrimalSpanningTree(), 
TreeCotree::buildDualSpanningCoTree(), and TreeCotree::buildCycles(), which can be found in TreeCotree.h. 
8.2.2. Harmonic Bases. Once we have the homology generators, the harmonic 1-form bases can also be found 
using a rather simple procedure [TACSD06]. In fact, we can take advantage of our newly-acquired knowledge 
of Hodge decomposition. Suppose we start out with a 1-form . that is closed but not exact. From Exercise 
44, we know that . must then have the form . = da + . for some 0-form a and harmonic 1-form .. Using 
our procedure for Helmholtz-Hodge decomposi­tion (Algorithm 8.1.3) we can easily extract just the harmonic 
part. In fact, since . has no coexact component we need only solve the .rst equation dda = d., or equivalently 
.a = d.. In other words, just a standard scalar Poisson equation. We can then extract the harmonic compo­nent 
via . = . - da as before. Sounds pretty good, but where did . come from in the .rst place? In other words, 
how do we construct a 1-form that is closed but not exact? Well, once we have our generators, it s quite 
easy. For every edge crossing from the left of the generator to the right, set . to +1; for every edge 
crossing from the right to the left, set . to -1: For all remaining edges, set . to zero. The resulting 
1-form is exact. Why? Well, remember that the discrete exterior derviative on 1-forms is just the (oriented) 
sum of edge values around each triangle. Therefore, in each triangle crossed by our generator, we get 
1 - 1 + 0 = 0: 8.2. HOMOLOGY GENERATORS AND HARMONIC BASES (In all other triangles we get 0 + 0 + 0 
= 0.) Ok, so this particular choice of . is closed. But is it also exact? EX E R CI S E 50. Show that 
the 1-form . described above is not exact, i.e., it has a nonzero harmonic component. Hint: Stokes theorem! 
CO D I N G 18. Implement the method HarmonicBases::buildClosedPrimalOneForm(), which constructs a closed 
discrete primal 1-form corresponding to a given homology generator. Be careful about orientation! If 
you successfully completed Exercise 50, you probably noticed that . integrates to a nonzero value along 
the corresponding generator £. Likewise, it s not hard to verify that . vanishes when integrated along 
any other generator. As a result, we can use this procedure to construct a basis for the harmonic 1-forms 
on a triangulated surface. EX E R C I S E 51. Let £1, . . . , £2g be a collection of homology generators, 
constructed as described in Section ??. Let .1, . . . , .1 be the corresponding closed 1-forms, and let 
.1, . . . , .n be the corre­sponding harmonic components. Show that the .i are mutually orthogonal. Algorithm 
5: HA R M O N I C -B A S I S 1: Compute homology generators £1, . . . , £n using TR E E -C O T R E E. 
2: for i = 1, . . . , n do 3: Construct a closed 1-form .i corresponding to £1. 4: Solve .ai = d.i. 5: 
.i . .i - dai 6: end for As discussed in the chapter on parameterization, we can greatly accelerate 
the process by prefactoring the Laplace operator. Since the cost of prefactorization is typically far 
greater than the cost of backsubstitution (and since TR E E -C O T R E E amounts to a simple linear traversal), 
the overall cost of this algorithm is roughly the same as the cost of solving a single Poisson equation. 
CO D I N G 19. Implement the method HarmonicBases::build() which implements the algorithm HA R M O N 
I C -B A S I S described above. You should prefactor the Laplacian using a SparseFactor object, and solve 
the Poisson equations via backsolvePositiveDefinite(). 8.3. CONNECTIONS AND PARALLEL TRANSPORT  8.3. 
Connections and Parallel Transport In discrete differential geometry, there are often many ways to discretize 
a particular smooth object. As discussed earlier, however, the hope is that we can discretize all the 
objects in a given theory so that relationships from the smooth theory still hold in the discrete picture. 
For instance, when we looked at Hodge decomposition we discretized the exterior derivative and the Hodge 
star in such a way that the Hodge decomposition has an identical expression in both the smooth and discrete 
world: . = da + d ß + .. In the case of surfaces, we represented a vector .eld as a discrete 1-form, 
i.e., a number associated with each oriented edge giving the circulation along (or .ux through) that 
edge. In this section we re going to adopt a different perspective based on the theory of connections 
and parallel transport. This time around, we re going to represent a vector .eld as an angle-valued dual 
0-form. More plainly, we re going to store an angle on each face that gives the direction of the .eld. 
Note that this representation ignores magnitude, so what we re really working with is a direction .eld. 
Before going too much further with the discrete theory, though, let s .rst talk about the smooth objects 
we want to discretize! 8.3.1. Parallel Transport. Suppose we have a tangent vector u = df (X) sitting 
on an immersed surface f (M). How do we move from one point of the surface to another while preserving 
the direction of u? If f (M) is completely .at (like the plane itself) then the most natural thing is 
to slide u from one point to the other along a straight path keeping the angle with some reference direction 
.xed to obtain a new vector u '. This process is called parallel transport, and the tangent vectors u 
and u ' are, as usual, said to be parallel. Parallel transport on a curved surface is a bit trickier. 
If we keep u pointing in the same direction, then it ceases to be tangent and now sticks out the side. 
On the other hand, if we instead keep u .at against the surface then we no longer have a consistent, 
global reference direction. Overall, the notion of same direction is not very well-de.ned! 8.3. CONNECTIONS 
AND PARALLEL TRANSPORT Ppq Still, having some notion of same direction could be very convenient in a 
lot of situations. So, rather than looking for some natural de.nition, let s de.ne for ourselves what 
it means to be parallel! Ok, but how do we do that? One idea is to explicitly specify a parallel transport 
map Ppq : Tp M . Tq M that immediately teleports vectors from the tangent plane Tp M to the tangent plane 
Tq M. We could then say that by de.nition two vectors X . Tp M and Y . Tq M are parallel if Ppq (X) = 
Y. (Or equivalently, if the embedded vector u := df (X) is the same as v := df (Y)). Unfortunately we 
d have to specify this map for every pair of points p, q on our surface. Sounds like a lot of work! But 
we re on the right track. An alternative is to describe what it means for vectors to be parallel locally. 
In other words, how must a vector change as we move along the surface in order to remain parallel? One 
way to encode this information is via a connection 1-form, which we can express as a linear map . : T 
M . T M, i.e., given a direction of motion Z, the quantity .(Z) tells us how much a vector X must change 
in order to remain parallel. (The reason . is called a connection is because it tells us how to connect 
nearby tangent spaces, i.e., how to identify tangent vectors in one space with vectors in a nearby space.) 
To get any more formal than this takes a bit of work for now let s just make sure we have a solid geometric 
intuition, which should serve us well in the discrete setting: EX E R C I S E 52. Take a stiff piece 
of cardboard and draw an arrow on it. Now roll it around on the surface of a basketball for a while. 
In effect, you re de.ning a map between the tangent plane where you .rst set down the cardboard and the 
tangent plane at the current location. The rolling and twisting motion you apply at any given moment 
effectively de.nes a connection (at least along a particular path). Try the following experiment. Starting 
at some clearly marked initial point, be very careful to note which direction your arrow points. Now 
roll the cardboard around for a while, eventually bringing it back to the initial point. Does the arrow 
point in the same direction as it did initially? What happens if you take a different path? The phenomenon 
you ve (hopefully) just observed is something called the holonomy of the connection, i.e., the failure 
of the connection to preserve the direction of a vector as we go around a closed loop. We ll say a bit 
more about holonomy in just a minute.  8.3.2. Discrete Connections. 8.3. CONNECTIONS AND PARALLEL TRANSPORT 
How should we specify a connection in the discrete setting? Well, for a given a pair of triangles (i, 
j), we can imagine rigidly unfolding them the plane, translating a vector from one to the other, applying 
a rotation by some small angle .i j , and then rigidly refolding these triangles into their initial con.guration, 
as illustrated above. In other words, we can describe a connection on a triangle mesh via a single angle 
.i j . R for each oriented dual edge in our mesh. We should also make sure that .ji = -.i j . In other 
words, the motion we make going from face j to face i should be the opposite of the motion from i to 
j. Enforcing symmetry ensures that our notion of parallel is consistent no matter which direction we 
travel. The whole collection of angles . . R|E| is called a discrete connection. By the way, does this 
object sound familiar? It should! In particular, we have a single number per oriented dual edge, which 
changes sign when we change orientation. In other words, . is a real-valued, dual, discrete 1-form. 8.3.3. 
The Levi-Civita Connection. In terms of the picture above, we said that an angle .i j = 0 means just 
translate; don t rotate. If we set all of our angles to zero, we get a very special connection called 
the Levi-Civita connection3. The Levi-Civita connection effectively tries to twist a tangent vector as 
little as possible as it moves it from one point to the next. There are many ways to describe the Levi-Civita 
connection in the smooth setting, but a particularly nice geometric description is given by Kobayashi: 
TH E O R E M 1. (Kobayashi) The Levi-Civita connection on a smooth surface is the pullback under the 
Gauss map of the Levi-Civita connection on the sphere. N × Z N Z X What does this statement mean? First, 
recall that the Gauss map N : M . S2 takes a point on the surface to its corresponding unit normal this 
normal can also be thought of as a point on the unit sphere. And what s the Levi-Civita connection on 
the sphere? Well, we said that Levi-Civita tries to twist vectors as little as possible. On a sphere, 
it s not hard to see that the motion of 3Those with some geometry background should note that a discrete 
connection really encodes the deviation from Levi-Civita; it should not be thought of as the connection 
itself. 8.3. CONNECTIONS AND PARALLEL TRANSPORT least twist looks like a rotation of the tangent plane 
along a great arc in the direction Z of parallel transport. More explicitly, we want a rotation around 
the axis N × Z, where N is the normal of our initial tangent plane. Altogether, then, Kobayashi s theorem 
says the following. If we start out with a tangent vector X on our surface and want to transport it in 
the direction Z , we should .rst .nd the tangent plane with normal N on the sphere, and the two corresponding 
tangent vectors X and Z. (Extrinsically, of course, these are just the same two vectors!) We can then 
apply an (in.nitesimal) rotation along the great arc in the direction Z, dragging X along with us. EX 
E R C I S E 53. Use Kobayashi s theorem to justify the unfold, translate, refold procedure that is used 
to de.ne the discrete Levi-Civita connection. Hint: think about unfolding as a rotation. 8.3.4. Holonomy. 
 At this point you may be wondering what all this stuff has to do with vector .eld design. Well, once 
we de.ne a connection on our mesh, there s an easy way to construct a vector .eld: start out with an 
initial vector, parallel transport it to its neighbors using the connection, and repeat until you ve 
covered the surface (as depicted above). One thing to notice is that the vector .eld we end up with is 
completely determined by our choice of connection. In effect, we can design vector .elds by instead designing 
connections. However, something can go wrong here: depending on which connection we use, the procedure 
above may not provide a consistent description of any vector .eld. For instance, consider the planar 
mesh below, and a connection that applies a rotation of 18. as we cross each edge in counter­clockwise 
order. By the time we get back to the beginning, we ve rotated our initial vector 0 by only 5 × 18. = 
90.. In other words, our connection would have us believe that 0 and < are parallel vectors! This phenomenon 
is referred to as the holonomy of the connection. More generally, holonomy is the difference in angle 
between an initial and .nal vector that has been transported around a closed loop. (This de.nition applies 
in both the discrete and smooth setting.) 8.3. CONNECTIONS AND PARALLEL TRANSPORT 8.3.5. Trivial Connections. 
To construct a consistently-de.ned vector .eld, we must ensure that our connection has zero holonomy 
around every loop. Such a connection is called a trivial connection. In fact, the following exercise 
shows that this condition is suf.cient to guarantee consistency everywhere: EX E R C I S E 54. Show that 
parallel transport by a trivial connection is path-independent. Hint: consider two different paths from 
point a to point b. As a result we can forget about the particular paths along which vectors are transported, 
and can again imagine that we simply teleport them directly from one point to another. If we then reconstruct 
a vector .eld via a trivial connection, we get a parallel vector .eld, i.e., a .eld where (at least according 
to the connection) every vector is parallel to every other vector. In a sense, parallel vector .elds 
on surfaces are a generalization of constant vector .elds in the plane. But actually, the following exercise 
shows that any vector .eld can be considered parallel as long as we choose the right connection: EX E 
R C I S E 55. Show that every discrete vector .eld (i.e., a vector per face) is parallel with respect 
to some trivial discrete connection. Hint: think about the difference between vectors in adjacent triangles. 
8.3.6. Curvature of a Connection. We can use a trivial connection to de.ne a vector .eld, but how do 
we .nd a trivial connection? The .rst thing you might try is the Levi-Civita connection after all, it 
has a simple, straightforward de.nition. Sadly, the Levi-Civita connection is not in general trivial: 
EX E R CI S E 56. Show that the holonomy of the discrete Levi-Civita connection around the boundary of 
any dual cell equals the angle defect of the enclosed vertex. Therefore, unless our mesh is completely 
.at, Levi-Civita will exhibit some non-zero amount of holonomy. Actually, you may recall that angle defect 
is used to de.ne a discrete notion of Gaussian curvature. We can also use a connection to determine curvature 
in particular, the curvature of a connection (smooth or discrete) over a topological disk D . M is given 
by the holonomy around the region boundary .D. EX E R CI S E 57. Show that these two notions of curvature 
are the same, i.e., show that the curvature of the discrete Levi-Civita connection over any disk D equals 
the total discrete Gaussian curvature over that region. Hint: use induction on faces. Curvature gives 
us one tool to test whether a connection is trivial. In particular, a trivial connection must have zero 
curvature everywhere. For this reason it s reasonable to say that every trivial connection is .at. But 
is every .at connection also trivial? Well, remember that the 8.3. CONNECTIONS AND PARALLEL TRANSPORT 
curvature of a connection is de.ned in terms of the holonomy around region boundaries. Any such boundary 
is called a contractible loop because it can be continuously deformed to a point without catching on 
anything: In general, there may also be noncontractible loops on a surface that cannot be described 
as the boundary of any disk. For instance, consider the loop . pictured on the torus to the left: In 
general a surface of genus g will have 2g basic types of noncontractible loops called generators. More 
precisely, two loops are said to be homotopic if we can get from one to the other by simply sliding it 
along the surface without ever breaking it. No two distinct generators are homotopic to each other, and 
what s more, we can connect multiple copies of the generators to generate any noncontractible loop on 
the surface. For instance, consider the loop .3, which consists of three copies of . joined end-to-end. 
(Formally, the space of loops together with the operation of concatenation describe the .rst homology 
group on the surface.) If we want to check if a connection is trivial, we need to know that it has nontrivial 
holonomy around both contractible and noncontractible loops. Equivalently: it must have zero curvature 
and nontrivial holonomy around noncontractible loops. As you re about to demonstrate, though, we don 
t need to check all the loops just a small collection of basis loops. EX E R C I S E 58. Show that the 
holonomy around any discrete loop is determined by the curvature at each vertex and the holonomy around 
a collection of 2g generators. 8.3.7. Singularities. There s one more issue we run into when trying to 
.nd a trivial connection. You may remember the Gauss-Bonnet theorem, which says that .v.V d(v) = 2p., 
i.e., the total Gaussian curvature over a surface equals 2p times the Euler characteristic .. In fact, 
this theorem holds if we replace the Gaussian curvature with the curvature of any connection (not just 
Levi-Civita). But something s odd here: didn t we say that a trivial connection should have zero holonomy 
hence zero curvature? So unless . = 0 (i.e., M is a torus) we have a problem! Fortunately the solution 
is simple: we can permit our connection to exhibit nonzero holonomy (hence nonzero curvature) around 
some loops, as long as this holonomy is an integer multiple of 2p. Geometrically, a vector parallel transported 
around any closed loop will still end up back where it started, even if it spins around some whole number 
of times k along the way. Any vertex 8.4. VECTOR FIELD DESIGN where k = 0 is called a singularity (see 
below for some examples). As we ll see in the moment, singularities actually make it easier to design 
vector .elds with the desired appearance, since one can control the global appearance of the .eld using 
only a small number of degrees of freedom.   8.4. Vector Field Design Now on to the fun part: designing 
vector .elds. At this point, you ve already written most of the code you ll need! But let s take a look 
at the details. To keep things simple we re going to assume that M is a topological sphere, so you can 
forget about non-contractible loops for now. Our goal is to .nd a connection 1-form . such that the holonomy 
around every loop is zero. If we let .ij = 0 for every dual edge eij *, then the holonomy around any 
dual cell will be equal to the integrated Gaussian curvature over that cell (Exercise 56), which we ll 
denote by K . R|V|. Therefore, we need to .nd angles .i j such that d0 T . = -K, (2) i.e., the holonomy 
around the boundary of every dual cell should exactly cancel the Gaussian curvature. (Notice that dT 
is the discrete exterior derivative on dual 1-forms.) We also need to 0 incorporate singularities. That 
s easy enough: we can just ask that the angles .i j cancel the existing Gaussian curvature, but add curvature 
corresponding to singularities: d0 T . = -K + 2pk. (3) Here k . Z|V| is a vector of integers encoding 
the type of singularity we want at each vertex. To design a vector .eld, then, we can just set k to a 
nonzero value wherever we want a singularity. Of course, we need to make sure that .i ki = . so that 
we do not violate Gauss-Bonnet. CO D I N G 20. Implement the method Vertex::totalGaussCurvature() which 
computes the total Gauss curvature of the dual vertex associated with a given vertex, i.e., 2p minus 
the sum of incident angles. We now have a nice linear system whose solution gives us a trivial connection 
with a prescribed set of singularities. One last question, though: is the solution unique? Well, our 
connection is determined by one angle per edge, and we have one equation to satisfy per dual cell or 
equivalently, one per vertex. But since we have roughly three times as many edges as vertices (which 
you showed earlier on!), this system is underdetermined. In other words, there are many different trivial 
connections on our surface. Which one gives us the nicest vector .eld? While there s no completely objectively 
answer to this question, the most reasonable thing may be to look for the trivial connection closest 
to Levi-Civita. Why? Well, remember that Levi-Civita twists vectors as little as possible, so we re effectively 
asking for the smoothest vector .eld. Computationally, then, we need to .nd the solution to Equation 
3 with minimum norm (since the 8.4. VECTOR FIELD DESIGN angles .i j already encode the deviation from 
Levi-Civita). As a result, we get the optimization problem min || .||2 ..R|E| (4) s.t. d0 T . = -K + 
2pk. One way to solve this problem would be to use some kind of steepest descent method, like we did 
for mean curvature .ow. However, we can be a bit more clever here by recognizing that Equation 4 is equivalent 
to looking for the solution to Equation 3 that has no component in the null space of d0 T any other solution 
will have larger norm. EX E R CI S E 59. Show that the null space of dT is spanned by the columns of 
d1 T . Hint: what 0 happens when you apply d twice? Hence, to get the smoothest trivial connection with 
the prescribed curvature we could (1) compute any solution . to Equation 3, then (2) project out the 
null space component by computing . = . - d1 T (d1d1 T)-1d1 . . Overall, then, we get a trivial connection 
by solving two nice, sparse linear systems. Sounds pretty good, and in fact that s how the algorithm 
was originally proposed way back in 2010. But it turns out there s an even nicer, more elegant way to 
compute trivial connections, using Helmholtz-Hodge decomposition. (This formulation will also make it 
a little easier to work with surfaces of nontrivial topology.) 8.4.1. Trivial Connections++. So far, 
we ve been working with a 1-form ., which describes the deviation of our connection from Levi-Civita. 
Just for fun, let s rewrite this problem in the smooth setting, on a closed surface M of genus g. Our 
measure of smoothness is still the total deviation from Levi-Civita, which we can again write as ||.||2. 
We still have the same constraint on simply-connected cycles, namely d. = v where v = -K + 2pk (in the 
smooth setting, we can think of k as a sum of Dirac deltas). This time around, we ll also consider the 
holonomy around the 2g nontrivial generators £i. Again, we ll use the 1-form . to cancel the holonomy 
we experience in the smooth setting, i.e., we ll enforce the constraint . = vi, £i where -vi is the holonomy 
we get by walking around the loop £i. (In the discrete setting we can measure this quantity as before: 
transport a vector around each loop by unfolding, sliding, and refolding without any extra in-plane rotation.) 
Overall, then, we get the optimization problem min || .||2 . s.t. d. = u, (5) s £i . = vi, i = 1, . . 
. , 2g. Like any other 1-form, . has a Hodge decomposition, i.e., we can write it as . = da + d ß + . 
for some 0-form a, 2-form ß, and harmonic 1-form .. This expression can be used to simplify our optimization 
problem, as you are about to show! EX E R C I S E 60. Show that Equation 5 can be rewritten as min ||ß||2 
+ ||.||2 . s.t. dd ß = u, s £i d ß + . = vi, i = 1, . . . , 2g. 8.4. VECTOR FIELD DESIGN Hint: use Stokes 
theorem and the result of Exercise 42. There are a couple nice things about this reformulation. First, 
we can .nd the coexact part by simply solving the linear equation dd ß = u, or equivalently d * d * ß 
= u. As with Hodge decomposition, we can make this system even nicer by making the substitution ß := 
*ß, in which case we end up with a standard scalar Poisson problem .ß = u. (6) We can then recover ß 
itself via ß = *ß , as before. Note that the solution ß is unique up to a constant, since on a compact 
domain the only harmonic 0-forms are the constant functions (as you showed when we studied the Poisson 
equation). Of course, since constants are in the kernel of d, we still get a uniquely-determined coexact 
part d ß. The second nice thing about this formulation is that we can directly solve for the harmonic 
part . by solving the system of linear equations . = vi - d ß, £i £i i.e., since dß is uniquely determined 
by Equation 6, we can just move it to the right-hand side. A slightly nicer way to write this latter 
system is using the period matrix of our surface. Let £1, . . . , £2g be a collection of homology generators, 
and let .1, . . . , .2g be a basis for the harmonic 1-forms. The period matrix P . R2g×2g is then given 
by Pij = .i, £i i.e., it measures how much each harmonic basis lines up with each generating cycle. Period 
matrices have an intimate relationship with the conformal structure of a surface, which we discussed 
when looking at parameterization. But we don t have time to talk about that now we have to compute vector 
.elds! In the discrete setting, we can compute the entries of the period matrix by simply summing up 
1-form values over generating cycles. In other words, if £i is a collection of dual edges forming a loop, 
and .i is a dual discrete 1-form (i.e., a value per dual edge), then we have Pij = . (.j)k. e * k .£i 
CO D I N G 21. Implement the method Connection::buildPeriodMatrix(), which simply sums up the values 
of the harmonic 1-form bases over each homology generator. (You should compute these quantities using 
your existing implementation of TR E E -CO T R E E and HA R M O N I C -BA S I S.) Once we have the period 
matrix, we can express our constraint on generator holonomy as follows. Let z . R2g be the coef.cients 
of the harmonic component . with respect to our basis of harmonic 1-forms, i.e., 2g . = . zi.i. i=1 Also, 
let v . R2g be the right-hand side of our constraint equation, encoding both the desired generator holonomy 
as well as the integrals of the coexact part along each generator: v i = vi - d ß. £i 8.4. VECTOR FIELD 
DESIGN Then the harmonic component can be found by solving the 2g × 2g linear system Pz = v , where the 
period matrix P is constant (i.e., it depends only on the mesh geometry and not the con.guration of singularities 
or generator holonomy). Overall, then, we have the following algorithm for computing the smoothest vector 
.eld on a simplicial surface with a prescribed collection of singularities: Algorithm 6: TR I V I A L 
-CO N N E C T I O N + + Require: Vector k . Z|V| of singularity indices adding up to 2p. 1: Solve .ß 
= u 2: Solve Pz = v 3: . . dß + . The resulting 1-form can be used to produce a unit vector .eld via 
the procedure described in Section 8.3.4. Note that the most expensive part of the entire algorithm is 
prefactoring the cotan-Laplace matrix, which is subsequently used to compute both the harmonic 1-form 
bases and to update the potential ß. In comparison, all other steps (.nding generating loops, etc.) have 
a negligible cost, and moreover can be computed just once upon initialization (e.g., the period matrix 
P). In short, .nding the smoothest vector .eld with prescribed singularities costs about as much as solving 
a single scalar Poisson problem! If you ve been paying attention, you ll notice that this statement is 
kind of a theme in these notes: if treated correctly, many of the fundamental geometry processing tasks 
we re interested in basically boil down to solving a Poisson equation. (This outcome is particularly 
nice, since in principle we can use the same prefactorization for many different applications!) CO D 
I N G 22. Write the method Connection::compute(), which implements the algorithm TR I V I A L -C O N 
N N E C T I O N + +. You should now be able to edit vector .elds through the Viewer by shift-clicking 
on singularities. CHAPTER 9   Conclusion Given the framework you ve already built, a bunch of other 
geometry processing algorithms can be implemented almost immediately. For instance, one can compute shortest 
paths or geodesic distance by solving a Poisson equation and integrating a heat .ow, just as in Chapter 
6 [CWW13]. One can also improve the quality of the mesh itself, again by solving simple Poisson equations 
[MMdGD11]. More broadly, one can use these tools to simulate mechanical phenomena such as elastic bodies 
[ACOL00]. These topics (and more!) will be covered in a future revision of these notes. 120 Bibliography 
[ACOL00] Marc Alexa, Daniel Cohen-Or, and David Levin. As-Rigid-as-Possible Shape Interpolation. In Proc. 
ACM SIGGRAPH, pages 157 164, 2000. [CDS10] Keenan Crane, Mathieu Desbrun, and Peter Schr ¨ oder. Trivial 
Connections on Discrete Surfaces. Comp. Graph. Forum, 29(5):1525 1533, 2010. [CWW13] Keenan Crane, Clarisse 
Weischedel, and Max Wardetzky. Geodesics in Heat: A New Approach to Comput­ing Distance Based on Heat 
Flow. ACM Trans. Graph., 2013. [dGC10] Fernando de Goes and Keenan Crane. Trivial Connections Revisited: 
A Simpli.ed Algorithm for Simply-Connected Surfaces, 2010. [DHLM05] Mathieu Desbrun, Anil Hirani, Melvin 
Leok, and Jerrold Marsden. Discrete Exterior Calculus. ArXiv e-prints, 2005. [DKT08] Mathieu Desbrun, 
Eva Kanso, and Yiying Tong. Discrete Differential Forms for Computational Modeling. In Alexander I. Bobenko, 
Peter Schr ¨unther M. Ziegler, editors, Discrete oder, John M. Sullivan, and G ¨Differential Geometry, 
volume 38 of Oberwolfach Seminars, pages 287 324. Birkh ¨auser Verlag, 2008. [DMSB99] Mathieu Desbrun, 
Mark Meyer, Peter Schr ¨ oder, and Alan Barr. Implicit Fairing of Irregular Meshes using Diffusion and 
Curvature Flow. In Proc. ACM SIGGRAPH, pages 317 324, 1999. [Epp03] David Eppstein. Dynamic Generators 
of Topologically Embedded Graphs. In Proc. ACM-SIAM Symp. Disc. Alg. (SODA), 2003. [EW05] Jeff Erickson 
and Kim Whittlesey. Greedy Optimal Homotopy and Homology Generators. In Proc. ACM- SIAM Symp. Disc. Alg. 
(SODA), 2005. [Hir03] Anil Hirani. Discrete Exterior Calculus. PhD thesis, Pasadena, CA, USA, 2003. [KCPS13] 
Felix Kn ¨oder. Globally Optimal Direction Fields. In oppel, Keenan Crane, Ulrich Pinkall, and Peter 
Schr ¨Proc. ACM SIGGRAPH, 2013. [Mac49] Richard MacNeal. The Solution of Partial Differential Equations 
by means of Electrical Networks. PhD thesis, Caltech, 1949. [MMdGD11] Patrick Mullen, Pooran Memari, 
Fernando de Goes, and Mathieu Desbrun. HOT: Hodge-optimized triangulations. In Proc. ACM SIGGRAPH, 2011. 
[MTAD08] Patrick Mullen, Yiying Tong, Pierre Alliez, and Mathieu Desbrun. Spectral Conformal Parameterization. 
Comp. Graph. Forum, 27(5):1487 1494, 2008. [TACSD06] Y. Tong, P. Alliez, D. Cohen-Steiner, and M. Desbrun. 
Designing Quadrangulations with Discrete Harmonic Forms. In Proc. Symp. Geom. Proc., 2006. 121  Appendices 
 122 APPENDIX A A Nice Formula for Normal Curvature f N T n X f (c) c M Consider a unit-speed curve 
c(t) on a domain M . R2, and an immersion f of M into R3; the composition . = f (c) de.nes a curve in 
R3. Letting X = c.denote the time derivative of c, we can express the unit tangent .eld on . as df (X) 
T = . |df (X)| Recall from our notes on curves that the curvature normal .n is de.ned as the change in 
tangent direction as we travel along the curve at unit speed (we ll use a lowercase n here to distinguish 
from the surface normal N). In this case, however, the initially unit-speed curve c may get stretched 
out by the map f . Therefore, to get the curvature normal we have to evaluate dT .n = , d£ where £ denotes 
the distance traveled in R3 along .. The normal curvature .n(.) can be de.ned as the projection of the 
curvature normal onto the surface normal N. More explicitly, we have dT dT dt .n = N · .n = N · = N · 
. d£ dt d£ The quantity ddt £ is just the amount by which the curve gets stretched out as we go from 
M into R3 , which we can also write as |df (X)|. We therefore have 123 A. A NICE FORMULA FOR NORMAL CURVATURE 
dT |df (X)|.N = N · dt d = N · df (X)|df (X)|-1 dt d d = N · dt df (X) |df (X)|-1 + N · df (X) dt |df 
(X)|-1 =0 d = N · dt df (c.) |df (c.)|-1 N · df (c¨) = . |df (c.)| Noting that N · df (c.) = 0 implies 
N · df (c¨) = -N.· df (c.), and moreover that N.= d N (c.), we get -d N (c.) · df (c.) |df (X)|.N = , 
|df (c.)| or equivalently -d N (X) · df (X) .N = , |df (X)|2 which is the formula introduced earlier. 
APPENDIX B Why Are Principal Directions Orthogonal? Earlier we stated that the unit principal directions 
X1, X2 are orthogonal with respect to the metric g induced by the immersion f , i.e., g(X1, X2) = df 
(X1) · df (X2) = 0. First, let s show that g(SX, Y) = g(X, SY), i.e., S is self-adjoint with respect 
to the induced metric (equivalently: the second fundamental form II is symmetric in its two arguments, 
i.e., II (X, Y) = II (Y, X)). To see why, consider that (by de.nition) the normal N is orthogonal to 
any tangent vector df (X): N · df (X) = 0. Differentiating this expression with respect to some other 
direction Y, we get d N (Y) · df (X) = -N · d(df (X))(Y). Using the equality of mixed partial derivatives, 
we see that S is indeed self-adjoint with respect to g: g(SX, Y) = d N (X) · df (Y) = -N · d(df (X))(Y) 
= -N · d(df (Y))(X) = d N (Y) · df (X) = g(X, SY). (By the way, the essential trick we used here comes 
up all the time: if you see a product involving a derivative, try expressing it in terms of the derivative 
of a product.) Returning to our original question, we have .1 g(X1, X2) = .1df (X1) · df (X2) = d N (X1) 
· df (X2) = d N (X2) · df (X1) = .2df (X2) · df (X1) = .2g(X1, X2). Therefore, either .1 = .2 or else 
g(X1, X2) = 0. But if .1 = .2 (i.e., the maximum and minimum principle curvatures are equal) then we 
re at an umbilic point where all normal curvatures are equal in this case we re free to pick the principal 
directions however we please in particular, we can use an arbitrary pair of orthogonal directions. The 
phenomenon we experience here re.ects a more general phenomenon in linear algebra: roughly speaking, 
if A is self-adjoint with respect to B, then A s eigenvectors will be orthogonal with respect to B. One 
.nal question: why should .1 and .2 be the maximum and minimum normal curvatures? Well, think about what 
the largest and smallest eigenvalues of a linear map represent: they represent the largest and smallest 
amount of stretch experienced by a unit vector in any direction. Hence, the normal curvature can be no 
larger than .1 and no smaller than .2. 125  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2013</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>2504443</article_id>
		<sort_key>80</sort_key>
		<display_label>Article No.</display_label>
		<pages>42</pages>
		<display_no>8</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>8</seq_no>
		<title><![CDATA[Numerical methods for linear complementarity problems in physics-based animation]]></title>
		<page_from>1</page_from>
		<page_to>42</page_to>
		<doi_number>10.1145/2504435.2504443</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2504443</url>
		<abstract>
			<par><![CDATA[<p>This course provides an introduction to the definition of linear complementarity problems (LCPs) and outlines the derivation of a toolbox of numerical methods. It also presents a small convergence study on the methods to illustrate their numerical properties. The course is a good introduction to implementing numerical methods, because it includes tips and tricks for implementation based on considerable practical experience.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4193502</person_id>
				<author_profile_id><![CDATA[81100164861]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Kenny]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Erleben]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[K&#248;benhavns Universitet]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1778819</ref_obj_id>
				<ref_obj_pid>1778765</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[{AFC+10} J&#233;r&#233;mie Allard, Fran&#231;ois Faure, Hadrien Courtecuisse, Florent Falipou, Christian Duriez, and Paul G. Kry. Volume contact constraints at arbitrary resolution. <i>ACM Trans. Graph</i>., 29:82:1--82:10, July 2010.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2019410</ref_obj_id>
				<ref_obj_pid>2019406</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[{AO11} Iv&#225;n Aldu&#225;n and Miguel A. Otaduy. Sph granular flow with friction and cohesion. In <i>Proc. of the ACM SIGGRAPH/Eurographics Symposium on Computer Animation</i>, 2011.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[{AP97} M. Anitescu and F. A. Potra. Formulating dynamic multi-rigid-body contact problems with friction as solvable linear complementarity problems. <i>Nonlinear Dynamics</i>, 14:231--247, 1997. 10.1023/A:1008292328909.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[{AT08} Mihai Anitescu and Alessandro Tasora. An iterative approach for cone complementarity problems for nonsmooth dynamics. <i>Computational Optimization and Applications</i>, Nov 2008.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>74356</ref_obj_id>
				<ref_obj_pid>74334</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[{Bar89} David Baraff. Analytical methods for dynamic simulation of nonpenetrating rigid bodies. <i>SIGGRAPH Comput. Graph</i>., 23(3):223--232, 1989.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[{Bar93} David Baraff. Issues in computing contact forces for nonpenetrating rigid bodies. <i>Algorithmica. An International Journal in Computer Science</i>, 10(2-4):292--352, 1993. Computational robotics: the geometric theory of manipulation, planning, and control.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>192168</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[{Bar94} David Baraff. Fast contact force computation for nonpenetrating rigid bodies. In <i>SIGGRAPH '94: Proceedings of the 21st annual conference on Computer graphics and interactive techniques</i>, pages 23--34, New York, NY, USA, 1994. ACM.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>618272</ref_obj_id>
				<ref_obj_pid>616036</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[{Bar95} David Baraff. Interactive simulation of solid rigid bodies. <i>IEEE Comput. Graph. Appl</i>., 15(3):63--75, 1995.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1276502</ref_obj_id>
				<ref_obj_pid>1276377</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[{BBB07} Christopher Batty, Florence Bertails, and Robert Bridson. A fast variational framework for accurate solid-fluid coupling. <i>ACM Trans. Graph</i>., 26, July 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1899410</ref_obj_id>
				<ref_obj_pid>1899404</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[{BDCDA11} Florence Bertails-Descoubes, Florent Cadoux, Gilles Daviet, and Vincent Acary. A nonsmooth newton solver for capturing exact coulomb friction in fiber assemblies. <i>ACM Trans. Graph</i>., 30:6:1--6:14, February 2011.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[{BETC12} Jan Bender, Kenny Erleben, Jeff Trinkle, and Erwin Coumans. Interactive Simulation of Rigid Body Dynamics in Computer Graphics. In Marie-Paule Cani and Fabio Ganovelli, editors, <i>EG 2012 - State of the Art Reports</i>, pages 95--134, Cagliari, Sardinia, Italy, 2012. Eurographics Association.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[{Bil95} Stephen Clyde Billups. <i>Algorithms for complementarity problems and generalized equations</i>. PhD thesis, University of Wisconsin at Madison, Madison, WI, USA, 1995.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[{Bri08} Robert Bridson. <i>Fluid Simulation for Computer Graphics</i>. A K Peters, 2008.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1582116</ref_obj_id>
				<ref_obj_pid>1581383</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[{CA09} Hadrien Courtecuisse and J&#233;r&#233;mie Allard. Parallel Dense Gauss-Seidel Algorithm on Many-Core Processors. In <i>High Performance Computation Conference (HPCC)</i>. IEEE CS Press, jun 2009.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[{CK00} Bintong Chen and Xiaojun Chenand Christian Kanzow. A penalized fischer-burmeister ncp-function. <i>Math. Program</i>, 88:211--216, 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[{Cla90} F. H. Clarke. <i>Optimization and Nonsmooth Analysis</i>. Society for Industrial Mathematics, 1990.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2019418</ref_obj_id>
				<ref_obj_pid>2019406</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[{CM11} Nuttapong Chentanez and Matthias M&#252;ller. A multigrid fluid pressure solver handling separating solid boundary conditions. In <i>Proceedings of the 2011 ACM SIGGRAPH/Eurographics Symposium on Computer Animation</i>, SCA '11, pages 83--90, New York, NY, USA, 2011. ACM.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[{CPS92} Richard Cottle, Jong-Shi Pang, and Richard E. Stone. <i>The Linear Complementarity Problem</i>. Computer Science and Scientific Computing. Academic Press, February 1992.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2024173</ref_obj_id>
				<ref_obj_pid>2070781</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[{DBDB11} Gilles Daviet, Florence Bertails-Descoubes, and Laurence Boissieux. A hybrid iterative solver for robustly capturing coulomb friction in hair dynamics. <i>ACM Trans. Graph</i>., 30(6):139:1--139:12, December 2011.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1100960</ref_obj_id>
				<ref_obj_pid>1100864</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[{DDKA06} Christian Duriez, Frederic Dubois, Abderrahmane Kheddar, and Claude Andriot. Realistic haptic rendering of interacting deformable objects in virtual environments. <i>IEEE Transactions on Visualization and Computer Graphics</i>, 12(1):36--47, January 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>354525</ref_obj_id>
				<ref_obj_pid>354511</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[{DLFK00} Tecla De Luca, Francisco Facchinei, and Christian Kanzow. A theoretical and numerical comparison of some semismooth algorithms for complementarity problems. <i>Computational Optimization and Applications</i>, 16:173--205, 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[{Ebe10} David H. Eberly. <i>Game Physics</i>. Morgan Kaufmann, 2 edition, April 2010.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1243986</ref_obj_id>
				<ref_obj_pid>1243980</ref_obj_pid>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[{Erl07} Kenny Erleben. Velocity-based shock propagation for multibody dynamics animation. <i>ACM Transactions on Graphics (TOG)</i>, 26(2):12, 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[{Erl11} Kenny Erleben. num4lcp. Published online at code.google.com/p/num4lcp/, October 2011. Open source project for numerical methods for linear complementarity problems in physics-based animation.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[{Fis92} A. Fischer. A special newton-type optimization method. <i>Optimization</i>, 24(3-4):269--284, 1992.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[{FK98} Michael C. Ferris and Christian Kanzow. Complementarity and related problems; a survey. Technical report, University of Wisconsin -- Madison, 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>316274</ref_obj_id>
				<ref_obj_pid>316252</ref_obj_pid>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[{FM99} Michael C. Ferris and Todd S. Munson. Interfaces to path 3.0: Design, implementation and usage. <i>Comput. Optim. Appl</i>., 12:207--227, January 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1921434</ref_obj_id>
				<ref_obj_pid>1921427</ref_obj_pid>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[{GZO10} Jorge Gasc&#243;n, Javier S. Zurdo, and Miguel A. Otaduy. Constraint-based simulation of adhesive contact. In <i>Proc. of the ACM SIGGRAPH/Eurographics Symposium on Computer Animation</i>, 2010.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[{Hec04} Chris Hecker. Lemke's algorithm: The hammer in your math toolbox? Online slides from Game Developer Conference, accessed 2011, 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[{Kip07} Peter Kipfer. <i>LCP Algorithms for Collision Detection Using CUDA</i>, chapter 33, pages 723--730. Number 3 in GPU Gems. Addison-Wesley, 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>306359</ref_obj_id>
				<ref_obj_pid>306355</ref_obj_pid>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA[{KK98} Christian Kanzow and Helmut Kleinmichel. A new class of semismooth newton-type methods for nonlinear complementarity problems. <i>Comput. Optim. Appl</i>., 11(3):227--251, December 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1409117</ref_obj_id>
				<ref_obj_pid>1409060</ref_obj_pid>
				<ref_seq_no>32</ref_seq_no>
				<ref_text><![CDATA[{KSJP08} Danny M. Kaufman, Shinjiro Sueda, Doug L. James, and Dinesh K. Pai. Staggered projections for frictional contact in multibody systems. <i>ACM Trans. Graph</i>., 27(5), 2008.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>33</ref_seq_no>
				<ref_text><![CDATA[{Lac03} Claude Lacoursiere. Splitting methods for dry frictional contact problems in rigid multibody systems: Preliminary performance results. In Mark Ollila, editor, <i>The Annual SIGRAD Conference</i>, number 10 in Link&#248;ping Electronic Conference Proceedings, November 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>34</ref_seq_no>
				<ref_text><![CDATA[{L&#246;t84} Per L&#246;tstedt. Numerical simulation of time-dependent contact and friction problems in rigid body mechanics. 5(2):370--393, 1984.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>35</ref_seq_no>
				<ref_text><![CDATA[{Man84} Jan Mandel. A multilevel iterative method for symmetric, positive definite linear complementarity problems. <i>Applied Mathematics and Optimization</i>, 11(1):77--95, February 1984.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>36</ref_seq_no>
				<ref_text><![CDATA[{Mur88} Katta G. Murty. <i>Linear Complementarity, Linear and Nonlinear Programming</i>. Helderman-Verlag, 1988.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>37</ref_seq_no>
				<ref_text><![CDATA[{NW99} Jorge Nocedal and Stephen J. Wright. <i>Numerical optimization</i>. Springer Series in Operations Research. Springer-Verlag, New York, 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1272715</ref_obj_id>
				<ref_obj_pid>1272690</ref_obj_pid>
				<ref_seq_no>38</ref_seq_no>
				<ref_text><![CDATA[{OGRG07} Miguel A. Otaduy, Daniel Germann, Stephane Redon, and Markus Gross. Adaptive deformations with fast tight bounds. In <i>Proceedings of the 2007 ACM SIGGRAPH/Eurographics symposium on Computer animation</i>, SCA '07, pages 181--190, Aire-la-Ville, Switzerland, Switzerland, 2007. Eurographics Association.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>39</ref_seq_no>
				<ref_text><![CDATA[{OTSG09} Miguel A. Otaduy, Rasmus Tamstorf, Denis Steinemann, and Markus Gross. Implicit contact handling for deformable objects. <i>Computer Graphics Forum (Proc. of Eurographics)</i>, 28(2), apr 2009.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>82479</ref_obj_id>
				<ref_obj_pid>82471</ref_obj_pid>
				<ref_seq_no>40</ref_seq_no>
				<ref_text><![CDATA[{Pan90} Jong-Shi Pang. Newton's method for b-differentiable equations. <i>Math. Oper. Res</i>., 15(2):311--341, 1990.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>154035</ref_obj_id>
				<ref_obj_pid>154032</ref_obj_pid>
				<ref_seq_no>41</ref_seq_no>
				<ref_text><![CDATA[{QS93} Liqun Qi and Jie Sun. A nonsmooth version of newton's method. <i>Math. Programming</i>, 58(3):353--367, 1993.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>42</ref_seq_no>
				<ref_text><![CDATA[{rpi} rpi-matlab-simulator. Published online at code.google.com/p/rpi-matlab-simulator/.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>829576</ref_obj_id>
				<ref_seq_no>43</ref_seq_no>
				<ref_text><![CDATA[{Saa03} Yousef Saad. <i>Iterative Methods for Sparse Linear Systems, 2nd edition</i>. SIAM, Philadelpha, PA, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>44</ref_seq_no>
				<ref_text><![CDATA[{Sch94} Stefan Scholtes. Introduction to piecewise differential equations. Prepring No. 53, May 1994.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2185602</ref_obj_id>
				<ref_obj_pid>2185520</ref_obj_pid>
				<ref_seq_no>45</ref_seq_no>
				<ref_text><![CDATA[{SKV&#60;sup&#62;+&#60;/sup&#62;12} Breannan Smith, Danny M. Kaufman, Etienne Vouga, Rasmus Tamstorf, and Eitan Grinspun. Reflections on simultaneous impact. <i>ACM Trans. Graph</i>., 31(4):106:1--106:12, July 2012.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>46</ref_seq_no>
				<ref_text><![CDATA[{SNE09} Morten Silcowitz, Sarah Niebe, and Kenny Erleben. Nonsmooth newton method for fischer function reformulation of contact force problems for interactive rigid body simulation. In <i>Proceedings of Virtual Reality Interaction and Physical Simulation (VRIPHYS)</i>, November 2009.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>47</ref_seq_no>
				<ref_text><![CDATA[{SNE10} Morten Silcowitz, Sarah Niebe, and Kenny Erleben. A nonsmooth nonlinear conjugate gradient method for interactive contact force problems. <i>The Visual Computer</i>, 2010.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>48</ref_seq_no>
				<ref_text><![CDATA[{ST96} David E. Stewart and Jeff C. Trinkle. An implicit time-stepping scheme for rigid body dynamics with inelastic collisions and coulomb friction. <i>International Journal of Numerical Methods in Engineering</i>, 39(15):2673--2691, 1996.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>49</ref_seq_no>
				<ref_text><![CDATA[{Sta99} Jos Stam. Stable fluids. In <i>Proceedings of the 26th annual conference on Computer graphics and interactive techniques</i>, SIGGRAPH '99, pages 121--128, New York, NY, USA, 1999. ACM Press/Addison-Wesley Publishing Co.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2185601</ref_obj_id>
				<ref_obj_pid>2185520</ref_obj_pid>
				<ref_seq_no>50</ref_seq_no>
				<ref_text><![CDATA[{TBV12} Richard Tonge, Feodor Benevolenski, and Andrey Voroshilov. Mass splitting for jitter-free parallel rigid body simulation. <i>ACM Trans. Graph</i>., 31(4):105:1--105:8, July 2012.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Numerical Methods for Linear Complementarity Problems in Physics-based Animation Kenny Erleben Department 
of Computer Science University of Copenhagen kenny@diku.dk February 2013 Abstract In physics-based 
animation linear complementarity problems (LCPs) have historically been used as models of contact forces 
between rigid bodies. Re­cently LCPs are being deployed for other types of animation like deformable 
models, .uids, and granular material. Thus, LCPs are becoming a general important fundamental model. 
Hence, there is a real need for providing the numerical foundation for solving LCPs with numerical methods 
that are suitable for computer graphics. This is the void that these course notes tries to .ll out providing 
a toolbox of solutions for people in physics-based animation. The contribution of these notes is twofold. 
First, we explain the nature of LCPs and discuss the properties of the LCPs encountered in physics-based 
animation. Second, we present a range of numerical meth­ods for solving the LCPs. To help make our results 
available to others we supplement our course notes with Matlab implementations of all iterative methods 
discussed. Keywords: Linear Complementarity Problems, Newton Methods, Splitting Methods, Interior Point 
Methods, Convergence Rate, Performance Study. Contents 1 Introduction 4 2 Understanding The LCP 4 3 Going 
to Higher Dimensions 7 4 Examples of LCPs in Physics-based Animation 8 4.1 The Contact Force Model Example 
. . . . . . . . . . . . . . . . . 8 4.2 The Fluid LCP Model Example . . . . . . . . . . . . . . . . . 
. . . 10 5 The Numerical Methods 12 5.1 Pivoting Methods . . . . . . . . . . . . . . . . . . . . . . 
. . . . . 12 5.1.1 Incremental Pivoting Baraff Style . . . . . . . . . . . . . 13 5.2 Splitting Methods 
. . . . . . . . . . . . . . . . . . . . . . . . . . . 15 5.3 PGS and PSOR from QPs . . . . . . . . . 
. . . . . . . . . . . . . . 17 5.4 The Minimum Map Newton Method . . . . . . . . . . . . . . . . 18 5.5 
The Fischer Newton Method . . . . . . . . . . . . . . . . . . . . 23 6 Tips, Tricks and Implementation 
Hacks 27 7 Convergence, Performance and Robustness Experiments 28 8 Discussion and Future Work 34 1 Introduction 
The linear complementarity problems (LCPs) are notorious in computer graph­ics as being hard to solve 
and dif.cult to understand. However, LCPs are im­portant general purpose models that extend beyond rigid 
body dynamics where they found initial applications [Bar89, Bar93, Bar94, Bar95]. It is evident that 
now deformable models, granular materials and .uids are being formulated using LCPs [DDKA06, BBB07, OGRG07, 
CM11, OTSG09, GZO10, AO11]. Previous work in the .eld of computer graphics on numerical methods for solving 
LCPs are sparse. Baraff [Bar94] introduced a Dantzig pivoting method to the graphics community. More 
recently Lemke s pivoting method has received some attention [Hec04, Kip07, Ebe10]. There are examples 
of projected Gauss Seidel (PGS) type methods [Erl07, CA09, GZO10]. These works often overlook the problem 
that PGS methods may not converge on the problems that are being solved. Recently, a multilevel PGS method 
was presented [CM11] but did not provide any convergence guarantees. In [OGRG07] a multilevel method 
for solving the elastic deformation of a deformable model was combined with a PGS-style solver. However, 
the paper did not address multigrid PGS. Many details on LCPs can be found in applied mathematics textbooks 
[Mur88, CPS92]. We speculate that LCPs will become even more attractive models on a wider scope in computer 
graphics once fast and ef.cient numerical solutions are easily accesible by researchers in simple to 
implement numerical methods. This is why we wrote these notes. A supplementary code repository may be 
found in [Erl11] containing Matlab implementations of all the iterative methods covered in these notes 
and a few implementations in Python. C++ (uBLAS) and CUSP versions of few selected methods are available 
by email request. These course notes assume that the reader is familiar with linear algebra and differential 
calculus. All numerical methods presented are described in a pseudo-code style independent of any speci.c 
programming language, and should be understandable by any computer scientist with rudimentary program­ming 
skills. 2 Understanding The LCP A one-dimensional complementarity problem (CP) can be stated as having 
two real variables x, y . R where we seek to make sure that they always satisfy the complementarity constraint, 
y > 0 . x = 0 or x > 0 . y = 0. (1) This results in a .ip-.op problem that either one variable is positive 
and the other is zero or vice versa. This is written compactly in the notation 0 = y . x = 0. (2) The 
solution space forms a corner shape given by the positive x and y axes. If we now extend the problem 
to include a linear relation between the y and x variables, y = ax + b where a, b . R then we have a 
LCP, y = ax + b, (3a) y = 0, x = 0, and xy = 0. (3b) We can conceptually visualize the whole solution 
space by adding the linear relation on top of the corner shape. Now the solutions for the model are the 
intersection points between the line and the corner shape. This geometric tool provides us with an approach 
to study the nature of LCPs. We ask the reader to consider what would happen if b < 0 and a < 0 or b 
= 0 and a = 0? Apparently, the model parameters a and b determine whether we can expect to .nd a solution 
and whether a solution is unique or not. If we eliminate the y-variable then the LCP can be written compactly 
as, ax + b = 0, (4a) x = 0, (4b) x (ax + b) = 0. (4c) This new form suggests a different approach to 
.nding the solutions. We ob­serve that the complementarity condition (4c) has the familiar form of a 
quadratic function. Because of the inequalities (4a)-(4b) (called unilateral constraints ), any feasible 
x-value will result in a nonnegative value of the quadratic function. The quadratic function will be 
zero only for a solution of the LCP. In other words, we may rephrase the LCP as solving the optimization 
problem x * = arg min x (ax + b) (5) x subject to x = 0 and ax + b = 0. (6) Following this line of inspiration 
and exploring the connection to optimiza­tion problems, we .nd the .rst-order optimality conditions (also 
known as the Karush-Kuhn-Tucker conditions or KKT conditions for short) for the optimiza­tion problem 
[NW99], x * = arg min x1 ax + b, (7) x=0 2 to be (3), where y will be the Lagrange multiplier. We will 
show this in detail. Let the objective function be written as f(x) = 1 12 + bx then the Lagrangian is 
formally de.ned as ax + b = ax 2 2 L(x, y) = f(x) - yx, (8) where we let y denote the Lagrange multiplier 
corresponding to the unilateral constraint x = 0. The .rst-order optimality conditions are 'xL(x, y) 
= 0, (9a) y = 0, (9b) x = 0, (9c) yx = 0. (9d) Noting that 'xL(x, y) = ax + b - y = 0 we have y = ax 
+ b. Substitution yields ax + b = 0 (10) x = 0 (11) x (ax + b) = 0 (12) In other words we arrive at the 
LCP again. This is an important observation. The LCP is not the same as an optimization problem. However, 
all .rst-order optimality solutions for the optimization problem above will be a solution for the LCP 
and vice versa. Again the optimization reformulation gives us insight into whether one can expect to 
.nd solutions. For this 1D case we note that whenever a > 0 one has a strict convex optimization problem 
subject to linear constraints. Thus, constraint quali.cations are ful.lled and one is guaranteed a solution 
exists [NW99] (Constraint quali.cations are suf.cient conditions for an optimization problem such that 
the tangent cone and the set of linearized feasible directions are the same set. This are necessary regularity 
conditions that ensure the .rst-order conditions are well-posed). If a < 0 then the objective is unbounded 
from below and we may get into trouble. Observe the 1D LCP is a combinatorial problem. As soon as one 
has chosen whether y or x is positive then the problem is reduced to that of a linear relation from which 
the solution is trivially computed. Before moving into higher-dimensional spaces we will introduce more 
ideas on reformulating the LCP into other types of problems. The .rst reformulation we will present is 
known as the minimum map reformulation[Pan90, CPS92, Mur88]. Essentially it can be written as h(x, y) 
= min(x, y). (13) Here the minimum map function, min(x, y), is de.ned as x if x < y min(x, y) =(14) 
y otherwise * * Now a solution x , y * for the LCP ful.lls h(x , y *) = 0. Thus, we have * * h(x , y 
* ) = 0 iff 0 = y . x * = 0. (15) This can be proven by a case-by-case analysis as shown here h(x, y) 
y < 0 y = 0 y > 0 x < 0 < 0 < 0 < 0 x = 0 < 0 = 0 = 0 x > 0 < 0 = 0 > 0 For the LCP we know that y is 
a function of x and thus h is essentially only a function of x, we write h(x) = min(x, ax + b). (16) 
The bene.t of this reformulation of the LCP is that we have converted our prob­ * lem into a root .nding 
problem. Thus, any solution x for h(x) = 0 will be a solution for our LCP. Another popular reformulation 
is based on the Fischer-Burmeister function which is de.ned as [Fis92, CPS92, Mur88] f(x, y) =x2 + y2 
- x - y. (17) Again we have a similar property as for the minimum map reformulation, namely * * f(x , 
y * ) = 0 iff 0 = y . x * = 0. (18) As before this can be proven by a case-by-case analysis. f(x, y) 
y < 0 y = 0 y > 0 x < 0 > 0 > 0 > 0 x = 0 > 0 = 0 = 0 x > 0 > 0 = 0 < 0 Again we have the option of solving 
our problem by .nding the roots of f(x) = 0. One may believe that one can simply plug the h(x) or f(x) 
into the Newton Raphson method and .nd a solution. The problem with h and f is that they are nonsmooth 
functions implying that for certain points one can not compute the derivatives . h(x) and .f(x) . To 
circumvent the problem we will need nonsmooth .x . x analysis that allows us to compute a generalized 
Jacobian which we can use to create a generalized Newton method. 3 Going to Higher Dimensions Having 
gained familiarity with the one-dimensional LCP we will now extend the ideas to higher dimensions. Let 
b, x . Rn and A . Rn×n so y = Ax + b. For the n-dimensional LCP we have for all i . [1..n], xi = 0, (19a) 
(Ax + b)i = 0, (19b) xi(Ax + b)i = 0. (19c) We can write this compactly in matrix-vector notation as 
x = 0, (20a) (Ax + b) = 0, (20b) x T (Ax + b) = 0. (20c) Observe that = implies that the inequality holds 
element-wise. That is x = 0 implies xi = 0 for all i. We may now rediscover the reformulations we have 
in­troduced earlier for the one-dimensional case. Assuming a symmetric A-matrix the Quadratic Programming 
(QP) reformulation becomes x * = arg min f(x) (21) x=0 where f(x) = 1 xT Ax + bT x. The .rst order optimality 
conditions of the opti­ 2 mization problem is the LCP [NW99], y = Ax + b, (22a) y = 0, (22b) x = 0, (22c) 
x T y = 0. (22d) Applying the minimum map reformulation in an element-wise manner results in the nonsmooth 
root search problem, . . h(x1, y1) H(x) = H(x, y) = . . . . . = 0. (23) h(xn, yn) The Fischer-Burmeister 
function can be applied individually to each comple­mentarity constraint to create the root search problem, 
. . f(x1, y1) . . . F(x) = F(x, y) = . . . = 0. (24) . f(xn, yn) From a practical viewpoint we are now 
concerned with how we can numerically solve these various problem formulations which will be the topic 
of Section 5. Before we study the numerical methods we will brie.y in Section 4 motivate lin­ ear complementarity 
problem cases found in physics-based animation through examples. 4 Examples of LCPs in Physics-based 
Animation We will in the following two subsections show two examples of LCP models one for contact forces 
and another for .uid-solid wall boundaries. 4.1 The Contact Force Model Example We will present a LCP 
model for frictional contact force computations [Löt84, ST96, AP97]. It should be noted that contact 
forces can be modeled in other ways than using LCPs [BDCDA11, DBDB11, AT08]. We refer the interested 
reader to [BETC12] for complete references on alternatives. However, a lot of recent work in the .eld 
of graphics is using LCPs [KSJP08, CA09, AFC+10, SKV+12, TBV12]. Here we do not seek to cover all alternatives 
but settle for motivating how a simulation problem can be written as a LCP problem. Rigor­ous modeling 
details may be found in [BETC12]. To simplify notation we will without loss of generality present the 
model abstractly as though we were considering a single contact point. To model non­penetration one use 
the complementarity condition 0 = vn . .n = 0 (25a) where vn is the normal component of the relative 
contact velocity and .n is the magnitude of the normal contact impulse. This means if there is a separation 
vn > 0 then there can be no normal contact impulse. On the other hand if there is a normal contact impulse 
.n > 0 this is non-sticking and there must be a resting contact vn = 0. Stewart and Trinkle, Anitescu 
and Potra [ST96, AP97] among others lin­ earized the 3D friction model using a polyhedral cone. The polyhedral 
cone is given by a positive span of K unit vectors ti. Let .n be the magnitude of the fT normal impulse 
and .t =.t1 · · · .tK the vector of friction impulses. The linearized model is 0 = vn . .n = 0, (26a) 
0 = ße + vt . .t = 0, (26b)  0 =µ.n -.ti. ß = 0, (26c) i where e is a K dimensional vector of ones. 
The .rst complementarity constraint models the non-penetration constraint as before. The second equation 
makes sure that in case we do have friction .ti > 0 for some i then ß will estimate the maximum sliding 
velocity along the ti s directions. Observe this equation is a K-dimensional vector equation. Its main 
purpose is to choose the ti direction that best approximates the direction of maximum dissipation. The 
last equation makes sure the friction force is bounded by the Coulomb friction cone. Notice that if ß 
> 0 the last equation will force the friction force to lie on the boundary of the polyhedral friction 
cone. If ß = 0 the two last equations model static friction. That is no sliding can occur and any friction 
force inside the friction cone is feasible. Due to the positive span of ti one usually have several vti 
0 for sliding = motion. However, the model will pick only one .ti to be non-zero. The ti ­direction chosen 
by the model is the one mostly opposing the sliding direction. Only in the rare case where the sliding 
direction is symmetrically between ti ­directions the model may pick two positive .ti values. f T Observe 
that.ti = eT .t and we have v =vn vt . From the dis­ i cretization of the Newton Euler equations we have 
the contact velocity-impulse relation v = B. + b. The term b contains initial velocity terms hence v 
is the .nal velocity obtained by applying the impulse .. Using all this we can write the .nal matrix 
form of the contact model as, . . . . . . . . Bnn Bnt 0 .n bn .n 0 = .Btn Btt e...t .+ .bt . . ..t .= 
0 (27) µ -eT 0 ß  0  ß A x b x where Bnn = JnM-1JT , Bnt = BT = JnM-1JT , Btt = JtM-1JT , bn = n 
tn t t JnM-1F, and bt = JtM-1F. Here M is the generalized mass matrix, Jn and Jt are the normal and tangential 
parts of the contact Jacobian such that = -JT and ft = -JT , and F is a vector including external loads 
and fn n .n t .t gyroscopic forces. Observe that A is non-symmetric and has a zero-diagonal block. Further 
the subblock B is symmetric and positive-semi-de.nite (PSD) matrix. As will be clear from Section 5 this 
implies that we can not use PGS for the contact LCP model. Rather we can use either Lemke s method or 
a Newton­based method such as the one in Section 5.5. There exist an alternative complementarity problem 
formulation which drops the ß-part of the model by ignoring principle of maximum dissipation. The re­sulting 
model is no longer a LCP, but one could apply a splitting based PGS method for this alternative model 
[Erl07]. This is similar to the method we present in Section 5.2. The alternative model is physically 
.awed in the sense that friction directions are decoupled and no convergence guarantees can be given 
for the PGS-type method [SNE09, SNE10]. 4.2 The Fluid LCP Model Example Recently, LCPs are being used 
to model .uid-solid wall boundary conditions [BBB07, CM11]. This is a recent approach and the literature 
is still sparse on exam­ples. In physics-based animation most works use the incompressible Euler equa­tions 
[Bri08] . .u = - (u · ') u - 'p - f, (28a) . t ' · u = 0, (28b) where . is mass density, u is the velocity 
.eld, p is the pressure .eld and f is the external force density. Traditionally one applies the boundary 
conditions p = 0 on free surfaces between .uid and vacuum and u · n = 0 between the .uid and a static 
solid wall with unit outward normal n. For simplicity we here just present ideas for a single phase .ow 
in vacuum. The ideas trivially generalize to multiphase .ow and dynamic solid wall boundary conditions 
[BBB07, CM11]. In physics-based animation coarse grids are used to keep the computational cost down. 
This causes a problem with the traditional solid wall boundary condition u · n = 0. Namely that cell-size 
thick layers of .uid are getting stuck on walls. This appears visually unrealistic. Thus, it has been 
proposed to change the solid wall boundary condition to, 0 = p . u · n = 0. (29) This allows the .uid 
to separate from the wall. The condition u · n > 0 enforces p = 0 making the interface act like a free 
surface. On the other hand if u · n = 0 then the .uid is at rest at the wall and there must be a pressure 
p > 0 acting on the .uid to keep it at rest. The current trend is to spatially discretize the equations 
of motion on a stag­gered regular grid using .nite difference approximations of the spatial deriva­tives. 
For the temporal derivative one deals with the partial differential equa­tion using a fractional step 
method (known as operator splitting) [Sta99]. This means that in the last sub-step of the fractional 
step method one is solving, .t n+1 u = u' - 'p, (30a) . ' · u n+1 = 0, (30b) where un+1 is the .nal divergence 
free velocity of the .uid and u' is the .uid velocity obtained from the previous step in the fractional 
step method. The time-step is given by .t. Substituting the .rst equation into the second yields n+1 
' · u = ' · u' - .t '2 p = 0. (31) . Introducing the spatial discretization, we obtain the Poisson equation 
which we for notational convenience write as Ap + b = 0, (32) -.t '2 where p is the vector of all cell-centered 
pressure values and A = . and b = {' · u'}. The matrix A is a symmetric diagonal banded matrix. In 2D 
it will have 5 bands when using a 5-point stencil, in 3D it will have 7 bands for a 7-point stencil. 
For regular grids all off diagonal bands have the same value. Further, A is known to be a PSD matrix, 
but adding the boundary condition p = 0 ensures that a unique solution can be found. Once the pressure 
vector has been computed, it can be used to compute the last step of the fractional step method (30a). 
Let us revisit the complementarity problem arising from the modi.ed bound­ n+1 ary condition and examine 
what happens if u·n > 0 at a solid wall boundary. To start the analysis we will examine what happens 
with (28b) in an arbitrary small control volume V around a solid wall boundary point, n+1 ' · u n+1 
dV =u · n dS > 0. (33) V S n+1 The last inequality follows from the assumption that u· n > 0. This means 
that if we pick the row of the discrete Poisson equation that corresponds to the solid wall boundary 
point, we obtain (for the jth row) Aj*p + bj > 0. (34) n+1 If on the other hand u· n = 0 at the solid 
wall, then we rediscover Aj*p + bj = 0. Although we skipped all the details of the discretization it 
should be in­tuitively clear that the pressure solve for the new modi.ed boundary conditions is given 
by the LCP, 0 = p . Ap + b = 0. (35) Having motivated LCP models through two examples we may now turn 
our attention towards developing numerical methods for solving such LCP models.  5 The Numerical Methods 
We may now embark on the real issue at hand how to make robust, ef.cient and fast methods for solving 
the kind of LCPs we encounter in physics-based animation. 5.1 Pivoting Methods We will exploit the combinatorial 
nature of the LCP to outline a guessing ap­proach for .nding a solution. In principle if all possible 
guesses are tested then one obtains a naive direct enumeration method that will .nd all solutions. By 
algebraic manipulation on y = Ax + b, f I -A y = b. (36) x We de.ne the integer index set I = {1, . . 
. , n} of all indices. Next we use two index sets one of free variables yi > 0 and one of active variables 
yi = 0, F = {i | yi > 0} and A = {i | xi > 0} . (37) Without loss of generality and to keep our presentation 
simple we assume that strict complementarity holds which means we never simultaneously have yi = 0 and 
xi = 0. This means we can assume that F n A = Ø and F . A = I. The idea is to create a method that can 
verify if a guess of F and A is a solution for the given LCP. Now using the index sets we make the partitioning, 
f I*F -A*A yF = b. (38) xA C s where I*F and A*A are the sub matrices given by the column indices F and 
A. Our problem is now simpli.ed to verify if the linear programming (LP) problem Cs = b subject to s 
= 0 (39) has a solution1. This is the same as testing if b is in the positive cone of C. That is, b . 
{Cs | s = 0} . (40) Traditionally C is called a complementarity matrix. Observe that it is con­structed 
by picking columns from either A or I. Clearly we can make 2n differ­ent complementarity matrices. For 
low-dimensional LCPs this suggests a geo­metric approach to .nd a solution for a LCP. In the worst case 
the time complexity of guessing would be O(n32n) which is not computationally ef.cient. Another strategy 
is to be clever in making new guesses, for example, by applying a pivoting strategy that moves an in­dex 
from one set to the other or some strategy that builds up the index sets incrementally. Among direct 
methods for LCPs based on pivoting are the Lemke method (a Matlab implementation can be found from CPNET) 
and the Keller method [CPS92, Lac03]. The pivoting methods are capable of .nding an accurate solution 
to the LCP whereas the iterative methods we cover in Section 5.2-5.5 only .nd approxi­ mate solutions. 
The accuracy is at the expense of having to form the A-matrix in the .rst place whereas the iterative 
methods often can exploit a factorization of the A-matrix. Even if A is sparse the .ll in of the inverse 
matrix A-1 AA may be dense implying that the worst case storage complexity of pivoting methods is of 
O(n2) complexity. The pivoting methods have some similarity with active set methods for con­strained 
QP problems [NW99]. In the case of symmetric PSD A-matrices, one may consider restating the problem as 
a QP problem rather than implementing the pivoting method. This is especially bene.cial due to the availability 
of QP solvers such as MOSEK, CPLEX, LANCELOT, SQP, SNOPT and many more. 5.1.1 Incremental Pivoting Baraff 
Style In the .eld of computer graphics Baraff presented an incremental pivoting method [Bar94]. This 
method incrementally builds up the index sets while keeping the complementarity constraints as invariants. 
In each iteration the method computes A-1 Whenever A is a symmetric positive de.nite (PD) AA. A-1 exist. 
Baraff reported that even when A is PSD (which is often the case in AA practice due to redundant contact 
constraints), he was able to compute A-1 AA. Baraff proves that the inner pivoting loop of his method 
only can be performed a .nite number of times as the index set A is never repeated. Thus, the cost of 
the inner loop is at worst that of computing A-1 which is O(n3). The outer loop of AA 1We could more 
precisely have written s > 0 but s = 0 also covers the more general non-strict case. this method runs 
for at most n iterations yielding a pessimistic time complexity of O(n4). Noticing that the pivot step 
only swaps one index and therefore only changes the size of A by one it is clear that an incremental 
factorization method can be used for computing A-1 . There exist incremental factorization running AA 
in O(n2) time complexity. Thus, a more realistic overall time complexity for the pivoting method is O(n3). 
For completeness we outline the pivoting method here. In the kth iteration a new index will be selected 
from the current set of unprocessed indices, U = I \ {F . A}. The index sets F and A are initially both 
empty. Throughout, the complementarity conditions are kept as invariants. We will use superscript k to 
denote the values at a given iteration number. For any unprocessed index j . U k we implicitly assume 
xj = 0. Initially in the kth iteration we use the partitioning . . . . . . . . k k yA AA,A AA,F AA,j 
xA bA .y. = . . .x. + . .. k k AF,A AF,F AF,j bF k F F yAU,A AU,F AU,U 0 bU U The next candidate index 
to be processed in the method is selected as the index k j . U that minimize yj as this corresponds to 
an index in U with a most violated k complementarity constraint. If for the minimum value the condition 
yj = 0 is ful.lled, the method terminates as this would indicate that all the remaining unprocessed indices 
trivially ful.ll the complementarity conditions. If no unique feasible minimum exists, one may pick a 
minimizing index at random. In the kth iteration, we use the partitioning and keep the complementarity 
conditions k+1 k+1 as invariants implying y = 0 and x = 0, so A F . . . .. . . . 0 x k+1 AA,A AA,F AA,j 
A bA k+1 . . . .. . + . . y F = AF,A AF,F AF,j 0 bF . k+1 k+1 yj Aj,A Aj,F Aj,j xj bj k+1 The changes 
in yF and xA with respect to xj > 0 are given by k+1 k k+1 x = xA + .xAx , A j k+1 k k+1 y = yF + .yF 
x , F j k+1 k k+1 y = yj + .yj x j j where = -A-1.xA A,AAA,j , = -A-1 .xA A,AAA,j , .yj = -Aj,j - Aj,AA-1 
AA,j. A,A k+1 The idea is to increase xj as mush as possible without breaking any of the k+1 complementarity 
constraints. Thus, xj is limited by the blocking constraint set k -x q (42a) BA = q .A . .xq < 0 , .xq 
k -y r (42b) BF = r .F . .yr < 0 . .yr k+1 If no blocking constraints exist, xj is unbounded by A and 
F. Thus, each partition results in the bounds A 8 : BA = Ø xj = , (43a) min BA ; F 8 : BF = Ø xj = , 
(43b) min BF ; k-yj ; .yj < 0 j . (43c) .yj x = j 0 ; k+1 The solution for the value of x will be the 
minimum bound. If a blocking j constraint is found from BA, a pivot operation is initiated moving the 
blocking index from A to F and vice versa if a blocking constraint is found in BF . The blocking constraint 
sets are changed as the active and free index sets A and F are changed by a pivoting operation. This 
implies that one could increase k+1 xj further after a pivoting step. Thus, we will continue to look 
for blocking constraints and perform pivoting on them until no more blocking constraints k+1 exist. Depending 
on the .nal value of xj , index j is assigned to either F or A. 5.2 Splitting Methods k We introduce 
the splitting A = M - N. We let c= b - Nxk, and LCP (20) becomes Mxk+1 + c k = 0, (44a) k+1 = 0, x (44b) 
k+1)T (Mxk+1 (x + c k) = 0. (44c) This results in a .xed-point formulation where we hope that for a suitable 
choice of M and N the complementarity subproblem might be easier to solve than the original problem. 
Imagine for instance picking M as the diagonal of A. This choice decouples all variables and we have 
a problem of n independent 1D LCPs. Later we list other known choices for M. The splitting method can 
be summarized as Step 0 Initialization, set k = 0 and choose an arbitrary nonnegative x0 = 0. Step 1 
Given xk = 0 solve the LCP (44). Step 2 If xk+1 satisfy some stopping criteria then stop, otherwise set 
k . k + 1 and go to step 1. The splitting is often chosen such that M is a Q-matrix 2 . This means that 
M belongs to the matrix class of matrices where the corresponding LCP has a k k+1 solution for all vectors 
c. Clearly if xis a solution for (44) and we have k+1 x= xk then by substitution into the subproblem 
given by (44) we see that xk+1 is a solution of the original problem (20). Next we will use the minimum 
map reformulation on the complementarity subproblem that is equivalent to k+1 min(x , Mxk+1 + c k) = 
0. (45) Subtract xk+1 and multiply by minus one, k k+1 max(0, -Mxk+1 - c + x k+1) = x . (46) Again we 
re-discover a .xed-point formulation. Let us perform a case-by-case analysis of the ith component. If 
we play the mind game and assume k k+1 - Mxk+1 - c x < 0 (47) i k+1 then we must have xi = 0. Otherwise 
our assumption is false and we must have k+1 - Mxk+1 - c k k+1 x = x . (48) ii That is k (Mxk+1)i = ci 
. (49) For a suitable choice of M and back-substitution of ck = b - Nxk we have M-1 k+1 Nxk - b = x . 
(50) ii Combining it all we have derived the closed form solution for the complemen­tarity subproblem, 
M-1 k+1 max 0, Nxk - b = x . (51) Iterative schemes like these are often termed projection methods. The 
reason for this is that if we introduce the vector zk = M-1 Nxk - b then k+1 k x = max 0, z . (52) That 
is, the k + 1 iterate is obtained by projecting the vector zk onto the posi­tive octant. In a practical 
implementation one would rewrite the matrix equa­tion (52) into a for loop that sweeps over the vector 
components and updates the x-vector in place. 2Q-matrix means the LCP given by M and c has a solution 
for all values of c. One would want to use a clever splitting such that the inversion of M is computationally 
cheap. Letting L, D and U be the strict lower, diagonal and strict upper parts of A, then three popular 
choices are: the projected Jacobi method M = D and N = L + U, the projected Gauss Seidel (PGS) method 
M = (L+D) and N = U, and the projected Successive Over Relaxation (PSOR) method M = (D + .L) and N = 
((1 - .) D - .U) where 0 = . = 2 is the relaxation parameter. More about this parameter in Section 5.3. 
It is worthwhile to note that A must at least have nonzero diagonal for these splittings to work. As 
far as we know there exist no convergence proofs in the general case of A being arbitrary. However, given 
appropriate assumptions on A such as being a contraction mapping or symmetric, one can make proofs of 
global convergence [CPS92, Mur88]. In section 5.3 we take a different approach to deriving the same iterative 
schemes. Here it follows by construction that if A is symmetric and PSD then the splitting schemes will 
always converge. For non-symmetric matrices one may experience divergence. 5.3 PGS and PSOR from QPs 
In our second approach for deriving the PGS and PSOR iterative methods, we will make use of the QP reformulation. 
Our derivation follows in the footsteps of [Man84]. The reformulation allows us to prove convergence 
properties of the PGS and PSOR methods. We assume that A is symmetric and PSD. Then, the LCP can be restated 
as a minimization problem of a constrained convex QP problem x * = arg min f(x) (53) x=0 1 T b. i where 
f(x) = xT Ax + xGiven the ith unit axis vector e i where e j = 0 2 i for all j = i and e i = 1 then the 
ith relaxation step consists in solving the one-dimensional problem t * = arg min f(x + te i) (54) x=0 
i and then setting x . x + t e . One relaxation cycle consists of one sequential sweep over all ith components. 
The one-dimensional objective function can be rewritten as f(x + te i) = 1 (x + te i)T A(x + t e i) + 
(x + t e i)T b, 2 = 1 t 2Aii + t (Ax + b)i + f(x). 2 r We .nd the unconstrained minimizer tu = -Ari ii 
. Considering the constraint xi + t = 0 we .nd the constrained minimizer to be tc = max (tu, -xi) which 
yields the .nal update rule for the relaxation step xi . max 0, xi - ri . (56)Aii This is algebraically 
equivalent to the ith component in the PGS update (52). Consider the polynomial g(t) = 1 t 2Aii + t ri. 
We know Aii > 0 so the legs of 2 the polynomial are pointing upwards. The polynomial has one trivial 
root t = 0 y i and a minimum at t = - ri where g- ri = - r 2 < 0. The other root is Aii Aii Aii ri found 
at t = -2 Aii . Thus, any t value in the interval between the two roots has the property t. = -. ri . 
g(t.) < 0, .. . [0..2]. (57) Aii It follows that f(x + t.e i) = g(t.) + f(x) = f(x), .. . [0..2] (58) 
with equality if t. = 0. This results in the over relaxed version xi . max 0, xi - . ri . (59) Aii This 
is in fact algebraically equivalent to the ith component of the PSOR update and contains the PGS method 
as a special case of . = 1. Observe that by (58) we are guaranteed a non increasing sequence of iterates 
by our relaxation method. The complete iterative method can be listed as 1 : method PSOR(N , ., x, A, 
b) 2 : for k = 1 to N 3 : for all i 4 : ri . Ai*x + bi  y 5 : xi . max0, xi - . ri Aii 6 : next i 
 7 : next k 8 : end method  where N is the maximum number of allowed iterations and . is the relaxation 
parameter. 5.4 The Minimum Map Newton Method Using the minimum map reformulation we have the root search 
problem where H : Rn . Rn is given by, . . h(x1, y1) H(x) = . . . . . = 0. (60) h(xn, yn) Recall y = 
Ax + b so yi = Aiixi + bi + j=i Aij xj, thus # Hi(x) = h(yi, xi), (61a) .. . . = min ..Aiixi + bi + Aijxj 
. , xi.. (61b) j=i # The idea is to use a Newton method to solve the nonsmooth equation (60). To do that 
we need to generalize the concept of derivative [Pan90]. The non­ smooth function Hi(x) is a selection 
function of the af.ne functions, xi and (Ax + b)i. Further, each Hi is Lipschitz continuous and since 
each of the com­ponents ful.ll this requirement, then so does H(x) [Sch94]. De.nition 5.1 Consider any 
vector function F : Rn . Rn . If there exist a func­tion BF(x, .x) that is positive homogeneous in .x, 
that is, for any a = 0 BF(x, a.x) = aBF(x, .x), (62) such that the limit F(x + .x) - F(x) - BF(x, .x) 
lim = 0 (63) .x.0 I .x I exists, then we say that F is B-differentiable at x, and the function BF(x, 
·) is called the B -derivative. Notice that since H(x) is Lipschitz and directionally differentiable, 
then it is B-differentiable. The B-derivative BH(x, ·) is continuous, piecewise linear, and positive 
homogeneous. Observe that the B-derivative as a function of x is a set­valued mapping. We will use the 
B-derivative to calculate a descent direction for the merit function, .(x)= 1 H(x)T H(x). (64) 2 It is 
clear that a minimizer of (64) is a solution of equation (60). We use this B­ derivative to formulate 
a linear subproblem whose solution will always provide a descent trajectory to (64). In fact, the biggest 
computational task for solving the nonsmooth and nonlinear system (60) is the solution of a large linear 
system of equations. This is similar to what Billups does to solve his nonsmooth system in [Bil95] and 
we repeat this same technique in this work. The generalized Newton equation in the kth iteration is k 
H(x k) + BH(x , .x k) = 0. (65) Each Newton iteration is .nished by doing a correction of the previous 
iterate, k+1 k k x = x + tk.x (66) k where tk is called the step length and .xis the Newton direction. 
The fol­lowing theorems, see [Pan90, QS93, FK98, DLFK00], guarantee that .xk will always provide a descent 
direction for the the merit function .(x). Theorem 5.1 Let H : Rn . Rn be B-differentiable, and let . 
: Rn . R be de.ned by .(x)= 1 H(x)T H(x) (67) 2 Then . is B-differentiable and its directional derivative 
at xk in direction .xk is k k B.(x , .x k) = H(x k)T BH(x , .x k). (68) Moreover, if (65) holds then 
the directional derivative of . is k B.(x , .x k) = -H(x k)T H(x k) (69) Proof Since H is B-differentiable, 
(68) follows from the chain rule. Since k k B.(x, .xk) = H(xk)T BH(x, .xk), we have from the Newton equation 
(65) that B.(xk , .xk) = -H(xk)T H(xk). k Observe that a direct consequence of (69) is that any solution 
.xof the generalized Newton equation (65) will always provide a descent direction to the merit function 
.(xk). The following theorem shows that even if we solve (65) approximately, we can still generate a 
descent direction provided the residual is not too big. Theorem 5.2 Suppose we solve equation (65) approximately. 
That is, suppose that .xk satisfy the residue equation, k k r = H(x k) + BH(x , .x k), (70) k and de.ne 
the function .(x) as above. Then, .xwill always provide a descent direction for .(xk) provided k I H(x 
k) + BH(x , .x k) I= . I H(x k) I (71) for some prescribed positive tolerance . < 1. Proof In order to 
show that the vector .x provide a descent direction for k k .(x) we need to prove that B.(x, .x) < 0. 
Suppose .xsatisfy (70), then BH(xk , .xk) = rk - H(xk). We have that k k B.(x , .x k) = H(x k)T BH(x 
, .x k), (72a) = H(x k)T (r k - H(x k)), (72b) k = -H(x k)T H(x k) + H(x k)T r . (72c) k Now notice that 
B.(x, .xk) < 0 if and only if k)T k)T H(x k) k H(x r < H(x (73) k)T k or H(xr<I H(xk) I2. This implies 
that rk should lie inside a ball of radius at least I H(xk) I, in other words I rk I= . I H(xk) I for 
some . < 1. We will now present an ef.cient way of computing the B-derivative. Given the index i we have, 
yi if yi < xi Hi(x) = . (74) xi if yi = xi Recall y = Ax + b. All of these are af.ne functions and from 
[Sch94] we can .Hi compute the B-derivative BHij = .xj .xj as follows We de.ne two index sets corresponding 
to our choice of active selection func­tions, (1) If yi < xi then .Hi .xj = Aij. (75) (2) If yi = xi 
then .Hi .xj = 1 0 if j = i otherwise . (76) A = {i | yi < xi} and F = {i | yi = xi} . (77) Next we 
use a permutation of the indexes such that all variables with i . F are shifted to the end. Hereby we 
have created the imaginary partitioning of the B-derivative, k AAA AAF .xk A BH(x , .x k) = k . (78)0 
IF F .xF Notice this convenient block structure with AAA a principal submatrix of A. The matrix IF F 
is an identify matrix of the same dimension as the F. If we use the blocked partitioning of our B-derivative 
from (78) then the corresponding permuted version of the Newton equation (65) is k AAA AAF .xA HA(xk) 
= - . (79) k 0 IF F .xF HF (xk) Observe that this can be trivially reduced to k AAA.x = AAF HF - HA. 
(80) A k Our problem is reduced to a potentially smaller linear system in .x. Whether A an exact solution 
can be found for this reduced system depends on the matrix properties of the original matrix A. For physics-based 
animation this matrix could be symmetric PSD implying that the reduced matrix could inherit these properties 
and one might end up with a singular system to solve. The good news is that one does not need an accurate 
solution to guarantee a descent as we proved previously. In practice we have found GMRES to be suitable 
as a general purpose choice. Due to the connection of the generalized Newton method with the classical 
Newton method, global convergence is unlikely if we start with an arbitrary it­ 1 erate x. To remedy 
this we perform an Armijo type line search on our merit function .(·). The ideal choice for a step length 
tk would be a global mini­ k mizer for the scalar function .(t ) = .(xt ) where xt = x+ t.xk. In practice 
this could be expensive to compute, requiring too many evaluations of .(·) and possibly B.(·, ·). The 
Armijo condition stipulates that the reduction in .(t ) should be proportional to both the step length 
tk and the directional derivative k '.(0) = B.(x, .xk). For a suf.cient decrease parameter a . (0, 1) 
we state this as .(tk) = .(0) + at k'.(0). (81) To rule out unacceptably short steps we introduce a second 
condition, called the curvature condition, '.(t k) = s'.(0) (82) where the curvature parameter is selected 
such that s . (a, 1). Conditions (81) and (82) are known collectively as the Wolfe conditions. If the 
line search method chooses its candidate step length appropriately, by using a back-tracking approach, 
we can eliminate the extra condition (82). Now the Armijo condition implies to .nd the largest h . Z0 
such that .(tk) = .(0) + at k'.(0) (83) where tk = ßht 0 , t 0 = 1, and the step-reduction parameter 
a < ß < 1. In 1 Nocedal and Wright it reads that a is often quite small, say a = 10-4 and ß = 2 is often 
used [NW99]. In practice we have observed that the minimum map Newton method may converge to a local 
minimum corresponding to an infeasible iterate. To remedy this we apply a projected Armijo back-tracking 
line search. This means we k project the line search iterate xt = max(0, x+ t .xk) before computing the 
value of the merit function .(t ) = .(xt ). 1 : method projected-line-search k 2 : (.0, '.0) . (.(xk), 
B.(x, .xk)) 3 : t . 1 4 : while forever  k 5 : xt . max(0, x+ t.xk) 6 : .t . .(xt ) 7 : if .t = 
.0 + at '.0 then 8 : return t 9 : end if 10 : t . ßt  11 : end while 12 : end method  The back-tracking 
line search method we have outlined is general and could be used with any merit function. In rare cases 
one may experience that t becomes too small. Thus, it may be bene.cial to add an extra stop criterion 
after line 9 testing if t < d where 0 < d « 1 is some user-speci.ed tolerance. If the test passes one 
simply returns the current value of t as the tk value. Many have commented that globalizing the minimum 
map Newton method is dif.cult. As far as we know the projected back tracking line search has not been 
reported in the literature. Instead Levenberg-Marquard style search direc­tions are chosen with an occasional 
gradient descent direction [FK98, DLFK00]. We now combine all the ingredients of the minimum map Newton 
method into pseudo code. 1 : method minimum-map-Newton 2 : while forever 3 : yk . Axk + b k  4 : Hk 
. min(y, xk) 5 : A . {i | yi < xi}  6 : F . {i | yi = xi} 7: .xk . -Hk F F k - Hk 8 : solve AAA.x= 
AAF Hk A FA 9 : tk . projected-line-search(...) k+1 . xk k 10 : x + t k.x 11 : if converged then 12 
: return xk+1 13 : end 14 : k . k + 1 15 : end while 16 : end method  We will discuss possible stopping 
criteria later in Section 6. 5.5 The Fischer Newton Method We will introduce another Newton method based 
on [Fis92]. Many have inves­ tigated this formulation[FK98, KK98, DLFK00, CK00]. We use the reformulation 
. . F(x) = F(x, y) = .. f(x1, y1) . . . f(xn, yn) .. = 0. (84) This is a nonsmooth root search problem 
solved using a generalized Newton method. In an iterative fashion one solves the generalized Newton equation 
k J.x = -F(x k) (85) k for the Newton direction .x. Here J . .F(xk) is any member from the gen­ eralized 
Jacobian .F (xk). Then the Newton update yields x k+1 = x k + tk.x k (86) where t k is the step length 
of the kth iteration. De.nition 5.2 Given F and let D . Rn be the set of all x . Rn where F is continuously 
differentiable. Further, assume F is Lipschitz continuous at x then the B subdifferential of F at x is 
de.ned as .F(xk) .BF(x) = J .xk . D . lim = J . (87) xk.x .x De.nition 5.3 Clarke s generalized Jacobian 
of F at x is de.ned as the convex hull of the B subdifferential [Cla90], .F(x) = co (.BF(x)) . (88) Let 
us look at an example. Consider the Euclidean norm e : R2 . R de.ned as v e(z) =I z I= zT z (89) Then, 
for z . R2 \ {0} we have T . e(z) z . e(z) = .Be(z) = = .z = 0. (90) .z I z I For z = 0 we have T .Be(0) 
= {v | v . R2 . I v I= 1}, (91a) T . e(0) = {v | v . R2 . I v I= 1}. (91b) We now have most pieces to 
deal with the generalized Jacobian of the Fischer­ fT Burmeister function. For z = x y . R2 we may write 
the Fischer-Burmeister function as f(x, y) = f(z) = e(z) - g(z) (92) yfT where g(z) = 1 1 z. From this 
we .nd .g(z) .Bf(z) = .Be(z) - , (93a) .z . g(z) . f(z) = . e(z) - . (93b) .z Hence for z = 0, T zfT 
. f(z) = .Bf(z) = - 1 1 (94) I z I and fT T - .Bf(0) = {v 1 1 | v . R2 . I v I= 1}, (95a) fT .f(0) = 
{v T - 1 1 | v . R2 . I v I= 1}. (95b) Having studied the one-dimensional case we can now move on to 
the higher­dimensional case. Theorem 5.3 The Generalized Jacobian of the Fischer-Burmeister reformulation 
can be written as .F(x) = Dp(x) + Dq(x)A (96) where Dp(x) = diag (p1(x), . . . , pn(x)) and Dq(x) = diag 
(q1(x), . . . , qn(x)) are diagonal matrices. If yi = 0 or xi = 0 then xi - 1, (97a) pi(x) = i + y i 
yi 22 x - 1, (97b) qi(x) = i + yelse if yi = xi = 0 then pi(x) = ai - 1, (98a) qi(x) = bi - 1 (98b) fT 
for any ai, bi . R such that I ai bi I= 1 Proof Assume yi = 0 or xi = 0 then the differential is 2 2 
x i y 1 2 - d (xi + yi) (99) 2+ yi 2 i dFi(x, y) = d x By the chain rule 1 1 - xi + y 222 xi + y 2 i 
- dxi - dyi, (100a) 2 dFi(x, y) = d i 2 xidxi + yidyi - dxi - dyi, (100b) = 2 x 2 i + y i xi yi - 1 - 
1 .. ... ... dxi . (100c) 2 xi + y 2 i 2 xi + y 2 i = dyi pi(x) qi(x) Finally dy = Adx, so dyi = Ai*dx 
by substitution T dFi(x, y) = pi(x)e + qi(x)Ai* dx. (101) i .Fi(x) The case xi = yi = 0 follows from 
the previous examples. The next problem we are facing is how to solve the generalized Newton equa­tion. 
The issue is that we need to pick one element J from .F(xk). We have explored four strategies. Random 
One strategy may be to pick a random value for all ai and bi where xi = yi = 0. Zero Instead of random 
values one may pick ai = bi = 0. Perturbation Another strategy may be to perturb the problem slightly 
when­ever xi = yi = 0. For instance given a user-speci.ed tolerance 0 < e « 1 ' one could use x = e in-place 
of xi when evaluating the generalized Jaco­ i bian. Approximation One could exploit the cases where the 
Newton equation is solved with an iterative method such as preconditioned conjugate gra­dient (PCG) method 
or generalized minimum residual (GMRES) method. Then one only need to compute matrix vector products 
Jp for some given search direction vector p. By de.nition of directional derivative F(x + hp) - F(x) 
Jp = lim . (102) h.0+ h This means we can numerically approximate Jp using .nite differences. To globalize 
the Fischer Newton method we apply the same projected Armijo back-tracking line search explained previously. 
We rede.ne the natural merit function to be, .(x) = 1 F(x)T F(x). (103) 2 k Assuming we have a solution 
to J.x= -F(xk) then by the chain rule we .nd the directional derivative of the merit function to be, 
k k '.(x k)T .x = F(x)T J.x . (104) These are the modi.cations needed in the projected back-tracking 
line search method. Observe that an accurate solution for .xk will always result in a de­k scent direction 
as '.(xk)T .x= -2.(xk) < 0. De.ning the residual vector as k r= F(xk) + J.xk and following the same recipe 
from our previous proof in Section 5.4, one may show that if I r k I< . I F(x k) I (105) for some prescribed 
0 < . < 1 then .xk will be a descent direction for the merit function. Observe that this result is useful 
to determine a suf.cient stopping threshold for an iterative linear system method that will guarantee 
a descent direction. The pseudo code of the Fischer Newton method is, 1 : method Fischer-Newton 2 : 
while forever k = -Fk 3 : solve J.x 4 : t k . projected-line-search(...)  k+1 . xk k 5 : x+ t k.x 
 6 : if converged then 7 : return xk+1 8 : end 9 : k . k + 1  10 : end while 11 : end method  We 
will discuss possible stopping criteria later in Section 6.  6 Tips, Tricks and Implementation Hacks 
The Newton equations for both Newton methods can be solved using an iter­ative linear system method. 
We have successfully applied the two Krylov sub­space methods PCG or GMRES [Saa03]. GMRES is more general 
than PCG and can be used for any non-singular matrix whereas PCG requires the matrix to be symmetric 
PD. PCG can not be used for the full Newton equation in case of the minimum map reformulation. However, 
for the Shur reduced system it may be possible if the principal submatrix is symmetric PD. GMRES is more 
general purpose and one incurs an extra storage cost for this. The iterative linear system methods are 
an advantage in combination with the .nite difference approximation strategy that we introduced for the 
Fischer Newton method. The same trick could be applied for the minimum map Newton method. The advantage 
from this numerical approximation is an overall numer­ical method that does not have to assemble the 
global A-matrix. Instead it can work with this matrix in an implicit form or using some known factorization 
of the matrix. In particular for interactive rigid body dynamics this is an advantage as a global matrix-free 
method holds the possibility of linar computation time scaling rather than quadratic in the number of 
variables. Often the storage will only be linear for a factorization whereas the global matrix could 
be dense in the worst case and require quadratic storage complexity. For .uid problems one would often 
not assemble the global matrix but rather use the .nite difference stencils on the regular .uid grid 
to implicitly compute matrix-vector products. Thus, for .uid problems iterative linear system methods 
should be used. For the Newton methods one can in fact use PCG. In our implementation we use GMRES to 
keep our Newton methods more general. We have not experimented with preconditioners. It should be noted 
that PCG can deal with PSD matrix if for instance an incomplete Cholesky precon­ditioner is applied. 
GMRES would most likely bene.t from the same type of preconditioner as the PCG method. Newton methods 
can bene.t if one uses a good starting iterate. For the Newton methods that we have derived one would 
usually be content with using x = 0 as the starting iterate. For problem cases where PGS applies one 
can create a hybrid solution taking advantage of the robustness and low iteration cost of PGS to quickly 
within a few iterations compute a starting iterate for a Newton method. All the iterative methods we 
have introduced would at some point require stopping criteria to test if a method has converged or if 
some unrecoverable sit­uation has been encountered. To monitor this convergence process a numerical method 
would use a merit function. We already saw two de.nitions of merit functions for the Newton methods. 
For the PGS the QP reformulation may serve as the de.nition of the merit function. However, one may want 
to use the mod­i.ed merit function de.nition .(x) = x T |Ax + b| . (106) This has the bene.t that the 
merit function is bounded from below by zero and the drawback that it does not work for x = 0. It is 
often a good principle not to use only one stopping criterion but rather use a combination of them. For 
instance an absolute stopping criterion would be .(x k+1) < eabs (107) for some user-speci.ed tolerance 
0 < eabs « 1. In some cases convergence may be too slow. In these cases a relative convergence test is 
convenient, k+1) - .(x .(x k) < erel .(x k) (108) for some user-speci.ed tolerance 0 < erel « 1. A simple 
guard against the num­ber of iterations exceeding a prescribed maximum helps avoid in.nite looping. A 
stagnation test helps identifying numerical problems in the iterative values k+1 k max x - x < estg (109) 
i i i for some user-speci.ed tolerance 0 < estg « 1. This test usually only works numerically well for 
numbers close to one. A rescaled version may be better for large numbers. Besides the above stopping 
criteria one may verify numerical properties. For instance for the Newton type methods it can be helpful 
to verify that the Newton direction is a descent direction. Global convergence of the Newton methods 
often implies that they converge to an iterate with a zero gradient. This is not the same as having found 
a global minimizer. Thus, it may be insightful to test if the gradient of the merit function is close 
to zero and halt if this is the case. What should one do in case one does not converge to something meaning­ful? 
In off-line simulations one may have the luxury to restart a Newton method with a new starting iterate 
and hope that the bad behavior will be avoided. This may not be the best way to spend computing time 
or it may even be im­possible in an interactive simulator. One remedy is to fall back on using the gradient 
direction as the search direction whenever one fails to have a well de.ned Newton direction to work with. 
 7 Convergence, Performance and Robustness Ex­periments We have implemented all the numerical methods 
in Matlab (R2010a) [Erl11] and run our experiments on a MacBook with 2.4 GHz Intel Core 2 Duo, and 4 
GB RAM on Mac OSX 10.6.8. All tests took approximately 100 computing hours. To quickly generate a large 
number of test runs and automate our experi­ments the Matlab suite contains a small small suite of methods 
that can gen­erate synthetic fake .uid or contact LCPs with the properties we discussed in Section 4. 
Fig. 1 shows two examples of generated matrices for the LCPs. This (a) (b) Figure 1: Fill patterns of 
a .uid matrix (a) and a contact matrix (b). Observe that the matrices are extremly sparse and that the 
.uid matrix is symmetric whereas the contact matrix is non-symmetric. has the added bene.t of being able 
to quickly re-run experiments under varying parameters and offers a great deal of control. Our synthetic 
tests are no substitute for real world problems and only serve to demonstrate the inherent convergence 
properties. As a remark we note that all numerical methods have been tested on RPI s Matlab rigid body 
simula­tor [rpi] and compared against PATH [FM99]. Fischer Newton method rivals PATH robustness but scales 
better due to the iterative sub-solver. PATH seems slightly more robust due to its non-monotone line-search 
method. We examine the convergence rate of the iterative methods using a 1000­variable .uid problem and 
300-variable contact problem. The problem sizes are limited by the Matlab programming environment. For 
the PSOR and PGS methods we use the modi.ed merit function (106). The Fischer Newton and minimum map 
Newton method uses their natural merit functions. Our results are shown in Fig. 2. As expected we observe 
linear convergence rate for PSOR and PGS while Newton methods show quadratic convergence rate. Due to 
a non-singular Newton equation matrix the minimum map Newton method does not work on contact problems. 
It gives up after a few iterations where it gets into a non-descent iterate. This leaves only the Fischer 
Newton method as a real alternative for contact problems. For .uid problems the mini­mum map Newton method 
.nished in lower iterations. Finally we observe that PSOR using . = 1.4 converges faster than PGS. Next 
we will examine how the different Newton equation strategies for the Fischer Newton method affect the 
overall convergence behavior. For this we have generated a .uid LCP corresponding to 1000 variables and 
a contact LCP (a) (b) Figure 2: Convergence rates for a .uid (a) and a contact problem (b). For the 
.uid problem we observe high accuracy of the Newton methods within few iterations. For the contact problem 
only the Fischer Newton method works. with 180 variables. Our results are shown in Fig. 3. For contact 
problems we have observed that the perturbation strategy at times uses fewer iterations than the other 
strategies. For .uid problems all strategies appear similar in conver­gence behavior. The approximation 
strategy always result in a less accurate solution than the other strategies. We performed a parameter 
study of PSOR for a 1000 variable .uid problem. Here we plotted convergence rate for various relaxation 
parameter values as shown in Fig. 4. Our results show that a value of . = 1.4 seems to work best. For 
all iterative methods we have measured the wall clock time for 10 it­erations. The results of our performance 
measurements are shown in Fig. 5. Not surprisingly we observe the cubic scaling of the Lemke method. 
This will quickly make it intractable for large problems 3. In case of the .uid problem we found the 
Fischer Newton method to scale worse than linear this is unexpected as we use GMRES for solving the Newton 
equation. The explanation of the behavior is that in our implementation we are always assembling the 
Jacobian matrix J. We do this to add extra safe guards in our implementation against non descent iterates. 
If we apply dense matrices then the assembly of the Jacobian will scale quadratically if sparse matrices 
are used the assembly will scale in the number of non zeros. Theoretically, if one omitted the safe guards 
and used the approximation strategy then the Fischer Newton iteration should scale linear. We examined 
the robustness of our implemented methods. For this we have generated 100 .uid problems with 1000 variables 
and 100 contact problems with 180 variables. In all cases we used a relative tolerance of 10-6, an abso­lute 
tolerance of 10-3, and a maximum upper limit of 100 iterations. For each invocation we recorded the .nal 
state as being relative convergence, absolute 3A multilevel method [Erl11] is shown. Due to space considerations 
and poor behavior we have omitted all details about this method in these course notes. (a) (b) Figure 
3: Convergence rates of Fischer Newton method using different strate­gies. A .uid (a) and a contact (b) 
problem are displayed. Observe that the perturbation strategy works a little better for the contact case 
and that the ap­proximation strategy is less accurate. Figure 4: Parameter study of PSOR. Notice that 
a value of approximately 1.4 seems to be best. (a) (b) Figure 5: Performance measurements for .uid problems 
(a) and contact prob­ lems (b). Table 1: Robustness for 100 .uid and contact problems. (a) Final state 
on .uid problems (b) Final state on contact problems Relative Absolute Absolute Non-descent Fischer 0 
100 Fischer 100 0 Min. Map. 0 100 Min. Map. 0 100 PGS 0 100 PSOR 0 100 convergence, stagnation of iterates, 
local minimum iterate, non descent iterate, or max iteration limit reached. Table 1 displays our results. 
The minimum map Newton method does not work at all for contact problems. We observe that the rest of 
the methods are robust for this test setup. We investigated the number of iterations required for absolute 
and relative convergence. The results are shown in Table 2. We observe a low standard deviation for the 
Newton type methods. This suggest that one a priori can determine suitable maximum limits for these types 
of methods. PSOR is on average twice as fast as PGS. The iterations of PGS vary wildly. This implies 
that PGS does not have predictable performance if accurate solutions are wanted. Newton methods can bene.t 
from a good initial starting iterate. To illustrate the impact of this we have generated 100 dense PD 
problems with 100 variables. We solved the problems using a zero valued starting iterate and a starting 
iterate obtained from 10 iterations of PGS. In the tests we used a relative tolerance of 10-6, an absolute 
tolerance of 10-2, and a maximum upper limit of 30 iterations for the Newton methods. Fig. 6 summarizes 
our .ndings. To reach absolute convergence we observe that warm starting reduces the number of iterations 
to less than or equal to half of the number of iteration without warm starting. Considering the computational 
cost of PGS this is a Table 2: Statistics on number of iterations. (a) Absolute convergence contact problem 
Method Mean Min Max Std Fischer 14.54 9.00 26.00 2.79 (b) Absolute convergence .uid problem Method Mean 
Min Max Std Fischer 5.00 5.00 5.00 0.00 Min map 4.07 3.00 5.00 0.29 PGS 49.38 9.00 83.00 17.48 PSOR 25.29 
16.00 35.00 3.77 Figure 6: The number of iterations used to reach absolute convergence with (w) and 
without (wo) warm starting using PGS. good tradeoff. We have tested the numerical methods ability to 
deal with over determined systems. This is numerically equivalent to zero eigenvalues. For an increasing 
ratio of zero eigenvalues we have generated 100 dense PSD problems. For all cases we used a relative 
tolerance of 10-4, absolute tolerance of 10-2 and a maximum iteration bound of 100. In our initial trial 
runs we have observed that the approximate strategy works better for PSD problems whereas the other strategies 
behave similar to the minimum map Newton method. We therefore applied the approximation strategy for 
this experiment. Due to space consider­ations we omit showing detailed histograms of the .nal solver 
states. We observe that overall Fischer Newton method seems better at reaching relative convergence whereas 
the minimum map Newton is less successful. For large ratios of zero eigenvalues we clearly see that both 
Newton methods get into trouble. A high number of zero eigenvalues cause ill conditioning or even singularity 
of the Newton equations and result in a local minimum or a non descent iterate. In all test runs the 
PGS and PSOR methods ends up reaching their maximum iteration limit. This is a little misleading as it 
does not mean they end up with a bad iterate rather it implies they converge slowly. 8 Discussion and 
Future Work We have shown the LCP model for contact forces has a coef.cient matrix that is non-symmetric 
and has a zero-diagonal block. This limits the type of numerical methods that can be used to either pivoting 
methods like Lemke or Keller, or Newton methods like the Fischer Newton method. There exist general purpose 
Newton methods for LCPs such as PATH from CPNET [FM99]. PATH is a general solver and can solve nonlinear 
complementarity problems (NCPs) whereas the Newton-based methods we present are more lean and mean and 
tailored for the speci.c LCPs that is encountered in physics-based animation. For instance PATH scales 
quadratically in the number of unknowns as it requires the entire A-matrix of the LCP. The Fischer Newton 
method we have outlined can ex­ploit factorizations or numerically approximate the matrix-vector products 
and achieve better scaling than PATH. If one dislikes pivoting or Newton methods then one may apply the 
idea of staggering [KSJP08] to the LCP contact force model (27). In terms of our notation this would 
require us to solve the two coupled LCPs, 0 = Bnn.n + (bn + Bnt.t) . .n = 0, (110a) Btt e .t bt + Btn.n 
.t 0 = + . = 0. (110b) T -e0 ß µ.n ß Taking a staggered approach one solves the top-most LCP .rst (normal 
force problem) and then the bottom-most LCP second (the friction force problem) and continues iteratively 
until a .xed-point is reached. Observe that the normal force problem has a symmetric PSD coef.cient matrix 
Bnn making QP refor­mulations possible whereas the frictional problem has a non-symmetric matrix. One 
may exploit a QP reformulation anyway, because the friction LCP corre­sponds to the .rst-order optimality 
conditions of the QP problem T . * t = arg min 1 .T t Btt.t + ct .t (111) 2 subject to .t = 0 and cn 
- e T .t = 0, (112) where cn = µ.n and ct = bt + Btn.n. Thus, any convex QP method can be used to solve 
for the normal and friction forces and one is guaranteed to .nd a solution for each subproblem. Whether 
the sequence of sub QP problems converge to a .xed point is not obvious. All the methods we have covered 
could in principle be applicable for the .uid LCP problem. The splitting/QP-based PGS methods are straightforward 
to apply even in cases were one does not have the global matrix A. For the Newton type methods one could 
evaluate the .nite difference approximations directly on the regular grid to avoid building the A-matrix. 
Given the properties of the A-matrix these methods should be able to use PCG for solving the Newton equations. 
This could result in fast Newton methods with quadratic convergence rates and computing time that scales 
linear in the number of .uid grid nodes. Table 3 summarizes properties of the numerical methods we have 
covered in these notes. Observe that the column A-matrix properties is limited to the problem classes 
we used in our discussions. 36 Table 3: Numerical properties of numerical methods. The table shows worst-case 
complexities under the assumption that direct methods are used for linear systems. Convergence properties 
are not listed for pivoting methods as these give exact solutions in one single iteration. Time complexity 
refers to per iteration cost for the iterative methods and total cost for pivoting methods. A-Matrix 
Time Storage Convergence Global Method Type Properties Complexity Complexity Rate Convergence (splitting) 
PGS Iterative Nonzero diagonal O(n) O(n) Linear No (QP) PGS/PSOR Iterative Symmetric PSD O(n) O(n) Linear 
Yes Dantzig Pivoting Symmetric PD O(n4) O(n2) - - Lemke Pivoting P-matrix O(2n) O(n2) - - Minimum map 
Newton Iterative No assumptions O(n3) O(n2) Quadratic Yes Fischer Newton Iterative No assumptions O(n3) 
O(n2) Quadratic Yes There are other types of methods that we have not covered in these notes, such as 
the instance interior point (IP) methods, trust region methods and con­tinuation methods. One can develop 
any of these using the QP reformulation of the LCP. For LCPs with non-symmetric coef.cient matrices, 
the problems is not quite as straightforward. We leave these type of methods for future work. Pivoting 
and PGS methods are common-place today. The Newton meth­ods we have introduced do offer better convergence 
behavior than PGS-type methods and run faster than pivoting methods. We speculate that the iterative 
methods for the Newton equation should .t GPU implementation well and thus bene.t from hardware that 
is well suited for parallel matrix-vector products.  Acknowledgements Thanks to Jernej Barbi.c for proof-reading 
and constructive comments. Thanks to Sarah Niebe, Morten Silcowitz and Michael Andersen for working out 
many details in implementing and testing the numerical solvers. Thanks to Jeff Trinkle and his team for 
creating a MATLAB framework we could test our numerical solvers against. References [AFC+10] Jérémie 
Allard, François Faure, Hadrien Courtecuisse, Florent Falipou, Christian Duriez, and Paul G. Kry. Volume 
contact con­straints at arbitrary resolution. ACM Trans. Graph., 29:82:1 82:10, July 2010. [AO11] Iván 
Alduán and Miguel A. Otaduy. Sph granular .ow with fric­tion and cohesion. In Proc. of the ACM SIGGRAPH 
/ Eurographics Symposium on Computer Animation, 2011. [AP97] M. Anitescu and F. A. Potra. Formulating 
dynamic multi-rigid­body contact problems with friction as solvable linear comple­mentarity problems. 
Nonlinear Dynamics, 14:231 247, 1997. 10.1023/A:1008292328909. [AT08] Mihai Anitescu and Alessandro Tasora. 
An iterative approach for cone complementarity problems for nonsmooth dynamics. Compu­tational Optimization 
and Applications, Nov 2008. [Bar89] David Baraff. Analytical methods for dynamic simulation of non­penetrating 
rigid bodies. SIGGRAPH Comput. Graph., 23(3):223 232, 1989. [Bar93] David Baraff. Issues in computing 
contact forces for nonpenetrat­ing rigid bodies. Algorithmica. An International Journal in Com­puter 
Science, 10(2-4):292 352, 1993. Computational robotics: the geometric theory of manipulation, planning, 
and control. [Bar94] David Baraff. Fast contact force computation for nonpenetrating rigid bodies. In 
SIGGRAPH 94: Proceedings of the 21st annual con­ference on Computer graphics and interactive techniques, 
pages 23 34, New York, NY, USA, 1994. ACM. [Bar95] David Baraff. Interactive simulation of solid rigid 
bodies. Comput. Graph. Appl., 15(3):63 75, 1995. IEEE [BBB07] Christopher Batty, Florence Bertails, and 
Robert Bridson. A fast variational framework for accurate solid-.uid coupling. ACM Trans. Graph., 26, 
July 2007. [BDCDA11] Florence Bertails-Descoubes, Florent Cadoux, Gilles Daviet, and Vincent Acary. 
A nonsmooth newton solver for capturing exact coulomb friction in .ber assemblies. ACM Trans. Graph., 
30:6:1 6:14, February 2011. [BETC12] Jan Bender, Kenny Erleben, Jeff Trinkle, and Erwin Coumans. In­teractive 
Simulation of Rigid Body Dynamics in Computer Graph­ics. In Marie-Paule Cani and Fabio Ganovelli, editors, 
EG 2012 -State of the Art Reports, pages 95 134, Cagliari, Sardinia, Italy, 2012. Eurographics Association. 
[Bil95] Stephen Clyde Billups. Algorithms for complementarity problems and generalized equations. PhD 
thesis, University of Wisconsin at Madison, Madison, WI, USA, 1995. [Bri08] Robert Bridson. Fluid Simulation 
for Computer Graphics. A K Peters, 2008. [CA09] Hadrien Courtecuisse and Jérémie Allard. Parallel Dense 
Gauss-Seidel Algorithm on Many-Core Processors. In High Performance Computation Conference (HPCC). IEEE 
CS Press, jun 2009. [CK00] Bintong Chen and Xiaojun Chenand Christian Kanzow. A penal­ized .scher-burmeister 
ncp-function. Math. Program, 88:211 216, 2000. [Cla90] F.H. Clarke. Optimization and Nonsmooth Analysis. 
Industrial Mathematics, 1990. Society for [CM11] Nuttapong Chentanez and Matthias Müller. A multigrid 
.uid pres­sure solver handling separating solid boundary conditions. In Pro­ceedings of the 2011 ACM 
SIGGRAPH/Eurographics Symposium on Computer Animation, SCA 11, pages 83 90, New York, NY, USA, 2011. 
ACM. [CPS92] Richard Cottle, Jong-Shi Pang, and Richard E. Stone. The Linear Complementarity Problem. 
Computer Science and Scienti.c Com­puting. Academic Press, February 1992. [DBDB11] Gilles Daviet, Florence 
Bertails-Descoubes, and Laurence Boissieux. A hybrid iterative solver for robustly capturing coulomb 
friction in hair dynamics. ACM Trans. Graph., 30(6):139:1 139:12, December 2011. [DDKA06] Christian Duriez, 
Frederic Dubois, Abderrahmane Kheddar, and Claude Andriot. Realistic haptic rendering of interacting 
de­formable objects in virtual environments. IEEE Transactions on Visualization and Computer Graphics, 
12(1):36 47, January 2006. [DLFK00] Tecla De Luca, Francisco Facchinei, and Christian Kanzow. A theo­retical 
and numerical comparison of some semismooth algorithms for complementarity problems. Computational Optimization 
and Applications, 16:173 205, 2000. [Ebe10] David H. Eberly. Game Physics. Morgan Kaufmann, 2 edition, 
April 2010. [Erl07] Kenny Erleben. Velocity-based shock propagation for multi­body dynamics animation. 
ACM Transactions on Graphics (TOG), 26(2):12, 2007. [Erl11] Kenny Erleben. num4lcp. Published online 
at code.google.com/p/num4lcp/, October 2011. Open source project for numerical methods for linear complementarity 
problems in physics-based animation. [Fis92] A. Fischer. A special newton-type optimization method. Optimiza­tion, 
24(3-4):269 284, 1992. [FK98] Michael C. Ferris and Christian Kanzow. Complementarity and re­lated problems; 
a survey. Technical report, University of Wisconsin Madison, 1998. [FM99] Michael C. Ferris and Todd 
S. Munson. Interfaces to path 3.0: Design, implementation and usage. Comput. Optim. Appl., 12:207 227, 
January 1999. [GZO10] Jorge Gascón, Javier S. Zurdo, and Miguel A. Otaduy. Constraint­based simulation 
of adhesive contact. In Proc. of the ACM SIG-GRAPH / Eurographics Symposium on Computer Animation, 2010. 
[Hec04] Chris Hecker. Lemke s algorithm: The hammer in your math tool­box? Online slides from Game Developer 
Conference, accessed 2011, 2004. [Kip07] Peter Kipfer. LCP Algorithms for Collision Detection Using CUDA, 
chapter 33, pages 723 730. Number 3 in GPU Gems. Addison-Wesley, 2007. [KK98] Christian Kanzow and Helmut 
Kleinmichel. A new class of semis­mooth newton-type methods for nonlinear complementarity prob­lems. 
Comput. Optim. Appl., 11(3):227 251, December 1998. [KSJP08] Danny M. Kaufman, Shinjiro Sueda, Doug L. 
James, and Dinesh K. Pai. Staggered projections for frictional contact in multibody sys­tems. ACM Trans. 
Graph., 27(5), 2008. [Lac03] Claude Lacoursiere. Splitting methods for dry frictional contact problems 
in rigid multibody systems: Preliminary performance re­sults. In Mark Ollila, editor, The Annual SIGRAD 
Conference, num­ber 10 in Linkøping Electronic Conference Proceedings, November 2003. [Löt84] Per Lötstedt. 
Numerical simulation of time-dependent contact and friction problems in rigid body mechanics. 5(2):370 
393, 1984. [Man84] Jan Mandel. A multilevel iterative method for symmetric, posi­tive de.nite linear 
complementarity problems. Applied Mathemat­ics and Optimization, 11(1):77 95, February 1984. [Mur88] 
Katta G. Murty. Linear Complementarity, Linear and Nonlinear Pro­gramming. Helderman-Verlag, 1988. [NW99] 
Jorge Nocedal and Stephen J. Wright. Numerical optimization. Springer Series in Operations Research. 
Springer-Verlag, New York, 1999. [OGRG07] Miguel A. Otaduy, Daniel Germann, Stephane Redon, and Markus 
Gross. Adaptive deformations with fast tight bounds. In Proceed­ings of the 2007 ACM SIGGRAPH/Eurographics 
symposium on Com­puter animation, SCA 07, pages 181 190, Aire-la-Ville, Switzer­land, Switzerland, 2007. 
Eurographics Association. [OTSG09] Miguel A. Otaduy, Rasmus Tamstorf, Denis Steinemann, and Markus Gross. 
Implicit contact handling for deformable objects. Computer Graphics Forum (Proc. of Eurographics), 28(2), 
apr 2009. [Pan90] Jong-Shi Pang. Newton s method for b-differentiable equations. Math. Oper. Res., 15(2):311 
341, 1990. [QS93] Liqun Qi and Jie Sun. A nonsmooth version of newton s method. Math. Programming, 58(3):353 
367, 1993. [rpi] rpi-matlab-simulator. Published online at code.google.com/p/rpi­matlab-simulator/. [Saa03] 
Yousef Saad. Iterative Methods for Sparse Linear Systems, 2nd edi­tion. SIAM, Philadelpha, PA, 2003. 
[Sch94] Stefan Scholtes. Introduction to piecewise differential equations. Prepring No. 53, May 1994. 
[SKV+12] Breannan Smith, Danny M. Kaufman, Etienne Vouga, Rasmus Tam­storf, and Eitan Grinspun. Re.ections 
on simultaneous impact. ACM Trans. Graph., 31(4):106:1 106:12, July 2012. [SNE09] Morten Silcowitz, Sarah 
Niebe, and Kenny Erleben. Nonsmooth newton method for .scher function reformulation of contact force 
problems for interactive rigid body simulation. In Proceedings of Virtual Reality Interaction and Physical 
Simulation (VRIPHYS), November 2009. [SNE10] Morten Silcowitz, Sarah Niebe, and Kenny Erleben. A nonsmooth 
nonlinear conjugate gradient method for interactive contact force problems. The Visual Computer, 2010. 
 [ST96] David E. Stewart and Jeff C. Trinkle. An implicit time-stepping scheme for rigid body dynamics 
with inelastic collisions and coulomb friction. International Journal of Numerical Methods in Engineering, 
39(15):2673 2691, 1996. [Sta99] Jos Stam. Stable .uids. In Proceedings of the 26th annual conference 
on Computer graphics and interactive techniques, SIGGRAPH 99, pages 121 128, New York, NY, USA, 1999. 
ACM Press/Addison-Wesley Publishing Co. [TBV12] Richard Tonge, Feodor Benevolenski, and Andrey Voroshilov. 
Mass splitting for jitter-free parallel rigid body simulation. ACM Trans. Graph., 31(4):105:1 105:8, 
July 2012.  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2013</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>2504444</article_id>
		<sort_key>90</sort_key>
		<display_label>Article No.</display_label>
		<pages>7</pages>
		<display_no>9</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>9</seq_no>
		<title><![CDATA[Ray tracing is the future and ever will be...]]></title>
		<page_from>1</page_from>
		<page_to>7</page_to>
		<doi_number>10.1145/2504435.2504444</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2504444</url>
		<abstract>
			<par><![CDATA[<p>The primary objective of this course is to present a coherent summary of the state of the art in ray tracing technology. The course covers the most recent developments and practical aspects of the parallel construction of acceleration data structures and traversal of such acceleration data structures using highly parallel processors, including a discussion of divergent code paths and memory accesses as well as occupancy. Ray tracing in real-time games is considered one of the main application opportunities, but an important part of the course focuses on hardware for ray tracing applications in mobile platforms.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4193503</person_id>
				<author_profile_id><![CDATA[81504682709]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Alexander]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Keller]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[NVIDIA Research]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4193504</person_id>
				<author_profile_id><![CDATA[81456614249]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Tero]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Karras]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[NVIDIA Research]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4193505</person_id>
				<author_profile_id><![CDATA[81100041422]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Ingo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wald]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Intel Corporation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4193506</person_id>
				<author_profile_id><![CDATA[81100649025]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Timo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Aila]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[NVIDIA Research]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4193507</person_id>
				<author_profile_id><![CDATA[81100622664]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Samuli]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Laine]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[NVIDIA Research]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4193508</person_id>
				<author_profile_id><![CDATA[81416609214]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Jacco]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bikker]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[NHTV University of Applied Sciences Breda]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4193509</person_id>
				<author_profile_id><![CDATA[81100561107]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>7</seq_no>
				<first_name><![CDATA[Christiaan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gribble]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[SURVICE Engineering Company]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4193510</person_id>
				<author_profile_id><![CDATA[82458727657]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>8</seq_no>
				<first_name><![CDATA[Won-Jong]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lee]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Samsung Advanced Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4193511</person_id>
				<author_profile_id><![CDATA[82458845957]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>9</seq_no>
				<first_name><![CDATA[James]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[McCombe]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Imagination Technologies Limited]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Ray Tracing is the Future and ever will be... A. Keller (Organizer)* NVIDIA T. Karras NVIDIA I. Wald 
Intel S. Laine§ NVIDIA T. Aila¶ NVIDIA J. Bikkerl NHTV/IGAD Breda C. Gribble** SURVICE W.-J. Lee Samsung 
J. A. McCombe Imagination Technologies SIGGRAPH 2013 Course Abstract The phrase Ray tracing is the 
future and ever will be , as coined by David Kirk (NVIDIA fellow and former Chief Scientist), allows 
for two interpretations: Either, it will never be feasible, or it will be a disruptive technology. In 
order to show how close the state of the art is to the latter, presenters from industry will highlight 
the enabling technical aspects as well as the current challenges and opportunities of ray tracing. *e-mail:keller.alexander@gmail.com 
tkarras@nvidia.com ingo.wald@intel.com §slaine@nvidia.com ¶taila@nvidia.com Ibikker.j@gmail.com **christiaan.gribble@survice.com 
joe.w.lee@samsung.com james.mccombe@imgtec.com Figure 1: Although ray tracing primarily is associated 
with light transport simulation for photorealistic image synthesis (Image courtesy of Jeff Pat­ton), 
new technologies for parallel ray tracing and upcoming hardware have the potential to .nally democratize 
ray tracing as a disruptive tech­nology even in the mobile space. Contents 1 Course Objective 3 2 Course 
Syllabus 4 3 Course Presenter Information 5 3.1 Alexander Keller, NVIDIA (Organizer) . . . . . . . . 
. . . . 5 3.2 Tero Karras, NVIDIA . . . . . . . . . . . . . . . . . . . . . . 5 3.3 Ingo Wald, Intel 
. . . . . . . . . . . . . . . . . . . . . . . . 5 3.4 Samuli Laine, NVIDIA . . . . . . . . . . . . . 
. . . . . . . . 5 3.5 Timo Aila, NVIDIA . . . . . . . . . . . . . . . . . . . . . . . 6 3.6 Jacco Bikker, 
NHTV/IGAD Breda . . . . . . . . . . . . . . 6 3.7 Christiaan Gribble, SURVICE . . . . . . . . . . . . 
. . . . . 6 3.8 Won-Jong Lee, Samsung . . . . . . . . . . . . . . . . . . . 6 3.9 James A. McCombe, Imagination 
Technologies . . . . . 7  1 Course Objective The primary objective is to present a coherent state of 
the art in advanced ray tracing technology. Therefore, the course will cover the most recent developments 
and practical aspects of the parallel construction of hierarchical acceleration data structures and their 
traversal using highly parallel processors including the discussion of divergent code paths and memory 
accesses as well as occupancy. Ray tracing in real-time games is considered one of the main op­portunities, 
while an important part of the course is concerned with hardware for ray tracing reaching out for mobile 
platforms as an­other opportunity. Fig. 1 shows the result of a light transport simulation based on path 
tracing. While such simulations require ray tracing to assemble light transport paths, the course will 
raise intriguing questions such as: What about ray tracing primary rays instead of rasterizing the geom­etry 
and how feasible is it to trace shadow rays in order to overcome most of the issues of shadow maps ? 
Can ray tracing hardware be more power ef.cient than rasterization? 2 Course Syllabus The course documentation 
such as the presentations will be pro­vided at http://sites.google.com/site/raytracingcourse/. 2:00 PM 
Ray Tracing is the Future and ever will be... Alexander Keller will survey the basics of ray tracing, 
its com­plexity and numerical issues, as well as the algorithmic princi­ples of accelerated ray tracing. 
The course presenters will be introduced and it will be pointed out how their work is con­nected. 2:30 
PM Parallel Hierarchy Construction Tero Karras will present the latest and novel developments in the 
parallel construction of acceleration hierarchies for ray tracing that are amazingly fast. 2:50 PM Combining 
Single and Packet Ray Tracing Ingo Wald will review the traversal of acceleration hierarchies for single 
rays as well as for packets of rays, how these ap­ proaches can be combined, and how they can be ef.ciently 
implemented on processors with SIMD instructions. 3:10 PM Improving Coherence for Path Tracing Samuli 
Laine and Timo Aila will elaborate on the implementa­tion of path tracing algorithms on SIMD and SIMT 
architectures such that occupancy remains high and coherence can be ex­ ploited. This includes special 
considerations for complex shad­ing as required in professional applications. break 3:45 PM Ray Tracing 
in Real-time Games Jacco Bikker will elaborate on ray tracing under real-time con­ straints. New algorithms 
will be presented in the light of the op­portunities of ray tracing in games. 4:05 PM Integer Ray Tracing 
Christiaan Gribble reviews the basics of restricting ray tracing operations to integers. Aside from obvious 
savings in chip area as compared to .oating point ray tracing hardware, interesting precision issues 
are discussed. 4:25 PM MIMD Hardware Architecture for Incoherent Ray Tracing Won-Jong Lee presents a 
ray tracing hardware that is devel­oped at Samsung. 4:50 PM Low Power Consumption Ray Tracing James A. 
McCombe talks about the ray tracing hardware that is developed at Imagination Technologies. 3 Course 
Presenter Information 3.1 Alexander Keller, NVIDIA (Organizer) Alexander Keller is a senior research 
manager at NVIDIA and leads advanced rendering research. Before, he had been the Chief Sci­entist of 
mental images, where he had been responsible for research and the conception of future products and strategies 
including the design of the iray R ® renderer. Prior to industry, he worked as a full pro­fessor for 
computer graphics and scienti.c computing at Ulm Univer­sity, where he co-founded the UZWR (Ulmer Zentrum 
f ¨ur wissenschaft­liches Rechnen). Alexander Keller holds a Ph.D. in computer science, authored 25 granted 
patents, and published more than 50 papers mainly in the area of quasi-Monte Carlo methods and photorealistic 
image synthesis using ray tracing. 3.2 Tero Karras, NVIDIA Tero Karras is a senior research scientist 
at NVIDIA. His expertise ran­ges from parallel architectures and algorithms to ef.cient image syn­thesis 
and light transport. In the past, he has also worked on mobile graphics hardware and software at Hybrid 
Graphics. His current re­search interests include GPU-based methods for ef.cient global illu­mination, 
as well as algorithms tailored for massively parallel proces­sors in general. 3.3 Ingo Wald, Intel Ingo 
Wald holds a Ph.D. in engineering from Saarland University, and is currently a research scientist at 
Intel Labs. After his Ph.D., Dr. Wald was a post-doctoral research associate at the Max Planck Institute 
for Informatics in Saarbr ¨ucken, Germany, followed by a Research Professorship at the Scienti.c Computing 
and Imaging Institute (SCI) and School of Computing at the University of Utah. His work con­centrates 
on all aspects of real-time ray tracing and photorealistic rendering, high-performance graphics, throughput 
computing, pa­rallel/high-performance hardware architectures, and, most recently, on programming paradigms 
and compilers for multi-core wide-SIMD compute architectures. In addition to having authored a large 
num­ber of ray tracing related papers and code bases Dr. Wald is also the main author of the IVL SPMD 
Program Compiler for Intel Architec­tures as well as one of the leading contributors to the open-source 
Intel SPMD Program Compiler (ISPC) and upcoming Embree 2.0 ray tracing engines. 3.4 Samuli Laine, NVIDIA 
Samuli Laine is a Principal Research Scientist and Distinguished Inven­tor at NVIDIA. He received M.Sc. 
and Ph.D. degrees in Information Technology from Helsinki University of Technology, Finland, in 2006. 
Samuli s research interests include algorithm and architecture de­sign for GPUs, ray tracing, stochastic 
rendering, and ef.cient recon­struction of high-quality images from sparse data. He holds graphics technology 
patents in US, Europe and Asia, and has co-authored nu­merous conference and journal papers, technical 
reports, and book chapters. 3.5 Timo Aila, NVIDIA Timo Aila is a principal research scientist at NVIDIA. 
His expertise ran­ges from real-time rendering in computer games (eg. Max Payne, third-party engine development 
for numerous games, the .rst com­mercial occlusion culling library dPVS) to hardware architectures, and 
recently also to high-quality image synthesis with contributions to the PantaRay rendering system used 
in Avatar and Tintin. He also gained expertise in mobile graphics as the chief scientist of Hy­brid Graphics. 
Timo has co-authored over 50 articles, patents, and patent applications in the .eld of graphics technologies. 
 3.6 Jacco Bikker, NHTV/IGAD Breda Jacco Bikker is Associate Professor for the Academy of Digital Enter­tainment 
of the NHTV University of Applied Sciences in Breda, The Netherlands, and works as an consultant on cloud-based 
physically­based rendering for OTOY Inc. He received his doctorate degree from the Technical University 
in Delft. Jacco worked for ten years in the Dutch game industry, and is the author of the Arauna real­time 
ray tracer and the Brigade real-time path tracer. Arauna and Brigade have been designed speci.cally with 
games in mind. 3.7 Christiaan Gribble, SURVICE Christiaan Gribble is a Research Scientist in the Applied 
Technology Operation of SURVICE Engineering Company. His work explores the synthesis of interactive visualization 
and high-performance comput­ing, focusing on algorithms, architectures, and systems for predictive rendering 
and physics-based simulation. Prior to joining SURVICE in 2012, Gribble served as an Associate Professor 
of Computer Science at Grove City College. He holds a Ph.D. in computer science from the University of 
Utah, and has previously served as an Assistant Pro­fessor at Grove City, as a post-doctoral fellow and 
research assistant at the Scienti.c Computing and Imaging (SCI) Institute, and as a re­search assistant 
at the Pittsburgh Supercomputing Center. 3.8 Won-Jong Lee, Samsung Won-Jong Lee is a senior researcher 
of SAIT (Samsung Advanced In­stitute of Technology) and leads a project developing a mobile GPU based 
on ray tracing. In SAIT, he has been responsible for various ar­eas involving mobile GPUs: architecture 
modeling/exploration, simu­lation framework, and scheduling algorithms for many-core systems. Before 
joining SAIT, he has researched graphics architecture, visual­ization algorithms, and parallel rendering 
systems in Yonsei Univer­sity and AIST (National Institute of Advanced Industrial Science and Technology). 
He received Ph.D. degree in computer science from Yonsei University in Korea, wrote more than 20 patents, 
and pub­lished more than 30 papers regarding volume visualization and GPU architecture. 3.9 James A. 
McCombe, Imagination Technolo­gies James A. McCombe is an engineer and entrepreneur in the .eld of parallel 
computing, hardware architecture and computer graph­ics. James currently works at Imagination Technologies, 
a UK based company which, among other technologies, develops the PowerVR GPU which is integrated into 
many of the mobile phones and tablets on the market today. James is currently working to augment the 
PowerVR GPU architecture to support ray tracing in constrained, low­power environments with the intention 
of bringing ray tracing to the broadest possible range of devices. Previously, he was the Founder and 
Chief Technical Of.cer of Caustic Graphics where he devel­oped methods for improving computational ef.ciency 
in many as­pects of a ray tracing system. Imagination Technologies acquired Caustic Graphics in 2010. 
Prior to this, James was a lead engineer of the mobile and desktop OpenGL graphics system at Apple Com­puter, 
contributing to the evolution of programmable shading on GPUs. James A. McCombe holds more than 11 granted 
patents in the areas of parallel hardware scheduling, numeric representation and spatial data structures. 
 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2013</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>2504445</article_id>
		<sort_key>100</sort_key>
		<display_label>Article No.</display_label>
		<pages>11</pages>
		<display_no>10</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>10</seq_no>
		<title><![CDATA[Story]]></title>
		<subtitle><![CDATA[it's not just for writers... anymore]]></subtitle>
		<page_from>1</page_from>
		<page_to>11</page_to>
		<doi_number>10.1145/2504435.2504445</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2504445</url>
		<abstract>
			<par><![CDATA[<p>When studios say "it is about the story!" everyone nods in agreement. But story creation remains a mystery for many in computer animation, VFX, and games because they have not focused on screenwriting. This course covers the universal elements of story: plot, characters, and distinctive narrative structure. It analyzes conflict (internal, external, environmental), turning points, cause and effect, archetypes vs. stereotypes, inciting incidents, and how choice defines character. In also reviews the questions raised in all stories:</p> <p>&#8226; What is at stake (survival, safety, love, esteem, etc.)?</p> <p>&#8226; What is the motivation (inciting incident) of the main character (protagonist)?</p> <p>&#8226; Will that be enough to move the main character from ordinary, comfortable life to a different world (where the action takes place)?</p> <p>&#8226; What "changes" are necessary to make the story dramatic?</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4193512</person_id>
				<author_profile_id><![CDATA[82459316257]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Craig]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Caldwell]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Utah]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[{Campbell1972} Campbell J., <i>Hero with a Thousand Faces</i>, Princeton University Press, 1972.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[{Glebas2008} Glebas, F., <i>Directing the Story</i>, Focal Press, 2008.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[{Jung1981} Jung, C., <i>The Archetypes and The Collective Unconscious</i>, Princeton University Press, 1981]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[{McKee1997} McKee, R., <i>Story: Substance, Structure, Style and The Principles of Screenwriting</i>, Regan Books, 1997]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[{Seger2010} Seger, L., <i>Making a Good Script Great</i>, Silman-James Press, 2010]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[{Synder2005} Synder, B., <i>Save The Cat!</i>, Michael Weise Books, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1628659</ref_obj_id>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[{Sullivan2008} Sullivan, K., Alexander, K., Schumer, G., <i>Ideas for the Animated Short: Finding and Building Stories</i>, Focal Press, 2008.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[{Vogler 2012} http://en.wikipedia.org/wiki/The_Writer's_Journey:_Mythic_Structure]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[{Watts2010} Watts, N., <i>Write a Novel and Get It Published</i>, McGraw-Hill, 2010.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Story: It s not just for writers anymore SIGGRAPH 2013 Course Notes Monday, 22 July, 2013 Organizer 
&#38; Lecturer Craig Caldwell University of Utah, Salt Lake City USA  Abstract This course has been 
designed for technical directors, artists, animators, modelers, programmers, and designers whose work 
is essential in making the story come to life. This information can be particularly useful when communicating 
with screenwriters, directors, producers, and supervisors. This course answers the question what is story? 
(and you don t even have to take a course in screenwriting). This course uses numerous clips to demonstrate 
how story has been used in feature films, animation and VFX. The purpose is to take the mystery out of 
what is story for those programmers, artists, and game designers whose work is essential in making Animation, 
VFX, and Games successful. The attendees will know the basic elements of story, so the next time a producer 
or director talk about what they want for the story, they will know what specific story benchmarks the 
producer/director are trying to meet in connecting emotionally with an audience. This course will build 
from the knowledge that story is a sequence of events (acts) that builds to a climax . and then lays 
out the universal elements of story that make up plot, character development, and narrative structure. 
This course emphasizes story elements in context (i.e. theme, character, setting, conflict etc.) and 
their relationship to classic story structure (i.e. setup, inciting incident, rising action, climax, 
resolution etc.). It analyzes conflict (i.e. internal, external, environmental), turning points, cause 
&#38; effect, archetype vs. stereotypes, inciting incident, and how choice defines character. In all 
stories there must be questions raised: What is at stake (i.e. survival, safety, love, esteem, etc.)? 
What is going to motivate (inciting incident) the main character (protagonist)? Will that be enough to 
move characters from the ordinary (where they are comfortable) to go out into a different world (where 
the action takes place)?, and How will the character "change"(necessary for all dramatic stories)? These 
are just a few of the storytelling elements necessary to structure a solid story. This course is for 
all whose work makes the story come alive but their job isn t creating the story. About the Speaker 
Craig Caldwell, USTAR (Utah Science Technology and Research) Professor, Digital Media Cluster, University 
of Utah and DeTao Master Academy, Institute of Animation and Creative Content, located at the Shanghai 
Institute of Visual Arts, Shanghai, China. craig.caldwell@utah.edu Industry experience: 3D Technology 
Specialist, Walt Disney Feature Animation in Burbank, CA and Creative Training at Electronic Arts, Tiburon 
Studio. Academic background includes Head of the largest Film School in Australia at Griffith University. 
The Griffith Film School is known for its interdisciplinary program in Film, Animation, and Games. Previously 
was Head of the Media Arts Department, University of Arizona and Associate Director of the Triestman 
New Media Center. Currently Director of Arts Track, Master Games Studio (ranked #2 by Princeton Review 
in 2013 and our undergraduate is ranked #1), University of Utah. Recent conference presentations such 
as FMX 12 &#38; 13, SIGGRAPH ASIA 12, Pixel 7 12, and Mundos Digitales 12 &#38; 13. Course Content 1. 
Introduction -What is story? Story Structure incorporates both plot and story elements. Plot is a sequence 
of events with a Beginning, Middle, End composed of basic steps that include Setup, Inciting Incident, 
Rising Action, Climax, and Resolution (e.g. Hero s Journey [12-point story structure], Nigel Watts [8-point 
story structure])... Setup involves Image and Exposition, the Inciting Incident incorporates issues of 
Survival, Safety, Love etc., and in the end stories must have satisfying a Climax and Resolution. 1.1 
Turning Points. Turning Points drive the Rising Action. Characters must make choices that propel the 
action forward in the story. 1.2 Character Arc. Characters have a distinct point of view, have something 
at stake, and must undergo a change through an observable character arc. 1.3 Audience. Audiences come 
to stories with expectations. They have an innate sense of story genres. Their interest is maintained 
through narrative questions and gaps between their expectations and knowledge. 2. Story Elements includes 
Character, Setting, Theme, Goal, and Conflict. Characters action must reflect plausible cause &#38; effect, 
match setting with story line, theme will be discovered at the end, and conflict types; internal, external, 
or environment. The question at the beginning of every story that must be answered is what does the character 
want? 3. Character, Archetype versus Stereotype. Characters in stories have specific dramatic roles 
that fall under the banner of either Archetype or Stereotype. 3.1 Classic Archetypes 4. Conclusion 
 Story Structure checklist 5. References  1. What is Story? It is first, a sequence of events. But 
there is more to it than that. A (dramatic) story is character s reactions (choices)  to a sequence 
of connected events  that build to a climax .  that results in change .  Stories are primarily plot 
driven or character driven. The plot is the sequential ordering of connected events and incidents (cause-and-effect) 
that sustains momentum. Of the three basic elements of a movie s story (plot, theme, character), plot 
is not what the audience generally remembers it is the situation and characters (sometimes the theme). 
The theme is based on the values that are fundamental to the either the character or story. What is 
plot? Plot is the choice of events and their connections in sequence in a story; composed through choices 
in what to include?, what to exclude?, what comes before or after?, and what are it s turning points? 
These choices are the plot. Plot is all the action that takes place during the story. It is important 
to remember that all stories have a beginning, middle, and end; though, today, they may not necessarily 
be in that order, Most plots have a Act 1, Act 2 and Act 3 loosely corresponding to a beginning (departure), 
middle (initiation), and end (return). There are other act structures; Situation Comedy has 2 acts (e.g. 
Friends, Big Bang, etc.), Dramatic Series has 4 acts (e.g. Law and Order) and Shakespeare has 5 acts. 
Plot comes in a variety of structures for different stories and lengths. The most well known is Hero 
s Journey by Joseph Campbell used in feature film (e.g. Star Wars, Croods). It is a familiar structure 
that Campbell derived from common mythological narratives found in cultures around the world. Myths follow 
this basic hero s journey structure: 1. Heroes are introduced in the ORDINARY WORLD 2. They receive 
the CALL TO ADVENTURE 3. They are RELUCTANT at first or REFUSE THE CALL, but 4. Are encouraged by a 
MENTOR to 5. CROSS THE THRESHOLD and enter the Special World, where 6. They encounter TESTS, ALLIES, 
AND ENEMIES. Figure 1: Plot and Story Elements 7. They APPROACH THE IN-MOST CAVE, cross a second threshold 
 8. Where they endure the ORDEAL 9. They take possession of their REWARD and 10. Are pursued on THE 
ROAD BACK to the Ordinary World. 11. They cross the third threshold, experience a RESURRECTION, and 
are transformed by the experience.   12.They RETURN WITH THE ELIXIR, a boon or treasure to benefit 
the ORDINARY WORLD. [Vogler2012] Chris Vogler, in his book The Writer's Journey: Mythic Structure For 
Writers clarified this Hero s Journey structure for Hollywood consumption. In all stories do they all 
have these steps? No, but all stories have some of them. Audiences today are saturated, jaded with the 
hero s journey; but they still want it but in a way they don t expect. Thus writers must decide which 
set of steps not only work for them but work for the story they are creating. Vogler points out that 
the Hero s Journey also has an Inner Journey that encompasses the fear that prevents most people from 
pursuing their dreams, which is fundamental for the audience to relate to the characters. In Ideas for 
the Animated Short, the author Karen Sullivan has found that another set of steps works best in short 
animations: Character wants something, Something happens to start the action, Conflict, Things gets worse, 
Almost all is lost, Lesson is learned that changes things, A choice is made, and Success. Others have 
taken this basic monomyth (Hero s Journey) and arrived at a different set of steps. Nigel Watts in Write 
a Novel condensed the steps on Everyday Life, Trigger, Quest, Surprise, Critical Choice, Reversal, Climax, 
and Resolution.  In any story there must be an Inciting Incident. This is an unexpected event, a catalyst 
that begins the story action. There are 3 types of Inciting Incident: Action, a murder, someone see, 
something happens; Character receives information, this orients the audience to the story theme; and/or 
Situational, incidents that orient us (this takes longer). Inciting Incidents often coincide with opening 
images that quickly engage the audience. Inciting Incidents lay out the big narrative question: what 
is the goal? and will it be achieved? These are key for maintaining the engagement of the audience. 
The most common type of narrative question is the goal question: Will the characters achieve their goal? 
Most narrative questions are variations of this type. Question-asking can be a powerful persuasive device 
because questions structure our decision-making process. They do this by directing our thoughts about 
the issues at hand and by implicitly specifying the range of possible answers. [Glebas2008] The narrative 
is how the story is told through narrative questions. This is why we can start watching in the middle 
of a television show and have an idea what is happening. Narrative questions provide the structure that 
makes this possible. 1.1 Turning Points Between the Departure and Return is the Initiation phase of the 
story. This is where the major turning points are located. These include Change of Plans,  Point of 
No Return,  Major Setback (Figure 6). The two built-in turning points are Inciting Incident and Climax. 
 Figure 5: Turning Points 1.2 Character Arc Often the specific dramatic questions in the plot center 
around the fundamental question: Will the character grow and develop enough in order to solve her challenges? 
The change is gradual and that change is revealed through the character s reaction during the turning 
points. A character's development doesn t come easily, they don t choose the challenges, and don t choose 
to have their life change. The character doesn t even have the choice to give up (maybe temporarily) 
or you don t have a story. The challenges that result in change must in someway fit the situation (even 
the "fish out of water" is a fit of a kind). The problem must relate to the character and the situation, 
with all attempts to reach their goal and reveal the theme relating to one another 1.3 Audience A good 
story is built from the motivation to touch the audience. Poor stories are constructed from the motivation 
to prove something, express themselves, or impress. There is an unspoken covenant between a director/writer 
and their audience. The audience agrees to give the story a chance if the director promises not to bore, 
manipulate, and use the audience for their own ends. An audience is not only smart; it is collectively 
smarter than any one person. No film can be made without an understanding of the reactions and anticipations 
of the audience. The story is shaped to both realize the narrative idea and satisfy the audience or 
player s expectations. Without the audience what is the point of everything that is created for the film 
or game? The audience comes with distinct expectations of the story that can be described in terms of 
genres or types. Robert McKee has complied a list of Genres that includes Adventure, Comedy, Coming of 
Age, Crime etc. [McKee1997] while Blake Snyder approached it from a list of Types such as Monster in 
the House, Golden Fleece, Out of the Bottle etc. [Synder2005] Each is valid. A storyteller must choose 
which approach works best for them. 1.4 Identification Identification with the character is fundamental 
if the audience is going to relate to the character and become engaged in the story. There are a number 
of different ways to identify but some of the classics are: 1. Something unfair happens to the character. 
 2. The character is in jeopardy (physical danger, loss of job, loss of love etc.) 3. Make the character 
likeable: the character follows through on commitments, character shows courage, and character is kind. 
 4. Characters that are funny. 5. Characters are strong (good at what they do).  Recently there are 
two very different examples. In OZ, the Great and Powerful the film struggled because the main character 
had none of the classic identification traits. There were reviews that indicated the film was going for 
anti-hero to bring something different to the story but did it work for the audience? In Man of Steel 
the main character has many of the traits audience relate to for identification. Identification is a 
psychological process where audiences experience the same emotions and feelings as the characters. Audiences 
empathize with emotion. It is important as the character is created to include flaws that that not only 
make them unique but ones we can relate to in the story. 2. Story Elements Elements of the story are 
remembered long after the plot is forgotten. Characters, who is the story is about; thought whose eyes 
the story is told; The World, where are the characters located, and the type of environment; Theme, what 
is the story about, courage, love, redemption etc.; Goal, what does the character want; and Conflict, 
what is between the character and the goal. 3. Character: Archetypes versus Stereotypes No matter where 
a story is made in the world, if it works it has an archetypal quality. An archetypal story is a universal 
experience (e.g. myth) wrapped inside a unique, culture­specific expression. Story is archetypes, not 
stereotypes [McKee1997]. The stereotypical story reverses this pattern: confined to a cliché, culture-specific 
experience dressed in generalities. A stereotype example would be the 19th century Spanish custom that 
once dictated that daughters must be married off in order from oldest to youngest. Inside family would 
be a strict patriarch, a powerless mother, an unmarriageable oldest daughter, and a long-suffering daughter. 
While this historical practice is attention­grabbing it is not likely to generate empathy from anyone 
outside this cultural experience. [Glebas2008]. However if the story uses an archetypical scenario as 
a platform (i.e. Like Water for Chocolate) where mother and daughter clash over the demands of dependence 
and independence, permanence versus change, and self versus others then here is a conflict that we are 
familiar and can relate. (Figure 11).  A stereotype is a cliché character that has been so overused 
that it has lost originality. Such characters in animation and VFX today would include robots, aliens, 
mimes, ninjas, fairies, dragons, pirates, cleavage with guns, superheroes and even a representative family. 
However stereotypes are very useful if we exploit the archetypal quality as one is played off another. 
3.1 Archetypes Archetypes -are universal character types in all cultures [Jung1981].  Hero brave, self-sacrificing, 
moral  Father figure wise, mentor  Mother figure nurturing, intuitive.  Shadow figure can help 
hero, can oppose hero, negative side of hero, may range to evil.  Animal archetypes positive or negative 
with traits, consistence with animal types.  Trickster causing chaos, selfish, uses wit and cunning, 
range is harmless to evil.   Myths whatever our culture, we share similar experiences and journeys 
in life; underneath it all it is the same story. The trappings may be different but underneath it, it's 
the same story, drawn from our same experiences. These search and hero stories are myths, the stories 
common to all cultures [Seger2010]. It is a true story because someone has experienced it and it is true 
because we have lived it at some level. Thus it connects and speaks to all of us. Many stories are combinations 
of myths (Tootsie is Shakespearean in that he dresses up to accomplish a task).  4 Conclusion -Story 
Structure Plot Character Elements . Act 1 -Setup . Character Opening Image Archetype/Stereotype  
Inciting Incident . World  Ordinary World versus . Theme  Different World  What is at Stake . Act 
2 -Rising Action must feel like Life and Death Turning Points . Goal what character wants Change of 
Plans . Conflict  Point of No Return . Character Arc  Major Setback Must be a Change . Act 3 -Climax 
 Must make a Choice Descending Action . Narrative Questions  Resolution . Audience Expectations  Genres 
Story Structure . Hero s Journey (12 points) . Nigel Watts (8 points) . Ideas for Animated Short (8 
points) . Beginning . Middle . End But not necessarily in that order.  5 References: [Campbell1972] 
Campbell J., Hero with a Thousand Faces, Princeton University Press, 1972. [Glebas2008] Glebas, F., Directing 
the Story, Focal Press, 2008. [Jung1981] Jung, C., The Archetypes and The Collective Unconscious, Princeton 
University Press, 1981 [McKee1997] McKee, R., Story: Substance, Structure, Style and The Principles of 
Screenwriting, Regan Books, 1997 [Seger2010] Seger, L., Making a Good Script Great, Silman-James Press, 
2010 [Synder2005] Synder, B., Save The Cat!, Michael Weise Books, 2005. [Sullivan2008] Sullivan, K., 
Alexander, K., Schumer, G., Ideas for the Animated Short: Finding and Building Stories, Focal Press, 
2008. [Vogler 2012] http://en.wikipedia.org/wiki/The_Writer s_Journey:_Mythic_Structure [Watts2010] Watts, 
N., Write a Novel and Get It Published, McGraw-Hill, 2010. 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2013</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>2504446</article_id>
		<sort_key>110</sort_key>
		<display_label>Article No.</display_label>
		<pages>155</pages>
		<display_no>11</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>11</seq_no>
		<title><![CDATA[Advances in new interfaces for musical expression]]></title>
		<page_from>1</page_from>
		<page_to>155</page_to>
		<doi_number>10.1145/2504435.2504446</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2504446</url>
		<abstract>
			<par><![CDATA[<p>Advances in digital audio technologies have led to a situation where computers now play a role in most music production and performance. Digital technologies offer unprecedented opportunities for creation and manipulation of sound, but the flexibility of these new technologies implies an often confusing array of choices for composers and performers. Some artists have responded by using computers directly to create music, which has generated an explosion of new musical forms. However, most would agree that the computer is not a musical instrument, in the same sense as traditional instruments, and it is natural to ask "how to play the computer" using interface technology appropriate for human brains and bodies.</p> <p>In 2001, we organized the first workshop on New Interfaces for Musical Expression (NIME) to attempt to answer this question by exploring connections with the better-established field of human-computer interaction. This course summarizes what has been learned at the annual NIME conferences since that first workshop. It begins with an overview of the theory and practice of new musical-interface design and explores what makes a good musical interface and whether there are any useful design principles or guidelines available. Topics include mapping from human action to musical output, control intimacy, and tools for creating musical interfaces (sensors and microcontrollers, audio synthesis techniques, and communication protocols such as Open Sound Control and MIDI). The remainder of the course consists of several case studies that represent the major broad themes of the NIME conference, including augmented and sensor-based instruments, mobile and networked music, and NIME pedagogy.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4193513</person_id>
				<author_profile_id><![CDATA[81550737156]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Sidney]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Fels]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of British Columbia]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4193514</person_id>
				<author_profile_id><![CDATA[81100493577]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Michael]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lyons]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Ritsumeikan University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Laptop Performance  A NIME Performance  What is NIME about? The Problem: Digital Technology &#38; 
computers involved in nearly all forms of contemporary music  But the computer is not a Musical Instrument 
  How to Play the Computer?  Computers offer a wide range of sound and music creation opportunities 
 How can we create new interfaces to play computers in a way that is appropriate to human brains &#38; 
bodies?   How to Play the Computer? This tutorial is all about progress in human- computer interfaces 
for making music from past NIMEs Objectives 1. introduce the theory and practice of NIME 2. NIME community 
is very accessible and growing 3. get to know some of the people of NIME 4. easy to start creating 
NIMEs and a lifetime of enjoyment to master 5. musical expression transcends gender and culture  6. 
if you are not having fun, it s probably not for you  A Brief History of NIME New Interfaces for Musical 
Expression First organized as a workshop of ACM CHI 2001 Experience Music Project - Seattle, April, 2001 
Lectures/Discussions/Demos/Performances A Brief History of NIME NIME-02 - Media Lab Europe, Dublin 
in May 2002 Conference-scale event with similar format to the NIME-01 workshop  since 2001  NIME Themes 
 Novel controllers &#38; interfaces  Performance &#38; composition with new interfaces  Interfaces 
for collaborative performance  Real-time gestural control of music  Interfaces for musical novices 
&#38; education  Cognition in Musical Interface Design  Haptic &#38; force feedback in musical control 
 Artistic, cultural, and social impact  Course structure Part I - 1h20m Module 1: So you want to 
build a NIME  Module 2: Camera-based Interfaces  Module 3: Design &#38; Aesthetics of NIME  Discussion 
(if time)  Break 15m  Part II - 1h20m  Module 4: NIME after NIME  Module 5: NIME Theory  Module 
6: NIME Education  Discussion  Six steps to build a NIME 1. Pick control space 2. Pick sound space 
 3. Pick mapping 4. Connect with software 5. Compose and practice 6. Repeat  1 and 2 often switched. 
Tools to help with steps 1-4. An example: Tooka (Fels et al., 2004) Vibrato Pitch Bend mapping with 
PureData Octave Sustain sound synthesis Pitch Volume Pick your control space Plethora of sensors 
available to measure: motion of body parts position, rotation, velocity and acceleration  translation 
and rotation (torque) forces isometric and isotonic sensors  pressure  airflow  proximity  temperature 
 neurophysiologicalsignals  heart rate  galvanic skin response  brain waves  muscle activities 
  light levels  and more  Physical property sensors  Piezoelectric Sensors  Force Sensing Resistors 
 Accelerometer (Analog Devices ADXL50)  Biopotential Sensors  Microphones  Photodetectors  CCDs 
and CMOS cameras  Electric Field Sensors  RFID  Magnetic trackers (Polhemus, Ascension)  and more 
 What can I measure?  Human Action Oriented Sensors Reach - EMF disturbance  Slide - resistive  
TapTile - Force sensitive resistor  Tilt  electrolytic, single axis (-70-+70 deg) Touch - 0 travel 
FSR  TouchGlove  TouchStrip  long touch sensor Turn potentiometer   Connecting sensors Sensor 
response requires transduction and digitizing: electrical voltage  resistance  impedance optical 
 colour  intensity  magnetic  induced current  field direction   mechanical force   Digitizing 
converting change in resistance into voltage typical sensor has variable resistance (R) sensor Vsrc 
(+5V) A simple voltage to digitizer divider circuit Vsrc * R (R+ R)  sensor Digitizers for Connecting 
to Computer  Some MIDI synthesizers, i.e., Yamaha mu100  Arduino board  Bluetooth module for wireless 
A/D  ICubeX A/D to MIDI Phidgets A/D to USB DAQ boards A/D to computer bus  Mapping Sensor to Music 
 The relationship between the change in the sensor value to the sound output is called a mapping  The 
mapping defines how much effort to learn andplay your NIME  Last step is to control your sound output: 
  communication protocol  sound synthesizer  This is the heart of the course and what NIME community 
is specialized in.  Sound output control: communications Musical Instrument Digital Interface (MIDI) 
 electronic instrument standard defined in 1982  specifies;  connectors, data rates, electrical properties, 
etc. 1 message/msec (approx) note on/off, velocity is typical packet  control messages to change instrument 
synthesis   Open Sound Control (OSC) (Wright and Freed, 1997)  TCP/IP, internet protocol, typically 
UDP based  faster, low latency, variable packet types  computer to computer, computer to hardware 
  Internal protocols, i.e. DAQ driver  Sound Synthesis Techniques Methods sampled  FM synthesis 
 additive/subtractive  granular  waveguide/physical modeling  scan  check out Computer Music Tutorial, 
Roads, C., MIT Press, 1996 Sound Synthesizers Hardware MIDI synthesizers Yamaha, Roland, Korg, Casio, 
Moog, Kowai, Symbolic Sound Corporation, Nord modular, and others Software STK (Cook)  PureData (Pd, 
Puckette)  JASS (van den Doel)  Max/MSP (cycling74.com)  Chuck (Wang and Cook, 2003)  Supercollider 
(McCartney, 1996)  and others   A few practical notes Portable: Batteries can be used to make portable 
 Wireless protocols available for portable  Write pieces for the instrument  Aesthetics are important 
 Plan your checklist for performance  too many things can go wrong with technology Plan your staging 
can severely impact performance of sensors Plan for producing stable versions hard to learn to play if 
NIME keeps changing Module 3 has more details. Summary  Making a NIME is usually easier than playing 
it (well)  Choose your:  movement type  sound space  sensing   Put together your input, mapping 
and output  Now you are ready to:  practice, practice, practice and perform  aesthetic principles 
covered in module 3  Camera-based Interfaces  F1 : visual feedback in the form of aligned graphics 
 Imaginary Piano: No visual feedback  Leonello Tarabella, NIME-02  Video camera with motion-sensitive 
zone  No primary feedback  Visual Input Only: Imaginary Piano  Leonello Tarabella, NIME-02  Visual 
Input &#38; Output Iamascope  This gives a colourful kaleidoscopic feedback of part of the player. 
Gestures are used to trigger harmonious chord progressions and arpeggios.  Quite good coordination 
between sound and graphics   Iamascope - video  Facial Gesture Musical Interface  Lyons, NIME-01 
 Mouthesizer  Colour &#38; intensity thresholding Image Morphological transform &#38; filtering processing 
operations  Connected components + shape analysis Lyons et al., NIME-03 Mouthesizer Video Guitar Effects 
Controller  Lyons (2001) Mapping: H Cutoff Frequency of Resonant Low Pass Filter W  Distortion Mouthesizer 
Video Guitar Effects Controller  Sonification of Facial Actions (SoFA)  Optical Flow triggers samples 
 Samples mapped to facial zones  Frame is recalibrated with face detection Saccades   Funk et al., 
NIME-05  Sonification of Facial Actions (SoFA)  Reactable  Video tracking of marked pucks on a table 
 Projection of visual feedback   Sergi Jordà et al., Universitat Pompeu Fabra  first presented at 
NIME-03  Reactable  3D Vision Interfaces OpenKinect Summary  Large number of works have used visual 
input and output as a way to enhance new musical interfaces  General principle is that vision offers 
a powerful way to capture gestural input  Visual output using camera input can provide transparency 
  Technological Expressionism  Shock of the New Techno-fetishism  Experimentalism  Human-machine 
relationship  Mari Kimura w/ Lemur Guitarbot  NIME Favors a Return to Process-oriented Music we are 
in a period of restoring fluidity to the musical transformative process of making music more process-oriented 
again and less artifact­oriented. Gideon D Arcangelo, NIME-04 New Folk?   Challenge of Performance 
 Audience may not understand your NIME  Expectations may be varied  No musical tradition to fall 
back on  A demo is not a performance  Hisashi Okamoto, NIME-04 The First Sailing with Limber-Row  
Hisashi Okamoto - Limber Row   Transparency for Performer &#38; Audience Complicated mapping OO OT 
TT  Simplify  OT  Complex mapping  TO T = transparent O = opaque TOOO How to achieve TT? (Gadd 
et al, 2003) Visual Cues &#38; Transparency Visual Appearance of Instrument  Visualization of Interaction 
 Visualization of Sound Output  Reactable Tenori-on Transparency &#38; Interaction Metaphor SoundSculpting 
(Mulder and Fels, 1998) - two Cybergloves and Trackers - map metaphor of rubber sheet onto sound space 
 - transparent for audience and performer  Transparency Simple &#38; Direct Interface Particle Kanta 
Horio, NIME-04 Contact Mics  Magnets  Paper clips   Aesthetics of Failure  Suspense highlights 
the technological challenge  If there are never difficulties, glitches etc then the limits are not 
being pushed  Technical difficulty delayed this performance, but improved the outcome Some Design Guidelines: 
Perry s Principles  Rules of thumb for the design of digital musical instruments  Several of the principles 
are heavily subscribed  Principles for Designing Computer Music Controllers P. Cook, NIME-01 Revised: 
Principles for Controlling Computer Music Designers P. Cook, Keynote talk, Perry s Principles Human/Artistic 
Principles P1: Programmability is a curse P2: Smart instruments are often not smart  P3: Copying an 
instrument is dumb, leveraging expert technique is smart P4: Some players have spare bandwidth, some 
do not P5: Make a piece, not an instrument or controller P6: Instant music, subtlety later Perry s Principles 
Human/Artistic Principles P1: Programmability is a curse P2: Smart instruments are often not smart  
 P3: Copying an instrument is dumb, leveraging expert technique is smart P4: Some players have spare 
bandwidth, some do not P5: Make a piece, not an instrument or controller P6: Instant music, subtlety 
later P1: Programmability is a curse P2: Smart Instruments are Often Not  Easy to add complexity, features, 
bandwidth  But instruments can quickly become complex, unstable, and difficult to learn  It is tempting 
to A.I. to instruments but this can often be bad design if the player feels theinstrument too obviously 
has a mind of its own  Perry s Principles Human/Artistic Principles P1: Programmability is a curse P2: 
Smart instruments are often not smart  P3: Copying an instrument is dumb, leveraging expert technique 
is smart P4: Some players have spare bandwidth, some do not P5: Make a piece, not an instrument or controller 
P6: Instant music, subtlety later Perry s Principles Human/Artistic Principles P1: Programmability is 
a curse P2: Smart instruments are often not smart  P3: Copying an instrument is dumb, leveraging expert 
technique is smart P4: Some players have spare bandwidth, some do not P5: Make a piece, not an instrument 
or controller P6: Instant music, subtlety later P5: Make a piece not a controller P6:Instant Music, Subtlety 
later  Making music is the goal  The ideal new musical interfaces has: Low entry fee with no ceiling 
on  virtuosity Wessel &#38; Wright, NIME-01 Perry s Principles Human/Artistic Principles P1: Programmability 
is a curse P2: Smart instruments are often not smart  P3: Copying an instrument is dumb, leveraging 
expert technique is smart P4: Some players have spare bandwidth, some do not P5: Make a piece, not an 
instrument or controller P6: Instant music, subtlety later Perry s Principles* Technological: P7: Miracle, 
Industry Designed, Inadequate P8: Batteries, Die (a command not an observation) P9: Wires are not that 
bad (compared to wireless) Misc.: P10: New algorithms suggest new controllers P11: New controllers suggest 
new algorithms P12: Existing Instruments suggest new controllers P13: Everyday objects suggest amusing 
controllers Perry s Principles* Technological: P7: Miracle, Industry Designed, Inadequate P8: Batteries, 
Die (a command not an observation) P9: Wires are not that bad (compared to wireless) Misc.: P10: New 
algorithms suggest new controllers P11: New controllers suggest new algorithms P12: Existing Instruments 
suggest new controllers P13: Everyday objects suggest amusing controllers P13: Everyday objects suggest 
controllers that are both amusing &#38; good Sonic Banana (E. Singer, NIME-03) Java mug &#38; Fillup 
Glass (P. Cook, NIME-01) Eric Singer - Sonic Banana  Perry s Principles* Technological: P7: Miracle, 
Industry Designed, Inadequate P8: Batteries, Die (a command not an observation) P9: Wires are not that 
bad (compared to wireless) Misc.: P10: New algorithms suggest new controllers P11: New controllers suggest 
new algorithms P12: Existing Instruments suggest new controllers P13: Everyday objects suggest amusing 
controllers Perry s Principles* New (as of 2007) P14: More can be better (but hard)  P15: Music + 
Engineering is a great Teaching (and Marketing) tool P17: Younger students are more fearless Perry s 
Principles* New (as of 2007) P14: More can be better (but hard)  P15: Music + Engineering is a great 
Teaching (and Marketing) tool P17: Younger students are more fearless P15: Music + Engineering is a great 
Teaching Tool  High student interest  Motivation for learning a range of core topicsincluding:  Sensors 
 HCI  DSP  Math skills  Programming  Networking  Joe Paradiso &#38; student (NIME-02)  Where 
to study this field? IRCAM, Paris  CCRMA, Stanford  CIRMMT, McGill  Princeton, CS &#38; Music  NYU 
Interactive Telecommunications Program  SARC, Queen s, Belfast  Growing field  URLs listed in the 
References  Specific Learning Resources  Miranda &#38; Wanderley (2006)  Igoe (2007)  Roads (1996) 
 NIME Proceedings  ICMC Proceedings  Computer Music Journal  Organized Sound  J. New Music Research 
  Summary  Technology is increasing the fluidity of musicalculture  NIME presents special challenges 
for performers  Well-designed visual feedback can greatly improve mapping transparency for audience 
and player  Interaction metaphors another strategy  Initial failure can enhance eventual success  
Perry s principles provide practical policies  Questions/Discussion Break Start again at 3:30!  Original 
NIMEs Leon Theremin, 1928 senses hand position relative to antennae  controls frequency and amplitude 
 Clara Rockmore playing   More original NIMEs Hugh Le Caine (1940s)  electronic sackbut  sensor 
keyboard  downward and side-to-side  potentiometers    Science Dimension volume 9 issue 6 1977 
 right hand can modulateloudness and pitch  left hand modulates waveform  precursor to the mod-wheel 
Canada Science and Technology Museum  &#38;#169;2013, S. Fels and M. Lyons Electronic Sackbut  1971 
commercial version Buchla s Midi Controllers  Thunder (1990) 36 touch sensors Lightning 2 (1996) 
LED based position sensing Marimba Lumina (1999)  pads and ribbon controllers (strips)  bars are sensitive 
to proximity, hit location and movement  4 different mallets for different effects  &#38;#169;2013, 
S. Fels and M. Lyons  Buchla 200e Series music controllers Modules can be combined: Control and Signal 
Router  Multi Dimensional Kinesthetic Input Port   Midi Decoder/Preset Manager  System Interface 
Arbitrary Function Generator (2 panel units)  Complex Waveform Generator  Source of Uncertainty 
 Quad Function Generator  Frequency Shifter / Balanced Modulator  Triple Morphing Filter  Quad 
Dynamics Manager  &#38;#169;2013, S. Fels and M. Lyons There s a lot of NIMEs out there  (Marshall, 
2009) Most are classed in the Alternate category Augmented Instruments Hypercello (Gershenfeld &#38; 
Chung, 1991) - related Hyperbow (Young, 2001)  Augmented Instuments Yamaha Disklavier MIDI controlled 
acoustic piano  solenoid actuators to play keys  records key press  Radio Baton + Disklavier performance 
Jaffe &#38; Schloss, The Seven Wonders of the Ancient World, 1991 Augmented Instruments Hyper-Flute 
(Palacio-Quintin, 2003) standard Boehm flute  sensors:  magnetic field, G# and C# keys  ultrasound 
tracking  mercury tilt switch  pressure sensors (left hand and thumbs)  light sensor  buttons  
  Alternative Instruments: Using different sensors  (Marshall, 2009) Alternative Instruments approaches 
to taxonomy: sensed property (i.e. wind)  player action (i.e. percussion)  instrument shape  relationship 
to body  Hands Only - free gesture + physical Lady s Glove (Sonami, 1991+) hall effect sensors, microswitches, 
resistive strips, pressure pad, accelerometer  controlled musical effects  free gesture + contact 
gesture + voice The Hands (Waisvisz, 1984) Hands Only - free gesture Manual Input Sessions (Levin and 
Leibermann, 2004) camera and OHP  SoundSculpting (Mulder and Fels, 1998) GloveTalkII/GRASSP/DIVA, (Fels 
et al., 1994+) cybergloves, tracker, switches  controlled formant synthesizer  and more Hands - Contact 
gesture Most typical type of NIME  Ski (Huott, 2002)  fibre optic multitouch pad Tactex Inc.  
mappings:  playback: linear, polar and angular controlmodes  percussive  pitch tuning:   MIDI 
controller  upright form factor   Hand - Contact gesture Pebblebox (O Modhrain &#38; Essl, 2004) 
 microphone + stones  granular synthesis  play with stones mixes granules  Hand - Contact gesture 
Crackle box (Waisvisz, 1975) analog circuit  op-amp with body resistance connected to pins  in the 
tradition of circuit bending   Hand - Contact gesture Lippold Haken sContinuum touch sensitive - neoprene 
covered  x, y along board  z - pressure   MIDI controller  sound effects  continuous frequency 
 pitch bends    Breath and Hands  Face/Head Control eSitar (Kapur et al, 2004) accelerometer for 
head tilt experimented with volume, duration, and more Mouthesizer (Lyons et al., 2003)  SoFA, (Funk 
et al., 2005)  Tongue n Groove (Vogt et al., 2002)  ultrasound probe to measure tongue movement &#38;#169;2013, 
S. Fels and M. Lyons Body  Miburi from Yamaha, 1994  bend sensors at arm joints  two buttons/finger 
and thumb  two pressure sensors/foot  MIDI controller    Inside Body Biomuse (Knapp and Lusted, 
1990) 8 channel signal amp EMG, EKG, EOG, EEG Tibeten singing bowls (Tanaka and Knapp, 2002) EMG and 
position sensing miniBioMuseIII (Nagashima, 2003) 8 EMG channels mapped to bandpass filters, sinewave 
generators and FM synthesizers used in BioCosmicStorm-II  Collaborative Instruments  Tooka (Fels and 
Vogt, 2002) pressure for breath  buttons for fingers  bend sensor  touch sensor   two players share 
breath  coordinate movements  MIDI mapping  Collaborative Instruments OROBORO (Carlile &#38; Hartmann, 
2005) haptic mirror between hand paddles  4 FSRs/hand  mapped using Pd to:  violins sounds  sampled 
sounds    NIMEs for Novices: Jam-o-drum (Blaine and Perkis, 2000) 4 player audio/visual interface 
drum pads sensors with rotation sensor around rim Drum circle concept  Various musical games  turn 
taking  collaboration   NIMEs for Novices  Interactive instruments embody all of thenuance, power, 
and potential of deterministic instruments, but the waythey function allows for anyone, from themost 
skilled and musically talentedperformers to the most unskilled membersof the large public, to participate 
in amusical process (Chadabe, 2002)  Walk up and play  NIMEs for Novices (Blaine &#38; Fels, 2003) 
Brain Opera (Machover et al, 1996) Aptitude Novice Virtuoso Capacity Single player Single interface Electronic 
Bullroarer Iamascope Duet on piano Multiple interfaces Musical Trinkets Jazz Ensembles Multiple players 
Single interface Beatbugs Squeezables Audio Grove Sound Mapping Speaking Orbs Jamodrum Mikrophonie I, 
Tooka Multiple interfaces Augmented Groove Brain Opera Drum Circle Mikrophonie II &#38;#169;2013, S. 
Fels and M. Lyons  Multiple NIMEs as part of a larger connected setof interaction Forest station  
Harmonic driving  Melody easel  Rhythm tree  Gesture wall  Digital baton  Audience sensing in performance 
space  Sensor chair  Brain Opera (Paradiso, 1999)  Sensor chair multiple antenae to track hand positions 
 two antenae for feet  buttons  lights  MIDI mapping Brain Opera NIME  Summary  Creating a NIME 
is easy to do  Creating a good mapping is hard  Playing it well takes practice to be a virtuoso  some 
NIMEs created to be easy to play but not so expressive Without a piece, difficult to gain acceptance 
 Often audience doesn t know what is going on  Many explorations trying different ways to make music 
  NIME Generic Model I Gestural M : Mapping,  : Primary &#38; Secondary Feedback F1, F2 Based 
on: Miranda &#38; Wanderley (2006)  Feedback Design: F1 and F2 Sound   F2  Tactile*  Kinesthetic 
  F1  Visual**  *Includes vibro-tactile feedback due to sound waves on the instrument ** Re: Module 
2 on Visual Interfaces Model: Traditional Instrument  NIMEs decouple Control separate from Synthesis 
 Mapping (M) is designed  Feedback (F1 and F2) is designed  Controller/Interface is designed  NIME 
representations discrete vs. continuous controllers keys vs knobs  acoustic vs electronic sound output 
vibrating string vs. speaker digital vs analog representations bits vs. voltage  NIME, DMI, Instrument 
 musical interface and nime used interchangeably  DMI Digital Musical Instrument  DMI &#38; MI may 
be preferable because a NIME will not be new forever  Digital NIME Computer enables arbitrary design 
of interface behaviour: controller  feedback (F1 &#38; F2)  mapping (M)  synthesizer  NIME Generic 
Model I Gestural M : Mapping,  : Primary &#38; Secondary Feedback F1, F2 Based on: Miranda &#38; 
Wanderley (2006) Designing Controllers: Gestural Input Free gesture interfaces no physical contact 
Physical contact interfaces all acoustic instruments NIMEs can be in either  Free Gesture Interface 
 Theremin (1919)  Sound feedback (F2) only  No primary tactile  or visual feedback (F1)  Have been 
few virtuosos  Considered difficult to  master  Léon Theremin  Gestural Input  F2   Theremin 
lacks significant primary feedback The Hands  Passive F1 STEIM, Amsterdam (Studio for Electro-instrumental 
Music) NIME Generic Model I Gestural M : Mapping, : Primary &#38; Secondary Feedback F1, F2 Based 
on: Miranda &#38; Wanderley (2006) F1 : Visual &#38; Tactile Feedback  Nishiburi &#38; Iwai NIME-06 
 Tenori-on  NIME Generic Model I Gestural M : Mapping, : Primary &#38; Secondary Feedback F1, F2 
Based on: Miranda &#38; Wanderley (2006) Instrument Mapping  T. Kriese Fairlight CMI, 1980s Polyphonic 
Digital Sampling Synth Matrix (Overholt, 2001)  Shakuhachi  Mapping Problem : How to design the gesture 
to sound mapping? Mapping Aspects of the Mapping Problem  Dimensionality  Complexity  Mapping Strategy 
  Other aspects   The mapping layer can be considered as the essence of a musical interface Hunt, 
Wanderley, and Paradis (2003) Dimensionality: Types of Mapping N-to-1 N-to-N  1-to-1 1-to-N   
Complexity: Simple &#38; Complex Mappings  Simple Complex Hunt, Wanderley, and Paradis (2003) Mapping 
Complexity complexity can lead to better expression - 1 to 1 usually doesn t do the trick * not interesting 
 * not enjoyable * not satisfying  Hunt, Wanderley, &#38; Paradis, NIME-02 Understanding Complexity: 
Three Layer Mapping Strategy  Abstract Mapping Layer example Mouthesizer interface (Module 2: Camera-based 
Interfaces) Controlling a Formant Filter using Mouth Shape [ o ] [ a ] [ i ] Lyons et al., NIME-03 
Mouthesizer Vowel Mapping  Mapping Design Strategy Advantage to have a control interface which is based 
on the perceptual qualities of timbre spaces  Better mapping leads to more playable interface  How 
do we characterize playability?  Musical Control Intimacy ... the match between the variety of musically 
desirable sounds produced and the psycho­physiological capabilities of a practiced per- former. Moore 
(1988) Control Intimacy depends (somehow) upon gesture to sound mapping Flow in musical expression 
 Quality of Control Experience of Mapping Intimacy Flow  Special contact with the instrument  Development 
of a subtle feeling for sound  Feeling of effortlessness  Playful &#38; Free-spirited feeling handling 
of the material  A. Burzick (2002) Threats to Intimacy Latency between gesture and sound  Lack of 
primary feedback  Poor mapping  Summary  Generic musical interface model is helpful in understanding 
what makes &#38; breaks a NIME  Mapping constitutes the essence of a digital NIME  Mapping is not 
straightforward and many design strategies have been tried  Multiplayer mappings can be better than 
simple one-to-one mappings  Studies of mapping and feedback are core research topics of NIME  Education 
and NIME  Sound Synthesis  Sensors, Effectors, Microcontrollers  Basic Electronics  Communication 
Protocols (MIDI, OSC, TCP etc.)  Sound Synthesis and Processing  Acoustics  Human-Computer Interaction 
 Music  Where to study this field? IRCAM, Paris  CCRMA, Stanford  CIRMMT, McGill  Princeton, CS 
&#38; Music  NYU Interactive Telecommunications Program  SARC, Queen s, Belfast  Growing field  
URLs listed in the References  Specific Learning Resources  Miranda &#38; Wanderley (2006)  Igoe 
(2007)  Roads (1996)  NIME Proceedings  ICMC Proceedings  Computer Music Journal  Organized Sound 
 J. New Music Research   Curricula  beginning graduate or senior undergraduate level  Courses tend 
to be project oriented  Students learn what they need  Live performance or Demo is necessary for completion 
of the course (ITP, CCRMA)  NYU ITP NIME Course  Master s program in design &#38; technology attracting 
students from a wide range of backgrounds   Gideon D Arcangelo Hans C. Steiner Jamie Allen Taku Lippit 
(NIME-04)  NIME Curriculum - Topics  Historical Survey of Musical Instrument Types  Attributes of 
Musical Expression  Music Theory and Composition  Musical Interface Responsiveness  Discrete vs. Continuous 
Controllers  Gestures and Mapping  Novice and Expert Interfaces  Spectacle and Visual Feedback in 
Performance  Collaborative Interfaces  Substantial resources for learning about NIME  NIME courses 
are usually project based  Number of universities offering programs of study is expanding  Next frontier: 
high schools, science fairs  Summary   How to Play the Computer?  Computers offer a wide range of 
sound and music creation opportunities  How can we create new interfaces to play computers in a way 
that is appropriate to human brains &#38; bodies?  Here s how  NIME tools  NIME principles  NIME 
examples  NIME theory  NIME education  How to get involved NIME community community@nime.org subscribe 
with community-request@nime.org NIME website www.nime.org ICMC website www.computermusic.org/ Related 
conferences  INTERACT 2011 (NIME Session)  ICEC 2011 (www.icec2011.org)  Bigger picture  1. introduced 
the theory and practice of NIME 2. NIME community is very accessible and growing 3. get to know some 
of the people of NIME 4. easy to start creating NIMEs and a lifetime of enjoyment to master 5. musical 
expression transcends gender and culture  6. if you are not having fun, it s probably not for you  
Questions &#38; Discussions Contact us: Sidney Fels, ssfels@ece.ubc.ca  Michael Lyons, lyons@im.ritsumei.ac.jp 
 www.nime.org www.ece.ubc.ca/~ssfels/SIGGRAPH2011/NIME-course-Fels­Lyons.pdf 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2013</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>2504447</article_id>
		<sort_key>120</sort_key>
		<display_label>Article No.</display_label>
		<display_no>12</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>12</seq_no>
		<title><![CDATA[Advances in real-time rendering in games part I]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2504435.2504447</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2504447</url>
		<abstract>
			<par><![CDATA[<p>Advances in real-time graphics research and the ever-increasing power of mainstream GPUs and consoles continue to generate an explosion of innovative algorithms suitable for fast, interactive rendering of complex and engaging virtual worlds. Every year, the latest video games employ a vast variety of sophisticated algorithms to produce ground-breaking 3D rendering that pushes the visual boundaries and interactive experience of rich environments.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4193515</person_id>
				<author_profile_id><![CDATA[81487640839]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Natasha]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tatarchuk]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Bungie Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2013</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>2504448</article_id>
		<sort_key>130</sort_key>
		<display_label>Article No.</display_label>
		<pages>108</pages>
		<display_no>13</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>13</seq_no>
		<title><![CDATA[Lights! speed! action!]]></title>
		<subtitle><![CDATA[fundamentals of physical computing for programmers]]></subtitle>
		<page_from>1</page_from>
		<page_to>108</page_to>
		<doi_number>10.1145/2504435.2504448</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2504448</url>
		<abstract>
			<par><![CDATA[<p>The definition of "computer graphics" as used by artists in new media and kinetic areas of the arts is much more expansive than simply rendering to a screen. A visit to the SIGGRAPH 2013 Art Gallery, for example, reveals a wide variety of uses of physical computing, embedded control, sensors, and actuators in the service of art. This course is for programmers, educators, artists, and others who would like to learn the basic skills necessary to include physical components in their computing systems.</p> <p>The course is targeted at programmers with little or no electronics background. It begins with basic electronics concepts as they are used with physical computing components, then reviews a variety of sensors that provide information about the physical environment (light, motion, distance from objects, flex, temperature, etc.), programmer-controlled lights (LEDs), and programmer-controlled motion (servos, motors). The use of these components is described in the context of the Arduino microcontroller, but the topics are general and will transfer to a variety of other computing platforms. Although the course includes a few simple formulas, the strong focus will be on practical usage and common-sense applications in real circuits.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4193516</person_id>
				<author_profile_id><![CDATA[81100034829]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Erik]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Brunvand]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Utah]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Lights! Speed! Action! Fundamentals of Physical Computing for Programmers Erik Brunvand School of Computing 
University of Utah v1.0, April 26, 2013 Figure 1: Serpente Rosso, Erik Brunvand 2012, acrylic, LEDs, 
wire, circuits, and computer control Copyright c&#38;#169;Erik Brunvand, 2013 Contents Abstract 1 Course 
Schedule 2 1 Electronics Fundamentals 3 1.1 The Water Analogy . . . . . . . . . . . . . . . . . . . . 
. . . . . . . . . . . . . 4 1.2 Ohm s Law . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
. . . . . . . . 6 1.3 Kirchhoff s Laws . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
. . . 8 1.4 Practicalities of Wiring and Powering Circuits . . . . . . . . . . . . . . . . . 10 1.5 The 
Arduino Platform . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13 1.5.1 Arduino Electrical 
Overview . . . . . . . . . . . . . . . . . . . . . . . 14 1.5.2 Arduino Software Overview . . . . . . 
. . . . . . . . . . . . . . . . . 16  2 Lights! LEDs 23 2.1 Multiple LEDs driven from digital pins 
. . . . . . . . . . . . . . . . . . . . . 26 2.2 Diming an LED with pulse width modulation . . . . . 
. . . . . . . . . . . . 29 2.3 Driving lots of LEDs directly from Arduino . . . . . . . . . . . . . . 
. . . . . 30 2.4 Driving lots of LEDs using external LED-driver chips . . . . . . . . . . . . . 31 2.4.1 
74HC595 latched shift register . . . . . . . . . . . . . . . . . . . . . . 32 2.4.2 STP08DP05 LED driver 
chip . . . . . . . . . . . . . . . . . . . . . . . 36 2.4.3 MAX 7219/7221 LED driver chip . . . . . . 
. . . . . . . . . . . . . . . 41 2.4.4 The Texas Instruments TLC5940 LED driver chip . . . . . . . . 
. . . 49  2.5 Questions about LEDs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52 
 3 Speed! Sensors 55 3.1 Switches . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
. . . . . . 55 3.1.1 Arduino Built-in Pullup Resistors . . . . . . . . . . . . . . . . . . . . 59 3.1.2 
Switch Bounce and Debouncing . . . . . . . . . . . . . . . . . . . . . 59 3.1.3 Aside: Serial Communications 
with the Host Machine . . . . . . . . 63 3.1.4 Another type of Switch: Passive Infrared (PIR) Motion 
Detector . . . 64  3.2 Resistive Sensors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
. . . . 66 3.2.1 Potentiometers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66 3.2.2 
Other Resistive Sensors . . . . . . . . . . . . . . . . . . . . . . . . . . 69  3.3 Analog Sensors . 
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71 3.4 Sensor Calibration . . . . 
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 72 3.5 Questions about Sensors . . . . . 
. . . . . . . . . . . . . . . . . . . . . . . . . 75  4 Action! Motors 79 4.1 Hobby Servos . . . . . 
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 79 4.2 DC Motors . . . . . . . . . . . 
. . . . . . . . . . . . . . . . . . . . . . . . . . . 82 4.2.1 Motor Speed Control . . . . . . . . . 
. . . . . . . . . . . . . . . . . . . 86 4.2.2 Motor Direction Control . . . . . . . . . . . . . . . 
. . . . . . . . . . . 87  4.3 Stepper Motors . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
. . . . . . . 90 4.4 Questions About Motors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
. 98 5 Supply Sources 101 6 Kinetic Art using Physical Computing 103  Abstract The de.nition of computer 
graphics as used by artists in new media and kinetic areas of the arts is much more expansive than simply 
rendering to a screen. A visit to the SIGGRAPH art gallery, for example, will showcase a wide variety 
of uses of physical computing, em­bedded control, sensors, and actuators in the service of art. This 
course is for programmers, educators, artists and others who would like to learn the basic skills and 
intuition necessary to incorporate physical components into their computing systems. The course is targeted 
at programmers with little or no electronics background. We start with basic electronics concepts as 
they are used with these components. We then cover a variety of sensors that provide information about 
the physical environment (light, motion, distance from objects, .ex, temperature, etc.), programmer-controlled 
lights (LEDs), and programmer controlled motion (servos, motors). We will describe the use of these compo­nents 
in the context of the Arduino microcontroller, but the skills learned will be general and should transfer 
easily to a variety of other computing platforms. Although there will be a few simple formulas in this 
material, the strong focus will be on practical usage and common sense applications in real circuits. 
In fact, for those physi­cists in the audience, there are places where a common-sense or rule-of-thumb 
description will be used that may not be completely accurate in terms of electrostatic or quantum me­chanical 
reality. Please take these models for what they are: intuitive descriptions that help understand the 
situation in practice, even though they may gloss over some second or­der effects. After taking this 
course you should feel more comfortable in selecting, wiring, using, interfacing, and controlling a variety 
of simple physical computing components. Following a few simple rules of thumb, and knowing what questions 
to ask, should keep you from blowing up too many components. These physical computing components can 
allow you to add physical computer graphics to your repertoire. That is, real, physical machines controlled 
by computers that make graphical marks! About the Author Erik Brunvand is an Associate Professor in the 
School of Computing at the University of Utah in Salt Lake City, Utah. His research and teaching interests 
include the design of application-speci.c computers, graphics processors, ray tracing hardware and software, 
asynchronous systems, and VLSI. Starting in 2009 he has co-developed and taught an arts/tech collaborative 
course with a colleague, Paul Stout, in the Department of Art and Art History at the University of Utah 
entitled Embedded Systems and Kinetic Art (www.eng.utah.edu/ cs5789/). As an artist he is a printmaker, 
co-founder of Saltgrass Printmakers (a non-pro.t printmak­ing studio and gallery in Salt Lake City: www.SaltgrassPrintmakers.org), 
and also works in mixed-media computer-controlled kinetic arts. elb@cs.utah.edu www.cs.utah.edu/ elb 
  Course Schedule 5min: Introduction and motivation 20min: Section1: Electronics Fundamentals Charge, 
Voltage, Current -the water model  Ohms Law -applications to physical circuits  Kirchoffs Laws -physical 
arrangements of components  Practicalities of wiring and powering circuits  Arduino Platform  15min: 
Section 2: Lights! -LEDs Electrical properties  Solutions for lots of LEDs  20min: Section 3: Speed! 
-Environmental Sensors Switches  Resistive sensors  Analog sensors  Analog to digital conversion 
 20min: Section 4: Action! -Servos and motors Hobby servos  DC motors  Stepper motors  10min: Conclusions 
and Context -Lightening review of kinetic art and physical computing, and how this all re­lates to a 
broad de.nition of computer graphics in the .ne arts. These course notes have far more information than 
can be presented in a 90min course. The live course will present the high level concepts, and these notes 
serve as the details.  Chapter 1 Electronics Fundamentals Why review basic electronics? You ve probably 
seen all this in another class you took as an undergraduate or even in high school. However if you haven 
t used this information lately it s likely that you could use a review. Many of the physical computing 
components that we ll be talking about have speci.c voltage and current requirements, and the plat­form 
that you re using to control them (like the Arduino, for example) has certain electrical limitations. 
By reviewing basic electronics we ll make sure that we re all on the same page when it comes to asking 
the right questions about electrical interfacing, and also knowing what the right questions are in the 
.rst place. Electricity is a term that encompasses a wide variety of physical phenomena related to the 
behavior of subatomic charged particles and electromagnetic .elds. For our pur­poses we re mostly concerned 
about the movement of charge in response to an electric .eld. Where does the charge come from? That comes 
from an inherent property of sub­atomic particles. In the Bohr theory of the atom1, a positively charged 
nucleus (positively charged protons and uncharged neutrons) is orbited by neg­atively charged electrons. 
The charged particles interact with each other: particles with the same charge repel each other, and 
particles with different charges attract each other. It is this electrostatic attraction/repulsion that 
keeps the electrons orbiting the nucleus and holds the atom together. This is, of course, quite an oversimpli.cation, 
but it is a .ne .rst-order model to understand charge and charge movement. Why is charge movement important? 
Because that s the essential part of electricity that we re concerned with. The movement of charge in 
a conductor is the source of power and function­ality of pretty much all the physical computing components 
we ll be talking about. The movement of charge past a .xed point in a conductor is electrical current 
and is measured in amperes or amps. The force that causes the charge to move is measured in volts. Let 
s get some de.nitions out of the way: Conductor: A material where the electrons are weakly bound to the 
other atoms in the material so that when a force is applied (electric .eld), the electrons can move from 
atom to atom (electrical current can .ow). Copper is an example of a good conductor. 1Niels Bohr, Danish 
physicist, 1882 -1962, Nobel Prize in Physics, 1922. Insulator: A material where the electrons are so 
tightly bound to the other atoms that it takes a huge amount of force to get them to move around. For 
practical purposes, the electrons don t move, so current doesn t .ow. Glass is an example of a good insulator. 
Current: The amount of charge moving past a point in a conductor. If you could count atoms as they move 
by a point in a conductor, you could directly compute the current. Charge is measured in coulombs. One 
coulomb of charge is the amount of charge on 6.241 × 1018 electrons. That s a LOT of electrons. Fortunately 
there are a LOT of electrons .oating around in a conductor. Copper, for example, has 1.38 × 1024free 
electrons per cubic inch. Ampere: Also known as an amp, this is the unit of current in an electrical 
circuit. An ampere is one coulomb of charge passing a point in one second. Voltage: The electromagnetic 
force that causes charge to move, measured in volts. Voltage is actually a form of potential energy. 
It s the amount of energy required to move one coulomb of charge from one point to another. Voltage is 
related to other measures of energy such as the joule. Raising the voltage of one coulomb of charge by 
one volt takes one joule of energy. The high voltage node in a system is often abbreviated as VDD. This 
is somewhat obscure notation based on the Drain to Drain Voltage in transistor based circuits. Ground: 
This is an arbitrary point in an electrical circuit that is de.ned to have a voltage of 0. This is arbitrary 
because voltage is a relative term. Two points in the circuit are de.ned by their difference in voltage 
(their difference in potential energy). But, this is just a difference in energy. There is no absolute 
zero. We simply de.ne one point in the circuit to be zero volts and measure voltage relative to that 
point. In your house, this point is actually the ground. There is a metal stake in the basement of most 
houses that goes into the ground and is de.ned as the arbitrary zero volt reference for your house. That 
s why it s called ground! Ground is often abbreviated GND. Power: Measured in watts, power is simply 
volts times amps (equivalently joules/sec). This is instantaneous power. Power over time has slightly 
more complicated mea­sures to smooth out the differences over time. Big Idea Charge moving in a conductor 
(current) under the in.uence of an electrical force (voltage) is the main electrical activity that we 
re interested in. This phenomenon powers LEDs, makes motors move, and is the property that we ll sense 
in a sensor to measure our environment. Causing current to .ow, and controlling that current, is one 
of our main goals! 1.1 The Water Analogy One way to get an intuitive feel for current and voltage is 
to think of electricity .owing in a conductor like water .owing in a river. The electrical current is 
very much like the water current. The voltage is like slope that provides the potential energy that lets 
the water .ow. A higher slope provides more energy to move the water than a gentle slope. Like water, 
you can think of interesting situations with high and low current, and high and low power (voltage). 
Elaborating on this model a little bit: High Current, High Voltage: This is like Niagara Falls: There 
is lots of water and lots of energy being dissipated. In electronics this is like a high-tension, high-voltage 
power 1.1: The Water Analogy Table 1.1: A water analogy for systems with high/low current and voltage 
High Current Low Current High Voltage By JohnnyAlbert10 [Public domain], via Wikimedia Commons By Tomaszp 
[CC-BY-SA-3.0], via Wikimedia Commons Low Voltage USGS image, via Wikimedia Commons By User:Ruhr.sch 
[GFDL] via Wikimedia Commons transmission line. Long-range power transmission lines might be at 100-500 
kilo­volts (kV) at 100 amps or more. Shorter lines might be 50kV at 10kA. Yikes! This implies kilo-Watts 
and Mega-Watts of power. Low Current, High Voltage: Imagine Angel Falls in the dry season -the water 
is falling from a huge height (lots of voltage), but there isn t much of it (low current) so it doesn 
t really hurt when it falls on your head. Static electricity is a great example. When you rub your feet 
on the carpet and then touch a doorknob, you might be dissipating 10,000 or 20,000 volts! But this voltage 
is at very low currents so you can see the spark, but you don t get hurt. Energy dissipated ranges from 
1millijoule to 50millijoules (a millijoule is a thousandth of a joule) (1 watt is 1joule/sec). High Current, 
Low Voltage: Like the Mississippi river -lots of water, but moving very slowly. Still, there s lots of 
energy in that system! An arc welder is an electrical example: it might use a step-down transformer to 
bring the line voltage down to 20-40v, but at 600amps which can weld metal. Another example is a graphics 
chip by NVIDIA or ATI. Their high-end chips consume 200-300watts, but with a power supply of around 1v. 
That implies 200-300 amps of current (at low voltage) going into those chips! Low Current, Low Voltage: 
like a small creek that doesn t do major work, but has just the right amount of energy to make a pleasant 
babbling sound. Think embedded computing here. Your phone runs for days on a battery -it s consuming 
tiny current (milliamperes or mA) at low voltages ( 1v). It s still doing a lot of stuff, but trying 
to consume as little power as possible doing it.  1.2 Ohm s Law Ohm s law 2 describes the relationship 
between voltage, current, and a property of physical materials called resistance. Resistance, as the 
name implies, is the property of a material to resist the .ow of electricity. The precise relationship 
is V = I R where V is the potential difference between two points in the circuit (measured in volts), 
I is the current .owing between those points (measured in amps), and R is the resistance of the material 
(measured in ohms, often using O as a symbol). Resistance to the .ow of charge is similar to friction 
in a mechanical system. Or, using the water analogy, you can think of this as the diameter of the pipe 
through which the water (current) .ows. A narrow pipe restricts the .ow of water (current), whereas a 
larger diameter pipe lets the current .ow more freely. One ohm is de.ned as the resistance in a system 
when one volt of energy produces one amp of current. Big Idea Ohm s law tells us about the relationship 
between voltage, current, and resistance. Suppose you want to keep the voltage the same, but reduce the 
current in a circuit? Ohm s law tells us we can do this by increasing the resistance. This is known as 
a current limiting resistor and we ll see it again soon. Very useful! Ohm s law can tell us how to compute 
any one of voltage, current, and resistance, if the other Big Idea two are known. You can use Ohm s triangle 
to remind yourself how to do this. Cover the unit you re looking for with you .nger, and the remaining 
parts of the triangle tell you how to compute that unit. For example, if you want to compute the resistance 
in a circuit, block out R, and you re left with V/I as the way to compute that resistance. Ohm s law 
makes a great deal of common sense if you think about the water anal­ ogy again. Imagine that you have 
a system of water running through a pipe with some force. If you make the pipe smaller, but don t increase 
the force (voltage), then the current must go down because of the smaller pipe (more resistance). This 
is the effect of a current­ limiting resistor in a system -to reduce the current while the voltage remains 
the same. In the same situation, if you want the same amount of current to .ow through the smaller pipe, 
you need to increase the force (voltage) to force that current through the smaller pipe. This is used 
when a particular current is required in a circuit (a component that requires a .xed current). You then 
know how to set the voltage to achieve that amount of current. Resistors are electrical components that 
look like little cylinders with colored bands printed on them, and wire connections on each end. The 
colored bands tell you how much resistance each component has. Reading the colored bands is a bit of 
an art form. There are many calculators on the web that can help you decode this. A couple examples are: 
www.hobby-hour.com/electronics/resistorcalculator.php www.hobby-hour.com/electronics/resistorcalculator.php 
 Resistors are not directional -it doesn t matter which end is connected to + and which end to -. They 
just act like smaller or larger pipes for the current to .ow through. If you draw a resistor in a circuit 
schematic you usually use a zigzag symbol or a rectangle that looks like the right side of Figure 1.1. 
The R1 and R2 are the component names. In a complex schematic it s important to give each of the components 
identi.ers so that you 2Named for Georg Simon Ohm (1789-1854), a German physicist and mathematician. 
 1.2: Ohm s Law can refer to speci.c components by name. As a practical matter, the ranges of current, 
voltage, and resistance that you re likely to encounter in physical computing circuits areas follows: 
Current: useful quantities for our circuits range from amps to milliamps (mA). A mA is a thousandth of 
an amp: 1mA = .001A. Amps are usually denoted as i or I in circuits for reasons that nobody is completely 
sure about. Voltage: useful quantities for our circuits range from a few 10 s of volts to millivolts 
(mV). Again, a mV is a thousandth of a volt: 1mV = .001V. Resistance: useful quantities range from ohms, 
to kilo-ohms (kohm or kO), to megaohms (Mohms or MO). A kilo-ohm is 1,000O and a megaohm is 1,000,000O. 
 Some practical examples before we move on: Q: How much current does a 1200watt toaster draw from a 
120v power line? A: If P = V I , then I = P /V , so I (amps) = 1200watts/120v = 10A of current.  Q: 
If you have a 220v electric stove top, and the heating element has 30O of resis­tance, how many amps 
go through that heating element?  A: V = I R, so R = V /R = 220v / 30O = 7.33A  Q: How much power does 
that heating element dissipate?  A: P = V I = 220v 7.33A = 1,613.33 watts A2: P = V I , and V = I R, 
so substituting in the .rst equation, P = I2R = (7.33)2 × 30O = 1,613.33 watts. Whew! Before we move 
on, we should make clear one confusing aspect of voltage, current, and resistance. Current is measured 
in terms of positive current. That is, it is measured in terms of charge moving Confusing from the more 
positive point in the circuit (typically the power supply) to the more negative point Concept #1 in the 
circuit (typically ground). BUT, the actual things that are moving are the negatively charged electrons! 
So, the current that we measure as positive current is moving in the opposite direction as the actual 
charge that is moving. Believe it or not, this confusion goes back directly to Ben Franklin. Yes, THAT 
Ben Franklin. During his experiments with electricity, he arbitrarily chose to de.ne one type of charge 
negative, and the other positive. Later, when batteries were more common, it was natural to consider 
current .owing from the positive to the negative terminal of the battery. Only many years later did we 
discover that the actual subatomic particles that were moving were the electrons, and they moved in the 
opposite direction as the de.ned current. What a mess! But, we re stuck with it now... Just remember: 
current is measured as positive charge moving from the higher voltage in the circuit to the lower voltage. 
But, if you want to confuse yourself, remember that although you re thinking about positive charge moving 
to ground, inside the conductor it s actually the other way around: negative charge is moving towards 
the higher voltage! Big Idea #4 If that s too much, just ignore it. Current is positive and moves from 
the higher voltage (the + end of the battery, or Vdd) to the lower voltage (the -end of the battery, 
or Ground).  1.3 Kirchhoff s Laws Kirchhoff s laws3 tell us about how voltage and current behave in 
an electrical circuit that has series and parallel connections of components through nodes and loops. 
First let s make sure we know what a node and a loop are in a circuit. A node in a circuit is any part 
of the circuit that is connected together through a conductor. All points on the node are at the same 
electrical potential (voltage) because they are connected. As long as you connect all the pieces of the 
node together through a conductor, the physical circuit will be the same as the schematic. A loop is 
a path that starts at a node, passes through connected components, and ends up at the same node at which 
it started. With a battery in the circuit the loop can start at the + end of the battery and end at the 
-end. Without a battery, a loop can be from the voltage source (5v say) through components to ground 
(0v). That may not look like a loop, but it is because the battery node is implied by the 5v (+) and 
0v (-) in the circuit. Kirchhoff s Voltage Law (KVL): The sum of voltages around a loop is zero. This 
is like saying that if you are hiking and you start at the parking lot, go up a hill, down a hill, and 
end up back at the parking lot, you will have a net altitude gain/loss around the loop of zero. You will 
have gone up just as much as you went down because you ended up right where you started. Kirchhoffs Current 
Law (KCL): The sum of the currents going into and out of a node is zero. This is like saying that all 
the current that .ows into a node must also .ow out. This is like a river splitting into three branches: 
the water upstream that comes into the split must all .ow out through the combination of the output branches. 
3Gustav Robert Kirchhoff (1824 -1887), German physicist. 1.3: Kirchhoff s Laws In a way, these are obvious 
laws, but to some extent that s their point: that current and voltage obey the obvious rules that seem 
to make sense. There s a big idea hiding in Kirchhoff s Voltage Law: If the battery supplies a certain 
amount of voltage (say 5v), then KVL tells us that all of that voltage is gone when you get back to ground 
(0v). Where did it go? It got used up through each of the components in the loop. There is a voltage 
drop across each of the components, and those voltage drops add up, in this case, to 5v. How much voltage 
drops across each component? Ohm s Law tells us (combined with Kirchhoff s Current Law). If all the components 
in the loop are in series, then they all see the same amount of current (KCL tells us that the current 
can t sneak out through magic). If all the components in the loop see the same current, then the voltage 
across each component is related to Ohm s Law: V = I R. The voltage drop across a component is directly 
related to that component s resistance. This is Big Idea the basis of a voltage divider. A voltage divider 
is a series connection of resistors. KVL and KCL tell us that the resistors will see the same current, 
and the total voltage drop across both resistors will equal the total voltage. Ohm s law tells us that 
the drop across each resistor is directly related to its resistance. The circuit for a basic voltage 
divider is shown in Fig­ure 1.3. There are two resistors in series between power (Vdd) and ground (GND). 
The voltage will drop across both resis­tors in proportion to their relative resistances. Q: If Vdd is 
5v, R1 is 100O, and R2 is 200O, what is the volt­age at OUT (with respect to ground at the bottom of 
the circuit)? A: I = V /R for the whole circuit 5v/(100O + 200O) = .0167A V 1 = I × R1 = 0.167A × 100O 
= 1.667v (Vdd to Vout) V 2 = I × R2 = 0.167A × 200O = 3.333v (Vout to GND) Check result: 1.667 + 3.333 
= 5v (total drop) Voltage dividers are extremely useful circuits that allow us to divide a voltage into 
many parts, or to compute how much voltage is dropped across a speci.c component. We ll see them later 
when we talk about sensors! Another very useful practical result of Kirchhoff s Laws relates to current 
in a branching node. In a voltage divider, we use the fact that all the series-connected components see 
the same current -there s nowhere else for that current to go. But, if there is somewhere else for it 
to go, it gets split up in proportion to the resistance of the branches. This makes sense back in our 
water analogy: if there is a large input pipe (lots of current) branching into three smaller pipes (resistors), 
then the current is split in proportion to the diameters of the smaller pipes. If all the smaller pipes 
are the same diameter, the current is split into thirds. This will come in handy when we talk about things 
like LEDs where we want a speci.c amount of current to .ow through each component. Kirchhoff s and Ohm 
s laws can tell us exactly what we need to know!  1.4 Practicalities of Wiring and Powering Circuits 
Circuits can be wired up using any conductive material, but insulated wires are the easiest and most 
common materials used. A schematic tells you about the logical organization of the electrical components. 
It uses standard symbols that represent electrical components and lines that represent electrical nodes. 
It s a recipe for how you should assemble the physical circuit. The physical realization of that circuit 
could look very different, but as long as you connect the components into the same set of nodes and loop 
as in the schematic, it will have the same function as the schematic. For example, the following is an 
extremely simple schematic that describes two components connected in series: an LED (the triangle-ish 
component) and a resistor (the zig-zag). The schematic says that you should connect one terminal of the 
resistor to Pin 10 of the Arduino Controller, and the other end of the resistor to one end of the LED. 
Finally, the other end of the LED should be connected to GND (ground). To make this circuit you would 
.nd physical examples of all the main components (the Arduino Controller, the resistor, and the LED) 
and use wire to connect them together. This schematic doesn t specify the resistance value of the resistor, 
or the color or electrical speci.cations of the LED. In a real circuit that information would be annotated 
on the schematic, or in a supplemental document that references the component names R1 and D1. You could 
solder wires and components together to make a nice solid permanent bond, and in fact you probably want 
to do this for your .nished product. But, for prototyping and testing, it s nice to have a less permanent, 
and less complex solution. One way of doing this is with a solderless breadboard or prototyping board. 
This is a board that has lots of little holes in it in which you can poke a wire. Inside the board are 
connections such that all the wires plugged into one row will be electrically connected (that is, that 
row is one electrical node). Using these breadboards, you can prototype a circuit by plugging and unplugging 
wires and components into the holes, and you can quickly change things to try out new ideas, or .x bugs. 
In this breadboard, the vertical columns on either side of the board that are marked with red and blue 
lines are vertical buses. Any wire plugged into that column (e.g. the columns marked in Figure 1.6) will 
be connected to any other wire plugged into that column. The 1.4: Practicalities of Wiring and Powering 
Circuits 11 entire column is one electrical node. These columns are typically (but not always) used 
for power and ground connections, so they re marked with + and -in case you want to use them for that 
purpose. In this board, shown in Figure 1.5, the .ve holes in a row marked with abcde are also connected 
as a node. That is, any wire plugged into row 5, column a will be connected to any other wire in columns 
b, c, d, or e of that row. Likewise, the f, g, h, i, and j holes are also connected in a row. There is 
no connection across the valley in the middle of the board. This is designed so that integrated circuit 
packages (chips) can be placed in the board with legs on either side of the valley. All sorts of other 
components can also be placed in these breadboards. As a very simple example, Figure 1.7 shows the same 
circuit we saw before with the resistor and LED. In the physical version of this circuit there is a green 
wire connecting the GND connection of the board on the left (an Arduino board -we ll see that in a minute) 
to the blue bus column. The red LED has one leg in the blue bus column, and the other leg in row 5 column 
A. The resistor has one leg in row 5, column E (and is thus connected to the LED leg), and the other 
leg in row 8, column A. Also connected to row 8 (column A) is a purple wire going back to the blue board 
into the connection labeled 10. Although it doesn t look identical, this is a correct physical realization 
of the schematic using a solder­less breadboard. We ll see further examples of how breadboards are used 
to connect components as the course progresses. For now, there s another Big Idea: When you connect multiple 
devices together, all the ground terminals of all the devices MUST Big Idea be connected together. Ground 
is the zero-volt reference for your system. If you don t connect them all together, then they might all 
have different notions of zero volts. That s a very bad thing! Notice in the preceding extremely simple 
example, the GND connection of the board on the left is connected to a GND bus in the breadboard on 
the right. Nice!  1.5: The Arduino Platform 1.5 The Arduino Platform When we get to the part about 
interfacing electrical components to a computer controller, we need a computer controller to connect 
to. There are lots of choices here. You could use your desktop computer or your laptop, although those 
aren t really optimized for attaching physical computing components like LEDs, motors, and sensors. You 
could use a micro­controller chip directly -there are hundreds to choose from: ARM, Atmel AVR, Freescale 
68HC11, Intel 80C51, MIPS, PowerPC, Microchip PIC, or TI MSP430 just to name a very few. But, that s 
a pretty complex endeavor -these chips aren t at all trivial to connect in a system, and it can be tricky 
to .gure out how to upload programs once you have designed a system around them. The easiest solution 
is to .nd an embedded computing board that is already designed for you, along with programming support 
and data uploading and downloading infras­tructure. There are quite a few of these boards available. 
One popular option is the Ar­duino. The name Arduino corresponds to (at least) two different things: 
A small embedded controller board based on an AVR 8-bit micro (currently the ATmega328 on the common 
Duemilanove and Uno boards), and a programming environment (a somewhat basic In­tegrated Development 
Environment (IDE)). The IDE is designed to support the controller board and is based on gcc (actually 
a version of gcc called avr-g++). The Arduino web site introduces things in this way: Arduino is an open-source 
electronics prototyping platform based on .exi­ble, easy-to-use hardware and software. It s intended 
for artists, designers, hob­byists, and anyone interested in creating interactive objects or environments. 
Both parts of Arduino come originally from a group of folks in Italy led by Massimo Banzi but has since 
made the transition to a web-based open-source project (both open source SW and HW) with the main Arduino 
team scattered all over the world. The term open source hardware means that all the hardware design .les 
(schematics and board layouts) are available free of charge so that users can study, understand, and 
modify the hardware to their own liking. The main web site for all things Arduino is www.arduino.cc. 
It is from this site that you can download the of.cial Arduino IDE, and where all the of.cially supported 
soft­ware packages are available. It s also where some of the text in this document was bor­rowed from. 
It s de.nitely the .rst stop when looking for Arduino information. There is also an unof.cial site with 
even more web-community-supplied Arduino goodies at www.freeduino.org (titled The World Famous Index 
of Arduino &#38; Freeduino Knowl­edge ). I ll include a more comprehensive list of Arduino-friendly web 
sites a little later. An Arduino assembled from raw parts by hand will cost around $6. You can also easily 
buy pre-assembled boards for a reasonable price (around $30). One example is shown in Figure 1.8. This 
is that standard Arduino form factor and is about the size of a credit card. Other form factors include 
tiny Arduinos that .t in a breadboard (Arduino Nano and Arduino Mini), to really really tiny versions 
that are basically a backpack for the AT­mega328 chip (Ardweeny), to mega versions that use a bee.er 
version of the ATmega chip (the ATmega2560) that has twice the memory and lots more I/O pins, and even 
a version called the LilyPad which is designed to be sewn into clothing for wearable appli­cations. You 
can see an overview of the different of.cial boards at arduino.cc/en/ Main/Hardware, and more of the 
user-designed boards at arduino.cc/playground/ Main/SimilarBoards. The Arduino comes in a variety of 
versions, but they re all very similar in capability, and they all work with the Arduino IDE. On the 
Arduino board shown in Figure 1.8. The black connectors at the top of the picture are for 14 digital 
input/output pins (numbered 0 through 13). These pins can be used either as inputs (e.g. for sensing 
the voltage on a switch) or for outputs (e.g. for driving 5v or 0v to an LED) On the bottom of the picture 
are connectors for power (5v and 3.3v), Ground, and six analog inputs (numbered A0 through A5). Analog 
inputs are for sampling continuous voltages between 5v and 0v. We ll see in the sensor section how useful 
these are for environmental sensing. The silver box on the top left of the picture is the USB connection, 
used both to power the Arduino and to upload/download data. The black connector on the bottom left is 
an auxiliary power connector, and the large black chip is the microcontroller itself. This course isn 
t really about using the Arduino, although we will use it for controller examples The best thing about 
Arduino is that it s designed to be extremely simple to use with physical computing components. LEDs 
(Chapter 2) and servo controls (Chapter 4) can be connected directly to the digital I/O pins. Larger 
motors can be controlled through the digital I/O and a switching transistor (see Section 4). Environmental 
sensors can be connected easily through the analog inputs (Chapter 3). Programs can be written in C/C++ 
using the Arduino IDE and uploaded from your PC/Mac or laptop using a USB cable. All in all a very spiffy 
little system! 1.5.1 Arduino Electrical Overview These speci.cations may not make complete sense at the 
moment, but they should be a use­ful reference for later. You can also .nd information about speci.c 
Arduino boards (all of which may have slightly different specs) on the Arduino web site http://www.Arduino.cc. 
This overview is for the main generic Arduino boards that you re likely to see (e.g. the Duemilanove 
and Uno). Power to the Arduino board: You can supply power to your Arduino board either through the USB 
cable that you use for uploading code, or through a separate power supply 1.5: The Arduino Platform using 
a 2.1mm center-positive male connector. If you use the separate power supply you can use anything in 
the range of 7-12 volts, although 9v is usually considered the optimal power supply. Higher than 9v causes 
the voltage regulator on the Arduino to get warm (or hot!). Note that you can also use a 9v battery to 
power the Arduino. You can easily .nd 9v battery connectors (snaps) that have a 2.1mm center-positive 
male connector that mates with the power input of the Arduino. Voltage on digital pins: Each of the 14 
digital pins on the Arduino can be con.gured to be an output where the Arduino is driving the voltage 
on the pins, or an input where an external circuit is driving the voltage on the pins. In either case 
the range of accept­able voltages is 0v to 5v. Digital pin usage restrictions: Of the 14 digital pins, 
a few have potential restrictions or special uses: Pins 0 and 1 are also used for UART communication 
to the host so it s good practice to avoid using them for general circuit connections unless you need 
them. If you do need to use them for general I/O, it s sometimes necessary to disconnect them during 
program upload, and then connect the wires again after the program has been uploaded. You should not 
use them at all if your own program is using the Serial communication capability.  Pins 2 and 3 can 
also be used for external interrupts. This is a bit of an advanced feature that we won t cover, and you 
don t have to worry about using these as general I/O pins if you re not using the interrupt feature. 
 Pins 3, 5, 6, 9, 10, and 11 can be con.gured to provide 8-bit pulse width mod­ulation (PWM) using the 
analogWrite() function. If you re not using this function, you can use them as general I/O pins.  Pins 
10, 11, 12, and 13 are used for hardware-controlled SPI communication (Serial Peripheral Interface) using 
the SPI Library. If you re not using this library, or communicating to external devices using this protocol, 
you can use them as general I/O. As a side note, this restriction is only if you re using the hardware­supported 
SPI. There is also a software-only SPI library that uses any of the 14 digital pins.  Pin 13 has a built-in 
LED connected to it. Whenever you drive pin 13 high the LED will be lit, and when you drive pin 13 low, 
it will be off.  Current through digital pins: The digital pins on the Arduino can provide or receive 
a max of 40mA/pin. Providing or receiving current to a high voltage is known as sourcing the current, 
and providing/receiving current to ground is known as sinking the current. Additionally there is a limit 
of 200mA max total among all 14 pins. This means, for example, that you can t simultaneously sink 40mA 
from all 14 pins! Voltage on analog inputs: The analog to digital converter (ADC) that is receiving the 
ana­log inputs on pins A0 through A5 can handle voltages in the range of 0v to 5v. The Arduino s ADC 
has a resolution of 10 bits which means that the voltage range of 0v to 5v is mapped to digital values 
from 0 to 1023. Analog pin usage restrictions: Analog pins A4 and A5 are used for I2C (also known as 
the two-wire interface (TWI)) communication with external devices using the Wire library. If you re not 
using this communication library, or communicating to external devices using this protocol, you can use 
them as general analog inputs. Power for external devices: The Arduino can pass through the power signal 
for use with external devices through 5v and 3.3v pins. These power signals come from on-board voltage 
regulators on the Arduino. If you re using the USB connection for power there is a max of 500mA total 
for the entire system including the Arduino and ex­ternal devices (limited by the USB speci.cation). 
If you re using the 2.1mm power connector and a 7-12v power adaptor the max power around 800mA (perhaps 
a little more if you attach a heat sink to the regulator). The 3.3v connection provides a max of 50mA. 
This is smaller because of the limit on the separate voltage regulator used to generate the 3.3v signal. 
Arduino Memory: The ATmega328p that is the microcontroller on the Duemilanove and Uno boards is an 8-bit 
processor, but the C/C++ compiler handles the use of larger data types when it generates code. This microprocessor 
has 32KB of .ash memory for storing code, of which 2KB is used by the boot loader (0.5KB on the Uno). 
This means that your code s binary (produced by the Arduino IDE compiler) must .t in the re­maining Flash 
memory space (30KB on Duemilnove, 31.5KB on Uno). It also means that because the code is in .ash, once 
you upload your code to the Arduino it stays there even when you cycle power. The ATmega328p also has 
2KB of SRAM (used for variables by the compiler) and 1KB of EEPROM (accessible using the EEPROM library). 
 1.5.2 Arduino Software Overview The Arduino software environment consists of two main parts: an integrated 
development environment (IDE), and a set of built-in functions and libraries that allow the user to eas­ily 
write C and C++ programs that interact with the I/O ports on the Arduino hardware. The Arduino integrated 
development environment (IDE) contains a text editor for writing code, a message area, a text console, 
a toolbar with buttons for common functions, and a series of menus. It connects to the Arduino hardware 
to upload programs and commu­nicate with them. An image of the Arduino IDE interface, with a very simple 
example program, is shown in Figure 1.9. The text editor uses syntax coloring to help identify parts 
of your program syntax. As you can see in the .gure, C and C++ keywords (including the built-in functions 
added by Arduino) are colored orange. Constants that are de.ned in the included header .les are colored 
blue. Other program text is black. The IDE is free and available on the www.arduino.cc web site. It runs 
on Windows, Mac, and Linux. One note: in Arduino-speak programs are known as sketches. This means that 
your sketches go in your sketchbook folder when you save them. I m not fond of this terminology so I 
ll continue to call them programs. This section is a non-comprehensive overview of the Arduino software 
environment. For a more complete reference see the Arduino web site, or one of the many books available 
on Arduino programming. These notes will include code listings as the electrical components are introduced. 
The control buttons on the Arduino IDE are the following: Verify/Compile This runs the gcc compiler 
on your program and returns any er­rors or warnings in the status area at the bottom of the window. Stop 
 Stops the serial monitor (a text window opened on the host), or unhighlights other buttons. 1.5: The 
Arduino Platform New Creates a new blank program (sketch) in the IDE editor. Open Presents a menu 
of all the sketches in your sketchbook (all the programs in your local program directory). Clicking one 
will open it in the current window. On some versions os the Arduino IDE (due to a Java bug), this menu 
will not scroll. So, if you need to open a program that is late in the list, use the File . Sketchbook 
menu instead. Save Save your program. The default location (sketchbook) used to save programs is set 
in the preferences under the Arduino menu. Upload to your Arduino board This button compiles your code 
and uploads it to your Arduino board through the USB interface. Serial Monitor This opens the serial 
monitor on the host. This is a text-based win­dow that can be printed to using Arduino functions print(value) 
and println(value), and that can be used to send ASCII data back to the Arduino. Additional commands 
are found in the .ve menus: File, Edit, Sketch, Tools, and Help. These are documented in the Arduino 
IDE and on the Arduino web site. Some of the more important menu choices are: File Sketchbook This opens 
a menu into your local program directory so that you can select one of your previously saved programs 
(sketches). Examples This contains the example programs that come with the IDE. These can be very useful 
to see how various libraries are used. Edit Copy Copy program text for pasting into another program Copy 
for Forum Copy in a format suitable for posting to the Arduino form including syntax coloring Copy as 
HTML Copy in HTML format suitable for embedding in web pages. Sketch Show Sketch Folder Open a window 
to the folder that contains the program (sketch) you re currently working on. Import Library Add the 
appropriate #include< ... > lines for libraries that are in your path. Tools Board Select which Arduino 
board you re going to compile for, and upload to. Port Select which USB port you will be using to upload 
the program to the Arduino board. Help These menu choices open a browser onto the arduino.cc page that 
has information about the listed topic. The Reference topic, for example, opens the page that has the 
Arduino built-in function reference. The standard programming environment for Arduino is C/C++ through 
the Arduino IDE and avr-g++. The basic C++ language is augmented with a set of functions that are built 
in to the Arduino IDE. 1.5: The Arduino Platform Arduino application structure: Arduino requires that 
all programs (sketches) consist of two top-level functions. These functions can be seen in Figure 1.9 
that shows a very simple Arduino program. The functions are: void setup() -called exactly once to set 
things up. This function is typically used to set the direction of digital pins (input or output), and 
initialize libraries if needed. This function never returns a value so it is always declared as returning 
void.  void loop() -runs and loops forever after setup() .nishes. This is a fun­damental part of the 
reactive nature of a typical Arduino application. These applications either repeat their behavior over 
and over, or repeatedly wait in a forever-loop for some external event and then react to that event. 
The exter­nal event is often waiting for some value on the external pins, either digital or analog. This 
function also never returns a value. In fact, it never returns at all because its behavior is to be an 
endless loop. It is also always declared to return void.  An exception (no pun intended) to putting 
the program behavior in the loop() function is if you re using interrupts. If the program activity is 
all handled in the interrupt service routines, it s possible to have an empty loop() function, but you 
still have to have that empty function in your program. Arduino data types: These are basically C/C++ 
data types, but remember that the C stan­dard is purposely vague on certain things, like the bit width 
of an int. The basic data types are listed below for the Duemilinove and Uno boards, but there are more 
(e.g. unsigned types). boolean (1 bit -true/false)  char(signed, 8-bits)  byte (unsigned, 8-bits) 
 int (signed, 16 bits)  long (signed, 32 bits)  float (32 bit fp)  double (also 32 bit fp)  Arduino 
functions for digital pins: These functions all operate on any of the 14 digital pins (numbered 0 through 
13). Recall that C/C++ is case sensitive, so be careful with case in the function names. pinMode(pin 
number, mode)  Sets the direction of that digital pin  mode can be INPUT, OUTPUT, or INPUT PULLUP 
 If mode is INPUT PULLUP an internal pullup resistor is enabled on the in­put. More on this later in 
Chapter 3.  Typically used within the setup() function.   digitalRead(pin number)  Reads the voltage 
currently on that digital pin (pin should be con.gured to INPUT mode)  Returns HIGH (1) if the voltage 
on the pin is around 3v or greater (up to 5v)  Returns LOW (0) if the voltage on the pin is around 2v 
or lower   digitalWrite(pin number, value) value can be HIGH (1) in which case the pin is forced to 
5v, or or LOW (0) in which case the pin is forced to 0v (pin should be con.gured to OUTPUT mode)  analogWrite(pin 
number, value)  This function is only meaningful on digital pins 3, 5, 6, 9, 10, and 11 on the Duemilanove 
and Uno boards  value is a number between 0 (fully off) and 255 (fully on).  A number between 0 and 
255 will vary the duty cycle of the pulse width modulation on the pin which acts as a partial on value 
in a variety of cases   Arduino functions for analog pins: these pins are connected through a 10-bit 
analog to digital converter (ADC) so that analog voltages between 0v and 5v on the pins will return an 
int between 0 and 1023. analogRead(pin number) Analog pins may be referred to as pin A0 through A5, and 
it s good practice to do so to differentiate them from the digital pins Arduino functions for delays 
and timing: These functions are used to deal with timing in Arduino programs. millis()  Returns the 
number of milliseconds (thousands of seconds) since the pro­gram started as an unsigned long  This number 
over.ows (goes back to 0) after about 50 days of running   micros()  Returns the number of microseconds 
(millionths of a second) since the pro­gram started running as an unsigned long  This number over.ows 
after about 70 minutes  On a 16MHz processor like the Duemilanove and Uno this has a resolution of 4 
microseconds (i.e. the number is always a multiple of four)   delay(ms)  Delay the program by busy-looping 
for ms milliseconds  Argument can be up to an unsigned long   delayMicroseconds(us)  Delay the program 
by busy-looping for us microseconds.  Argument is an int, and the largest value for which an accurate 
delay can be produced is 16383  Accuracy isn t guaranteed for arguments of 3 or less   Arduino functions 
for communication to host: The Arduino has a UART on-board that it uses to communicate with the host. 
This UART is used for uploading the program code, and also for general communication with the host once 
the program is running. The Arduino IDE has a built-in terminal that can be used to display data from 
the Arduino, and for the user to send characters from the host to the Arduino. Note that it looks like 
you ve connected your Arduino to the host with a USB cable, and you have. But, the Arduino is actually 
communicating with a serial protocol layered on top of the USB connection. From the Arduino programmer 
s point of view, it s just a serial connection. 1.5: The Arduino Platform Serial object -The Serial 
object is pre-de.ned in the Arduino IDE, and all the functions used for serial communication are methods 
called on that object. There are many more methods than the ones described here.  Serial.begin(speed) 
 This is the baud rate for the serial communication.  Legal values are 300, 600, 1200, 2400, 4800, 
9600, 14400, 19200, 28800, 38400, 57600, or 115200.  Make sure the terminal you re connecting to (e.g. 
the serial monitor in the Arduino IDE) is set to the same baud rate  This is typically called in setup() 
   Serial.print(arg)  Prints data to the serial port as human-readable ASCII text. This command can 
take many forms.  Numbers are printed using an ASCII character for each digit.  Floats are similarly 
printed as ASCII digits, defaulting to two decimal places.  Bytes are sent as a single character.  
Characters and strings are sent as is.  An optional second parameter speci.es the base (format) to use; 
 permitted values are BIN, OCT, DEC, and HEX  For .oating point numbers, this parameter speci.es the 
number of decimal places to use.   Serial.println(arg) Same as Serial.print(arg), but follows the 
printed data with a carriage return character (ASCII 13, or \r ) and a newline character (ASCII 10, or 
\n )  Serial.available()  Returns the number of bytes (characters) available for reading from the se­rial 
port.  This is data that s already arrived and stored in the serial receive buffer (which holds 64 bytes) 
  Serial.read()  Returns the .rst byte of incoming serial data from the serial receive buffer (as 
an int)  Returns -1 if no data is available   Arduino helper functions: There are quite a few here, 
but I ll just mention a few that we ll see in the next sections. random(min, max) Returns a pseudo-random 
number between min and max-1  map(value, fromLo, fromHi, toLo, toHi)  Remaps (interpolates) the value 
argument from the range (fromLo, fromHi) to the range (toLo, toHi)  Does NOT constrain the value to 
be within the to range  The lower bounds of either range can be higher or lower than the other bounds. 
You can use that to reverse the range of a number.  Uses integer math so it will not generate fractional 
values   constrain(value, a, b) Constrains the value argument to be between a and b (inclusive)  
shiftOut(data pin, clock pin, bit order, value)  Sends 8-bits of data on a serial (SPI) link.  data 
pin: the pin on which to output each bit (int)  clock pin: the pin to toggle once the data pin has been 
set to the correct value (int)  bit order: which order to shift out the bits; either MSBFIRST or LSBFIRST. 
(Most Signi.cant Bit First, or, Least Signi.cant Bit First)  value: the data to shift out. (byte)  
   Chapter 2 Lights! LEDs There are a lot of different components you could use to generate light, 
but LEDs are a great choice because they re cheap, and they use very little power to generate light. 
An incandescent light bulb, for example, takes much more current than an Arduino pin can provide, but 
an Arduino digital output pin can easily provide enough current to light up an LED. Plus they come in 
a wide variety of shapes, sizes, and colors. They re a great way to add some .ash to your projects. LED 
stands for Light Emitting Diode. They are an example of a more general type of semiconductor device called 
simply a diode. A diode is essentially a one-way valve for current. That is, current can .ow one direction 
through the diode, but not the other. They re used in a variety of situations where the circuit designer 
wants to make sure that current is only .owing in one direction and never backing up in the other direction. 
Big Idea # 8 The two terminals of a diode (light emitting, or not) are the anode and the cathode. Current 
can .ow from anode to cathode, but not the other way, except in certain exceptional situations. Because 
they are directional components, there should be some way of identifying which terminal is which in a 
schematic. The schematic symbol for a diode looks like the left hand side of Figure 2.1. The anode (input) 
is on the left, and cathode (output) is on the right with the vertical bar. Current can .ow in the direction 
of the triangle, and not back through the bar. Physically, general diodes typically look a bit like resistors 
-they are little cylinders with wires attached to the ends. Instead of colored bands to indicate value, 
diodes typically have tiny numbers printed on them. There is also usually a single band on the cylinder 
that indicates the cathode end. Think of the band on the diode as marking the position of the bar on 
the diode symbol. An LED is a diode, so it behaves like a regular diode with respect to current .ow. 
But, it has the added feature that when current does .ow from anode to cathode, it lights up. So, the 
schematic symbol looks like a diode, but adds an indication of the light-emitting feature. It looks like 
the right hand side of Figure 2.1. Physically, LEDs come in a wide variety of shapes, sizes, and colors. 
One important consideration is how to .gure out which lead is the anode, and which is the cathode. In 
a standard through-hole LED (i.e. one that is designed to be inserted through the holes in a circuit 
board), one lead will be longer than the other. The longer lead is the anode, and the shorter is the 
cathode. The good news is that unless you apply a very large voltage to the leads of the LED you can 
t really hurt it by putting it in a circuit backwards. That s what diodes are designed for after all, 
to block current in the reverse direction. So, if you don t remember which lead is which, or if you ve 
cut the leads of the LED to the same length, you can just try it both ways and see which way lights up. 
 An important electrical consideration for any diode, LEDs included, is the forward volt­age. A diode 
does indeed conduct from anode to cathode, but only after the voltage dif­ference from anode to cathode 
is raised above a certain voltage, known as the forward voltage, or Vf . For many regular diodes, the 
forward voltage is 0.7v. For an LED, the for­ward voltage is higher: often between 1.5v and 3.5v. Red 
LEDs typically have the lowest forward voltage (1.7v is a fairly typical value), and in general the forward 
voltage goes up as the frequency of the emitted light goes up. So, Vf goes up as LEDs move from up the 
spectrum (red, orange, yellow, green, blue) with blue LEDs having a Vf of 3.0 to 3.5v. Of course, the 
best plan is to get the specs for the LEDs you plan to use. Big One rami.cation of the forward voltage 
is that the LED doesn t conduct until the voltage dif­ference between anode and cathode gets above that 
value. Another is that because of that behavior, the diode uses up that much voltage once it does conduct. 
That is, if you put 5v between anode and cathode of an LED with a 2.0v Vf, the voltage at the cathode 
will be 3v with respect to ground. Said in another way, the LED has a 2.0v voltage drop across the diode. 
Another important consideration with respect to diodes is how much current the diode can support. Regular 
diodes come in a huge variety of current capacities from milliamps to hundreds or thousands of amps. 
Regular LEDs, on the other hand, are almost always designed to produce maximum light output at around 
20mA, and look good when the cur­rent is between 10-20mA. You can get super-bright high-powered LEDs 
that are designed 25 for 800mA or 1000mA (i.e. up to 1A), but the generic inexpensive 3mm or 5mm LEDs 
that you re likely to encounter will almost certainly be designed for around 20mA. One question that 
is critical for using LEDs is how to make sure that the current you re using to drive the LED is limited 
to the right value. This is a perfect application for Ohm s law from Chapter 1. If we know the total 
voltage we re putting through the LED, the forward voltage Vf , and the desired current, we can use Ohm 
s law to compute the correct resistor value for that circuit. Example: If the Arduino digital output 
pin drives to 5v, and the LED has a Vf of 2.0v, and a current limit of 20mA, then we can compute the 
resistor required to make this all work. Remember that the Vf is subtracted from the total voltage (used 
up by the LED), and 20mA is 0.020A. Ohm s law uses whole-unit values for Ohms, Volts, and Amps. R = Vtotal/I 
R = (Vsource - Vf )/I R = (5.0v - 2.0v)/0.020A R = 150O For this example, you would put a 150O resistor 
in series with the LED to limit the current to the 20mA as shown in Figure 2.3. LEDs always need a current-limiting 
resistor. Never connect an LED without computing what Big the current limiting resistor should be. The 
only exception to this rule is for special LED drivers that have constant current outputs (we ll see 
some in the next sections). The value of the resistor can be computed using Ohm s law and the forward 
voltage of the LED using the equation R = (Vsource - Vf )/Idesired . Most generic LEDs work well with 
between 10mA to 20mA of current. When in doubt, a resistor in the range of 220O to 470O will usually 
do the trick and be safe. As an aside, those odd resistance values represent standard resistance values. 
You can special-order resistors in any ohm rating that exists, but there are a set of values that are 
considered standard and that are readily available at any electronics shop. Some standard values that 
are useful for typical current-limiting applications with standard LEDs are 150, 180, 220, 330, and 470O. 
Recall from Section 1.5.1 that each pin of the Arduino can supply up to 40mA. This means that each of 
the 14 digital pins can light up an LED, and you can control the on/off of the LEDs using digitalWrite() 
functions. Remember, though, that the max current for the entire Arduino is 200mA. So, if you have 10 
LEDs and you will be lighting them up all at once, you ll need to limit the current to 20mA or less per 
LED to avoid hitting that max current limit!  Figure 2.4: Driving an LED from an Arduino pin using digitalWrite(). 
If the pin is driven high, the LED s anode voltage is higher than the cathode, so current .ows and the 
LED lights up. If the pin is driven LOW, the anode and cathode are at the same voltage (ground) so no 
current .ows and the LED is not lit. Remember that the voltage difference must be greater than the forward 
voltage (Vf ) in order for current to .ow. Vf is typically between 1.5v and 3.5v depending on the LED. 
In this example, the voltage difference is +5v, which is plenty. Figure 1.9 shows a screen shot of the 
Arduino IDE with the blink program. This pro­ gram, the default program loaded in a new Arduino board, 
sets up digital pin 13 as an output in the setup() function, and then .ashes the LED connected to that 
pin once per second. The digitalWrite() functions set the pin to a high or low voltage, and the delay() 
function spin-waits and delays the action by the given length of time (1000ms, or 1sec). Pin 13 is a 
special digital pin in that it has an LED (with the required current-limiting resistor) built into the 
board, so no external wiring is needed to run this program and see the .ashing LED output. If you wanted 
to repeat this program, but with a different pin, you would need to wire up an external LED and resistor 
and connect them to another of the Arduino s pins. This is what is shown in Figure 1.7 with Arduino digital 
pin 10. The only difference required to the blink program in Figure 1.9 is to change the value of the 
led variable to 10. 2.1 Multiple LEDs driven from digital pins One LED is .ne as a starting point, but 
what about multiple LEDs? You could connect a different LED, each with its own current-limiting resistor, 
to each of the 14 digital pins. You should make sure that you understand the amount of current each LED 
is consuming so that you don t approach the 200mA Arduino current limit when all the LEDs are lit. You 
can do this by choosing your current-limiting resistor carefully. The picture in Figure 2.5 shows an 
Arduino with eight LEDs and eight resistors connected to eight of the Arduino s pins. The resistors are 
chosen so that each LED is consuming approximately 13mA of current at 5v (the resistors in the picture 
have bands of brown, red, and red making them 220O resistors). So, even if all eight are turned on at 
one time, the total current will be only 96mA. There are many ways you could write an Arduino program 
to light up these LEDs. The easiest would be just to write individual digitalWrite() commands for each 
LED. 2.1: Multiple LEDs driven from digital pins That program would look something like this (additional 
Arduino example programs are included in the auxiliary materials for this course): / * * E i g h t -LED 
e x a m p l e # 1 *  * T h i s e x a m p l e t a k e s a b r u t e -f o r c e a p p r o a c h t o m 
a k i n g p a t t e r n s * a p p e a r o n t h e 8 LEDs . *  * N o t e t h a t t h e p i n s u s e 
d f o r t h e LEDs a r e 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 . * T h i s i s b e c a u s e p i n s 0 a n d 
1 a r e a l s o s h a r e d w i t h t h e * c o m m u n i c a t i o n p i n s t h a t a l l o w A r 
d u i n o t o t a l k w i t h t h e PC . * You c o u l d s t i l l u s e t h e m , b u t y o u m i g 
h t h a v e t o d i s c o n n e c t * t h e m d u r i n g p r o g r a m m i n g , a n d t h e n c o 
n n e c t t h e m a g a i n . *  * s e t u p ( ) i s t h e f u n c t i o n t h a t r u n s o n c e t 
o s e t t h i n g s up . We l l * u s e i t t o s e t a l l o f t h e LED p i n s t o b e OUTPUTs 
 * / v o i d s e t u p ( ) {pinMode ( 2 , OUTPUT) ; / / D e f i n e a l l 8 p i n s a s o u t p u t s 
. . . pinMode ( 3 , OUTPUT) ; pinMode ( 4 , OUTPUT) ; pinMode ( 5 , OUTPUT) ; pinMode ( 6 , OUTPUT) ; 
pinMode ( 7 , OUTPUT) ; pinMode ( 8 , OUTPUT) ; pinMode ( 9 , OUTPUT) ; d i g i t a l W r i t e ( 2 , 
HIGH ) ; / / S t a r t w i t h t h e LED o n p i n 2 HIGH (ON) } / * l o o p ( ) i s t h e f u n c t 
i o n t h a t r e p e a t s t h e A r d u i n o s b e h a v i o r o v e r a n d * o v e r when t h e 
p o w e r i s o n . * / v o i d l o o p ( ) {i n t delayMS = 1 0 0 ; / / V a r i a b l e t o h o l d 
t h e t i m e ( i n m i l l i s e c o n d s ) t o / / d e l a y b e t w e e n p a r t s o f t h e LED 
p a t t e r n . / / T h i s s e t o f d i g i t a l W r i t e ( ) c o mm a nds m a k e s ( h a l f o 
f ) t h e / / C y l o n e y e b e h a v i o r . / / S t a r t b y g o i n g f r o m p i n 2 LED t o 
p i n 9 LED . / / N o t e t h a t t h e f i r s t t i m e t h r o u g h l o o p ( ) , p i n 2 i s a 
l r e a d y HIGH d e l a y ( delayMS ) ; / / W a i t f o r d e l a y M S m i l l i s e c o n d s d i 
g i t a l W r i t e ( 2 , LOW) ; / / s e t p i n 2 LED OFF d i g i t a l W r i t e ( 3 , HIGH ) ; / / 
s e t p i n 3 LED ON d e l a y ( delayMS ) ; / / W a i t f o r d e l a y M S m i l l i s e c o n d s 
d i g i t a l W r i t e ( 3 , LOW) ; / / s e t p i n 3 LED OFF d i g i t a l W r i t e ( 4 , HIGH ) ; 
/ / s e t p i n 4 LED ON d e l a y ( delayMS ) ; / / W a i t f o r d e l a y M S m i l l i s e c o n 
d s d i g i t a l W r i t e ( 4 , LOW) ; / / e t c d i g i t a l W r i t e ( 5 , HIGH ) ; d e l a y ( 
delayMS ) ; d i g i t a l W r i t e ( 5 , LOW) ; d i g i t a l W r i t e ( 6 , HIGH ) ; d e l a y ( delayMS 
) ; d i g i t a l W r i t e ( 6 , LOW) ; d i g i t a l W r i t e ( 7 , HIGH ) ; d e l a y ( delayMS ) 
; d i g i t a l W r i t e ( 7 , LOW) ; d i g i t a l W r i t e ( 8 , HIGH ) ; d e l a y ( delayMS ) ; 
d i g i t a l W r i t e ( 8 , LOW) ; d i g i t a l W r i t e ( 9 , HIGH ) ; d e l a y ( delayMS ) ; 
/ / F o r t h e f u l l c y l o n e y e y o u w o u l d n e e d t o l i g h t up t h e LEDs / / i n r 
e v e r s e o r d e r h e r e / / e n d o f l o o p ( ) - g o b a c k a n d s t a r t a g a i n } A slightly 
fancier version of this program makes use of a separate function to set the state of all eight LEDs based 
on the value expressed as an 8-bit byte. / * * * T h i s e x a m p l e u s e s a s e p a r a t e f u 
n c t i o n t o s e t t h e LED o u t p u t s . T h i s * new f u n c t i o n c a n b e c a l l e d 
b y t h e u s e r e a c h t i m e t h e LED s h o u l d b e * set . * T h i s e x a m p l e u s e s 
a r e l a t i v e l y a d v a n c e d l o w-l e v e l t e c h n i q u e . I t * c o d e s t h e LED 
v a l u e s i n a s i n g l e b y t e w i t h e a c h b i t o f t h e b y t e * b e i n g t h e 1 o 
r 0 t h a t d e t e r m i n e s t h e LED ON/ OFF s t a t e . T h i s * r e q u i r e s t h e s e t 
L E D s ( ) f u n c t i o n t o p i c k o f f e a c h b i t o f t h e b y t e i n * t u r n . *  * 
/ / * * D e f i n e t h e a r r a y t o h o l d t h e LED p i n n u m b e r s . I t s d e f i n e d 
h e r e * o u t s i d e a l l t h e f u n c t i o n s s o t h a t i t s a g l o b a l v a r i a b 
l e  * a n d c a n b e s e e n b y a l l f u n c t i o n s .  * / i n t l e d P i n s [ ] = { 2 , 3 
, 4 , 5 , 6 , 7 , 8 , 9 } ; / / An a r r a y t o h o l d t h e A r d u i n o p i n / / n u m b e r s 
t h a t e a c h LED i s // connected to. / * * s e t u p ( ) i s t h e f u n c t i o n t h a t r u n 
s o n c e t o s e t t h i n g s up . * / v o i d s e t u p ( ) { / / D e f i n e a l l 8 p i n s a s 
o u t p u t s . . . / / T h e p i n s a r e r e f e r e n c e d t h r o u g h t h e l e d P i n s a r 
r a y f o r ( i n t i = 0 ; i <8; i + + ) { / / l o o p e i g h t t i m e s pinMode ( l e d P i n s 
[ i ] , OUTPUT) ; / / s e t e a c h LED p i n t o OUTPUT }} / / e n d o f s e t u p ( ) 2.2: Diming an 
LED with pulse width modulation / * l o o p ( ) i s t h e f u n c t i o n t h a t r e p e a t s t h e 
A r d u i n o s b e h a v i o r f o r e v e r * w h i l e t h e p o w e r i s o n . * / v o i d l o 
o p ( ) {i n t delayMS = 1 0 0 ; / / V a r i a b l e t o h o l d t h e t i m e ( i n m i l l i s e c 
o n d s )  / / t o d e l a y b e t w e e n p a r t s o f t h e LED p a t t e r n . / / T h i s s e t 
o f s e t L E D ( ) c o mm a n ds m a k e s t h e C y l o n e y e b e h a v i o r / / / / T h i s v 
e r s i o n t a k e s a s i n g l e b y t e a s a n a r g u m e n t t o t e l l s e t L E D s / / w h 
a t t h e ON/ OFF s t a t e i s i n e a c h s t e p o f t h e p a t t e r n . T h e A r d u i n o / / 
s y n t a x f o r t h i s i s B 1 0 1 0 1 0 1 0 f o r a s i n g l e b y t e v a l u e . / / T h e r e 
 s a s u b t l e t y h e r e - B 0 0 0 0 0 0 0 1 m e a n s t h a t b i t 0 o f t h e b y t e // is 1. 
 s e t L E D s ( B 0 0 0 0 0 0 0 1 , delayMS ) ; s e t L E D s ( B 0 0 0 0 0 0 1 0 , delayMS ) ; s e 
t L E D s ( B 0 0 0 0 0 1 0 0 , delayMS ) ; s e t L E D s ( B 0 0 0 0 1 0 0 0 , delayMS ) ; s e t L E 
D s ( B 0 0 0 1 0 0 0 0 , delayMS ) ; s e t L E D s ( B 0 0 1 0 0 0 0 0 , delayMS ) ; s e t L E D s ( 
B 0 1 0 0 0 0 0 0 , delayMS ) ; s e t L E D s ( B 1 0 0 0 0 0 0 0 , delayMS ) ; s e t L E D s ( B 0 1 
0 0 0 0 0 0 , delayMS ) ; s e t L E D s ( B 0 0 1 0 0 0 0 0 , delayMS ) ; s e t L E D s ( B 0 0 0 1 0 
0 0 0 , delayMS ) ; s e t L E D s ( B 0 0 0 0 1 0 0 0 , delayMS ) ; s e t L E D s ( B 0 0 0 0 0 1 0 0 
, delayMS ) ; s e t L E D s ( B 0 0 0 0 0 0 1 0 , delayMS ) ; s e t L E D s ( B 0 0 0 0 0 0 0 1 , delayMS 
) ; / / T h i s i s t h e e n d o f t h e c y l o n p a t t e r n . R e m e m b e r t h a t l o o 
p ( ) / / s t a r t s o v e r a f t e r i t s d o n e , s o t h e p a t t e r n r e p e a t s . } / 
/ e n d o f l o o p ( ) - g o b a c k a n d s t a r t a g a i n / / T h i s i s t h e f u n c t i o n 
t h a t a c t u a l l y a p p l i e s t h e p a t t e r n t o t h e LED / / a n d t h e n d e l a y s 
f o r t h e s p e c i f i e d a m o u n t o f t i m e . / / T h i s v e r s i o n t a k e s a s i n g 
l e b y t e a s i n p u t t h a t h o l d s t h e ON/ OFF / / v a l u e s f o r t h e LEDs . I t l o 
o p s t h r o u g h e a c h b i t o f t h a t b y t e t o s e t / / t h e LED v a l u e s . T h e LED 
p i n n u m b e r s a r e h e l d i n t h e g l o b a l L E DPins / / a r r a y . v o i d s e t L E D 
s ( b y t e L EDvalues , i n t delayMS ) {/ / You c a n a c c e s s e a c h b i t o f t h e L E D v a 
l u e s b y t e u s i n g t h e / / A r d u i n o s y n t a x : b i t R e a d ( nu m b e r , w h i c 
h B i t ) ; f o r ( i n t i = 0 ; i <8 ; i + + ) { / / l o o p 8 t i m e s - i = 0 , 1 , 2 , 3 , 4 , 
5 , 6 , 7 d i g i t a l W r i t e ( l e d P i n s [ i ] , b i t R e a d ( L EDvalues , i ) ) ; } / / 
Now w a i t f o r d e l a y M S m i l l i s e c o n d s s o y o u c a n s e e t h e c h a n g e d e l 
a y ( delayMS ) ; } / / End o f s e t L E D s ( ) 2.2 Diming an LED with pulse width modulation An interesting 
side note on LEDs is that they re not easily dimmable. With an incandescent light bulb if you turn the 
voltage down, the bulb gets dimmer, and turning the voltage up results in a brighter light (until you 
go too far and the bulb burns out). This works for incandescent bulbs driven by either DC or AC power. 
LEDs work differently. They start to conduct current when the voltage exceeds the Vf forward voltage, 
but changing the voltage has no effect on the light output because they re current-controlled devices 
(once the voltage is higher than Vf ). There are minor variations in brightness as you increase the current, 
but mostly they re either on or off. By the way, it s easy to burn out an LED by putting too much current 
through -another reason for using a current-limiting resistor! Big The good news about LEDs is that 
they turn on and off really fast. So, if you turn them on and off very quickly, they can look dimmer 
to our eyes. Our eyes are not as quick as an LED, so if they .ash quickly they look on, but dim. This 
technique is known a pulse width modulation or PWM. Using PWM the signal is pulsed on and off very quickly 
(on the order of 500Hz for Arduino s PWM). This can simulate the effect of being at an intermediate voltage 
between 0v and 5v by adjusting the percentage of time that the PWM pulses are high and low. Arduino has 
a function that controls this type of pulse width modulation called analogWrite(pin, value). The pin 
argument is the digital pin to control. This must be one of the PWM pins 3, 5, 6, 9, 10, and 11. The 
value is an int between 0 (fully off) and 255 (fully on). If the value is somewhere between 0 and 255, 
the PWM signal will be mod­ ulated to simulate a voltage between 0v and 5v by adjusting the pulses. When 
connected to an LED, this can provide a wide range of apparent brightness for that LED. Note that analogWrite() 
is ONLY usable on digital pins, not analog pins, and doesn t really pro­ duce an analog voltage, just 
a simulation of the analog voltage using PWM. Analog pins are used only as inputs on the Arduino. Here 
s a snippet of code that fades the LED from full off to full on and back again using analogWrite(): / 
* U s e a n a l o g W r i t e ( ) t o f a d e a n LED * / i n t l e d P i n = 1 0 ; / / LED o n p i n 
1 0 ( l i k e F i g u r e 4 ) v o i d s e t u p ( ) {pinMode ( l e d P i n , OUTPUT) ; / / s e t p i 
n 1 0 a s o u t p u t } v o i d l o o p ( ) { / / f a d e LED f r o m min t o max i n i n c r e m e n 
t s o f 5 s t e p s f o r ( i n t v a l = 0 ; v a l <2 5 6 ; v a l + = 5 ) {a n a l o g W r i t e ( l 
e d P i n , v a l ) ; / / s e t s t h e v a l u e ( f r o m 0 t o 2 5 5 ) d e l a y ( 3 0 ) ; / / w a 
i t a b i t t o s e e t h e e f f e c t } / / f a d e LED f r o m max t o min i n i n c r e m e n t s 
o f 5 s t e p s f o r ( i n t v a l = 2 5 5 ; v a l >=0; v a l -=5) {a n a l o g W r i t e ( l e d P 
i n , v a l ) ; / / s e t s t h e v a l u e ( f r o m 0 t o 2 5 5 ) d e l a y ( 3 0 ) ; / / w a i t a 
b i t t o s e e t h e e f f e c t }} 2.3 Driving lots of LEDs directly from Arduino Causing an LED to 
.ash, or a handful of LEDs to .ash in a pattern, is fun, but what if you wanted to .ash a lot of LEDs? 
What s a lot? 50? 100? 1000? The previous example showed each LED connected directly to a digital pin 
of the Arduino. Using this technique you could light up a maximum of 14 LEDs. You could technically light 
up more LEDs by attaching multiple LEDs to each pin of the Arduino. Remember Kirchhoff s current law 
says that components connected in series see the same current. So, if you connected two LEDs in series, 
they would each see the same amount of current sourced from the Arduino pin. But, remember that each 
LED also uses up Vf of the total voltage. So, if the Arduino 2.4: Driving lots of LEDs using external 
LED-driver chips drives a digital output to 5v, and the Vf of your LED is, say, 2v, then you can t put 
more than two LEDs in series because each one will use 2v, and after two of them are in series, there 
s only one volt left. This is not enough Vf to turn on the LED and let current pass through. Also, both 
LEDs would be on and off at exactly the same time because they re connected to the same digital pin, 
so it provides more light, but not extra visual complexity. If you would like to .gure out how to connect 
multiple LEDs together in series and parallel connections, there are a variety of LED calculators on 
the web that do just that. You can tell these web-based applications how many LEDs you would like to 
connect, what the power supply voltage is, what the Vf of the LEDs is, and the application will tell 
you what current-limiting resistor to use, and how to connect the LEDs in a series/­parallel circuit. 
Example of web based LED calculators are ledcalculator.net/ and led.linear1.org/led.wiz. Examples of 
using the .rst calculator listed are seen in the following Figure. I ve speci.ed a 5v power supply, Vf 
of 1.9v, I want 18mA of current, and would like .ve LEDs. The tool shows me a series/parallel array that 
works with these specs, and the correct values for the required current-limiting resistors. Note that 
you could not attach the +5v connection of these circuits to an Arduino dig­ital pin to turn things on 
and off because each of the three branches of the circuit would consume 18mA, for 54mA total. This would 
exceed the Arduino s 40mA per-pin max. If you wanted to drive .ve LEDs from one Arduino pin you would 
have to specify a current of 13mA (for 39mA total) which would result in current-limiting resistors of 
100O, 100O, and 240O for the circuit shown in Figure 2.6. 2.4 Driving lots of LEDs using external LED-driver 
chips For situations where you want to drive a lot more LEDs, and drive them each separately on and off, 
you need external chip support. There are a large number of LED driver chips that you could choose from. 
These chips are really the way to go if you want to drive lots and lots of LEDs. Note that depending 
on the total number of LEDs in your system, you may need to provide extra power. Recall that even if 
you re using, say, a 9v external power supply into the 2.1mm power input, that power connection can provide 
a max of 800mA or so of total power. If you have enough LEDs that you re bumping up against that power 
limit, you ll need a separate power supply for the LEDs. Remember that if you re using multiple power 
supplies, you should tie all the grounds of all the sub-circuits together into one big ground. See Big 
Idea #7. 2.4.1 74HC595 latched shift register This chip is an 8-bit shift register. That is, it has a 
single input and on each shift-event it takes the logic value at the input (high voltage for 1 or low 
voltage for 0) and puts it in an 8-bit register, pushing all the other values one step further into the 
register. Imagine a pipe .lled with ping pong balls that are labeled with a logical value (1 or 0). When 
you push one more ping pong ball into the pipe, the ball at the end of the pipe pops off and falls out. 
That s what a shift register is like. You enter data into the register one bit at a time, but you can 
see all eight bits that are currently in the register on separate output pins (like windows into the 
pipe looking at the value painted on each ping pong ball). This shift register is referred to by its 
number 74HC595. The 74 refers to an entire family of integrated circuits known as the 7400 series that 
all start their part numbers with 74. The HC is a section of the code that tells in what technology the 
chip is fabricated. In this case it is made in a High-speed CMOS (HC) process. The 595 is the identi.er 
that says that this is the 8-bit serial-in, serial or parallel-out shift register with 3-state output 
latches chip in the 74HCxxx family. It s shorter, but more mysterious, to refer to it as a 74HC595. To 
really see how any chip is used you need to look at its data sheet. I m including data sheets for all 
the chips mentioned in this course in the additional materials. The pin out of the chip, and the logic 
diagram of the chip, are shown in Figure 2.7. These diagrams tell the practiced user what s inside the 
chip and how to connect to that chip. The logic diagram says that there is an 8-bit shift register that 
uses the following pins: DS (pin 14) is the data input to the shift register.  SH CP is the clock that 
determines when new data is shifted into the register. The data is shifted into the shift register on 
the rising edge of this clock.  MR is a Master Reset that clears the register to 0. The fact that the 
signal is shown going into a bubble in the diagram, and has a bar over the MR, means that it is active 
low. That means that the action of that signal happens when the voltage is low, not when it is high. 
 The ST CP signal takes the data in the 8-stage shift register and transfers it all at once to the 8-bit 
storage register.  From there, the 8 data values currently in the register are connected to the output 
through the Q7 -Q0 signals.  These signals are 3-state signals which means that you can turn off their 
voltage drive using the OE signal (again, an active-low signal). If you turn off the drive to those outputs, 
they act as if they are disconnected from the circuit. (Advanced note -you can dim all of the LEDs driven 
by the 595 by using analogWrite() and PWM con­nected to this OE pin.)  To use the 74HC595 as an LED 
driver, you would use the Arduino (for example) to send bits one at a time into the shift register, and 
then use the ST CP signal to transfer them to 2.4: Driving lots of LEDs using external LED-driver chips 
 the output storage register. Then those bits would appear at the Q outputs, and you could put an LED 
(and a current-limiting resistor) on each of those outputs. If the data in Q3 (for example) is a 0, the 
LED would be dark, and if Q3 was high, the LED would light up. An example of this circuit is shown in 
Figure 2.8. In the circuit in Figure 2.8 the Arduino is driving the data into the 595 on pin 11 of the 
Arduino, controlling the clock that causes new data to be shifted in on pin 12, and controlling the signal 
that transfers shifted data to the output register on pin 8. This circuit allows the Arduino to control 
eight LEDs using only three pins on the Arduino. The MR and OE signals in the circuit of Figure 12 are 
tied to 5v so that they re never active. This means that you re never resetting the register to all 0 
s, and you re never turning off the drive to the outputs. Another version of this schematic is seen in 
Figure 2.9, with a diagram of the physical wiring in Figure 2.10. Figure 2.9: Another schematic using 
a 74HC595 latched shift register. This one is from the Arduino.cc web site (arduino.cc/en/Tutorial/ShiftOut) 
and features a few extra ideas. Like Figure 13, the MR is tied high (inactive) and the OE is tied low 
(active). The 1uf component is a capacitor. This is a component that stores electrical charge. It s being 
used here as a decoupling capacitor between the latching signal and ground to avoid electrical noise 
on that signal which might capture incorrect data into the shift register. It s a nice feature, but not 
absolutely required. The circuit will almost certainly work without it. Note that the ordering of the 
series connection of the resistor and LED has been reversed from Figure 2.8. In this case the resistor 
connects to the chip and the LED connects to ground. The relative order of the components in a series 
connection does not matter because Kirchhoff s current law says that series connected components will 
experience the same current .ow regardless of where they are in the series. 2.4: Driving lots of LEDs 
using external LED-driver chips Figure 2.10: Another .gure from the Arduino.cc web site (http://arduino.cc/en/Tutorial/ShiftOut). 
This picture shows the physical wiring of the circuit in Figure 14 with an Arduino on the right and two 
solderless breadboards on the left. The white chip in the center is the 74HC595. Note that in addition 
to the signal wires, the chip has been connected to power (+5v) using the red wire in the upper right 
of the chip, and to ground using the black wire on the lower left. All external chips must be connected 
to power and ground so that those chips get the power they need to operate. Also, you could chain these 
shift registers together because the data that pops off the end of the shift register is on pin Q7. That 
pin could be connected to the DS pin of a second 74HC595 to chain them together. Now the Arduino could 
shift out 16 bits of data before capturing it in the output register, and get 16 LEDs for only three 
Arduino pins. Code for driving a circuit like the one in Figure 2.8 or 2.9 is shown below. It turns out 
that this sort of serial data transfer is common enough that it has its own name: Serial Peripheral Interface 
or SPI (sometimes pronounced spy ), and it has a special function built in to Arduino called shiftOut() 
that sends 8-bits of data on this type of link (see the Software Overview in Section 1.5.2). / * * An 
e x a m p l e o f s e n d i n g d a t a t o a n e x t e r n a l 7 4 HC595 c h i p u s i n g t h e * 
s h i f t O u t ( ) f u n c t i o n .  * / / / P i n D e f i n i t i o n s i n t d a t a = 1 1 ; / / 
P i n c o n n e c t e d t o t h e 5 9 5 s d a t a ( DS ) i n p u t i n t c l o c k = 1 2 ; / / p i n 
c o n n e c t e d t o t h e 5 9 5 s c l o c k ( SH\ CP ) i n p u t i n t c a p t u r e = 8 ; / / P i 
n c o n n e c t e d t o t h e 5 9 5 s c a p t u r e ( ST\ CP ) i n p u t / / s e t up t h e t h r e 
e p i n s f r o m A r d u i n o t o t h e 5 9 5 a s o u t p u t s v o i d s e t u p ( ) {pinMode ( d 
a t a , OUTPUT ) ; pinMode ( c l o c k , OUTPUT ) ; pinMode ( c a p t u r e , OUTPUT) ; } / / s e t t 
h e p i n s t o t h e b i n a r y n u m b e r s 0 t h r o u g h 2 5 5 o v e r a n d o v e r v o i d l 
o o p ( ) { i n t delayMS = 1 0 0 ; / / D e l a y b e t w e e n LED u p d a t e s f o r ( i n t i = 0 
; i < 2 5 6 ; i + + ) { / / c o u n t 0 t o 2 5 5 s e t L E D s ( i ) ; / / u p d a t e t h e LEDs t 
o v a l u e i d e l a y ( delayMS ) ; / / d e l a y b e t w e e n u p d a t e s }} / / u s e s h i f 
t O u t ( ) t o s e n d 8 b i t s o f d a t a t o t h e 5 9 5 , a n d t h u s / / u p d a t e t h e o 
n / o f f s t a t u s o f e a c h o f t h e LEDs v o i d s e t L E D s ( i n t v a l u e ) {d i g i t 
a l W r i t e ( c a p t u r e , LOW) ; / / S e t t h e c a p t u r e p i n l o w ( i n a c t i v e ) 
s h i f t O u t ( d a t a , c l o c k , MSBFIRST , v a l u e ) ; / / t r a n s f e r 8 b i t s t o t 
h e 5 9 5 d i g i t a l W r i t e ( c a p t u r e , HIGH ) ; / / s e t t h e c a p t u r e p i n h i 
g h t o t r a n s f e r } 2.4.2 STP08DP05 LED driver chip The 74HC595 is a great chip -it s inexpensive 
(around 45-90 cents per chip), and it s easy to use with the shiftOut() function. But, it has a big disadvantage: 
you still need to use a sep­arate currently-limiting resistor for every LED. That s a lot of extra wiring, 
but necessary to limit the current to a safe value. The STP08DP05 chip is very similar to the 595 in 
function: you can send data to the chip in exactly the same way as the 595 using the shiftOut() function. 
The biggest advantage of this chip over the 74HC595 is that the STP08DP05 has constant current outputs. 
The description of this chip on the datasheet is Low voltage 8-bit constant current LED sink. This means 
that there are special drivers on the output pins of this chip that automatically limit the output current 
to a particular value. The output current is set through a single resistor that sets the output current 
for the whole chip. The pins of the SPT08DP05 are shown in Figure 2.11. If you wanted to use the same 
code as for the 74HC595, you would connect the data pin to the SDI input, the clock pin to the CLK input, 
and the capture pin to the LE/DM1 input. The LEDs can be connected directly to the 8 outputs labeled 
OUT0 to OUT7. A close examination of the logic diagram (Figure 14) shows that the drivers for the outputs 
can only pull the outputs low, not drive them high (that s why the chip description is a constant current 
LED sink ). That is, the pins can sink current to ground, but not source current to a high voltage. Thus, 
the LEDs must be connected with their anodes connected to a power source (5v) and their cathodes connected 
to the STP08DP05 outputs. The R-EXT pin on the STP08DP05 is where the current-limiting resistor for 
the entire chip is connected. The value of the resistor sets the current limit for all the outputs. A 
2.4: Driving lots of LEDs using external LED-driver chips table in the STP08DP05 gives a range of values 
for a range of currents. For example, if you want to limit the output current to 10mA, choose an external 
resistor of around 1900O. The external resistor is connected to the R-EXT pin, and then to ground outside 
the chip. Using this chip you need only one resistor per chip instead of one resistor per LED. You can 
also chain these chips together in the same way as you would chain 595 chips together. You pay a little 
more for the STP08DP05 because of the advantage of the constant current outputs. This chip sells for 
around $1.50-$2.00 each. This next set of .gures shows a detailed look at wiring up an external STP08DP05 
chip on a solder less breadboard. If you ve wired things up before I m sure you can skip it. If you ve 
never wired anything, I though it might be nice to see the whole process. Figure 2.14: First Step: Put 
the STP08DP05 chip into the solderless breadboard. Note that the legs of the chip straddle the gap in 
the middle of the board. This means that each leg has four other holes that wires can go into and connect 
to that leg of the chip. If you look carefully in this .gure you ll see a U-shaped divot in the top of 
the chip. This tells you where the top is with respect to the pin numbering. Pin 1 is in the top left, 
just like in Figure 2.11. The .rst connections to make are GND (pin 1), Vdd (Pin 16), and OEbar (pin 
13). I ve tied OEbar to ground so that the outputs are always enabled. I ve also put a resistor between 
the R-EXT pin (pin 15) and ground to set the current limit for all the LEDs. This resistor is 2kO. 2.4: 
Driving lots of LEDs using external LED-driver chips   Figure 2.19: Now the Arduino is also connected. 
The SDI (data-input), CLK, and LE (latch-enable) are connected to pins 12, 11, and 10 on the Arduino. 
I ve also connected GND on the Arduino to the far right ground bus on the breadboard (black wire), and 
+5v of the Arduino to the far right power bus on the breadboard (red wire). Not seen in this photo, the 
power bus on the far right is connected to the power bus on the left side of the breadboard out of range 
of the camera. I m now ready to run a program like the one on the previous pages to drive values onto 
the STP08DP05 and have them show up on the LEDs. 2.4: Driving lots of LEDs using external LED-driver 
chips 2.4.3 MAX 7219/7221 LED driver chip The STP08DP05 is a great little chip -it can drive 8 LEDs 
directly while using only three pins on the Arduino. If you daisy-chain multiple chips you can drive 
even more LEDs without using more pins on the Arduino. However, there s an LED driver chip that can drive 
even more LEDs -the MAX 7219 and MAX 7221 chips (these chips are essentially identical for our purposes) 
can drive 64 LEDs from one chip, and these chips also use constant current drivers so you don t need 
64 resistors. These chips are actually designed to drive 7-segment numeric LED displays. These are displays 
like you might .nd on a calculator. A seven-segment LED has, you might expect, seven LED segments. These 
are typically shaped like bars instead of circles, and are organized as a .gure eight so they can be 
can be used to display the numbers 0 through 9. Actually, most seven-segment LEDs have an 8th segment 
that is the decimal point for each number as seen in Figure 2.20. Each of the eight LEDs in one digit 
has a separate anode, but is connected to a common cathode. If you want to see a 0, for example, you 
would set the anodes of segments A, B, C, D, E and F to 1, and the anodes of segments G and DP to 0. 
Then when you connect the common cathode to ground, current .ows and the LED segments light up. The 
reason that the segments have a common cathode is more obvious when you put multiple digits in a row. 
All of the anodes for each segment in the multi-digit display are connected together and each digit has 
a common cathode. That is, if you have a 5­digit display, all .ve of the anodes for the A segments are 
connected together. The other segments likewise have their anodes connected together. Each digit has 
a separate cathode that connects all the cathodes in that digit. This way you can cycle through the digits 
one at a time. Start with all .ve of the cathodes high. That is, none of the LEDs or digits is lit up. 
First you set the A through G (plus DP) segments for the .rst digit, then pull that digit s cathode low 
to light it up. Then you set the .rst digit s cathode back to high, change the segment values to the 
values for the second digit, and pull the second digit s cathode low to light it up. If you do this for 
all .ve digits in turn, then each of the .ve digits can have a different value, but they light up in 
sequence, not all together. However, remember the PWM technique for dimming an LED? If you .ash LEDs 
quickly enough, they look like they re on all the time. Your eye integrates the on-time and makes it 
look like it s all the way on when instead it s .ashing too quickly to make out the .ashes. That s how 
multiple digit LED segments work: the driving chip cycles through each digit and does it fast enough 
that they all look on at the same time, albeit a little bit dimmer than if they really were on all the 
time. The Max 7219/7221 chip scans digits at 800Hz (800 times per second): Much too fast for your eyes 
to see the blinking. Figure 2.21: The pinout of the Max 7219/7221 chip, and an example of how it is 
used to drive a, 8-digit numeric (seven-segment) display. Note that the 8-segments are driving the A-G 
and DP segments (i.e the anodes of those LEDs), and the 8-digits are pulling down the common cathodes 
of each digit in sequence. On the microprocessor side, you would use an Arduino and choose whatever digital 
pins you like. You could then use shiftOut() to send data to the Max chip in the same way as the code 
for the STP08DP05. Diagrams are from the MAX 7219/7221 data sheet, Maxim Integrated Products. 2.4: Driving 
lots of LEDs using external LED-driver chips Figure 2.21 shows how the Max 7219/7221 chip could be used 
to drive an 8-digit nu­ meric display where each digit in the display is a seven-segment digit (with 
8 LED seg­ments). The Max chip handles the sequencing of the digits to make it look like they re all 
on at once. The user sends data from the microcontroller to the Max chip to tell it what should be displayed 
on each of the 8 digits. This communication is a little more complex than for the previous chips because 
you re specifying more things. In addition to telling the Max chip what should be displayed on each of 
the digits, you can tell the chip how bright things should be, which of the digits should actually be 
displayed, and a variety of other things. This is all layered on top of the simple SPI serial communication 
protocol used in the previous chips, so you can use shiftOut() to send the data, but the data to be sent 
is a little more complex. More on that in a moment. Another thing that you can use the Max 7219/7221 
chip for is driving 64 separate LEDs. If you think about what s involved in driving an 8-digit display 
where each digit consists of eight common-cathode LEDs, that means that the chip is driving a total of 
8x8=64 LEDs. It does this by cycling through the LEDs in groups of eight, but it does this at 800Hz so 
the human eye thinks that they re all on at the same time. So, if you want to light up 64 separate LEDs, 
and you re willing to wire them up as eight groups of eight cathode­connected LEDs, this is the chip 
for you. . Figure 2.22 shows an 8x8 array of LEDs connected in a way that works with the Max 7219/7221. 
Each row of this diagram has a common cathode for the whole row (like the LEDs in one digit). Each column 
is anode-connected (like the segments in a digit). Con­nected in this way, the Max7219/7221 can set each 
of the 64 LEDs independently and by cycling through the rows it will look like they re all lit (or not) 
at the same time. This .g­ure shows the LEDs in a square, and of course you could connect them that way. 
In fact, you can buy pre-assembled LED matrixes that are constructed like this. But, as long as you maintain 
this logical connectivity, your LEDs could be physically placed in whatever pattern you like. Now it 
s a simple matter of programming to choose how to light up the individual LEDs to make the patterns that 
you want to see. The Max 7219/7221 chips have constant-current drivers so you don t need separate current-limiting 
resistors on every LED. The current drive is set using one resistor, similar to the STP08DP08. But, the 
value is different, and in this case the resistor connected to the ISET pin (remember that the symbol 
for current is I) is connected to Vdd (power), not ground. The table of current-setting resistor values 
for the Max7219/7221 chips is shown in Figure 22. Note that the values in this table change depending 
on the forward voltage Vf of your LEDs, and they are in kilo-ohms. You can see where this resistor connects 
to the chip and to the power supply in the schematic in Figure 2.21. OK, maybe it s not all that simple. 
The Max7219/7221 requires that you send multiple commands and data to the chip to set it up to control 
the LEDs. The data sheet has all the details. Luckily, someone has written a great library that captures 
all that information. In fact, because it s so popular, there are multiple libraries for this chip. You 
can .nd them on the Arduino.cc web site in the playground section: playground.arduino.cc/ Main/LEDMatrix. 
This has pointers to a number of different libraries that interface to this chip. I like the led-control 
library documented at playground.arduino.cc/Main/ LedControl. It allows you to send digits to a Max 7219/7221 
connected to a multi-digit display, and also control each LED separately if you re using 64 separate 
LEDs. ledControl Library This library, written by Eberhard Fahle, is a nice API for the Max7219/7221 
chip that has the user create an object for each Max chip, or series-connected set of Max chips. User 
code can then set individual LEDs on or off, set entire rows or columns to an 8-bit value all at once, 
or display a number on a speci.c digit of a multi-digit display. The ledControl library, and documentation, 
is available at playground.arduino.cc//Main/LedControl. The example program that comes with the library 
and that drives patterns onto an 8x8 LED 2.4: Driving lots of LEDs using external LED-driver chips  
matrix (connected as in Figure 2.22) is shown here (I ve augmented the comments some­ what). / / E x 
a m p l e p r o g r a m f r o m l e d C o n t r o l d i s t r i b u t i o n . # i n c l u d e L e d 
C o n t r o l . h / / We a l w a y s h a v e t o i n c l u d e t h e l i b r a r y / * Now we n e e 
d a n L e d C o n t r o l o b j e c t t o w o r k w i t h . ***** C h a n g e t h e s e p i n s n u m 
b e r s t o w o r k w i t h y o u r h a r d w a r e ***** * p i n 1 2 i s c o n n e c t e d t o t h 
e D a t a I n * p i n 1 1 i s c o n n e c t e d t o t h e CLK * p i n 1 0 i s c o n n e c t e d t o 
LOAD * We h a v e o n l y a s i n g l e MAX72XX .  */ L e d C o n t r o l l c = L e d C o n t r o l 
( 1 2 , 1 1 , 1 0 , 1 ) ; un s i g ned l o n g d e l a y t i m e = 1 0 0 ; / / d e l a y b e t w e e 
n u p d a t e s o f t h e d i s p l a y v o i d s e t u p ( ) { l c . shutdown ( 0 , f a l s e ) ; / 
/ Wake u p t h e MAX72XX l c . s e t I n t e n s i t y ( 0 , 8 ) ; / / S e t t h e b r i g h t n e s 
s t o a medium v a l u e s l c . c l e a r D i s p l a y ( 0 ) ; / / a n d c l e a r t h e d i s p l 
a y  } / * * T h i s m e t h o d w i l l d i s p l a y t h e c h a r a c t e r s f o r t h e * w o 
r d A r d u i n o o n e a f t e r t h e o t h e r o n t h e m a t r i x . * ( y o u n e e d a t l 
e a s t 5 x 7 l e d s t o s e e t h e w h o l e c h a r s )  * / v o i d w r i t e A r d u i n o O n 
M a t r i x ( ) {/ * h e r e i s t h e d a t a f o r t h e c h a r a c t e r s * / b y t e a [ 5 ] = 
{ B 0 1 1 1 1 1 1 0 , B 1 0 0 0 1 0 0 0 , B 1 0 0 0 1 0 0 0 , B 1 0 0 0 1 0 0 0 , B 0 1 1 1 1 1 1 0 } 
; b y t e r [ 5 ] = { B 0 0 1 1 1 1 1 0 , B 0 0 0 1 0 0 0 0 , B 0 0 1 0 0 0 0 0 , B 0 0 1 0 0 0 0 0 , 
B 0 0 0 1 0 0 0 0 } ; b y t e d [ 5 ] = { B 0 0 0 1 1 1 0 0 , B 0 0 1 0 0 0 1 0 , B 0 0 1 0 0 0 1 0 , 
B 0 0 0 1 0 0 1 0 , B 1 1 1 1 1 1 1 0 } ; b y t e u [ 5 ] = { B 0 0 1 1 1 1 0 0 , B 0 0 0 0 0 0 1 0 , 
B 0 0 0 0 0 0 1 0 , B 0 0 0 0 0 1 0 0 , B 0 0 1 1 1 1 1 0 } ; b y t e i [ 5 ] = { B 0 0 0 0 0 0 0 0 , 
B 0 0 1 0 0 0 1 0 , B 1 0 1 1 1 1 1 0 , B 0 0 0 0 0 0 1 0 , B 0 0 0 0 0 0 0 0 } ; b y t e n [ 5 ] = { 
B 0 0 1 1 1 1 1 0 , B 0 0 0 1 0 0 0 0 , B 0 0 1 0 0 0 0 0 , B 0 0 1 0 0 0 0 0 , B 0 0 0 1 1 1 1 0 } ; 
b y t e o [ 5 ] = { B 0 0 0 1 1 1 0 0 , B 0 0 1 0 0 0 1 0 , B 0 0 1 0 0 0 1 0 , B 0 0 1 0 0 0 1 0 , B 
0 0 0 1 1 1 0 0 } ; / * now d i s p l a y t h e m o n e b y o n e w i t h a s m a l l d e l a y */ / 
* I v e s t a c k e d m u l t i p l e s t a t e m e n t s p e r l i n e t o s a v e s p a c e */ l 
c . setRow ( 0 , 0 , a [ 0 ] ) ; l c . setR ow ( 0 , 1 , a [ 1 ] ) ; l c . setRow ( 0 , 2 , a [ 2 ] ) 
; l c . setRow ( 0 , 3 , a [ 3 ] ) ; l c . setR ow ( 0 , 4 , a [ 4 ] ) ; d e l a y ( d e l a y t i m 
e ) ;  l c . setRow ( 0 , 0 , r [ 0 ] ) ; l c . setR ow ( 0 , 1 , r [ 1 ] ) ; l c . setRow ( 0 , 2 , 
r [ 2 ] ) ; l c . setRow ( 0 , 3 , r [ 3 ] ) ; l c . setR ow ( 0 , 4 , r [ 4 ] ) ; d e l a y ( d e l 
a y t i m e ) ;  l c . setRow ( 0 , 0 , d [ 0 ] ) ; l c . setR ow ( 0 , 1 , d [ 1 ] ) ; l c . setRow 
( 0 , 2 , d [ 2 ] ) ; l c . setRow ( 0 , 3 , d [ 3 ] ) ; l c . setR ow ( 0 , 4 , d [ 4 ] ) ; d e l a 
y ( d e l a y t i m e ) ;  l c . setRow ( 0 , 0 , u [ 0 ] ) ; l c . setR ow ( 0 , 1 , u [ 1 ] ) ; l 
c . setRow ( 0 , 2 , u [ 2 ] ) ; l c . setRow ( 0 , 3 , u [ 3 ] ) ; l c . setR ow ( 0 , 4 , u [ 4 ] 
) ; d e l a y ( d e l a y t i m e ) ;  l c . setRow ( 0 , 0 , i [ 0 ] ) ; l c . setR ow ( 0 , 1 , i 
[ 1 ] ) ; l c . setRow ( 0 , 2 , i [ 2 ] ) ; l c . setRow ( 0 , 3 , i [ 3 ] ) ; l c . setR ow ( 0 , 
4 , i [ 4 ] ) ; d e l a y ( d e l a y t i m e ) ;  l c . setRow ( 0 , 0 , n [ 0 ] ) ; l c . setR ow 
( 0 , 1 , n [ 1 ] ) ; l c . setRow ( 0 , 2 , n [ 2 ] ) ; l c . setRow ( 0 , 3 , n [ 3 ] ) ; l c . setR 
ow ( 0 , 4 , n [ 4 ] ) ; d e l a y ( d e l a y t i m e ) ;  l c . setRow ( 0 , 0 , o [ 0 ] ) ; l c . 
setR ow ( 0 , 1 , o [ 1 ] ) ; l c . setRow ( 0 , 2 , o [ 2 ] ) ; l c . setRow ( 0 , 3 , o [ 3 ] ) ; 
l c . setR ow ( 0 , 4 , o [ 4 ] ) ; d e l a y ( d e l a y t i m e ) ;  l c . setRow ( 0 , 0 , 0 ) ; 
l c . set Row ( 0 , 1 , 0 ) ; l c . setRow ( 0 , 2 , 0 ) ; l c . setRow ( 0 , 3 , 0 ) ; l c . set Row 
( 0 , 4 , 0 ) ; d e l a y ( d e l a y t i m e ) ;  2.4: Driving lots of LEDs using external LED-driver 
chips } / * * T h i s f u n c t i o n l i g h t s up s o m e LEDs i n a r o w . * T h e p a t t e r 
n w i l l b e r e p e a t e d o n e v e r y r o w . * T h e p a t t e r n w i l l b l i n k a l o n 
g w i t h t h e row-n u m b e r . * r o w n u m b e r 4 ( i n d e x = = 3 ) w i l l b l i n k 4 t i 
m e s e t c .  * / v o i d rows ( ) { f o r ( i n t row =0 ; row <8; row + + ) { d e l a y ( d e l a 
y t i m e ) ; l c . setRo w ( 0 , row , B 1 0 1 0 0 0 0 0 ) ; d e l a y ( d e l a y t i m e ) ;  l 
c . setRo w ( 0 , row , ( b y t e ) 0 ) ;  f o r ( i n t i = 0 ; i <row ; i + + ) { d e l a y ( d e 
l a y t i m e ) ; l c . setRow ( 0 , row , B 1 0 1 0 0 0 0 0 ) ; d e l a y ( d e l a y t i m e ) ; 
 l c . setRow ( 0 , row , ( b y t e ) 0 ) ;  }}} / * * T h i s f u n c t i o n l i g h t s up s o m 
e LEDs i n a c o l u m n . * T h e p a t t e r n w i l l b e r e p e a t e d o n e v e r y c o l u m 
n . * T h e p a t t e r n w i l l b l i n k a l o n g w i t h t h e c o l u m n-n u m b e r . * c o 
l u m n n u m b e r 4 ( i n d e x = = 3 ) w i l l b l i n k 4 t i m e s e t c .  * / v o i d c o l umn 
s ( ) { f o r ( i n t c o l = 0 ; c o l <8; c o l + + ) { d e l a y ( d e l a y t i m e ) ; l c . se 
tC o l u mn ( 0 , c o l , B 1 0 1 0 0 0 0 0 ) ; d e l a y ( d e l a y t i m e ) ;  l c . se tC o l u 
mn ( 0 , c o l , ( b y t e ) 0 ) ;  f o r ( i n t i = 0 ; i <c o l ; i + + ) { d e l a y ( d e l a y 
t i m e ) ; l c . s e t C o l um n ( 0 , c o l , B 1 0 1 0 0 0 0 0 ) ; d e l a y ( d e l a y t i m e 
) ;  l c . s e t C o l um n ( 0 , c o l , ( b y t e ) 0 ) ;  }}} / * * T h i s f u n c t i o n w i 
l l l i g h t up e v e r y L e d o n t h e m a t r i x . * T h e l e d w i l l b l i n k a l o n g w 
i t h t h e ro w-n u m b e r . * r o w n u m b e r 4 ( i n d e x = = 3 ) w i l l b l i n k 4 t i m e 
s e t c .  * / v o i d single () {f o r ( i n t row =0 ; row <8; row + + ) { f o r ( i n t c o l = 0 
; c o l <8 ; c o l + + ) { d e l a y ( d e l a y t i m e ) ; l c . s e t L e d ( 0 , row , c o l , t 
r u e ) ; d e l a y ( d e l a y t i m e ) ; f o r ( i n t i = 0 ; i <c o l ; i + + ) { l c . s e t L 
e d ( 0 , row , c o l , f a l s e ) ; d e l a y ( d e l a y t i m e ) ; l c . s e t L e d ( 0 , row 
, c o l , t r u e ) ; d e l a y ( d e l a y t i m e ) ;  }}}} v o i d l o o p ( ) {w r i t e A r d u 
i n o O n M a t r i x ( ) ; rows ( ) ; colu m n s ( ) ; single () ; } The ledControl library is fully 
documented on the website playground.arduino. cc//Main/LedControl. The main features are: The Max chips 
are communicated to through an ledControl object. The ledControl creation method has four arguments: 
ledControl <name> = ledControl(Data, Clock, Latch, Num); The pin number connected to the Data pin on 
the Max chip  The pin number connected to the Clock pin on the Max chip  The pin number connected to 
the Latch pin on the Max chip  The number of Max chips connected together. The Max chips can be chained 
in a serial chain by connecting the Clock and Latch wires in parallel to all chips, and then connecting 
the Dout (Data-Out) pin of one chip to the Din (Data-In) pin of the next chip in series. The ledControl 
library can handle eight Max chips in series for each ledControl object. See Figure 2.26 for an example 
of chaining two Max chips in series.  By default the Max chip is in shutdown mode where it s not driving 
the inputs. Take it out of shutdown with <name>.shutdown(num, false); where num is the ID of the Max 
chip you re talking to (numbered starting at 0 for the .rst chip). You can set the intensity (brightness) 
of the LEDs using the <name>.setIntensity(num, level); where num is the ID of the Max chip you re talking 
to, and level is a number 0 though 15 where 15 is the brightest intensity. You can clear the display 
to all-off with <name>.clearDisplay(num); Set the on/off status of an individual LED with <name>.setLed(num, 
row, col, state); where num is the ID of the chip, row and col are the address of the LED you re changing, 
and state is true for on and false for off. Set the on/off status of all LEDs in a row of the matrix 
using <name>.setRow(num, row, byte); where num is the chip ID, row is the address of the row, and byte 
is the data for that row. The 1-bits correspond to turning the LED on, and 0-bits for off. Set the on/off 
status of all LEDs in a column of the matrix using <name>.setColumn(num, col, byte); This works the same 
way as setRow(), but be aware that it requires a more complex set of commands to the chip than setRow() 
so it takes longer. That might not matter, but of speed does matter, organize things to be able to use 
setRow() instead. 2.4: Driving lots of LEDs using external LED-driver chips If you re using the Max 
chip to control a multi-digit seven-segment display, then you can use <name>/setChar(num, digit, char, 
dp); to make a digit take on the shape of a character. As usual, num tells which chip you re talking 
to. Digit is the address of the digit you re changing, and char is the character to put on that digit. 
Use dp to turn on/off the decimal point (the eighth segment in the seven-segment display). The char can 
be one of the following: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, a, b, c, d, e, f, h, l, p, -, ., , and <space> 
(denoted as  in the argument list). Those are all the characters that can be distinguished easily on 
a seven-segment display. Figure 2.26: This shows two Max 7219/7221 chips installed in series. The LOAD 
DATA and CLOCK signals from the Arduino go to all the chips. The DATA-IN on the far left comes from the 
Arduino. For the series connected chips the DOUT from one chips goes to the DIN of the next. Note that 
each chip needs its own external current-setting resistor on the ISET pin. The 0.1uF capacitor is optional 
and decouples the 5v supply from ground. The chips are shown here controlling 8-digit displays, but they 
could just as easily each be controlling an 8x8 LED array.  2.4.4 The Texas Instruments TLC5940 LED 
driver chip All of the chips described so far will drive a set of LEDs on the output, and are controlled 
by a simple SPI communication through the Arduino s shiftOut() function. The STP05DP08 and Max 7219/7221 
have constant-current drivers on the outputs so that the output current for the connected LEDs is set 
by a single current-setting resistor. The Max chip controls the brightness of the LEDs using an internal 
command that changes the pulse width modula­tion at the output, but for all the LEDs at one time. You 
can similarly control the brightness of all the LEDs connected to the STP05DP08 chip by using the Arduino 
s PWM on the OE (Output Enable) pin. However, these brightness settings control all the LEDs connected 
to the chip at once. If you d like to individually control the brightness (using PWM) of each LED attached 
to the LED driver chip, you need a different chip. You need a TLC5940. This chip drives only 16 LEDs 
(not the 64 that the Max chips drives), but it has individual PWM on every pin. In fact, there are 4096 
levels of brightness for each LED driven by the TLC5940. That is, 4096 different pulse width modulation 
options for each pin. The downside of this chip is that the communication protocol is not as simple as 
SPI, so you can t just use shiftOut() to send data to the chip. In fact, it s so complex that it takes 
a carefully designed timer-driven interrupt handler to get it right. Luckily, this too has a nice library 
of commands written by a helpful open-source community member (Alex Leone). The library is available 
on Google Code at code.google.com/p/tlc5940arduino/. Basic usage for the TLC5940 library is as follows. 
There are more advanced functions that are documented on the Google Code repository: The library pre-de.nes 
which pins to use for the required connections from the Ar­duino to the TLC5940:  Pin 13 of the Arduino 
to pin 25 of the TLC5940 (SCLK)  Pin 11 of the Arduino to pin 26 of the TLC5940 (SIN)  Pin 10 of the 
Arduino to pin 23 of the TLC5940 (BLANK)  Pin 9 of the Arduino to pin 24 of the TLC5940 (XLAT)  Pin 
3 of the Arduino to pin 18 of the TLC5940 (GSCLK)   The other connections required by the TLC5940 are: 
 +5v to pins 21 and 19 of the TLC5940 (VDD and DCPRG) GND to pins 22 and 27 of the TLC5940 (GND and 
VPRG)  A resistor for setting current limit from pin 20 of the TLC5940 to GND. Typically this is between 
2k (20mA) and 4k (10mA).  The library assumes that you re only using one TLC4950, and de.nes NUM TLCS 
to 1. If you daisy chain more chips, you need to update this value in the source code in tlc config.h 
in the library folder.  The library prede.nes an object for the chip called Tlc.  Use Tlc.init() to 
initialize the library -typically in setup().  Use Tlc.clear() to set all the grayscale values of the 
LEDs to 0. This does not actually send the values, it just sets them for the next time that they ll be 
sent.  Tlc.set(channel, value) sets the output value (brightness) for an LED. The channel is the pin 
number between 0 and 15 for one chip. If you daisy chain extra chips the channel number keeps increasing. 
The second chip would drive LEDs 16­31, for example. The value is the grayscale value for that LED in 
the range of 0 (off) to 4095 (fully on). Like Tlc.clear(), this doesn t actually send the data to the 
chip, it just accumulates updates.  Tlc.update() sends the accumulated LED brightness values to the 
chip. An exam­ple from the library is given below. It shows how to wire up the chip, and a simple program 
that makes the Knight Rider (also known as the Cylon eye) effect on the LEDs connected to the TLC5940. 
It makes use of the fading values so that the moving  2.4: Driving lots of LEDs using external LED-driver 
chips LED is fully on (PWM value 4095), and the trailing LED to that one is faded to a much lower value 
(PWM value of 1000). / / T h i s i s t h e B a s i c U s e e x a m p l e f r o m t h e TLC5940 l i b 
r a r y b y / / A l e x L e o n e . I v e r e f o r m a t t e d s o m e o f t h e c o m m e n t s t 
o f i t / / t h i s c o u r s e n o t e s f o r m a t . ( E r i k B r u n v a n d ) / * B a s i c P 
i n s e t u p : ------------ ---u---- ARDUINO 13|-> SCLK ( p i n 2 5 ) OUT1 | 1 2 8 | OUT c h a n n e 
l 0 1 2 | OUT2 | 2 27|-> GND ( VPRG ) 11|-> SIN ( p i n 2 6 ) OUT3 | 3 26|-> SIN ( p i n 1 1 ) 10|-> 
BLANK ( p i n 2 3 ) OUT4 | 4 25|-> SCLK ( p i n 1 3 ) 9|-> XLAT ( p i n 2 4 ) . | 5 24|-> XLAT ( p i 
n 9 ) 8 | . | 6 23|-> BLANK ( p i n 1 0 ) 7 | . | 7 22|-> GND 6 | . | 8 21|-> VCC ( + 5V ) 5 | . | 9 
20|-> 2K R e s . -> GND 4 | . |1 0 19|-> +5V ( DCPRG ) 3|-> GSCLK ( p i n 1 8 ) . |1 1 18|-> GSCLK ( 
p i n 3 ) 2 | . |1 2 17|-> SOUT 1 | . |1 3 16|-> XERR 0 | OUT14 |1 4 1 5 | OUT c h a n n e l 1 5 ------------ 
-------- - P u t t h e l o n g e r l e g ( a n o d e ) o f t h e LEDs i n t h e +5V a n d t h e s h 
o r t e r l e g ( c a t h o d e ) i n OUT( 0 -1 5 ) . - +5V f r o m A r d u i n o -> TLC p i n 2 1 a 
n d 1 9 ( VCC a n d DCPRG ) - GND f r o m A r d u i n o -> TLC p i n 2 2 a n d 2 7 (GND a n d VPRG ) 
- d i g i t a l 3 -> TLC p i n 1 8 ( GSCLK ) - d i g i t a l 9 -> TLC p i n 2 4 ( XLAT ) - d i g i t 
a l 1 0 -> TLC p i n 2 3 ( BLANK ) - d i g i t a l 1 1 -> TLC p i n 2 6 ( SIN ) - d i g i t a l 1 3 -> 
TLC p i n 2 5 ( SCLK ) - T h e 2K r e s i s t o r b e t w e e n TLC p i n 2 0 a n d GND w i l l l e t 
 2 0mA t h r o u g h e a c h LED . To b e p r e c i s e , i t s I = 3 9 . 0 6 / R ( i n o h m s ) . 
T h i s d o e s n t d e p e n d o n t h e LED d r i v i n g v o l t a g e . A 3 k r e s i s t o r = 
 1 3mA, a n d 4 k i s 1 0mA - ( O p t i o n a l ) : p u t a p u l l -up r e s i s t o r ( 1 0 k ) 
b e t w e e n +5V a n d BLANK s o t h a t a l l t h e LEDs w i l l t u r n o f f when t h e A r d u i 
n o i s r e s e t . I f y o u a r e d a i s y -c h a i n i n g m o r e t h a n o n e TLC , c o n n e 
c t t h e SOUT o f t h e f i r s t TLC t o t h e SIN o f t h e n e x t . A l l t h e o t h e r p i n 
s s h o u l d j u s t b e c o n n e c t e d t o g e t h e r : BLANK o n A r d u i n o -> BLANK o f TLC1 
-> BLANK o f TLC2 -> . . . XLAT o n A r d u i n o -> XLAT o f TLC1 -> XLAT o f TLC2 -> . . . T h e o 
n e e x c e p t i o n i s t h a t e a c h TLC n e e d s i t s own r e s i s t o r b e t w e e n p i n 
2 0 a n d GND. T h i s l i b r a r y u s e s t h e PWM o u t p u t a b i l i t y o f d i g i t a l p 
i n s 3 , 9 , 1 0 , a n d 1 1 . Do n o t u s e a n a l o g W r i t e ( . . . ) o n t h e s e p i n s 
. T h i s s k e t c h d o e s t h e K n i g h t R i d e r s t r o b e a c r o s s a l i n e o f LEDs 
. A l e x L e o n e <a c l e o n e AT g m a i l . com >, 2009-02 -03 */ # i n c l u d e T l c 5 9 
4 0 . h / / r e m e m b e r t o i n c l u d e t h e l i b r a r y ! v o i d s e t u p ( ) {/ * C a l 
l T l c . i n i t ( ) t o s e t u p t h e t l c . * You c a n o p t i o n a l l y p a s s a n i n i t 
i a l PWM v a l u e ( 0 - 4 0 9 5 ) f o r a l l * channels .  * / T l c . i n i t ( ) ; } / * T h i 
s l o o p w i l l c r e a t e a K n i g h t R i d e r -l i k e e f f e c t i f y o u h a v e LEDs * p 
l u g g e d i n t o a l l t h e TLC o u t p u t s . NUM TLCS i s d e f i n e d i n * t l c c o n f i 
g . h i n t h e l i b r a r y f o l d e r . A f t e r e d i t i n g t l c c o n f i g . h * f o r y 
o u r s e t u p , d e l e t e t h e T l c 5 9 4 0 . o f i l e t o s a v e t h e c h a n g e s .  * / 
v o i d l o o p ( ) {i n t direction = 1; f o r ( i n t c h a n n e l = 0 ; c h a n n e l<NUM TLCS *1 
6 ; c h a n n e l += d i r e c t i o n ) { / * T l c . c l e a r ( ) s e t s a l l t h e g r a y s c 
a l e v a l u e s t o z e r o , b u t d o e s n o t * s e n d t h e m t o t h e TLCs . To a c t u a l 
l y s e n d t h e d a t a , c a l l * T l c . u p d a t e ( )  */ T l c . c l e a r ( ) ; / * T l c 
. s e t ( c h a n n e l ( 0 -1 5 ) , v a l u e ( 0 -4 0 9 5 ) ) s e t s t h e g r a y s c a l e v a l 
u e * f o r o n e c h a n n e l ( 1 5 i s OUT15 o n t h e f i r s t TLC , i f m u l t i p l e TLCs * 
a r e d a i s y -c h a i n e d , t h e n c h a n n e l = 1 6 w o u l d b e OUT0 o f t h e s e c o n d 
 * TLC , e t c . ) . T h e v a l u e g o e s f r o m o f f ( 0 ) t o a l w a y s o n ( 4 0 9 5 ) . * 
 * L i k e T l c . c l e a r ( ) , t h i s f u n c t i o n o n l y s e t s up t h e d a t a , * T l 
c . u p d a t e ( ) w i l l s e n d t h e d a t a .  */ i f ( c h a n n e l == 0 ) { d i r e c t i o 
n = 1 ; } e l s e { T l c . s e t ( c h a n n e l - 1 , 1 0 0 0 ) ; } / / e n d i f -e l s e T l c . 
s e t ( c h a n n e l , 4 0 9 5 ) ; i f ( c h a n n e l != NUM TLCS*1 6 - 1 ) { T l c . s e t ( c h a 
n n e l + 1 , 1 0 0 0 ) ; } e l s e { d i r e c t i o n = -1; } / / e n d i f -e l s e / * T l c . u 
p d a t e ( ) s e n d s t h e d a t a t o t h e TLCs . T h i s i s when t h e LEDs * w i l l a c t u 
a l l y c h a n g e . */ T l c . u p d a t e ( ) ; d e l a y ( 7 5 ) ; / / w a i t t o s e e t h e e 
f f e c t . } / / e n d f o r ( ) ; } / / e n d l o o p ( )  2.5 Questions about LEDs The questions 
that you should ask when connecting LEDs to a microcontroller like the Arduino are: What is the forward 
voltage (Vf ) of the LED that you re trying to use? You can get this value from the vendor of the LEDs, 
or you can .gure it out using a resistor, power supply, and voltmeter. To .gure it out on your own, connect 
the LED through a largish resistor (470O for example) from +5v to ground. Measure the voltage drop across 
the lit LED. This is the forward voltage. How much current should you put through the LED? Your spec 
for the LED should tell you what the max current capability of your LED is. If it doesn t, assume that 
20mA is the maximum. You can always back off a little bit from the LED s max without having much impact 
on the brightness. What current-limiting resistor should you use? 2.5: Questions about LEDs You can compute 
this for a single LED using the formula R = (Vsource - Vf )/Idesired where Vsource is the power supply 
voltage, Vf is the LED s forward voltage, and Idesired is the amount of current you would like to put 
through the LED. If you re using an LED driver chip that has constant current outputs, you need to compute 
the current-setting resistor according to the data sheet of that chip. How bright would you like your 
LED? Use digitalWrite(pin, HIGH/LOW) to set the LED full on or full off. Use analogWrite(pin, value) 
to set the LED to an intermediate brightness with value being between 0 and 255. This only works on Arduino 
pins 3, 5, 6, 9, 10, and 11 on the Duemilinove and Uno boards (see Arduino.cc for details of other Arduino 
boards). If you d like to drive more than one LED from a single microcontroller pin, what series/parallel 
organization should you use, and what resistors? The easiest approach here is to use an LED calculator 
from the web like ledcalculator. net/ and led.linear1.org/led.wiz. If you re planning on using more LEDs 
than one Arduino (or other microcontroller) can support, what external chip should you use? If you need 
a small number of extra LEDs, the STP08DP05 is a good choice. It drives 8 LEDs per chip, has constant-current 
outputs and has a simple shift­register interface that lets you daisy-chain extra chips for even more 
LEDs. Use shiftOut() to send data to the chip(s).  If you need more LEDs, or are using seven-segment 
digit displays the MAX 7219/7221 is a good choice. It also has constant-current outputs and can drive 
up to 64 LEDs, or 8 digits of seven-segment digit display. It s also daisy-chainable. I recommend using 
the ledControl library. It s available at playground.arduino. cc/Main/LedControl.  If you need each 
LED to have its own brightness control, the TLC5940 is a good choice. The chip drives only 16 LEDs per 
chip, but has constant-current outputs, 4096 levels of brightness per LED, and you can daisy-chain multiple 
chips. I recommend using the Tlc5940 library. It s available at code.google.com/p/ tlc5940arduino/. 
  Chapter 3 Speed! Sensors LEDs are a wonderful way to add pizzazz to a project, but they re primarily 
outputs. It turns out that you can actually use them as inputs by using them backwards as light sen­sors, 
but that bit of esoterica aside, they re really output devices that light up. If you want to have your 
physical computing system react to the environment in which it is installed, you need environmental sensors. 
An environmental sensor can be as simple as a switch that can be read by the program to see if it is 
in the on or off position, of medium complex­ity like a whole range of sensors that change their resistance 
based on a physical condition like light or temperature, or as complex as something like GPS sensing 
that receives multi­ple satellite signals to pin down very precise latitude and longitude. We ll primarily 
look at the .rst two styles of sensors. 3.1 Switches Switches are great environmental sensors: they re 
simple, they re easy to interface, and they re easy to understand. A simple switch is either open or 
closed, and that property can result in an electrical signal that is easily interpreted by your program 
as on or off (or true or false if you prefer). Some terminology for switches: Open vs. Closed A switch 
is a mechanical device that has conductors that can either be in con­tact and thus conduct electricity 
(a closed switch), or be separated and thus not conduct electricity (an open switch).  Normally Open 
vs. Normally Closed A switch can be in either state in its quiescent position, and then some action 
must take place to change to the other state.  Toggle vs. Momentary action  The action that changes 
the switch state can be static in the sense that once the action is removed the switch stays in that 
state, or momentary in the sense that when the action is removed the switch reverts to its beginning 
stage. Think of a toggle switch vs. a pushbutton with a spring that returns when you release the button. 
Poles and Throws Switches are often de.ned in terms of these parameters. Poles are the number of separate 
circuits that are controlled by the same physical switch. Throws are the number of positions that the 
switch can take.  A single-throw switch has one pair of contacts that can be connected or not. A double-throw 
switch has a common contact that can be connected to one of two other contacts. A triple-throw switch 
can connect to any of three contacts, etc.  A single-pole switch has only one set of connections. A 
double-pole switch has two completely separate circuits that are switched using the same physical switching 
mechanism.  Some common switch types are shown as schematic symbols in Figure 3.1.  Switches come 
in all shapes, size, and con.gurations. A quick look at any home im­provement store, electronics store, 
or web site will reveal a huge variety of switches. You can also make your own switches out of any conducting 
and insulating materials. For ex­ample, you could make a switch from two pieces of aluminum foil with 
wires connected to the foil and where contact is made by pressing the foil together. You could make a 
switch from a hinged piece of wood with thumb tacks placed so that when the mechanism shuts the thumb 
tacks make contact. You could make a switch from a wind chime where one connection is the metal chime 
and the other is a metal chimer that hits the chime so that the switch closes when the wind blows. The 
variations are endless. The question is, how does a microcontroller like the Arduino connect to a switch 
and sense whether it s open or closed? Recall that Arduino digital pins can be con.gured to either OUTPUT 
or INPUT. We ve used only OUTPUT up until now because we ve been driv­ing LEDs. Switches are a case where 
we can use the pins in INPUT mode. In this mode, the digitalRead(pin) function of Arduino looks at the 
voltage on a digital pin and returns a Boolean 0 (LOW) if the voltage is low, and Boolean 1 (HIGH) if 
the voltage is high. For this purpose, a low voltage is somewhere between 0v and approximately 2v, and 
a high voltage is somewhere between approximately 3v and 5v. 3.1: Switches  So, to use switches, we 
need to make a simple circuit that will be around 0v in one position of the switch, and around +5v for 
the other position of the switch. We also need to be careful about how much current is .owing in that 
circuit. If you connect a switch directly between the power connection (+5v) and GND, and you close that 
switch, the low-resistance conductor used in the switch will allow huge currents to .ow. Remember Ohm 
s law: the current is directly proportional to voltage, and inversely proportional to resistance (V = 
I R). So, with very low resistance, you get very high current for a given voltage. This is bad for at 
least two reasons: one is that high currents burn a lot of power, and the other is that high currents 
can (because of that power) physically burn up your circuit. What s needed is our old friend the current-limiting 
resistor. In this case we need almost no current at all, we just want a voltage difference, so we can 
use a fairly large resistor that will dissipate almost no power. Typically a 10kO resistor is used as 
shown in Figure 3.3. The .gure shows two different versions of the simple SPST switch connection: one 
where the signal sent to the Arduino is high until the switch is closed, and one where the signal is 
Big low until the switch is closed. Like an LED, a switch that goes between power and ground always needs 
a current-limiting resistor! A typical value is 10kO which works well for most situations. In practice, 
this is a non­ critical resistance value: anything between 5kO and 100kO will likely work. Some simple 
code that demonstrates the use of a SPST pushbutton switch in an Arduino program is shown below. The 
switch is connected using a circuit such as the one shown in Figure 3.3, and is connected to digital 
pin 2 on the Arduino. / / T h i s e x a m p l e c o d e i s i n t h e p u b l i c d o m a i n . I t 
 s t a k e n f r o m t h e / / b u t t o n e x a m p l e i n t h e A r d u i n o IDE . I t s e n s e 
s t h e v a l u e o n t h e / / e x t e r n a l s w i t c h a n d t u r n s t h e b u i l t -i n LED 
o n p i n 1 3 o n a n d / / o f f b a s e d o n t h a t s w i t c h . / / h t t p : / / www . a r d u 
i n o . c c / e n / T u t o r i a l / B u t t o n / / c o n s t a n t s won t c h a n g e . T h e y 
 r e u s e d h e r e t o / / s e t p i n n u m b e r s : c o n s t i n t b u t t o n P i n = 2 ; / / 
t h e n u m b e r o f t h e p u s h b u t t o n p i n c o n s t i n t l e d P i n = 1 3 ; / / t h e n 
u m b e r o f t h e LED p i n / / v a r i a b l e s w i l l c h a n g e : i n t b u t t o n S t a t 
e = 0 ; / / v a r f o r r e a d i n g t h e p u s h b u t t o n s t a t u s v o i d s e t u p ( ) {pinMode 
( l e d P i n , OUTPUT) ; / / i n i t i a l i z e t h e LED p i n a s a n o u t p u t pinMode ( b u t 
t o n P i n , INPUT ) ; / / i n i t i a l i z e t h e s w i t c h p i n a s a n i n p u t } v o i d l 
o o p ( ) { b u t t o n S t a t e = d i g i t a l R e a d ( b u t t o n P i n ) ; / / r e a d t h e s 
t a t e o f t h e s w i t c h / / c h e c k i f t h e p u s h b u t t o n i s p r e s s e d . / / i f 
i t i s , t h e b u t t o n S t a t e i s HIGH : i f ( b u t t o n S t a t e == HIGH ) { d i g i t a 
l W r i t e ( l e d P i n , HIGH ) ; / / t u r n LED o n } e l s e { d i g i t a l W r i t e ( l e d 
P i n , LOW) ; / / t u r n LED o f f } / / e n d i f -e l s e } / / e n d l o o p ( ) 3.1: Switches 
3.1.1 Arduino Built-in Pullup Resistors According to Big Idea #12, every switch that makes contact between 
power (VDD) and ground (GND) must have a current-limiting resistor. This means that you re always look­ing 
around for a resistor whenever you connect a switch to one of the digital input pins of the Arduino. 
However, if you re using the simple switch schematic on the left hand side of Figure 3.3 you can make 
use of a nifty feature of the Arduino input pins: they all have built-in pullup resistors that you can 
enable when the pins are being used as inputs. These built-in resis­tors are pullups meaning that they 
connect the pin to Vdd (+5v in this case) through the resistor. The built-in pullup on the ATmega microcontroller 
that is used on the Arduino Duimilenove and Uno boards has a value of 20kO so it s a .ne value to use 
with switches. To enable the internal pullup on a digital pin, declare that pin to be INPUT PULLUP instead 
of simply INPUT. For the example code listed previously, the only line that needs to change is in setup(): 
pinMode(buttonPin, INPUT PULLUP); // Enable internal pullup resistor With this declaration, you can connect 
a SPST switch directly to, in this case, pin 2 and ground without the extra resistor. The 20kO pullup 
resistor is enabled inside the Arduino s microprocessor. Other microprocessor chips have similar features. 
 3.1.2 Switch Bounce and Debouncing Switches are mechanical devices. Inside the body of most switches 
you ll .nd small pieces of metal that are moved to become in contact with each other (closed switch), 
or to not be in contact (open switch). Because these are physical pieces of metal, with mass, and with 
springiness, they have the unfortunate property that when they are physically brought into contact with 
each other they can bounce while making contact. This means exactly what it sounds like: the metal bounces 
up and down while making contact and thus makes and breaks contact a number of times before settling 
down to a solid connection (or disconnec­tion). This bouncing happens very quickly from a human s point 
of view. Typical bounce times range from 0.1ms to 6ms which is pretty fast for a human, but an amazingly 
long time for a computer. Consider that our Arduinos are running at a relatively slow 16MHz (16,000,000 
ticks per second). This is pretty slow from a modern computer point of view where an iPhone 5 has a processor 
running at 1GHz (1000 MHz). Nevertheless, even at 16MHz, each tick of the Arduino s clock is 16.5ns, 
and the assembly instructions that are running on Arduino are issued one per clock. So, if a switch bounces 
for 0.1ms, which is 100us, or 100,000ns, that means that it s bouncing (and thus not a stable high or 
low signal) for as long as it takes to execute 1600 Arduino assembly language instructions. That s plenty 
of time for a bouncing switch to be seen as changing many times when the user really only pressed it 
once! There are many ways to address the problem of switch bounce. For example you might build a mercury 
wetted switch where the contact is made with a ball of mercury that makes the desired electrical contact. 
These are becoming much less common as the hazards of mercury are well known. An easy solution is simply 
to wait after the initial indication that the switch has been pressed. If the program that is reading 
the switch waits after the initially sensed change, and waits longer than the longest anticipated bounce 
time, this can keep the program from seeing the bounces as extra button presses. The downside of this 
simple approach is that it delays the reaction to the switch. Code from the Arduino example set for debouncing 
is as follows. / * * T h i s d e b o u n c e c o d e f r o m t h e A r d u i n o E x a m p l e s s e 
t . I v e a d d e d a f e w * a d d i t i o n a l c o m m e n t s . T h i s c o d e i p l e m e n t 
s s i m p l e d e b o u n c i n g b y * c h e c k i n g f o r a c h a n g e o n t h e b u t t o n s 
i g n a l ( e . g . t h e b u t t o n h a s * b e e n p r e s s e d o r r e a l e a s e d ) . When t 
h e b u t t o n s i g n a l h a s c h a n g e d * c h a n g e d s t a t e , t h i s c o d e w a i t 
s f o r d e b o u n c e D e l a y ms b e f o r e * a s s u m i n g t h a t t h a t s t a t e i s p e 
r m a n e n t . N o t e t h a t i t r e s t a r t s * t h e d e b o u n c e t i m e r a f t e r e a 
c h c h a n g e i n t h e s i g n a l s s t a t e , s o * t h i s w a i t s f o r d e b o u n c e D 
e l a y ms a f t e r t h e l a s t s i g n a l c h a n g e t h a t * w a s s e e n .  * / / / c o n 
s t a n t s won t c h a n g e . T h e y r e u s e d h e r e t o s e t p i n n u m b e r s : c o n s 
t i n t b u t t o n P i n = 2 ; / / t h e n u m b e r o f t h e p u s h b u t t o n p i n c o n s t i 
n t l e d P i n = 1 3 ; / / t h e n u m b e r o f t h e LED p i n / / V a r i a b l e s w i l l c h a 
n g e : i n t l e d S t a t e = HIGH ; / / t h e c u r r e n t s t a t e o f t h e o u t p u t p i n 
i n t b u t t o n S t a t e ; / / t h e c u r r e n t r e a d i n g f r o m t h e i n p u t p i n i n 
t l a s t B u t t o n S t a t e = LOW; / / t h e p r e v i o u s r e a d i n g f r o m t h e i n p u 
t p i n / / T h e f o l l o w i n g v a r i a b l e s a r e l o n g s b e c a u s e t h e t i m e , 
m e a s u r e d i n / / m i l i s e c o n d s , w i l l q u i c k l y b e c o m e a b i g g e r n u m 
b e r t h a n c a n b e s t o r e d / / i n a n i n t . l o n g l a s t D e b o u n c e T i m e = 0 ; 
/ / t h e l a s t t i m e t h e o u t p u t p i n w a s t o g g l e d l o n g d e b o u n c e D e l a 
y = 5 0 ; / / d e b o u n c e t i m e ; i n c r e a s e i f o u t p u t f l i c k e r s v o i d s e t 
u p ( ) { 3.1: Switches pinMode ( b u t t o n P i n , INPUT ) ; pinMode ( l e d P i n , OUTPUT) ; } / 
/ / / b u t t o n D r i v e u s e s a n LED a n w hen e x t e r n a l t h e b p u l l u p u t t o n i 
s r e s i s t o r p r e s s e d v o i d l o o p ( ) { / / r e a d t h e s t a t e o f t h e s w i t 
c h i n t o a l o c a l v a r i a b l e i n t r e a d i n g = d i g i t a l R e a d ( b u t t o n P i 
n ) ; / / C h e c k t o s e e i f y o u j u s t p r e s s e d t h e b u t t o n / / ( i . e . t h e i 
n p u t w e n t f r o m LOW t o HIGH ) , a n d y o u v e w a i t e d / / l o n g e n o u g h s i n c 
e t h e l a s t p r e s s t o i g n o r e a n y n o i s e . / / I f t h e s w i t c h c h a n g e d , 
d u e t o n o i s e o r p r e s s i n g : i f ( r e a d i n g != l a s t B u t t o n S t a t e ) {l 
a s t D e b o u n c e T i m e = m i l l i s ( ) ; / / r e s e t t h e d e b o u n c i n g t i m e r } 
i f ( ( m i l l i s ( ) - l a s t D e b o u n c e T i m e ) > d e b o u n c e D e l a y ) { / / w h a 
t e v e r t h e r e a d i n g i s a t , i t s b e e n t h e r e f o r l o n g e r t h a n / / t h e 
d e b o u n c e d e l a y , s o t a k e i t a s t h e a c t u a l c u r r e n t s t a t e b u t t o n 
S t a t e = r e a d i n g ; } / / s e t t h e LED u s i n g t h e s t a t e o f t h e b u t t o n d i 
g i t a l W r i t e ( l e d P i n , b u t t o n S t a t e ) ; / / s a v e t h e r e a d i n g . N e x 
t t i m e t h r o u g h t h e l o o p , / / i t l l b e t h e l a s t B u t t o n S t a t e : l a s 
t B u t t o n S t a t e = r e a d i n g ; } This debouncing code is simple, and reasonably effective. 
It s not a perfect solution because of the extra delay involved, but it s a pretty short delay in human 
terms so it s almost always good enough. Another technique or debouncing that uses extra hardware, but 
is much faster at sens­ing the initial .ipping of the switch, uses a set-reset .ip .op and a single-pole, 
double throw (SPDT) switch. This circuit, shown in Figure 3.5, is a little tricky to understand. The 
set-reset .ip .op is implemented with the cross-coupled NAND gates (the circuits with the round fronts 
and the bubbles on the outputs). A NAND gate is a logical gate that will drive its output to logic-1 
(+5v) whenever either of the inputs is low, and drive its output to logic-0 (0v) only when both inputs 
are high. By cross-coupling the NAND gates in this con.guration they form a .ip-.op: a circuit that can 
store one bit of information. Without going into too much detail, the key to this design is that when 
the common connection of the SPDT switch is in between the output contacts, both of the signals coming 
from the switch will be pulled high by the pullup resistors. Only when the switch has settled into one 
of its two main (non-moving) positions will one of the outputs be pulled low. When this happens, the 
set-reset (SR) .ip-.op will be either set or reset depending on which wire is pulled low. If the switch 
bounces, it simply sets (for example) a bunch of times in a row, but after the .rst set the output of 
the .ip-.op (Out) will not change. It s only when the .ip-.op is reset that the output will change to 
a low signal. And in that case, a series of reset signals will cause it to reset, and multiple reset 
signals (a bouncing reset) will not have any visible impact on the Out signal. This circuit completely 
solves the bouncing problem, but at the expense of using a SPDT switch, two pullup resistors, and two 
NAND gates. NAND gates come four-to-a-package when implemented as a chip, but that s still a lot of extra 
circuitry. If you re willing to use a SPDT switch, and use two digital inputs on the Arduino instead 
of one, you can simulate this .ip-.op approach in software, and leverage the built-in pullup resistors 
in the Arduino s digital inputs. This uses two input pins for a single non-bouncing input, but uses only 
a switch and a ground connection external to the Arduino. If you need a non-bouncing signal and have 
pins to spare, this is a good solution. In this case you would connect the Set and Reset signals in Figure 
3.5 directly to two of the digital inputs of the Arduino. Some example code is seen below: / * T h i 
s c o d e e m u l a t e s t h e c r o s s -c o u p l e d NAND s t y l e o f d e b o u n c i n g * a SPDT 
s w i t c h u s i n g a f l i p -f l o p . * / c o n s t i n t s w i t c h s e t = 2 ; / / T h e S e 
t i n p u t f r o m t h e s w i t c h c o n s t i n t s w i t c h r e s e t = 3 ; / / T h e r e s e t 
i n p u t c o n s t i n t l e d P i n = 1 3 ; / / T h e b u i l t -i n LED p i n b o o l e a n s t a 
t e = LOW; / / t h e s t a t e o f t h e s w i t c h  v o i d s e t u p ( ) {pinMode ( s w i t c h s 
e t , INPUT PULLUP ) ; / / E n a b l e p u l l u p s o n pinMode ( s w i t c h r e s e t , INPUT PULLUP 
) ; / / t h e s w i t c h i n p u t s pinMode ( l e d P i n , OUTPUT) ; / / d r i v e t h e LED } v o 
i d l o o p ( ) { / / r e a d t h e v a l u e s c o m i n g i n f r o m t h e s w i t c h . N o t e t 
h a t t h e s e / / a r e a c t i v e -l o w v a l u e s b e c a u s e t h e y r e b e i n g p u l l 
e d t o / / g r o u n d b y t h e s w i t c h . S o , t h e a c t i o n ( s u c h a s s e t ) h a p 
p e n s / / w hen t h e i n c o m i n g v a l u e i s l o w . B e c a u s e o f t h e p h y s i c a l 
/ / c o n s t r u c t i o n o f t h e SPDT s w i t c h , y o u l l n e v e r g e t b o t h / / s w i 
t c h s e t a n d s w i t c h r e s e t b e i n g l o w a t t h e s a m e t i m e . i f ( d i g i t a 
l R e a d ( s w i t c h s e t ) == 0 ) s t a t e = HIGH ; i f ( d i g i t a l R e a d ( s w i t c h r 
e s e t ) == 0 ) s t a t e = LOW; / / Now u s e t h e s t a t e v a l u e a s t h e d e b o u n c 
e d v a l u e c o m i n g d i g i t a l W r i t e ( l e d P i n , s t a t e ) ; / / l i g h t up LED 
i f s t a t e i s HIGH } 3.1: Switches 3.1.3 Aside: Serial Communications with the Host Machine In Section 
1.5.2 there was a quick overview of some useful functions that are pre-de.ned by the Arduino IDE. One 
of them is the serial monitor. This is a way that your Arduino can communicate through the serial connection 
(the UART) to the host machine that you re connected to. This serial monitor can be used for a wide variety 
of things, but one of the most useful is for calibration and debugging of Arduino programs. The serial 
monitor lets your program print things to the screen. This is easy if you re writing a C++ program in 
Linux, but not as easy if your program is running on an embedded controller like the Arduino that has 
no operating system, and more importantly, no display. To use the serial monitor you .rst initialize 
the communication in the setup() function using the Serial.begin(baud) function. The baud argument sets 
the baud rate (serial bit rate) for the communication. Because this uses standard EIA232 serial protocols, 
your choice of baud rate must be one of 300, 600, 1200, 2400, 4800, 9600, 14400, 19200, 28800, 38400, 
57600, or 115200. You must also make sure that the serial monitor that you start from the Arduino IDE 
is set to the same baud rate as your Arduino program is using. Once your serial connection is initialized 
in the setup() function, you can print to the monitor using the Serial.print(arg) and Serial.println(arg) 
commands. The only difference between them is that the second prints a newline after the argument is 
printed to the monitor. Once the code is uploaded and running on the Arduino hardware, you can start 
the serial monitor on the host using the Arduino IDE. You can either use the menus with Tools . SerialMonitor 
or you can use the Serial Monitor icon in the upper right of the IDE s main screen. The following is 
a simple program that uses a button, an internal pullup for the button, and the serial monitor instead 
of an LED to indicate the button s state. / * * R e a d t h e s t a t e o f a b u t t o n a n d p r i 
n t i t t o t h e S e r i a l M o n i t o r * / i n t p u s h B u t t o n = 2 ; / / b u t t o n i s c 
o n n e c t e d t o d i g i t a l p i n 2 v o i d s e t u p ( ) { / / i n i t i a l i z e s e r i a l 
c o m m u n i c a t i o n a t 9 6 0 0 b i t s p e r s e c o n d S e r i a l . b e g i n ( 9 6 0 0 ) ; 
/ / m a k e t h e p u s h b u t t o n s p i n a n i n p u t u s i n g a b u i l t -i n p u l l u p pinMode 
( p u s h B u t t o n , INPUT PULLUP ) ; / / p r i n t a h e a d e r t o t h e s e r i a l m o n i t 
o r S e r i a l . p r i n t l n ( T e s t i n g a b u t t o n u s i n g t h e S e r i a l M o n i t o 
r ) ; } v o i d l o o p ( ) { / / r e a d t h e i n p u t p i n i n t b u t t o n S t a t e = d i g i 
t a l R e a d ( p u s h B u t t o n ) ; / / p r i n t o u t t h e s t a t e o f t h e b u t t o n S e 
r i a l . p r i n t ( B u t t o n s t a t e i s : ) ; S e r i a l . p r i n t l n ( b u t t o n S t a 
t e ) ; / / b u t t o n S t a t e w i l l b e 0 o r 1 d e l a y ( 1 0 0 ) ; / / d e l a y b e t w e 
e n r e a d s } This program continuously prints the state of the button on the serial monitor. The delay 
means that the serial monitor will get an update (a new value will be printed) ev­ery 100msec (10 times 
a second). That will still scroll things up the serial monitor pretty quickly. The following example 
will print something only when the program sees the state of the button change. You could use this to 
see, for example, if your program is seeing any bouncing with the button. If so, you ll see multiple 
button press messages when you press the button once (or .ip the switch if you re using a toggle switch). 
Note that there s no delay in the loop here so the program will check the button as fast as it can, and 
should show the effect of a bouncing switch. / * * P r i n t a m e s s a g e o n t h e S e r i a l M 
o n i t o r w h e n e v e r y o u s e e t h e s t a t e * o f t h e s w i t c h c h a n g e . * / i 
n t swPin = 2 ; / / s w i t c h i s c o n n e c t e d t o p i n 2 i n t v a l ; / / v a r i a b l e f 
o r r e a d i n g t h e p i n v a l u e i n t s w S t a t e ; / / v a r i a b l e t o h o l d t h e l 
a s t s w i t c h s t a t e v o i d s e t u p ( ) {pinMode ( swPin , INPUT PULLUP ) ; / / u s e i n t 
e r n a l p u l l u p o n s w i t c h S e r i a l . b e g i n ( 9 6 0 0 ) ; / / I n i t s e r i a l c 
o m m u n i c a t i o n - 9 6 0 0 b p s s w S t a t e = d i g i t a l R e a d ( swPin ) ; / / r e a d 
t h e i n i t i a l v a l u e } v o i d l o o p ( ) {v a l = d i g i t a l R e a d ( swPin ) ; / / s 
t o r e s w i t c h v a l u e i n v a l i f ( v a l != s w S t a t e ) { / / t h e s w i t c h s t a 
t e h a s c h a n g e d i f ( v a l == LOW) { / / c h e c k i f t h e b u t t o n i s p r e s s e d S 
e r i a l . p r i n t l n ( The b u t t o n was j u s t p r e s s e d ) ; } e l s e { / / t h e b 
u t t o n i s n o t p r e s s e d S e r i a l . p r i n t l n ( B u t t o n j u s t r e l e a s e d 
 ) ; } / / e n d i f ( v a l == LOW) } / / e n d i f ( v a l ! = s w S t a t e ) s w S t a t e = v a 
l ; / / s a v e t h e new s w i t c h s t a t e } 3.1.4 Another type of Switch: Passive Infrared (PIR) 
Motion Detector Toggle and pushbutton switches, and homemade contact switches are great sources of hu­man 
input to your projects. Another form of switch that doesn t require physical contact is a passive infrared 
(PIR) motion detector. These sensors are used, for example, by alarm systems to sense whether anything 
is moving around in a room. They re a great way to cause something to happen when a person or animal 
has entered the vicinity of the project. A PIR sensor is a reasonably complex sensor. If you look on 
the back of one you ll see a number of integrated components. What the sensor is doing is taking two 
readings of infrared (IR) light and comparing them. If the values ever differ, it assumes that something 
warm has moved in its .eld of view and it raises the voltage on the output pin. If nothing changes after 
some amount of time, it lowers that signal. The Fresnel lens allows a whole hemisphere map onto those 
two regions of interest. From your microprocessor program s point of view this looks just like a switch. 
You can read the value on the P0 signal (Figure 28) using digitalRead() and use a HIGH reading to indicate 
that something is moving. Your sensor may have a different pin connection than the one shown here. It 
may even have an open drain output that requires a pullup resistor just like a toggle switch. There may 
also be ways to adjust the sensitivity and timeout 3.1: Switches Figure 3.6: A passive infrared (PIR) 
sensor. The milky white plastic piece on the front is a Fresnel lens that focuses infrared radiation 
(heat) onto the sensor. That lens modi.es a physical sensor that looks only in one direction and allows 
it to see into an entire hemisphere. The three pins at the bottom of the right-hand image are the connection 
pins: signal, power (3v to 6v DC), and ground. The P0 pin in this .gure will be low until motion is sensed. 
That pin goes high when motion has been observed by the sensor. (the time it takes to reset after movement 
has been detected) of your sensor. Read the information that came with your sensor carefully. A simple 
program to test a PIR sensor using the serial monitor and the Arduino s built­in LED as indicators is 
shown here. This simple PIR test program is from Limor Fried (Lady Ada) and can be found with more details 
about PIR sensors at www.ladyada.net/ wiki/tutorials/learn/sensors/pir.html. / * * PIR s e n s o r t 
e s t e r * / i n t l e d P i n = 1 3 ; / / c h o o s e t h e p i n f o r t h e LED i n t i n p u t P 
i n = 2 ; / / c h o o s e t h e i n p u t p i n ( f o r PIR s e n s o r ) i n t p i r S t a t e = LOW; 
/ / we s t a r t , a s s u m i n g n o m o t i o n d e t e c t e d i n t v a l = 0 ; / / v a r i a b 
l e f o r r e a d i n g t h e p i n s t a t u s v o i d s e t u p ( ) {pinMode ( l e d P i n , OUTPUT) 
; / / d e c l a r e LED a s o u t p u t pinMode ( i n p u t P i n , INPUT ) ; / / d e c l a r e s e n 
s o r a s i n p u t S e r i a l . b e g i n ( 9 6 0 0 ) ; / / i n i t s e r i a l c o n n e c t i o n 
f o r o u t p u t } v o i d l o o p ( ) {v a l = d i g i t a l R e a d ( i n p u t P i n ) ; / / r e 
a d i n p u t v a l u e i f ( v a l == HIGH ) { / / c h e c k i f t h e i n p u t i s HIGH d i g i t 
a l W r i t e ( l e d P i n , HIGH ) ; / / t u r n LED ON i f ( p i r S t a t e == LOW) { / / we h a 
v e j u s t t u r n e d o n S e r i a l . p r i n t l n ( Mo ti on d e t e c t e d ! ) ; / / We o 
n l y w a n t t o p r i n t o n t h e o u t p u t c h a n g e , n o t s t a t e p i r S t a t e = HIGH 
; } / / e n d i f ( p i r s t a t e == LOW) } e l s e { / / ( v a l != HIGH ) d i g i t a l W r i t e 
( l e d P i n , LOW) ; / / t u r n LED OFF i f ( p i r S t a t e == HIGH ) { / / we h a v e j u s t 
t u r n e d o f f S e r i a l . p r i n t l n ( Mo ti on ended ! ) ; / / We o n l y w a n t t o p 
r i n t o n t h e o u t p u t c h a n g e , n o t s t a t e p i r S t a t e = LOW; } / / e n d i f ( 
p i r S t a t e == HIGH} } / / e n d e l s e ( v a l != HIGH ) } / / e n d l o o p ( )  3.2 Resistive 
Sensors Switches are great sensors. They re easy to use, easy to sense, and easy to install both physically 
and electrically. But, they have only two states: on and off. There is another class of sensors that 
can return information over a whole range of values. The easiest to use of these sensors are the so-called 
resistive sensors. These sensors act like resistors, but with the feature that based on some environmental 
condition they will change their resistivity. This change in resistivity can be sensed by your program 
and used to change program behavior based on a range of conditions. Because these sensors react to conditions 
by changing their resistance, in order to use them you must be able to sense a change in resistance. 
The easiest way to do this is with a voltage divider circuit (see Big Idea #6 in Section 1.2). To review, 
a voltage divider is a circuit that has two resistors in series. Kirchhoff s current and voltage laws 
tell us that both resistors will see the same current, and the entire voltage from the power supply to 
ground will be dropped across the series-connected device. Furthermore, Ohm s law tells us that the magnitude 
of the voltage drop will be proportional to the resistance of the components. If you build a voltage 
divider with two resistors, but one of the resistors is a variable resistor, then the voltage divided 
in the middle of the circuit will change proportionally to the changing resistance. In Figure 3.7 there 
are two variations on this theme: one where the OUT signal goes to Vdd (+5v in our case) as the variable 
resistance goes towards 0O, and one where OUT goes to 0v in that case. All that s needed now are to .nd 
sensors that behave like variable resistors, and to .gure out how to sense the continuously variable 
voltage that results from a voltage divider under changing resistance situations. 3.2.1 Potentiometers 
A potentiometer is a variable resistor where the resistance is changed by turning a knob. These types 
of components are everywhere: volume knobs and light dimmers are two extremely common examples. The sliding 
controls you see on music mixing boards are an example of a linear style potentiometer. A potentiometer 
(called a pot for short) is a piece of resistive material that is connected just like a regular resistor. 
The distinguishing feature is that there s third terminal that is connected to a wiper that can move 
up and down, or around, that material and make contact partway through the resistor. The resistance between 
the endpoints is .xed. The resistance seen at the third terminal varies by how far you ve turned the 
knob, or how far you ve moved the slider. The behavior of a potentiometer allows a particularly simply 
technique for connecting it to a microprocessor like Arduino. As seen in Figure 3.9, the pot can be connected 
with 3.2: Resistive Sensors  Figure 3.8: A cutaway drawing of a potentiometer. The resistance between 
terminals A and B is always .xed because the signal goes through all of the resistive material. As you 
turn the knob, the wiper goes further to the left or right. As it does, the relative resistance between 
A and W changes, as does the resistance between W and B. If the wiper is all the way counterclockwise, 
then the resistance between A and W is very low (essentially 0O) and the resistance between W and B is 
the max amount of resistance for the pot. If the wiper is turned all the way clockwise, the result is 
opposite. Vdd (+5v) at one end, and GND at the other. Because the resistance is .xed at the end terminals 
of the pot, the only concern is whether there is enough total resistance in the pot to avoid a meltdown 
(current-limiting). Any pot with a total resistance of 10kO or more should be .ne. Now if you turn the 
knob (or move the slider) you ll be changing the point where you re dividing the resistance, and thus 
see a voltage that varies between Vdd and GND on that OUT signal. The remaining question is how to connect 
the OUT signal (wiper) of the pot to the Ar­duino in a way that your program can sense the entire range 
of voltages that are generated by the pot. This is where the analog inputs of the Arduino are used. The 
analog inputs are connections to an internal circuit on the Arduino s microprocessor called an Analog 
to Digital Converter (ADC). An ADC takes a continuously varying voltage across some range and converts 
it into a digital number, again with some range. In our case the ADC has 10 bits of resolution. That 
is, it can take the range of voltages that it s designed for (0v to +5v in our case) and sense intermediate 
voltages in that range in 1024 increments. That is, it can sense 1024 steps in voltage between 0v and 
5v. Another way to think about this is that it can sense approximately 4.9mV (0.0049v) changes in the 
analog signal. The practical effect is that you can put a voltage that varies between 0v and 5v into 
an analog input, and get back an integer between 0 and 1023 that tells you what that voltage is in 4.9mV 
steps. The Arduino function to use is analogRead(pin) where the analog pin is A0, A1, A2, A3, A4 or A5. 
Plain integers 0 through 5 also work as identi.ers for the analog pin being read. Some simple Arduino 
code that reads an analog sensor (like a potentiometer) and uses it to adjust the brightness of an LED 
is shown below. Note that analogRead() reads the value of the analog pins. analogWrite() sends a PWM 
value to a digital output pin. They sound similar, but they work on completely different sets of pins 
on the Arduino. 3.2: Resistive Sensors / * * R e a d t h e a n a l o g v a l u e c o m i n g f r o m 
a p o t e n t i o m e t e r ( t h e s e n s o r ) * a n d u s e t h a t v a l u e t o f a d e a n LED 
u s i n g a n a l o g W r i t e ( ) .  * / i n t s e n s o r P i n = A2 ; / / a n a l o g p i n u s 
e d t o c o n n e c t t h e p o t e n t i o m e t e r i n t l e d P i n = 9 ; / / d i g i t a l p i n 
f o r a n e x t e r n a l LED ( a PWM p i n ) i n t v a l ; / / v a r i a b l e t o r e a d t h e v a 
l u e f r o m t h e a n a l o g p i n v o i d s e t u p ( ) {pinMode ( l e d P i n , OUTPUT) ; / / d 
e f i n e l e d P i n a s o u t p u t } v o i d l o o p ( ) {v a l = a n a l o g R e a d ( s e n s o 
r P i n ) ; / / r e a d s t h e s e n s o r ( v a l u e 0 -1 0 2 3 ) v a l = map ( v a l , 1 8 0 , 6 
2 0 , 0 , 2 5 5 ) ; / / s c a l e i t b e t w e e n 0 a n d 2 5 5 v a l = c o n s t r a i n ( v a l , 
0 , 2 5 5 ) ; / / k e e p i t i n t h e r i g h t r a n g e a n a l o g W r i t e ( l e d P i n , v a 
l ) ; / / f a d e t h e LED d e l a y ( 1 5 ) ; / / w a i t a b i t } For an example of a physical connection 
of a potentiometer to an Arduino see Fig­ure 3.10. In this .gure the endpoints of the pot are connected 
to +5v and GND, and the center tap (the wiper) is connected to analog pin A2. This physical connection 
is the same as the schematic in the left part of Figure 3.9.  3.2.2 Other Resistive Sensors A potentiometer 
is in some sense the most fundamental resistive sensor. It is also the most physical: it is sensing how 
much the knob is turned or the slider is moved. There are other resistive sensors that react to environmental 
conditions without physical contact or movement. For example, a Cadmium Sul.de (CdS) light sensor is 
a component that changes its resistance depending on how much light falls on the sensor. These sensors 
often have a .at-topped look with a squiggly line in the surface of the sensor. That squiggly line is 
the CdS light sensitive material. It is this material that changes its resistance when light on it. It 
has a very high resistance in the dark, and has a much lower resistance when light is shining on it. 
Typical values might be 200kO in the dark and 10kO in bright light, but these cells vary tremendously 
so it s a good idea to test them using a resistance meter. These sensors can take the place of the variable 
resistors in Figure 3.7. You can use them in either con.guration as seen in Figure 3.11. Connect the 
OUT signal from the voltage di­ vider to one of the analog inputs of the Arduino and read the value using 
analogRead(pin). Because the CdS cell has high resistance in the dark and lower resistance in the light, 
the OUT value for the left hand circuit in Figure 3.11 will be low in the dark and high in the light. 
The right hand circuit will be opposite: OUT will be high in the dark and lower in the light. Some CdS 
light sensors in various sizes are shown in Figure 3.12 There are a huge range of resistive sensors 
that behave just like CdS light sensors. That is, they react to some environmental condition and change 
their resistance accordingly. They can all be connected in the same way as the CdS light sensors in Figure 
3.11. The only issue to be careful of is to know or check the range of possible resistances that the 
sensor can take on. If the sensor can take on a value close to 0O then the .xed resistor must be large 
enough to limit the current to a safe level. Like a switch (which can take on a value very close to 0O 
when the switch is closed) a 10kO resistor is a good starting point. If the sensor can t go below 10kO 
on its own, then a separate .xed resistor may not be necessary. Environmental sensors that react by changing 
their resistance include: Temperature: known as thermistors, these sensors typically lower their resistance 
as tem­perature rises. 3.3: Analog Sensors Flex: This ribbon-like sensors change their resistance when 
bent. Pressure: These sensors react to force applied directly to the sensor. Gas: These sensors change 
their resistance based on concentrations of various gasses in the atmosphere. Speciality sensors for 
Alcohol, C02, CO, Methane, Hydrogen, and other gasses are common.  3.3 Analog Sensors Resistive sensors 
are easily used in a voltage divider con.guration. By putting them in a voltage divider you are generating 
an analog voltage that changes as the sensor changes. There is another class of sensor that generates 
an analog output signal directly without having to be connected as a voltage divider. These analog-output 
sensors can be connected directly to an analog input of the Arduino and be read using the same analogRead(pin) 
function as used with a voltage divider. A great example of an analog sensor is the Sharp IR Distance 
sensor. This sensor, shown in Figure 3.13, sends an IR signal out of one side, and senses the return 
on the other. It uses this information to sense the closest object (that re.ects IR) to the sensor. The 
sensors come in a range of sizes that detect objects in different distance ranges. The sensor shown in 
Figure 3.13, for example, senses objects in the range of 20cm to 150cm (approx 8in to 60in). Other Sharp 
sensors have different ranges including very short distances from the sensor (10cm or less). The interface 
is three wires: power (+5v), ground, and output. The output in this case is an analog voltage that ranges 
from approx 2.7v at 20cm to approx 0.4v at 150cm. This output signal can be connected directly to an 
analog input pin on the Arduino where an analogRead(pin) will result in an integer approximately in the 
range of 550 (2.7v) to 80 (0.4v). Unfortunately, a linear change in distance sensed does not result in 
a linear change in output voltage. The curve from the Sharp data sheet is shown in Figure 3.13. Some 
mapping code in software, perhaps something as simple as a lookup table, will be required to really decode 
distance. Sharp distance sensors are great devices, especially for short distances. However, if you 
d like to sense longer distances, a sonar-based sensor like those made by Maxbotics are a good choice. 
These operate by sending a sonar ping and sensing the returned signal with a small microphone. They are 
interfaced in exactly the same way as the Sharp sensor: power ground and analog signal. An example of 
this type of sensor is seen in Figure 3.14. This sensor operates over a wider range of power supply voltage 
(2.5v to 5v), over a longer distance (15cm (6in) to 645cm (254in)), and with a more linear analog behavior 
(Vdd/512v per inch). They are a little more expensive than the Sharp IR sensors, and not effective at 
short ranges under 15cm. As with the resistive sensors, there are a wide variety of sensors that return 
an analog voltage that relates to the property being sensed. Some examples include: Acceleration: You 
can get 1-axis, 2-axis, and 3-asix accelerometers that return an analog voltage that varies with the 
current acceleration in that axis direction. Gyroscopes: Gyros tell you what their current orientation 
are in 2-or 3-dimensions. This information can be returned as an analog voltage. Color: This is similar 
to a light sensor, but returns different analog voltage levels that en­code each of the Red, Green, and 
Blue components of the light falling on the sensor. 3.4 Sensor Calibration Many sensors have a wide 
variation in their behavior. A CdS light sensor, for example, might vary in its range of light and dark 
resistance by more than 50% even within the same 3.4: Sensor Calibration size and batch of sensors. 
Even if you know exactly what range of resistances you will see with a particular sensor, the behavior 
will be very different depending on the light levels in the environment in which the sensor is installed. 
A perfectly working system in one room might not react at all in a different room with a different light 
pro.le. Other sensors such as the IR distance sensors have nominal voltage/distance curves, but will 
have noticeably different behavior at different temperatures, or with small power supply voltage variations. 
Because of these variations in behavior, it s always a good idea to calibrate your sensors. Calibration 
in this context simply means to run your program in a mode where the range of sensor values is reported 
so that you can modify your system to react to the range of values that you ll actually see. The easiest 
way to calibrate is simply to use the serial monitor to print the range of values that you re seeing 
from your sensor. A simple print statement calibration program is seen below. / * * T h i s c o d e i 
s b a s e d o n t h e r e a d A n a l o g V a l u e c o d e i n t h e A r d u i n o * e x a m p l e 
s e t . I t r e a d s t h e a n a l o g v a l u e c o m i n g i n o n a n a n a l o g * i n p u t a 
n d p r i n t s i t t o t h e s e r i a l m o n i t o r , a l o n g w i t h a n e s t i m a t e * o f 
t h e a c t u a l v o l t a g e t h a t t h e ADC i s r e a d i n g .  * / i n t s e n s o r P i n 
= A0 ; / / w h i c h p i n a r e y o u r e a d i n g s e n s o r d a t a o n ? v o i d s e t u p ( ) 
{S e r i a l . b e g i n ( 9 6 0 0 ) ; / / i n i t i a l i z e t h e s e r i a l c o n n e c t i o n 
} v o i d l o o p ( ) {i n t s e n s o r V a l u e = a n a l o g R e a d ( s e n s o r P i n ) ; / / 
r e a d a n a l o g p i n f l o a t v o l t a g e = s e n s o r V a l u e * ( 5 . 0 / 1 0 2 3 . 0 ) ; 
/ / c o n v e r t t o a v o l t a g e / / p r i n t t h e v a l u e s S e r i a l . p r i n t ( A n a 
l o g v a l u e and v o l t a g e a r e : ) ; S e r i a l . p r i n t ( s e n s o r V a l u e ) ; S e 
r i a l . p r i n t ( ADC v a l u e , ) ; S e r i a l . p r i n t ( v o l t a g e ) ; S e r i a l . p 
r i n t l n ( v ) ; d e l a y ( 1 0 0 ) ; / / d e l a y f o r a b i t } This form of manual calibration 
is simple, but effective. It s easy to build this in to your program, perhaps as a separate mode that 
runs when a switch is .ipped. It could also be a separate calibration program that you run when you are 
concerned that conditions might have changed. Based on the values you see printed on the serial monitor, 
you can update map() functions or other pieces of interpolation in your program. Another technique for 
calibration is to automatically calibrate by entering a program mode that collects data from the sensor 
and that records the high and low values seen. After that phase those high and low values can be used 
to interpolate the remaining data. The Arduino IDE includes an example of this sort of self-calibration. 
That code is repeated here: / * C a l i b r a t i o n D e m o n s t r a t e s o n e t e c h n i q u e 
f o r c a l i b r a t i n g s e n s o r i n p u t . T h e s e n s o r r e a d i n g s d u r i n g t h 
e f i r s t f i v e s e c o n d s o f t h e s k e t c h e x e c u t i o n d e f i n e t h e minimum a 
n d maximum o f e x p e c t e d v a l u e s a t t a c h e d t o t h e s e n s o r p i n . T h e s e n 
s o r minimum a n d maximum i n i t i a l v a l u e s may s e e m b a c k w a r d s . I n i t i a l l 
y , y o u s e t t h e minimum h i g h a n d l i s t e n f o r a n y t h i n g l o w e r , s a v i n g 
i t a s t h e new minimum . L i k e w i s e , y o u s e t t h e maximum l o w a n d l i s t e n f o r 
a n y t h i n g h i g h e r a s t h e new maximum . T h e c i r c u i t : * A n a l o g s e n s o r ( 
p o t e n t i o m e t e r w i l l d o ) a t t a c h e d t o a n a l o g i n p u t 0 * LED a t t a c 
h e d f r o m d i g i t a l p i n 9 t o g r o u n d c r e a t e d 2 9 O c t 2 0 0 8 By D a v i d A M 
e l l i s m o d i f i e d 3 0 Aug 2 0 1 1 By Tom I g o e h t t p : / / a r d u i n o . c c / e n / T 
u t o r i a l / C a l i b r a t i o n T h i s e x a m p l e c o d e i s i n t h e p u b l i c d o m a 
i n .  * / / / T h e s e c o n s t a n t s won t c h a n g e c o n s t i n t s e n s o r P i n = A0 
; / / p i n t h a t t h e s e n s o r i s a t t a c h e d t o c o n s t i n t l e d P i n = 9 ; / / p 
i n t h a t t h e LED i s a t t a c h e d t o // variables : i n t s e n s o r V a l u e = 0 ; / / t 
h e s e n s o r v a l u e i n t s e n s o r M i n = 1 0 2 3 ; / / minimum s e n s o r v a l u e i n t 
se n s o r M a x = 0 ; / / maximum s e n s o r v a l u e v o i d s e t u p ( ) { / / t u r n o n LED 
t o s i g n a l t h e s t a r t o f t h e c a l i b r a t i o n p e r i o d : pinMode ( 1 3 , OUTPUT) 
; d i g i t a l W r i t e ( 1 3 , HIGH ) ; / / c a l i b r a t e d u r i n g t h e f i r s t f i v e 
s e c o n d s w h i l e ( m i l l i s ( ) < 5 0 0 0 ) { s e n s o r V a l u e = a n a l o g R e a d ( 
s e n s o r P i n ) ; / / r e c o r d t h e maximum s e n s o r v a l u e i f ( s e n s o r V a l u 
e > s e n s o r M a x ) { s e n s o r M a x = s e n s o r V a l u e ; } / / r e c o r d t h e minimum 
s e n s o r v a l u e i f ( s e n s o r V a l u e < s e n s o r M i n ) { s e n s o r M i n = s e n s 
o r V a l u e ; } } 3.5: Questions about Sensors / / s i g n a l t h e e n d o f t h e c a l i b r 
a t i o n p e r i o d d i g i t a l W r i t e ( 1 3 , LOW) ; } v o i d l o o p ( ) { / / r e a d t h 
e s e n s o r : s e n s o r V a l u e = a n a l o g R e a d ( s e n s o r P i n ) ; / / a p p l y t h 
e c a l i b r a t i o n t o t h e s e n s o r r e a d i n g s e n s o r V a l u e = map ( s e n s o r 
V a l u e , s e n s o r M i n , s enso r M a x , 0 , 2 5 5 ) ; / / i n c a s e s e n s o r v a l u e 
i s o u t s i d e t h e r a n g e s e e n d u r i n g c a l i b r a t i o n s e n s o r V a l u e = c 
o n s t r a i n ( s e n s o r V a l u e , 0 , 2 5 5 ) ; / / f a d e t h e LED u s i n g t h e c a l i 
b r a t e d v a l u e : a n a l o g W r i t e ( l e d P i n , s e n s o r V a l u e ) ; } 3.5 Questions 
about Sensors Some questions that you should ask when considering sensors for use with a controller like 
the Arduino are: How many digital and/or analog pins are available to use? Switches, PIR motion sensors, 
and other switch-style sensors use digital pins. Re­sistive and analog sensors use analog pins. There 
are external chip solutions for ex­panding the number of both types of pins, but without resorting to 
those external chips, there are a limited number of each of these pins (14 digital pins and 6 analog 
pins). If you re using a switch, what type of switch do you need? The most common switches are single 
pole, single throw (SPST) and single pole, dou­ble pole (SPDT). Make sure you know what the connections 
are. Use a voltmeter con.gured as a continuity tester if you re not sure. Almost all voltmeters have 
a con­tinuity setting. In this setting you attach the two leads of the tester to the terminals that you 
re interested in. If the meter buzzes, the terminals are connected. If it makes no sound, they are not 
connected. Will your sensor need debouncing? Many switch-type sensors will bounce. There may be situations 
where you don t care about occasional extra bounces being read as separate events. But if you do care, 
and there is a possibility of bouncing, you need to consider a debouncing strategy. Do you need a manufactured 
switch, or can you make your own?  If using a digital input with a switch of some sort, how much current 
will be drawn when the switch is closed?  Switch functionality can be made of almost any conductive 
material. There are many creative ways to make something that acts like a switch. Any time there might 
be a connection between power and ground, you should al­ways ask whether a current-limiting resistor 
is needed, and if so, what value of resis­tor is needed? Standard switches make a very low-resistance 
connection when closed so a 10kO or larger resistor is typically used. Other switch types, or homemade 
switch devices might have different closed-resistances. If using a resistive sensor in a voltage divider, 
what should the .xed resistance be? This is yet another current-limiting question. You want to make sure 
that at the limits of the variable resistor that not too much current will .ow from power to ground. 
The tradeoff is that if you size the .xed resistor to be too large, then the range of voltages that you 
ll see from the voltage divider will be restricted, so you won t get as much resolution. If you know 
the range of resistances that your sensor will produce you can size the .xed resistor so that the entire 
voltage divider has 5kO to 50kO in the lowest resistance situation. Potentiometers are great sensors 
and are easy to use. Is there a way to use a potentiometer to sense what you need to sense? There are 
lots of ways to connect things to pots to sense their position. Remember the classic arcade driving games 
with steering wheels? Those were simply connected to pots to see how far the user turned the wheel. A 
linear pot (slider) is a great way to sense movement in on dimension. If using an analog sensor, what 
range of analog voltages will you see at the analog inputs? Knowing the range of possible voltages, either 
through the data sheet or through calibration, can enable you to size your map() function to get optimal 
range out of your sensor. Can you build a calibration mode into your application? Calibration is very 
important when using sensors. If there s any way to build a cali­bration mode into your application it 
will make your life much easier. Don t assume that the values you see the .rst time you run your system 
will be the same over the life of the system, or when you take your system to alternate locations. How 
frequently do you need to check your sensor s value? Your Arduino is running at 16MHz (16,000,000 cycles/sec.). 
That means that it can execute a new assembly language instruction on every clock tick. Of course, a 
C++ instruction might take multiple assembly instructions to execute, but that s still a lot of instructions 
in human terms per second. If you are reading a sensor that operates on a human time scale, you may not 
need to check it as frequently as you think in order to get good data. That being said, using a delay(ms) 
instruction is a busy-wait. The processor is not doing anything useful other than counting down until 
the delay is .nished. If there are other things that your application can be doing, you should use the 
millisecond counting technique to decide when to check the sensor again. A simple example of this follows 
(from the Arduino blinkWithoutDelay program). More advanced solu­tions would use timer interrupts, or 
interrupts based on external events, to trigger activities. Those techniques will not be covered in this 
course. Ideas about using interrupts can be found on the Arduino web site, and other places on the web. 
c o n s t i n t l e d P i n = 1 3 ; / / t h e n u m b e r o f t h e LED p i n i n t l e d S t a t e = 
LOW; / / l e d S t a t e u s e d t o s e t t h e LED l o n g p r e v i o u s M i l l i s = 0 ; / / w 
i l l s t o r e l a s t t i m e LED w a s u p d a t e d / / t h e f o l l o w v a r i a b l e s i s 
a l o n g b e c a u s e t h e t i m e , m e a s u r e d i n / / m i l i s e c o n d s , w i l l q u i 
c k l y b e c o m e a b i g g e r n u m b e r t h a n c a n b e s t o r e d / / i n a n i n t . l o n 
g i n t e r v a l = 1 0 0 0 ; / / i n t e r v a l a t w h i c h t o b l i n k ( m i l l i s e c o n d 
s ) v o i d s e t u p ( ) { 3.5: Questions about Sensors pinMode ( l e d P i n , OUTPUT ) ; / / s e 
t t h e d i g i t a l p i n a s o u t p u t : } v o i d l o o p ( ) { / * * H e r e i s w h e r e y 
o u d p u t c o d e t h a t n e e d s t o b e r u n n i n g a l l t h e * t i m e .  */ / / c h e 
c k t o s e e i f i t s t i m e t o b l i n k t h e LED ; t h a t i s , i f t h e / / d i f f e r e 
n c e b e t w e e n t h e c u r r e n t t i m e a n d l a s t t i m e y o u b l i n k e d / / t h e LED 
i s b i g g e r t h a n t h e i n t e r v a l a t w h i c h y o u w a n t t o / / b l i n k t h e LED 
. u n sig n e d l o n g c u r r e n t M i l l i s = m i l l i s ( ) ; i f ( c u r r e n t M i l l i 
s - p r e v i o u s M i l l i s > i n t e r v a l ) { / / s a v e t h e l a s t t i m e y o u b l i n 
k e d t h e LED p r e v i o u s M i l l i s = c u r r e n t M i l l i s ; / / i f t h e LED i s o f f 
t u r n i t o n a n d v i c e -v e r s a : i f ( l e d S t a t e == LOW) l e d S t a t e = HIGH ; e l 
s e l e d S t a t e = LOW; / / s e t t h e LED w i t h t h e l e d S t a t e o f t h e v a r i a b l 
e : d i g i t a l W r i t e ( l e d P i n , l e d S t a t e ) ; } } How much power does your sensor 
consume? All the sensors described in this course are reasonably low-power sensors that shouldn t tax 
the total power available from your Arduino s voltage regulator. But, there are certainly sensors out 
there that use more power. A miss-sized resistor can also con­sume extra power. If you do have a high-power 
sensor, you may need to use a sepa­rate power supply. Remember to connect the grounds of all separate 
power supplies together.  Chapter 4 Action! Motors We ve looked at lights (LEDs), and inputs (sensors). 
The next big piece of physical com­puting, in many ways the essence of physical computing, is movement. 
The easiest way to make things move is with a motor of some sort. There are other ways of causing move­ment, 
but motors are inexpensive, and easy to use. They do take a bit of planning though, especially in terms 
of power. Making anything signi.cant move will take a lot more power than an Arduino board can provide. 
The general strategy is to have the controller (like Arduino) control the movement, but have the power 
for the motors come from a separate power supply. In fact, the power supply that is powering the motor 
can often also be used to power the Arduino. 4.1 Hobby Servos Hobby servos are designed to easily move 
small objects through a controlled range of mo­tion. They are an example of a motor connected to a servo-mechanism 
which means that there is some sort of feedback involved in the system that can be used to control the 
posi­tion of the motor. In this case the servo s motor is sent through some gears to reduce the range 
of motion (and increase the torque) and ends up going through a potentiometer that is used as the feedback. 
The servo s output shaft is controlled by sending a PWM signal to the motor. That signal, combined with 
the position sensing from the potentiometer, con­trols the amount of rotation of the servo s shaft. That 
means that by adjusting the PWM signal the servo shaft s position can be controlled reasonably precisely 
in a range of around 180 degrees. Hobby servos are designed primarily for radio-controlled (RC) models. 
RC airplanes and cars have radio transmitters and receivers to transfer information, and that information 
is, for the most part, what position the servos should be in. A typical servo expects to see a control 
pulse every 20ms. The width of that pulse determines how far the motor turns. Common values for the pulse 
width are 1ms for 0 degrees rotation (typically anti-clockwise as far as the servo shaft will rotate), 
a 1.5ms pulse for 90 degree rotation from that 0 point, and 2ms for 180 degrees from the starting point. 
Using the Arduino delay functions it would be reasonably easy to write a program that drives the control 
signal of a servo according to that formula. However, servos are so common that the Arduino IDE comes 
with a Servo library that works well and saves you the trouble of .guring out when to raise and lower 
the control signal. The Servo library  4.1: Hobby Servos works by having you create a Servo object for 
each servo that you would like to control. The API is: #include <Servo.h> Include the Servo library in 
your program. Servo <name1>, <name2>; Create servo objects for each servo in your system <name1>.attach(pin); 
Attaches a servo to an Arduino digital output pin <name1>.write(angle); Move the servo to angle degrees. 
The angle argument must be in the range 0 to 179. <name1>.writeMicroseconds(us); Use this method if you 
d like to adjust the microseconds of the PWM pulse directly. Useful ranges are 1000 to 2000 for standard 
servos, but you may have an oddball servo that needs different pulse widths. There are also methods for 
reading back the current angle of the servo <name1>.read(), what pin it s currently attached to <name1>.attached(), 
and detaching the servo from that pin <name1>.detach(), but they are less frequently used. A nice example 
program for a servo is found in the Arduino example set: / / C o n t r o l l i n g a s e r v o p o s 
i t i o n u s i n g a p o t e n t i o m e t e r / / ( v a r i a b l e r e s i s t o r ) / / b y M i c 
h a l R i n o t t h t t p : / / p e o p l e . i n t e r a c t i o n -i v r e a . i t / m . r i n o t 
t / / / / M o d i f i e d s l i g h t l y b y E r i k B r u n v a n d / / T h i s c o d e a s s u m e 
s t h a t y o u h a v e a p o t e n t i o m e t e r a t t a c h e d w i t h / / t h e e n d t e r m i 
n a l s c o n n e c t e d t o +5 v a n d 0 v , a n d t h e c e n t e r w i p e r / / t e r m i n a l 
a t t a c h e d t o a n a l o g p i n A0 . T h e S e r v o h a s i t s p o w e r / / c o n n e c t e 
d t o +5 v , g r o u n d t o 0 v , a n d c o n t r o l t o d i g i t a l p i n 9 / / T h e d e l a y 
( 1 5 ) i s i m p o r t a n t - y o u h a v e t o g i v e t h e s e r v o t i m e / / t o g e t t o i 
t s new p o s i t i o n a f t e r y o u u p d a t e . S e r v o s a r e p h y s i c a l / / o b j e c 
t s a n d t a k e t i m e t o m o v e t o t h e i r new p o s i t i o n . / / T h e S e r v o l i b r 
a r y i s d o c u m e n t e d a t / / h t t p : / / a r d u i n o . c c / e n / R e f e r e n c e / S 
e r v o # i n c l u d e <S e r v o . h> S e r v o myservo ; / / c r e a t e s e r v o o b j e c t t o 
c o n t r o l a s e r v o i n t p o t P i n = A0 ; / / a n a l o g p i n u s e d t o c o n n e c t t 
h e p o t e n t i o m e t e r i n t v a l ; / / v a r i a b l e t o r e a d t h e v a l u e f r o m t 
h e a n a l o g p i n v o i d s e t u p ( ) {myservo . a t t a c h ( 9 ) ; / / a t t a c h t h e s e 
r v o o b j e c t t o p i n 9 o b j e c t } v o i d l o o p ( ) {v a l = a n a l o g R e a d ( p o t 
P i n ) ; / / r e a d f r o m t h e p o t ( 0 -1 0 2 3 ) v a l = map ( v a l , 0 , 1 0 2 3 , 0 , 1 7 
9 ) ; / / s c a l e f o r t h e s e r v o ( 0 -1 7 9 ) v a l = c o n s t r a i n ( v a l , 0 , 1 7 9 
) ; / / m a k e s u r e t o s t a y i n r a n g e myservo . w r i t e ( v a l ) ; / / s e t s e r v o 
p o s i t i o n t o t h e s c a l e d v a l u e d e l a y ( 1 5 ) ; / / w a i t f o r t h e s e r v o 
t o g e t t h e r e } Servos are wonderful devices -they re inexpensive, easy to control with the Servo 
li­brary, and provide reasonably precise position control in the range of 0-179 degrees. There are a 
variety of ways to connect them to things in your system that allow things to rotate, or (with an appropriate 
linkage) move back and forth. Servos also use a bit more power than any of the other devices that we 
ve seen in this course. A couple small servos should be .ne when connected to the +5v output of the Arduino, 
but remember that there s a limit to how much current that output can pro­vide (500mA total when connected 
to USB, and 800mA when connected to a 9v power adapter). If you re using more than a couple small servos, 
or larger servos, you ll want to connect them to a separate power supply. You can easily use a separate 
+5v power supply (like an AC adapter, or wall wart) just for the servos. Most servos will operate with 
a DC voltage from around 4.8v to 6.0v. Connect your servo power and ground to that separate power supply, 
and your Arduino to USB or to its own power supply. Remember Big Idea #7 and connect the grounds of the 
servo power and the Arduino power together. The Arduino s Servo library can control up to 12 servos, 
and you should note that it also disables the PWM (analogWrite()) functionality on pins 9 and 10 whether 
there is a servo connected to those pins or not. If you need to control more than 12 servos, or if you 
would like to control multiple servos from fewer pins, there are many multiple servo-driver solutions 
out there. Some are based on the TLC5940 chip that we saw in Section 2.4.4. That chip has 16 outputs 
where each output has its own separate PWM frequency. It s natural to use those for dimming LEDs, but 
it works almost as well for controlling multiple servos. The TLC5940 library has a servo-control option. 
A standard servo moves through 180 degrees of motion, but you can also buy con­tinuous rotation servos 
that spin rather than stopping at a .xed spot. These servos are controllable using the same PWM signals 
that a standard servo uses, and can thus be con­trolled using the Servo library and <name>.write(degrees). 
For a continuous rotation servo the 90 degree point is where the servo stops. A degree rating in between 
0 and 89 will spin the motor anti-clockwise with the motor speeding up as you go from 89 to 0. A degree 
speci.cation of 91 to 179 will spin the motor clockwise with the motor speeding up as you move from 91 
to 179.  4.2 DC Motors Inside of a servo is a small DC motor. That works well for the speci.c use in 
a servo, but sometimes you want a motor that spins (perhaps faster or with more torque than a continuous 
rotation servo), or a larger motor that can drive a larger load. In other words, sometimes you want to 
use a DC motor directly. DC motors are actually extremely simple devices -simpler in many respects than 
servos. A DC motor will spin whenever a suf.­ciently positive DC voltage is applied to the two inputs 
of the motor. If you reverse the voltage (reversing Vdd and GND), the motor will spin backwards. If a 
motor is rated for a range of voltages, the motor will spin faster and have more torque if you increase 
the voltage. A tiny motor like the one in Figure 4.3 will spin very fast (some tiny motors spin up to 
10,000RPM) but with very low torque. These motors consume very little power and can be powered from the 
+5v pin of the Arduino (assuming your motor is rated for 5v). A larger motor will not only be physically 
larger, it will require much more power (more current), and be much stronger. DC motors in sizes from 
something similar to a D-battery, to motors the size of a garbage can, or even larger, are staples of 
all sorts of machinery. A selection of motors slightly larger than the one in Figure 4.3 is shown in 
Figure 4.4. These 4.2: DC Motors motors run at voltages ranging from 9v to 24v. Some have gear reduction 
heads that slow the rotation speed and increase torque. All of these motors consume much more current 
than an Arduino can supply, so a separate power supply is a must. The external supply is also required 
if the motor supply is higher than the Arduino can tolerate (such as the 24v motors). What is needed 
in the case of these larger motors is some way of turning the motor on and off from a microcontroller 
like the Arduino, but have the power to the motor supplied by a separate supply. We could easily turn 
the motor on and off by putting a switch in series with the power supply connection to the motor. If 
we want a program to control the motor, we need an Arduino controlled switch that turns the voltage on 
and off to the motor. The perfect component for this is the transistor. A transistor is a component that 
does many interesting things. One of the simplest things that it can do is act as a switch. A basic transistor 
is a three-terminal device that has a schematic symbol as seen in Figure 4.6. The bipolar devices have 
three terminals named Base, Collector, and Emitter. The MOS devices have three terminals named Gate, 
Source and Drain. For our purposes these are identical devices with slightly different names. There are 
two main types of transistors that we re interested in, and two techniques for constructing them from 
silicon. The two types are NPN/NFET which turn on (close the switch) when the Base/Gate is at a high 
voltage, and PNP/PFET that close the switch when the Base/Gate is low. The operation (Bipolar terminology) 
is that if a high voltage is applied to the Base ter­minal, the transistor closes the switch and current 
can .ow between Collector and Emitter. If the Base is at a low voltage, the switch is open and no current 
can .ow between Collector and Emitter. This makes the transistor act like a switch that is controlled 
by the Base volt­age. The thing that makes this a great switch for our motor application is that a very 
small signal on the Base can switch a very large signal between Collector and Emitter. Bipolar devices 
like this can switch quite high voltages and quite high currents. A 2N2222 transis­tor is a standard 
example of a very common NPN device. It can switch up to 40v between Collector and Emitter, and can conduct 
up to 800mA of current.  Images by Wapcaplet, wikimedia commons 4.2: DC Motors Figure 4.6: Schematic 
symbols for the major types of transistors used in physical computing. The three terminals o the devices 
have different names, but behave the same for our purposes (using the transistors as switches). The bipolar 
devices have a Base that controls the switching between Collector and Emitter. The MOS devices have a 
Gate that controls the switching between Source and Drain. The NPN Bipolar and the NFET MOS devices will 
turn on (conduct) when the Base or Gate inputs are high. The PNP Bipolar and PFET MOS devices will conduct 
when the Base or Gate inputs are low. If you need to switch more current or higher voltages, a TIP120 
is another common NPN device. Technically this is what is known as a Darlington pair device, that is, 
it has two NPN transistors in series to boost the capacity. The TIP120 can switch up to 60v at 5A of 
current. If you need to pass more than about 1A of current you should put a heat sink on the transistor. 
The heat sink attaches to the metal tab on the package (see Figure 4.7). One extra thing to keep in 
mind with NPN Bipolar transistors is that when you raise a voltage on the Base terminal, some current 
will .ow into that terminal. It s common to put a small resistor in series with the Base terminal to 
limit that current. One could .gure exactly what this resistor could be, but as a rule of thumb most 
people use around 1000-1500O. As an example of using a transistor like the TIP120 to control a DC motor 
is shown in Figure 4.8. The transistor is acting as a switch controlled by the Arduino s digital output 
pin. The 1000O resistor is a current-limiting resistor that keeps too much current from .owing from the 
Arduino pin into the transistor s Base. This is a nice general schematic that can switch all sorts of 
devices that require more voltage or current than the Arduino can supply directly. Remember that if you 
use multiple power supplies you should always connect all the ground signals together.  Figure 4.8: 
An example of using a TIP120 transistor to switch a DC motor on and off. Pin 9 of the Arduino is driven 
to +5v or 0v using a digitalWrite(9); function. When the pin is at 5v the TIP120 conducts from Collector 
to Emitter and the motor be activated. When the pin is at 0v the TIP120 does not conduct and the motor 
turns off. The 1N4001 is a diode. It is used as a snubber diode to protect the circuit against reverse 
current as the motor is spinning down. With a TIP120 transistor, the Motor Power Supply can be up to 
60v DC. Big If you need to turn a device on and off that uses a higher voltage or more current than the 
Arduino can supply, you should use a transistor as a switch. The signal going to the Base of the transistor 
is easily driven by the Arduino, and the device power can be switched through the transistor s Collector 
and Emitter terminals. A DC motor is an example of an inductive load. This means that it acts somewhat 
like an inductor when current .ows though the device. Without going into details, this means that when 
current stops .owing in the device it reacts by resisting that stoppage of current though a burst of 
reverse current. In the motor s case this happens because a motor that is still turning after the power 
is removed acts like a generator. In any case, this possibility of reverse current can cause damage to 
the circuit. The diode in Figure 4.8 is a .yback diode or snubber diode that protects against this reverse 
current by letting it .ow through the diode instead of the transistor. The size of the diode is not critical 
and the 1N4001 is an easily obtained common diode for this application that can conduct up o one amp. 
If you re switching an inductive load with a transistor you should always use a snubber diode. 4.2.1 
Motor Speed Control DC motors have very simple control: when you apply a voltage to the terminals of 
the motor, the motor turns. They also have the interesting property that if you send pulses to motor, 
the motor will essentially integrate those pulses and act as though it is receiving a lower voltage. 
This can be used a speed control for the motor. This is yet another use for pulse width modulation (PWM). 
The percentage of time that the pulses are high is directly proportional to the speed of the motor. This 
PWM speed control works if the motor is connected directly to the Arduino, and it also works if it is 
connected through a transistor as in Figure 4.8 (as most motors will be). The switching tran­ 4.2: DC 
Motors sistor is more than fast enough to transfer a PWM signal on the Base into PWM switching between 
Collector and Emitter. To use PWM as speed control use analogWrite(pin); on the pin that is driving the 
Base of the switching transistor. 4.2.2 Motor Direction Control Another interesting feature of DC motors 
is that if you reverse the voltage on the control wires, the motor will switch direction. This is a feature 
that is used in all sorts of mechanical equipment. The trick is how to organize the circuit so that you 
can switch the voltages without grabbing the wires and physically changing them. The circuit that accomplishes 
this trick is the H-bridge. The H-bridge is a clever connection of transistors that allows the connection 
to a motor to be switched to be in either direction. A schematic for an H-bridge is shown in Figure 4.9. 
It consists of four switches or­ganized so that closing pairs of switches can cause the direction that 
current .ows in the motor to reverse (Figure 4.10. If the current reverses, so does the direction of 
the motor. The switches in an H-bridge can be physical switches, or electronic switches such as tran­sistors. 
For motors of the size typically used in physical computing systems transistors are the most common switches 
used. For larger motors that might need hundreds of volts, physical switches such as relays or contactors 
(heavy duty industrial relays) are often used in H-bridge circuits so that extremely high voltages and 
current can be switched. You could make an H-bridge yourself with four switches, or with four transistors. 
It s not uncommon for people to wire up four TIP120 transistors to make a high-current H­bridge. Actually, 
it s more likely that they would use two TIP120s and two TIP125s. The TIP120 NPN transistors turn on 
when you apply a high Base voltage, and the TIP125 s turn on when you supply a low Base voltage so it 
s easier to build an H-bridge with its one on and one off style with these complementary types of transistors. 
But, even more convenient than building an H-bridge with transistors is simply using an H-bridge chip. 
Because there are so many places where DC motors are used, there are a lot of pre-designed H-bridge chips 
that you could choose from. Two common chips that are great for physical computing because they re small, 
convenient, and perfectly sizes for the relatively small motors that physical computing systems tend 
to use (i.e. an amp or two) are the L293D and SN754410. These chips are called quad half H-bridge chips 
because they have four clusters of circuits on the chip, and each of the clusters implements half of 
an H-briidge (the left side of the circuit in Figure 4.9 for example). An overview of these chips is 
shown in Figure 4.11. Eavh of the triangle-shaped components in the .gure are half of an H-bridge as 
seen in the right side of the .gure. Figure 4.12 shows how these half H-bridges can be used to control 
motors. If you re just controlling a single-direction motor you can use a single half­H-bridge just to 
provide high-current switching as seen in the right side of the .gure. If you would like to reverse the 
direction of your motor you can use two of them con.gured as a full H-bridge as seen in the left of the 
.gure. The L293D and SN754410 are great chips to use for motor control. They come in a variety of packages 
and are easy to use with solderless breadboards. Some things to keep in mind when using these chips: 
 There is a separate Vdd pin for the motors, and for the chip itself. The chip itself needs +5v. The 
motor supply can be any voltage in the range of +5v to 36v.  Remember that if you re using multiple 
power supplies (such as one from Arduino  4.2: DC Motors Figure 4.11: Chips that implement H-bridge 
circuits include L293D or SN754410. These diagrams are taken from an L293D datasheet. These chips are 
known as quad half H-bridge chips because there are four circuits in the chip, each of which is half 
of an H-bridge as shown in the right side of this .gure. Two of these half H-bridge circuits can be used 
to make a full H-bridge and be used for controlling the direction of a DC motor. The enable inputs (pin 
1 and pin 9 on the chip) can be used with PWM for motor speed control. Figure 4.12: This .gure is taken 
from the data sheet for an L293D chip. It shows the chip being used to control a bidirectional motor 
on the left, and two single-direction motors on the right. In the bidirectional motor case, the Arduino 
would control the direction using pins 2 and 7 on the H-bridge chip. The unidirectional motors are controlled 
through pins 15 and 10 of the chip. These chip inputs may be driven direction from Arduino pins: no series 
resistance is needed because the H-bridge chip is designed to not sink current through the pins. Note 
that in all cases snubber diodes have been used to protect the circuit. In this circuit, Vcc2 at the 
bottom left is the motor s power supply and can be up to 36v. Vcc1 in the upper right is the chip s power 
and is +5v. This can come from the Arduino because although the chip switches high power devices, it 
doesn t itself consume must power. and one for the motor) you should tie the ground signals together. 
 If your motor is at a relatively high voltage you may want to put a heat sink on the driver chip. Note 
the ground pins in Figure 4.11 (pins 4, 5, 12, and 13). They are also marked as Heat Sink. This means 
that they dissipate heat from the chip as well as being ground connections. You can attach a metal heat 
sink to these pins, either by buying a clip-on heat sink for chips like this, or by soldering a piece 
of metal to the pins. That heat sink will radiate the generated heat and allow the chip to operate at 
higher motor voltages.  You can use PWM on the Enable signals to control the motor s speed. In Figure 
4.12 the bi-directional motor s speed could be controlled with PWM on Pin 1 of the driver chip.   4.3 
Stepper Motors Servo motors are great because they are positionable across a reasonably precise range 
of 0 to 180 degrees, and will hold that position once put there. But, they aren t very powerful, and 
they are limited to a 180 degree range. DC motors are great because they spin as long as you power them, 
they can be very powerful, and it s easy to control the speed with PWM. But, they aren t precisely positionable, 
and they don t stay put. A stepper motor is, in some sense, the combination of these two types of motors: 
it is a powerful DC motor that is also precisely positionable and will hold its position when stopped. 
They are used in a huge variety of consumer applications that require precise movement. The sensor/imager 
of a scanner, and the paper drive in a printer are driven from steppers, for example. The axes of a numerically 
controlled milling machine or X-Y plotter are also driven by stepper motors. The only downside of a stepper 
motor is that they re a little more complex to control than a DC motor or a servo. Stepper motors have 
multiple armatures that are wired so that if you energize them in sequence, the shaft is pulled around 
in small discrete steps (see Figure 4.13). Typical stepper motors have steps that range from 7.5 to 0.9 
degrees per step. This corresponds to 40 to 400 steps per revolution. This gives them nice .ne-control 
of position by choosing how many steps to move. One easy way to tell if the motor that you have is a 
regular DC motor or a stepper is by the number of wires used to control the stepper. A regular DC motor 
from the previous section has two wires. Stepper motors have four, six, or eight wires. The multiple 
wires control different coils inside the motor and are energized in very speci.c sequences to make the 
motor advance. There are two main varieties of stepper motors: unipolar and bipolar (see Figures 4.14 
and 4.15). A unipolar stepper has multiple coils that are each driven in sequence, but that are always 
driven in the same direction. That is, in Figure 4.14 the A1-A2 coil is always driven with + on A1 and 
GND on A2. In fact, A2 is always GND: the second coil is driven with + on A3 and GND on A2. A unipolar 
stepper will have six or eight wires depending on whether the GND wires of the .rst two coils are connected 
(as shown in Figure 4.14), or separated. A bipolar stepper has only four wires (as seen in the .gure). 
The circuit that drives a bipolar stepper must be able to reverse the current in those coils for different 
phases of stepper operation. If that sounds like an H-bridge might be the right circuit to use, you would 
be correct. 4.3: Stepper Motors Wapcaplet, wikimedia commons Vlastn fotogra.e, wikimedia commons http://www.ermicro.com/blog/ 
  http://members.home.nl/bzijlstra/hardware/stepper/stepper.htm 4.3: Stepper Motors Some example sequences 
for driving steppers are seen in Figure 4.16. If you connected the control wires of a unipolar stepper 
to an Arduino, for example, and then drove them in this sequence (with the center tap wires of the unipolar 
stepper tied to GND), the stepper would rotate one step for each step in the sequence. Actually, that 
s not true. You would burn out your Arduino because each phase of the step sequence would draw more current 
than the Arduino could supply. So, instead you would drive each of the four coil activation wires (A1, 
A3, B1, and B3 in Figure 4.14 on the left) through transistors that could provide more current to the 
motor. You could use a connection such as the one in Figure 4.8 to drive each of the control wires. The 
Arduino web site shows another connection through a Darlington array as seen in Figure 4.17. A Darlington 
array is just a chip that has six separate Darlington-type transistors on it, so it s exactly the equivalent 
of using discrete transistors, but contained in a handy chip. A bipolar stepper takes a slightly more 
complex circuit because the four wires of this type of stepper need to be used as two coils that can 
each be driven in both directions. So, you need some sort of circuit that can reverse the direction of 
the current like an H-bridge. The H-bridge chips discussed in the previous section (the L293D and SN754410) 
work well for this application. A connection of a bipolar stepper to an H-bridge chip is shown in Figure 
4.18. A more detailed image of this connection is seen in Figure 4.19. You could easily write code to 
send the appropriate sequence of signals on the digital pins of the Arduino to make the stepper move. 
However, this is such a commonly used type of motor that there is built-in Stepper library in the Arduino 
IDE. This library lets you instantiate a Stepper object for each stepper that you have connected according 
to Figures 4.17 or 4.18. It turns out that if you have the steppers connected in this way, the same sequence 
works for both unipolar and bipolar steppers. The current switching is handled through the H-bridge connection 
for the bipolar motor. To use the Stepper library: Include the Stepper library with #include <Stepper.h> 
 Instantiate a Stepper object for each motor that you have connected. Note that each motor requires 
four Arduino pins. The object instantiation function takes the number of steps in one revolution and 
the four pins you re connected to as arguments  Stepper <name1>(steps, pin, pin, pin, pin); Now you 
can use the following methods on that Stepper object setSpeed(rpm) Sets the speed of the stepper in RPM. 
This adjusts the delay be­tween steps that the lib ray uses when driving the stepper step(steps) Make 
the stepper turn for a number of steps. A positive number of steps turns the motor one way, and a negative 
number makes it turn the other way. An example Stepper program that comes with the Arduino IDE is called 
MotorKnob. This program reads the analog value of a potentiometer (the knob), and turns the stepper motor 
to match the rotation of the knob. That is, when you turn the knob, the stepper will turn the same amount. 
4.3: Stepper Motors / * * M o t o r K n o b *  * A s t e p p e r m o t o r f o l l o w s t h e t u 
r n s o f a p o t e n t i o m e t e r * ( o r o t h e r s e n s o r ) o n a n a l o g i n p u t 0 . 
*  * h t t p : / / www . a r d u i n o . c c / e n / R e f e r e n c e / S t e p p e r * T h i s e 
x a m p l e c o d e i s i n t h e p u b l i c d o m a i n .  * / # i n c l u d e <S t e p p e r . h> 
/ / c h a n g e t h i s t o t h e n u m b e r o f s t e p s o n y o u r m o t o r # d e f i n e STEPS 
1 0 0 / / c r e a t e a n i n s t a n c e o f t h e s t e p p e r c l a s s , s p e c i f y i n g / / 
t h e n u m b e r o f s t e p s o f t h e m o t o r a n d t h e p i n s i t s // attached to S t e p 
p e r s t e p p e r ( STEPS , 8 , 9 , 1 0 , 1 1 ) ; / / t h e p r e v i o u s r e a d i n g f r o m t 
h e a n a l o g i n p u t i n t p r e v i o u s = 0 ; v o i d s e t u p ( ) { / / s e t t h e s p e e 
d o f t h e m o t o r t o 3 0 RPMs s t e p p e r . s e t S p e e d ( 3 0 ) ; } v o i d l o o p ( ) { 
/ / g e t t h e s e n s o r v a l u e i n t v a l = a n a l o g R e a d ( 0 ) ; / / m o v e a n u m b 
e r o f s t e p s e q u a l t o t h e c h a n g e i n t h e // sensor reading s t e p p e r . s t e p 
( v a l - p r e v i o u s ) ; / / r e m e m b e r t h e p r e v i o u s v a l u e o f t h e s e n s o 
r p r e v i o u s = v a l ; } The Stepper library makes using unipolar and bipolar steppers very easy. 
However, it doesn t make use of the most modern stepper driving circuits, and it uses four wires per 
stepper motor. The Arduino web site shows how to reduce this to two wires per stepper with the addition 
of a few more parts, but it s still the case that there are more full-featured stepper drivers out there. 
The main extra feature of other driver chips is the ability to do micro stepping. This is a technique 
where multiple coils are driven at the same time in a more complex pattern that allows the stepper to 
move a half step or less on each step. This can greatly increase the resolution of the stepper. The other 
thing that the more mod­ern stepper drivers have is the equivalent of constant current outputs for motors. 
These outputs are called chopping drivers because they limit the total amount of current deliv­ered to 
the pins independent of the voltage by chopping up the higher voltage current into smaller pieces. The 
reason these types of chopper drivers are interesting is that stepper motors are pri­marily rated by 
how much current you can put through the coils. The current determines the strength of the magnetic .elds 
that move the motor so it s an important parameter. Sometimes stepper motors are rated by voltage and 
resistance of the coils, but the really important number is the current rating. If you have only the 
resistance of the coils and the recommended voltage you can use Ohm s law to .gure out how much current 
that rated  4.3: Stepper Motors voltage would result in. It s the current limit that the chopping drive 
enforces, independent of voltage. This raises the possibility of using a higher than rated voltage, and 
letting the chopping drive limit the current to a safe level. This allows the motor to move more quickly 
because of the higher voltage, but still be safe because the current is limited. For example, you might 
have a stepper motor that s rated at 6v DC with 7.9O in each of the coils. Ohm s law tells us that this 
rated voltage and resistance will result in 0.76A of current (760mA). So, you could set your chopping 
drive on a stepper driver to deliver 760mA of current, but then bump the voltage up to 24v. The higher 
voltage will make the motor snappier when it moves, but it will still be safe because the current will 
be limited to the correct amperage. Almost all industrial use of steppers makes use of this type of high-voltage 
chopping driver. There are a number of purpose-built stepper driver chips and boards that use this tech­nique. 
The EasyDriver and Pololu A4988 are two examples and are shown in Figure 4.20. These boards can each 
control one bipolar stepper motor using chopping current drive on the outputs. They will accept a motor 
power supply up to 30v. The interface to the mi­crocontroller is through two wires: step and dir. The 
microcontroller makes a pulse on the step signal (raises the signal to +5v and then lowers it o 0v), 
and the driver will make the motor take one step for each time that signal rises. The dir pin controls 
the direction of that step. These drivers also allow the motor to be driven in micro-step mode where 
each step is a full, half, 1/4, or 1/8th of the stepper motors normal full step. This is controlled by 
some mode pins on the stepper drivers. The pins of these stepper driver boards are on the same pitch 
as the holes in a solderless breadboard so they can be used with that connection technique. A picture 
of the wiring of one of these boards to an Arduino and a motor is shown in Figure 4.21. The power connection 
shown to the right of the motor is the motor s power supply. In this picture it is assumed that the Arduino 
is powered separately. The Step and Dir outputs from the Arduino are on digital pins 8 and 9. Note that 
the GND of the Arduino is connected to the GND of the motor driver board -always a good idea! A feature 
of this board that is not being used in the .gure is that the board also includes a +5v regulator. So, 
even if you re powering the motor with 30v, you could still take a +5v signal from that board for powering 
the Arduino. This is a very handy feature for installing things after programming and debugging is .nished. 
You would only need one power supply for the entire system. Note that these motor drivers are only for 
bipolar motors. They will not work with unipolar motors. Luckily bipolar motors are very common. Even 
more luckily, you can almost always take a unipolar motor and wire it for use as a bipolar motor. Look 
at the wiring in Figure 4.14. If you take the unipolar motor on the left and ignore (don t connect) the 
two center-tap wires (A2 and B2) you can use the remaining four wires as a bipolar stepper. These stepper 
drivers are in many ways the preferred ways to driver stepper motors. They re a little more expensive 
than just using an H-bridge chip, but they allow higher voltages and chopping current drive, and micro-stepping. 
They also only use two wires per motor so you can use more of them with a single Arduino. A code snippet 
showing how to use the step/dir interface is shown below. There are also libraries available that deal 
with these step/dir style drivers. The AccelStepper library is one of the fancier ones that includes 
a feature to slowly accelerate your stepper to full speed and then slow down when it reaches the end 
of its rotation. I won t discuss all its features here, but you can .nd the details at www.airspayce.com/mikem/arduino/AccelStepper/. 
 https://www.sparkfun.com/products/10267 http://www.pololu.com/catalog/product/1183 / / t h e n u m b 
e r o f s t e p s t o t a k e i s i n t h e s t e p s v a r i a b l e / / a n d t h e d e s i r e d 
d i r e c t i o n i s i n t h e d i r v a r i a b l e d i g i t a l W r i t e ( d i r P i n , d i r ) 
; / / s e t t h e d e s i r e d d i r e c t i o n f o r ( i n t i = 0 ; i < s t e p s ; i + + ) { / / 
f o r e a c h d e s i r e d s t e p d i g i t a l W r i t e ( STEP PIN , HIGH ) ; / / m o v e t h e s 
t e p p i n h i g h d e l a y M i c r o s e c o n d s ( u s D e l a y ) ; / / w a i t a b i t f o r t 
h e m o t o r t o m o v e ( 1 u s i s plenty ) d i g i t a l W r i t e ( STEP PIN , LOW) ; / / now s 
e t s t e p l o w f o r t h e n e x t t i m e d e l a y M i c r o s e c o n d s ( u s D e l a y ) ; / 
/ a n d w a i t a g a i n }  4.4 Questions About Motors Some questions that you should ask when considering 
motors for use with a controller like the Arduino are: What will you be moving with your motor? If you 
re moving small things, and they don t need to move very much, you should use a servo. They re inexpensive, 
easy to control, and relatively precise. A couple of small servos can even be driven directly from your 
Arduino with no extra power supply. You will want to add a separate supply if you have many of them though. 
If you re making something move by driving things like wheels, or reeling something in and out, you probably 
want a DC motor. They re much stronger than servos, and de.nitely need a separate power supply. Control 
them through a transistor like the 2n2222 or TIP120. Remember that you can control the speed of the motor 
with PWM. Steppers are the right choice for precise rotational control and ample torque. They re the 
fanciest motors, and the most expensive, but the most versatile. 4.4: Questions About Motors How many 
servos are you driving? The Arduino s servo library can control up to 12 servos. If you need more than 
that you need to look for an external servo-driver board. They re available at many of the sources listed 
in the sources chapter of these notes. How much current will your stepper motor draw? You should know 
this number so that you can set the chopping driver on your step­per driver board appropriately. There 
s a small potentiometer on these boards that you use to set the current limit. The question is, what 
is the current limit for your mo­tor? Some motors will tell you in the specs when you buy them. Other 
motors will (strangely, I think) tell you indirectly by giving you a rated voltage and resistance for 
the coil. You should use that and Ohm s law to .gure out what the current limit is. You can drive the 
motor at higher than the rated voltage if you have a chopping current drive set appropriately. Are the 
ground signals from all your power supplies connected together? Remember that whenever you have separate 
power supplies you must connect the grounds together so that they all have a common 0v reference. Which 
wires are which on your servo? Servos always have a three-wire interface. This is a standard connection 
so that ser­vos can be swapped out of things like radio-controlled airplanes without rewiring. The power 
is always in the middle, and the GND and control are on the outside of the three-wire bundle. The ground 
will be the darker of the two (usually black or brown). The control wire will often be white, yellow, 
or orange. Which wires are which on your DC motor? It doesn t matter. What matters is if the motor is 
spinning the direction you want it to when you apply a voltage to the wires. If it s not, reverse the 
wires. DC motors are designed to spin both directions by reversing the current. Which wires are which 
on your stepper motor? This is a little tricky. Stepper motors seldom come with data sheets unless you 
re lucky, and there s no standard for color or for physical arrangement. First you need to .gure out 
whether you have a unipolar (6 to 8 wires) or a bipolar (4 wires) motor. Then you should use an ohmmeter 
to check the resistance between pairs of wires. Look at Figure 4.14 as a guide. For a bipolar stepper 
the wires that are part of the same coil will have some measur­able resistance between them, and wires 
in different coils will not be connected. If you get the orientation of two wires turned around your 
motor will stutter when you drive it. Reverse one pair of wires in your circuit and that should .x it. 
For a unipolar motor .rst .gure out which wires are in the same coils. Then look at resistance within 
the coil. The center tap wire should have roughly half the resistance to the other wires in that coil 
because it comes out of the middle.  Chapter 5 Supply Sources This is a de.nitely non-exhaustive list 
of some of the equipment mentioned in this course, a rough guide to prices, and some example suppliers. 
Arduino: The main site for the Arduino is www.arduino.ccThis is the of.cial site for all open-source 
information, and for software downloads. A great secondary source for Arduino is www.freeduino.orgwhich 
is a non-of.cial community-supported site. Arduino comes in many hardware variations. The current generic 
Arduino is the Arduino Uno. It retails for around $30. Some suppliers include: Sparkfun: www.sparkfun.com 
  Radio Shack: www.radioshack.com  Adafruit: www.adafruit.com  Hacktronics: www.hacktronics.com 
  These are also great sources for all sorts of things related to embedded systems, phys­ical computing, 
making, and hardware hacking. Switches and sensors: The previously mentioned sources also have loads 
of switches and sensors of all sorts. Many of the sensors that we didn t discuss in this workshop are 
also resistive sensors and can be used in exactly the same way as we used pots and light sensors. Other 
sources for small electronic parts like this include: Jameco: www.jameco.com  Digikey: www.digikey.com 
  Mouser: www.mouser.com  Of these suppliers, Jameco is more for the hobbyist, and Digikey and Mouser 
are more for the professional. They all sometimes have great prices, especially in quan­tity, but you 
sometimes really have to know what you re looking for. As an example an individual light sensor at Sparkfun 
or Jameco runs between $1.25 and $2.00. But, with a little more searching, Jameco has a bag of 50 assorted 
light sensors for $15. Look for Jameco s bags of assorted switches, assorted LEDs, etc. They can be a 
great way to quickly stock a set of kits. Servos: These are the type of servos used in radio controlled 
model airplanes and model cars, and for small robotics projects. So, any hobby shop will have them. Radio 
Shack, Sparkfun, etc. also carry them, but you might .nd a better deal at a hobby shop or a robotics 
hobbyist shop. Some examples: Hobby Partz: www.hobbypartz.com  Trossen Robotics: www.trossenrobotics.com 
  Pololu: www.pololu.com  Electronics surplus resellers: Sometimes you can .nd great deals on surplus 
parts at a surplus reseller. Here you really do need to know exactly what you re getting, and be willing 
to take some chances, but you can .nd some real bargains if you look around. Some examples: Electronics 
Goldmine: www.goldmine-elec.com  Marlin P. Jones: www.mpja.com/  Alltronics: www.alltronics.com/ 
  Electronics Surplus: www.electronicsurplus.com/  Lady Ada (Limor Fried) at Adafruit also has some 
great suggestions: www.ladyada.net/ library/procure/hobbyist.html  Chapter 6 Kinetic Art using Physical 
Computing Kinetic art contains moving parts or depends on motion, sound, or light for its effect. The 
kinetic aspect is often regulated using microcontrollers connected to motors, actuators, transducers, 
and sensors that enable the sculpture to move and react to its environment. But, distinct from other 
types of computer art, the computer itself is usually not visible in the artwork. It is a behind the 
scenes controller. An embedded system is a special-purpose computer system (microcontroller) designed 
to perform one or a few dedicated functions, often reacting to environmental sensors. It is embedded 
into a complete device including hardware and mechanical parts rather than being a separate computer 
system. Kinetic art using embedded control is a marriage of art and technology. Artistic sen­sibility 
and creativity are required for concept and planning, and computer science and engineering skills are 
required to realize the artistic vision. Using embedded computer control to build things such as kinetic 
art is, in some ways, the essence of physical computing. Wikipedia s de.nition of physical computing 
is Physical computing, in the broadest sense, means building interactive phys­ical systems by the use 
of software and hardware that can sense and respond to the analog world. In this course we ve looked 
at some fundamental building blocks that would enable a pro­grammer to start using physical computing 
elements in his or her work. Hopefully this brief introduction to the world of electronics as it pertains 
to physical computing will help point you in the right direction. Once you re off in the right direction, 
the choice of how you employ that direction in your own work is up to you! 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2013</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>2504449</article_id>
		<sort_key>140</sort_key>
		<display_label>Article No.</display_label>
		<pages>65</pages>
		<display_no>14</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>14</seq_no>
		<title><![CDATA[Combining GPU data-parallel computing with OpenGL]]></title>
		<page_from>1</page_from>
		<page_to>65</page_to>
		<doi_number>10.1145/2504435.2504449</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2504449</url>
		<abstract>
			<par><![CDATA[<p>Data-parallel computing is a programming paradigm in which the same analysis code is applied to different data elements. Many applications in visual computing fall into this category, such as particle systems, image processing, chain models, cloth models, flow analysis, and structural modeling. And, because of the nature of graphics architectures, GPUs are the natural place to perform such operations quickly.</p> <p>If you are an OpenGL programmer, you have two options:</p> <p>&#8226; OpenCL, an industry-wide standard created by the Khronos Group, the same body that governs</p> <p>&#8226; OpenGL, has been available for several years.</p> <p>&#8226; OpenGL compute shaders became available in the summer of 2012 in the OpenGL 4.3 release.</p> <p>The presence of these two multi-vendor solutions allows application developers considerable flexibility to examine their needs and choose the solution that most closely matches. These two solutions are especially important because each can use OpenGL buffers for their data storage. This means that the data never leave the GPU. They are local for both the computing and the rendering, increasing the speed of the application.</p> <p>This course examines both solutions, shows how each can be used to solve data-parallel computing problems, and explains how each interfaces with OpenGL's rendering.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4193517</person_id>
				<author_profile_id><![CDATA[82458766857]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Mike]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bailey]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Oregon State University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[&#8226; Dave Shreiner, Graham Sellers, John Kessenich, and Bill Licea-Kane, <i>OpenGL Programming Guide, 8th Edition</i>, 2013.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1971951</ref_obj_id>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[&#8226; Peter Pacheco, <i>An Introduction to Parallel Programming</i>, Morgan-Kaufmann, 2011.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2049883</ref_obj_id>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[&#8226; Aaftah Munshi, Benedict Gaster, Timothy Mattson, James Fung, and Dan Ginsburg, <i>OpenCL Programming Guide</i> Addison-Wesley, 2012.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2046379</ref_obj_id>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[&#8226; Benedict Gaster, Lee Howes, David Kaeli, Perhaad Mistry, and Dana Schaa, <i>Heterogeneous Computing with OpenCL</i>, Morgan-Kaufmann, 2012.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Combining GPU Data-Parallel Computing with OpenGL Mike Bailey mjb@cs.oregonstate.edu Oregon State University 
 Mike Bailey  Professor of Computer Science, Oregon State University  Has worked at Sandia Labs, Purdue 
University, Megatek, San Diego Supercomputer Center (UC San Diego), and OSU  Has taught over 5,000 students 
in his classes  mjb@cs.oregonstate.edu   Topics  Introduction to Data Parallel Computing  Introduction 
to OpenGL Vertex Buffers  OpenGL Compute Shaders OpenCL OpenCL  References   Introduction to Data 
Parallel Computing The scenario many pieces of data need to undergo the same operation:  Particles 
Game of Life Fluids Cloth Chain  Introduction to Data Parallel Computing The scenario many pieces 
of data need to undergo the same operation:   Oregon State University Computer Graphics  Introduction 
to Data Parallel Computing The scenario many pieces of data need to undergo the same operation:  
Oregon State University Computer Graphics Note: GPUs are designed to Handle Data Parallel Computing 
Well   Data Parallel / OpenGL Vertex Interoperability: The Basic Idea   In the Beginning of OpenGL 
 You listed the vertices individually: glBegin( GL_TRIANGLES ); glVertex3f( x0, y0, z0 ); glVertex3f( 
x1, y1, z1 ); glVertex3f( x2, y2, z2 );  4 3 2  0  Oregon State University Computer Graphics mjb 
May 7, 2013  Then Someone Noticed That You Were Transmitting and Processing Each Vertex Several Times 
 For example:  A Little Background --the OpenGL Rendering Context The OpenGL Rendering Context contains 
all the characteristic information necessary to produce an image from geometry. This includes transformations, 
colors, lighting, textures, where to send the display, etc. More Background What is an OpenGL Object 
? An OpenGL Object is pretty much the same as a C++, C#, or Java object: it encapsulates a group of 
data items and allows you to treat them as a unified whole. For example, a Vertex Buffer Object could 
be defined in C++ by: class VertexBufferObject { enum dataType; // int, float, void *memStart; int memSize; 
}; Then, you could create any number of Vertex Buffer Object instances, each with its own characteristics 
encapsulated within it. When you want to make that combination current, you just need to bring in ( bind 
) that entire object. When you bind an object, all of its information comes with it. Oregon State University 
Computer Graphics mjb May 7, 2013 More Background How do you Create an OpenGL Object ? In C++, objects 
are pointed to by their address. In OpenGL, objects are pointed to by an unsigned integer handle. You 
can assign a value for this handle yourself (not recommended), or have OpenGL generate one for you that 
is guaranteed to be unique. For example: GLuint bufA;  glGenBuffers( 1, &#38;bufA ); This doesn t actually 
allocate memory for the buffer object yet, it just acquires a unique handle. To allocate memory, you 
need to bind this handle to the Context. More Background -- Binding to the Context The OpenGL term 
binding refers to attaching or docking (a metaphor which I find to be more visually pleasing) an OpenGL 
object to the Context. You can then assign characteristics, and they will flow through the Context into 
the object. Vertex Buffer Object  Oregon State University Computer Graphics mjb May 7, 2013 More Background 
-- Binding to the Context When you want to use that Vertex Buffer Object, just bind it again. All of 
the characteristics will then be active, just as if you had specified them again. Vertex Buffer Object 
 Vertex Buffers: Putting Data in the Buffer Object glBufferData( type, numBytes, data, usage ); type 
is the type of buffer object this is: GL_ARRAY_BUFFER to store floating point vertices, normals, colors, 
and texture coordinates GL_ELEMENT_ARRAY_BUFFER to store integer vertex indices to connect for drawing 
 numBytes is the number of bytes to store in all. Not the number of numbers, but the number of bytes! 
 data is the memory address of (i.e., pointer to) the data to be transferred to the graphics card. This 
can be NULL, and the data can be transferred later.  Oregon State University Computer Graphics mjb 
May 7, 2013 Vertex Buffers: Putting Data in the Buffer Object  usage is a hint as to how the data will 
be used: GL_xxx_yyy where xxx should be one of: STATIC this buffer will be written seldom from the CPU 
DYNAMIC this buffer will be written often from the CPU STATIC this buffer will be written often from 
the GPU and yyy should be: DRAW this buffer will be used for drawing  Oregon State University Computer 
Graphics mjb May 7, 2013 GLfloat Vertices[ ][3] = { { 1., 2., 3. }, { 4., 5., 6. }, . . . }; Vertex 
Buffers: Step #2 Create the Buffers and Fill Them glGenBuffers( 1, &#38;bufA ); glBindBuffer( bufA, 
GL_ARRAY_BUFFER ); glBufferData( GL_ARRAY_BUFFER, 3*sizeof(float)*numVertices, Vertices, GL_STATIC_DRAW 
);  Oregon State University Computer Graphics mjb May 7, 2013  glEnableClientState( type ) where type 
can be any of: GL_VERTEX_ARRAY GL_COLOR_ARRAY GL_NORMAL_ARRAY GL_SECONDARY_COLOR_ARRAY GL_TEXTURE_COORD_ARRAY 
 Call this as many times as you need to enable all the arrays that you will need. There are other types, 
too.  To deactivate a type, call:  glDisableClientState( type ) Vertex Buffers: Step #4 To Draw, 
First Bind the Buffers  Oregon State University Computer Graphics mjb May 7, 2013 vs. size is the 
spatial dimension, and can be: 2, 3, or 4 type can be:  stride is the byte offset between consecutive 
entries in the array (0 means tightly packed)  Oregon State University Computer Graphics mjb May 7, 
2013 4 If the vertices are not in order: 0 If all the vertices are in order:  Vertex Buffers: Writing 
Data Directly into a Vertex Buffer Map the buffer from GPU memory into the memory space of the application: 
 usage is an indication how the data will be used: GL_READ_ONLY the vertex data will be read from, 
but not written to GL_WRITE_ONLY the vertex data will be written to, but not read from GL_READ_WRITE 
the vertex data will be read from and written to You can now use vertexArray[ ] like any other floating-point 
array. When you are done, be sure to call: glUnMapBuffer( GL_ARRAY_BUFFER );   OpenGL Compute Shaders 
 OpenGL Compute Shader the Basic Idea  If I Know GLSL, What Do I Need to Do Differently to Write a 
Compute Shader? Not much: 1. A Compute Shader is created just like any other GLSL shader, except that 
its type is GL_COMPUTE_SHADER (duh ). You compile it and link it just like any other GLSL shader program. 
 2. A Compute Shader must be in a shader program all by itself. There cannot be vertex, fragment, etc. 
shaders in there with it. 3. A Compute Shader has access to uniform variables and buffer objects, but 
cannot access any pipeline variables such as attributes or variables from cannot access any pipeline 
variables such as attributes or variables from other stages. It stands alone. 4. A Compute Shader needs 
to declare the number of work-items in each of its work-groups in a special GLSL layout statement.  
 Oregon State University Computer Graphics mjb May 7, 2013 The Example We Are Going to Use Here is 
a Particle System  #define NUM_PARTICLES 1024*1024 // total number of particles to move #define WORK_GROUP_SIZE 
128 // # work-items per work-group struct pos { float x, y, z, w; // positions }; struct vel { float 
vx, vy, vz, vw; // velocities }; struct color { float r, g, b, a; // colors }; // need to do the following 
for both position, velocity, and colors of the particles: GLuint posSSbo; GLuint velSSbo GLuint colSSbo; 
   #WorkGroups = WorkGroupSize 20 5 4 = x 4 Oregon State University Computer Graphics mjb May 7, 
2013   which is further Divided into Smaller Units Called Work-Items  4 3 x Oregon State University 
Computer Graphics Running the Compute Shader from the Application    glBindBufferBase( GL_SHADER_STORAGE_BUFFER, 
4, posSSbo ); glBindBufferBase( GL_SHADER_STORAGE_BUFFER, 5, velSSbo ); glBindBufferBase( GL_SHADER_STORAGE_BUFFER, 
6, colSSbo ); . . . glUseProgram( MyComputeShaderProgram ); glDispatchCompute( NUM_PARTICLES / WORK_GROUP_SIZE, 
1, 1 ); glMemoryBarrier( GL_SHADER_STORAGE_BARRIER_BIT ); . . . glUseProgram( MyRenderingShaderProgram 
); glBindBuffer( GL_ARRAY_BUFFER, posSSbo ); glVertexPointer( 4, GL_FLOAT, 0, (void *)0 ); glEnableClientState( 
GL_VERTEX_ARRAY ); glDrawArrays( GL_POINTS, 0, NUM_PARTICLES ); glDisableClientState( GL_VERTEX_ARRAY 
); glBindBuffer( GL_ARRAY_BUFFER, 0 ); Special Pre-set Variables in the Compute Shader in uvec3 gl_NumWorkGroups 
; Same numbers as in the glDispatchComputecall const uvec3 gl_WorkGroupSize ; Same numbers as in the 
layout local_size_* in uvec3 gl_WorkGroupID ; Which workgroup this thread is in in uvec3 gl_LocalInvocationID 
; Where this thread is in the current workgroup in uvec3 gl_GlobalInvocationID ; Where this thread is 
in al the work items in uint gl_LocalInvocationIndex ; 1D representation of the gl_LocalInvocationID 
(used for indexing into a shared array) (used for indexing into a shared array) 0 = gl_WorkGroupID = 
gl_NumWorkGroups 1 0 = gl_LocalInvocationID = gl_WorkGroupSize 1 gl_GlobalInvocationID = gl_WorkGroupID 
* gl_WorkGroupSize + gl_LocalInvocationID gl_LocalInvocationIndex = gl_LocalInvocationID.z * gl_WorkGroupSize.y 
* gl_WorkGroupSize.x + gl_LocalInvocationID.y * gl_WorkGroupSize.x + gl_LocalInvocationID.x Oregon State 
University Computer Graphics mjb May 7, 2013   const vec3 G = vec3( 0., -9.8, 0. ); const float DT 
= 0.1; . . . uint gid = gl_GlobalInvocationID.x; // the .y and .z are both 1 in this case      
 OpenCL Computing Shaders  All of this Happens on the GPU  = Buffer Owner Oregon State University 
Computer Graphics mjb May 7, 2013 1. Program Header #include <stdio.h> #define _USE_MATH_DEFINES #include 
<math.h> #include <string.h> #include <stdlib.h> #include <ctype.h> #include <omp.h> #ifdef WIN32 #include 
<windows.h> #endif  Structures We Will Use to Fill the Vertex Buffers  OpenCL Global Variables size_t 
GlobalWorkSize[3] = { NUM_PARTICLES, 1, 1 }; size_t LocalWorkSize[3] = { LOCAL_SIZE, 1, 1 }; gluint hPobj; 
// host opengl object gluint hCobj; // host opengl object struct xyzw * hVel; // host c array cl_mem 
dPobj; // device memory buffer cl_mem dCobj; // device memory buffer  A Deceptively-Simple Main Program 
  { . . . status = clGetDeviceIDs( Platform, CL_DEVICE_TYPE_GPU, 1, &#38;Device, NULL ); PrintCLError( 
status, "clGetDeviceIDs: " ); // since this is an opengl interoperability program, // check if the opengl 
sharing extension is supported // (no point going on if it isn t): // (we need the Device in order to 
ask, so we can't do it any sooner than right here) // (we need the Device in order to ask, so we can 
t do it any sooner than right here) if( IsCLExtensionSupported( "cl_khr_gl_sharing" ) ) { fprintf( stderr, 
"cl_khr_gl_sharing is supported.\n" ); } else { fprintf( stderr, "cl_khr_gl_sharing is not supported 
--sorry.\n" ); return; } Oregon State University Computer Graphics mjb May 7, 2013 Querying the Existence 
of an OpenCL Extension bool IsCLExtensionSupported( const char *extension ) { // see if the extension 
is bogus: if( extension == NULL || extension[0] == '\0' ) return false; char * where = (char *) strchr( 
extension, ' ' ); if( where != NULL ) return false; // get the full list of extensions: size_t extensionSize; 
clGetDeviceInfo( Device, CL_DEVICE_EXTENSIONS, 0, NULL, &#38;extensionSize ); char *extensions = new 
char [ extensionSize ]; clGetDeviceInfo( Device, CL_DEVICE_EXTENSIONS, extensionSize, extensions, NULL 
); for( char * start = extensions ; ; ) { where = (char *) strstr( (const char *) start, extension ); 
if( where == 0 ) { delete [ ] extensions; return false; } char * terminator = where + strlen(extension); 
// points to what should be the separator if( *terminator == ' ' || *terminator == '\0' || *terminator 
== '\r' || *terminator == '\n' ) { delete [ ] extensions; return true; } start = terminator; } } void 
InitCL( ) { . . . // get the platform id: status = clGetPlatformIDs( 1, &#38;Platform, NULL ); PrintCLError( 
status, "clGetPlatformIDs: " ); // get the device id: status = clGetDeviceIDs( Platform, CL_DEVICE_TYPE_GPU, 
1, &#38;Device, NULL ); P i tCLE ( t t " lG tD i ID : " ); PrintCLError( status, "clGetDeviceIDs: " 
); // 3. create a special opencl context based on the opengl context: cl_context_properties props[ ] 
= { CL_GL_CONTEXT_KHR, (cl_context_properties) wglGetCurrentContext( ), CL_WGL_HDC_KHR, (cl_context_properties) 
wglGetCurrentDC( ), CL_CONTEXT_PLATFORM, (cl_context_properties) Platform, 0 }; cl_context Context = 
clCreateContext( props, 1, &#38;Device, NULL, NULL, &#38;status ); PrintCLError( status, "clCreateContext: 
" );  Computer Graphics mjb May 7, 2013 For Windows: cl_context_properties props[ ] = { CL_GL_CONTEXT_KHR, 
(cl_context_properties) wglGetCurrentContext( ), CL_WGL_HDC_KHR, (cl_context_properties) wglGetCurrentDC( 
), CL_CONTEXT_PLATFORM, (cl_context_properties) Platform, 0 }; cl_context Context = clCreateContext( 
props, 1, &#38;Device, NULL, NULL, &#38;status ); For Linux: cl_context_properties props[ ] = void 
InitCL( ) { . . . // create the velocity array and the opengl vertex array buffer and color array buffer: 
 delete [ ] hVel; hVel = new struct xyzw [ NUM_PARTICLES ]; glGenBuffers( 1, &#38;hPobj ); glBindBuffer( 
GL_ARRAY_BUFFER, hPobj ); lB ff D ( GL ARRAY BUFFER 4 * NUM PARTICLES * i f(fl ) NULL GL STATIC DRAW 
) glBufferData( GL_ARRAY_BUFFER, 4 * NUM_PARTICLES * sizeof(float), NULL, GL_STATIC_DRAW ); glGenBuffers( 
1, &#38;hCobj ); glBindBuffer( GL_ARRAY_BUFFER, hCobj ); glBufferData( GL_ARRAY_BUFFER, 4 * NUM_PARTICLES 
* sizeof(float), NULL, GL_STATIC_DRAW ); glBindBuffer( GL_ARRAY_BUFFER, 0 ); // unbind the buffer // 
fill those arrays and buffers: ResetParticles( );  Oregon State University Computer Graphics mjb May 
7, 2013 void ResetParticles( ) { glBindBuffer( GL_ARRAY_BUFFER, hPobj ); struct xyzw *points = (struct 
xyzw *) glMapBuffer( GL_ARRAY_BUFFER, GL_WRITE_ONLY ); for( int i = 0; i < NUM_PARTICLES; i++ ) { points[ 
i ].x = Ranf( XMIN, XMAX ); points[ i ].y = Ranf( YMIN, YMAX ); points[ i ].z = Ranf( ZMIN, ZMAX ); points[ 
i ].w = 1.; } glUnmapBuffer( GL_ARRAY_BUFFER ); glBindBuffer( GL_ARRAY_BUFFER, hCobj ); struct rgba 
*colors = (struct rgba *) glMapBuffer( GL_ARRAY_BUFFER, GL_WRITE_ONLY ); for( int i = 0; i < NUM_PARTICLES; 
i++ ) { colors[ i ].r = Ranf( 0., 1. ); colors[ i ].g = Ranf( 0., 1. ); colors[ i ].b = Ranf( 0., 1. 
); colors[ i ].a = 1.; } glUnmapBuffer( GL_ARRAY_BUFFER ); . . .   Oregon State University Computer 
Graphics mjb May 7, 2013 void InitCL( ) { . . . // 5. create the opencl version of the velocity array: 
dVel = clCreateBuffer( Context, CL_MEM_READ_WRITE, 4*sizeof(float)*NUM_PARTICLES, NULL, &#38;status ); 
PrintCLError( status, "clCreateBuffer: " ); // 6. write the data from the host buffers to the device 
buffers: st t lE W it B ff ( C dQ dV l CL FALSE 0 4*si f(fl t)*NUM PARTICLES hV l 0 NULL NULL ); status 
= clEnqueueWriteBuffer( CmdQueue, dVel, CL_FALSE, 0, 4*sizeof(float)*NUM_PARTICLES, hVel, 0, NULL, NULL 
); PrintCLError( status, "clEneueueWriteBuffer: " ); // 5. create the opencl version of the opengl buffers: 
dPobj = clCreateFromGLBuffer( Context, 0, hPobj, &#38;status ); PrintCLError( status, "clCreateFromGLBuffer 
(1)" ); dCobj = clCreateFromGLBuffer( Context, 0, hCobj, &#38;status ); PrintCLError( status, "clCreateFromGLBuffer 
(2)" );  Oregon State University Computer Graphics mjb May 7, 2013  to Match the Kernel s Parameter 
List  The Idle Function Tells OpenCL to Do Its Computing void Animate( ) { // acquire the vertex buffers 
from opengl: glutSetWindow( MainWindow ); glFinish( ); cl_int status = clEnqueueAcquireGLObjects( CmdQueue, 
1, &#38;dPobj, 0, NULL, NULL ); PrintCLError( status, clEnqueueAcquireGLObjects (1) : ); status = clEnqueueAcquireGLObjects( 
CmdQueue, 1, &#38;dCobj, 0, NULL, NULL ); PrintCLError( status, clEnqueueAcquireGLObjects (2) : ); 
double time0 = omp_get_wtime( ); // 11. enqueue the Kernel object for execution: cl_event wait;  Redrawing 
the Scene: The Particles void Display( ) { . . . glBindBuffer( GL_ARRAY_BUFFER, hPobj ); glVertexPointer( 
4, GL_FLOAT, 0, (void *)0 ); glEnableClientState( GL_VERTEX_ARRAY ); glBindBuffer( GL_ARRAY_BUFFER, hCobj 
); glColorPointer( 4, GL_FLOAT, 0, (void *)0 ); 13. Clean-up void Quit( ) { Glui->close( ); glutSetWindow( 
MainWindow ); glFinish( ); glutDestroyWindow( MainWindow );  Oregon State University Computer Graphics 
mjb May 7, 2013  Oregon State University Computer Graphics mjb May 7, 2013   GigaParticless / Second 
 Number of Particles (x1024)  Oregon State University Computer Graphics mjb May 7, 2013 How Do You 
Choose Between Compute Shaders and OpenCL? OpenCL and Compute Shaders are great! They do a super job 
of using the GPU for general-purpose data-parallel computing. So, how do you choose between Compute Shaders 
and OpenCL? Here s what I think: OpenCL requires installing a separate driver and separate libraries. 
While this is not a huge deal, it does take time and effort. Compute Shaders are just there as part of 
OpenGL 4.3.  OpenCL is more feature-rich than OpenGL compute shaders.  Compute Shaders use the GLSL 
language, something that all OpenGL programmers should already be familiar with (or will be soon).  
Compute shaders use the same context as does the OpenGL rendering pipeline. There is no need to acquire 
and release the context as OpenGL+OpenCL must do.  Calls to OpenGL compute shaders appear to be more 
lightweight than calls to OpenCL kernels are. This should result in better performance.  Using OpenCL 
is more involved. It requires more setup (queries, platforms, devices, queues, kernels, etc.). Compute 
Shaders are more convenient. They just flow in with the graphics.   Oregon State University Computer 
Graphics mjb May 7, 2013 How Do You Choose Between Compute Shaders and OpenCL?  Oregon State University 
Computer Graphics mjb May 7, 2013 References  Dave Shreiner, Graham Sellers, John Kessenich, and Bill 
Licea-Kane, OpenGL Programming Guide, 8th Edition, 2013.  Peter Pacheco, An Introduction to Parallel 
Programming, Morgan-Kaufmann, 2011.  Aaftah Munshi, Benedict Gaster, Timothy Mattson, James Fung, and 
Dan Ginsburg, OpenCL Programming Guide Addison-Wesley, 2012.  Benedict Gaster, Lee Howes, David Kaeli, 
Perhaad Mistry, and Dana Schaa, Heterogeneous Computing with OpenCL, Morgan-Kaufmann, 2012.   
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2013</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>2504450</article_id>
		<sort_key>150</sort_key>
		<display_label>Article No.</display_label>
		<display_no>15</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>15</seq_no>
		<title><![CDATA[Advances in real-time rendering in games part II]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2504435.2504450</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2504450</url>
		<abstract>
			<par><![CDATA[<p>Advances in real-time graphics research and the ever-increasing power of mainstream GPUs and consoles continue to generate an explosion of innovative algorithms suitable for fast, interactive rendering of complex and engaging virtual worlds. Every year, the latest video games employ a vast variety of sophisticated algorithms to produce ground-breaking 3D rendering that pushes the visual boundaries and interactive experience of rich environments.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4193518</person_id>
				<author_profile_id><![CDATA[81487640839]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Natasha]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tatarchuk]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Bungie Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2013</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>2504451</article_id>
		<sort_key>160</sort_key>
		<display_label>Article No.</display_label>
		<display_no>16</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>16</seq_no>
		<title><![CDATA[OpenSubdiv from research to industry adoption]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2504435.2504451</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2504451</url>
		<abstract>
			<par><![CDATA[<p>Catmull-Clark Subdivision Surfaces were invented in the 1970s. The specification was extended with local edits, creases, and other features, and formalized into a usable technique for animation in 1998. But the extended technology has not enjoyed widespread adoption in animation for a variety of reasons. Recently, Pixar decided to release its subdivision patents and working codebase, in the hope that giving away its high-performance GPU-accelerated code will create a standard for geometry throughout the animation industry.</p> <p>This course answers several questions:</p> <p>&#8226;What is a subdivision surface?</p> <p>&#8226;What are the extended features, and how exactly do they work?</p> <p>&#8226;What is the Feature Adaptive algorithm, and how does it make the surfaces useful on GPUs?</p> <p>&#8226;What is OpenSubdiv, and how does it implement these algorithms?</p> <p>&#8226;How do you integrate OpenSubdiv into an application and a pipeline?</p> <p>&#8226;What are the challenges and solutions associated with putting OpenSubdiv on a mobile device?</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<seq_no />
				<first_name />
				<middle_name />
				<last_name />
				<suffix />
				<role />
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2013</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>2504452</article_id>
		<sort_key>170</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>17</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>17</seq_no>
		<title><![CDATA[Multithreading and VFX]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2504435.2504452</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2504452</url>
		<abstract>
			<par><![CDATA[<p>Parallelism is important to many aspects of visual effects. In this course, experts in several key areas present their specific experiences in applying parallelism to their domain of expertise. The problem domains are very diverse, and so are the solutions employed, including specific threading methodologies. This allows the audience to gain a wide understanding of various approaches to multithreading and compare different techniques, which provides a broad context of state-of-the-art approaches to implementing parallelism and helps them decide which technologies and approaches to adopt for their own future projects. The presenters describe both successes and pitfalls, the challenges and difficulties they encountered, and the approaches they adopted to resolve these issues.</p> <p>The course begins with an overview of the current state of parallel programming, followed by five presentations on various domains and threading approaches. Domains include rigging, animation, dynamics, simulation, and rendering for film and games, as well as a threading implementation for a full-scale commercial application that covers all of these areas. Topics include CPU and GPU programming, threading, vectorization, tools, debugging techniques, and optimization and performance-profiling approaches.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4193519</person_id>
				<author_profile_id><![CDATA[81341495683]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[James]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Reinders]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Intel Corporation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4193520</person_id>
				<author_profile_id><![CDATA[81100428446]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[George]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[ElKoura]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Pixar Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4193521</person_id>
				<author_profile_id><![CDATA[81327488143]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Erwin]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Coumans]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Advanced Micro Devices, Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4193522</person_id>
				<author_profile_id><![CDATA[82459284257]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Ron]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Henderson]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[DreamWorks Animation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4193523</person_id>
				<author_profile_id><![CDATA[81548023772]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Martin]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Watt]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[DreamWorks Animation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4193524</person_id>
				<author_profile_id><![CDATA[81488670649]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Jeff]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lait]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Side Effects Software Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2013</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>2504453</article_id>
		<sort_key>180</sort_key>
		<display_label>Article No.</display_label>
		<pages>54</pages>
		<display_no>18</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>18</seq_no>
		<title><![CDATA[Efficient real-time shadows]]></title>
		<page_from>1</page_from>
		<page_to>54</page_to>
		<doi_number>10.1145/2504435.2504453</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2504453</url>
		<abstract>
			<par><![CDATA[<p>This course provides an overview of efficient, real-time shadow algorithms. It presents the theoretical background but also discusses implementation details for facilitating efficient realizations (hard and soft shadows, volumetric shadows, reconstruction techniques). These elements are of relevance to both experts and practitioners. The course also reviews budget considerations and analyzes performance trade-offs, using examples from various AAA game titles and film previsualization tools. While physical accuracy can sometimes be replaced by plausible shadows, especially for games, film production requires more pecision, such as scalable solutions that can deal with highly detailed geometry.</p> <p>The course builds upon earlier SIGGRAPH courses as well as the recent book Real-Time Shadows (A K Peters, 2011) by four of the instructors (due to its success, a second edition is planned for 2014). And with two instructors who have worked on AAA game and movie titles, the course presents interesting behindthe-scenes information that illuminates key topics.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4193525</person_id>
				<author_profile_id><![CDATA[81310501633]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Elmar]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Eisemann]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Delft University of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4193526</person_id>
				<author_profile_id><![CDATA[81100618307]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Ulf]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Assarsson]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Chalmers University of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4193527</person_id>
				<author_profile_id><![CDATA[81540597656]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Michael]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Schwarz]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Weta Digital]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4193528</person_id>
				<author_profile_id><![CDATA[81100496272]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Michal]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Valient]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Guerrilla Games]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4193529</person_id>
				<author_profile_id><![CDATA[81100084933]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Michael]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wimmer]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Technische Universit&#228;t Wien]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>2383555</ref_obj_id>
				<ref_obj_pid>2383533</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Timo Aila and Samuli Laine. Alias-free shadow maps. In <i>Proceedings of Eurographics Symposium on Rendering 2004</i>, pages 161--166, 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>581935</ref_obj_id>
				<ref_obj_pid>581896</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Tomas Akenine-M&#246;ller and Ulf Assarsson. Approximate soft shadows on arbitrary surfaces using penumbra wedges. In <i>Proceedings of Eurographics Workshop on Rendering 2002</i>, pages 297--306, 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Tomas Akenine-M&#246;ller, Eric Haines, and Natty Hoffman. <i>Real-Time Rendering</i>. A K Peters, 3rd edition, 2008.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>988877</ref_obj_id>
				<ref_obj_pid>988834</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Graham Aldridge and Eric Woods. Robust, geometry-independent shadow volumes. In <i>GRAPHITE '04: Proceedings of the 2nd international conference on Computer graphics and interactive techniques in Australasia and South East Asia</i>, pages 250--253, 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2383857</ref_obj_id>
				<ref_obj_pid>2383847</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Thomas Annen, Tom Mertens, Philippe Bekaert, Hans-Peter Seidel, and Jan Kautz. Convolution shadow maps. In <i>Proceedings of Eurographics Symposium on Rendering 2007</i>, pages 51--60, 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1360633</ref_obj_id>
				<ref_obj_pid>1360612</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Thomas Annen, Zhao Dong, Tom Mertens, Philippe Bekaert, Hans-Peter Seidel, and Jan Kautz. Real-time, all-frequency shadows in dynamic scenes. <i>ACM Transactions on Graphics (Proceedings of ACM SIGGRAPH 2008)</i>, 27(3):34:1--34:8, 2008a.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1375741</ref_obj_id>
				<ref_obj_pid>1375714</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Thomas Annen, Tom Mertens, Hans-Peter Seidel, Eddy Flerackers, and Jan Kautz. Exponential shadow maps. In <i>Proceedings of Graphics Interface 2008</i>, pages 155--161, 2008b.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1009599</ref_obj_id>
				<ref_obj_pid>1009379</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Jukka Arvo. Tiled shadow maps. In <i>Proceedings of Computer Graphics International 2004</i>, pages 240--246, 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>882300</ref_obj_id>
				<ref_obj_pid>882262</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Ulf Assarsson and Tomas Akenine-M&#246;ller. A geometry-based soft shadow volume algorithm using graphics hardware. <i>ACM Transactions on Graphics (Proceedings of ACM SIGGRAPH 2003)</i>, 22(3):511--520, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Lionel Atty, Nicolas Holzschuch, Marc Lapierre, Jean-Marc Hasenfratz, Charles Hansen, and Fran&#231;ois X. Sillion. Soft shadow maps: Efficient sampling of light source visibility. <i>Computer Graphics Forum</i>, 25(4):725--741, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1866200</ref_obj_id>
				<ref_obj_pid>1882262</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Ilya Baran, Jiawen Chen, Jonathan Ragan-Kelley, Fr&#233;do Durand, and Jaakko Lehtinen. A hierarchical volumetric shadow algorithm for single scattering. <i>ACM Transactions on Graphics</i>, 29(6 (Proceedings of ACM SIGGRAPH Asia 2010)):178:1--178:10, 2010.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1401061</ref_obj_id>
				<ref_obj_pid>1401032</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Louis Bavoil, Miguel Sainz, and Rouslan Dimitrov. Image-space horizon-based ambient occlusion. In <i>ACM SIGGRAPH 2008 Talks</i>, pages 22:1--22:1, 2008.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[P. Bergeron. A general version of crow's shadow volumes. <i>IEEE Computer Graphics and Applications</i>, 6(9):17--28, 1986.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1921487</ref_obj_id>
				<ref_obj_pid>1921479</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Markus Billeter, Erik Sintorn, and Ulf Assarson. Volumetric shadows using polygonal light volumes. In <i>Proceedings of High Performance Graphics 2010</i>, pages 39--45, 2010.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[William Bilodeau and Mike Songy. Real time shadows, 1999. Creativity 1999, Creative Labs Inc. Sponsored game developer conferences, Los Angeles, California, and Surrey, England.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Venceslas Biri, Didier Arqu&#232;s, and Sylvain Michelin. Real Time Rendering of Atmospheric Scattering and Volumetric Shadows. <i>Journal of WSCG</i>, 14(1):65--72, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1944759</ref_obj_id>
				<ref_obj_pid>1944745</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Ji&#345;&#237; Bittner, Oliver Mattausch, Ari Silvennoinen, and Michael Wimmer. Shadow caster culling for efficient shadow mapping. In <i>Proceedings of ACM SIGGRAPH Symposium on Interactive 3D Graphics and Games 2011</i>, pages 81--88, 2011.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>801255</ref_obj_id>
				<ref_obj_pid>800064</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[James F. Blinn. Light reflection functions for simulation of clouds and dusty surfaces. <i>Computer Graphics</i>, 16(3 (Proceedings of ACM SIGGRAPH 82)):21--29, 1982. ISSN 0097-8930.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[Michael Bunnell. Dynamic ambient occlusion and indirect lighting. In Matt Pharr, editor, <i>GPU Gems 2: Programming Techniques for High-Performance Graphics and General-Purpose Computation</i>, pages 223--233, Reading, MA, USA, 2006. Addison-Wesley Professional. ISBN 0-321-33559-7.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[John Carmack. Z-fail shadow volumes. Internet Forum, 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1944752</ref_obj_id>
				<ref_obj_pid>1944745</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[Jiawen Chen, Ilya Baran, Fr&#233;do Durand, and Wojciech Jarosz. Real-time volumetric shadows using 1D min-max mipmaps. In <i>Proceedings of ACM SIGGRAPH Symposium on Interactive 3D Graphics and Games 2011</i>, pages 39--46, 2011.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2383556</ref_obj_id>
				<ref_obj_pid>2383533</ref_obj_pid>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[Hamilton Y. Chong and Steven J. Gortler. A lixel for every pixel. In <i>Proceedings of Eurographics Symposium on Rendering 2004</i>, pages 167--172, 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[Hamilton Yu-Ik Chong. Real-time perspective optimal shadow maps. Senior thesis, Harvard University, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>360354</ref_obj_id>
				<ref_obj_pid>360349</ref_obj_pid>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[James H. Clark. Hierarchical geometric models for visible surface algorithms. <i>Communications of the ACM</i>, 19(10):547--554, 1976.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>563901</ref_obj_id>
				<ref_obj_pid>563858</ref_obj_pid>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[Franklin C. Crow. Shadow algorithms for computer graphics. <i>Computer Graphics</i>, 11(2 (Proceedings of ACM SIGGRAPH 77)):242--248, 1977.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1357016</ref_obj_id>
				<ref_obj_pid>1342250</ref_obj_pid>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[Qinghua Dai, Baoguang Yang, and Jieqing Feng. Reconstructable geometry shadow maps. In <i>ACM SIGGRAPH Symposium on Interactive 3D Graphics and Games 2008: Posters</i>, page 4:1, 2008.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>569060</ref_obj_id>
				<ref_obj_pid>569046</ref_obj_pid>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[Yoshinori Dobashi, Tsuyoshi Yamamoto, and Tomoyuki Nishita. Interactive rendering of atmospheric scattering effects using graphics hardware. In <i>Proceedings of Graphics Hardware 2002</i>, pages 99--107, 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1111440</ref_obj_id>
				<ref_obj_pid>1111411</ref_obj_pid>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[William Donnelly and Andrew Lauritzen. Variance shadow maps. In <i>Proceedings of ACM SIGGRAPH Symposium on Interactive 3D Graphics and Games 2006</i>, pages 161--165, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1111424</ref_obj_id>
				<ref_obj_pid>1111411</ref_obj_pid>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[Elmar Eisemann and Xavier D&#233;coret. Fast scene voxelization and applications. In <i>Proceedings of ACM SIGGRAPH Symposium on Interactive 3D Graphics and Games 2006</i>, pages 71--78, 2006a.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[Elmar Eisemann and Xavier D&#233;coret. Plausible image based soft shadows using occlusion textures. In <i>Proceedings of SIBGRAPI 2006</i>, pages 155--162, 2006b.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA[Elmar Eisemann and Xavier D&#233;coret. Occlusion textures for plausible soft shadows. <i>Computer Graphics Forum</i>, 27(1):13--23, 2008.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1015778</ref_obj_id>
				<ref_obj_pid>1015706</ref_obj_pid>
				<ref_seq_no>32</ref_seq_no>
				<ref_text><![CDATA[Elmar Eisemann and Fr&#233;do Durand. Flash photography enhancement via intrinsic relighting. <i>ACM Transactions on Graphics</i>, 23(3 (Proceedings of ACM SIGGRAPH 2004)):673--678, 2004. URL http://artis.imag.fr/Publications/2004/ED04.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2049989</ref_obj_id>
				<ref_seq_no>33</ref_seq_no>
				<ref_text><![CDATA[Elmar Eisemann, Michael Schwarz, Ulf Assarsson, and Michael Wimmer. <i>Real-Time Shadows</i>. A K Peters/CRC Press, Boca Raton, FL, USA, 2011. ISBN 978-1-56881-438-4.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1730830</ref_obj_id>
				<ref_obj_pid>1730804</ref_obj_pid>
				<ref_seq_no>34</ref_seq_no>
				<ref_text><![CDATA[Eric Enderton, Erik Sintorn, Peter Shirley, and David Luebke. Stochastic transparency. In <i>Proceedings of ACM SIGGRAPH Symposium on Interactive 3D Graphics and Games 2010</i>, pages 157--164, 2010.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>35</ref_seq_no>
				<ref_text><![CDATA[Wolfgang Engel. Cascaded shadow maps. In Wolfgang Engel, editor, <i>ShaderX</i>&#60;sup&#62;<i>5</i>&#60;/sup&#62;<i>: Advanced Rendering Techniques</i>, pages 197--206. Charles River Media, Hingham, MA, USA, 2006. ISBN 978-1-58450-499-3.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1730823</ref_obj_id>
				<ref_obj_pid>1730804</ref_obj_pid>
				<ref_seq_no>36</ref_seq_no>
				<ref_text><![CDATA[Thomas Engelhardt and Carsten Dachsbacher. Epipolar sampling for shadows and crepuscular rays in participating media with single scattering. In <i>Proceedings of ACM SIGGRAPH Symposium on Interactive 3D Graphics and Games 2010</i>, pages 119--125, 2010.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1187153</ref_obj_id>
				<ref_obj_pid>1187112</ref_obj_pid>
				<ref_seq_no>37</ref_seq_no>
				<ref_text><![CDATA[Randima Fernando. Percentage-closer soft shadows. In <i>ACM SIGGRAPH 2005 Sketches and Applications</i>, page 35, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>38</ref_seq_no>
				<ref_text><![CDATA[Vincent Forest, Lo&#239;c Barthe, and Mathias Paulin. Accurate shadows by depth complexity sampling. <i>Computer Graphics Forum (Proceedings of Eurographics 2008)</i>, 27(2):663--674, 2008.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1598039</ref_obj_id>
				<ref_obj_pid>1597990</ref_obj_pid>
				<ref_seq_no>39</ref_seq_no>
				<ref_text><![CDATA[Pascal Gautron, Jean-Eudes Marvie, and Guillaume Fran&#231;ois. Volumetric shadow mapping. In <i>ACM SIGGRAPH 2009 Talks</i>, pages 49:1--49:1, 2009.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1268545</ref_obj_id>
				<ref_obj_pid>1268517</ref_obj_pid>
				<ref_seq_no>40</ref_seq_no>
				<ref_text><![CDATA[Markus Giegl and Michael Wimmer. Fitted virtual shadow maps. In <i>Proceedings of Graphics Interface 2007</i>, pages 159--168, 2007a.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1230112</ref_obj_id>
				<ref_obj_pid>1230100</ref_obj_pid>
				<ref_seq_no>41</ref_seq_no>
				<ref_text><![CDATA[Markus Giegl and Michael Wimmer. Queried virtual shadow maps. In <i>Proceedings of ACM SIGGRAPH Symposium on Interactive 3D Graphics and Games 2007</i>, pages 65--72, 2007b.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2383922</ref_obj_id>
				<ref_obj_pid>2383894</ref_obj_pid>
				<ref_seq_no>42</ref_seq_no>
				<ref_text><![CDATA[Ga&#235;l Guennebaud, Lo&#239;c Barthe, and Mathias Paulin. Real-time soft shadow mapping by back-projection. In <i>Proceedings of Eurographics Symposium on Rendering 2006</i>, pages 227--234, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>43</ref_seq_no>
				<ref_text><![CDATA[Tim Heidmann. Real shadows real time. <i>IRIS Universe</i>, 18:28--31, 1991.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1730819</ref_obj_id>
				<ref_obj_pid>1730804</ref_obj_pid>
				<ref_seq_no>44</ref_seq_no>
				<ref_text><![CDATA[Robert Herzog, Elmar Eisemann, Karol Myszkowski, and Hans-Peter Seidel. Spatio-temporal upsampling on the GPU. In <i>Proceedings of ACM SIGGRAPH Symposium on Interactive 3D Graphics and Games 2010</i>, pages 91--98, 2010.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>45</ref_seq_no>
				<ref_text><![CDATA[Jared Hoberock and Yuntao Jia. High-quality ambient occlusion. In Hubert Nguyen, editor, <i>GPU Gems 3</i>, pages 257--274. Addison-Wesley Professional, Reading, MA, USA, 2007. ISBN 978-0-321-51526-1.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>46</ref_seq_no>
				<ref_text><![CDATA[J.-C. Hourcade and A. Nicolas. Algorithms for antialiased cast shadows. <i>Computers & Graphics</i>, 9(3):259--265, 1985.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>47</ref_seq_no>
				<ref_text><![CDATA[Jing Huang, Tamy Boubekeur, Tobias Ritschel, Matthias Hollaender, and Elmar Eisemann. Separable approximation of ambient occlusion. In <i>Eurographics 2011 Short Papers</i>, 2011.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1283966</ref_obj_id>
				<ref_obj_pid>1283953</ref_obj_pid>
				<ref_seq_no>48</ref_seq_no>
				<ref_text><![CDATA[Takashi Imagire, Henry Johan, Naoki Tamura, and Tomoyuki Nishita. Anti-aliased and real-time rendering of scenes with light scattering effects. <i>The Visual Computer</i>, 23(9):935--944, 2007. ISSN 0178-2789.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>957190</ref_obj_id>
				<ref_obj_pid>957155</ref_obj_pid>
				<ref_seq_no>49</ref_seq_no>
				<ref_text><![CDATA[Robert James. True volumetric shadows. In Jeff Lander, editor, <i>Graphics programming methods</i>, pages 353--366. Charles River Media, Rockland, MA, 2003. ISBN 1-58450-299-1.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1095889</ref_obj_id>
				<ref_obj_pid>1095878</ref_obj_pid>
				<ref_seq_no>50</ref_seq_no>
				<ref_text><![CDATA[Gregory S. Johnson, Juhyun Lee, Christopher A. Burns, and William R. Mark. The irregular z-buffer: Hardware acceleration for irregular data structures. <i>ACM Transactions on Graphics</i>, 24(4):1462--1482, 2005. ISSN 0730-0301.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1507159</ref_obj_id>
				<ref_obj_pid>1507149</ref_obj_pid>
				<ref_seq_no>51</ref_seq_no>
				<ref_text><![CDATA[Gregory S. Johnson, Warren A. Hunt, Allen Hux, William R. Mark, Christopher A. Burns, and Stephen Junkins. Soft irregular shadow mapping: Fast, high-quality, and robust soft shadows. In <i>Proceedings of ACM SIGGRAPH Symposium on Interactive 3D Graphics and Games 2009</i>, pages 57--66, 2009.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>15902</ref_obj_id>
				<ref_obj_pid>15922</ref_obj_pid>
				<ref_seq_no>52</ref_seq_no>
				<ref_text><![CDATA[James T. Kajiya. The rendering equation. <i>Computer Graphics</i>, 20(4 (Proceedings of ACM SIGGRAPH 86)):143--150, 1986.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>808594</ref_obj_id>
				<ref_obj_pid>800031</ref_obj_pid>
				<ref_seq_no>53</ref_seq_no>
				<ref_text><![CDATA[James T. Kajiya and Brian P Von Herzen. Ray tracing volume densities. <i>Computer Graphics</i>, 18 (3 (Proceedings of ACM SIGGRAPH 84)):165--174, 1984. ISSN 0097-8930.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>54</ref_seq_no>
				<ref_text><![CDATA[Byungmoon Kim, Kihwan Kim, and Greg Turk. A shadow volume algorithm for opaque and transparent non-manifold casters. <i>Journal of Graphics Tools</i>, 13(3):1--14, 2008.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>55</ref_seq_no>
				<ref_text><![CDATA[Oliver Klehm, Tobias Ritschel, Elmar Eisemann, and Hans-Peter Seidel. Bent normals and cones in screen-space. In <i>Vision, Modeling and Visualization Workshop</i>, 2011.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>56</ref_seq_no>
				<ref_text><![CDATA[Oliver Klehm, Tobias Ritschel, Elmar Eisemann, and Hans-Peter Seidel. Screen-space bent cones: A practical approach. In Wolfgang Engel, editor, <i>GPU Pro</i>&#60;sup&#62;<i>3</i>&#60;/sup&#62;<i>: Advanced Rendering Techniques</i>, pages 191--207. A K Peters/CRC Press, Boca Raton, FL, USA, 2012.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1276497</ref_obj_id>
				<ref_obj_pid>1276377</ref_obj_pid>
				<ref_seq_no>57</ref_seq_no>
				<ref_text><![CDATA[Johannes Kopf, Michael F. Cohen, Dani Lischinski, and Matt Uyttendaele. Joint bilateral upsampling. <i>ACM Transactions on Graphics</i>, 26(3 (Proceedings of ACM SIGGRAPH 2007)): 96:1--96:5, 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1073327</ref_obj_id>
				<ref_obj_pid>1073204</ref_obj_pid>
				<ref_seq_no>58</ref_seq_no>
				<ref_text><![CDATA[Samuli Laine, Timo Aila, Ulf Assarsson, Jaakko Lehtinen, and Tomas Akenine-M&#246;ller. Soft shadow volumes for ray tracing. <i>ACM Transactions on Graphics (Proceedings of ACM SIGGRAPH 2005)</i>, 24(3):1156--1165, 2005a.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1073327</ref_obj_id>
				<ref_obj_pid>1073204</ref_obj_pid>
				<ref_seq_no>59</ref_seq_no>
				<ref_text><![CDATA[Samuli Laine, Timo Aila, Ulf Assarsson, Jaakko Lehtinen, and Tomas Akenine-M&#246;ller. Soft shadow volumes for ray tracing. <i>ACM Transactions on Graphics</i>, 24(3 (Proceedings of ACM SIGGRAPH 2005)):1156--1165, 2005b.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>60</ref_seq_no>
				<ref_text><![CDATA[Hayden Landis. Production-ready global illumination, 2002. In <i>ACM SIGGRAPH 2002 Course Notes</i>, RenderMan in Production.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>61</ref_seq_no>
				<ref_text><![CDATA[Andrew Lauritzen. Summed-area variance shadow maps. In Hubert Nguyen, editor, <i>GPU Gems 3</i>, pages 157--182. Addison-Wesley Professional, Reading, MA, USA, 2007. ISBN 978-0-321-51526-1.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1375739</ref_obj_id>
				<ref_obj_pid>1375714</ref_obj_pid>
				<ref_seq_no>62</ref_seq_no>
				<ref_text><![CDATA[Andrew Lauritzen and Michael McCool. Layered variance shadow maps. In <i>Proceedings of Graphics Interface 2008</i>, pages 139--146, 2008.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1944761</ref_obj_id>
				<ref_obj_pid>1944745</ref_obj_pid>
				<ref_seq_no>63</ref_seq_no>
				<ref_text><![CDATA[Andrew Lauritzen, Marco Salvi, and Aaron Lefohn. Sample distribution shadow maps. In <i>Proceedings of ACM SIGGRAPH Symposium on Interactive 3D Graphics and Games 2011</i>, pages 97--102, 2011.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1187126</ref_obj_id>
				<ref_obj_pid>1187112</ref_obj_pid>
				<ref_seq_no>64</ref_seq_no>
				<ref_text><![CDATA[Aaron Lefohn, Shubhabrata Sengupta, Joe Kniss, Robert Strzodka, and John D. Owens. Dynamic adaptive shadow maps on graphics hardware. In <i>ACM SIGGRAPH 2005 Sketches and Applications</i>, page 13, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1289611</ref_obj_id>
				<ref_obj_pid>1289603</ref_obj_pid>
				<ref_seq_no>65</ref_seq_no>
				<ref_text><![CDATA[Aaron E. Lefohn, Shubhabrata Sengupta, and John D. Owens. Resolution matched shadow maps. <i>ACM Transactions on Graphics</i>, 26(4):20:1--20:17, 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2383561</ref_obj_id>
				<ref_obj_pid>2383533</ref_obj_pid>
				<ref_seq_no>66</ref_seq_no>
				<ref_text><![CDATA[D. Brandon Lloyd, Jeremy Wendt, Naga K. Govindaraju, and Dinesh Manocha. CC shadow volumes. In <i>Proceedings of Eurographics Symposium on Rendering 2004</i>, pages 197--205, 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2383921</ref_obj_id>
				<ref_obj_pid>2383894</ref_obj_pid>
				<ref_seq_no>67</ref_seq_no>
				<ref_text><![CDATA[D. Brandon Lloyd, David Tuft, Sung-eui Yoon, and Dinesh Manocha. Warping and partitioning for low error shadow maps. In <i>Proceedings of Eurographics Symposium on Rendering 2006</i>, pages 215--226, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2383554</ref_obj_id>
				<ref_obj_pid>2383533</ref_obj_pid>
				<ref_seq_no>68</ref_seq_no>
				<ref_text><![CDATA[Tobias Martin and Tiow-Seng Tan. Anti-aliasing and continuity with trapezoidal shadow maps. In <i>Proceedings of Eurographics Symposium on Rendering 2004</i>, pages 153--160, 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>5515</ref_obj_id>
				<ref_obj_pid>5513</ref_obj_pid>
				<ref_seq_no>69</ref_seq_no>
				<ref_text><![CDATA[Nelson Max. Light diffusion through clouds and haze. <i>Computer Vision, Graphics, and Image Processing</i>, 33(3):280--292, 1986a.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>15899</ref_obj_id>
				<ref_obj_pid>15922</ref_obj_pid>
				<ref_seq_no>70</ref_seq_no>
				<ref_text><![CDATA[Nelson L. Max. Atmospheric illumination and shadows. <i>Computer Graphics</i>, 20(4 (Proceedings of ACM SIGGRAPH 86)):117--124, 1986b. ISSN 0097-8930.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1501592</ref_obj_id>
				<ref_obj_pid>1501585</ref_obj_pid>
				<ref_seq_no>71</ref_seq_no>
				<ref_text><![CDATA[&#192;lex M&#233;ndez-Feliu and Mateu Sbert. From obscurances to ambient occlusion: A survey. <i>The Visual Computer</i>, 25(2):181--196, 2009.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>72</ref_seq_no>
				<ref_text><![CDATA[Jason Mitchell. Light shafts: Rendering shadows in participating media, 2004. Presentation, <i>Game Developers Conference 2004</i>. http://developer.amd.com/media/gpu_assets/Mitchell_LightShafts.pdf.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1280098</ref_obj_id>
				<ref_obj_pid>1280094</ref_obj_pid>
				<ref_seq_no>73</ref_seq_no>
				<ref_text><![CDATA[Diego Nehab, Pedro V. Sander, Jason Lawrence, Natalya Tatarchuk, and John R. Isidoro. Accelerating real-time shading with reverse reprojection caching. In <i>Proceedings of Graphics Hardware 2007</i>, pages 25--35, 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>37437</ref_obj_id>
				<ref_obj_pid>37401</ref_obj_pid>
				<ref_seq_no>74</ref_seq_no>
				<ref_text><![CDATA[Tomoyuki Nishita, Yasuhiro Miyawaki, and Eihachiro Nakamae. A shading model for atmospheric scattering considering luminous intensity distribution of light sources. <i>Computer Graphics</i>, 21(4 (Proceedings of ACM SIGGRAPH 87)):303--310, 1987. ISSN 0097-8930.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1230111</ref_obj_id>
				<ref_obj_pid>1230100</ref_obj_pid>
				<ref_seq_no>75</ref_seq_no>
				<ref_text><![CDATA[Christopher Oat and Pedro V. Sander. Ambient aperture lighting. In <i>Proceedings of ACM SIGGRAPH Symposium on Interactive 3D Graphics and Games 2007</i>, pages 61--64, 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2383861</ref_obj_id>
				<ref_obj_pid>2383847</ref_obj_pid>
				<ref_seq_no>76</ref_seq_no>
				<ref_text><![CDATA[Ryan Overbeck, Ravi Ramamoorthi, and William R. Mark. A real-time beam tracer with application to exact soft shadows. In <i>Proceedings of Eurographics Symposium on Rendering 2007</i>, pages 85--98, 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>77</ref_seq_no>
				<ref_text><![CDATA[David Pajak, Robert Herzog, Elmar Eisemann, Karol Myszkowski, and Hans-Peter Seidel. Scalable remote rendering with depth and motion-flow augmented streaming. <i>Computer Graphics Forum</i>, 30(2 (Proceedings of Eurographics 2011)):415--424, 2011.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>78</ref_seq_no>
				<ref_text><![CDATA[Minghao Pan, Rui Wang, Weifeng Chen, Kun Zhou, and Hujun Bao. Fast, sub-pixel antialiased shadow maps. <i>Computer Graphics Forum</i>, 28(7 (Proceedings of Pacific Graphics 2009)): 1927--1934, 2009.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1555902</ref_obj_id>
				<ref_obj_pid>1555880</ref_obj_pid>
				<ref_seq_no>79</ref_seq_no>
				<ref_text><![CDATA[Vincent Pegoraro, Mathias Schott, and Steven G. Parker. An analytical approach to single scattering for anisotropic media and light distributions. In <i>Proceedings of Graphics Interface 2009</i>, pages 71--77, 2009.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2383634</ref_obj_id>
				<ref_obj_pid>2383616</ref_obj_pid>
				<ref_seq_no>80</ref_seq_no>
				<ref_text><![CDATA[Vincent Pegoraro, Mathias Schott, and Steven G. Parker. A closed-form solution to single scattering for general phase functions and light distributions. <i>Computer Graphics Forum</i>, 29(4 (Proceedings of Eurographics Symposium on Rendering 2010)):1365--1374, 2010.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1015777</ref_obj_id>
				<ref_obj_pid>1015706</ref_obj_pid>
				<ref_seq_no>81</ref_seq_no>
				<ref_text><![CDATA[Georg Petschnigg, Richard Szeliski, Maneesh Agrawala, Michael Cohen, Hugues Hoppe, and Kentaro Toyama. Digital photography with flash and no-flash image pairs. <i>ACM Transactions on Graphics</i>, 23(3 (Proceedings of ACM SIGGRAPH 2004)):664--672, 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>37435</ref_obj_id>
				<ref_obj_pid>37401</ref_obj_pid>
				<ref_seq_no>82</ref_seq_no>
				<ref_text><![CDATA[William T. Reeves, David H. Salesin, and Robert L. Cook. Rendering antialiased shadows with depth maps. <i>Computer Graphics</i>, 21(4 (Proceedings of ACM SIGGRAPH 87)):283--291, 1987.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1507161</ref_obj_id>
				<ref_obj_pid>1507149</ref_obj_pid>
				<ref_seq_no>83</ref_seq_no>
				<ref_text><![CDATA[Tobias Ritschel, Thorsten Grosch, and Hans-Peter Seidel. Approximating dynamic global illumination in image space. In <i>Proceedings of ACM SIGGRAPH Symposium on Interactive 3D Graphics and Games 2009</i>, pages 75--82, 2009.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2159641</ref_obj_id>
				<ref_obj_pid>2159616</ref_obj_pid>
				<ref_seq_no>84</ref_seq_no>
				<ref_text><![CDATA[Paul Rosen. Rectilinear texture warping for fast adaptive shadow mapping. In <i>Proceedings of ACM SIGGRAPH Symposium on Interactive 3D Graphics and Games 2012</i>, pages 151--158, 2012.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>85</ref_seq_no>
				<ref_text><![CDATA[Marco Salvi. Rendering filtered shadows with exponential shadow maps. In Wolfgang Engel, editor, <i>ShaderX</i>&#60;sup&#62;<i>6</i>&#60;/sup&#62;<i>: Advanced Rendering Techniques</i>, pages 257--274. Charles River Media, Hingham, MA, USA, 2008. ISBN 978-1-58450-544-0.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2383856</ref_obj_id>
				<ref_obj_pid>2383847</ref_obj_pid>
				<ref_seq_no>86</ref_seq_no>
				<ref_text><![CDATA[Daniel Scherzer, Stefan Jeschke, and Michael Wimmer. Pixel-correct shadow maps with temporal reprojection and shadow test confidence. In <i>Proceedings of Eurographics Symposium on Rendering 2007</i>, pages 45--50, 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>87</ref_seq_no>
				<ref_text><![CDATA[Michael Schwarz and Marc Stamminger. Bitmask soft shadows. <i>Computer Graphics Forum (Proceedings of Eurographics 2007)</i>, 26(3):515--524, 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2448209</ref_obj_id>
				<ref_obj_pid>2448196</ref_obj_pid>
				<ref_seq_no>88</ref_seq_no>
				<ref_text><![CDATA[Michael Schw&#228;rzler, Christian Luksch, Daniel Scherzer, and Michael Wimmer. Fast percentage closer soft shadows using temporal coherence. In <i>Proceedings of ACM SIGGRAPH Symposium on Interactive 3D Graphics and Games 2012</i>, I3D '13, pages 79--86, New York, NY, USA, 2013. ACM. ISBN 978-1-4503-1956-0. doi: 10.1145/2448196.2448209. URL http://doi.acm.org/10.1145/2448196.2448209.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>882301</ref_obj_id>
				<ref_obj_pid>882262</ref_obj_pid>
				<ref_seq_no>89</ref_seq_no>
				<ref_text><![CDATA[Pradeep Sen, Mike Cammarano, and Pat Hanrahan. Shadow silhouette maps. <i>ACM Transactions on Graphics</i>, 22(3 (Proceedings of ACM SIGGRAPH 2003)):521--526, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>90</ref_seq_no>
				<ref_text><![CDATA[Li Shen, Ga&#235;l Guennebaud, Baoguang Yang, and Jieqing Feng. Predicted virtual soft shadow maps with high quality filtering. <i>Computer Graphics Forum</i>, 30(2 (Proceedings of Eurographics 2011)):493--502, 2011.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>91</ref_seq_no>
				<ref_text><![CDATA[Oles Shishkovtsov. Deferred shading in S. T. A. L. K. E. R. In Matt Pharr, editor, <i>GPU Gems 2: Programming Techniques for High-Performance Graphics and General-Purpose Computation</i>, pages 143--166. Addison-Wesley Professional, Reading, MA, USA, 2006. ISBN 0-321-33559-7.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2383497</ref_obj_id>
				<ref_obj_pid>2383465</ref_obj_pid>
				<ref_seq_no>92</ref_seq_no>
				<ref_text><![CDATA[Erik Sintorn, Elmar Eisemann, and Ulf Assarsson. Sample based visibility for soft shadows using alias-free shadow maps. <i>Computer Graphics Forum</i>, 27(4 (Proceedings of Eurographics Symposium on Rendering 2008)):1285--1292, 2008a.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2383497</ref_obj_id>
				<ref_obj_pid>2383465</ref_obj_pid>
				<ref_seq_no>93</ref_seq_no>
				<ref_text><![CDATA[Erik Sintorn, Elmar Eisemann, and Ulf Assarsson. Sample based visibility for soft shadows using alias-free shadow maps. <i>Computer Graphics Forum (Proceedings of Eurographics Symposium on Rendering 2008)</i>, 27(4):1285--1292, 2008b.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2024187</ref_obj_id>
				<ref_obj_pid>2070781</ref_obj_pid>
				<ref_seq_no>94</ref_seq_no>
				<ref_text><![CDATA[Erik Sintorn, Ola Olsson, and Ulf Assarsson. An efficient alias-free shadow algorithm for opaque and transparent objects using per-triangle shadow volumes. <i>ACM Transactions on Graphics, 30</i>(6 (Proceedings of ACM SIGGRAPH Asia 2011)):153:1--153:10, 2011.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1409080</ref_obj_id>
				<ref_obj_pid>1457515</ref_obj_pid>
				<ref_seq_no>95</ref_seq_no>
				<ref_text><![CDATA[Pitchaya Sitthi-amorn, Jason Lawrence, Lei Yang, Pedro V. Sander, Diego Nehab, and Jiahe Xi. Automated reprojection-based pixel shader optimization. <i>ACM Transactions on Graphics</i>, 27 (5 (Proceedings of ACM SIGGRAPH Asia 2008)):127:1--127:11, 2008.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>280927</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>96</ref_seq_no>
				<ref_text><![CDATA[Cyril Soler and Fran&#231;ois X. Sillion. Fast calculation of soft shadow textures using convolution. In <i>Proceedings of ACM SIGGRAPH 98</i>, pages 321--332, 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>97</ref_seq_no>
				<ref_text><![CDATA[Tiago Sousa, Nickolay Kasyan, and Nicolas Schulz. Secrets of CryENGINE 3 graphics technology, 2011. In <i>ACM SIGGRAPH 2011 Courses</i>, Advances in Real-Time Rendering in 3D Graphics and Games.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>566616</ref_obj_id>
				<ref_obj_pid>566654</ref_obj_pid>
				<ref_seq_no>98</ref_seq_no>
				<ref_text><![CDATA[Marc Stamminger and George Drettakis. Perspective shadow maps. <i>ACM Transactions on Graphics</i>, 21(3 (Proceedings of ACM SIGGRAPH 2002)):557--562, 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>99</ref_seq_no>
				<ref_text><![CDATA[Martin Stich, Carsten W&#228;chter, and Alexander Keller. Efficient and robust shadow volumes using hierarchical occlusion culling and geometry shaders. In Hubert Nguyen, editor, GPU <i>Gems 3</i>, pages 239--256. Addison-Wesley Professional, Reading, MA, USA, 2007. ISBN 978-0-321-51526-1.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1073309</ref_obj_id>
				<ref_obj_pid>1073204</ref_obj_pid>
				<ref_seq_no>100</ref_seq_no>
				<ref_text><![CDATA[Bo Sun, Ravi Ramamoorthi, Srinivasa G. Narasimhan, and Shree K. Nayar. A practical analytic single scattering model for real time rendering. <i>ACM Transactions on Graphics</i>, 24(3 (Proceedings of ACM SIGGRAPH 2005)):1040--1049, 2005. ISSN 0730-0301.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1724975</ref_obj_id>
				<ref_obj_pid>1724963</ref_obj_pid>
				<ref_seq_no>101</ref_seq_no>
				<ref_text><![CDATA[L&#225;szl&#243; Szirmay-Kalos, Tam&#225;s Umenhoffer, Bal&#225;zs T&#243;th, L&#225;szl&#243; Sz&#233;csi, and Mateu Sbert. Volumetric ambient occlusion for real-time rendering and games. <i>IEEE Computer Graphics and Applications</i>, 30:70--79, 2010. ISSN 0272-1716. URL http://dx.doi.org/10.1109/MCG.2010.19.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>793029</ref_obj_id>
				<ref_obj_pid>792758</ref_obj_pid>
				<ref_seq_no>102</ref_seq_no>
				<ref_text><![CDATA[Katsumi Tadamura, Xueying Qin, Guofang Jiao, and Eihachiro Nakamae. Rendering optimal solar shadows using plural sunlight depth buffers. In <i>Proceedings of Computer Graphics International 1999</i>, pages 166--173, 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>103</ref_seq_no>
				<ref_text><![CDATA[Katsumi Tadamura, Xueying Qin, Guofang Jiao, and Eihachiro Nakamae. Rendering optimal solar shadows with plural sunlight depth buffers. <i>The Visual Computer</i>, 17(2):76--90, 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>104</ref_seq_no>
				<ref_text><![CDATA[Bal&#224;zs T&#243;th and Tam&#225;s Umenhoffer. Real-time volumetric lighting in participating media. In <i>Eurographics 2009 Short Papers</i>, 2009.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>897869</ref_obj_id>
				<ref_seq_no>105</ref_seq_no>
				<ref_text><![CDATA[Yulan Wang and Steven Molnar. Second-depth shadow mapping. Technical Report TR 94-019, University of North Carolina at Chapel Hill, 1994.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>106</ref_seq_no>
				<ref_text><![CDATA[D. Weiskopf and T. Ertl. Shadow Mapping Based on Dual Depth Layers. In <i>Eurographics 2003 Short Papers</i>, pages 53--60, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>807402</ref_obj_id>
				<ref_obj_pid>800248</ref_obj_pid>
				<ref_seq_no>107</ref_seq_no>
				<ref_text><![CDATA[Lance Williams. Casting curved shadows on curved surfaces. <i>Computer Graphics</i>, 12(3 (Proceedings of ACM SIGGRAPH 78)):270--274, 1978.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2383553</ref_obj_id>
				<ref_obj_pid>2383533</ref_obj_pid>
				<ref_seq_no>108</ref_seq_no>
				<ref_text><![CDATA[Michael Wimmer, Daniel Scherzer, and Werner Purgathofer. Light space perspective shadow maps. In <i>Proceedings of Eurographics Symposium on Rendering 2004</i>, pages 143--152, 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>130802</ref_obj_id>
				<ref_obj_pid>130745</ref_obj_pid>
				<ref_seq_no>109</ref_seq_no>
				<ref_text><![CDATA[Andrew Woo. The shadow depth map revisited. In David Kirk, editor, <i>Graphics Gems III</i>, pages 338--342. Academic Press, Boston, MA, 1992. ISBN 0-12-409673-5.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1900003</ref_obj_id>
				<ref_obj_pid>1899950</ref_obj_pid>
				<ref_seq_no>110</ref_seq_no>
				<ref_text><![CDATA[Chris Wyman. Interactive voxelized epipolar shadow volumes. In <i>ACM SIGGRAPH ASIA 2010 Sketches</i>, pages 53:1--53:2, 2010.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>111</ref_seq_no>
				<ref_text><![CDATA[Chris Wyman and Shaun Ramsey. Interactive volumetric shadows in participating media with single scattering. In <i>Proceedings of the IEEE Symposium on Interactive Ray Tracing 2008</i>, pages 87--92, 2008.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2383597</ref_obj_id>
				<ref_obj_pid>2383586</ref_obj_pid>
				<ref_seq_no>112</ref_seq_no>
				<ref_text><![CDATA[Baoguang Yang, Jieqing Feng, Ga&#235;l Guennebaud, and Xinguo Liu. Packet-based hierarchal soft shadow mapping. <i>Computer Graphics Forum</i>, 28(4 (Proceedings of Eurographics Symposium on Rendering 2009)):1121--1130, 2009.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>113</ref_seq_no>
				<ref_text><![CDATA[Baoguang Yang, Zhao Dong, Jieqing Feng, Hans-Peter Seidel, and Jan Kautz. Variance soft shadow mapping. <i>Computer Graphics Forum (Proceedings of Pacific Graphics 2010)</i>, 29(7): 2127--2134, 2010.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1128975</ref_obj_id>
				<ref_obj_pid>1128923</ref_obj_pid>
				<ref_seq_no>114</ref_seq_no>
				<ref_text><![CDATA[Fan Zhang, Hanqiu Sun, Leilei Xu, and Lee Kit Lun. Parallel-split shadow maps for large-scale virtual environments. In <i>Proceedings of ACM International Conference on Virtual Reality Continuum and Its Applications 2006</i>, pages 311--318, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>115</ref_seq_no>
				<ref_text><![CDATA[Hansong Zhang. Forward shadow mapping. In <i>Proceedings of Eurographics Workshop on Rendering 1998</i>, pages 131--138, 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Ef.cient Real-Time Shadows SIGGRAPH 2013 Course Notes www.realtimeshadows.com Elmar Eisemann Delft 
University of Technology Ulf Assarsson Chalmers University Michael Schwarz Michal Valient Guerrilla 
Games / Sony Computer Entertainment Michael Wimmer Vienna University of Technology  About the Authors 
 Elmar Eisemann (Organizer) Professor, Delft University of Technology, The Netherlands Elmar Eisemann 
is a professor at Delft University of Technology, heading the Computer Graph­ics and Visualization Group. 
His interests include real-time rendering, shadow algorithms, al­ternative representations, and GPU techniques. 
Previously, he was associate professor at Télé­com ParisTech (2009-2012) and research group leader in 
the Cluster of Excellence (Saarland University/Max-Planck-Institut Informatik) (2008-2009). He studied 
at the École Normale Su­périeure Paris (2001) and received his Master (2004)/Ph.D. (2008) from Grenoble 
Universities. He was a local organizer of EGSR 2010, 2012, HPG 2012 and is an associate editor of CGF 
and JCGT. In 2011, he was honored with the Eurographics Young Researcher Award. Ulf Assarsson Associate 
professor, Department of Computer Science and Engineering, Chalmers University of Technology, Sweden 
 Ulf Assarsson is head of a research group focusing primarily on real-time and non-real-time soft shadows 
as well as ray tracing. He received his M.Sc. in Engineering Physics in 1997 and Ph.D. in Computer Graphics 
in 2003. His research interests include GPU techniques and global illumination. Michael Schwarz Michael 
Schwarz received a Diploma in 2005 and a Ph.D. in 2009 from the University of Erlangen-Nuremberg. His 
graphics-related research interests include real-time computer graph­ics, GPU techniques, global illumination, 
procedural modeling, scalable approaches, and percep­tion-aware graphics. About the Authors Michal Valient 
Lead Tech, Guerrilla Games / Sony Computer Entertainment, The Netherlands Michal Valient leads the technology 
team at Guerrilla. He spends his time working on the en­gine technology powering highly acclaimed games 
such as Killzone 2 and Killzone 3 as well as some yet unreleased projects. Prior, he worked as a programmer 
and a lead at Caligari where he developed the shader-based real-time rendering engine for Caligari trueSpace7. 
His interests include many aspects of light transfer, shadows and parallel processing. He believes in 
sharing the knowledge and gave GDC and SIGGRAPH talks and wrote graphics papers published in ShaderX 
books and conference journals. Michael Wimmer Associate professor, Institute of Computer Graphics and 
Algorithms, Vienna University of Technology, Austria Michael Wimmer is an associate professor at the 
Institute of Computer Graphics and Algorithms of the Vienna University of Technology, where he received 
an M.Sc. in 1997 and a Ph.D. in 2001. His current research interests are real-time rendering, computer 
games, real-time visualization of urban environments, point-based rendering and procedural modeling. 
He has coauthored many papers in these .elds, and was papers co-chair of EGSR 2008 and Paci.c Graphics 
2012, and is associate editor of Computers &#38; Graphics.  Course Schedule  Introduction (Eisemann, 
5 min)  Why shadows? What are shadows? What are the problems?  Course overview   Basic algorithms 
(Assarsson, 15 min)  Shadow volumes Z-pass, Z-fail, ZP+, CUDA shadow volumes, . . . Shadow mapping Bias, 
dual-depth shadow maps, . . . Hard shadows (Wimmer, 35 min) Shadow-map aliasing  Shadow-map reparameterization 
 Perspective shadow maps, rectilinear warping, . . . Shadow-map partitioning Cascaded shadow maps, sample 
distribution shadow maps, adaptive shadow maps, . . . Shadow reconstruction Silhouette shadow maps, . 
. . Precise hard shadows Irregular z-bu.er, alias-free shadow maps, sub-pixel antialiased shadow maps, 
. . . Other topics Temporal reprojection, shadow-caster culling, . . . Filtering hard shadows (Eisemann, 
20 min) Introduction Percentage-closer .ltering E.cient .ltering Variance shadow maps, convolution shadow 
maps, exponential shadow maps, . . . Issues PCF bias, accelerations, . . . Course Schedule Image-based 
soft shadows (Schwarz, 15 min) Percentage-closer soft shadows and variants PCSS, CSSM, VSSM, . . . Occlusion 
textures  Soft shadow mapping (backprojection)  Break  Geometry-based soft shadows (Eisemann, 15 
min)  Approximate Penumbra wedges, soft shadow volumes Accurate View-sample mapping, depth-complexity 
sampling Volumetric shadows (Assarsson, 20 min) Light shafts 1D min-max mipmaps, polygonal light volumes, 
voxelized shadow volumes Volumetric shadow maps AVSM, Fourier opacity shadow maps Practical considerations 
and outlook for games and movies (Valient &#38; Schwarz, 45 min) Realistic budgets for performance/memory 
 Real-time upsampling techniques  Varying samples, bilateral upsampling, . . . Combination with other 
e.ects AO, SSAO, . . . Showcases AAA titles (including Killzone 2 &#38; 3) and upcoming games Outlook 
on shadow reconstruction techniques Reconstruction for distribution e.ects and shadow light-.eld frequency 
analysis, axis-aligned .ltering, decoupled sampling, temporal-coherence methods Conclusion, outlook, 
and Q&#38;A (All, 10 min) Contents 1 Introduction 3 1.1 Qualitative De.nition of a Shadow . . . . . 
. . . . . . . . . . . . . . . . . . . . 3 1.2 Quantitative De.nition of a Shadow . . . . . . . . . . 
. . . . . . . . . . . . . . 3 1.3 Shadow Types and Computation . . . . . . . . . . . . . . . . . . . 
. . . . . . . 5 2 Basic Algorithms 7 2.1 Shadow Maps . . . . . . . . . . . . . . . . . . . . . . . . 
. . . . . . . . . . . . 7 2.2 Shadow Volumes . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
. . . . . . 8 3 Hard Shadows 11 3.1 Shadow-Map Reparameterization . . . . . . . . . . . . . . . . . 
. . . . . . . . . 11 3.2 Global Shadow-Map Partitioning . . . . . . . . . . . . . . . . . . . . . . . 
. . . 11 3.3 Adaptive Shadow-Map Partitioning . . . . . . . . . . . . . . . . . . . . . . . . . 12 3.4 
Shadow Reconstruction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13 3.5 Precise Hard 
Shadows . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13 3.6 Other Considerations . . 
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13 4 Filtering Hard Shadows 17 4.1 Introduction: 
Percentage-Closer Filtering . . . . . . . . . . . . . . . . . . . . . 17 4.2 E.cient Filtering Approaches 
. . . . . . . . . . . . . . . . . . . . . . . . . . . 17 4.2.1 Variance Shadow Maps . . . . . . . . 
. . . . . . . . . . . . . . . . . . . 18 4.2.2 Convolution Shadow Maps . . . . . . . . . . . . . . . 
. . . . . . . . . . 18 4.2.3 Exponential Shadow Maps . . . . . . . . . . . . . . . . . . . . . . . . 
. 19 5 Soft Shadows 21 5.1 Image-Based Solutions . . . . . . . . . . . . . . . . . . . . . . . . . . 
. . . . . 21 5.2 Geometry-Based Solutions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23 
6 Volumetric Shadows 25 6.1 Single Scattering . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
. . . . . . 25 6.2 Approaches . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
. 26 6.2.1 Ray-Marching Approaches . . . . . . . . . . . . . . . . . . . . . . . . . 26 viii Contents 
 6.2.2 Shadow-Volume Based Approaches . . . . . . . . . . . . . . . . . . . . 27  6.3 Summary . . . 
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27 7 Practical Considerations 29 
7.1 Deferred Shading and Upsampling . . . . . . . . . . . . . . . . . . . . . . . . . 29 7.2 Ambient 
Occlusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31 8 Conclusion 33 8.1 Hard 
Shadows . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33 8.2 Filtered Hard 
Shadows . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33 8.3 Soft Shadows . . . . . . 
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34 8.4 Further Topics . . . . . . . . . . 
. . . . . . . . . . . . . . . . . . . . . . . . . . 34 8.5 Last Words . . . . . . . . . . . . . . . . 
. . . . . . . . . . . . . . . . . . . . . . 34  Preface What is this course about? This course is a 
resource for e.cient real-time shadow algo­rithms. It gives an overview of various techniques and addresses 
practical and game-relevant solutions. Besides conventional topics, such as hard or soft shadows, we 
also address recent practically-relevant topics, e.g., volumetric shadows. We will provide the theoretical 
background but also discuss details on implementation issues in order to facilitate e.cient realizations. 
These elements are of relevance to experts but also to practitioners, as they support the general understanding 
and provide interesting views. A particu­lar focus will be put on budget considerations and the analysis 
of existing performance trade-o.s; physical accuracy can sometimes be replaced by plausible shadows, 
and we will describe sce­narios in which approximate methods are likely to work or fail. In particular, 
we will present showcases that illustrate the techniques behind major game titles and upcoming engines. 
Who should participate? The course will be useful for many di.erent .elds, ranging from the game industry 
(as an overview and guide) over the movie industry (as a resource for previ­sualization techniques) to 
other branches, such as the visualization community (shadows have a strong impact on spatial perception). 
It will allow its participants to gain insight into several methods that .nd application in high-performance 
simulations and to receive practical advice. Where can I get more information? The course builds upon 
a solid foundation in form of previous courses, as well as the recent book Real-time Shadows [Eisemann 
et al., 2011] (A K Peters/CRC Press) that appeared in 2011 and was written by four of the presenters. 
The book is a compendium of many topics in the realm of shadow computation on 400 pages. Furthermore, 
the course features one contributor from the industry, who has a strong background in shadow techniques 
and was a lead for several AAA titles. Please also visit our webpage that o.ers additional information 
on shadow algorithms: http://www.realtimeshadows.com/ 1 Introduction 1.1 Qualitative De.nition of a 
Shadow When asking what a shadow is, people will give various answers and for most, you will .nd counterexamples 
of shadow phenomena that would not have been captured. Even dictionaries have a hard time coming up with 
proper formulations. A mathematical way to de.ne a shadow in the context of computer graphics is to consider 
a point p in space. If each ray from the light source L reaches p without encountering any scene object 
(so-called blockers or shadow casters) on its way, p is considered lit, otherwise in shadow. In other 
words, we are in shadow, if from our point p the light source L is at least partially occluded. The region 
of space where the light source is fully occluded is called umbra, whereas the remaining shadow is referred 
to as penumbra. The di.erent regions are indicated in Fig. 1.1. 1.2 Quantitative De.nition of a Shadow 
While the previous de.nition allows us to de.ne where we .nd shadows, it does not allow us to say anything 
about the way its appearance is in.uenced by being in shadow. To clarify this point, we will place ourselves 
in a simple context. This .rst de.nition will be su.cient for most of this course, but we will later 
give an outlook on extensions. The physical interaction can be described well by a soft shadow equation 
that builds upon a modi.cation of the rendering equation introduced by Kajiya [1986] where we only consider 
direct light from the source: Lo(p, .) = fr(p, ., p . q) G(p, q) Le(q, q . p) V(p, q) dq, (1.1) L where 
Lo(p, .) is the outgoing radiance in direction . from point p, f is the BRDF (bidirectional re.ectance 
function) encoding the material surface properties, G is a geometric term taking the con.guration of 
source and receiver into account, Le(q, q . p) is the emitted energy from the source point q towards 
p, and .nally V(p, q) encodes the visibility and is zero if there is a blocker between the two points 
q and p, and otherwise one. If we further assume that all surfaces are Lambertian (perfectly di.use), 
the BRDF becomes independent of direction, i.e. fr(p, ., . ) = .(p)/p where .(p) denotes re.ectance. 
Consequently, Figure 1.1 Depending on the visibility of the source, there are three di.erent regions 
in space; the umbra region, where the light is invisible, and the penumbra region, where it is partially 
visible constitute the shadow, the rest of the scene is lit. the outgoing radiance Lo also no longer 
depends on the outgoing direction. The equation simpli­.es to: .(p) Lo(p) = Le(q, q . p) G(p, q) V(p, 
q) dq, p L In many situations, another simpli.cation can be applied in form of a separation of the integral: 
.(p) 1 Lo(p) = G(p, q) dq · Le(q, q . p) V(p, q) dq . |L| p L L _ _ _ _ ___ Shading Shadow Shading and 
shadows are hereby decoupled. Most existing shadow algorithms aim at evaluating the shadow term of the 
above equation, or even assume a homogenous light source of emission ¯ Lc, leading to a visibility integral 
that modulates the shading and represents the actual shadow component of the equation: ¯ Lc V(p, q) dq. 
(1.2) L Most real-time applications, aim at solving this equation when aiming for realistic shadows. 
Nevertheless, some methods allow us to compute Equation 1.1 at a supplementary cost. 1 Introduction 5 
  1.3 Shadow Types and Computation In many cases, the Equation (1.2) is solved by sampling the source 
point-wise. This has an interesting consequence, for a point light source, the whole integral becomes 
a single visibility query; shadows are binary. This is also re.ected by the absence of a penumbra region. 
Shadows are thus hard because the transition between light and darkness is immediate. For volumetric 
or area sources, the transition is smooth, hence, the name soft shadows. Fig. 1.2 shows the result obtained 
when approximating a soft shadow by evaluating several positions on the light source. Even though Equation 
(1.2) does not seem to complicated, one should notice, that it means testing several rays from each point 
in the scene towards the source. The outcome can no longer be locally decided on the point, but all triangles 
in the scene are potentially involved. This makes an e.cient solution di.cult. In the following of this 
course, we will see various solutions of di.ering degrees of accuracy and performance. As we will see, 
the best algorithmic choice will depend heavily on the con.gu­ration of the scene, the type of light 
source, the wanted accuracy, the way the scene is represented, or even the viewpoint. The realm of possibilities 
is large and to make a good choice for your par­ticular needs, a good comprehension, a performance and 
quality analysis, and a comprehensive overview are needed. Our goal is to provide this information with 
this course. 2  Basic Algorithms In this course section, the most basic algorithms for hard shadows 
will be described. Most of the more advanced techniques are derived from these techniques including the 
ones targeting soft shadows. There are three main classes of such methods: projection shadows, shadow 
maps, and shadow volumes. Projection shadows typically mean that the shadow is projected onto a plane 
and drawn there as a dark object. This will not be further treated here. Shadow maps and shadow volumes 
correspond to two di.erent ways of thinking about shadows: as places not seen by the light source and 
as volumes of space that are dark. The former corresponds to shadow maps, explained next, and the latter 
to shadow volumes. 2.1 Shadow Maps The shadow maps method was introduced in 1978 [Williams, 1978]. The 
algorithm starts by rendering an image from the light source. Here, only the depth is stored for each 
pixel. This image (called the shadow map) represents all locations in space that are in light. Next, 
the scene is rendered from the camera. For each pixel, the fragment shader tests if the sampled point 
is represented in the light s view, i.e., the shadow map. If so, the point is in light. Else, the point 
is in shadow. Due to the discrete resolution of the shadow map, a view sample will rarely be exactly 
rep- 8 2.2 Shadow Volumes  (a) (b) resented in the shadow map. This results in two problems: jagged 
shadows and the need to introduce a tolerance threshold for the comparison. The threshold, or bias, must 
be .ne tuned for each scene, and no bias is guaranteed to exist that avoids artifacts. A too large bias 
results in light leakage at contact shadows, while a too small bias results in incorrect self-shadowing. 
Increasing the shadow map resolution can be helpful, since then a smaller bias can be used. But it does 
not remove the problem. Solving this is the main target of most of the more advanced methods for hard 
shadows, described further on in the course. Relatively simple methods exist, however, that mostly pushes 
the biasing problem to near the silhouette edges, as seen from the light source, of the shadow casting 
objects [Hourcade and Nicolas, 1985; Wang and Molnar, 1994; Woo, 1992; Weiskopf and Ertl, 2003]. 2.2 
Shadow Volumes In its most basic form, the shadow volume algorithm creates a volume of space in shadow 
from each triangle. Each view sample is then tested for inclusion in any such shadow volume by using 
the stencil bu.er to perform the test [Heidmann, 1991]. Although the technique was introduced already 
in 1977 [Crow, 1977], it was not until 1991 that the algorithm was e.ciently mapped onto graphics cards. 
Then, it was, however, no longer fully robust, and .xing this became an area for the next 20 years of 
research, resulting in for instance the Z-fail, ZP+, and ++ZP algorithms [Bilodeau and Songy, 1999; Carmack, 
2000; Eisemann et al., 2011]. Another problem of the shadow volume algorithm is that their rasterization 
puts a high demand on the .ll-rate capacity of the graphics hardware. This inherently makes the algorithm 
slower than shadow maps. Thus, major focus has been put on lowering the amount of necessary rasterization. 
Shadow volumes are created per object instead of per triangle [Bergeron, 1986; Aldridge and Woods, 2004; 
Kim et al., 2008], and culling and clamping of the volumes are used [Clark, 1976; Lloyd et al., 2004; 
Stich et al., 2007; Eisemann and Décoret, 2006a]. 2 Basic Algorithms 9 One of the most recent developments 
of shadow volumes is the Per-Triangle Shadow Volume algorithm [Sintorn et al., 2011]. CUDA is used to 
rasterize per-triangle shadow volumes onto a hierarchical frame bu.er resulting in a low overdraw and 
that transparent shadow casters trivially can be supported (see Figure 2.2). 3  Hard Shadows In the 
previous part of this course, the basic hard shadow algorithms have been explained. In this part, we 
will discuss several methods to reduce shadow-map aliasing artifacts. After analyzing aliasing in more 
detail and showing the di.erent components of aliasing, we will show di.erent strategies to reduce sampling 
error. 3.1 Shadow-Map Reparameterization When projecting the view frustum into the shadow map, it becomes 
apparent that higher sampling densities are required near the viewpoint and lower sampling densities 
far from the viewpoint. In some cases, it is possible to apply a single transformation to the scene before 
projecting it into the shadow map such that the sampling density is globally changed in a useful way 
(Perspective Shadow Maps (PSM) [Stamminger and Drettakis, 2002]). It can be shown that a logarithmic 
transformation along the z-axis of the viewer provides an optimal sampling rate for the whole depth range 
in the view frustum [Wimmer et al., 2004], however, this requires logarithmic ras­ terization, which 
is currently infeasible. Practical warping schemes use perspective transformations to redistribute samples 
towards the near plane [Wimmer et al., 2004; Martin and Tan, 2004; Chong, 2003; Chong and Gortler, 2004]. 
Figure 3.1 shows the idea based on light-space perspective shadow mapping (LiSPSM), where a perspective 
transform in the shadow-map plane is used to redistribute samples. A recent approach tries to adapt the 
warping locally according to the scene content while still using only a single shadow map, which is possible 
using a rectilinear warping grid [Rosen, 2012]. 3.2 Global Shadow-Map Partitioning While warping works 
very well in some con.gurations, especially if the light is overhead, there are other con.gurations where 
warping degenerates to uniform shadow mapping. A better alter­native is to use more than one shadow map. 
The most prominent approach and one of the most practical algorithms is to subdivide the view frustum 
along the z-axis, and calculate a separate equal-sized shadow map for each sub­frustum. This algorithm 
goes by the names of Cascaded Shadow Maps (CSM) [Engel, 2006], Parallel Split Shadow Maps (PSSM) [Zhang 
et al., 2006], or z-partitioning [Lloyd et al., 2006], 12 3.3 Adaptive Shadow-Map Partitioning  warp, 
objects near the viewer appear bigger in the shadow map and therefore receive more samples (right). but 
was actually already discovered earlier [Tadamura et al., 1999, 2001]. Using this approach, the sampling 
density decreases for each successive partition, because the same number of shadow map samples cover 
a larger and larger area. The bene.t can be maximized by analyzing the actual distribution of the depth 
values in the view frustum [Lauritzen et al., 2011] in so-called sample distribution shadow maps. Figure 
3.2 shows an example con.guration for PSSM. Reparametrization and partitioning can also be combined (see 
Figure 3.3). 3.3 Adaptive Shadow-Map Partitioning The advantage of global partitioning algorithm is 
that they are very fast. On the other hand, they completely ignore surface orientation and can therefore 
not improve undersampling due to surfaces that are viewed almost edge-on by the light source (projection 
aliasing). There are a number of algorithms that try to allocate samples in a more optimal way by ana­lyzing 
the scene before creating the shadow map. This inevitably incurs some overhead due to the analysis step 
(which often necessitates a costly read-back), but leads to much better results in general cases. Prominent 
examples are Adaptive Shadow Maps (ASM) [Lefohn et al., 2005], Resolution Matched Shadow Maps (RSMS) 
[Lefohn et al., 2007], Queried Virtual Shadow Maps (QSM) [Giegl and Wimmer, 2007b], Fitted Virtual Shadow 
Maps (FVSM) [Giegl and Wimmer, 2007a], and Tiled Shadow Maps (TiledSM) [Arvo, 2004]. Figure 3.4 shows 
the e.ect of one such adaptive partitioning method, QVSM, in action. 3 Hard Shadows 13  3.4 Shadow 
Reconstruction All methods discussed so far assume that shadow maps are sampled at certain positions, 
and re­construction accesses these samples. However, we can also get a more accurate reconstruction of 
shadow edges by storing, for example, information about silhouettes in the shadow map. Notable techniques 
are forward shadow mapping [Zhang, 1998] and silhouette shadow maps [Sen et al., 2003], and reconstructable 
geometry shadow maps [Dai et al., 2008]. 3.5 Precise Hard Shadows The aliasing artifacts in hard shadow 
mapping stem from the fact that the shadow map query locations do not correspond to the shadow map sample 
locations. Ideally, one would like to create shadow map samples exactly in those positions that will 
be queried later on. Di.cult as that may seem, it is actually possible and has been proposed independently 
by Aila and Laine [2004], and Johnson et al. [2005], implemented in hardware [Sintorn et al., 2008b], 
and later extended to provide e.cient antialiasing [Pan et al., 2009]. 3.6 Other Considerations Finally, 
we will show how to improve shadow quality through temporal coherence [Scherzer et al., 2007], and to 
speed up shadow mapping for large scenes by shadow caster culling [Bittner et al., 2011]. 14 3.6 Other 
Considerations  3 Hard Shadows 15  4  Filtering Hard Shadows In this part of the course, we discuss 
several .ltering methods for shadow mapping, which are mainly useful for reducing resampling error. Filtering 
is also often used to hide the fact that the resolution of the shadow map is too low by smoothing or 
blurring the shadow boundaries, and sometimes even to provide a rough approximation to soft shadows. 
We start with a general technique, which quickly becomes infeasible for larger .lter kernel sizes, and 
then show several e.cient .ltering techniques that rely on di.erent levels of precomputation. 4.1 Introduction: 
Percentage-Closer Filtering While the idea of shadow-map .ltering is very close to texture mapping, in 
practice there is an important di.erence. It is in general not possible to apply a .lter function to 
the shadow map and then shadow test the result. In that case, the depth values would be averaged, but 
the resulting shadows would still show the same aliasing artifacts because for each view sample, the 
shadow test still leads to a binary outcome. Instead, one needs to .lter the shadow signal, not the depth 
signal. This approach is called percentage-closer .ltering (PCF) [Reeves et al., 1987]. Formally, it 
is as simple as changing the order of depth testing and .ltering, i.e., for every sample in the .lter 
kernel, a texture lookup is performed, the depth test is carried out, and only then is the .lter applied. 
This approach is very simple and cheap for small .lter kernels. Thus, it is mostly useful when the shadow 
is magni.ed on screen. However, for larger .lter kernels, this test implies a large performance penalty 
because the complete .lter kernel needs to be evaluated for every shadow lookup. While PCF produces blurred 
shadows, these shadows do not correspond to soft shadows caused by an area light source, although for 
smaller .lter kernels, the impression of a small area light source can be reasonably invoked. Figure 
4.1 shows PCF for small and large .lter kernels. 4.2 Ef.cient Filtering Approaches While for texture 
mapping, the result of .ltering with larger .lters can be precomputed, for example using mip-mapping, 
this is not easily possible with shadow mapping. The main problem is that we need to .lter the outcome 
of the shadow test, and not the depth signal. Since the shadow test function is not linear, we cannot 
change the order of computations. 18 4.2 Ef.cient Filtering Approaches  There are two main ways of getting 
around this limitation: either we interpret the depth sam­ples as a distribution and then model the depth 
comparison statistically. The other option is to approximate the depth comparison function with a linear 
combination of functions that are linear in the depth component. 4.2.1 Variance Shadow Maps Variance 
shadow maps (VSM), introduced by Donnelly and Lauritzen [Donnelly and Lauritzen, 2006], are the .rst 
example of using statistics to facilitate precomputation of shadow-map .l­ tering. In VSM, the depth 
distribution of samples that need to be evaluated by a .lter kernel is modeled using .rst and second 
moments, and the percentage of samples that are hidden is esti­mated using the Chebyshev inequality. 
The .rst two moments correspond to the average depth and average squared depth, which can be easily precomputed 
in a manner similar to mipmap­ping. VSMs su.er from light leaks, which can be avoided using layered variance 
shadow maps (LVSM) [Lauritzen and McCool, 2008], albeit at higher cost. To allow arbitrary rectangular 
.lter kernels, VSMs can be evaluated at runtime using summed area tables [Lauritzen, 2007]. 4.2.2 Convolution 
Shadow Maps Convolution shadow maps (CSM) [Annen et al., 2007] are the .rst approach to allow .lter pre­ 
computation based on a linear signal-theory framework. The shadow-test function, which is a step function, 
is approximated by its (truncated) Fourier expansion. The coe.cients of the Fourier expansion are stored 
in textures and can be mipmapped. Similar to VSMs, CSMs su.er from light leaks. Figure 4.2 shows variance 
shadow maps and convolution shadow maps in comparison. 4 Filtering Hard Shadows 19  visible when scaling 
the light intensity by a factor of four (right of each pair). Newer ap­ proaches are able to reduce these 
artifacts signi.cantly. 4.2.3 Exponential Shadow Maps Annen et al. [Annen et al., 2008b] and Salvi [Salvi, 
2008] proposed new basis functions for the CSM approach. They suggested replacing the Fourier expansion 
by a simple exponential. This choice voids much of the storage requirements and thus addresses one of 
the major issues. However, the exponential approximation is not valid where the depth is in front of 
the reference depth, and such cases have to be handled by resorting to PCF. An interesting alternative 
is to combine the idea of using exponentials with the VSM approach. Lauritzen and McCool [Lauritzen and 
McCool, 2008] propose using the variance shadow-map approach and applying it to depth maps that were 
warped by an exponential function, leading to a fast and robust solution that mostly avoids light leaks. 
5  Soft Shadows While point light sources are very popular in computer graphics, especially in the 
real-time do­main, real light sources have a certain extent that often cannot be neglected. In particular, 
the shadows cast by them are not hard but feature partially lit penumbrae (transition regions from completely 
lit to fully occluded), leading to a soft appearance. In this part of the course, we will discuss the 
challenges faced when computing soft shadows and present various practical approaches of varying quality, 
speed, and accuracy. Unlike with hard shadows, it is not enough to just determine whether the light source 
is visible from a receiver point or not. Instead, the visible fraction of the light source has to be 
computed, and it is primarily this point-region visibility problem that renders computing soft shadows 
hard and expensive. For instance, one faces the occluder fusion problem: as occluders may interact in 
non-trivial ways, it is generally not possible to simply process individual occluders, determine their 
respective occlusion factors, and then combine these to get the overall light visibility. Con­sequently, 
many research e.orts were and are still dedicated to fast approximate solutions aiming at producing results 
that are reasonably close to the correct result or at least look plausible. On the other hand, advances 
in computational power and programmability nowadays enable approaches that yield accurate results at 
interactive rates even for complex scenes. 5.1 Image-Based Solutions For the majority of real-time applications, 
approximate methods are currently most relevant thanks to their speed. Most of them employ an image-based 
scene representation, typically re­sorting to a standard shadow map. One large group of approaches builds 
on the observation that blurring hard shadow test re­sults yields a soft-shadow-like appearance. Percentage-closer 
soft shadows (PCSS) [Fernando, 2005] adaptively choose the amount of blurring using a single-planar-occluder 
approximation and then applies standard percentage-closer .ltering (see Figure 5.1). As this scheme involves 
many shadow map accesses, several techniques for speeding up the computations were devised. Some advanced 
methods like convolution soft shadows [Annen et al., 2008a] and variance soft shadow mapping [Yang et 
al., 2010] employ alternative shadow map representations and pre­ .ltering, rendering the soft-shadow 
computation basically a constant-time operation. Targeting higher quality, Shen et al. [2011] introduce 
an advanced .ltering method and employ adaptive shadow-map partitioning, guided by a perceptual resolution 
prediction metric that exploits the 22 5.1 Image-Based Solutions  (a) Blocker search (b) Penumbra width 
estimation (c) Filtering typically low-frequency nature of penumbrae. Schwärzler et al. use temporal 
coherence to avoid shadow recomputation in areas that were not changed [Schwärzler et al., 2013]. A quite 
di.erent technique is occlusion textures [Eisemann and Décoret, 2006b, 2008], where the scene gets decomposed 
into slices, representing each slice by a planar occluder. By adaptively blurring these occluders image-based 
representations with a box .lter [Soler and Sillion, 1998], which can e.ciently be done via pre.ltering, 
and combining them, approximate soft shadows can be obtained rapidly. Soft shadow mapping [Atty et al., 
2006; Guennebaud et al., 2006] is a rather accurate approach for which many variants exist. It employs 
the shadow map to reconstruct an approximation of the occluders, unprojecting the shadow-map texels into 
world space. The resulting micro-occluders are then backprojected onto the light source, and by aggregating 
the occluded parts, the light s visibility is determined (see Figure 5.2). Like all previously mentioned 
methods, this aggregation combines scalar occlusion values for individual occluders and hence su.ers 
from the occluder fusion problem. This is alleviated by bitmask soft shadows [Schwarz and Stamminger, 
2007], where the extended light is represented by many point lights, and the binary visibility of these 
sample points is tracked with an occlusion bitmask. While, in principle, this enables accurate results 
(if a su.ciently high number of well-distributed samples is used), the inherently approx­imate nature 
of image-based representations ultimately precludes them. Yang et al. improve the performance of soft 
shadow mapping by exploiting screen and light space coherence [Yang et al., 2009]. 5 Soft Shadows 23 
  5.2 Geometry-Based Solutions By contrast, geometry-based solutions avoid aliasing problems of image-based 
approaches, but this typically comes along with a slower speed. Soft shadow volumes [Assarsson and Akenine-Möller, 
2003] build on shadow volumes for hard shadows and additionally employ penumbra wedges [Akenine-Möller 
and Assarsson, 2002] to account for penumbra regions. These wedges are constructed for all silhouette 
edges and encompass the resulting penumbrae (see Figure 5.3). For each covered pixel, the edge is backprojected 
onto the light, and ultimately, the light area covered by the corresponding occluder is computed using 
Green s formula. However, the method su.ers from the occluder fusion problem, leading to wrong results 
if occluders overlap. This is addressed by depth complexity sampling [Laine et al., 2005a] where the 
light area is represented by several light sample points, and a counter for each sample point is maintained, 
keeping track of the number of occluders overlapping the sample point. Originally developed for o.ine 
ray­tracing, a GPU variant exists as well [Forest et al., 2008]. Avoiding the high .ll rate of shadow-volume-based 
methods, view-sample mapping [Sintorn et al., 2008b] inserts the view samples (i.e., the shadow-receiving 
pixel sample points) into an alias-free shadow map and then rasterizes the occluders triangles into this 
map. For each shadow map entry, an occlusion bitmask is maintained, and the light sample points overlapped 
by a triangle are set. Ultimately, the number of occluded points yields the amount of occlusion. This 
method not only produces accurate results but is also reasonably fast for interactive applications. A 
related method is soft irregular shadow mapping [Johnson et al., 2009], which makes some compromises 
concerning accuracy in favor of visual smoothness, abandoning point sampling of the light visibility 
and resorting to silhouettes instead of triangles. 24 5.2 Geometry-Based Solutions  6  Volumetric 
Shadows In this part of the course, we will talk about shadows in participating media. The word par­ticipating 
media means that the medium through which the light travels interacts with the light itself. In reality, 
the world around us is .lled with participating media, e.g., air, fog, clouds and smoke. A participating 
medium scatters light, which means that photons bounce o. the particles in the medium. The light undergoes 
re.ections and possibly also minor refractions, for instance when hitting microscopic water drops. The 
light rays will bounce around before, at least some, will eventually reach the eye. This phenomenon makes 
the participating medium visible. 6.1 Single Scattering For real-time purposes, it is common to only 
consider single scattering, i.e., only one light bounce is taken into account. The light travels from 
the light source, undergoes one re.ective bounce and reaches the eye (so called in-scattering). This 
light, scattered towards the viewer and making the participating medium visible, is also referred to 
as airlight. Multiple scattering is mostly too expensive to compute in real-time. Nevertheless, for optically 
thin media, e.g., air, for which the transmittance is close to 100%, the single scattered rays constitute 
the dominating part of the visual appearance. It is also easy and common practice to account for out 
scattering along the 26 6.2 Approaches light rays. This corresponds to attenuation of the intensity 
with the traveling distance. There are many ways to mathematically solve the airlight computation [Sun 
et al., 2005; Pegoraro et al., 2009, 2010]. There are semi-analytic solutions with texture lookups which 
are used in real-time applications and purely analytical, which are slow. 6.2 Approaches Research on 
volumetric shadows started already in the early 1980s. Blinn introduced a model that describes light 
re.ection for clouds and dusty surfaces consisting of many small particles [Blinn, 1982]. Soon, ray-tracing 
based approaches were used to compute shadows in and by participat­ ing media [Kajiya and Von Herzen, 
1984]. Shadow volumes were also tried early on to produce atmospheric shadows [Max, 1986b,a; Nishita 
et al., 1987]. With the introduction of programmable shading and compute shaders (e.g. CUDA), there has 
recently been a strong revival on how to compute the shadows in participating media with the single-scattering 
assumption. Many new solutions have been proposed. These are still divided into ray marching techniques 
[Dobashi et al., 2002; Mitchell, 2004; Imagire et al., 2007; Gautron et al., 2009; Tóth and Umenho.er, 
2009; Engelhardt and Dachsbacher, 2010; Baran et al., 2010; Chen et al., 2011; Wyman, 2010] and shadow-volume 
based techniques [James, 2003; Biri et al., 2006; Wyman and Ramsey, 2008; Billeter et al., 2010]. These 
techniques will be presented brie.y. (a) (b) (c) Figure 6.2 (a) Light is scattered in the participating 
medium here by microscopic water drops in the air. (b) Ray marching along a view ray to compute airlight 
contribution for a pixel (i.e., in-scattered light with attenuation from out scattering). (c) Alternatively, 
shadow-volume based approaches can be used to compute the amount of single-scattered light towards the 
eye. 6.2.1 Ray-Marching Approaches Ray-marching approaches step along each per-pixel view ray (see Figure 
6.2(b)). For each delta step and position that is in light, the in scattering towards the eye is computed. 
Shadows are 6 Volumetric Shadows 27 checked against a shadow map. The ray marching can be done by drawing 
alpha-blended planes [Dobashi et al., 2002; Imagire et al., 2007; Mitchell, 2004], looping in a fragment 
shader [Gautron et al., 2009; Tóth and Umenho.er, 2009; Engelhardt and Dachsbacher, 2010], or using OpenCL 
or CUDA [Baran et al., 2010; Chen et al., 2011]. Wyman notes that shadow volume planes can be used to 
bound the ray marching [Wyman and Ramsey, 2008] . Tóth and Umenho.er [Tóth and Umenho.er, 2009] only 
ray marches a few samples per pixel and borrows results from nearby pixels. Chen et al. [Chen et al., 
2011] instead reduces the ray marching steps by utilizing that all positions further from the light than 
another shadowed position, along the same light ray, have to remain in shadow. The most recent method 
[Wyman, 2010] is based on voxelizing the partic­ ipating media, where each voxel in light stores a bit 
set to one, and each voxel in shadow stores a bit set to zero. The method then very rapidly computes 
the number of lit voxels along each eye ray by using GPU-based pre.x-sums.  6.2.2 Shadow-Volume Based 
Approaches If shadow volumes from separate shadow casters are guaranteed to not overlap, airlight can 
be computed as a sum of order-independent terms by adding contribution for each front-facing shadow volume 
quad and subtracting for each back-facing quad. Otherwise, some sorting of the shadow quads is required. 
Biri et al. [Biri et al., 2006] sort the shadow volume quads, back­ to-front, from the camera, while 
James [James, 2003] instead orders them using depth peeling. Billeter et al. notice that non-overlapping 
shadow volumes can be guaranteed by constructing them from a shadow map [Billeter et al., 2010].  6.3 
Summary We will here summarize the characteristics of the three most recent methods, of which the two 
.rst are ray-marching based [Chen et al., 2011; Wyman, 2010] and the third is shadow-volume based [Billeter 
et al., 2010]. All three methods are very fast and capable of producing frame rates in the order of a 
hundred fps for typical game scenes. The method by Chen et al. is based on utilizing coherency as much 
as possible to reduce the amount of ray marching that needs to be done. This method is the most versatile, 
since it is capable of handling textured light sources. Wyman s algorithm is probably the fastest, but 
is more approximate than the other two, since it does not properly consider the individual light attenuation 
for each voxel, i.e., the intensity and fading of the in-scattered light at each voxel. The approach 
by Billeter et al. avoids any ray marching and voxelization and instead computes the airlight integration 
using a technique very similar to shadow volumes. This one is the easiest to implement and does not require 
CUDA, which even makes it possible to use with WebGL. 7  Practical Considerations The goal of this part 
of the course is to provide an insight into techniques that are often employed in professional game titles. 
We will investigate what happens behind the scenes and which tech­niques are e.cient enough to be applied 
in practice. During the course, these elements will be re.ned, and we will provide statistics on the 
typical rendering budgets that are associated to the various aspects. In these course notes, we will 
concentrate on some techniques that are used to accelerate com­putations and ameliorate shadow quality. 
In particular, we will focus on upsampling techniques and additional e.ects, such as ambient occlusion, 
that allow us to reach a more convincing illu­mination simulation. 7.1 Deferred Shading and Upsampling 
The idea of deferred shading is to derive an image-based representation of the scene. In other words, 
one renders the scene once, while extracting a so-called G-bu.er, a collection of images where each pixel 
stores information about the underlying surface, such as normals, depth, mate­rial properties, etc. Only 
in a second pass, these images are used to evaluate the actual lighting computation, which has proven 
useful in practice for several games (cf. Stalker [Shishkovtsov, 2006]). The advantage is that hidden 
geometry will never see their incoming light evaluated, and, 30 7.1 Deferred Shading and Upsampling 
 furthermore, the computations become more e.cient. Basically, the calculations become more structured 
and map better to the hardware. One can write several attributes of a G-bu.er in a single render pass 
by using multiple render targets. This makes this process particularly e.cient. Despite the fact that 
a pixel only stores at­tributes for one surface location, simple transparency e.ects are possible by 
relying on dithering strategies, similar to stochastic transparency [Enderton et al., 2010]. The second 
possibility to accelerate computations is to distribute calculations spatially over adjacent pixels. 
The idea is to only partly evaluate the shadow. For instance, when employing percentage-closer .ltering 
one can evaluate a di.erent set of samples for each pixel (to de.ne the set of samples to evaluate, di.erent 
sets are de.ned for di.erent positions in the screen plane). Unfortunately, such a choice often leads 
to noisy results. To combat this artifact, one would like to .lter the image, but a standard process 
would lead to visible artifacts at geometric discontinuities and clearly visible halos. In order to avoid 
this problem, a specialized .ltering can be applied. A good strategy is cross-or joint-bilateral .ltering 
[Eisemann and Durand, 2004; Petschnigg et al., 2004]. The principle is to share irradiance values between 
pixels that are geometrically similar (share the same normal, are nearby in space); hereby, .ltering 
is not performed across edges. Basically, the result of the .ltering process for a pixel location t is 
then ti.K(t) .(t, ti)I(ti)I.ltered(t) = , ti.K(t) .(t, ti) where K(t) is a neighborhood around pixel 
position t and the weights . ensure that very di.erent pixels (compared to t) will not contribute to 
its .ltered value. To test this similarity, the weights are based on geometric resemblance evaluated 
via the G-bu.er. Mathematically, the weights can be de.ned as: .(t, ti) = G(sn, 1 - normal(t) · normal(ti)) 
G(sp, .position(t) - position(ti).), where G is a Gaussian kernel, position(·) and normal(·) are the 
G-bu.er values of the extracted position and surface normals. Hereby, only a fraction of the standard 
computation time is neces­sary. To further increase computational e.ciency, one can evaluate shadows 
also for a subset of the pixels in the full-resolution image. The result is then upsampled and spread 
to groups of pixels. In many cases, a good upsampling can be achieved by a slight adaptation of the joint/cross-bilateral 
.ltering, the so-called joint-bilateral upsampling [Kopf et al., 2007]. Here, each pixel s irradiance 
is de.ned as a weighted sum of irradiance values in a low resolution image. Again, geometric similarity 
between the high and low resolution pixels a.ects the weights. Finally, recent strategies making use 
of spatio-temporal upsampling [Herzog et al., 2010] and reprojection [Nehab et al., 2007; Sitthi-amorn 
et al., 2008; Herzog et al., 2010] start to become valuable assets to reuse shading over time. These 
solutions could even play a role for remote rendering con.gurations [Pajak et al., 2011]. 7 Practical 
Considerations 31   7.2 Ambient Occlusion To increase realism in the scene, it is also important to 
approximate complex e.ects, such as global illumination, where light bounces in the scene several times. 
Ambient occlusion approx­imates this e.ect by assuming a uniform white, hemispherical incident illumination. 
A sur­vey [Méndez-Feliu and Sbert, 2009] and recent work [Bunnell, 2006; Hoberock and Jia, 2007] give 
a good overview of various techniques. The most e.cient strategies approximate the incoming illumination 
based on the depth bu.er [Akenine-Möller et al., 2008]. Basically, the depth bu.er from the point of 
view is used as a proxy of the original geometry. To evaluate how much light is impinging at a certain 
point a small neighborhood of surrounding pixels is evaluated. There are various techniques to derive 
an estimate, such as [Bavoil et al., 2008; Szirmay-Kalos et al., 2010; Oat and Sander, 2007; Huang et 
al., 2011]. Directional lighting e.ects become possible via directional occlusion [Ritschel et al., 2009], 
bent normals [Landis, 2002], or bent cones [Klehm et al., 2011, 2012]. The latter are de.ned in screen 
space and lead to a high performance, and variants are employed in recent AAA ti­tles [Sousa et al., 
2011]. 8  Conclusion Today, we have not yet found an ultimate algorithm to compute shadows. Nonetheless, 
for most situations, there are particular techniques that are most suitable. Furthermore, combining several 
methods can be a very good choice. For example, shadow-map repartition and reparameterization can be 
successfully combined to result in a better algorithm than both isolated strategies. Hence, an overview, 
such as in this course, can be very bene.cial as it allows you to choose among the best options. In the 
following, we will pinpoint a few good choices. 8.1 Hard Shadows Hard shadows do not exist in the real 
world. Nonetheless, distant light sources (e.g., the sun) can often be well approximated with such techniques. 
An e.cient accurate computation is possible by relying on shadow-volume solutions with specialized rasterization 
techniques [Sintorn et al., 2011]. In particular, omnidirectional light sources can be treated very easily. 
For a light frustum and depending on the triangle size, the irregular z-Bu.er can also be an option [Johnson 
et al., 2005; Aila and Laine, 2004; Sintorn et al., 2008a]. Approximate solutions can rely on z-partitioning 
approaches [Engel, 2006; Zhang et al., 2006], where the view frustum is decomposed into several distances. 
Such techniques are widely used and a good compromise between quality and cost. Furthermore, partitions 
can even be steered from the viewpoint [Giegl and Wimmer, 2007a]. On top of partitioning, reparameterization 
is possible, such as light perspective shadow maps [Wimmer et al., 2004]. These come basically for free 
and do not in.ict any performance penality. Hence, they are always a good choice for interactive applications. 
A smarter reconstruction of the shadow boundary, e.g., via silhouette shadow maps [Sen et al., 2003] 
is another interesting option. 8.2 Filtered Hard Shadows Reconstruction of the shadow signal also relates 
to .ltering approaches. Most techniques propose accelerations of percentage-closer .ltering [Reeves et 
al., 1987]. 34 8.3 Soft Shadows For low memory cost, variance shadow maps [Donnelly and Lauritzen, 2006] 
are a good choice. Nonetheless, light leaks may appear. Alternatives consuming more memory and re­sources 
exist [Lauritzen and McCool, 2008]. An interesting tradeo. that is exploitable in games are exponential 
shadow maps [Annen et al., 2008b; Salvi, 2008], whose memory consumption is acceptable, but some post 
treatment of artifacts might be necessary. Especially, the variant described in [Lauritzen and McCool, 
2008] is a good option. Filtered shadows are relatively cheap, but not physically based. Nonetheless, 
a designer can often tweak a scene in order to hide these shortcomings. 8.3 Soft Shadows Soft shadows 
approach physically-reasonable shadow behavior and simulate penumbrae. The most e.cient accurate solutions 
[Sintorn et al., 2008a] are too slow for games, but could be used for previsualization and be good additions 
to other o.ine processes [Laine et al., 2005b; Overbeck et al., 2007] . For games and interactive applications 
approximate solutions are most adequate. PCSS-orient­ed solutions [Fernando, 2005] are already often 
applied in practice. Occlusion textures [Eisemann and Décoret, 2008] can be a practical option for scenes 
of a smaller extent, especially if the light source is relatively large. 8.4 Further Topics In the future, 
other shadow-related topics will become important. Volumetric shadow e.ects result in god rays [Chen 
et al., 2011], but also semi-transparent objects, or even shadows cast from indirect sources, will play 
an increasingly important role. 8.5 Last Words . . . We hope you enjoyed this course, and we invite 
you to visit our webpage: http://www.realtimeshadows.com/. Here, we will make the course slides available. 
Further, we will provide more information on recent topics and future trends. We want to keep track of 
the developments in shadow algorithms and be a guiding light that casts a long-lasting shadow.  Acknowledgments 
 Many people helped with their input and support. Many thanks go to Louis Bavoil, Brandon Lloyd, Zhao 
Dong, Cyril Crassin, Tobias Ritschel, Thomas Annen, Robert Herzog, Erik Sintorn, Oliver Klehm, Emmanuel 
Turquin, Bert Buchholz, Aaron Lefohn, Andrew Lauritzen, Martin Eisemann, Hedlena Bezerra, Alice Peters, 
all of which also helped us with our book Real-time Shadows on which this course is based. We also thank 
our colleagues (Telecom ParisTech: Yves Grenier, Isabelle Bloch, Tamy Boubekeur, . . . ; TU Vienna: Daniel 
Scherzer, Werner Purgathofer, . . . ; Chalmers: Ola Olsson, Markus Billeter, . . . ). For the models 
we thank Marco Dabrovic, Martin Newell, the Stanford 3D Scanning Repository, INRIA, and Aim@shape. This 
work was partially funded by the Intel Visual Computing Institute (IVCI) at Saarland University. Bibliography 
 Timo Aila and Samuli Laine. Alias-free shadow maps. In Proceedings of Eurographics Sympo­sium on Rendering 
2004, pages 161 166, 2004. Tomas Akenine-Möller and Ulf Assarsson. Approximate soft shadows on arbitrary 
surfaces using penumbra wedges. In Proceedings of Eurographics Workshop on Rendering 2002, pages 297 
306, 2002. Tomas Akenine-Möller, Eric Haines, and Natty Ho.man. Real-Time Rendering. A K Peters, 3rd 
edition, 2008. Graham Aldridge and Eric Woods. Robust, geometry-independent shadow volumes. In GRAPHITE 
04: Proceedings of the 2nd international conference on Computer graphics and interactive techniques in 
Australasia and South East Asia, pages 250 253, 2004. Thomas Annen, Tom Mertens, Philippe Bekaert, Hans-Peter 
Seidel, and Jan Kautz. Convolution shadow maps. In Proceedings of Eurographics Symposium on Rendering 
2007, pages 51 60, 2007. Thomas Annen, Zhao Dong, Tom Mertens, Philippe Bekaert, Hans-Peter Seidel, and 
Jan Kautz. Real-time, all-frequency shadows in dynamic scenes. ACM Transactions on Graphics (Pro­ceedings 
of ACM SIGGRAPH 2008), 27(3):34:1 34:8, 2008a. Thomas Annen, Tom Mertens, Hans-Peter Seidel, Eddy Flerackers, 
and Jan Kautz. Exponential shadow maps. In Proceedings of Graphics Interface 2008, pages 155 161, 2008b. 
Jukka Arvo. Tiled shadow maps. In Proceedings of Computer Graphics International 2004, pages 240 246, 
2004. Ulf Assarsson and Tomas Akenine-Möller. A geometry-based soft shadow volume algorithm using graphics 
hardware. ACM Transactions on Graphics (Proceedings of ACM SIGGRAPH 2003), 22(3):511 520, 2003. Lionel 
Atty, Nicolas Holzschuch, Marc Lapierre, Jean-Marc Hasenfratz, Charles Hansen, and François X. Sillion. 
Soft shadow maps: E.cient sampling of light source visibility. Computer Graphics Forum, 25(4):725 741, 
2006. Bibliography Ilya Baran, Jiawen Chen, Jonathan Ragan-Kelley, Frédo Durand, and Jaakko Lehtinen. 
A hier­archical volumetric shadow algorithm for single scattering. ACM Transactions on Graphics, 29(6 
(Proceedings of ACM SIGGRAPH Asia 2010)):178:1 178:10, 2010. Louis Bavoil, Miguel Sainz, and Rouslan 
Dimitrov. Image-space horizon-based ambient occlu­sion. In ACM SIGGRAPH 2008 Talks, pages 22:1 22:1, 
2008. P. Bergeron. A general version of crow s shadow volumes. IEEE Computer Graphics and Applications, 
6(9):17 28, 1986. Markus Billeter, Erik Sintorn, and Ulf Assarson. Volumetric shadows using polygonal 
light volumes. In Proceedings of High Performance Graphics 2010, pages 39 45, 2010. William Bilodeau 
and Mike Songy. Real time shadows, 1999. Creativity 1999, Creative Labs Inc. Sponsored game developer 
conferences, Los Angeles, California, and Surrey, England. Venceslas Biri, Didier Arquès, and Sylvain 
Michelin. Real Time Rendering of Atmospheric Scattering and Volumetric Shadows. Journal of WSCG, 14(1):65 
72, 2006. Ji.rí Bittner, Oliver Mattausch, Ari Silvennoinen, and Michael Wimmer. Shadow caster culling 
for e.cient shadow mapping. In Proceedings of ACM SIGGRAPH Symposium on Interactive 3D Graphics and Games 
2011, pages 81 88, 2011. James F. Blinn. Light re.ection functions for simulation of clouds and dusty 
surfaces. Computer Graphics, 16(3 (Proceedings of ACM SIGGRAPH 82)):21 29, 1982. ISSN 0097-8930. Michael 
Bunnell. Dynamic ambient occlusion and indirect lighting. In Matt Pharr, editor, GPU Gems 2: Programming 
Techniques for High-Performance Graphics and General-Purpose Computation, pages 223 233, Reading, MA, 
USA, 2006. Addison-Wesley Professional. ISBN 0-321-33559-7. John Carmack. Z-fail shadow volumes. Internet 
Forum, 2000. Jiawen Chen, Ilya Baran, Frédo Durand, and Wojciech Jarosz. Real-time volumetric shadows 
using 1D min-max mipmaps. In Proceedings of ACM SIGGRAPH Symposium on Interactive 3D Graphics and Games 
2011, pages 39 46, 2011. Hamilton Y. Chong and Steven J. Gortler. A lixel for every pixel. In Proceedings 
of Eurographics Symposium on Rendering 2004, pages 167 172, 2004. Hamilton Yu-Ik Chong. Real-time perspective 
optimal shadow maps. Senior thesis, Harvard University, 2003. James H. Clark. Hierarchical geometric 
models for visible surface algorithms. Communications of the ACM, 19(10):547 554, 1976. Bibliography 
Franklin C. Crow. Shadow algorithms for computer graphics. Computer Graphics, 11(2 (Pro­ceedings of ACM 
SIGGRAPH 77)):242 248, 1977. Qinghua Dai, Baoguang Yang, and Jieqing Feng. Reconstructable geometry shadow 
maps. In ACM SIGGRAPH Symposium on Interactive 3D Graphics and Games 2008: Posters, page 4:1, 2008. Yoshinori 
Dobashi, Tsuyoshi Yamamoto, and Tomoyuki Nishita. Interactive rendering of at­mospheric scattering e.ects 
using graphics hardware. In Proceedings of Graphics Hardware 2002, pages 99 107, 2002. William Donnelly 
and Andrew Lauritzen. Variance shadow maps. In Proceedings of ACM SIGGRAPH Symposium on Interactive 3D 
Graphics and Games 2006, pages 161 165, 2006. Elmar Eisemann and Xavier Décoret. Fast scene voxelization 
and applications. In Proceedings of ACM SIGGRAPH Symposium on Interactive 3D Graphics and Games 2006, 
pages 71 78, 2006a. Elmar Eisemann and Xavier Décoret. Plausible image based soft shadows using occlusion 
tex­tures. In Proceedings of SIBGRAPI 2006, pages 155 162, 2006b. Elmar Eisemann and Xavier Décoret. 
Occlusion textures for plausible soft shadows. Computer Graphics Forum, 27(1):13 23, 2008. Elmar Eisemann 
and Frédo Durand. Flash photography enhancement via intrinsic relighting. ACM Transactions on Graphics, 
23(3 (Proceedings of ACM SIGGRAPH 2004)):673 678, 2004. URL http://artis.imag.fr/Publications/2004/ED04. 
Elmar Eisemann, Michael Schwarz, Ulf Assarsson, and Michael Wimmer. Real-Time Shadows. A K Peters/CRC 
Press, Boca Raton, FL, USA, 2011. ISBN 978-1-56881-438-4. Eric Enderton, Erik Sintorn, Peter Shirley, 
and David Luebke. Stochastic transparency. In Pro­ceedings of ACM SIGGRAPH Symposium on Interactive 3D 
Graphics and Games 2010, pages 157 164, 2010. Wolfgang Engel. Cascaded shadow maps. In Wolfgang Engel, 
editor, ShaderX5: Advanced Ren­dering Techniques, pages 197 206. Charles River Media, Hingham, MA, USA, 
2006. ISBN 978-1-58450-499-3. Thomas Engelhardt and Carsten Dachsbacher. Epipolar sampling for shadows 
and crepuscu­lar rays in participating media with single scattering. In Proceedings of ACM SIGGRAPH Symposium 
on Interactive 3D Graphics and Games 2010, pages 119 125, 2010. Randima Fernando. Percentage-closer soft 
shadows. In ACM SIGGRAPH 2005 Sketches and Applications, page 35, 2005. Bibliography Vincent Forest, 
Loïc Barthe, and Mathias Paulin. Accurate shadows by depth complexity sam­pling. Computer Graphics Forum 
(Proceedings of Eurographics 2008), 27(2):663 674, 2008. Pascal Gautron, Jean-Eudes Marvie, and Guillaume 
François. Volumetric shadow mapping. In ACM SIGGRAPH 2009 Talks, pages 49:1 49:1, 2009. Markus Giegl 
and Michael Wimmer. Fitted virtual shadow maps. In Proceedings of Graphics Interface 2007, pages 159 
168, 2007a. Markus Giegl and Michael Wimmer. Queried virtual shadow maps. In Proceedings of ACM SIGGRAPH 
Symposium on Interactive 3D Graphics and Games 2007, pages 65 72, 2007b. Gaël Guennebaud, Loïc Barthe, 
and Mathias Paulin. Real-time soft shadow mapping by back­projection. In Proceedings of Eurographics 
Symposium on Rendering 2006, pages 227 234, 2006. Tim Heidmann. Real shadows real time. IRIS Universe, 
18:28 31, 1991. Robert Herzog, Elmar Eisemann, Karol Myszkowski, and Hans-Peter Seidel. Spatio-temporal 
upsampling on the GPU. In Proceedings of ACM SIGGRAPH Symposium on Interactive 3D Graphics and Games 
2010, pages 91 98, 2010. Jared Hoberock and Yuntao Jia. High-quality ambient occlusion. In Hubert Nguyen, 
editor, GPU Gems 3, pages 257 274. Addison-Wesley Professional, Reading, MA, USA, 2007. ISBN 978­0-321-51526-1. 
J.-C. Hourcade and A. Nicolas. Algorithms for antialiased cast shadows. Computers &#38; Graphics, 9(3):259 
265, 1985. Jing Huang, Tamy Boubekeur, Tobias Ritschel, Matthias Hollaender, and Elmar Eisemann. Sep­arable 
approximation of ambient occlusion. In Eurographics 2011 Short Papers, 2011. Takashi Imagire, Henry Johan, 
Naoki Tamura, and Tomoyuki Nishita. Anti-aliased and real-time rendering of scenes with light scattering 
e.ects. The Visual Computer, 23(9):935 944, 2007. ISSN 0178-2789. Robert James. True volumetric shadows. 
In Je. Lander, editor, Graphics programming methods, pages 353 366. Charles River Media, Rockland, MA, 
2003. ISBN 1-58450-299-1. Gregory S. Johnson, Juhyun Lee, Christopher A. Burns, and William R. Mark. 
The irregular z-bu.er: Hardware acceleration for irregular data structures. ACM Transactions on Graphics, 
24(4):1462 1482, 2005. ISSN 0730-0301. Gregory S. Johnson, Warren A. Hunt, Allen Hux, William R. Mark, 
Christopher A. Burns, and Stephen Junkins. Soft irregular shadow mapping: Fast, high-quality, and robust 
soft shadows. In Proceedings of ACM SIGGRAPH Symposium on Interactive 3D Graphics and Games 2009, pages 
57 66, 2009. Bibliography James T. Kajiya. The rendering equation. Computer Graphics, 20(4 (Proceedings 
of ACM SIGGRAPH 86)):143 150, 1986. James T. Kajiya and Brian P Von Herzen. Ray tracing volume densities. 
Computer Graphics, 18 (3 (Proceedings of ACM SIGGRAPH 84)):165 174, 1984. ISSN 0097-8930. Byungmoon Kim, 
Kihwan Kim, and Greg Turk. A shadow volume algorithm for opaque and transparent non-manifold casters. 
Journal of Graphics Tools, 13(3):1 14, 2008. Oliver Klehm, Tobias Ritschel, Elmar Eisemann, and Hans-Peter 
Seidel. Bent normals and cones in screen-space. In Vision, Modeling and Visualization Workshop, 2011. 
Oliver Klehm, Tobias Ritschel, Elmar Eisemann, and Hans-Peter Seidel. Screen-space bent cones: A practical 
approach. In Wolfgang Engel, editor, GPU Pro3: Advanced Rendering Techniques, pages 191 207. A K Peters/CRC 
Press, Boca Raton, FL, USA, 2012. Johannes Kopf, Michael F. Cohen, Dani Lischinski, and Matt Uyttendaele. 
Joint bilateral up­sampling. ACM Transactions on Graphics, 26(3 (Proceedings of ACM SIGGRAPH 2007)): 
96:1 96:5, 2007. Samuli Laine, Timo Aila, Ulf Assarsson, Jaakko Lehtinen, and Tomas Akenine-Möller. Soft 
shadow volumes for ray tracing. ACM Transactions on Graphics (Proceedings of ACM SIG-GRAPH 2005), 24(3):1156 
1165, 2005a. Samuli Laine, Timo Aila, Ulf Assarsson, Jaakko Lehtinen, and Tomas Akenine-Möller. Soft 
shadow volumes for ray tracing. ACM Transactions on Graphics, 24(3 (Proceedings of ACM SIGGRAPH 2005)):1156 
1165, 2005b. Hayden Landis. Production-ready global illumination, 2002. In ACM SIGGRAPH 2002 Course Notes, 
RenderMan in Production. Andrew Lauritzen. Summed-area variance shadow maps. In Hubert Nguyen, editor, 
GPU Gems 3, pages 157 182. Addison-Wesley Professional, Reading, MA, USA, 2007. ISBN 978-0­321-51526-1. 
Andrew Lauritzen and Michael McCool. Layered variance shadow maps. In Proceedings of Graphics Interface 
2008, pages 139 146, 2008. Andrew Lauritzen, Marco Salvi, and Aaron Lefohn. Sample distribution shadow 
maps. In Pro­ceedings of ACM SIGGRAPH Symposium on Interactive 3D Graphics and Games 2011, pages 97 102, 
2011. Aaron Lefohn, Shubhabrata Sengupta, Joe Kniss, Robert Strzodka, and John D. Owens. Dy­namic adaptive 
shadow maps on graphics hardware. In ACM SIGGRAPH 2005 Sketches and Applications, page 13, 2005. Bibliography 
Aaron E. Lefohn, Shubhabrata Sengupta, and John D. Owens. Resolution matched shadow maps. ACM Transactions 
on Graphics, 26(4):20:1 20:17, 2007. D. Brandon Lloyd, Jeremy Wendt, Naga K. Govindaraju, and Dinesh 
Manocha. CC shadow volumes. In Proceedings of Eurographics Symposium on Rendering 2004, pages 197 205, 
2004. D. Brandon Lloyd, David Tuft, Sung-eui Yoon, and Dinesh Manocha. Warping and partitioning for low 
error shadow maps. In Proceedings of Eurographics Symposium on Rendering 2006, pages 215 226, 2006. Tobias 
Martin and Tiow-Seng Tan. Anti-aliasing and continuity with trapezoidal shadow maps. In Proceedings of 
Eurographics Symposium on Rendering 2004, pages 153 160, 2004. Nelson Max. Light di.usion through clouds 
and haze. Computer Vision, Graphics, and Image Processing, 33(3):280 292, 1986a. Nelson L. Max. Atmospheric 
illumination and shadows. Computer Graphics, 20(4 (Proceedings of ACM SIGGRAPH 86)):117 124, 1986b. ISSN 
0097-8930. Àlex Méndez-Feliu and Mateu Sbert. From obscurances to ambient occlusion: A survey. The Visual 
Computer, 25(2):181 196, 2009. Jason Mitchell. Light shafts: Rendering shadows in participating media, 
2004. Presentation, Game Developers Conference 2004. http://developer.amd.com/media/gpu_assets/Mitchell_LightShafts.pdf. 
Diego Nehab, Pedro V. Sander, Jason Lawrence, Natalya Tatarchuk, and John R. Isidoro. Ac­celerating real-time 
shading with reverse reprojection caching. In Proceedings of Graphics Hardware 2007, pages 25 35, 2007. 
Tomoyuki Nishita, Yasuhiro Miyawaki, and Eihachiro Nakamae. A shading model for atmo­spheric scattering 
considering luminous intensity distribution of light sources. Computer Graphics, 21(4 (Proceedings of 
ACM SIGGRAPH 87)):303 310, 1987. ISSN 0097-8930. Christopher Oat and Pedro V. Sander. Ambient aperture 
lighting. In Proceedings of ACM SIG-GRAPH Symposium on Interactive 3D Graphics and Games 2007, pages 
61 64, 2007. Ryan Overbeck, Ravi Ramamoorthi, and William R. Mark. A real-time beam tracer with appli­cation 
to exact soft shadows. In Proceedings of Eurographics Symposium on Rendering 2007, pages 85 98, 2007. 
David Pajak, Robert Herzog, Elmar Eisemann, Karol Myszkowski, and Hans-Peter Seidel. Scal­able remote 
rendering with depth and motion-.ow augmented streaming. Computer Graphics Forum, 30(2 (Proceedings of 
Eurographics 2011)):415 424, 2011. Bibliography Minghao Pan, Rui Wang, Weifeng Chen, Kun Zhou, and Hujun 
Bao. Fast, sub-pixel antialiased shadow maps. Computer Graphics Forum, 28(7 (Proceedings of Paci.c Graphics 
2009)): 1927 1934, 2009. Vincent Pegoraro, Mathias Schott, and Steven G. Parker. An analytical approach 
to single scatter­ing for anisotropic media and light distributions. In Proceedings of Graphics Interface 
2009, pages 71 77, 2009. Vincent Pegoraro, Mathias Schott, and Steven G. Parker. A closed-form solution 
to single scat­tering for general phase functions and light distributions. Computer Graphics Forum, 29(4 
(Proceedings of Eurographics Symposium on Rendering 2010)):1365 1374, 2010. Georg Petschnigg, Richard 
Szeliski, Maneesh Agrawala, Michael Cohen, Hugues Hoppe, and Kentaro Toyama. Digital photography with 
.ash and no-.ash image pairs. ACM Transactions on Graphics, 23(3 (Proceedings of ACM SIGGRAPH 2004)):664 
672, 2004. William T. Reeves, David H. Salesin, and Robert L. Cook. Rendering antialiased shadows with 
depth maps. Computer Graphics, 21(4 (Proceedings of ACM SIGGRAPH 87)):283 291, 1987. Tobias Ritschel, 
Thorsten Grosch, and Hans-Peter Seidel. Approximating dynamic global illu­mination in image space. In 
Proceedings of ACM SIGGRAPH Symposium on Interactive 3D Graphics and Games 2009, pages 75 82, 2009. Paul 
Rosen. Rectilinear texture warping for fast adaptive shadow mapping. In Proceedings of ACM SIGGRAPH Symposium 
on Interactive 3D Graphics and Games 2012, pages 151 158, 2012. Marco Salvi. Rendering .ltered shadows 
with exponential shadow maps. In Wolfgang En­gel, editor, ShaderX6: Advanced Rendering Techniques, pages 
257 274. Charles River Media, Hingham, MA, USA, 2008. ISBN 978-1-58450-544-0. Daniel Scherzer, Stefan 
Jeschke, and Michael Wimmer. Pixel-correct shadow maps with tempo­ral reprojection and shadow test con.dence. 
In Proceedings of Eurographics Symposium on Rendering 2007, pages 45 50, 2007. Michael Schwarz and Marc 
Stamminger. Bitmask soft shadows. Computer Graphics Forum (Proceedings of Eurographics 2007), 26(3):515 
524, 2007. Michael Schwärzler, Christian Luksch, Daniel Scherzer, and Michael Wimmer. Fast percentage 
closer soft shadows using temporal coherence. In Proceedings of ACM SIGGRAPH Sym­posium on Interactive 
3D Graphics and Games 2012, I3D 13, pages 79 86, New York, NY, USA, 2013. ACM. ISBN 978-1-4503-1956-0. 
doi: 10.1145/2448196.2448209. URL http://doi.acm.org/10.1145/2448196.2448209. Bibliography Pradeep Sen, 
Mike Cammarano, and Pat Hanrahan. Shadow silhouette maps. ACM Transactions on Graphics, 22(3 (Proceedings 
of ACM SIGGRAPH 2003)):521 526, 2003. Li Shen, Gaël Guennebaud, Baoguang Yang, and Jieqing Feng. Predicted 
virtual soft shadow maps with high quality .ltering. Computer Graphics Forum, 30(2 (Proceedings of Eurograph­ics 
2011)):493 502, 2011. Oles Shishkovtsov. Deferred shading in S.T.A.L.K.E.R. In Matt Pharr, editor, GPU 
Gems 2: Programming Techniques for High-Performance Graphics and General-Purpose Computa­tion, pages 
143 166. Addison-Wesley Professional, Reading, MA, USA, 2006. ISBN 0-321­33559-7. Erik Sintorn, Elmar 
Eisemann, and Ulf Assarsson. Sample based visibility for soft shadows using alias-free shadow maps. Computer 
Graphics Forum, 27(4 (Proceedings of Eurographics Symposium on Rendering 2008)):1285 1292, 2008a. Erik 
Sintorn, Elmar Eisemann, and Ulf Assarsson. Sample based visibility for soft shadows using alias-free 
shadow maps. Computer Graphics Forum (Proceedings of Eurographics Symposium on Rendering 2008), 27(4):1285 
1292, 2008b. Erik Sintorn, Ola Olsson, and Ulf Assarsson. An e.cient alias-free shadow algorithm for 
opaque and transparent objects using per-triangle shadow volumes. ACM Transactions on Graphics, 30(6 
(Proceedings of ACM SIGGRAPH Asia 2011)):153:1 153:10, 2011. Pitchaya Sitthi-amorn, Jason Lawrence, Lei 
Yang, Pedro V. Sander, Diego Nehab, and Jiahe Xi. Automated reprojection-based pixel shader optimization. 
ACM Transactions on Graphics, 27 (5 (Proceedings of ACM SIGGRAPH Asia 2008)):127:1 127:11, 2008. Cyril 
Soler and François X. Sillion. Fast calculation of soft shadow textures using convolution. In Proceedings 
of ACM SIGGRAPH 98, pages 321 332, 1998. Tiago Sousa, Nickolay Kasyan, and Nicolas Schulz. Secrets of 
CryENGINE 3 graphics tech­nology, 2011. In ACM SIGGRAPH 2011 Courses, Advances in Real-Time Rendering 
in 3D Graphics and Games. Marc Stamminger and George Drettakis. Perspective shadow maps. ACM Transactions 
on Graphics, 21(3 (Proceedings of ACM SIGGRAPH 2002)):557 562, 2002. Martin Stich, Carsten Wächter, and 
Alexander Keller. E.cient and robust shadow volumes using hierarchical occlusion culling and geometry 
shaders. In Hubert Nguyen, editor, GPU Gems 3, pages 239 256. Addison-Wesley Professional, Reading, MA, 
USA, 2007. ISBN 978-0-321­51526-1. Bo Sun, Ravi Ramamoorthi, Srinivasa G. Narasimhan, and Shree K. Nayar. 
A practical ana­lytic single scattering model for real time rendering. ACM Transactions on Graphics, 
24(3 (Proceedings of ACM SIGGRAPH 2005)):1040 1049, 2005. ISSN 0730-0301. Bibliography László Szirmay-Kalos, 
Tamás Umenho.er, Balázs Tóth, László Szécsi, and Mateu Sbert. Vol­umetric ambient occlusion for real-time 
rendering and games. IEEE Computer Graphics and Applications, 30:70 79, 2010. ISSN 0272-1716. URL http://dx.doi.org/10.1109/MCG. 
2010.19. Katsumi Tadamura, Xueying Qin, Guofang Jiao, and Eihachiro Nakamae. Rendering optimal solar 
shadows using plural sunlight depth bu.ers. In Proceedings of Computer Graphics International 1999, pages 
166 173, 1999. Katsumi Tadamura, Xueying Qin, Guofang Jiao, and Eihachiro Nakamae. Rendering optimal 
solar shadows with plural sunlight depth bu.ers. The Visual Computer, 17(2):76 90, 2001. Balàzs Tóth 
and Tamás Umenho.er. Real-time volumetric lighting in participating media. In Eurographics 2009 Short 
Papers, 2009. Yulan Wang and Steven Molnar. Second-depth shadow mapping. Technical Report TR 94-019, 
University of North Carolina at Chapel Hill, 1994. D. Weiskopf and T. Ertl. Shadow Mapping Based on Dual 
Depth Layers. In Eurographics 2003 Short Papers, pages 53 60, 2003. Lance Williams. Casting curved shadows 
on curved surfaces. Computer Graphics, 12(3 (Pro­ceedings of ACM SIGGRAPH 78)):270 274, 1978. Michael 
Wimmer, Daniel Scherzer, and Werner Purgathofer. Light space perspective shadow maps. In Proceedings 
of Eurographics Symposium on Rendering 2004, pages 143 152, 2004. Andrew Woo. The shadow depth map revisited. 
In David Kirk, editor, Graphics Gems III, pages 338 342. Academic Press, Boston, MA, 1992. ISBN 0-12-409673-5. 
Chris Wyman. Interactive voxelized epipolar shadow volumes. In ACM SIGGRAPH ASIA 2010 Sketches, pages 
53:1 53:2, 2010. Chris Wyman and Shaun Ramsey. Interactive volumetric shadows in participating media 
with single scattering. In Proceedings of the IEEE Symposium on Interactive Ray Tracing 2008, pages 87 
92, 2008. Baoguang Yang, Jieqing Feng, Gaël Guennebaud, and Xinguo Liu. Packet-based hierarchal soft 
shadow mapping. Computer Graphics Forum, 28(4 (Proceedings of Eurographics Symposium on Rendering 2009)):1121 
1130, 2009. Baoguang Yang, Zhao Dong, Jieqing Feng, Hans-Peter Seidel, and Jan Kautz. Variance soft shadow 
mapping. Computer Graphics Forum (Proceedings of Paci.c Graphics 2010), 29(7): 2127 2134, 2010. Bibliography 
Fan Zhang, Hanqiu Sun, Leilei Xu, and Lee Kit Lun. Parallel-split shadow maps for large-scale virtual 
environments. In Proceedings of ACM International Conference on Virtual Reality Continuum and Its Applications 
2006, pages 311 318, 2006. Hansong Zhang. Forward shadow mapping. In Proceedings of Eurographics Workshop 
on Ren­dering 1998, pages 131 138, 1998. 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2013</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>2504454</article_id>
		<sort_key>190</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>19</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>19</seq_no>
		<title><![CDATA[OpenVDB]]></title>
		<subtitle><![CDATA[an open-source data structure and toolkit for high-resolution volumes]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2504435.2504454</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2504454</url>
		<abstract>
			<par><![CDATA[<p>OpenVDB has already been integrated into the next major release of the high-end 3D animation package Houdini, and there is anecdotal evidence that many of the major VFX and production houses are in the process of either evaluating or adopting VDB. This course presents a comprehensive overview of OpenVDB, an open-source C++ library comprising a novel hierarchical data structure and a suite of tools for efficient storage and manipulation of sparse volumetric data discretized on three-dimensional grids..</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4193530</person_id>
				<author_profile_id><![CDATA[81424592239]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Ken]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Museth]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[DreamWorks Animation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4193531</person_id>
				<author_profile_id><![CDATA[81488670649]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Jeff]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lait]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Side Effects Software Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4193532</person_id>
				<author_profile_id><![CDATA[82458789057]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[John]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Johanson]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Digital Domain 3.0, Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4193533</person_id>
				<author_profile_id><![CDATA[81421595413]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Jeff]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Budsberg]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[DreamWorks Animation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4193534</person_id>
				<author_profile_id><![CDATA[82459284257]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Ron]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Henderson]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[DreamWorks Animation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4193535</person_id>
				<author_profile_id><![CDATA[82459061057]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Mihai]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Alden]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[DreamWorks Animation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4193536</person_id>
				<author_profile_id><![CDATA[82458937757]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>7</seq_no>
				<first_name><![CDATA[Peter]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Cucka]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[DreamWorks Animation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4193537</person_id>
				<author_profile_id><![CDATA[82459290357]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>8</seq_no>
				<first_name><![CDATA[David]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hill]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[DreamWorks Animation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4193538</person_id>
				<author_profile_id><![CDATA[82458639057]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>9</seq_no>
				<first_name><![CDATA[Andrew]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Pearce]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[DreamWorks Animation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2013</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>2504455</article_id>
		<sort_key>200</sort_key>
		<display_label>Article No.</display_label>
		<pages>55</pages>
		<display_no>20</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>20</seq_no>
		<title><![CDATA[A practical guide to art/science collaborations]]></title>
		<page_from>1</page_from>
		<page_to>55</page_to>
		<doi_number>10.1145/2504435.2504455</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2504455</url>
		<abstract>
			<par><![CDATA[<p>This practical guide to art/science (A/S) collaborations advises artists and scientists on how to foster, prepare for, and improve interdisciplinary work. The current state of the art is presented by Dan Sandin, a computer graphics pioneer who has been working across disciiplinary boundaries for 35 years. Then the course introduces the instructors' collaborative research study on the important elements of successful A/S collaboration. It concludes with three case studies of collaborative A/S initiatives and critical practical advice.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4193539</person_id>
				<author_profile_id><![CDATA[81335498954]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Daria]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tsoupikova]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Illinois at Chicago]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4193540</person_id>
				<author_profile_id><![CDATA[81442619552]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Helen-Nicole]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kostis]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Illinois at Chicago]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4193541</person_id>
				<author_profile_id><![CDATA[81100455127]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Dan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sandin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Illinois at Chicago]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Alexa Wright, Alf Linney, The Art and Science of a Long-term Collaboration, New Constellations Conference, Museum of Contemporary Art, Sydney, 17 March 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>581713</ref_obj_id>
				<ref_obj_pid>581710</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Brent MacGregor, Cybernetic serendipity revisited, Proceeding C&C '02 Proceedings of the 4th conference on Creativity & cognition ACM New York, NY, 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Chris Comer, SymBIOtic ART & Science: An Investigation at the Intersection of Life Sciences and the Arts, Final Report for the NSF Art and Science Workshop, NSF and the NEA, 2011, Arlington, VA, published on Jan 7th, 2012.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Dan Sandin http://www.evl.uic.edu/dan/]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[DigiArts, UNESCO Knowledge Portal, a reference website on art, science and technology. http://portal.unesco.org/culture/en/ev.php- URL_ID=1391&URL_DO=DO_TOPIC&URL_SECTION=201.html]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Edward A. Shanken, Artists in Industry and the Academy: Collaborative Research, Interdisciplinary Scholarship and the Creation and Interpretation of Hybrid Forms, Leonardo 38:5, 2005, p.415-18.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Interdisciplinary Product Development (IPD), UIC http://www.ipd.uic.edu/IPD/]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Michael J. Moravcsik, Scientists and Artists: Motivations, Aspirations, Approaches and Accomplishments, Leonardo, Vol. 7, pp. 255-259, Pergamon Press, Britain, 1974.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[NASA Visualization Explorer for the iPad - Credits. NASA SVS, n. d. Web. 12 February 2013, http://svs.gsfc.nasa.gov/nasaviz/credits.html]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Roger Malina, Art Science Radar http://malina.diatrope.com/category/art-science-radar-2/]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Synapse¿ supported by Australian Network for Art & Technology (ANAT) http://www.synapse.net.au/index.php]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[The STEM to STEAM Briefing, Rhode Island School of Design, September 2011, http://stemtosteam.org/]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[The network for Sciences, Engineering, Arts and Design (SEAD) Working Group on White Papers issued by the NSF NSEAD workshop at MICA, chaired by Roger F Malina, ATEC, UT Dallas and co chaired by Carol Strohecker, Center for Design Innovation, University of North Carolina system and an international steering committee. http://seadnetwork.wordpress.com/about/]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[The UCLA Art|Sci Center, dedicated to pursuing and promoting the evolving "Third Culture" by facilitating the infinite potential of collaborations between (media) arts and (bio/nano) sciences. http://artsci.ucla.edu/?q=about]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
      PracticalGuidetoArt/ScienceCollaborations  SIGGRAPH2013CourseNotes Short  course(1.hours) 
     Authors DariaTsoupikova,SchoolofArtandDesign,UniversityofIllinoisatChicago,  datsoupi@evl.uic.edu 
Helen-­-NicoleKostis,  GESTAR/USRA,NASAGoddardSpaceFlightCenter,helen-­-nicole.kostis@nasa.gov ManoramaKhare,CenterforResearchinWomenandGender,UniversityofIllinoisatChicago, 
 mkhare1@uic.edu AdvisoryBoard: DanielSandin,ElectronicVisualizationLaboratory(EVL),UniversityofIllinoisatChicagoandCaliforniaInstitute 
ForTelecommunicationsAndInformationTechnology(CalIT2),UniversityofCalifornia,San  Diego,  dan@uic.edu 
Theresa-­-MarieRhyne,VisualizationConsultant,  theresamarierhyne@gmail.com   Instructors  DanielSandin,ElectronicVisualizationLaboratory(EVL),UniversityofIllinoisatChicagoandCalifornia 
 Institute ForTelecommunicationsAndInformationTechnology(CalIT2),UniversityofCalifornia,San  Diego,  dan@uic.edu 
DariaTsoupikova,SchoolofArtand  Design,UniversityofIllinoisatChicago,  datsoupi@evl.uic.edu  Summary 
 Thiscourseexaminesthe  role  ofArt/Science  collaborations,  presents  historical   examplesoArt/Science 
 projects,thmotivationsandimpactofsuccollaborationsIt servesasapracticalguidebyidentifyingandanalyzingtheimportant 
 elements  of successful  cases,based  onresearch  andpersonal  experiences  fromprofessionals  in relevant 
 fields.    Image:  Heatmap datavisualizationofArt/ScienceCollaborationResearch Survey Question 16: 
From Your perspective, how important iseach of the following outcomes of anA/S  project for you?  Please 
base your answerothe most recentcollaborativeA/S project you  participated  in. The heat map represents 
the percentage of  respondents that ranked  theimportance ofeight (8)outcomesofA/S  collaborations.Eachoutcome(Patents,Publications,etc.)is 
 representedby  agroupofthreerows.Withineachoutcome,thefirstrow(blue)correspondstoanswerssuppliedbyScientists,the 
 second(pink)  tothosesupplied byArtists,and the third(purple)  bythosewho self-­-identifiedas"BothaScientistandanArtist".Columnsfrom 
lefttorightcorrespondtothe  degree  ofimportance  ofeach oftheeight  outcomes,from"Not  Important"to"VeryImportant".Thepercentageofresponses 
 foreachdegree  ofimportance  issignifiedbythe  lightnessofthe  cells:highpercentagescorrespondtodarkercolors. 
 IntendedAudience EveryoneinterestedinArt/Sciencecollaborations.Artists,Scientists,Educators,industryspecialists,project 
 managers,students,andanyonewhoisinterestedorinvolvedininterdisciplinarycollaborativeteamsbetweenartistsandscientists. 
 Prerequisites None Level Introductory BioForms DanSandin ElectronicVisualization  Laboratory(EVL), 
UniversityofIllinoisatChicagoandCaliforniaInstitute ForTelecommunicationsAndInformationTechnology(CalIT2), 
UniversityofCalifornia,SanDiego dan@uic.edu  DanSandinisavideoandcomputergraphicsartist/researcher. 
 He  isaProfessorEmeritusofthe  SchoolofArt&#38;Design,University  ofIllinoisatChicago.Heisaco-­-founder 
 oftheElectronicVisualizationLaboratory(EVL)atthe  Universityof  Illinoisat  Chicago,thefirst  formalA/Scollaborativeresearchprogram 
in the UnitedStates.He isaninternationallyrecognizedpioneerincomputergraphics,electronic  artandvisualization 
andhisworkembodiesaninterdisciplinaryfusion.Sandin,whocametovideo andcomputersinthe1970swithabackground 
inphysics,wasinstrumental  inthedevelopmentofimagingdevicesthatcouldbemadeaccessibletoartistsfortheir 
 ownduplicationanduse.HeistheinventoroftheSandinImageProcessor,apatch  programmableanalogcomputerforreal-­-timemanipulation 
of  videoinputsthroughthecontrolof  the grey levelinformation.Dan Sandinisafather  of  theCAVE,theCAVEAutomaticVirtualEnvironment,whichhedevelopedincollaborationwithTomDeFantiandartandsciencegraduatestudents 
 in 1991.Sandin'scomputer/videoart  hasbeenexhibitedat  conferencesandmuseumsworldwide,andhehasreceivedmanyawards.Sandinhasreceivedmanygrantsandfellowshipsfromsuchdistinguished 
organizationsastheRockefeller  Foundation,theGuggenheim Foundation,andthe  NationalEndowmentfortheArts;hisworkisincluded 
 in  theinauguralcollectionofvideoartattheMuseum ofModernArtinNewYork.  DariaTsoupikova  SchoolofArtand 
Design UniversityofIllinoisatChicago datsoupi@evl.uic.edu DariaTsoupikovaisan  AssociateProfessor  intheSchoolof 
 Art  andDesignand the ElectronicVisualization Laboratory(EVL)attheUniversityofIllinoisatChicago.Her 
researchandartworkincludesdevelopmentofvirtualreality(VR)artprojectsand   networked  multi-­-userexhibitionsfor 
 VRprojectionsystems,suchastheCave AutomaticVirtualEnvironmenttheatre(CAVE),aswellasthedesign  ofinteractive 
educationalmultimedia  forchildren.Hervirtualrealityresearch,publicationsand artworkexplore  the  relationshipbetweenthe 
aestheticsofvirtualenvironments, traditionalarts,and theeffect  of  virtualrealityaestheticsonthe  user 
sperceptionsand emotions.Herworkliesatthecrossroadsofartisticandtechnologicalinnovation,and exploresthe 
 potentialofnew media  and  interactivityin  relation  to  traditionalarts.Her currentworks  areapplicationsofcomputergraphics 
 arttovarious  researchdomains suchas  educationalmultimedia,  culturalheritageandvirtualrehabilitation 
forstroke   survivors.Herworkwasexhibitedand published  atSIGGRAPH,IEEE  VRandISEAand manyothervenues. 
    Course  Description  ThiscourseservesasapracticalguidetoArt/Sciencecollaborations.Thepurposeofthisintroductorycourse 
 is  threefoldandintendedtoprovide  artistsandscientistswithadviceonhowto:1)preparethemforsuchcollaborations,2)improveexistingcollaborations,and3)fostersuchinitiatives.Thenotesareaimedatanyonewhoisinterestedinsuchcollaborations,includingartists,scientists,educators,students, 
 project  managersandindustryspecialists,irrespectiveofpriorexperience.  ThecoursewillalsoincludeabriefhistoricaloverviewofA/Scollaborationsuptothecurrent 
 stateoftheart,presentedbycomputergraphicspioneerDanSandin,whocarriesa35-­-yearexperienceofparticipationinsuchprojects.DanSandinwillaugment 
 thepresentationbysharinghiexperiences  asanartist,scientist  andeducator. Thiscourseisstructuredasashortcourse(1.5hours).Inthesecondsectionofthecourse 
 wewillintroducethecollaborativeresearchstudyinitiatedbytheauthorsofthiscourseandpresent  itsresultsregardingtheimportant 
 elementsofasuccessfulA/Scollaboration.Wewillsharepersonaladvicefromprofessionalswithsignificant  experienceinA/Scollaborativeresearchaswellasprovidereferencestotheirexperiences.Finally,wewillpresentthreecasestudiesofA/Scollaborativeinitiativesandsharecriticaadvicefrotheproject 
 authors.  CourseSchedule 1Introduction.............................................................................................................3 
 (3minutes)  Instructor: Dan Sandin Overview ofthe course goals and content. 1.1 Motivation andpurpose 
ofthecourse ..........................................................................3 1.2 Introduction 
of A/S researchteam ................................................................................3 
 1.3  Overview ofthe course content ....................................................................................4 
 1.4  Acknowledgements .......................................................................................................4 
 2A/SCollaborationBackground&#38;Experiences ...........................................................6(27 
minutes)  Instructor: Dan  Sandin A/S collaboration: History, impact,and  current state ofthe art. 2.1 
 What is A/S collaboration? ............................................................................................6 
 2.2 Contemporary history ofA/S collaboration ...................................................................6 
 2.2.1 Examples of Art/Science  collaborations 7 ........................................................................ 
 2.3 Motivations................................................................................................................11 
 2.3.1  Why scientists need artists? ...........................................................................................11 
 2.3.2  What s in it for the  artists? .............................................................................................12 
 3Research-­-DrivenDevelopment:PracticalGuideforSuccessfulA/SCollaboration .....................................................................................................................................14 
 (35 minutes) Instructor: Daria Tsoupikova Practicaladvice  for successful A/S  collaborations, based 
on results of collaborative  research study and personal advice from the authors of  this course and 
professionals with significant experience in A/S. 3.1 A/S researchsurvey....................................................................................................14 
 3.1.1  Survey design .............................................................................................................14 
 3.1.2  Survey results .............................................................................................................16 
 3.2 Practicalsuggestionsfor A/S collaboration ................................................................. 
19 3.2.1 Prepare  for collaboration: ...........................................................................................19 
 3.2.2 Secure  funding: ...........................................................................................................21 
 3.2.3 Define and learn common language: ..........................................................................23 
 3.2.4 Heighten mutual respect and trust: ............................................................................24 
 3.2.5 Adhere to  timeline:.....................................................................................................24 
 3.2.6 Don t forgetto acknowledgeyour colleagues: ...........................................................24 
 3.2.7 Appoint proper managementand  leadership:24 ............................................................ 
 3.2.8 Learn the terminology: 25 3.2.9 Understand thetechnology:.......................................................................................25 
 3.2.10 Sharpen your skills: ...................................................................................................26 
 3.2.11 Clarify the outcomes:................................................................................................27 
 3.2.12 Discovernew opportunities: .....................................................................................31 
 3.3 Future directions33 ......................................................................................................... 
 4Case  Studies...........................................................................................................34 
 (24 minutes)Instructors:Dan Sandin &#38;Daria Tsoupikova Presentation of three  case  studiesthat illustrate 
 how artists and scientists work together in differenttypes ofcollaborative projects. 4.1  Julia  Sets 
 collaborative projects  by Dan Sandin ...............................................................34 
 4.2 Interdisciplinary ProductDevelopment(IPD):collaborative course inacademia39 .......... 4.3 Industryexample:NASA 
Visualization Explorer...........................................................43 5References,Links,Bibliography..............................................................................47(1minutes) 
Instructor:  Daria Tsoupikova  1Introduction 1.1Motivationanpurposeothecourse Art/Science(A/S)collaborationshavepropelledcultural,historicalandtechnologicalevolutionsinoursocietyandhavegainedmoreimportanceasresearchprogramsinindustryandacademia 
 recognizethesignificanceofmultidisciplinaryapproaches,lateralthinkingandcreativeengagement  intheprocessofinnovation.However,despitethisgrowthanddevelopment,themethodologyforA/Scollaborationhasyet 
 tobecomeasubject  forresearchandimplementationinmainstreamculturalinstitutionsandeducationalprograms.Whilemanyeducationalinstitutionsrecognizetheimportanceofdialoguebetweenartistsandscientists,stillonlyhandfulofcontemporaryinstitutionalprogramshavedevelopedcurricula 
 that  engagestudentsinsuchprojectsorteachstrategiesonhowtoachievecollaboration.Therehasbeensignificant 
 controversyabout  whetherartistsandscientistshavedifferent  cultures,motivations,aspirations,approachesandmeasuresofaccomplishment.So,thekeyquestionshavebecome: 
 Howcanwepreparebothsidesandeducatethem  about  eachother sviews?  HowcanwefosterA/Scollaborations? 
 Howcanweimprovesuchcollaborations?   1.2IntroductionofA/Sresearchteam InOctober2011,withtheabovequestionsinmind,theArt/ScienceCollaborationResearchProject 
 wasinitiatedaimingtoprovidepracticaladvicetoartistsandscientistsandpreparethemforA/Scollaborations.Wearesmallgroupofresearchersconsistingofscientists,artistswhoalsoholdscientificbackgrounds,andprofessionalevaluator.OurexperienceinthefieldisderivedfromourparticipationinA/Scollaborationsforoverdecadefortwoteammembers(Daria 
 Tsoupikova  andHelen-­-NicoleKostis),25yearsofexperienceforourAdvisoryBoardmemberMarie-­-Theresa  Rhyneand35yearsofexperienceforDanSandin. 
 Themembersofthiscollaborativeteamhaveworkedinthefieldsofvirtualandaugmentedreality,scientificvisualization,culturalheritage,webandmobile/appdevelopmentandcomputergames.Theyhavebeeninvolvedinwidevarietyofprojects:largeandsmall,withmanyorfewteammembers,lengthyorshort 
 induration,withorwithout  funding,successfulornot.Withtime,theystartedidentifyingpatternsandelementsthat 
 makethesecollaborationseasier.Forthiscourse,theyextractedthosepatterns,convertedthemtohypothesesandvalidatedthembyanalyzingsurveydesignedforscientists,artists,andparticipantswithbothbackgroundswhohaveexperience 
 inA/Sprojects. 1.3  Overviewofthecoursecontent ThefirstpartpresentsbrieflythecontemporaryhistoryofA/Scollaboration, 
 its  impact  andstateoftheart  today.Sections2.1an2.describeA/Scollaborationswithillustrativeexamplesdescribedinsection2.2.1. 
 Section2.3describesmotivationsandreasonsforartistsandscientiststoparticipateintheA/Scollaborativeinitiatives.Ialsocoverexpectations,contributionsandoutcomescharacteristicofsuchprojects. 
 Thesecondpartprovides  information  aboutthe recent research survey  on A/S collaboration developed 
and ranby  the  authors  ofthiscourse.  Section3.2identifiestheimportant  elementsofsuccessfulA/Scollaborationsupportedbydata 
 fromthesurveyinthefollowingareas;planning,goals,communicationmethodologies,funding,management,technology,skills,andopportunitiesforA/Scollaborations.Inthissectionofthecoursepersonaladvicefromprofessionalswithsignificant 
 experienceinA/Scollaborativeresearchisshared,aswellasreferencestotheirexperiences.Thelastpartothecourseprovidesthedetails(goalsandlessonslearned)ofthreecasestudiesthat 
 illustratehowscientistsandartistsworktogetherinresearch,academiaandindustry.   1.4Acknowledgements 
Wewouldliketoespeciallythankalltheanonymousscientistsandartiststhat  participatedenthusiasticallyinthesurveyandfortheirthoughtfulfeedback. 
SpecialthankstoTheresa-­-MarieRhyneandDanSandin,theadvisoryboardofthisresearchteamfortheirguidanceandsupport.Thanks 
 toMaxine  Brown forall thehelpfuladvicefrom  thebeginningoftheproject.  SpecialthankstoStephenMelamedandMichaelJ.Scott,theleadersoftheInterdisciplinaryProduct 
 Development  ProgramaUICInnovationCenter.ThisresearchispartiallysupportedbytheDean sResearchAwardoftheCollegeof 
ArchitectureandtheArts,UniversityofIllinoisaChicago.ThisresearchhasbeenapprovedbyUniversityofIllinoisIRB(protocolnumber:2012-­-0200) 
 2A/SCollaborationBackgrounExperiences 2.1WhatisA/Scollaboration? Art/Sciencecollaborationisthecollaborationbetweenartistsandscientists 
 that  generatesprojects,processes,experiences,knowledge,ideas,andorganizationsbeneficialtobothfields.Resultsofart/science/technologycollaborationsdirectlycanaffect 
 educationalandpedagogicalpracticeandsocietyalarge.  Themovement  tointegratescienceandart  toexamineconvergencepoints,andhowthefieldscanempowereachother,hasalreadybegun.Inthelast 
 fewyears,therehaveemergednumberofmeetingsthat  studyart  andscienceconvergencessupportedbyeffortsoftheNationalScienceFoundation(NSF),theNationalEndowment 
 oftheArts(NEA),theAssociationforComputingMachinery(ACM),theConferenceonHumanFactorsinComputingSystems(HCI),aswellasotherinstitutions(Comer2011;STEAM 
 2011).Despitetheseefforts,theintegrationofartsandscienceinindustry,researchandeducation,therolesofartistsandscientistsandtheircontributionstothesuccessorfailureoncollaborationhaveyet 
 tobecomesubjectsforresearchandimplementationinmainstreamculturalinstitutionsandeducationalprograms. 
 2.2ContemporaryhistoryofA/S  collaboration "CyberneticSerendipity"(London,1968)wasthefirst  formalexhibitiontoexplore 
therelationshipsbetweentechnologyandcreativity (MacGregor2002).ThislegendarymilestoneinthehistoryofA/Scollaborationincludedrobotics,algorithms,devicesforgeneratingmusic,kineticenvironments,movingimages,installations,proceduralanimation,andcomputergraphicsimagesproducedoncathoderayoscilloscopesanddigitalplotters.Teamsofartists,scientists,andengineersexperimentedinmost 
 daringwaywithconceptsandtechnology,whichcreatedthebodyofworkthat  emergedasaculturallandmark.Different 
 formsofcollaborativeart  andscienceworksandresearchstartedtobecomeapparent  inthelate1980 swiththeadvancement 
 oftechnology.Suchcollaborationsflourishedinthe90 safterthecomputerbecameanaffordablepart  ofdailylife.Theestablishment 
 ofthefirst  collaborativeresearchinstitutionssuchastheElectronicVisualizationLaboratoryatheUniversityofIllinoisaChicago,theComputerGraphicsResearchGroupatheOhioStateUniversityandtheComputerScienceDepartment 
 attheUniversityofUtahinthemid70screatedkeycentersforexperimentationathenexusofartsandsciences.ExhibitionsofA/Scollaborativeprojectshavecontinuedoverthelast 
 fortyyearsinconferencesandfestivalssuchasACMSIGGRAPH,ArsElectronica,ISEAandin  ScienceCenters,suchastheExploratoriumandmanyotherinstitutions.Therearenumbersoffieldsthat 
 transcendthehistoricallyassumedboundariesbetweenart  andscienceandinformeachothersuchasComputerGraphics,ComputerGeneratedAnimation,BiomedicalIllustration,ScientificVisualization,MedicalVisualization,VirtualReality,SciencePhotography,Architecturalreconstruction, 
 ComputerGamesetc. 2.2.1ExamplesofArt/Sciencecollaborations InthissectionDanSandinwillintroducehisbackground(scientificandartistic)tothecourseaudienceandsharehisprofessionalexperiences,whileestablishingtheElectronicVisualizationLaboratory,aUICwithTomDeFanti.Theslidesbelowincludepart 
 oftheinformationthat  willbesharedwiththecourse  audience.         Additionalexampleswillbepresentedderivedfromacademic, 
 government  institutions,andtheindustry.  2.3Motivations 2.3.1  Whyscientistsneedartists? 2.3.2What 
s  initfortheartists?         3Research-­-DrivenDevelopment:APracticalGuide  forSuccessfulA/SCollaboration 
 3.1A/Sresearchsurvey 3.1.1Surveydesign Thepresentedresearchisbasedon42-­-itemanonymousonlinesurveyquestionnairerelatingtocollaborators 
experiencesinArt/Sciencecollaborativeprojects.Thesurveywassent  totargetedgroupsofspecialists(morethan100subjects)fromvariousscientificandartisticfields.Theobjectiveofthesurveywastoexaminepracticalmattersthat 
 scientistsandartistsfacewhenworkingtogetheronprojectsandtoascertainbarriersand facilitatorstosuccessfulA/Scollaboration.Thesurveyquestionsweredesignedtoprovideanassessment 
 ofthescientists andartists subjectiveexperienceworkingwitheachother.  Thequestionsweretargetedtoprovideinputfromscientistsandartistsfromalldisciplines,aswellasprofessionalsthat 
 havebothbackgrounds.Forexample,weconsideredscientistsfromthefieldsofcomputerscience,mathematics,physicalandlifesciences,andsocialandappliedsciences(technology,engineering).We 
 consideredartistsofalldisciplinesaswell,includingvisual,performing,media,appliedandcomputerarts(Figure1, 
 Pleaseseethecompletetableofallrespondents fieldsintheReferencessection).ThesurveytargetedscientistsandartistswhohadalreadyparticipatedinmorethanoneA/Scollaborativeprojects.Thesurveyquestionswerebasedonpilot 
 studythat  wasconductedi2011,addressinggroupofscientists(50+  specialists)whohadworkedonor  moreA/ScollaborativeprojectsandexaminedtheirexperiencesinA/Scollaborations. 
 Figure  1.Wordcloudrepresentation of  data provided in the survey question: Please  specifywhat you 
would consideryour primary  field using  one totwo keywords. Oursurveycontainedtwodifferent  typesofquestions:29structuredmultiple-­-choice 
 questionsand13non-­-structuredquestions;wherethespecialistswereencouragedtoreplyalengthiordertoprovidetheircommentsandfeedback.Wecarriedout 
 pre­studyinvolvingfivecolleaguesconcernedwithourresearch.Thegoalofthepre-­-studywastorefinethecontent 
 andstructureofthesurvey.  Afterreceivingtheinput  fromthepre-­-study,thesurveywasrevisedandthequestionnairewasupdated.Anemailinvitationwithalinktosurveymonkey.comwasemailedtotargetedgroupofscientistsandartistsalongwithcoverletterandbriefintroductiontothestudy.Thesurveyhadanoptionalquestion,whichaskedspecialiststosharelinks/references/bibliographyandnarrativedescriptionofA/Scollaborativeprojectstheirhadparticipatedin.Thisquestionwaslinkedtoseparatedatabase,sothat 
 responsestothisquestionwouldnot  compromisetheanonymityofthedata  fromtherest  ofthesurvey. 3.1.2Survey 
 results Thesurveywasopenforfeedbackfor2-­-week  period.Woverallreceived68responses.Someoftheparticipantsinthestudyindicatedthat 
 theyhadnoA/Scollaborativeexperienceandconsequentlywerenot  includedinthedata  analysis.Still,wewereabletoselect 
 anexperiencedgroupofspecialistsas50%ofsubjectsreportedtohaveworkedon6ormoreA/Scollaborativeprojects.Onaveragethetotaltimespent 
 onA/Scollaborativeprojectswasreportedbetween2and3years. Thesurveyaskedtheparticipantstoindicateiftheyconsiderthemselvesasartists,scientistsorbothanartist 
 andscientist,assomecontemporaryprofessionsoperateinbothfields.Interestinglyenough53%ofallparticipantsansweredthat 
 theyarebothartistsandscientists(While22%answeredasscientistsand25%answeredasartists)(seeFigure2).Thenumberofparticipantswhoconsiderthemselvesasartistsandscientistswasanunexpectedmajority.Duetothisinterestingresult,wplantofollowupwiththisgroupofspecialiststolearnmoreabout 
 theireducation,background,projectsandexperiencesianupcomingversionofthesurveywithin2013.  Figure  2:Datachart 
from Art/ScienceCollaboration Research  Survey Question 3:   Would you consider yourself ascientist or 
an artistorboth? Thesurveyresultsindicatethat  73%ofscientistsratedtheircollaborativeexperiencewithartistsasverygoodorexcellent(Figure3). 
 Weaskedspecialiststodescribetheirpersonalcollaborativeexperiencesusing3keywords.Weanalyzedtheresult 
 text  data  usingwordcloudvisualization(Figure4).Themost  commonwordsthat  equallystandout  ofthewholedata 
 are:Frustrating,Enlightening,ChallengingandRewarding. In general, how would you describe your experience 
with A/S collaboration? 30 25 Scientist 20 15  Artist 10 Both a Scientist and an 5 Artist 0  Figure 
 3:  Graph representationofdata analysis ofA/S Research  Survey Question 8: In general,howwould  you 
 describe yourexperiencewith  A/S collaboration? Byanalyzingthequestionnairesandthecomments,wefoundthat 
 collaboratorswhoexperiencedfrustrationduringthecollaborationreportedalsoothertypes  ofproblems,  includinglackof:communication,funding,management,commonlanguagebetweenallproject 
 members.Pleasenotethat  statementsinquotes(unlessreferenced)intherest  ofthecoursenotesareextractedfromopen-­-endedquestionsfromthesurvey. 
 Figure  4.  Wordcloudrepresentation of  data provided in the survey question: Please  list words that 
 describe the impressions the A/S collaboration experience left  you with. The font size  of each word 
is associated with the  frequency itappeared in the  survey answers.  3.2Practicalsuggestionsfor  A/S 
 collaboration 3.2.1Prepareforcollaboration: Choosecollaboratorswisely . It  isimperativethat  you Sit 
 downandtalk withpotentialcollaborators, get  toknoweachother andseeifyou canactuallyworktogether asteam.Themajorityofspecialistsagreethat 
 thebest  waytoprepareforcollaborationistoaskcollaboratorstosharehighlights,discoveries,newsandtrends(74%)(Figure5). 
 Organizingseminarinthebeginningoftheproject  wasrecommendedbythemajorityofsubjects(51%).Inaddition,attendingpresentationsdescribingyourcollaboratorsfieldswasindicatedimportant 
 (49%).    Figure  5.  Responses to  A/S Collaboration Research Survey Question 15: In preparationforcollaboration 
which ofthefollowing things would you liketo do? Most  ofthespecialistsacrossthedisciplines(art,scienceandboth)unanimouslypreferworkshops 
 asthebest  waytoeducateartists/scientistsabout  collaborativeprojects(33%)(Figure6).Webvideo,multimedia,webinars,lab/studiotourswererecommendedforpreparationaswell.Inaddition,manyspecialistsemphasizedtheimportanceofdiscussionsandface-­-to-­-facemeetingsinpersonandwiththeentiregroup. 
 In preparation for an A/S collaboration, what do you think is the best way to educate artists/scientists 
about collaborative projects:  Scientist  Artist  Both a Scientist and an Artist  Figure  6.  Graph 
representation of dataanalysisofA/S Collaboration Research Survey Question19: In preparation for A/Scollaboration,what 
do you thinkisthebestwayto educate artists/scientists about collaborative projects. 3.2.2Securefunding: 
 Oneoftheimportant  aspectsofpreparingforcollaborativeprojectistosecurefunding(seedorlongtermfunding).Weaskedspecialistsiftheyhadreceivedanyfundingtoconduct 
 theirlast  collaborativeproject.Themajorityansweredyes(74%).Morethan  47ofspecialistsindicatedthat  it 
 isunlikelytheywouldhavepursuedtheproject  without  funding.  However,while33%ofscientistssaidthat  theywere 
 likely  topursuetheproject  withnofunding,thenumberofartistsinthiscategorywas0%(Figure7).Insubsequent 
 question,weaskedtheparticipantstoratetheirsatisfactionwiththefundingsources  intheirlast  A/Scollaboration.majority(50%) 
 ofscientistsindicatedthat  theywere  versatisfiedorsatisfiedwiththefundingtheyhadreceived.However,only20%ofartistswereverysatisfiedorsatisfiedwiththefundingintheirlast 
 collaborativeproject.Furthermore,anequalpercentageoartists(20%)wereverydissatisfiedwiththefunding,whilethenumberofverydissatisfiedscientistswas0%.70%ofparticipantsthat 
 arebothartistsandscientistsweresatisfiedwiththefundingandonly18%not  satisfied.  How likely were you 
to pursue this project without this funding? 9 8 7 6  Scientist 5 4  Artist 3   Both a Scientist 
and 2 an Artist 1 0 Very likely Likely Somewhat likely Not likely  Figure  7.  Graph representation 
of A/S Collaboration Research Survey  Question 11: How likelywere youto pursue thisproject withoutthisfunding? 
Out  oftheselast  collaborations,66%weremostlyfundedbyScience,11%byArt  and23%receivedanequalamount  offundingfromArt 
 andScience.Themost  commonactivefundingsourceswereindicatedasNSF(29%),  NIH  (11%),privatefunding(14%),grants(14%), 
 andUniversityandgovernmentinstitutions.  SomeoftheotherfundingsourceswerelistedasNEA,DOE,NEH,McCarthurFoundation,EUgovernment 
 andcommissions,  corporatefunding,  honoraria,  selfandmuseumcommissions  (Figure8).  Figure  8.  Wordcloudrepresentation 
of  data provided in the survey question:   Whatarethemostcommon funding sources  for the kinds  ofA/S 
projects  you do? Please list them. The font size ofeach word is associated with the  frequency itappeared 
in the  survey answers. 3.2.3Define  andlearn  common  language:   Commonlanguageisoneofthemost  important 
 elementsofA/Scollaboration.Whenweaskedspecialistshowsatisfiedtheywerewithcommonlanguageintheirlast  A/Scollaboration,while 
 60%ofscientistsweresatisfied,artistswereequallydivided(33%satisfiedand33%dissatisfied),and46%ofspecialistsinbothdisciplinesweresatisfied.Interestinglyenough,whenwaskedtoratetheimportanceofcommonlanguagebetweencollaboratorsallscientists(100%)ratedit 
 asveryimportant  orimportantelement  ofsuccessfulA/Scollaboration.However,29%ofartistsstatedit  isunimportant 
 orneither,with71%ofartistsconsideredit  asanimportant  element. 3.2.4Heighten  mutual  respectantrust: 
  Otherimportant  elementsaremutualrespect  andtrust  betweenmembersoftheteam.However,forscientistsmutualrespect 
 andtrust  aremoreimportant  thanforartists:100ofscientistssaidthat  mutualrespect  isveryimportant  and78%saidthesameabout 
 trust.Amongartists71%saidmutualrespectisveryimportant  and7%saidit  isneitherimportant  norunimportant.57%ofartistsratedtrust 
 asveryimportant  and21%saidit  ineitherimportant  norunimportant.   3.2.5Adheretotimeline:   Timelineorachievement 
 ofgoalsintimelymannerwasreportedthatneeds  morework,  especiallyamongscientists.50of  scientistsweredissatisfiedwiththetimelineorwithhowtheplannedproject 
 schedulesucceeded.Amongartists67%weresatisfiedandamongspecialistsinbothdisciplines66%weresatisfied. 
 Majorityofspecialist  agreethat  achievement of goals in a timely manner is very important or important 
(70%). 3.2.6Don tforgettacknowledge  yourcolleagues: Whilethemajorityofallsubjectswassatisfiedwiththeiracknowledgements,someartistswermore 
 dissatisfiedthanscientistswiththeproperacknowledgementsoftheircontributionsintheircollaborativeA/Sprojects:20%statedthat 
 theyverydissatisfiedordissatisfiedonthissubject.Amongscientiststhisnumberwas0%.83%ofartistsstatedthat 
 anagreement to acknowledge all team members is very important or important. 3.2.7Appointpropermanagement 
 and  leadership:   Openlydiscussexpectationsanddefinetherolesandresponsibilitiesbeforestartingtheproject. 
 Inrespondingtothequestionabout  management  inthelast  A/Scollaborativeproject,70%ofscientistsweresatisfiedwhileonly52%oftheartistsansweredthesame.33%ofartistswereingeneraldissatisfiedwithproject 
 management.Regardingtheleadershipwithintheteam,80%ofscientistswerehappyabout  it,andonly53%ofartistsregardedit 
 assatisfactory.Artistsandscientistsagree(100%)that  project  organization anddefinitionofteammemberrolesandresponsibilitiesisverycriticalelement 
 ofA/Sproject  management.Assumingeveryonebringsknowledgeandexpertisetotheproject  andworkswellwithothers,thenext 
 most  criticalelement  forthesuccessoftheproject  isitsmanagement.Withgoodmanagement  theteamwillbeableto:1)clearlydefinegoals,roles,timelinesanddeliverables,and2)communicatetheprocessandprogressclearlyandfrequently.So,embraceproject 
 management  methods.Iwillhelpyoubecomestrongasset  totheteamandmaketheproject  reality. 3.2.8Learn  theterminology: 
  Iwillbenefit  everyoneintheteamifproject  membersbecomefamiliarwiththelanguagesofcrosdisciplines  involvedintheproject. 
 Occasionally,specialistsencounterdifficultieswhentryingtocommunicatescientificorartisticmethodswithout 
 relyingonspecificterminology.Weaskedartistsifscientistsshouldunderstandartisticterminologyandweaskedscientistsifartistsshouldunderstandscientificterminology.Majorityofartists(54%)saidtheyneitheragreenordisagreewiththestatement 
 andforscientiststhenumbersweresplit  (38%neitheragreenot  disagreeand38%agree).Knowing  relevant  expressionsandkeywordsthoroughlyhelpsartistsunderstandthescientificfieldtheywork 
 in. 3.2.9Understand  thetechnology: Anoverwhelmingmajority(81%)ofthespecialistsagreedthat  it  isimportant 
 that  thecollaboratorhasgeneralunderstanding(notindetail)about  thecapabilities,timerequiredandlimitationsofthetechnologiesutilizedintheproject. 
 Evidentlythisimportanceishigherpriorityforscientists(89%)andlowerforartists(58%)(Figure9).  Comprehendwhat 
 ispossible(aswellaswhat  isnot),recognizethetechnicalchallengesandbecomefamiliarwiththeinputsandoutputsofthesystem. 
   Figure  9.  Understanding technologyis important.Graph representation of A/S Collaboration  Research 
 Survey Question  17. 3.2.10Sharpen  yourskills: Weaskedscientistsiftheartist-­-collaboratorshouldlearn/havespecificskills(ex.programming,data 
 analysis,understandingofstatistics,entrepreneurial)toworkwiththescientificsubject  matterintheirfield.63%ofscientists 
 consideredthisnot  necessary.However,scientistscommentedthat  analyticalskills,understandingoftheoverallconcept 
 ofthescience,  understandingofprogrammingconceptsandlimitationsofhardware/softwarewouldbeadvantageous.Weaskedartiststhesamequestionabout 
 scientist-­-collaborators(ihe/sheshouldunderstandtheimportanceofspecificconcepts(ex.Colortheory,form,perspective,composition,etc.)toworkontheA/Scollaborativeproject.Only23%ofartistsbelievedit 
 wasnecessary.Artistsindicatedthat  basicunderstandingofthehistory,art  history,  conceptualpractices,criticaltheory,theelementsandconventionsofmedia 
 artssuchasphotography,film,video,design,iterativeprototypingandinterfacedesigncouldbe  helpful.Ingeneral,it 
 isimportant  that  thescientist  collaboratorunderstandstheconcept  owhat  art  isbecausescientistskeepasking"What 
 isart sfunction?" Fromtheartists point  ofview,amongthemost  important  skillsthecollaboratorshouldhaveare:programming,openmind,conceptualart,design,media 
 art  history,creativeprocesses,andtoknowthat  not  everythingisquantifiable.Scientistsbelievedthat  theskillrequirementsvarywitheachproject,but 
 themost  commonskillsanartistcollaboratorshoulholdare:understandingofbasicprogramming,researchskills,data,visualdesign,scientificinquiryandtheory,andwillingnesstolearnnewthings.Itshouldbepointedoutthat 
 bothscientistsandartistsagreedthat  programmingandfundamentaldifferencesbetweenart  andsciencearethemost 
 important  skillsforcollaborators. 3.2.11Clarifythe  outcomes: Basedonthesurveyresponsesthemajority(78%)ofscientistsagreedthatpatentsareunimportant.Inaddition,0ofscientistscharacterizedrevenueasimportant(Figure10).Themost 
 important  outcomesofcollaborativeA/Sprojectsforscientistswerepublicoutreach(90%saidit  isimportant 
 orveryimportant)andconferences/presentations(89%saidit  isimportant  orveryimportant).Forartiststhemost 
 important  outcomeswereconferences/presentations(92%statedit  isimportant  orveryimportant).Surprisingly,exhibitionsinart 
 venuesandgallerieswerelessimportant  (77%)forartistsalongwithexhibitionsimuseumsandscience  centers(77%)andpublicoutreach(77%).Specialistsinbothfieldsindicatedpublicoutreachandconferences/presentationsasthemost 
 important  (85%each)alongwithpublications(81%). Anoverwhelmingmajorityofthespecialistsconsideredpublicoutreachveryimportantfortheirwork,whileleast 
 important  wasrevenue(profit).  Publicoutreachencompassesactivitiesandeducationalprogramsthat  bringtheexcitingmessageofscienceandarttoeveryoneandgenerateawarenessofthesocietalvalueofresearch.Suchprogramsinclude:MuseumsandScienceCenterexhibits,educationalhands-­-onactivities,interactiveinstallations,games,workshops,etc.Roll-­-upyoursleeves,becauseyourcreativityisindemandandget 
 readytotakeyourartisticvision,skillsandcollaborativeproductswheretheyareneeded.            
Figure  10.  From yourperspective,how importantiseachofthefollowing outcomesofanA/S project for you? 
Please base your answer othe most recentcollaborative A/S project you participated  in. 3.2.12Discover 
 newopportunities: Accordingtooursurvey,scientistsmostlyneedhelpwithvisualpresentationandengagingmaterials.Weaskedspecialiststoshareareasorfieldsthat 
 couldbenefit  fromscientificcontribution(areasthat  arenot  currentlyutilizingscientist  werpreferred).Amongthosewerelisted; 
 3Dscanning  Art  education  Art  history  Cinema  andFilm  Extremelyhighresolutionphotography  
Interactiveinstallationart  Media  andNews  Movement  learningandtrainingfordance  Among  fieldsthat 
 couldbenefit  fromartisticcontribution(areasthat  arenot  currentlyutilizingartists)weresuggested; 
Archaeology  Biochemistry  Climatology  Human-­-robot  interaction  Meteorology  Molecularbiology 
 Oceanography  Physicalrehabilitation  Proteomics  Segmentationandrenderingoflargescaleimagingdata 
 Wildlifepreservation  Onemight  askhowspecialistsinvariousdisciplinescanexplorenewhorizonsofpotentialapplications.Whyschoolsdonot 
 educateartistsandscientistsabout  A/Sapplications,especiallyinthefrontiersofscience,art,researchandtechnology?Informationiskey.Learnabout 
 current  trends,art/science/technologylabsinuniversities,industryandthegovernment  andhowtoget  accesstothem.Beopen-­-minded,becuriousandexplorenewhorizonsthat 
 artandsciencecanopenforyou. What  isthemost  important  adviceyouwouldgivetoanartist  /scientist  whoisplanningtostart 
 anA/Scollaborativeproject? Selectedanswers: Listenwell  Spendtimetogether  Communicateoften  Keeptheproject 
 proposalflexible  -­-it  willchangeanyway  Accept  thepossibilityoffailure  Bepatient  Don't  under-­-estimateyourself 
  3.3Futuredirections We  plantofollouwithsurveysdesignedtoreceiveadditionalinformation,suchas  detailedfeedbackfromspecialistsbelongingtobothA/Sfields(art 
 andscience).  Thiwill  enableustocollect  additionalinformationandprovidemoredata  onA/Scollaborations, 
 suchastheirbackground,professionandtypesofprojectstheyworkon.Iisimportant  tomentionthat  thissurveydoesnot 
 coveralltopicsconcerningA/Scollaborations.Otherissuessuchasintellectualpropertyandownership,andhoweducationalinstitutionsandtheindustryfosterandpromotesuchcollaborationswillbeincludedinourfuturework. 
  4Case  Studies Thispartwillcovethreecasestudiesthat  describesuccessesandfailuresiA/S  collaborations.Thecasestudieswillillustratehowartistsandscientistsworktogethertoovercomechallenges. 
 Authors,developers,instructorsandteammembersofthethreecasestudies,willbeaskedquestionsabout  goalsandlessonslearned.TheanswerstothesequestionswillbesharedwiththeSIGGRAPH 
 courseaudience.Sections4.1,4.2and 4.3containgeneralinformationabout  thecasestudies. 4.1JuliaSetscollaborative 
 projectsbyDan  Sandin Figure  11:Close uview ofJulia Set.Imageprovidedby Dan Sandin. ThevisualizationoftheJulia 
 Setsiscollaborativeeffort  betweenmathematicianLouisKauffmanandcomputergraphicspioneerDanSandin.Thecollaborationstartedintheearly80swithrenderingsoftwo-­-dimensionalJulia 
 setsonpre-­-IBM  PCcomputers,suchastheTRS80laptopandtheDatamaxUV1(Z-­-BOX).Julia  setsandotherfractalscanexist 
 in threeandhigherdimensions.Bothcollaboratorshavebeeninterestedinvisualizingthesethree-­-dimensionalfractalsbothtounderstandtheirbehaviorandproduceinterestingimagesandanimation.Thecollaborationevolvedduringthenextdecadesandmore 
 collaboratorsjoinedtheteam(scientistsandartists).  Fromthiscollaborativeworktwoanimationswereproduced;one-­-minute35mmstereomovie,"AVolumeoTwo­DimensionalJulia 
 Sets"forSIGGRAPH1990andsuperhighdefinition(2Kx2Kx60Hz)visualizationofquaternion(4D)Julia  setsforNipponTelephoneandTelegraph's50thbirthdaycelebration.Inaddition,bookandCD-­-ROM 
 havebeenproducedthat  describethiswork.Thiscollaborationprovedusefultothecollaboratorsinseveralimportant 
 ways:it  producedseriesofartworks,newcomputergraphicsrenderingtechniques,andmathematicalproofs.Inaddition,it 
 wasanexcellent  test  vehicleforevaluatingadvancedcomputationalandnetworktechniques.        
     Formoreinformation,pleasevisit:http://www.evl.uic.edu/hypercomplex/  4.2InterdisciplinaryProductDevelopment(IPD):collaborativecoursein 
 academia InterdisciplinaryProduct  Development  (IPD)aUICistwo-­-semestercurriculumthat  integratesthelatest 
 technologiesandbest  practicesforinnovativeproduct  development.ThecoursecombinesIndustrialDesign,Engineering,andMBA/Marketingstudentsfromallthreecollegestoworktogetherincross­functionalteamstoresearchanddevelopnewproduct 
 concepts.Thecoursefocusesontheearlystagesoftheproduct  development  process,fromidentifyingmarket  opportunitiesthroughinitialprototyping.Eschewingtheoldsequentialmodelofproduct 
 development,inwhichdesignidea  originatesinbusinessunit,isgivenvisualformbyindustrialdesign,andthenpassedofftoengineering,thiscourseteachescurrent 
 best  practicesoftrueintegrationofallthreedisciplinesfromtheveryearliest  stagesofproduct  development.Thecourseisteamtaught 
 byfacultyfromthethreecolleges.Eachyear,acorporatesponsorisrecruitedtoserveastheclient,andprovidesthefinancialsupport 
 toincorporateoutsideresourcesandmaterialsnot  normallyavailabletoanacademicenvironment.Fundingisusedtocoverthecourseexpensesassociatedwithpayingforprofessionallyconductedmarket 
 research,teambuildingexercises,innovationtraining,rapidprototyping,materialsandsupplies,aswellasothersignificant 
 courseexpenses.Theclient  company,inassociationwiththeIPDfaculty,alsoprovidesaninterestingandchallengingassignment 
 that  willresult  intheresearch,conceptionanddevelopment  ofinnovativeproduct  conceptsfortheclient. 
      Formoreinformation,pleasevisit:http://www.ipd.uic.edu/IPD/ 4.3Industryexample:NASA  VisualizationExplorer 
           5References,Links,  Bibliography ThissectionliststhereferencesreferencedinthesecoursenotesplusadditionalmaterialrelevanttotheA/Scollaborationresearch.Thereferencesarelistedinalphabeticalorderwithannotations.InadditionwehavelistedselectedA/Sorganizationsandstudies.Alexa 
 Wright,AlfLinney,TheArt  andScienceofLong-­-termCollaboration,NewConstellationsConference,MuseumofContemporaryArt,Sydney,17March2006.Brent 
 MacGregor,Cyberneticserendipityrevisited,ProceedingC&#38;C'02Proceedingsofthe4thconferenceonCreativity&#38;cognitionACM 
 NewYork,NY,2002ChrisComer,SymBIOticART&#38;Science:AnInvestigationatheIntersectionofLifeSciencesandtheArts,FinalReport 
 fortheNSFArt  andScienceWorkshop,NSFandtheNEA,2011,Arlington,VA,publishedonJan7th,2012.DanSandinhttp://www.evl.uic.edu/dan/DigiArts,UNESCOKnowledgePortal,referencewebsiteonart,scienceandtechnology. 
http://portal.unesco.org/culture/en/ev.php­URL_ID=1391&#38;URL_DO=DO_TOPIC&#38;URL_SECTION=201.htmlEdwardA.Shanken,ArtistsinIndustryandtheAcademy:CollaborativeResearch,InterdisciplinaryScholarshipandtheCreationandInterpretationofHybridForms,Leonardo38:5,2005,p.415-­-18.InterdisciplinaryProduct 
 Development  (IPD),UIChttp://www.ipd.uic.edu/IPD/List  ofFieldsofrespondentsthat  participatedinthesurvey(Note:thefieldsarelistedastheywereprovidedanddescribedbythespecialists) 
 Art  Astronomy  Biomedicalcommunications  Communication Computationalart Computergenerated images 
Computergraphics Computerscience Culturalanthropology Dancerehabilitation Design Digitalmedia Educationandoutreach 
Electronicvisualization Fine  artist HCI Humanmachineinterface Human-­-computerinteraction Immersivearts 
Industrial  design Informationsystems Interactiondesign Interactiveartandanimation Interactivemedia Learning 
 sciences Newmedia Newmediaarts Physicalscience;earthscience Physics Scientificresearch Software  developer 
Virtualreality Visualarts Visualtechnology Visualization VRHCI Computerscience,visual/performanceart 
Metalwork,computerdesign Photography,video,newmedia  installation  andAccomplishments,Leonardo,Vol.7pp.255-­-259,PergamonPress,Britain,1974.NASAVisualizationExplorerfortheiPad-­-Credits.NASA 
 SVS,  n.d.Web.12February2013,  http://svs.gsfc.nasa.gov/nasaviz/credits.htmlRogerMalina,Art  ScienceRadarhttp://malina.diatrope.com/category/art-­-science-­-radar-­-2/SynapseTMsupportedbyAustralianNetworkforArt 
 &#38;Technology(ANAT)http://www.synapse.net.au/index.phpTheSTEM  toSTEAM  Briefing,RhodeIslandSchoolofDesign,September2011,http://stemtosteam.org/ThenetworkforSciences,Engineering,ArtsandDesign(SEAD)WorkingGrouponWhitePapersissuedbytheNSFNSEADworkshopaMICA,chairedbyRogerFMalina,ATEC,UTDallasandcochairedbyCarolStrohecker,CenterforDesignInnovation,UniversityofNorthCarolina 
 systemandaninternationalsteeringcommittee.http://seadnetwork.wordpress.com/about/TheUCLAArt|SciCenter,dedicatedtopursuingandpromotingtheevolving 
ThirdCulture byfacilitatingtheinfinitepotentialofcollaborationsbetween(media)artsand(bio/nano)sciences.http://artsci.ucla.edu/?q=about 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2013</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>2504456</article_id>
		<sort_key>210</sort_key>
		<display_label>Article No.</display_label>
		<pages>14</pages>
		<display_no>21</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>21</seq_no>
		<title><![CDATA[Dynamic 2D/3D registration for the Kinect]]></title>
		<page_from>1</page_from>
		<page_to>14</page_to>
		<doi_number>10.1145/2504435.2504456</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2504456</url>
		<abstract>
			<par><![CDATA[<p>Image and geometry registration algorithms are essential components of many computer graphics and computer vision systems. With recent technological advances in RGB-D sensors, robust algorithms that combine 2D image and 3D geometry registration have become an active area of research.</p> <p>This course introduces the basics of 2D/3D registration algorithms and provides theoretical explanations and practical tools for designing computer vision and computer graphics systems based on RGB-D devices such as the Microsoft Kinect or Asus Xtion Live. To illustrate the theory and demonstrate practical relevance, the course briefly discusses three applications: rigid scanning, non-rigid modeling, and real-time face tracking.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4193542</person_id>
				<author_profile_id><![CDATA[81474669221]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Sofien]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bouaziz]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[&#201;cole polytechnique f&#233;d&#233;rale de Lausanne]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4193543</person_id>
				<author_profile_id><![CDATA[81100582775]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Mark]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Pauly]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[&#201;cole polytechnique f&#233;d&#233;rale de Lausanne]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>132022</ref_obj_id>
				<ref_obj_pid>132013</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[P. Besl and H. McKay. A method for registration of 3d shapes. <i>PAMI</i>, 1992.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>311556</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[V. Blanz and T. Vetter. A morphable model for the synthesis of 3d faces. Proc. of ACM SIGGRAPH, 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1281959</ref_obj_id>
				<ref_obj_pid>1281957</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[M. Botsch, M. Pauly, M. Gross, and L. Kobbelt. Primo: coupled prisms for intuitive surface modeling. SGP, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2346802</ref_obj_id>
				<ref_obj_pid>2346796</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[S. Bouaziz, M. Deuss, Y. Schwartzburg, T. Weise, and M. Pauly. Shape-up: Shaping discrete geometry with projections. <i>Comput. Graph. Forum</i>, 2012.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2461976</ref_obj_id>
				<ref_obj_pid>2461912</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[S. Bouaziz, Y. Wang, and M. Pauly. Online modeling for realtime facial animation. <i>ACM Trans. Graph</i>., 2013.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Y. Chen and G. Medioni. Object modeling by registration of multiple range images. In <i>ICRA</i>, 1991.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[T. Cootes and C. Taylor. Statistical models of appearance for computer vision, 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>250160</ref_obj_id>
				<ref_obj_pid>250152</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[D. W. Eggert, A. Lorusso, and R. B. Fisher. Estimating 3-d rigid body transformations: a comparison of four major algorithms. <i>Machine Vision and Applications</i>, 1997.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[P. Ekman and W. Friesen. <i>Facial Action Coding System: A Technique for the Measurement of Facial Movement</i>. Consulting Psychologists Press, 1978.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[B. K. P. Horn and B. G. Schunck. "determining optical flow": A retrospective. <i>Artif. Intell</i>., 1993.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1618521</ref_obj_id>
				<ref_obj_pid>1618452</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[H. Li, B. Adams, L. J. Guibas, and M. Pauly. Robust single-view geometry and motion reconstruction. <i>ACM Trans. Graph</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1623280</ref_obj_id>
				<ref_obj_pid>1623264</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[B. D. Lucas and T. Kanade. An iterative image registration technique with an application to stereo vision. IJCAI, 1981.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[M. Mirza and K. Boyer. Performance evaluation of a class of m-estimators for surface parameter estimation in noisy range data. <i>IEEE Transactions on Robotics and Automation</i>, 1993.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2120179</ref_obj_id>
				<ref_obj_pid>2120094</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[R. A. Newcombe, S. Izadi, O. Hilliges, D. Molyneaux, D. Kim, A. J. Davison, P. Kohli, J. Shotton, S. Hodges, and A. Fitzgibbon. Kinectfusion: Real-time dense surface mapping and tracking. ISMAR, 2011.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2354910</ref_obj_id>
				<ref_obj_pid>2354409</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[I. Oikonomidis, N. Kyriazis, and A. Argyros. Tracking the articulated motion of two strongly interacting hands. CVPR, 2012.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[S. Rusinkiewicz and M. Levoy. Efficient variants of the icp algorithm. <i>3DIM</i>, 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1282006</ref_obj_id>
				<ref_obj_pid>1281991</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[O. Sorkine and M. Alexa. As-rigid-as-possible surface modeling. SGP, 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1276478</ref_obj_id>
				<ref_obj_pid>1276377</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[R. W. Sumner, J. Schmid, and M. Pauly. Embedded deformation for shape manipulation. <i>ACM Trans. Graph</i>., 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2197180</ref_obj_id>
				<ref_obj_pid>2197075</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[J. Tong, J. Zhou, L. Liu, Z. Pan, and H. Yan. Scanning 3d full human bodies using kinects. <i>TVCG</i>, 2012.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2366207</ref_obj_id>
				<ref_obj_pid>2366145</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[X. Wei, P. Zhang, and J. Chai. Accurate realtime full-body motion capture using a single depth camera. <i>ACM Trans. Graph</i>., 2012.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1964972</ref_obj_id>
				<ref_obj_pid>2010324</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[T. Weise, S. Bouaziz, H. Li, and M. Pauly. Realtime performance-based facial animation. <i>ACM Trans. Graph</i>., 2011.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[T. Weise, B. Leibe, and L. V. Gool. Accurate and robust registration for in-hand modeling. CVPR, 2008.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[T. Weise, T. Wismer, B. Leibe, and L. Van Gool. In-hand scanning with online loop closure. 3DIM, 2009.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Dynamic 2D/3D Registration for the Kinect So.en Bouaziz Mark Pauly ´ Ecole Polytechnique F´ed´erale 
de Lausanne Abstract Image and geometry registration algorithms are an essential component of many computer 
graphics and computer vision systems. With recent technological ad­vances in RGB-D sensors, such as the 
Microsoft Kinect or Asus Xtion Live, robust algorithms that combine 2D image and 3D geometry registration 
have become an active area of research. The goal of this course is to introduce the basics of 2D/3D registration 
algorithms and to provide theoretical explanations and practical tools to design computer vision and 
computer graphics systems based on RGB-D devices. To illustrate the theory and demonstrate practical 
relevance, we brie.y discuss three applications: rigid scanning, non-rigid modeling, and realtime face 
tracking. Our course targets researchers and computer graphics practitioners with a background in computer 
graphics and/or computer vision. About the Lecturers So.en Bouaziz is a PhD student in the Computer 
Graphics and Geometry Labora­´ tory at the Ecole Polytechnique F´ed´erale de Lausanne (EPFL) under the 
supervision of Prof. Mark Pauly. He received his MSc degree in Computer Science from EPFL in 2009. His 
research interests include computer graphics, computer vision, and machine learning. So.en co-developed 
the facial motion capture software faceshift studio. Mark Pauly is an associate professor of computer 
science at EPFL in Lausanne, Switzer­land, where he directs the Computer Graphics and Geometry Laboratory. 
Prior to joining EPFL he was an assistant professor at ETH Zurich and a postdoctoral scholar at Stan­ford 
University. He received his Ph.D. degree in 2003 from ETH Zurich. His research interests include computer 
graphics and animation, shape analysis, geometry processing, and architectural design. So.en and Mark 
are co-founders of faceshift AG (www.faceshift.com), an EPFL spin-o. that brings high-quality markerless 
facial motion capture to the consumer market. Course Overview 5 minutes: Introduction Overview of the 
course and motivation 50 minutes: 2D/3D Registration 20 minutes: 3D Registration  20 minutes: 2D Registration 
 10 minutes: Putting It All Together  30 minutes: Applications 10 minutes: Rigid Scanning  10 minutes: 
Non-rigid Modeling  10 minutes: Realtime Face Tracking  Outlook and Q&#38;A  5 minutes: Conclusion 
  1 Introduction Recent technological advances in RGB-D sensing devices, such as the Microsoft Kinect, 
facilitate numerous new and exciting applications, for example in 3D scanning [19] and hu­man motion 
tracking [20, 15, 5]. While a.ordable and accessible, consumer-level RGB-D devices typically exhibit 
high noise levels in the acquired data. Moreover, di.cult light­ing situations and geometric occlusions 
commonly occur in many application settings, potentially leading to a severe degradation in data quality. 
This necessitates a partic­ular emphasis on the robustness of image and geometry processing algorithms. 
The combination of 2D and 3D registration is one important aspect in the design of robust applications 
based on RGB-D devices. This lecture introduces the main concepts of 2D and 3D registration and explains 
how to combine them e.ciently. An up-to-date version of these course notes as well as implementation 
details and source code can be found at http://lgg.epfl.ch/2d3dRegistration. 2 2D/3D Registration In 
the .rst part of the course we introduce the theory of 2D/3D registration algorithms suitable for processing 
RGB-D data. We focus on pairwise registration to compute the alignment of a source model onto a target 
model. This alignment can be rigid or non­rigid, depending on the type of object being scanned. We formulate 
the registration as the minimization of an energy Ereg = Ematch + Eprior. (1) The matching energy Ematch 
de.nes a measure of how close the source is from the target. The prior energy Eprior quanti.es the deviation 
from the type of transformation or defor­mation that the source is allowed to undergo during the registration, 
for example, a rigid motion or an elastic deformation. The goal of registration is to .nd a transformation 
of the source model that minimizes Ereg to bring the source into alignment with the target. For data 
acquired with RGB-D devices, registration can utilize both the geometric infor­mation encoded in the 
3D depth map, as well as the color information provided by the recorded 2D images. We show that Equation 
1 provides a uni.ed way to formulate both 2D and 3D registration, which simpli.es their integration. 
2.1 3D Registration In 3D registration we want to align a source surface X embedded in R3 to a target 
surface Y in R3 . To formalize this problem, we introduce a surface Z that is a transformed or deformed 
version of X that eventually aligns with Y. To solve the registration problem numerically, we represent 
the continuous surface X by a set of points X = {xi . X , i = 1 . . . n} and de.ne their corresponding 
points on the deformed surface Z as Z = {zi . Z, i = 1 . . . n}. Di.erent sampling strategies have been 
presented by Rusinkiewicz and Levoy [16]. 2.1.1 Matching Energy The matching energy measures how close 
the surface Z is to the surface Y and is de.ned as Ematch (Z) = .(z, Y)dz, (2) Z where z . R3 is a point 
on Z. The accuracy of the registration is evaluated by the metric . that measures the distance to Y. 
For simplicity, we use the squared Euclidian distance as metric for all the energies presented in this 
course. However, robust metrics [13] could be use instead to increase the robustness of the registration 
to noise and outliers. Using the set of points Z, we can discretize the matching energy as n n Ematch 
(Z) = lzi - PY (zi)l2 2 . (3) i=1 where PY (zi) is the closest point (using Euclidian distance) on the 
surface Y from zi. PY (zi) can also be seen as the orthogonal pro jection of zi onto Y.  2.1.2 Prior 
Energy In this section we present several prior energies that can be used for registration. These energies 
can also be combined to build more sophisticated priors. Priors encode proper­ties of the scanned objects. 
For example, when scanning rigid objects, a global rigidity prior can be used to limit the allowed transformations 
to rotations and translations. For deforming objects, for example a human body, geometric priors are 
often employed that try to mimic physical behavior such as an elastic deformation. We describe a simple 
local rigidity prior that approximates elastic deformations and facilitates e.cient imple­mentations. 
More complex deformation behavior can be captured using a data-driven approach. One popular method is 
based on a collection of sample shapes that repre­sent that allowable space of deformations. Using dimensionality 
reduction, for example principal component analysis, e.cient linear models can be derived that are suitable 
for realtime registration algorithms. Global Rigidity. The global rigidity of the 3D registration can 
be measured as n n Eprior(Z, R, t) = lzi - (Rxi + t)l2 2 , (4) i=1 where R . R3×3 is a rotation matrix 
and t . R3 a translation vector. In this case, the surface Z tries to follow a rigid transformation of 
the surface X . Local Rigidity. The local rigidity energy, following [17, 4], can be expressed as n 
n n Eprior(Z, Ri|n ) = l(zj - zi) - Ri(xj - xi)l2 2 , (5) i=1 i=1 j.Ni where the Ri . R3×3 are rotation 
matrices and Ni is the set of indices of the neighboring points of xi. In this case, each local neighborhood 
on the surface Z tries to follow a rigid transformation of its corresponding local neighborhood on the 
surface X . Other local rigidity energies can also be used as prior, see for example [3, 18]. Linear 
Model. A 3D linear shape model can be de.ned using a matrix P containing the shape model basis, and a 
mean shape vector m [7]. A new shape s can be de.ned as s = Pd + m, (6) where d is a vector containing 
the basis coe.cients. A linear model prior energy can be formulated as the deviation of the vertices 
from the linear model n Eprior(Z, d) = lzi - (Pid + mi)l2 2, (7) i=1 where Pi and mi are the part of 
P and m corresponding to the vertex zi. 2.1.3 Optimization How to best optimize the registration energy 
depends on the prior energy. In this section n we show, as an example, how to optimize a registration 
energy for two applications: rigid scanning and non-rigid modeling. Rigid Scanning Since single depth 
maps acquired with the RGB-D sensor exhibit high noise levels and do not cover the whole surface of the 
3D ob ject, an aggregation procedure is typically applied to obtain a complete model with reduced noise 
level. In order to aggregate multiple scans over time, di.erent methods can be used [22, 23, 14]. The 
classical approach is to perform a 3D rigid registration of the currently acquired scan of the ob ject 
with the already accumulated 3D data. The pairwise 3D alignment can be formulated as n n nn Ereg(Z, R, 
t) = w1 lzi - PY (zi)l2 2 + w2 lzi - (Rxi + t)l2 2 , (8) i=1 i=1 where the matching energy is combined 
with a global rigidity prior. To optimize Ereg(Z, R, t) we linearize the rotation matrix approximating 
cos . by 1 and sin . by . . . 1 -. ß R R = . . 1 -a . . (9) -ß a 1 The alignment is computed by 
solving iteratively n n nn t+1 t t+1 )l2 - ( arg min w1 lz - PY (z 2 + w2 ||z Rxi + t)||2 2, (10) ii 
i Zt+1 ,R,t i=1 i=1 where t is the iteration number and z0 i = xi. As PY (.) is a non linear function 
that is di.cult to optimize with, we use in the optimization the previous estimate PY (zi t). This correspond 
to the point-to-point matching error [1]. To speed up the convergence of the t+1 tt Tt+1 t optimization 
one can linearize lzi - PY (zi)l2 at PY (zi) which gives ni (zi - PY (zi)), where ni is the normal of 
the surface Y at PY (zi t). This leads to the point-to-plane matching error [6]. The optimization can 
be reformulated as nnT t+1 t t+1 - ( arg min w1 (n (z - PY (z )))2 + w2 ||z Rxi + t)||2 2. (11) iii i 
Zt+1 ,R,t i=1 i=1 Both Equation 10 and Equation 11 are quadratic, and therefore, can be optimized by 
setting the partial derivatives to zero by solving a linear system. It is interesting to note that if 
w2 = +8 then zi can be replaced into the matching energy by Rxi + t leading to a registration energy 
n n n Ereg(R, t) = lRxi + t - PY (Rxi + t)l2 2. (12) i=1 This energy can be minimized in a similar spirit 
by linearizing the rotation matrix and iteratively solving a linear system. Other approaches can be found 
in [8]. Non-rigid Modeling Registering a shape template towards a scanned 3D ob ject allows n to obtain 
a complete and clean 3D mesh [11]. An example is given below in the context of face modeling. In this 
case, the morphable model of Blanz and Vetter [2] that represents the variations of di.erent human faces 
in neutral expression was registered to a scan of a face. Non-rigid modeling using a morphable model 
can be formulated as n n nn Ereg(Z, d, Ri|n i=1, R, t) = w1 lzi - PY (zi)l2 2 + w2 lzi - (Rxi + t)l2 
2+ i=1 i=1 n n nn w3 lzi - (Pid + mi)l2 2 + w4 l(zj - zi) - Ri(xj - xi)l2 2 . (13) i=1 i=1 j.Ni A local 
rigidity energy is added to the optimization in order to get an accurate result, as the morphable model 
represents the large-scale variability but might not capture small scale details. As previously, we solve 
iteratively n n n nnT t+1 t t+1 arg min w1 (ni (z - PY (zi)))2 + w2 ||z Rxi + t)||2 2+ i i - ( Zt+1 
,d,Ri|n i=1,R,t i=1 i=1 n n nn t+1 t+1 t+1 w3 lz - (Pid + mi)l2 2 + w4 l(z - z ) - R i(xj - xi)l2 2, 
(14) i ji i=1 i=1 j.Ni which corresponds to solving a linear system. n  2.2 2D Registration In 2D 
registration we want to register a source image I to a target image J. During the registration process, 
the 2D pixel grid of the source image X = {xi . R2, i = 1 . . . n} is deformed to Z = {zi . R2, i = 1 
. . . n} to match the target image. 2.2.1 Matching Energy We de.ne I(x) as the pixel value of the image 
I located at the position x. The matching energy measures the color similarity between the source image 
and the target image wrapped onto the deformed grid Z. n n Ematch (Z) = lI(xi) - J(zi)l2 2 . (15) i=1 
 2.2.2 Prior Energy Similar to the 3D geometry registration, we can use di.erent prior energies that 
can be combined to build more complex priors. First Order Smoothness. In the Lucas-Kanade algorithm [12] 
the deformation is assumed to be constant within a patch around each pixel. This corresponds to the prior 
energy n n n Eprior(Z) = l(zj - xj) - (zi - xi)l2 2 , (16) i=1 j.Ni where Ni is the set of indices of 
the neighbors of xi. Laplacian Smoothness. In the Horn-Schunck algorithm [10] the smoothness of the .ow 
is de.ned using a Laplacian operator n n Eprior(Z) = nl(zi - xi) - 1 (zj - xj)l2 2 , (17) |Ni| i=1 j.Ni 
where |Ni| is the cardinality of Ni. This energy measures for each grid vertex the deviation of its deformation 
from the mean deformation of its neighbors. 2.2.3 Optimization In this section we show, as an example, 
how to optimize the matching energy combined with the laplacian smoothness energy. This is similar to 
the method presented in [10]. The optimization is de.ned as with Z * = arg min Z Ereg(Z) (18) nni=1 
i=1 n n n 1 lI(xi) - J(zi)l2 2 + w2 (zj - xj)l2 2. (19) Ereg(Z) = w1 l(zi - xi) - |Ni| j.Ni To solve 
this optimization we linearize J(.) at the current estimate and solve iteratively n t tt+1 t arg min 
w1 lI(xi) - J(z ) - \J(z )T (z - z )l2 2+ i iii Zt+1 i=1 n n n n 1 t+1 t+1 i - xi)l2 2. l(z - xi) - 
(z (20) w2 i |Ni| i=1 j.Ni  where \J =\Jx \Jy T is the image gradient, with \Jx the image gradient 
in x direction and \Jy the image gradient in y direction. As previously, the minimization can be computed 
by setting the partial derivative to zero, which corresponds to solving a linear system.  2.3 Putting 
It All Together We show how to combine 2D image registration and 3D geometry registration to best utilize 
the data provided by the RGB-D sensor. More speci.cally, we want to register a surface X . R3 with color 
information I, i.e. a texture mapped surface, to a 3D surface Y with corresponding color image J. As 
previously, the source X is deformed to a surface Z. We sample the continuous surface X to obtain a set 
of points X = {xi . X , i = 1 . . . n}. We de.ne their corresponding points on the deformed surface Z 
as Z = {zi . Z , i = 1 . . . n}. The color information of sample point xi is given by I(xi). 2.3.1 Matching 
Energy We formulate the energy measuring the quality of the 2D and 3D alignment as follow n n nn Ematch 
(Z) = w1 lzi - PY (zi)l2 2 + w2 lI(xi) - J(f(zi))l2 2 . (21) i=1 i=1 The .rst term is the matching energy 
presented in Section 2.1. The second term is similar to the 2D matching energy presented in Section 2.2. 
The only di.erence is the additional function f : R3 . R2 that projects a 3D point zi function could 
be a perspective pro jection of the form f(zi) = to the 2D image J. For example this T fzi,x fzi,y . 
zi,z zi,z  2.3.2 Optimization We illustrate 2D/3D registration in the context of a face tracking system 
that combines the 2D/3D matching energy with a 3D blendshape prior. A blendshape representation is a 
linear model de.ned as a set of blendshape meshes B = [b0 , ..., bn] where b0 is the rest pose and bi, 
i > 0 are di.erent expressions. A new expression can be generated as T = b0 + Bd, where B = [b1 - b0 
, ..., bn - b0]. The blendshape model shown below is inspired from Ekmans Facial Action Coding System 
[9]. Realtime face tracking using an RGB-D device can be formulated as a 2D/3D registration of the blendshape 
model to the 2D and 3D data [21]. The registration energy can be formulated as nn Ereg(Z, d, R, t) = 
w1 lzi - PY (zi)l2 2 + w2 lI(xi) - J(f(zi))l2 2+ i=1 i=1 n n nn ||zi - (Bid + b0)||2 lzi - (Rxi + t)l2 
(22) w3 i 2 + w4 2, i=1 i=1 To solve this optimization we linearize J(f(.)) at the current estimate and 
solve iteratively n n n T t+1 t arg min w1 (ni (zi - PY (zi)))2+ Zt+1 ,d i=1 n w2 ni=1 n t t . f (zi 
t) t+1 - z lI(xi) - J(f(zi)) + \J(f(zi))T (z t))l2 2+ .z n nnt+1 t+1 w3 ||z i )||2 2 + +w4 lz Rxi + 
t)l2 2. (23) i - (Bid + b0 i - ( i=1 i=1 T fzi,x fzi,y For a perspective projection f(zi) = we have zi,z 
zi,z n   f fzi,x 0 - .f (zi) = 2 zi,z z i,z . (24) f fzi,y .z 0 - 2 zi,z z i,z As previously, the 
minimization can be computed by solving a linear system. For tracking, another 2D matching energy can 
also be added n nt t+1 Ematch (Zt+1) = lJt(f(zi)) - Jt+1(f(zi ))l2 2 . (25) i=1 This energy enforces 
color consistency over time by measuring the variation of color from the previous image frame Jt to the 
current frame Jt+1 for each zi. In [21], the global rigidity is decoupled. In a .rst step, a 3D rigid 
alignment is performed, and in second step, a 2D/3D alignment of the blendshape model is computed.  
 3 Conclusion We have shown that 2D and 3D registration can be expressed and combined in a com­mon framework. 
Numerous application based on RGB-D devices can bene.t from this formulation that allows to combine di.erent 
priors in an easy manner. References [1] P. Besl and H. McKay. A method for registration of 3d shapes. 
PAMI, 1992. [2] V. Blanz and T. Vetter. A morphable model for the synthesis of 3d faces. Proc. of ACM 
SIGGRAPH, 1999. [3] M. Botsch, M. Pauly, M. Gross, and L. Kobbelt. Primo: coupled prisms for intuitive 
surface modeling. SGP, 2006. [4] S. Bouaziz, M. Deuss, Y. Schwartzburg, T. Weise, and M. Pauly. Shape-up: 
Shaping discrete geometry with pro jections. Comput. Graph. Forum, 2012. [5] S. Bouaziz, Y. Wang, and 
M. Pauly. Online modeling for realtime facial animation. ACM Trans. Graph., 2013. [6] Y. Chen and G. 
Medioni. Object modeling by registration of multiple range images. In ICRA, 1991. [7] T. Cootes and C. 
Taylor. Statistical models of appearance for computer vision, 2000. [8] D. W. Eggert, A. Lorusso, , and 
R. B. Fisher. Estimating 3-d rigid body transfor­mations: a comparison of four major algorithms. Machine 
Vision and Applications, 1997. [9] P. Ekman and W. Friesen. Facial Action Coding System: A Technique 
for the Measurement of Facial Movement. Consulting Psychologists Press, 1978. [10] B. K. P. Horn and 
B. G. Schunck. determining optical .ow : A retrospective. Artif. Intell., 1993. [11] H. Li, B. Adams, 
L. J. Guibas, and M. Pauly. Robust single-view geometry and motion reconstruction. ACM Trans. Graph. 
[12] B. D. Lucas and T. Kanade. An iterative image registration technique with an application to stereo 
vision. IJCAI, 1981. [13] M. Mirza and K. Boyer. Performance evaluation of a class of m-estimators for 
surface parameter estimation in noisy range data. IEEE Transactions on Robotics and Automation, 1993. 
[14] R. A. Newcombe, S. Izadi, O. Hilliges, D. Molyneaux, D. Kim, A. J. Davison, P. Kohli, J. Shotton, 
S. Hodges, and A. Fitzgibbon. Kinectfusion: Real-time dense surface mapping and tracking. ISMAR, 2011. 
[15] I. Oikonomidis, N. Kyriazis, and A. Argyros. Tracking the articulated motion of two strongly interacting 
hands. CVPR, 2012. [16] S. Rusinkiewicz and M. Levoy. E.cient variants of the icp algorithm. 3DIM, 2001. 
[17] O. Sorkine and M. Alexa. As-rigid-as-possible surface modeling. SGP, 2007. [18] R. W. Sumner, J. 
Schmid, and M. Pauly. Embedded deformation for shape manip­ulation. ACM Trans. Graph., 2007. [19] J. 
Tong, J. Zhou, L. Liu, Z. Pan, and H. Yan. Scanning 3d full human bodies using kinects. TVCG, 2012. [20] 
X. Wei, P. Zhang, and J. Chai. Accurate realtime full-body motion capture using a single depth camera. 
ACM Trans. Graph., 2012. [21] T. Weise, S. Bouaziz, H. Li, and M. Pauly. Realtime performance-based facial 
animation. ACM Trans. Graph., 2011. [22] T. Weise, B. Leibe, and L. V. Gool. Accurate and robust registration 
for in-hand modeling. CVPR, 2008. [23] T. Weise, T. Wismer, B. Leibe, and L. Van Gool. In-hand scanning 
with online loop closure. 3DIM, 2009. 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2013</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>2504457</article_id>
		<sort_key>220</sort_key>
		<display_label>Article No.</display_label>
		<pages>8</pages>
		<display_no>22</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>22</seq_no>
		<title><![CDATA[Physically based shading in theory and practice]]></title>
		<page_from>1</page_from>
		<page_to>8</page_to>
		<doi_number>10.1145/2504435.2504457</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2504457</url>
		<abstract>
			<par><![CDATA[<p>Physically based shading is increasingly important in both film and game production. By adhering to physically based, energy-conserving shading models, one can easily create high-quality, realistic materials that maintain their quality in a variety of lighting environments. Traditional "ad-hoc" models have required extensive tweaking to achieve the same result, so it is no surprise that physically based models have increased in popularity, particularly because they are often no more difficult to implement or evaluate. Since last year's course (Practical Physically Based Shading in Film and Game Production, SIGGRAPH 2012), many advances have been made in this field, and once again game and film studios present their latest research and techniques.</p> <p>The course begins with a brief introduction to the physics and mathematics of shading before speakers share examples of how physically based shading models have been used in production. The course introduces new research, explains its practical application in production, and discusses the advantages and disadvantages based on real-world examples.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4193544</person_id>
				<author_profile_id><![CDATA[81504687746]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Stephen]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[McAuley]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Ubisoft Entertainment]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4193547</person_id>
				<author_profile_id><![CDATA[81504687284]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Stephen]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hill]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Ubisoft Entertainment]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4193548</person_id>
				<author_profile_id><![CDATA[81466641907]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Adam]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Martinez]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Sony Pictures Imageworks]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4193549</person_id>
				<author_profile_id><![CDATA[82458862657]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Ryusuke]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Villemin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Pixar Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4193550</person_id>
				<author_profile_id><![CDATA[81490689698]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Matt]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Pettineo]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Ready at Dawn Studios, LLC]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4193551</person_id>
				<author_profile_id><![CDATA[82458838257]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Dimitar]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lazarov]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Treyarch]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4193552</person_id>
				<author_profile_id><![CDATA[82459098457]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>7</seq_no>
				<first_name><![CDATA[David]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Neubelt]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Ready at Dawn Studios, LLC]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4193553</person_id>
				<author_profile_id><![CDATA[82458702857]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>8</seq_no>
				<first_name><![CDATA[Brian]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Karis]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Epic Games, Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4193554</person_id>
				<author_profile_id><![CDATA[81332504075]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>9</seq_no>
				<first_name><![CDATA[Christophe]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hery]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Pixar Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4193545</person_id>
				<author_profile_id><![CDATA[81332504435]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>10</seq_no>
				<first_name><![CDATA[Naty]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hoffman]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[2K]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4193546</person_id>
				<author_profile_id><![CDATA[82458748757]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>11</seq_no>
				<first_name><![CDATA[Hakan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Zap Andersson]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Autodesk Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Physically Based Shading in Theory and Practice SIGGRAPH 2013 Course Notes Course Organizers Stephen 
Hill Stephen McAuley Ubisoft Montreal Presenters H°akan Zap Andersson Autodesk Christophe Hery Pixar 
Naty Hoffman 2K Brian Karis Epic Games Dimitar Lazarov Treyarch Adam Martinez Sony Pictures Imageworks 
David Neubelt Ready at Dawn Studios Matt Pettineo Ready at Dawn Studios Ryusuke Villemin Pixar Course 
Description Physically based shading is transforming the way we approach production rendering, and simplifying 
the lives of artists in the process. By adhering to physically based, energy-conserving models, one can 
easily create realistic materials that maintain their properties under a variety of lighting conditions. 
In contrast, traditional ad-hoc models have required extensive tweaking to achieve the same result. Building 
upon previous incarnations of the course, we present further research and practical advice on the sub 
ject, from .lm and game production. Level of Difficulty: Intermediate Intended Audience Practitioners 
from the videogame, CG animation, and VFX .elds, as well as researchers interested in shading models. 
 Prerequisites An understanding of shading models and their use in .lm or game production. Course Website 
All course materials can be found at http://selfshadow.com/publications/s2013-shading-course Contact 
Address questions or comments to s2013course@selfshadow.com  About the Presenters H°akan Zap Andersson 
is a rendering developer at Autodesk. He previously worked at mental images where his o.cial title was 
Shader Wizard and created many of the most commonly used mental ray shaders in existence today. Zap is 
a native of Sweden, with an engineering degree in elec­tronics and a CAD industry background. He wrote 
his .rst renderer around 1986 for the Swedish ABC80 computer, with a graphics card he hand-wired himself. 
Christophe Hery joined Pixar in June 2010, where he holds the position of Senior Scientist. He wrote 
new lighting models and rendering methods for Monsters University and The Blue Umbrella, and continues 
to spearhead research in the rendering arena. An alumnus of Industrial Light &#38; Magic, Christophe 
previously served as a research and development lead, supporting the facility s shaders and providing 
rendering guidance. He was .rst hired by ILM in 1993 as a senior technical director. During his career 
at ILM, he received two Technical Achievement Awards from the Academy of Motion Pictures Arts and Sciences. 
Naty Hoffman is Vice President of Technology at 2K. Previously he was employed at Activision (working 
on graphics R&#38;D for various titles, including the Call of Duty series), SCEA Santa Monica Studio 
(coding graphics technology for God of War III ), Naughty Dog (developing PS3 .rst-party libraries), 
Westwood Studios (leading graphics development on Earth and Beyond ) and Intel (driving Pentium pipeline 
modi.cations and assisting the SSE / SSE2 instruction set de.nition). Brian Karis is a Senior Graphics 
Programmer at Epic Games, where he works on the renderer for Unreal Engine 4. Prior to joining Epic in 
2012, he was employed at Human Head Studios and created the renderer for Prey 2, focusing on systems 
for virtual texturing, lighting and visibility. Dimitar Lazarov is the Lead Graphics Engineer at Treyarch, 
where he worked on the Call of Duty: Black Ops and Call of Duty: Black Ops II titles. He has over a decade 
of experience in game de­velopment and has contributed to a diverse portfolio of games, ranging from 
kids friendly titles such as Casper Spirit Dimensions and Kung Fu Panda, to action blockbusters such 
as Medal of Honor: European Assault, True Crime: New York City and Transformers: Revenge of the Fallen. 
Dimitar s main expertise is graphics programming and performance optimizations, and he is often involved 
in system and core engineering, tools programming and other areas that need his attention to detail. 
Adam Martinez is a Shader Writer for Sony Pictures Imageworks and a member of the Shading Department, 
which oversees all aspects of shader writing and production rendering at Imageworks. He is a pipeline 
developer, look development artist, and technical support liaison for productions at the studio. He was 
also one of the primary architects of Imageworks rendering strategy behind 2012 and Alice In Wonderland. 
David Neubelt has served as a Lead Graphics and Engine programmer at Ready at Dawn Studios since 2005, 
where he has shipped multiple PSP God of War titles, Daxter, and God Of War: Origins Collection for PS3. 
Most recently, he has helped shape their next-generation engine from its inception, contributing in many 
areas, including the development of production BRDFs and their 3D material scanning pipeline. Matt Pettineo 
is a Lead Graphics and Engine programmer at Ready at Dawn Studios, where he has worked since 2009, helping 
to develop a physically based shading model and material authoring pipeline for use in their upcoming 
title. He also focuses on hardware development and optimization for next-generation consoles. Ryusuke 
Villemin began his career at BUF Compagnie in 2001, where he co-developed BUF s in­house raytracing renderer. 
He later moved to Japan at Square-Enix as a rendering lead to develop a full package of physically based 
shaders and lights for mental ray. After working freelance for several Japanese studios, he joined Pixar 
in 2011 as a TD. Presentation Schedule 09:00 09:05 Introduction (Hill) 09:05 09:20 Background: Physics 
and Math of Shading (Ho.man) 09:20 09:40 Getting More Physical in Call of Duty: Black Ops II (Lazarov) 
09:40 10:00 Real Shading in Unreal Engine 4 (Brian Karis) 10:00 10:30 Crafting a Next-Gen Material Pipeline 
(Neubelt and Pettineo) 10:30 10:45 Break 10:45 11:10 Everything You Always Wanted to Know About mia material* 
 (* But Were Afraid to Ask) (Andersson) 11:10 11:35 OSL The Great and Powerful (Martinez) 11:35 12:15 
Physically Based Shading at Pixar (Hery and Villemin) Abstracts Background: Physically-Based Shading 
Naty Ho.man We will go over the fundamentals behind physically based shading models, starting with a 
qualitative description of the underlying physics, followed by a quantitative description of the relevant 
mathemat­ical models, and .nally discussing how these mathematical models can be implemented for shading. 
 Getting More Physical in Call of Duty: Black Ops II Dimitar Lazarov This talk will cover a number of 
improvements and optimizations made to our physically based shading approach since Call of Duty: Black 
Ops (as presented at SIGGRAPH 2011). In particular, we derived a new environment map Fresnel formulation, 
that is signi.cantly closer to ground truth ray-traced simulations. Additionally, we improved our environment 
map normalization strategy, which helped to ensure matching re.ections across varying geometry and baked 
lighting representations. We also implemented a new cosine-power-based environment map pre-.ltering technique, 
which contributed to a more uniform gloss response between point lights and environment maps. Last but 
not least, we further optimized all of our physically based shading math, which resulted in a ten percent 
overall shader speedup. Real Shading in Unreal Engine 4 Brian Karis Unreal Engine 4 has always had the 
basics of physically based shading. Last year we felt that we could make ma jor work.ow and quality improvements 
in how we authored materials, by layering and blending pre-made materials from a library instead of authoring 
components separately and redundantly for every use (e.g. di.use and specular textures). This in turn 
motivated a deeper review of our material and shading models, not only with the aim of increasing physical 
accuracy, but also to minimize the number of parameters without limiting expressiveness. The work Disney 
presented in this course last year provided the perfect starting point for our solution. I will discuss 
how we reduced the number of material parameters of Disney s model and simpli.ed it for real-time use 
in UE4. I will show how an importance-sampled IBL version of this shading model can be approximated using 
pre-.ltering. I will also discuss our experiences, the limitations that either we or our licensees have 
encountered and the results we have achieved with it. Finally, I will present an energy-conserving analytical 
area light model and show how the concepts used can be expanded to other light shapes such as capsules 
and rectangles. Crafting a Next-Gen Material Pipeline David Neubelt and Matt Pettineo Physically based 
shading has already made strong inroads into current-generation console and PC game development, where 
it has proven its merit as a practical methodology for producing materi­als with a higher degree of realism. 
With the jump in graphics processing capabilities promised by next-generation consoles, we are presented 
with an exciting opportunity to expand to more complex and robust re.ectance models. However, transitioning 
to Cook-Torrance and other physically based shading models can bring its own set of unique challenges 
that require practical solutions in order to simultaneously ensure quality, performance and production 
scalability. This talk will cover some of the issues that we ve encountered while developing a physically 
based shading model at Ready at Dawn Studios, and will also provide an overview of the solutions that 
were developed for these problems. First, we will discuss the development of our .lm-inspired material 
compositing and inheritance pipeline. Compositing enables material artists to carefully choose parameters 
for pure base materials such as gold or concrete that typically have few spatially varying properties. 
These base material parameters are then automatically converted into texture maps, and composited with 
other materials using blend maps (specifying material boundaries). The result is that rich materials 
with high degrees of spatial variance can be produced without artists needing to hand-paint albedo or 
roughness maps. Additionally, our artists wanted to create a work.ow for easily feeding our material 
pipeline di.erent textiles they acquired from costume designers and trade events. We developed a custom 
hardware acquisition device to scan in materials to acquire their albedo, normal, depth, and roughness 
maps. This allows us to create complex and highly detailed materials at a fraction of the cost it would 
take to author by hand. Finally, we were inspired by last year s SIGGRAPH talks on physically based shading 
and aliasing. We then asked the question: How can we control specular aliasing without modifying the 
work we ve done to our BRDFs? , since many solutions for aliasing are currently tied to a particular 
BRDF. We will present the results of our research into existing techniques for reducing aliasing of the 
specular term caused by undersampling in the pixel shader, as well as how we adapted them to maintain 
the integrity of our custom BRDFs. Everything You Always Wanted to Know About mia material H°akan Zap 
Andersson Development of the mental ray shader mia material started in 2006, with the express goal of 
helping users render with a more physical shading model. It has since gone on to be widely used in architec­tural 
visualization and blockbuster movies, become the root of all the standard Autodesk materials used throughout 
the product line, and is probably one of the most ubiquitous shaders on the planet, having had its feature 
set cloned by many a renderer developer. This talk will start by discussing the background of the shader, 
why it was made the way it was, and where the inspiration came from for some of its behavior. We will 
decipher the rationale behind the model of layering and energy conservation. We will then continue with 
some insights into the internals, and where it quite intentionally deviates from some standard practices 
within physical rendering, and why. We will touch both upon technical questions (e.g. What kind of glossiness 
is used? and What do certain values mean? ) as well as philosophical (e.g. Why does most glossiness models 
look unrealistic? and What perceptual quirks makes a user turn the wrong knob? ). We will also reveal 
some deep, dark secrets. Finally, we will end on some thoughts for the future: where to go from here, 
what would have been done di.erently today, and where the challenges in physical rendering lie going 
forward. OSL The Great and Powerful Adam Martinez Completion of the visual e.ects work on Disney s Oz 
The Great and Powerful (2013) represents a milestone in the development of OSL at Sony Pictures Imageworks. 
In addition to being the largest show in sheer shot count, it also contains the widest variety of character, 
creature, environment and e.ects work that been undertaken in the OSL shading pipeline to date. This 
talk will focus on Imageworks use of OSL on the show. For instance, we leveraged the .exibility and rapid 
shader development turnaround of the OSL work.ow to create a more e.ective skin shading solution. While 
previous skin shading techniques were largely the responsibility of individual productions, we built 
a generalized facility-level solution based on Eugene d Eon s work on Gaussian subsurface scattering 
di.usion pro.les. We also quickly and successfully integrated and deployed new hair shading techniques 
in the middle of production, without disruption. This allowed us to bring all of the subtleties of our 
image-based illumination pipeline to bear on hair, as e.ectively as on other surfaces. All of these techniques 
were employed for Finley the endearing .ying monkey who I will be using as a demonstration case for each 
topic. Additionally, volumetric e.ects played a signi.cant role in this production, so I will discuss 
some of the speci.cs of working with volumes in OSL. Both the talk and the accompanying course notes 
will provide practical examples of writing OSL shaders, building upon content from the SIGGRAPH 2012 
course. Physically Based Shading at Pixar Christophe Hery and Ryusuke Villemin Over the past few years, 
the shading system at Pixar has been rewritten to give artists a powerful but simple solution for lighting. 
The new system developed originally for the show Monsters University aims to provide good images right 
out of the box, whilst using ray-tracing throughout to minimize the number of caches or precomputations. 
It consists of three main parts: Lights: real, physically correct, sampled area lights  BRDFs: physically 
correct, energy conserving BRDF models, describing how light interacts with ob ject surfaces  Integrators: 
functions that combine lights and BRDFs to produce the .nal color  The .rst half of the talk will describe 
these three parts, and how they interact, passing information between them. In the second half, we will 
take the examples of the sphere area light, Beckmann BRDF and the direct integrator, and discuss how 
those are e.ectively implemented at Pixar in prman using co-shaders.  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2013</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>2504458</article_id>
		<sort_key>230</sort_key>
		<display_label>Article No.</display_label>
		<pages>88</pages>
		<display_no>23</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>23</seq_no>
		<title><![CDATA[Rendering massive virtual worlds]]></title>
		<page_from>1</page_from>
		<page_to>88</page_to>
		<doi_number>10.1145/2504435.2504458</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2504458</url>
		<abstract>
			<par><![CDATA[<p>This course is presented in four sections. The first two presentations show how huge data sets can be streamed and displayed in real time for virtual-globe rendering inside a web browser. Topics include pre-processing, storage, and transmission of real-world data, plus cache hierarchies and efficient culling algorithms.</p> <p>The third section reviews content generation using a combination of procedural and artist-driven techniques. It explores integration of content-generation applications into production tool chains and their use in creation of real-world video games. Topics include productivity, data dependencies, and the trade-offs of putting massive procedural content generation into production.</p> <p>The fourth section covers recent advances in graphics hardware architecture that allow GPUs to virtualize graphics resources (specifically, textures) by leveraging virtual memory. It discusses augmentation of traditional graphics APIs and presents several use cases and examples.</p> <p>The final presentation shows how support for hardware-assisted virtual texturing was integrated into a game engine. It reviews the challenges associated with ensuring that the engine continued to operate efficiently on hardware that does not support virtual texturing. It also illustrates the concessions made in the engine for limitations of existing hardware and proposes some future enhancements that would improve the usability of the solution.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4193555</person_id>
				<author_profile_id><![CDATA[81474698573]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Graham]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sellers]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Advanced Micro Devices, Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4193556</person_id>
				<author_profile_id><![CDATA[81335495702]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Juraj]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Obert]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Advanced Micro Devices, Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4193557</person_id>
				<author_profile_id><![CDATA[81466641240]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Patrick]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Cozzi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Pennsylvania]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4193558</person_id>
				<author_profile_id><![CDATA[81486649338]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Kevin]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ring]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Analytical Graphics, Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4193559</person_id>
				<author_profile_id><![CDATA[81442598009]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Emil]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Persson]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Avalanche Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4193560</person_id>
				<author_profile_id><![CDATA[82459103357]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Joel]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[de Vahl]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Avalanche Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4193561</person_id>
				<author_profile_id><![CDATA[81504687994]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>7</seq_no>
				<first_name><![CDATA[J.]]></first_name>
				<middle_name><![CDATA[M. P.]]></middle_name>
				<last_name><![CDATA[van Waveren]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Id Software, LLC]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[{Akeley90} Kurt Akeley. The Hidden Charms of the z-Buffer. IRIS Universe, 1990.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1283904</ref_obj_id>
				<ref_obj_pid>1283900</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[{Akeley06} Kurt Akeley and Jonathan Su. <u>Minimum Triangle Separation for Correct z-Buffer Occlusion</u>. Graphics Hardware, 2006]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[{Baker99} Steve Baker. <u>Learning to Love Your z-Buffer</u>. 1999]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2031444</ref_obj_id>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[{Cozzi11} Patrick Cozzi and Kevin Ring. <u>3D Engine Design for Virtual Globes</u>. A K Peters, Ltd., 2011]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Persson, E., 2010. Making it large, beautiful, fast and consistent --- Lessons learned developing Just Cause 2. GPU Pro.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Persson, E., 2013. Practical Clustered Shading. Advances in Real-Time Rendering in Games, http://advances.realtimerendering.com/(to appear)]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 RENDERING MASSIVE VIRTUAL WORLDS SIGGRAPH 2013 Courses Anaheim, California, July 21-25 2013 Graham 
Sellers Advanced Micro Devices J.M.P. van Waveren ID Software Patrick Cozzi, Kevin Ring Analytical Graphics 
Emil Persson, Joel de Vahl Avalanche Studios Abstract In recent years, connectivity of systems has meant 
that online worlds with huge stream­ing data sets have become common and widely available. From applications 
such as map rendering and virtual globes to online gaming, users expect online content to be presented 
in a timely and seamless manner, and expect the volume and variety of ofine content to match that which 
is available online. Generating, retrieving and displaying this content to users presents a number of 
considerable challenges. This course addresses some real-world solutions to the problems presented in 
the rendering of massive virtual worlds. Massive worlds present many daunting challenges. In our frst 
talk, we introduce the two most prominent challenges: data management and rendering artifacts. \e look 
at handling massive datasets like real-world terrain with out-of-core rendering, including parallelism, 
cache hierarchies on the client and in the cloud, and level-of-detail. Then we explore handling jittering 
artifacts due to performing vertex transform on large world coordinates with 32-bit precision on commodity 
GPUs, and handling z-fghting artifacts due to lack of depth bufer precision with large near-to-far distances. 
This serves as an introduction to the following talk, \orld-Scale Terrain Rendering. Rendering small 
and medium scale terrain is fairly straightforward these days, but ren­dering terrain for a detailed 
world the size of Earth is much more challenging. In our second talk, we discuss the design and implementation 
of a terrain engine that can render a zoomed­out view of the entire globe all the way down to a zoomed-in 
view where sub-meter details are visible. \e discuss processing "of the shelf" terrain data for efcient 
streaming and render­ing, an asynchronous load pipeline for bringing chunks of terrain through a cache 
hierarchy, efciently culling chunks of terrain that are below the horizon, driving terrain level-of-detail 
selection based on an estimate of pixel error, and more. \ith these techniques, we are able to achieve 
excellent performance even in the constrained environment of \ebGL and JavaScript running inside a web 
browser. Once we have dealt with the topic of storing and streaming huge amounts of content, we must 
next contend with production and generation of that content. The next talk will go through the production 
pipeline at Avalanche Studios and the issues encountered when flling Just Cause 2 with interesting content. 
It is mix of horror stories, good practices, and lessons learned and applied to titles currently in production. 
Issues discussed will include the reliability problems for the content tool-chain, the long turn-around 
times we had, un­documented and poorly understood data dependencies, and all the problems that followed 
with these. Then we will cover how we have solved these problems for our current content pipeline. \e 
will also talk about our approach to authoring the landscape, vegetation and locations for our large 
game worlds, and how we maintain productivity without sacrifcing variation. Many of the concepts discussed 
to this point address efcient generation, storage, retrieval and transmission of content. Recent advances 
in graphics hardware allow GPUs to assist in functions such as streaming texture data, managing sparse 
data sets and providing reasonable visual results in cases where not all of the data is available to 
render a scene. In the next talk, we take a deep-dive into AMD's partially resident texture hardware, 
briefy cover sparse texture extensions for OpenGL and then explore some use cases for the hardware and 
software features, including some live demos. In our fnal presentation, we discuss the practical challenges 
of integrating support for hardware virtual texturing into a real-world game engine, idTech5, which powers 
RAGE and a number of other titles. \e will describe cases where hardware virtual texturing 'just worked', 
and cases where more efort was required to integrate the technology into an existing engine, whilst maintaining 
support for software virtual texturing without loss of performance or features. \e assume that course 
participants are familiar with modern graphics rendering tech­niques, data compression, cache hierarchies 
and graphics hardware acceleration. \e will discuss in some detail virtual memory systems, culling techniques, 
level-of-detail selection and other related techniques. About the Authors Graham Sellers Advanced Micro 
Devices graham.sellers@amd.com Graham Sellers is the manager of the OpenGL driver team and a software 
architect at AMD. He represents AMD at the OpenGL ARB and Khronos Group and is responsible for the design 
and delivery of new features in AMD's OpenGL implementation, including extensions and new versions of 
the OpenGL API. He has authored over 20 OpenGL extensions, many of which are now part of the core API 
specifcation. He is also co-author of the OpenGL SuperBible and the OpenGL Programming Guide. He holds 
a Masters' degree in Engineering from the University of Southampton, UK. Patrick Cozzi Analytical Graphics, 
Inc. and University of Pennsylvania pjcozzi@siggraph.org Patrick Cozzi is coauthor of 3D Engine Design 
for Virtual Globes, coeditor of OpenGL Insights, and a contributor to GPU Pro 4, Game Engine Gems 2, 
and SIGGRAPH. At Analytical Graphics, Inc., he leads the graphics development of Cesium, a \ebGL virtual 
globe. He teaches GPU Programming and Architecture at the University of Pennsylvania, where he received 
a master's degree in computer science. Kevin Ring Analytical Graphics, Inc. kevin@kotachrome.com Kevin 
Ring is coauthor of 3D Engine Design for Virtual Globes and the lead architect of STK Components at Analytical 
Graphics, Inc. In recent years, he has immersed himself in the problem of massive terrain rendering and 
analysis while developing the terrain and imagery rendering engine for Cesium, a \ebGL virtual globe. 
Kevin received a bachelor's degree in Computer Science from Rensselaer Polytechnic Institute. Emil Persson 
Avalanche Studios emil.persson@avalanchestudios.se Emil Persson is the Head of Reseach at Avalanche Studios, 
where he is conducting forward­looking research, with the aim to be relevant and practical for game development, 
as well as setting the future direction for the Avalanche Engine. Previously, Emil was an ISV Engineer 
in the Developer Relations team at ATI/AMD. He assisted tier-1 game developers with the latest rendering 
techniques, identifying performance problems and applying optimizations. He also made major contributions 
to SDK samples and technical documentation. Joel de Vahl Avalanche Studios joel.de.vahl@avalanchestudios.se 
Joel de Vahl works as a Senior Engine Programmer at Avalanche Studios, focusing primarily on graphics 
and engine technology development. Previously, Joel worked as an Engine Programmer at Starbreeze Studios, 
focusing on lighting and rendering technology. J.M.P. van Waveren ID Software mrelusive@idsoftware.com 
J.M.P. van \averen studied computer science at Delft University of Technology in the Netherlands. He 
has been developing technology for computer games for over a decade and has been involved in the research 
for, and development of various triple-A game titles such as: Quake III Arena, Return to Castle \olfenstein, 
DOOM III and RAGE. Course Outline 10 minutes: Introduction Graham Sellers 1. Introduction to the course 
and its goals, course overview and introduction of all speak­ers 2. Introduction to virtual world rendering 
 30 minutes: Handling Planetary Scale Datasets Patrick Cozzi 1. An overview of handling massive datasets 
such as real-world terrain with out-of-core rendering, including parallelism, cache hierarchies on the 
client and in the cloud, and level-of-detail. 2. Handling jittering artifacts caused by performing vertex 
transform on large world co­ordinates with 32-bit precision on commodity GPUs, and handling z-fghting 
artifacts due to lack of depth bufer precision with large near-to-far distances.  30 minutes: World-Scale 
Terrain Rendering Kevin Ring 1. Processing and organizing data for efcient streaming to the client system: 
tile pyra­mids, mesh simplifcation. 2. Selecting the subset of terrain to render in a given view by 
estimating screen-space error. 3. Mapping terrain to an accurate model of the Earth, like the \GS84 
ellipsoid. 4. Culling terrain, especially terrain that is below the horizon. 5. The client-side pipeline 
for loading and preparing terrain. 6. Using the imagery that's available, even when it's not ideal, 
including: (a) Re-projecting web Mercator imagery on the GPU. (b) Rendering multiple textures to cover 
a given region of terrain.   30 minutes: Populating a Massive Game World Emil Persson, Joel de Vahl 
1. Filling a massive game (Just Cause 2) with interesting content. 2. Content production and pipeline 
issues associated with massive procedural content generation. 3. Authoring landscapes and placing vegetation 
and other entities. 4. Maintaining productivity without sacrifcing variety.  30 minutes: Hardware Virtual 
Texturing Graham Sellers 1. Hardware architecture of AMD's 'partially resident texture' support 2. Driver 
support and software API 3. Use cases and technical demos 4. Future development  30 minutes: High 
Quality Software and Hardware Virtual Textures J.M.P. van Waveren 1. Virtual texturing in idTech5 (RAGE) 
 2. Integrating support for hardware virtual texturing in idTech5 3. Supporting trilinear and anisotropic 
fltering in hardware and software 4. Addressing practical limitations to hardware virtual texturing 
 10+ minutes: Conclusion and Discussion All speakers Contents 1 Introduction 10 1.1 Virtual \orlds 
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10 2 Planetary Scale Datasets 12 
 3 World-Scale Terrain Rendering 44 3.1 Introduction -Massive worlds render massive quantities of terrain 
. . . . . . 44 3.2 Organizing and processing terrain data . . . . . . . . . . . . . . . . . . . . . 
45 3.3 Tile selection for rendering . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48 3.4 
Life of a tile . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51 3.4.1 Create 
imagery skeletons . . . . . . . . . . . . . . . . . . . . . . . . . 51 3.4.2 Request . . . . . . . . 
. . . . . . . . . . . . . . . . . . . . . . . . . . 53 3.4.3 Transform . . . . . . . . . . . . . . . 
. . . . . . . . . . . . . . . . . . 53 3.4.4 Create resources . . . . . . . . . . . . . . . . . . . 
. . . . . . . . . . . 54 3.4.5 Replacement . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
. . . 54 3.5 Terrain and imagery shaders . . . . . . . . . . . . . . . . . . . . . . . . . . . 55 3.6 
Horizon culling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56 3.7 Map repro 
jection on the GPU . . . . . . . . . . . . . . . . . . . . . . . . . . 59 3.8 Acknowledgements . . . 
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60 4 Populating a Massive Game World 61 4.1 
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61 4.2 Build system 
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61 4.2.1 Issues with the content 
build system . . . . . . . . . . . . . . . . . . 61 4.2.2 Untangling the mess . . . . . . . . . . . 
. . . . . . . . . . . . . . . . 62 4.2.3 Code build system . . . . . . . . . . . . . . . . . . . . . 
. . . . . . . 62  4.3 Content Production . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
. . 64 4.3.1 Modularization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64 4.3.2 Landscape 
authoring . . . . . . . . . . . . . . . . . . . . . . . . . . . 64  4.4 Bringing it to life . . . . 
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66 4.4.1 \orld simulation . . . . . . . . 
. . . . . . . . . . . . . . . . . . . . . 66 4.4.2 Landmarks . . . . . . . . . . . . . . . . . . . . 
. . . . . . . . . . . . 67 4.4.3 Lighting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
. . . . . 67  4.5 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
69 4.6 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69 5 Hardware 
Virtual Texturing 70 5.1 Virtual Memory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
. . . . 70 5.2 Page/Tile Residency Information . . . . . . . . . . . . . . . . . . . . . . . . 75 5.3 
Sparse Texture Use Cases &#38; Future Development . . . . . . . . . . . . . . . 75 5.3.1 Very Large 
Texture Arrays . . . . . . . . . . . . . . . . . . . . . . . . 75 5.3.2 Incomplete Mip-map Chains . 
. . . . . . . . . . . . . . . . . . . . . . 77 5.3.3 Truly Sparse Textures . . . . . . . . . . . . . 
. . . . . . . . . . . . . 78  5.4 Current Limitations and Thoughts on the Future . . . . . . . . . 
. . . . . . 78 6 High Quality Software and Hardware Virtual Textures 80 6.1 High Quality Software Virtual 
Textures . . . . . . . . . . . . . . . . . . . . . 80 6.1.1 Explicit Page Table LOD . . . . . . . . 
. . . . . . . . . . . . . . . . . 81 6.1.2 Tri-Linear Filtering and LOD Clamping . . . . . . . . . . 
. . . . . . 82 6.1.3 Texture Upsampling . . . . . . . . . . . . . . . . . . . . . . . . . . . 83 6.1.4 
Direct Texture Access . . . . . . . . . . . . . . . . . . . . . . . . . . 84  6.2 Hardware Virtual 
Textures . . . . . . . . . . . . . . . . . . . . . . . . . . . . 84 6.2.1 PRT Page Management . . . 
. . . . . . . . . . . . . . . . . . . . . . 85 6.2.2 PRT Size Limitation . . . . . . . . . . . . . . 
. . . . . . . . . . . . . 85  6.2.3 Compressed PRT Pages . . . . . . . . . . . . . . . . . . . . . . 
. . . 86 6.2.4 Borderless PRT Pages . . . . . . . . . . . . . . . . . . . . . . . . . . 86 Chapter 
1    Introduction This course material covers the practical details involved in rendering massive 
virtual worlds. At SIGGRAPH 2012, speakers Obert, van \averen and Sellers presented an overview of hardware 
accelerated virtual texturing and how it was used in the video game RAGE, by id Software. Expanding on 
the topic, this course covers more diverse sub jects such as con­tent generation and creation, streaming 
and out-of-core rendering, and the practicalities of integrating very large virtual texture and geometry 
datasets into production pipelines and real-world rendering applications. 1.1 Virtual Worlds In this 
context, we use the term virtual to mean data that is pageable, incomplete or where some or all of it 
is inaccessible at any given point in time. In general, these types of dataset fall into one of a few 
categories: Procedurally generated worlds consist of data that is created algorithmically, perhaps using 
an artist guided process either during production or just-in-time. Such data sets represent an overall 
design goal rather than something that must be conveyed with a great deal of accuracy.  Real datasets 
such as satellite imagery, geospatial and survey data must be presented to the user with high fdelity 
and renderings should faithfully convey the world from which they were recorded.  Artist generated data 
sets may not be as large as real-world data sets such as GIS data, but must nonetheless be stored, transmitted 
and rendered with accuracy demanded by the artist who authored them. Thus they present similar attributes 
and challenges as any other large data set.  Regardless of the source of the data set, similar challenges 
are encountered during their interactive or real-time rendering. For example, these data sets generally 
do not ft into a graphics processor's on-board memory and may not be entirely present in readily accessible 
form on the end-user's machine. Some or all of the data may be available over a network, or perhaps be 
stored locally in a heavily compressed form. To further complicate matters, generation, pre-processing, 
authoring, managing and stor­ing the volume of data required by modern virtual environment rendering 
applications is non-trivial. Such data sets can easily consume many terabytes of data when uncompressed. 
The number of discrete entities in a single virtual world can be very high -typically many tens of thousands, 
and perhaps into the millions. Clearly, having an artist design and place each individual tree, shrub 
and fower in a virtual forest is intractable. Therefore, we must fnd methods to generate huge numbers 
of entities, placing them semi-automatically, whilst allowing an artist to guide the process and edit 
the fnal results.  Chapter 2 Planetary Scale Datasets \e're going to look at some fundamental problems 
when rendering massive worlds. These problems are interesting -or perhaps even annoying -in that a typical 
graphics engine may never need to deal with them. However, when we scale our world up to a massive size 
and still want fne-grained detail, precision problems start to manifest themselves as rendering artifacts. 
Over the years we've seen many forum posts like "I wrote a simple terrain engine and everything was great, 
but then I tried to cover world-wide terrain, and now there is z­fghting. \hy?" and "I'm rendering the 
Earth using meters as units, and when I zoom in, static objects start to bounce around. \hy?" In this 
talk, we intuitively explain why, and present robust solutions that work for open worlds and make very 
few assumptions, e.g., we can't necessarily put fog in the distant or ask artists to design a level such 
that the maximum view distance is only so far. \e've used these solutions in our 3D tools at Analytical 
Graphics, Inc. with great success. Some of them are well known and have been in use literally since I 
was in high school. Others are more recent. In both cases, I want to motivate their use and provide implementation 
tips. In our work at AGI, we simulate the real-world, from ground stations surrounded by high-resolution 
terrain and sub-meter imagery... ... to satellites in high-earth orbit at 40,000 km above the Earth... 
... to interplanetary missions to the moon or mars.  \e've even helped NORAD with the very important 
job of tracking Santa on Christmas Eve. These use cases require massive scale in terms of view distances 
and precision for coor­dinate systems. For example, we may be zoomed in close to a satellite orbiting 
the Earth, and still want to see the sun in the distance. Satellites have solar panels that point towards 
the sun. A lot of classic tricks like eliminating z-fghting by introducing fog in the distance to preserve 
a sane far-to-near ratio won't always work for us. For example, some of our users want to see all the 
active unclassifed satellites in space at the same time. Today, I'm going to present rendering techniques 
we use to deal with these type of scale while still allowing fne-grained detail. In particular, we are 
going to solve two types of rendering artifacts: z-fghting  jittering  The presented solutions are 
very general and apply to geospatial visualization like work done by Esri and virtual globes like Google 
Earth and NASA \orld \ind. They also apply to massive-world games. \e'll start by looking at examples 
of z-fghting and jittering, then we'll look at the cause of each, and solutions. Here we have a Shadow 
UAV fying a mission through the mountains. There are no z-fghting artifacts in this images, but if we 
use a close near plane and a very distant far plane, we see z-fghting artifacts like these... There 
is z-fghting throughout the distant mountains. I circled a few of the major areas. Z-fghting is even 
more irritating when things are moving as shown in the video [videos/z­fghting.wmv]. I promise that our 
software doesn't do this by default; I hacked it to make the video. Notice how there is no z-fghting 
closer to the viewer, for example, with the UAV or nearby foothills. The other artifact we're going 
to look at today is jitter. Here we have the Landsat 7, which is an imaging satellite in orbit at about 
700,000 m above Earth. \hat's the diferent between the top image and the bottom image? In both images, 
the billboard and text have the same world position. However, a small change in the viewer position resulted 
in the white polyline representing the satellite's orbit, to bounce -or jitter -as can be seen at the 
billboard, where the orbit on the top is above the billboard, and the orbit on the bottom is below. (This 
is also noticed where the orbit overlaps the yellow orbit on the right). Like z-fghting, jittering is 
most noticeable and irritating when things are moving. In this video [videos/jitter.wmv], we see a red 
polyline that looks innocent enough. As we zoom in and rotate, it starts to bounce around. This polyline 
represents the boundary for Pennsylvania, and it is defned in \GS84, which is a coordinate system that 
has an origin at the center of the Earth. Now we zoom out and see the Landsat 7 satellite again. \ith 
the video it is easy to see the polyline bounce up and down. For this video, I modifed our engine to 
show jitter for the polyline, but not for the billboard and text. Now let's take a closer look at z-fghting 
in massive worlds and solutions. The classic z-fghting example is a piece of paper on a desk. If the 
paper and the desk are coplanar, we can see z-fghting artifacts due to foating-point round-of error. 
This particular case can be solved many ways including: Slightly adjusting the geometry, either when 
authoring or in a view-dependent way at runtime with a call like polygonOfset1 .  Stenciling. Stencil 
out the paper. Render the desk. Render the paper.  However, z-fghting can occur when geometry is not 
coplanar, even when geometry is far apart -in some sense. This is due to the non-linear relationship 
between z in eye coordinates and z in window coordinates when using a perspective projection. 1 http://www.opengl.org/archives/resources/faq/technical/polygonoffset.htm 
 The relationship between z-eye and z-window is controlled by the far-to-near ratio. The bigger the 
ratio the greater nonlinearity. \hy? \hen using perspective, if we multiply out the model-view-projection 
matrix, perspective division, and viewport transform, z-window becomes: Z-window = ((((f + n) / (f ---n)) 
+ (2fn / z-eye(f ---n))) + 1) / 2 (in the common case when the depth range near is zero and far is one). 
For a given perspective pro jection, everything on the right-hand side is constant except for z-eye, 
therefore z-window is proportional to 1/z-eye. This is the result of the perspec­tive divide, which causes 
perspective foreshortening, making objects in the distance appear smaller. As we can see in the graph, 
a small change in z-eye when z-eye is small results in a big change in 1/z-eye because 1/z-eye is a quickly 
moving function here. That same change in z-eye when z-eye is large results in a smaller change to 1/z-eye 
because 1/z-eye is slowly moving then. In the later case, a small change to z-eye may yield the same 
z-window, creating rendering artifacts. The relationship between z-window and z-eye also depends on 
the near and far distances as shown in this graph. The x-axis is the distance from the near plan in eye 
coordinates, from 0 to 100. The y-axis shows z-window from 0 to 1. In all cases the view distance (f 
-n) is 100. Here we see that as n gets smaller, and hence the far-to-near ratio gets bigger, the precision 
gets pushed to the near plane. A ratio of 1,000 is commonly considered acceptable for a 24-bit fxed-point 
depth bufer [Akeley90]. In practice, we've gotten away with higher values depending on how far apart 
geometry is. However, 1,000 or even 10,000 is not going to cut it for our massive worlds where Earth's 
semiminor axis alone is over 6,300,000 meters. In the video [videos/zfghtingcauses.wmv], we keep pushing 
the near plane out to elimi­nate z-fghting between the plane and the globe. As we zoom in, we do not 
see z-fghting because precision gets better; however, if we zoom out, z-fghting will occur again. Code: 
https://github.com/virtualglobebook/OpenGlobe (see Chapter06DepthBuferPrecision) The minimum triangle 
separation, s-min, for a distance from the eye, z-eye, is the mini­mum world-space separation between 
two triangles required for correct depth occlusion. [Baker99] provides an approximation to s-min for 
an x-bit fxed-point depth bufer: S-min = (z-eye z-eye) / ((2^x)n ---z-eye) * [Akeley06] also show that 
window coordinate precision, feld of view, and error accu­mulated by single-precision projection, viewport, 
and rasterization arithmetic contribute to efective depth bufer resolution.  \ith a far-to-near ratio 
of 1,000, only a few frustums are needed to cover a large view distance. Three frustums can cover from 
1 meter to a 1 billion meters. Each frustum has the same feld-of-view and aspect ratio, but a diferent 
near and far plane. Given that most objects will only overlap a single frustum, this can be more efcient. 
Let's aim to: Use the fewest frustums possible based on the needs of the current view, not a fxed near 
and far plane  Minimize the number of ob jects overlapping more than one frustum (by pushing out the 
near plane -and having a larger initial frustum)  Minimize CPU overhead Let's improve on this implementation. 
  The scene produces commands for the renderer to execute. In other literature, a command is also called 
a "draw call" or a "batch." Commands may come from the terrain engine or other entitles like 3D models, 
billboards, or polylines. A command has everything needed to execute a draw call using the underlying 
graphics API -\ebGL in my case. Our engine has both DrawCommands for issuing drawEle­ments/drawArrays 
calls and ClearCommands for clearing the framebufer. Here, we're just concerned with DrawCommands, which 
have: Vertex array, ofset, and count  Shader program and its uniforms, including a model matrix  Render 
state that defnes the fxed-function state of the pipeline  Framebufer, which is the target of the draw 
call  Strictly speaking, the bounding volume is not used for issuing the draw call, but it is needed 
for determining what frustums a command belongs to. Any bounding volume could work, but we use spheres 
everywhere for now. Code: https://github.com/AnalyticalGraphicsInc/cesium/blob/b16/Source/ Renderer/DrawCommand.js 
 The near and far plane distances can varying frame-to-frame. The farther we can push out the near plane 
and the closer we can bring in the far the plane, the fewer frustums we will have. So instead of just 
using the application-defned near and far distances, we compute them dynamically using each command's 
bounding volume and the application­defned distances as extremes. In our engine, we do both frustum culling 
and horizon culling, which is occlusion culling with the ellipsoid, as Kevin will discuss in the following 
talk. Code: createPotentiallyVisibleSet() in https://github.com/AnalyticalGraphicsInc/cesium/blob/b16/Source/Scene/Scene. 
js Code: updateFrustums() in https://github.com/AnalyticalGraphicsInc/cesium/blob/b16/Source/Scene/Scene. 
js  Now that we know the length of each frustum, we can walk through the commands again, and assign 
to them frustums that they overlap. As we'll see some commands will wind up in more than one frustum. 
So far, the renderer has made two passes over commands -the frst pass culls and computes the frustums, 
and the second pass assigns commands to frustums. A typically application will have 100s to 1,000s of 
commands so it would be nice to be able to do this in a single pass. \e can by exploiting temporal coherence. 
It is very likely that the frustums computed in the previous frame can be used in the current frame. 
\e defne our acceptance criteria as: desired near >= previous near  desired far <= previous far  desired 
number of frustums == previous number of frustums  Code: createPotentiallyVisibleSet() and insertIntoBin() 
in https://github.com/AnalyticalGraphicsInc/cesium/blob/b16/Source/Scene/Scene. js  Coherence can be 
really good, especially when the user-defned near distance is used because the closest bounding volume 
intersects it because, for example, the viewer is deep in the mountains or following a satellite as shown 
here. Coherence is not so good in other cases, like, when zooming in to an object because the desired 
near distance keeps getting smaller and smaller. \e could improve coherence by scaling the computed near/far 
to be slightly farther apart, but this runs the risk of requiring new frustums. So Steps 2-4 are really 
combined into a single pass over the commands. \e cull a command as before, then assign it to frustum(s) 
using the previous frame's frustum. As we walk through each command, we still compute the desired near/far 
distances. Then we use these to determine if the previous frame's frustums will work. If they won't we 
compute new frustums, and call the function again knowing that the computed frustum will work.  If 
adjacent frustums do not overlap slightly, we'll see tearing artifacts where the near plane of one frustum 
meets the far plane of the closer frustum. In the lower image here, we see several tears revealing the 
background color where two frustums meet. \hen we frst saw this artifact, we thought it was a driver 
bug, but we saw it across hardware vendors. \ith the near plane of a frustum equal to the far plane of 
the closer frustum, I can imagine one pixel artifacts, but these artifacts are much larger. To solve 
these, we move the near plane of each frustum, except the closest, slightly closer. Code: updateFrustums() 
in https://github.com/AnalyticalGraphicsInc/cesium/blob/b16/Source/Scene/Scene. js Video: videos/frustumOverlap.wmv 
 Overlapping frustums creates a new artifact for ob jects rendered with blending. In our engine, we render 
a lot of large translucent ob jects to represent sensors, e.g., a view volume of a camera attached to 
a satellite. Since these objects are large, they often overlap more than one frustum. Since they use 
alpha blending for translucency, the blending occurs twice where the frustums overlap leading to artifacts 
that can appear to slide back and forth when the viewer zooms. It should be possible to eliminate these 
artifacts with the stencil bufer, but we haven't investigated it yet. \hen a bounding volume overlaps 
more than one frustum, not only does it have the potential to create artifacts, but it leads to commands 
being executed redundantly, that is, executing draw calls for the same ob ject in more than one frustum. 
How, it's actually not as bad as you'd think. Let's look at a few diferent scenes. For this full world 
view, we execute 20 commands (ignoring clears, the sky box, and 2D overlays). They all ft within one 
frustum because we are able to dynamically push the near plane back really far, which allows the frst 
frustum to be large enough to include the entire globe. Here we are zoomed in much closer to the ground 
look at Mount Everest. For this scene, we execute 106 commands (horizon views are always a challenge; 
note there is no fog here allowing us to not draw distant tiles). The good news is 104 commands are executed 
once, and only two commands are executed twice. \e're not always this lucky with redundant calls. For 
example, we are high in space in this scene. \e need three frustums total. This scene only needs six 
commands, but two of them execute in all three frustums. Can you guess which? The billboards for the 
satellites are batched together as are the labels for the satellites. Since the bounding sphere for these 
includes the Molniya orbit high in space and right in front of the viewer -which peaks at 40,000 km above 
the globe, and the Geoeye in low earth orbit. This creates a huge bounding volume that overlaps all three 
frustums. Here's another screenshot showing the orbit line for us to better appreciate the scale. These 
shows that there is tension between batching and culling even more so when using multiple frustums. Traditionally, 
to make the best use of the GPU, and minimize CPU overhead, we always try to reduce the number of draw 
calls we make during a frame by batching. \hen we introduce multiple frustum rendering, batching generally 
increases bounding volume sizes, making it more likely that a command (batch) will need to be executed 
in more than one frustum, which increases the number of draw calls. Given that applications built on 
our engine are almost always CPU bound, we still see a win by batching aggressive even if it leads to 
some redundant draw calls. Ed did our original multi-frustum implementation in the late 90s. Dan did 
most of the recent implementation work in Cesium, our \ebGL engine. In addition to these more formal 
references, also see the implementation notes with pseudo-code for our engine -https://github.com/AnalyticalGraphicsInc/cesium/ 
wiki/Data-Driven-Renderer-Details. Chapter 3 World-Scale Terrain Rendering The most up-to-date version 
of these notes can be found at http://cesium.agi.com/ publications.html. 3.1 Introduction -Massive worlds 
render massive quan­tities of terrain Terrain datasets for massive worlds -especially those that aim 
to represent the massive world we call Earth -can easily measure in the terabytes. Add in detailed textures 
for the surface, such as color maps derived from satellite imagery or aerial photography, and it is not 
at all uncommon to see datasets measured in the hundreds of terabytes. Such datasets are much too large 
to ft in memory, and even too large to ft on a local system. \orking with such enormous datasets requires 
that we process and organize the source data for efcient streaming from a remote server, to disk or memory 
on a local client, and fnally into GPU memory for rendering. \e discuss how we solved these problems 
in Cesium, an open source virtual globe that runs inside a web browser without the need for a plugin. 
Cesium renders an accurate model of the Earth, from a global view where the entire planet is visible 
down to the street level where individual houses, cars, and trees are visible. The terrain surface is 
streamed as a mesh or heightmap from remote servers, and overlaid with multiple layers of imagery from 
diferent sources, such as \eb Map Service (\MS) servers, ArcGIS MapServers, Bing Maps, and more. The 
imagery need not share the same extent, tiling scheme, or even map projection as the terrain, nor do 
all the imagery sources need to share these attributes with each other. Cesium combines the disparate 
imagery on-the-fy, applies adjustments to each layer independently such as hue, saturation, and gamma, 
and renders the completed scene inside a web page at well over 60 FPS even on modest hardware.  3.2 
Organizing and processing terrain data Real terrain data for Earth and other planets is collected using 
satellite and aerial instru­ments. For example, the Shuttle Radar Topography Mission (SRTM) obtained 
elevation data for most of the Earth's surface with an instrument fying onboard the Space Shuttle Endeavor. 
Such datasets are usually provided as giant heightmaps, or perhaps as a large collection of smaller heightmaps, 
where height samples are arranged in a regular grid in some projection. It is not feasible to render 
such data directly. Instead, we preprocess it into a form that is amenable to streaming in small, multi-resolution 
chunks. For this sort of real-world terrain data, which conforms to an ellipsoidal model of the Earth 
and does not have any overhanging sections anyway due to the way it is collected, a quadtree is a very 
natural spatial data structure for managing culling and level-of-detail. Each node in the quadtree, known 
as a tile, represents a subset of the terrain at a particular resolution. The entire quadtree is known 
as a tile pyramid, because it has a small number of tiles at the root, usually between one and four, 
and millions or perhaps even billions of tiles at the leaves. For a world extruded from a plane, it is 
straightforward to use a quadtree to uniformly divide the world. For a spherical or ellipsoidal world 
like Earth, however, the quadtree only uniformly divides a 2D pro jection of the world. The choice of 
map pro jection is important, because no pro jection can represent an ellipsoidal world without some 
distortion. \e use a simple geographic projection, also known as equidistant cylindrical or plate carree, 
for our tile quadtree. This leads to singularities at the poles, where all samples in a row of the heightmap 
map to a single point on the globe. This is generally acceptable for Earth, however, and is easy to work 
with in fragment shaders during rendering. More sophisticated approaches are possible without changing 
the fundamental rendering algorithm, such as the Ellipsoidal Cube Maps described by Lambers and Kolb1 
.  A single terrain dataset at rendering time is built from multiple input datasets. For example, one 
terrain dataset might be a mosaic of GTOPO30 data covering the entire world, SRTM data between -60 and 
60 degrees latitude, and the National Elevation Dataset (NED) for the United States. At rendering time, 
we work with a single preprocessed tile pyramid and the many sources of the original data do not infuence 
the rendering algorithm. This is in contrast to our handling of imagery, where multiple imagery sources 
are mosaiced at rendering time using multitexturing. The process for incorporating a new source dataset 
into the tile pyramid is as follows: 1. Compute the deepest level in the tile pyramid to be populated 
by the source data. This is a function of the source data resolution and the desired maximum number of 
samples in a single tile at the deepest level. 2. If the source data does not cover the entire world, 
compute the portion of the tile pyramid overlapped by the source data. 3. Execute a depth-frst, post-order 
traversal of the overlapped portion of the tile pyramid, down to the deepest level to be populated, so 
that leaf tiles are processed frst and parent tiles are only processed after their children are processed. 
 (a) For each leaf (deepest-level) tile: i. Determine the set of height samples in the source that overlap 
the tile. ii. Transform each sample to ellipsoid-centric Cartesian coordinates, usually \GS84 for Earth. 
Typically, the pro jected (X, Y, Height) coordinates are transformed to geodetic (Longitude, Latitude, 
Height) before they are trans­formed again to ellipsoid-centric (X, Y, Z) coordinates. iii. Add the sample 
Cartesian position to the tile mesh as a vertex. iv. Add additional vertices at the edges of the tile 
by interpolating, because the edges and corners are unlikely to align with samples in the source dataset. 
 v. Add a skirt around the perimeter of the tile. During rendering, the skirt will hide cracks between 
adjacent tiles with diferent levels of detail.  vi. Connect vertices to form faces, using a regular 
triangulation of the source grid. vii. Store the fnished tile on disk. (b) For each non-leaf (parent) 
tile:   1 http://ecm-planet.sourceforge.net/lambers12ecm.pdf  i. Build a new mesh that is the sum 
of all of this tile's child tile meshes, minus the skirt. ii. Simplify the mesh by contracting vertex 
pairs until the desired geometric error limit is reached. The geometric error limit is discussed below. 
iii. Add a skirt around the perimeter of the tile. iv. Store the fnished tile on disk. This entire process 
is highly amenable to parallelization, by both utilizing multiple threads on a single machine and by 
utilizing multiple machines in a cluster. \hile pre­processing the terrain, we also compute and store 
per-tile metadata to aid in rendering, such as minimum and maximum heights, bounding volume, and a horizon 
occlusion point. The horizon occlusion point is a proxy for the tile that we can use in occlusion culling 
against the Earth ellipsoid. If the horizon occlusion point is below the horizon, we can be certain that 
the entire tile is below the horizon as well. The computation and use of the horizon occlusion point 
is discussed in more detail in the Horizon Culling section. \e choose the target geometric error of the 
root level -level zero -by answering a question: if a root tile contained a regular grid of vertices 
with a chosen number of rows and columns, what is the worst-case geometric error of this representation 
relative to the real world? \e can make a worst-case estimation of this error without getting into the 
details of the actual topology of the world. Imagine that Mount Everest rose straight out of the Mariana 
Trench. And now imagine that our grid of vertices happened to have two adjacent samples in the Mariana 
Trench, such that our aqueous Mount Everest is not represented at all. Certainly that's a worst case 
scenario in terms of the geometric error of our terrain representation. The height error would be the 
diference in height between Mount Everest and the Mariana Trench, or about 20 kilometers. The geometric 
error is, in fact, much higher, however, because we need to take into account the curvature of the Earth. 
For a grid width of 65 vertices at the equator, the maximum error is approximately 150 kilometers, and 
this is the error we target while simplifying tiles in level zero. For levels greater than zero, the 
geometric error limit is determined by a simple relation­ship. Each deeper level in the tile pyramid 
has half the geometric error of the level above it. At level 18, the geometric error drops below one 
meter. For tiles at all levels, vertex pairs are collapsed until collapsing the next pair would cause 
the estimated error of the tile to cross the maximum allowed for tiles at this level in the tile pyramid. 
Because the tile pyramid is built from multiple input datasets, and those datasets them­selves may have 
regions of no data (voids), the maximum depth of the tile pyramid varies across the globe. \e use a separate 
table of contents fle, generated during the preprocessing step, to inform the rendering engine of which 
tiles are present. 3.3 Tile selection for rendering Our preprocessing work does not reduce the absolute 
size of the terrain dataset; in fact, it increases it. All that work up front, however, puts us in a 
good position to be able to very efciently select a subset of the terrain to render in any given frame. 
Our rendering algorithm closely follows the "Chunked LOD" approach presented by Thatcher Ulrich at a 
SIGGRAPH course in 20022 . Tile selection proceeds recursively from the root of the tile pyramid. The 
entire tile pyramid is not expected to be resident on the client, but portions of it will instead be 
streamed as needed from a remote server. \e discuss that process in detail in the Life of a 2 http://tulrich.com/geekstuff/chunklod.html 
 Tile section. For each visited tile, the frst step is to perform simple view-frustum culling. If the 
tile lies entirely outside of the current view frustum, we don't need to render it or visit any of its 
children. Because our terrain is mapped to a globe, we also perform another type of visibility culling: 
horizon culling. The concept is simple: if we can detect that the tile is entirely below the horizon 
of the ellipsoid, as viewed from the current camera position, we don't need to render it. See the Horizon 
Culling section for more details. Once we have determined that a tile is visible, we really have two 
options: we can render this tile and stop our traversal of its branch of the quadtree, or we can refne. 
Refning means continuing traversal with this tile's child tiles, and rendering them, or their descendents, 
instead. Efectively, if we choose to refne, we are increasing the rendered level-of-detail for the region 
of the globe covered by this tile. The standard technique used in hierarchical LOD algorithms, and the 
one used here, is to drive refnement by an estimate of the error, in pixels on the screen, that would 
result from rendering this tile rather than refning. \e choose the tolerable screen-space error for our 
application, usually a small number like one or two pixels, and we refne whenever the estimated screen-space 
error for the tile is greater than the tolerance. By varying this tolerance, we are able to trade rendering 
accuracy for performance. Screen-space error (SSE) is estimated by projecting the estimated geometric 
error of the tile into screen space, using this standard LOD equation: ex S S E = . 2d tan 2 \here e 
is the geometric error of the tile in meters, x is the viewport width in pixels, . is the camera's feld-of-view 
angle in radians, and d is the distance to the tile. The feld-of-view angle of the camera and the width 
of the viewport are known. \e also know the geometric error because it is a simple function of the tile's 
level in the tile pyramid. Because we only know the maximum geometric error, not the local error throughout 
the tile, we conservatively assume that the point on the tile closest to the viewer has the maximum error. 
Even the distance to the closest point on the tile is expensive to compute, however. Instead, in our 
LOD equation, we use the distance to the closest point on a bounding volume that bounds the tile. \hile 
bounding spheres are useful for culling, we've found them to be very poor for estimating distance for 
LOD selection. The problem is that the viewer is very frequently inside the bounding sphere of candidate 
tiles as a result of the sphere extending far above and out from the edges of the tile. \hen the viewer 
is inside the bounding sphere, a conservative estimate of distance to the region of the tile with maximum 
geometric error is literally zero; the viewer could be right on top of it. In that case, we must refne, 
because the projected error is infnite. In Cesium, we use four planes to bound each tile, one at each 
horizontal boundary of the tile, plus a curved surface at the maximum height of the terrain in the tile. 
The two planes on the \estern and Eastern edges of the tile tightly bound the tile, but planes on the 
Northern and Southern boundaries are approximations due to the surface normals at those boundaries not 
lying in a plane. The distance from the ffth, curved, surface is easily computed by ransforming the viewer 
position to a height above the ellipsoid and then subtracting the tile's maximum height. \e can not render 
a tile that is not yet loaded, nor can we render a tile where any of its four siblings in the quadtree 
are not loaded. If the parent cannot be rendered, either, we will continue up the quadtree until we fnd 
a suitable tile to render. As more data is loaded, the visual quality of the globe will improve, but 
at no point do we suspend rendering to wait for data to load. Having selected the tiles to render this 
frame, it's now time to actually render them. In general, we issue one draw command per tile. Multiple 
color textures, a specular mask, and other efects are applied in a single pass. In some cases, a single 
draw command is executed multiple times per render frame, though, in order to enable correct depth testing 
of scene elements against terrain via multi-frustum rendering. The shaders used in rendering will be 
discussed in the next section. First, let's discuss the tile load pipeline. 3.4 Life of a tile Our 
terrain and imagery datasets are huge -much too big to ft in memory or even on a local disk -so a primary 
challenge of rendering them is loading and unloading subsets of data at appropriate times. As a web-based 
application, Cesium downloads terrain and imagery on demand over HTTP. It uses a tile load pipeline to 
manage the process of fully populating a selected tile with data, keeping much of the process asynchronous 
with rendering so that the frame rate remains relatively consistent even as tiles are downloaded and 
processed. A tile starts its life as a skeleton. Tile skeletons know their location in the world, and 
their relationship to other tiles, but do not yet have the geometry and texture data necessary to render. 
During tile selection, we can't refne to a skeleton. Instead, we add the skeleton tile to the load queue 
and render the parent tile for this frame. The load queue is a priority queue, implemented as a doubly-linked 
list. Tiles needed in the most recent frame are kept at the front of the queue, because those tiles are 
likely to be needed in the next frame as well. The tiles in the load queue that were requested in a single 
frame are further prioritized based on their level in the tile tree such that larger, lower-detail tiles 
are loaded before smaller, higher-detail ones, to maximize the average detail across the scene. Once 
per frame, Cesium processes the tiles in the load queue, moving them through the following steps: 3.4.1 
Create imagery skeletons Cesium enables imagery from multiple sources to be overlaid on terrain and layered 
or alpha blended together. For example, a base map with worldwide imagery can be overlaid with a very 
high-resolution image for a small area, or a base layer can alpha blended with a rasterized road layer. 
In support of this capability, the frst task in the load pipeline is to attach TileImagery skeletons 
to the Tile for each active imagery layer. Like a Tile skeleton, a TileImagery skeleton does not yet 
have any renderable resources. However, it knows which imagery tile will be needed, the minimum and maximum 
texture coordinates over which the imagery tile applies, and how to scale and translate the imagery tile 
in order to map it onto the terrain surface. Given a terrain tile and an imagery layer, Cesium frst determines 
which quadtree level from the imagery layer applies to the terrain tile. As a frst cut, we simply need 
to fnd the imagery level that has a texel spacing, in world coordinates, that is equal to or smaller 
than the geometric error of the terrain tile's level. For a good looking scene, however, we have found 
that it is important that the spacing between imagery texels once pro jected to the screen be smaller 
than the terrain screen-space error (SSE). In other words, while a maximum SSE for terrain geometry of 
two pixels looks quite good, the scene will look blurry if each imagery texel is allowed to be mapped 
to two pixels. Cesium selects an imagery level with texel spacing less than or equal to half the geometric 
error of the terrain tile. \ith the level selected, the next step is to identify which imagery tiles 
in the level overlap the terrain tile. For an imagery layer with a non-worldwide extent, there may not 
be any tiles that overlap a portion of the terrain tile, so we compute the rectangular extent that is 
the intersection between the terrain tile's extent and the imagery layer's extent. Then, we use the imagery 
layer's tiling scheme to fnd the tile coordinates of the intersection extent's northwest and southeast 
corners. As shown in the fgure below, the result is a rectangular range of tile coordinates, and we create 
a TileImagery object for each tile in the range.  To fnd the imagery tiles that overlap a terrain tile 
(yellow), we determine the imagery tiles that contain the northwest and southeast corners (red dots). 
The containing tiles form the extremes of a rectangular range, and all imagery tiles in the range are 
rendered on the terrain tile. Later, in the fragment shader for this terrain tile, we'll determine if 
the imagery tile applies to the current fragment by checking if its texture coordinates are within the 
texture coordinate extent for this imagery tile. If so, we'll transform the fragment's texture coordi­nates 
to imagery tile texture coordinates by multiplying by the TileImagery's scale property and then adding 
its translation property. These three properties, the texture coordinate ex­tent, scale, and translation, 
are set during this step in the load pipeline using a computation on the extents of the terrain and imagery 
tile, shown in the listing below. var terrainWidth = terrainExtent.east -terrainExtent.west; var terrainHeight 
= terrainExtent.north -terrainExtent.south; var scaleX = terrainWidth / (imageryExtent.east -imageryExtent.west); 
var scaleY = terrainHeight / (imageryExtent.north -imageryExtent.south); var translationX = scaleX (terrainExtent.west 
-imageryExtent.west) / terrainWidth; * var translationY = scaleY (terrainExtent.south -imageryExtent.south) 
/ terrainHeight; * var minU = Math.min(1.0, (imageryExtent.west -tile.extent.west) / (tile.extent.east 
-tile.extent.west)); // maxU, minV, and maxV are similar to minU and not shown Care must be taken to 
ensure that minU is 0.0 for an imagery tile that starts on the western edge of the terrain tile, and 
similar for the other edges, even in the face of small rounding errors. Otherwise, cracks will be visible 
between tiles even when skirts are used. In addition, for adjacent textures, maxU of one must equal minU 
of the next in order to avoid cracks in the middle of the tile. 3.4.2 Request The next task is to request 
the tile terrain and imagery from the remote sources. Because Cesium runs in a web browser, we use browser-based 
APIs to download images and other resources asynchronously. In a native application, we would use one 
or more threads to retrieve resources. \e take care to avoid creating too many simultaneous requests. 
This is especially im­portant in Cesium, which has a browser-imposed limit of six simultaneous requests 
per hostname. Because the tiles needed to render change as the camera moves, we may decide to start loading 
a particular tile, then before the request completes, fnd that it's no longer needed. Throttling requests 
allows us to re-order pending requests every frame in order to always keep the most important requests 
at the front, keeping the latency as small as possible. Cesium benefts from its host web browser's automatic 
caching of downloaded resources. In a native application, we would manually keep a cache of downloaded 
terrain and imagery tiles on disk. Such tiles cannot, of course, be prepared for rendering as quickly 
as those that are already in memory, but they can be readied much more quickly than those that must be 
requested from a remote server. \hen the requests complete, the tile transitions to the received state 
and is ready for the next step of the load process. 3.4.3 Transform The transform step takes the raw 
data received from the remote server and transforms it into a form more amenable to rendering. For example, 
we turn terrain expressed as a heightmap into a triangle mesh, and add skirts around its perimeter to 
hide the cracks between tiles that can result when two tiles of diferent LODs are adjacent to each other. 
Transforming a single tile is fast, but when the user moves the camera to a new part of the globe, the 
time to transform all the new tiles can signifcantly impact rendering performance. It's much better to 
continue to render low-detail tiles at interactive frame rates than to stall rendering while transforming 
a large set of new tiles. In Cesium, we use a task processing system that uses \eb \orkers to interpret 
height maps and compute vertices asynchronously, transferring the resulting array back to the main thread. 
Once the main thread receives the results, it transitions the tile to the transformed state and the tile 
is ready for the next step of the load process. 3.4.4 Create resources Finally, \ebGL resources, such 
as vertex bufers and textures, are created from the trans­formed data. For example, after the transform 
step, we have a TypedArray containing the interleaved vertex attributes for the tile, but that vertex 
data is not yet in a \ebGL vertex bufer and it is not yet available to the GPU. During this step, we 
upload the vertex data to the GPU as a vertex bufer. \e take care to share \ebGL resources whenever possible. 
A tile of imagery that overlaps multiple terrain tiles will only be loaded and uploaded to the GPU once. 
For heightmap­based terrain, all meshes across all levels of detail can share a single index bufer. This 
sharing is managed using reference counting, and yields a signifcant reduction in memory usage. Ideally, 
resource creation would be managed by a thread separate from the rendering thread, and in a native application 
it probably would be. Doing so reduces memory copies and can enable driver optimizations. On the web, 
however, it is currently not possible to create \ebGL resources in a \eb \orker. \e look forward to a 
future extension that allows this. \ith its \ebGL resources created, the tile is ready to be rendered, 
and will be added to the render list the next time the tile selection process selects the tile for rendering. 
 3.4.5 Replacement As the user moves around the globe, a large number of tiles will be loaded. Once the 
viewer moves elsewhere, old tiles are no longer needed. To keep the memory used by tiles from growing 
unbounded, Cesium places all partially or fully loaded tiles in a replacement queue. Each time a tile 
is traversed or rendered it is moved to the head of the queue, so the tiles at the tail of the queue 
are the ones that have been used least recently. To save memory, tiles are unloaded from the tail of 
the queue. \hen we unload a tile, we put its \ebGL resources, such as textures, back into a pool rather 
than destroying them outright. Later, when loading a new imagery tile, we can reuse an existing texture 
from the pool. This approach minimizes the number of calls to allocate \ebGL resources.  3.5 Terrain 
and imagery shaders Our vertex shader is straightforward. \e multiply an RTC model-view-pro jection matrix 
by the input vertex position, expressed relative to a center-point somewhere in the tile. By using RTC 
rendering, we avoid jittering artifacts when zoomed in close to a tile. The fragment shader is a bit 
more complicated, mostly due to the need to blend together imagery from multiple layers. \e start by 
assuming the fragment is a constant color, perhaps a nice shade of Earthy blue. Then, in layer order, 
from bottom to top, each texture has the opportunity to modify that color. The color resulting from one 
texture is passed as the input to the next. The color output by the last texture is the input to the 
lighting equations. A single layer may have multiple textures. In that case, their relative order is 
arbitrary because multiple textures from the same layer do not overlap. Because we allow diferent imagery 
layers to use diferent tiling schemes -from the terrain and from each other -it is possible and common 
that a given imagery texture will not completely overlap a tile. For each texture, we pass as packed 
uniforms the scaleX, scaleY, translationX, translationY, minU, minV, maxU, and maxV values computed during 
the "Create imagery skeletons" phase of the tile load pipeline. Collectively, these values control which 
portion of the current texture applies to which portion of the tile. Tile texture coordinates are (0.0, 
0.0) in the southwest corner of the tile and (1.0, 1.0) in the northeast corner. They are linear with 
longitude and latitude, which is consistent with a geographic pro jection. The minU, minV, maxU, and 
maxV values specify the rectangular portion of the tile that is overlapped by the imagery texture. If 
the current fragment's texture coordinates lie outside this range, this texture does not modify the color 
of the fragment. if (tileTextureCoordinates.s < minU || tileTextureCoordinates.s > maxU || tileTextureCoordinates.t 
< minV || tileTextureCoordinates.t > maxV) { return previousColor; } Next, we compute the texture coordinates 
with which to sample this texture by applying the scale and translation computed during loading of the 
tile: vec2 textureCoordinates = tileTextureCoordinates scale + translation; * After sampling the texture 
using these coordinates, we apply any adjustments to the sampled color, such as brightness, contrast, 
hue, saturation, gamma, or alpha. Finally, we alpha blend the sampled color with the previous color and 
return the new color: return mix(previousColor, color.rgb, color.a); Once the fnal color has been determined 
by applying all of the layers, we do one more texture lookup to determine if the fragment is on land 
or in water. If it's in water, we add a specular highlight and an animated wave efect, following the 
general approach used by Jonas \agner3 .  3.6 Horizon culling Horizon culling is a form of occlusion 
culling that is critical for any spherical or ellipsoidal massive world. It is critical because it allows 
us to determine that signifcant chunks of terrain are not visible to the viewer, and therefore to avoid 
rendering them. In the fgure below, terrain tiles covering the entire Earth lie inside the view frustum. 
Over half of them, however, are below the horizon and do not need to be rendered. The green points are 
visible to the viewer. The red points are not visible because they are outside the view frustum, which 
is represented as heavy white lines. The blue point is inside the view frustum, but it is not visible 
to viewer because it is occluded by the Earth. Horizon culling aims to separate the green points from 
the blue ones. Cesium performs horizon culling of each terrain tile using a novel technique that requires 
only a handful of foating-point operations to test against an ellipsoidal model of the hori­zon. \e achieve 
this by working in an ellipsoid-scaled space in which the ellipsoid is, very conveniently, represented 
as a unit sphere. To transform from a standard reference frame with its origin at the center of the ellipsoid 
and its axes aligned with the ellipsoid's axes to the ellipsoid-scaled space, we simply multiply each 
coordinate value by the inverse of the ellipsoid's radius along that axis. The basis for our horizon 
culling is a horizon occlusion point associated with each tile. 3 http://29a.ch/2012/7/19/webgl-terrain-rendering-water-fog 
  The horizon occlusion point has the useful property that when the point is below the horizon, the 
entire tile is below the horizon as well. Thus, we can conservatively horizon cull the tile by simply 
testing the point against the horizon. \e compute the horizon occlusion point during the preprocessing 
step. First, we arbi­trarily assume that the horizon occlusion point lies along a vector from the center 
of the ellipsoid to the center of the tile's bounding sphere. Then, for each vertex in the tile, we determine 
where on that center line the point is located.  In the fgure above, the Earth ellipsoid is shown in 
blue and a terrain tile is shown in brown. For a given vertex V, the horizon occlusion point P we're 
looking for is the intersection of with the center line, . Point H is a point on the horizon from the 
perspective of point V. There are an infnite number of horizon points from the perspective of point V, 
forming a circle on a sphere, but only two of these horizon points form a vector through point V that 
intersects with the center line. One of these intersections will occur after point V and the other before, 
but we're only concerned with the intersection that occurs afterward, because that one will occur farther 
from the center of the ellipsoid. A full derivation of the horizon occlusion point computation can be 
found on our blog4 . It all boils down to computing the magnitude of point P along as follows: var position 
= ...; var scaledSpacePosition = ellipsoid.transformPositionToScaledSpace(position); var magnitudeSquared 
= scaledSpacePosition.magnitudeSquared(); var magnitude = Math.sqrt(magnitudeSquared); var direction 
= scaledSpacePosition.divideByScalar(magnitude); var cosAlpha = direction.dot(scaledSpaceDirectionToPoint); 
var sinAlpha = direction.cross(scaledSpaceDirectionToPoint).magnitude(); var cosBeta = 1.0 / magnitude; 
var sinBeta = Math.sqrt(magnitudeSquared -1.0) cosBeta; * var magnitude = 1.0 / (cosAlpha cosBeta -sinAlpha 
sinBeta); * * \e repeat this computation for each vertex in the tile. The fnal magnitude is the greatest 
of the magnitudes computed for any vertex. Later, during tile selection, we perform the following computation 
each time the camera moves: var cv = ellipsoid.transformPositionToScaledSpace(cameraPosition, this._cameraPositionInScaledSpace); 
var vhMagnitudeSquared = Cartesian3.magnitudeSquared(cv) -1.0; Cartesian3.clone(cameraPosition, this._cameraPosition); 
this._cameraPositionInScaledSpace = cv; this._distanceToLimbInScaledSpaceSquared = vhMagnitudeSquared; 
 And then the following for each tile: var cv = this._cameraPositionInScaledSpace; var vhMagnitudeSquared 
= this._distanceToLimbInScaledSpaceSquared; var vt = Cartesian3.subtract(occludeeScaledSpacePosition, 
cv, scratchCartesian); var vtDotVc = -vt.dot(cv); var isOccluded = vtDotVc > vhMagnitudeSquared &#38;&#38; 
* vtDotVc vtDotVc / vt.magnitudeSquared() > vhMagnitudeSquared; If isOccluded is true, we do not render 
the tile. 4 http://cesium.agi.com/blog.html  3.7 Map reprojection on the GPU Cesium currently displays 
imagery referenced to either a \GS84 Geographic (EPSG:4326) projection or a \eb Mercator (EPSG:3857) 
projection, the two most common map pro jec­tions seen on the web. As mentioned previously, terrain vertices 
include texture coordinates that assume a Ge­ographic projection. In other words, the texture coordinates, 
regardless of the original map projection of the terrain data, are a function of the latitude and longitude 
of the vertex as a fraction of the total latitude and longitude spanned by the terrain tile. The GPU's 
interpolation of the vertex texture coordinates across fragments gives us reasonable, if not 100% accurate, 
Geographic texture coordinates for the fragments. If the source imagery tile is in a \eb Mercator projection, 
however, using these Geo­graphic texture coordinates will badly distort the image for tiles that cover 
a large spatial area because the texels in such an image have a non-linear mapping to latitude. Instead, 
we repro ject \eb Mercator imagery tiles to Geographic on the GPU by rendering them to a framebufer with 
a color attachment and doing the reprojection in the fragment shader. \e found this to be much lighter 
on memory than including multiple sets of texture coordinates for the diferent projections, and more 
performant than repro jecting the imagery on the CPU. Transforming Geographic texture coordinates to 
\eb Mercator in order to sample a source \eb Mercator texture is straightforward. Given the latitude 
in radians, the \eb Mercator Y-coordinate of the southern edge of the tile, and the latitude of the current 
fragment, the \eb Mercator vertical texture coordinate is: mercatorV = 0.5 log((1.0 + sin(latitude)) 
/ (1.0 -sin(latitude)) -southMercatorY; * The difculty, as is often the case when rendering a world as 
big as the Earth, is in dealing with the GPU's limited foating-point precision. In particular, the subtraction 
of southMercatorY is problematic because both the subtrahend and the minuend are of similar magnitude, 
so a great deal of precision is lost in the process. \e deal with this problem in two ways. First, we 
perform the subtraction using simulated double-precision using the DSFUN90 algorithm. Second, for spatially 
small tiles, we abandon the reprojection and simply use the \eb Mercator image as if it were already 
Geographic. This is acceptable because, over small distances, the transformation amounts to much less 
than a texel of diference in the texture coordinates. This is shown in the fgure below. \hile this approach 
works extremely well on desktop and laptop systems, Cesium is also intended to run on mobile devices 
using the increasingly high-quality \ebGL implementa­tions available in Mozilla Firefox and Google Chrome 
for Android. Most of these devices have reduced precision available to their fragment shaders, so cutting 
of the repro jection at level 11 is not soon enough. The result is smeared-looking textures at medium 
zoom levels. Our solution is to move the texture coordinate computation to the vertex shader instead 
of doing it in the fragment shader. This requires many more vertices -approximately one per fragment 
rather than four total -in order to avoid errors introduced by interpolation of the texture coordinates 
during rasterization. Fortunately, the vertex bufer is shared and unmodifed for all reprojections. In 
addition, our fragment shader becomes trivial with this modifed approach, so overall the performance 
is very similar to the original approach on desktop systems and has the nice beneft of producing correct 
results on mobile devices.  3.8 Acknowledgements Thanks to Scott Hunter for much of the content of the 
"Life of a Tile" section. Thanks to Frank Stoner for deriving the scaled-space formulation of the horizon 
culling technique, and for patiently explaining it to me. Chapter 4  Populating a Massive Game World 
4.1 Introduction In March 2010 Just Cause 2 was released. The game was generally very well received, 
praised for its vast open landscapes, the player freedom, and the seemingly never-ending amount of things 
to blow up. To this day Just Cause 2 still stands out among large games. But flling a game world of over 
1,000 km2 with meaningful content was challenging. The original Just Cause had a game world of the same 
size, but it had generally been perceived as rather empty, so we had to fnd better ways to produce content. 
\hile we ultimately reached our goals and shipped a high-quality game, getting there was not all a success 
story. It is also a story about poor tools, broken builds, horrible turnaround times, and the eforts 
to salvage the situation. 4.2 Build system 4.2.1 Issues with the content build system Our content build 
system had grown organically since the birth of the company. No one had a complete picture of everything 
it did, and it was simply a collection of loose compilers. The dependencies were unclear. Many compilers 
took results from other compiled data as input. Since already compiled data from previous compiles typically 
was around this usually worked, but could fail on clean builds, or even content was upgraded. Sometimes 
there was even dependencies on data from another platform. For instance, it was found at one point that 
doing a clean PS3 build required not only that the PS3 platform was built repeatedly something like 3 
to 5 times for all changes to propagated properly to all dependencies, but it also required a complete 
build of \in32 data. As a result of the poor to non-existing dependency management, our content builds 
were frequently broken. Sometimes the compile seemingly succeeded, but data was broken. Sometimes the 
build failed despite seemingly valid content. On top of this, the data building process was slow, in 
the order of many hours. Consequently the turn-around time for data builds tended to be overnight. If 
a check-in broke something, we would not know until the next day when the nightly build was broken. And 
sometimes fxing the fault only resulted in revealing the next broken piece of content. Occasionally a 
company-wide content check-in stop had to be enforced to bring the data back to a working condition. 
 4.2.2 Untangling the mess In parallel with the completion of Just Cause 2 the engine team took a holistic 
approach on fxing the content build system. A framework was built for compilers and a dependency walker 
that could fgure out exactly what needed to be rebuilt and in which order. All compilers were analyzed 
for whether they were still needed or could be removed, and what their true dependencies were. Any sort 
of dependency loops were eliminated. \ith this in place we could do proper incremental builds, which 
had the greatest impact on turn-around times. But a lot of efort was also spent on removing the worst 
bottlenecks in the compilers to further reduce build times. Many compilers were rewritten from scratch. 
The perhaps greatest challenge was to restore trust in the pipeline. If people are doing complete rebuilds 
because they don't trust an incremental build to do the right thing, then that's a great loss for productivity. 
A build error should indicate broken data, and if some­thing fails at runtime after a successful build, 
it should be assumed the code is to blame, not the data. To ensure that the incremental build was working 
properly we set up build servers to regularly do complete rebuilds and compare the results to the latest 
incremental build. \henever there was a mismatch the problem was promptly investigated and a proper solution 
devised. During the Just Cause 2 project missing data was replaced with placeholder content at runtime. 
Missing models became pink boxes and missing textures were replaced with a pink texture. This way the 
build could still run with broken data and hopefully be able to spot broken data in game. This may seem 
like a sensible approach, but we have since abandoned this idea. Instead we have worked hard to ensure 
that we avoid the problem to begin with, by having the build system fail the build and have the content 
creator fx the problem before it is ever checked in. The core philosophy is that if the content build 
succeeds, the resulting data should be complete and working. 4.2.3 Code build system A big problem is 
most large productions, whether it is a large game or any other sort of big code-base, is that compiling 
and linking the executable can take a considerable amount of time. Like at most game studios, our game 
is written primarily in C/C++, which is the best choice for runtime performance, but somewhat problematic 
for build times. In the early days we used third party tools like Incredibuild, which alleviated the 
problem somewhat by distributing the build cost across all available machines in the studio, but it could 
still take half an hour to do a clean rebuild of the game. This was of course a big problem for productivity. 
Our solution is not necessarily novel, but has worked out great for us. \e built a custom build system 
that batched up *.cpp fles, by default in chunks of 30 fles. Basically it creates a new set of source 
fles, containing nothing but #includes of the original source fles. This batching vastly reduces the 
amount of duplicated compiles, which is the fundamental faw of the C/C++ model. The number of translation 
units is drastically reduced, which also speeds up the linking process, which often tends to be a signifcant 
portion of the build time. Figure ?? shows the impact of batching on build performance. The build time 
was cut down to a fraction of the original. As an added bonus, with more source baked into the same 
translation unit we get smaller executables and faster code . The result is similar to what you get from 
"\hole Program Optimization", enabling for instance cross-module inlining, except for us it is not across 
the whole program, but only across a batch. But since batches are parsed out from the source directory 
tree, related fles are typically batched together and we get much of the same benefts. On the downside, 
batching cpp fles comes with a set of problems, primarily that code "leaks" across units. This means 
that if only one fle in a batch includes a header, it will be visible to all source fles. Similarly, 
statements like "using namespace Graphics;" in a global scope will also leak. The result of this is that 
code that is broken sometimes actually compiles, and only once a new fle is added to the pro ject and 
the batch arrangement shifts does the problem potentially emerge. To deal with this problem, our auto-build 
servers also regularly does unbatched builds to catch such errors. Typically the solution is to just 
include the right header. In rare cases the batching has created hard to understand compile error due 
to leaking, but overall the time lost to such debugging is vastly outweighed by the gain in far faster 
build times. Other things we have done over the last few years is to separate central systems into shared 
libraries. Originally the project was essentially a large collection of source code, and the engine was 
not really separated from the game. \hile there is more to be done in this area, the current situation 
is much improved, with many core system separated into their own modules, allowing easy sharing of functionality 
across projects and also speeding up compilation by pulling those out of the pro ject code base. Add 
hardware upgrades on top of that, including SSD drives in all developer machines, we can now do a complete 
rebuild of a current project in about 2.5 minutes.  4.3 Content Production 4.3.1 Modularization Filling 
a world of this size with content required us to work in a modular fashion. Content was created to be 
used repeatedly across the world. This means creating reusable chunks that can be placed in the world 
and easily combined without too much customizations. Other than collections of models it also included 
light sources, occlusion culling boxes and similar data. The content was arranged such that the entity 
could be updated and all instances inherited the new settings, unless they had been locally overridden. 
This allowed updated to art to propagate to the whole world, while maintaining the possibility to do 
local customizations. Artists are typically more passionate about art than performance, so placement 
of oc­cluder boxes for our occlusion culling system was often relatively poor or non-existant at frst. 
However, this setup allowed occluder boxes to be placed into entities and thus automatically placed in 
all locations that used it. Some content is generated completely procedurally. For instance light poles 
and other props around roads. 4.3.2 Landscape authoring The landscape is divided into axis-aligned patches 
of a fxed size of 512m × 512m. \e refer to these as stream patches as they are stored and streamed like 
this at runtime. A stream patch is a data container that holds not just the terrain vertex data and textures, 
the landscape physics representation, as well as vegetation and various meta-data. Each patch is stored 
separately on disk and in source control, which allows multiple artists to work on the landscape separately 
without collisions, as long as they work in diferent areas of the world. Our terrain is for the most 
part artist authored, using our in-house editor, but we do support importing and exporting subsets of 
the data for editing in external tools or crafting an initial terrain procedurally. At runtime the terrain 
comes in two diferent representations, one fxed-size height-map for the physics, and a graphical representation 
that has varying density depending on to­pographical complexity, proximity to water, and other factors. 
The graphics representation also has an elaborate LOD system. Editing the terrain is done at a fxed resolution, 
using our custom editor, and a terrain compiler then computes the fnal in-game terrain. \hile the editor 
uses the full game engine to render the terrain, the intermediate results in the editor might difer somewhat 
compared to the in-game results after compilation and optimization. For this reason, and practicality 
in general, we have tools for anchoring entities in the game to the terrain. This means that you can 
place items on a newly edited terrain and even if the compiler triangulates the area in a somewhat diferent 
fashion, the results will still be good. In particular, there will not be any items hanging in thin air 
(which was otherwise common in the early days of Just Cause 2), or worse, buried underground. The vegetation 
is a combination of procedural and artist driven. Most of the vegetation is procedurally placed. Normally 
it is not particularly important exactly where trees are in a forest, so the system does the vast amount 
of vegetation placement automatically. You care about the overall density and composition of trees. Artists 
set up properties that defne the vegetation in an area, and the system takes factors such as the slope, 
elevation and climate zone (desert, jungle, artic etc.) into account and places trees and plants in a 
randomized but deterministic fashion. \e only store a minimum amount of data to reproduce the placement 
and most placement is done with a seeded random function at runtime. Naturally we need some level of 
artist control over where vegetation is placed. For instance, we do not want trees to land inside of 
buildings, or in the middle of a road. Artists can set the density of the vegetation in an area and naturally 
also completely remove vegetation where it is undesired. The system also automatically removes vegetation 
under roads. Sometimes artists need fne control over vegetation. For instance, you may need a tree at 
a particular spot within a location. For this we have a manual vegetation placement system, allowing 
the artists to place trees and plants much like any other game ob ject. For larger vegetation, such as 
trees and plants, there are four diferent systems for ren­dering. Close to the camera trees are drawn 
as full-blown models. These render like any other models and have several LOD steps. Beyond some distance 
the models are replaced by simple impostors. At this distance an entire patch, with hundreds of trees, 
is drawn in a single draw-call. Further out the impostors are rendered as a textured mesh layered on 
top of the terrain. \e sometimes refer to this as the "forest carpet". Finally, to get a proper silhouette 
at distant mountains we also have something we call "forest fns". These  4.4 Bringing it to life An 
important aspect of making a large game world is to make it come to life. Filling it with content is 
very important, but it is not everything. There should also be things going on at idle times, or in-between 
locations or outside of missions, and you need to preserve a sense of a consistent and contiguous world 
even though you cannot keep everything in memory, draw all ob jects or simulate every entity in the world. 
For this we have a number of systems in place, both for the visual richness and for simulating an alive 
world. 4.4.1 World simulation \e use a world simulation system that always makes sure that something 
is going on in the world. The system is driven by the context in which the player is. If you are somewhere 
along a populated area, say in a village, there will be civilians walking around. Roads have a moderate 
fow of assorted vehicles passing by. Once in a while an airplane will fy over. In the water you will 
see boats. There is an important gameplay aspect of this too. It gives you an steady supply of vehicles 
that you can use. This is especially important if you end up crashing somewhere in the ocean. Swimming 
back to the shore is probably not going to be very exciting for the player. So we specifcally guide the 
system to have a boat of some kind passing by relatively close soon thereafter that the player can hijack 
and get back into action. There is all sorts of animal life as well. There is always a handful of birds 
around, fies, fsh in the sea, and even scorpions. Except for the scorpions maybe, these are extremely 
simple models. Birds are only a handful of polygons actually and costs nearly nothing to render. But 
they have a large impact on bringing life to the world. Another important element is the rolling day-night 
cycle. An interesting part of how this works in Just Cause 2 is that time is not linear. The time cycle 
has been tweaked with various considerations in mind, both visual and gameplay. It turns out most players 
enjoy daytime more than nighttime. This is besides the fact that visibility is better during daytime, 
even though we spent a lot of efort trying to highlight enemies at night through rim-lighting and other 
tweaks without breaking the illusion of the scene being dark or enemies popping out unnaturally from 
the environment. Nighttime is also visually less pleasing in general, except perhaps in the city (which 
looks great when it lights up at night). So we let the player enjoy the daytime for a longer time than 
they have to endure nighttime. But we also highlight visually pleasing moments like sunset and sunrise 
by staying longer there, but once sun is below the horizon, we fast-forward to full night. 4.4.2 Landmarks 
\e maintain a very large draw distance of 50,000m, which is actually enough to see any part of the world 
from anywhere, ignoring occlusion of course. Standing on the top of the highest mountain, or fying an 
airplane, you can see the entire world. Naturally can we only keep the closest locations streamed in 
at any point. However, it is important both for gameplay and visual richness that we are able to identify 
key locations even at a very large distance. If you are standing on the top of a mountain, you should 
be able to see all the interesting places around you, even if they are miles and miles away. This encourages 
the player to go explore those areas and makes it less necessary to interrupt core gameplay by opening 
up the map. It is more enjoyable for the player to have a visual target to aim for. For this we created 
a landmark system. Models are streamed in and out as needed. They come in diferent LODs for performance. 
The landmark system takes over once the regular models in a location gets unloaded. This replaces the 
entire location with a simple model capturing the essence of the location. Since they are to be viewed 
from a rather large distance, the models are very low-res in terms of polygon count and texture resolution. 
The landmark models are loaded at all times and are (ignoring occlusion) visible at all times. Refer 
to Figures 4.1 and 4.2 for an example of the iconic Mile High Club location visible over 25km. 4.4.3 
Lighting For lighting we had two diferent solutions for scale. The main system was for relatively close 
distances where we needed proper lighting. In Just Cause 2 this was handled with a world-space axis aligned 
tiled light indexing system. The details of this technique has been previously covered in GPU Pro [1]. 
For our current lighting solution, which is based on Clustered Deferred Shading, please refer to the 
Advances in Real-Time Rendering in Games course [2]. In both the old and the new system we work on two 
diferent scales. The full lighting is done up to a certain distance. In Just Cause 2 the limit was 300m. 
At the time of this writing, we are using 500m. As lights approach this distance they begin to fade out 
and smoothly transitions to a distant light system taking over the rest of the range. The distant light 
system simulates lighting from static light sources. No actual lighting is computed per se, it basically 
just highlights the light sources. At data compilation stage all the static light sources in the world 
are assembled and split up into a grid structure. The grid is used for frustum culling. Each grid cell 
is drawn with a single draw call. A very compact vertex bufer encodes all lights with only a position, 
radius and color. The lights are then rendered as simple point sprites. This highlights important locations, 
especially the city and villages, and makes them look fully lit from a distance. This is not only very 
cheap to render, but also very efective to bring a huge world to life. In Figure 4.3, note how all the 
interesting locations are clearly visible at night, giving the player a clue where to go next, in addition 
to a great visual efect. In Figure 4.4, note how the city stands out and looks alive.    4.5 Summary 
\hile in no way exhaustive, this article touches on some of the issues and some of the solutions developed 
at Avalanche Studios for creating a vast game world. 4.6 References [1] Persson, E., 2010. Making it 
large, beautiful, fast and consistent -Lessons learned developing Just Cause 2. GPU Pro. [2] Persson, 
E., 2013. Practical Clustered Shading. Advances in Real-Time Rendering in Games. http://advances.realtimerendering.com/ 
(to appear) Chapter 5  Hardware Virtual Texturing Partially Resident Textures provide direct hardware 
support for the majority of all tasks present in the virtual texturing pipeline. Application developers 
are no longer required to deal with managing of the page table, address translation and/or fguring out 
which texture types need to be supported. The responsibility of managing the virtual nature of a texture 
is moved toward the hardware and the driver. Partially Resident Textures are supported in all AMD Radeon 
HD 7xxx GPUs. The functionality is exposed to application developers via the AMDtsparsettexture OpenGL 
extension. PRT support in hardware relies on 3 core components: H\ Virtual Memory subsystem  Page Residency 
information propagation  Driver stack support for efcient mapping/unmapping  5.1 Virtual Memory Memory 
addresses used to fetch texture data on Radeon 7xxx GPUs are virtual. \hen a shader attempts to fetch 
a texel from a texture using UV coordinates, a dedicated GPU block frst computes the virtual memory address 
of the texel (or a block of texels if fltering is used). The address computation depends on the texture 
type, format, UV coordinate values, desired mipmap level, ofset, internal texture tiling, etc. The virtual 
memory address is then fed to the virtual memory subsystem. The VM subsystem performs the virtual-to-physical 
address translation and then initiates a read operation from the physical memory. \hen the read completes, 
the texel data is returned to the shader. The virtual-to-physical address translation inside the VM subsystem 
leverages a dedi­cated hardware page table. This is in stark contrast to software virtual texturing techniques 
in which the page table is just another texture managed by the application. A dedicated hardware page 
table provides several benefts when compared to a software one. Unifed format \hen dealing with software 
(texture) page tables, the application must decide on its size, format, etc. None of this is required 
with hardware page tables as they have a unifed format and support all texture types, formats and sizes. 
The advantage of this is that applications can use diferent formats for diferent purposes, without having 
to rewrite their address encoding schemes. The only downside of the unifed format is that texture tiles 
might have diferent dimen­sions based on what type/format they are using. In the current hardware, the 
page size is fxed to 64kB, which means that a 32-bit RGBA8 texture will have tile dimensions of 128 × 
128 texels. PRT tile dimensions for uncompressed 2D textures are listed in Table 6.1. Texture BPP PRT 
Tile \idth PRT Tile Height 128 64 64 64 128 64 32 128 128 16 256 128 8 256 256 Figure 5.1: PRT tile dimensions 
for uncompressed 2D textures.  Filtering Hardware page tables provide support for all texture fltering 
modes. \ith software page tables, certain fltering modes (e.g. trilinear or anisotropic fltering) are 
very difcult to implement in a robust fashion. As discussed in the previous chapter, this is because 
the physical texture coordinates are not contiguous across page boundaries. The hardware is not aware 
of any page boundaries and therefore cannot flter across pages. On the other hand, PRT-enabled hardware 
supports fltering across page boundaries without issues. Two-level structure Software page tables used 
in virtual texturing are traditionally only one-level (in the virtual memory terminology, they only contain 
the PTEs). This impacts their sizes and does not allow for any kind of compression. Hardware page tables 
can be two-level (they contain both PDEs and PTEs), which decreases their memory footprint if the virtual 
address space is only sparsely populated. Address translation performance Sampling from a virtual texture 
with software page tables amounts to a texture fetch using virtual texture coordinates followed by another 
texture fetch using physical texture coordinates. Both of these require roundtrips between the shader 
and memory, which can incur signifcant performance penalties should cache trashing occur. \ith hardware 
page tables, the address translation happens as a part of the texture fetch that uses virtual texture 
coordinates directly, diminishing the bandwidth requirements by 50% in the general case. Simplifed programming 
\hen dealing with software page tables, the application at­tempting to map/unmap a page needs to take 
care of the page table updates. Hardware page tables are programmed by the S\ driver stack when the client 
application commits/clears individual texture tiles. The application is only required to specify which 
parts of the texture should be resident for the upcoming commands. Caching efciency Hardware page tables 
can take advantage of special H\ caches to speed up lookups and the virtual-to-physical address translation. 
This is not possible with software page tables, as they go down the traditional texture fetch hardware 
path. Consider the following fragment shader that performs sampling using a software (texture) page table: 
uniform sampler2D samplerPageTable; // page table uniform sampler2D samplerPhysTexture; // physical texture 
in vec4 virtUV; // virtual texture coordinates out vec4 color; // output color vec2 getPhysUV(vec4 pte); 
// translation function void main() { vec4 pte = texture(samplerPageTable, virtUV.xy); // 1 vec2 physUV 
= getPhysUV(pte); // 2 color = texture(samplerPhysTexture, physUV.xy); // 3 } Figure 5.2 illustrates 
what happens inside the hardware during software-based virtual­to-physical address translation operation. 
In the frst texture fetch invocation (line 1), the virtual texture coordinates are used to look up the 
PTE (page table entry). The PTE is an application-specifc data structure stored in the page table texture. 
In the next step (line 2), the PTE is converted to physical texture coordinates, again by an application-specifc 
function that deals with the encoding scheme, fltering, formats, etc. Finally (line 3), the physical 
texture coordinates are used to fetch the texture data. From the hardware's point of view, both texture 
fetches (lines 1 and 3) are pretty much identical except that they are accessing diferent textures. One 
important detail to notice is that both texture fetches are dependent, i.e. the second one cannot be 
launched before the frst one completes. Dependent texture fetches are generally not a good approach when 
high-performance is desirable. Now consider a diferent fragment shader that takes advantage of a PRT-enabled 
texture fetch (the sparseTexture() texture sampling instruction is the new instruction introduced in 
the AMDtsparsettexture OpenGL extension -details in Chapter ??): uniform sampler2D samplerVirtTexture; 
in vec4 virtUV; out vec4 color; // // virtual output texture coordinates color void main() { // sparse 
texture fetch int code = sparseTexture(samplerVirtTexture, virtUV.xy, color); } The shader no longer 
uses two dependent texture fetches, but instead, the virtual-to­physical address translation is performed 
directly in hardware based on the virtual texture coordinates passed to the sampling function. The hardware 
function is depicted in Figure 5.3.  5.2 Page/Tile Residency Information The previous section introduced 
the concept of hardware page tables and discussed their advantages in the context of fetching data from 
pages that we know are resident in GPU memory. However, a completely diferent class of algorithms can 
be based on the idea of determining page residency information at runtime. Page fault is a virtual memory 
event that occurs when the client attempts to translate a virtual address that does not have an entry 
in the page table (i.e. the page is not mapped to any physical address). In virtual texturing systems, 
it is very convenient when a shader is able to determine page residency status in an efcient manner. 
Querying page residency status from software page tables is straightforward -the shader performs a texture 
fetch from the page table texture (as in Figure 5.2) and then tests the resulting address for validity 
(an application specifc value can be stored in the page table to indicate unmapped access). The cost 
of the texture fetch is equal to the cost of any other texture fetch. \ith PRTs, the hardware directly 
supports propagating of the page residency information from the page table to the shader core. In other 
words, if the shader attempts to read from an unmapped virtual address, the hardware will report failure 
without having to perform a read from the texture memory. The return code is referred to as a NACK in 
the rest of this document. The sequence of events is illustrated in Figure 5.4.  5.3 Sparse Texture 
Use Cases &#38; Future Development In the frst part of this chapter, we cover some use cases of PRTs 
in the real world and demonstrate techniques that may be implemented using the PRT feature. Use cases 
are enumerated below. In a second part of this chapter, we address some current limitations of the approach 
implemented in AMD's hardware, and some thoughts on future directions. 5.3.1 Very Large Texture Arrays 
First, we discuss the use of very large texture arrays as an application managed cache of textures that 
may be used to virtually eliminate texture binds in a real-time application. Under this scheme, one, 
very large texture array is allocated for each class of texture (say, difuse albedo, specular coefcients, 
normal maps, etc.). These array textures are bound and left bound for the lifetime of the application. 
Each material in a scene is assigned a slice of the array. On current hardware, we are able to support 
more than 8,000 slices in a single texture array, allowing more than 8,000 unique materials to be represented 
in a single array. Of course, a moderate sized array texture (of the order of 2K × 2K texels) with 8,000 
slices consumes more than 10s of gigabytes of address space and so it is impossible to ensure that all 
of the texture data is resident at all times. However, assuming that the live data set for a single rendering 
call can be made resident (i.e., it fts in GPU memory), several advantages arise from using texture arrays 
with sparse textures. The frst of these is that the layout of textures in memory is consistent between 
materials. All materials have access to their difuse albedo, specular coefcient and normal map textures 
if they have them. For those materials that do not have some of those com­ponents, then those slices 
of the array may be left non-resident without consuming precious physical memory -only virtual memory 
is reserved. This simplifes shader development as it allows texture layout to be declared boiler-plate 
style. The second, and perhaps more important aspect to this approach is that the texture array can be 
considered an application-controlled cache. \hen a material is about to be rendered, the application 
must ensure that the relevant slices of the appropriate texture arrays are present in GPU memory. However, 
there is no need to bind new textures as all of the texture data is actually part of the same set of 
array textures. As materials are rendered, new slices of the array are uploaded to the GPU as needed 
and then left resident. If the same slice is needed again, then it is already resident and no texture 
upload or rebind operation is necessary. If, during texture upload, an out-of-memory error is detected, 
slices that are no longer needed may be discarded and a new attempt to make pages resident made. If, 
during the rendering of a single frame, all textures needed ft into GPU memory, then nothing is discarded, 
everything remains resident and no paging is necessary on the next frame. Thus, the subsequent frame 
may be rendered with no texture binds at all. Once the need for binding textures between draw commands 
is eliminated, several com­mon optimizations found in modern realtime graphics engines become redundant. 
For ex­ample, engines often sort or bin geometry in order to reduce state changes. As a change in texture 
is no longer considered a state change, this sorting becomes less important. As another example, large, 
complex models consisting of surfaces with many materials are often broken into several smaller parts 
for rendering. As all of the texture data for these parts can now be made resident simultaneously, this 
can be avoided by simply attaching a per­chunk material ID to what would previously have been separate 
drawing commands. Other graphics features such as instancing become more applicable here. 5.3.2 Incomplete 
Mip-map Chains A second technique that becomes possible with sparse textures is the use of incomplete 
mip­map chains. These may be used for procedurally generated textures or streaming texture data from 
networks, optical drives or other slow media. Under such circumstances, a minimum level of detail is 
made resident before scene rendering begins. This level can be chosen by the developer, but due to artifacts 
of the PRT implementation, is likely to be a minimum of 64KB per texture. This data may, perhaps, be 
kept closer to the engine in the form of a decompressed base-level texture set, or a set of texture data 
that is downloaded frst. During rendering, a record is made of which textures are actually necessary 
during scene traversal. This could be done on the CPU based on some simple CPU-based rendering, through 
GPU assisted techniques such as occlusion queries, or entirely on the GPU by writing texture access data 
into images in GPU memory. The application then periodically examines the list of live textures and brings 
them into GPU memory on demand. On the shader side, an attempt is made to fetch the textures that are 
required to render the scene. If the necessary textures are not resident in GPU memory, a signal is returned 
to the shader to indicate so, and the shader begins traversing the mip-map pyramid until a resident texel 
is found. Because the application made all of the lowest resolution mip-map levels resident during initialization, 
it is guaranteed that some reasonable texture data is found during this pyramid walk. Over the next few 
frames, texture data becomes resident ­either by loading it from the slow resource, or by generating 
it on the fy using the CPU or even the GPU itself. Non-resident textures are displayed as blurry, downsampled 
versions of their higher resolution counterparts at frst, and over the course of one or more frames, 
become sharper. Because no physical address space is required for non-resident texture data, the largest 
resolution layer of the mip-map pyramid need not even exist if it is known a-priori that it will never 
be accessed by the texture. The same algorithm follows, though; traverse the mip-map pyramid, starting 
from the desired LoD until a resident texel is found. 5.3.3 Truly Sparse Textures Truly sparse textures 
are another excellent use case for PRT. For example, consider a tra­ditional texture atlas. In general, 
tools must fnd a balance between tightly packing atlas components in order to conserve empty space, and 
leaving enough space between those com­ponents to avoid bleeding during the generation of the mip-map 
chain. \ith PRT, this is not as necessary. Large regions of unused space may be left empty between components 
of a texture atlas. Any 64KB chunk of texture can be ignored as it will not be allocated in physical 
storage. Large, irregular shapes may be created in the atlas without worrying about flling the voids 
in the convex or even hollow outlines. Sparsity is even more relevant in 3D and volumetric data-sets. 
A 3D scan of a large volume can often consume many gigabytes of storage, but contain large homogeneous 
regions and even voids. By using a PRT to store these types of texture, larger volumes that would previously 
have been impossible to ren­der without complex shader driven page tables may be simply treated as large 
contiguous textures. Those regions that are completely empty may be left entirely un-allocated. For those 
regions where lower frequency or even single-valued data is acceptable, the very lowest level of the 
3D mip-map pyramid may be used. Use in ray-marching or slice-based rendering algorithms of these apparently 
complete data sets is then trivial.  5.4 Current Limitations and Thoughts on the Future The PRT feature 
we are shipping in hardware is certainly very powerful, but does not address all the wants or needs of 
the current SVT community. In particular, the maximum texture size has not changed -it is 16K × 16K × 
8K texels. The limit lies in the precision of the representation of texture coordinates with enough sub-texel 
resolution for artifact-free linear sampling. To some degree, this may be easy to lift, but we are seeing 
requests from developers to go as high as 1M × 1M or more in a single texture. This presents signifcant 
architectural challenges and may or may not be feasible in the near term. It is also easy to see that 
with large textures and high precision texel formats, we start to exhaust even the virtual address space 
of the GPU. The largest possible texture is 16K × 16K × 8K × 16 bytes per texel. This amounts to 32 terabytes 
of linear address space. This far exceeds the addressable space available to the GPU, irrespective or 
residency. Furthermore, as it is backed by the virtual memory subsystem, page table entries need to be 
allocated for those pages referenced by sparse textures. The approximate overhead of the page tables 
for a virtual allocation on current-generation hardware is 0.02% of the virtual allocation size. This 
does not seem like much and for traditional uses of virtual memory, it is not. However, when we consider 
ideas such as allocation of a single texture which consumes a terabyte of virtual address space, this 
overhead is 20GB -much larger than will ft into the GPU's physical memory. To address this, we need to 
consider approaches such as non-resident page tables and page table compression. There are several use 
cases for PRT that seem reasonable but that come with subtle complexities that prevent their clean implementation. 
One such complexity is in the use of PRTs as renderable surfaces. Currently, we support rendering to 
PRTs as color surfaces. \rites to un-mapped regions of the surface are simply dropped. However, supporting 
PRTs as depth or stencil bufers becomes complex. For example, what is the expected behavior of performing 
depth or stencil testing against a non-resident portion of the depth or stencil bufer? Also, supporting 
rendering to MSAA surfaces is not well supported. Because of the way compression works for multisampled 
surfaces, it is possible for a single pixel in a color surface to be both resident and non-resident simultaneously, 
depending on how many edges cut that pixel. For this reason, we do not expose depth, stencil or MSAA 
surfaces as renderable on current generation hardware. The operating system is another component in the 
virtual memory subsystem which must be considered. Under our current architecture, a single virtual allocation 
may be backed by multiple physical allocations. Our driver stack is responsible for virtual address space 
allocations whereas the operating system is responsible for the allocation of physical address space. 
The driver informs the operating system how much physical memory is available and the operating system 
creates allocations from these pools. During rendering, the operating system can ask the driver to page 
physical allocations in and out of the GPU memory. The driver does this using DMA and updates the page 
tables to keep GPU virtual addresses pointing at the right place. During rendering, the driver tells 
the operating system which allocations are referenced by the application at any given point in the submission 
stream and the operating system responds by issuing paging requests to make sure they are resident. \hen 
there is a 1-to-1 (or even a many-to-1) correspondence between virtual and physical allocations, this 
works well. However, when a large texture is slowly made resident over time, the list of physical allocations 
referenced by a single large virtual allocation can become very long. This presents some performance 
challenges that real-world use will likely show us in the near term and will need to be addressed. Chapter 
6  High Quality Software and Hardware Virtual Textures 6.1 High Quality Software Virtual Textures Modern 
simulations increasingly require the display of very large, uniquely textured worlds at interactive rates. 
In large outdoor environments and also high detail indoor environ­ments, like those displayed in the 
computer game RAGE, the unique texture detail requires signifcant storage and bandwidth. Virtual textures 
reduce the cost of unique texture data by providing a sparse representation which does not require all 
of the data to be present for rendering, while leaving the majority of the texture data in highly compressed 
form on secondary storage. A virtual texture is divided into small pages that are loaded into a pool 
of resident physical pages as required for rendering. In RAGE these small pages are square blocks of 
128 x 128 texels and the pool with physical pages is a fully resident texture that is logically subdivided 
into such square blocks of texels. \hile a virtual texture can be very large (say a million pages) and 
is never fully resident in video memory, the texture that holds the pool of physical pages is fully resident 
but much smaller (typically only 4096 x 4096 texels or 1024 pages). Virtual texture pages are mapped 
to physical texture pages, and during rendering virtual addresses need to be translated to physical ones. 
Virtual textures difer from other forms of virtual memory because frst, it is possible to fall back to 
slightly blurrier data without stalling execution, and second, lossy compression of the data is perfectly 
acceptable for most uses. Implementations of software virtual textures exploit these key diferences between 
virtual textures and other forms of virtual memory to maintain performance and reduce memory requirements 
at the cost of quality. Implement­ing virtual textures without special hardware support is challenging 
and inevitably comes down to fnding the right trade between performance, memory requirements, and quality. 
\hile the implementation of software virtual textures in RAGE emphasized performance, the visual fdelity 
of the virtual textures in RAGE can be improved in several ways that trade performance and memory for 
quality. 6.1.1 Explicit Page Table LOD The high performance software virtual textures implemented in 
RAGE use page table tex­tures to perform the virtual to physical translation. Such a page table texture 
must be point-sampled to retrieve individual page mappings without mangling the data by blending between 
adjacent but independent page mappings. Using the texture hardware to point­sample a page table texture 
does not necessarily result in a mapping to a physical texture page with the appropriate texture detail 
for the anisotropic texture fetch that follows. \ith­out any adjustments, the page table lookup returns 
a mapping to a physical texture page that is typically too coarse and provides too little detail. To 
provide more detail the page table lookup can be biased with the base-two-logarithm of the maximum anisotropic 
footprint. This results in the page table lookup returning a mapping to a texture page with enough detail 
for the anisotropic flter to work well on surfaces at an oblique angle to the viewer where the sampled 
footprint is maximized (anisotropic). However, this can cause noticeable shimmering or aliasing on surfaces 
that are orthogonal to the view direction where the sam­pled footprint is minimal (isotropic). To improve 
the quality, the correct LOD for the page table texture lookup can be calculated in the fragment program 
based on the anisotropy. The calculated LOD can then be explicitly passed to the page table texture lookup. 
The calculation of the page table LOD can be found below. Calculating the page table LOD for every fragment 
adds signifcant fragment program complexity. const float maxAniso = 4; const float maxAnisoLog2 = log2( 
maxAniso ); const float virtPagesWide = 1024; const float pageWidth = 128; const float pageBorder = 4; 
const float virtTexelsWide = virtPagesWide ( pageWidth -2 pageBorder ); * * vec2 texcoords = virtCoords.xy 
virtTexelsWide; * vec2 dx = dFdx( texcoords ); vec2 dy = dFdy( texcoords ); float px = dot( dx, dx ); 
float py = dot( dy, dy ); float maxLod = 0.5 * log2( max( px, py ) ); // log2(sqrt()) = 0.5*log2() float 
minLod = 0.5 log2( min( px, py ) ); * float anisoLOD = maxLod -min( maxLod -minLod, maxAnisoLog2 ); 
In RAGE texture feedback is rendered to a separate bufer that, for the virtual texture pages used in 
the current scene, stores the virtual page coordinates (x,y), desired mip level, and virtual texture 
ID (to allow multiple virtual textures). This feedback data is then used to make those virtual texture 
pages resident that are needed to render the current scene. Interestingly, the calculation of the desired 
mip level for texture feedback is equivalent to the calculation of the page table LOD. \hile in RAGE 
the feedback was rendered in a separate pass, the feedback can also be generated during normal rendering 
by using multiple render targets. This allows the texture LOD to be calculated once after which it can 
be used for both texture feedback and the page table texture lookup. 6.1.2 Tri-Linear Filtering and 
LOD Clamping It will happen sometimes that, despite all eforts, a signifcant latency will be incurred 
between the time that a texture page is needed and the time that the data for it is available. This can 
result in an unpleasant "pop" when the desired LOD for a page is of by more than one level and the right 
LOD suddenly becomes available. If tri-linear fltering with two virtual to physical translations is employed, 
then it is possible to include some delay in the transition from coarser to fner mip level when the desired 
level is not sufciently close to the currently displayed level. Tri-linear fltering with two virtual 
to physical translations is expensive because it requires double the number of page table and physical 
texture lookups. Tri-linear fltering by blending between two anisotropic texture lookups does, however, 
provide a noticeable quality improvement. Gradually blending in fner detail requires a minimum LOD texture 
with one texel per virtual page. The texels of the minimum LOD texture are gradually adjusted to reveal 
more and more detail after a new texture page has been made resident. The minimum LOD texture is used 
to clamp the texture lookup in the fragment program, forcing a gradual transition to fner detail as opposed 
to a sudden change which is perceived as a "pop". uniform sampler2D pageTable; // RGBA-FP32 -{ scaleS, 
scaleT, biasS, biasT } uniform sampler2D minLodTexture; // R-8 -{ minimum-LOD } uniform sampler2D physicalTexture; 
// RGBA-8 -{ red, green, blue, alpha } in vec4 virtCoords; // virtual texture coordinates out vec4 color; 
// output color void main() {  float anisoLOD = maxLod -min( maxLod -minLod, maxAnisoLog2 ); const 
float maxVirtMipLevels = 16; float clampLod = texture( minLodTexture, virtCoords.xy ).x maxVirtMipLevels; 
 * anisoLOD = max( anisoLOD, clampLod ); } The above code shows the complete fragment program that combines 
the explicit page table LOD calculation from the previous section (in blue), clamping of the page table 
LOD using a minimum LOD texture (in red), and tri-linear fltering using two page table lookups and two 
physical texture lookups (in green). 6.1.3 Texture Upsampling A common approach to increase the perceived 
detail on textured surfaces is to add detail textures. A detail texture is a small texture with high 
frequency data that can be easily tiled many times over many surfaces. Detail textures are blended over 
regular textures to give textured surfaces a more detailed appearance without increasing the texture 
resolution. Not only are detail textures yet another specialized form of texture compression, detail 
textures also have several limitations and disadvantages such as: local modulation, limited variety, 
creation cost, selection cost and run-time cost. Instead of using manually created and applied detail 
textures, the texture detail on rendered surfaces can also be programmatically enhanced. Normally the 
texture hardware uses a bilinear flter for texture magnifcation during rendering. This flter is implemented 
in graphics hardware and is very fast. Unfortunately, bilinear fltering tends to produce interpolation 
artifacts such as blurring and edge halos. More advanced upsampling flters and texture enhancement algorithms 
can be implemented in a fragment program. However, programmatic upsampling and enhancement of texture 
data in a fragment program is usually very costly. Virtual textures make it possible to upsample and 
enhance existing texture data without increasing the per fragment rendering cost. A virtual texture can 
be extended to have many more mip levels for which actual source data does not exist. A texture page 
for which no original source data is available can then be generated from a coarser parent page that 
does have source data. As opposed to upsampling the texture data for every rendered fragment, the cost 
is amortized by only upsampling and enhancing texture data once, as the view approaches a surface and 
the texture pages for that surface are made resident. Various interesting upsampling algorithms can be 
used to generate additional detail. 6.1.4 Direct Texture Access On systems with direct access to texture 
memory, the physical texture pages and page table textures can be updated asynchronously to the GPU. 
However, on systems where the only access to video memory is through an API like OpenGL or DirectX, the 
physical texture and page table texture updates cannot generally overlap with rendering and face signifcant 
API overhead. For instance, in RAGE more than 6 milliseconds of CPU time may be spent uploading and/or 
copying texture data through the graphics driver. For improved memory access patterns and consequently 
improved performance, textures are usually stored in tiled formats. Tiling of textures improves the spatial 
locality of the texture data for typical texture sampling patterns during rendering. Texture tiling is 
one of the reasons direct access to texture memory is not generally supported. By allowing direct texture 
access, the texture needs to be either stored in a linear (non-tiled) format or the application needs 
to be aware of the particular tiled format being used. More importantly by allowing direct texture access 
it is no longer possible to change the tiled format under­neath an application. Being able to change 
the tiling formats for existing applications can be important to achieve performance gains by developing 
new tiling formats that are designed specifcally for the memory architecture of new graphics hardware. 
However, while these performance gains may be very important for some applications, other applications 
may not beneft as much. For instance, the virtual textures in RAGE do not appear to beneft as much from 
diferent tiled formats. In particular when the texture data is stored in a block compression format such 
as DXT/S3TC/ETC, the benefts appear to be small because the block compression formats already improve 
the spatial locality by encoding small blocks of texels. Sampling of page table textures during rendering 
always exhibits very good locality and storing them in a linear format has good performance. At the same 
time, not allow­ing direct texture access and forcing texture updates through the driver causes signifcant 
overhead. Direct access to tiled textures is possible as long as the application is aware of the particular 
tiled format that is being used. For instance, in RAGE the texture transcoders can be easily modifed 
to write directly to textures in a tiled format as long as the tiled format is clearly specifed and does 
not change after the release of the game. \hile there may be a noticeable performance delta between linear 
and tiled textures, there is usually a much smaller performance delta between diferent tiled formats. 
In other words, having a couple of standardized tiled texture formats seems particularly useful.  6.2 
Hardware Virtual Textures \hen RAGE frst shipped there was no special hardware support for virtual textures. 
The virtual to physical address translation had to be implemented in a fragment program through page 
table and mapping textures. The latest AMD graphics hardware, however, supports hardware virtual textures 
also known as Partially Resident Textures (PRTs). Instead of using page table and mapping textures the 
hardware can perform the virtual to physical translation using the page tables of the underlying virtual 
memory system. Taking advantage of this special hardware does require some changes to the virtual texture 
system in RAGE. In RAGE, multiple virtual textures can be mapped to the same pool with physical pages 
or a single virtual texture can be mapped to multiple pools with physical pages. \hen using PRTs, a virtual 
texture is not mapped to one of the existing physical pages pools. Instead, a new physical pages pool 
is allocated for each virtual texture. Such a private pool does not use a regular small texture but instead 
the pool is implemented as a PRT with the same size as the virtual texture. Instead of mapping and unmapping 
texture pages using physical coordinates, pages are mapped and unmapped using virtual coordinates. The 
hardware then takes care of the virtual to physical address translation and there is no need for page 
table textures. This is an elegant solution that requires minimal changes throughout the RAGE virtual 
texture system. Nevertheless, various changes are necessary to properly support PRTs. 6.2.1 PRT Page 
Management In RAGE physical texture pages are allocated from fxed size physical textures. These physical 
textures tend to fll up quickly and once all pages from a physical texture have been allocated, an existing 
page will have to be unmapped and freed before a new texture page can be allocated. The page that is 
unmapped and freed may have been used for a diferent virtual texture when multiple virtual textures map 
to the same physical texture. A hardware virtual texture in the form of a PRT can have as many pages 
resident as there is physical memory available. \hen a PRT page is mapped the memory allocation will 
either succeed or fail based on whether or not physical memory is available. To make sure the virtual 
texture pages for a particular scene can be made resident, it is desirable to track all the resident 
texture pages and unmap and free those texture pages that are no longer needed. To allow a PRT page to 
be mapped without running out of physical memory, a page management system must be implemented that can 
track and if necessary unmap and free any PRT page from any virtual texture. \ith such a page management 
system in place, allocating a physical page for one PRT may cause a page from another PRT to be unmapped 
and freed. 6.2.2 PRT Size Limitation \hile a virtual texture in RAGE can be up to 128k x 128k, the maximum 
size of a PRT is only 16k x 16k. As a result, a large virtual texture cannot be mapped directly to a 
PRT. A virtual texture that is larger than 16k x 16k needs to somehow map to multiple PRTs. One solution 
is to use a Partially Resident Texture Array. This allows a virtual texture to be logically broken up 
into 16k x 16k tiles that map to the PRTs stored in an array. In a fragment program, the array index 
and the texture coordinates within the PRT can be calculated from the texture coordinates that address 
the full virtual texture through multiplication by the size of the full virtual texture divided by 16k. 
The integer parts can then be used to calculate the array index and the fractional parts are the texture 
coordinates within the PRT. To allow proper texture fltering without seams, this does, however, require 
that no texture island crosses a 16k boundary on the full virtual texture. These texture islands are 
chunks of geometry that are contiguous in texture space, often called "charts" in an "atlas" in literature. 
Texture islands larger than 16k x 16k will also have to be broken up into multiple islands. The algorithm 
that creates the layout of a virtual texture has to be modifed to split up islands that are too large, 
and to keep texture islands from crossing 16k boundaries on the full virtual texture. Splitting up texture 
islands larger than 16k x 16k is not always easy without introducing noticeable seams. Fortunately, few 
texture islands are actually larger than 16k x 16k. Making sure that texture islands never cross a 16k 
boundary on the virtual texture is much easier and can, for instance, be done by pre-allocating the texels 
on the 16k boundaries such that those texels cannot be allocated for any of the actual texture islands. 
 6.2.3 Compressed PRT Pages PRT texture pages do not have a fxed size in texels. Instead PRT texture 
pages always have a fxed size in memory which on current AMD graphics hardware is 64 kB. As a result, 
the size in texels of a texture page varies based on the texture format and compression. Format Tile 
\idth Tile Height uncompressed RGBA-8 DXT5/BC3 compressed DXT1/BC1 compressed 128 256 512 128 256 256 
 Table 6.1: Tile dimensions for compressed 2D textures. To keep things simple, uncompressed RGBA-8 texture 
pages can be used for a frst pass implementation of PRTs. To support DXT compression, multiple texture 
pages on disk have to be transcoded and uploaded simultaneously. 6.2.4 Borderless PRT Pages One of the 
unfortunate complexities of software virtual textures without special graphics hardware support is that 
the texture unit, being unaware of the actual texture pages, cannot flter across page boundaries. Texture 
pages that are adjacent in virtual texture space do not necessarily map to physical pages that are next 
to each other, let alone close to each other in the physical texture. In order to properly support hardware 
bi-linear fltering of software virtual textures, each physical texture page must have a border of texels 
around it. Hardware accelerated anisotropic fltering of software virtual textures can be supported if 
the page border is wider than one texel. \ith virtual textures supported in hardware in the form of PRTs 
there is no need for page borders. However, the virtual texture pages in RAGE are stored on disk with 
page borders. The texture pages in RAGE are 128 x 128 texels with a 120 x 120 payload surrounded by a 
4 texel border. One option is to upsample the center 120 x 120 to 128 x 128 when a page is transcoded 
from highly compressed form on disk to a format the GPU can use directly for rendering. Unfortunately 
this causes noticeable blurring due to the non-integer upsampling ratio. For the best quality the virtual 
textures will have to be reformatted to strip the borders and to re-subdivide the virtual texture into 
texture pages with a full 128 x 128 payload.  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2013</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
</content>
</proceeding>
