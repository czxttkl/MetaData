<?xml version="1.0" encoding="ISO-8859-1"?>
<!DOCTYPE proceeding SYSTEM "proceeding.dtd">
<proceeding ver="6.0" ts="04/10/2010">
<conference_rec>
	<conference_date>
		<start_date>07/21/2013</start_date>
		<end_date>07/25/2013</end_date>
	</conference_date>
	<conference_loc>
		<city><![CDATA[Anaheim]]></city>
		<state>California</state>
		<country></country>
	</conference_loc>
	<conference_url></conference_url>
</conference_rec>
<series_rec>
	<series_name>
		<series_id>SERIES382</series_id>
		<series_title><![CDATA[International Conference on Computer Graphics and Interactive Techniques]]></series_title>
		<series_vol></series_vol>
	</series_name>
</series_rec>
<proceeding_rec>
	<proc_id>2504459</proc_id>
	<acronym>SIGGRAPH '13</acronym>
	<proc_desc>ACM SIGGRAPH 2013 Talks</proc_desc>
	<conference_number>2013</conference_number>
	<proc_class>conference</proc_class>
	<proc_title></proc_title>
	<proc_subtitle></proc_subtitle>
	<proc_volume_no></proc_volume_no>
	<isbn13>978-1-4503-2344-4</isbn13>
	<issn></issn>
	<eissn></eissn>
	<copyright_year>2013</copyright_year>
	<publication_date>07-21-2013</publication_date>
	<pages>25</pages>
	<plus_pages></plus_pages>
	<price><![CDATA[]]></price>
	<other_source></other_source>
	<abstract>
		<par><![CDATA[<p>Talks are a less formal alternative to formal publication. They often highlight the latest developments before publication, present ideas that are still in progress, or showcase how computer graphics and interactive techniques are actually implemented and used, in graphics production or other fields. Talks take you behind the scenes and into the minds of SIGGRAPH 2013 creators in all areas of computer graphics and interactive techniques, including art, design, animation, visual effects, interactivity, research, and engineering.</p>]]></par>
	</abstract>
	<publisher>
		<publisher_id>PUB27</publisher_id>
		<publisher_code>ACMNY</publisher_code>
		<publisher_name>ACM</publisher_name>
		<publisher_address>2 Penn Plaza, Suite 701</publisher_address>
		<publisher_city>New York</publisher_city>
		<publisher_state>NY</publisher_state>
		<publisher_country>USA</publisher_country>
		<publisher_zip_code>10121-0701</publisher_zip_code>
		<publisher_contact>Bernard Rous</publisher_contact>
		<publisher_phone>212 869-7440</publisher_phone>
		<publisher_isbn_prefix></publisher_isbn_prefix>
		<publisher_url>www.acm.org/publications</publisher_url>
	</publisher>
	<sponsor_rec>
		<sponsor>
			<sponsor_id>SP932</sponsor_id>
			<sponsor_name>ACM Special Interest Group on Computer Graphics and Interactive Techniques</sponsor_name>
			<sponsor_abbr>SIGGRAPH</sponsor_abbr>
		</sponsor>
	</sponsor_rec>
	<categories>
		<primary_category>
			<cat_node/>
			<descriptor/>
			<type/>
		</primary_category>
	</categories>
	<ccc>
		<copyright_holder>
			<copyright_holder_name>ACM</copyright_holder_name>
			<copyright_holder_year>2013</copyright_holder_year>
		</copyright_holder>
	</ccc>
</proceeding_rec>
<content>
	<section>
		<section_id>2504460</section_id>
		<sort_key>10</sort_key>
		<section_seq_no>1</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[<i>Epic</i> tale]]></section_title>
		<section_page_from>1</section_page_from>
	<article_rec>
		<article_id>2504461</article_id>
		<sort_key>20</sort_key>
		<display_label>Article No.</display_label>
		<display_no>1</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Crafting the vision effect]]></title>
		<subtitle><![CDATA[an interactive, particle-based hologram for epic]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2504459.2504461</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2504461</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4193562</person_id>
				<author_profile_id><![CDATA[82458648857]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Andrew]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Schneider]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4193563</person_id>
				<author_profile_id><![CDATA[82459264157]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Matthew]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Roach]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4193564</person_id>
				<author_profile_id><![CDATA[82458793457]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Justin]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gladis]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2013</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>2504462</article_id>
		<sort_key>30</sort_key>
		<display_label>Article No.</display_label>
		<display_no>2</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Directable fluids in the world of epic]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2504459.2504462</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2504462</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4193565</person_id>
				<author_profile_id><![CDATA[82459107457]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Ilan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gabai]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4193566</person_id>
				<author_profile_id><![CDATA[82458708157]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[David]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Quirus]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4193567</person_id>
				<author_profile_id><![CDATA[82458745857]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Alen]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lai]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4193568</person_id>
				<author_profile_id><![CDATA[82458804757]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Diego]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Garzon]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2013</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>2504463</article_id>
		<sort_key>40</sort_key>
		<display_label>Article No.</display_label>
		<display_no>3</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Procedural texturing in epic]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2504459.2504463</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2504463</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4193569</person_id>
				<author_profile_id><![CDATA[82458655657]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Hugo]]></first_name>
				<middle_name><![CDATA[M.]]></middle_name>
				<last_name><![CDATA[Ayala]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4193570</person_id>
				<author_profile_id><![CDATA[82458690757]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Jamie]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Macdougall]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4193571</person_id>
				<author_profile_id><![CDATA[82458815557]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Chris]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Chapman]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2013</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>2504464</article_id>
		<sort_key>50</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>4</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[Bats, birds, and boggans]]></title>
		<subtitle><![CDATA[the simulated armies of <i>epic</i>]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2504459.2504464</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2504464</url>
		<abstract>
			<par><![CDATA[<p>In <i>Epic</i> (2013), crowds are integral to the narrative and form a character as a whole. This required a new type of crowd at Blue Sky Studios, one that permits dynamic interaction between crowd characters and the environments around them in addition to supporting the high-resolution geometry with fur, deformation rigs, and material complexities needed for shots where the crowd is close to camera. Our crowd framework centers around the choice to separate the simulation process from the technique used to render the crowd. This meant we could use different simulators for different shots. At times, the crowd exceeded 100,000 characters, far more than in any of our previous films. To manage all this data we store only per character joint animation instead of deformed geometry. This compact format allows us to both display art direct-able representations of the crowd in real-time and to defer evaluation of the expensive parts of the rig until render time. To render the crowd with our in-house ray-tracing renderer, <i>CGIStudio#8482;</i>, we build a custom, optimized deformation system that supports rendering of both deformed geometry and deformed voxels.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4193572</person_id>
				<author_profile_id><![CDATA[82458748357]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Thierry]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Dervieux-Lecocq]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Blue Sky Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[thierry@blueskystudios.com]]></email_address>
			</au>
			<au>
				<person_id>P4193573</person_id>
				<author_profile_id><![CDATA[81309504515]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[David]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gatenby]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Blue Sky Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[davidg@blueskystudios.com]]></email_address>
			</au>
			<au>
				<person_id>P4193574</person_id>
				<author_profile_id><![CDATA[82458707057]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Mark]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Adams]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Blue Sky Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[marka@blueskystudios.com]]></email_address>
			</au>
			<au>
				<person_id>P4193575</person_id>
				<author_profile_id><![CDATA[81300400801]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Justin]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bisceglio]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Blue Sky Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[justinb@blueskystudios.com]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Bats, Birds, and Boggans: The Simulated Armies of Epic Thierry Dervieux-Lecocq, David Gatenby, Mark 
Adams, and Justin Bisceglio Blue Sky Studios*  c Figure 1: Crowd Shots, Epic &#38;#169; 2013 Twentieth 
Century Fox Film Corporation. All Rights Reserved. 1 Introduction In Epic (2013), crowds are integral 
to the narrative and form a char­acter as a whole. This required a new type of crowd at Blue Sky Studios, 
one that permits dynamic interaction between crowd char­acters and the environments around them in addition 
to supporting the high-resolution geometry with fur, deformation rigs, and ma­terial complexities needed 
for shots where the crowd is close to camera. Our crowd framework centers around the choice to sepa­rate 
the simulation process from the technique used to render the crowd. This meant we could use different 
simulators for different shots. At times, the crowd exceeded 100,000 characters, far more than in any 
of our previous .lms. To manage all this data we store only per character joint animation instead of 
deformed geometry. This compact format allows us to both display art direct-able rep­resentations of 
the crowd in real-time and to defer evaluation of the expensive parts of the rig until render time. To 
render the crowd with our in-house ray-tracing renderer, CGIStudioTM, we build a custom, optimized deformation 
system that supports rendering of both deformed geometry and deformed voxels. 2 Up, Up, and Away Sixty 
percent of the simulated crowd shots in Epic featured charac­ters that walked, ran, and climbed on the 
ground or on walls. We turned to Massive exclusively for simulation on these shots to take advantage 
of its brain editing toolset for controlling the motion and behavior of individual agents. The other 
forty percent of shots con­sisted of highly choreographed .ocking bats, grackles, and hum­mingbirds with 
riders on their backs. For these crowds we decided on a hybrid approach using Houdini for the main .ocking 
motion and .nishing in Massive to overlay animation cycles and secondary motion. Hummingbirds and bats 
each have their own unique, styl­ized way of .ying. To simulate the specialized .ight patterns of our 
animals we created an expanded version of the Boids .ocking model in a Houdini DOP network. We modeled 
the erratic .ight of bats using noisy .gure eight cycles and for the hummingbirds over­laid sharp lateral 
xy-plane bursts. Once the motion and .ow of each .ock was approved we would import this data into Massive 
to con­trol the agents using a custom API plug-in. Through this plugin we could send arbitrary attributes 
created in Houdini to act as inputs to the Massive agent s brain. This allowed us to easily trigger actions, 
choose weapon types, or add secondary motion without having to create complex new brains modules inside 
Massive. 3 Creating, Viewing, and Editing Having developed our own render-time deformation system, we 
could save just the skeletal joint animation and not create geom­etry caches at simulation time. This 
decreased the .le footprint on disk while also increasing display and simulation speeds. To ac­curately 
visualize the crowd during post-simulation editing and di­rector approval, we created a real-time crowd 
viewing engine. Be­cause many departments at Blue Sky use Maya, we incorporated our viewing engine into 
Maya s existing viewport so the crowds could be seen in context with the rest of the scene and available 
for other departments, without needing to render. Various geome­try resolutions were available to the 
user, each displaying accurate material representation. This allowed the directors to evaluate and make 
notes con.dently knowing what they are seeing will hold true through render. The engine also provides 
a GUI that allows for art directing the crowd, including changing character and garment vari­ation, LOD, 
and transformations. Most editable features of the GUI are also provided as render-time handles. The 
engine uses OpenGL vertex buffer objects for each character that are transformed using the simulation 
data. For a crowd of 100,000, the data is about 1,000 times smaller than if we used a geo-cache, making 
it easy to .t in the GPU. 4 Render Optimization In rendering shots with crowds, taking an aggressive 
approach to minimizing the storage of geometric data had handsome payoffs. Rather than storing geometry 
for every deformed crowd character on disk, we developed a rendering system in CGIStudioTM where only 
the skeletal animation and key-framed attribute data was re­quired. Each crowd character species shares 
one rig and each vari­ation in a species shares one mesh. For each crowd character, in­dividual animation 
is supplied to the rig and evaluation of the rig generates a uniquely deformed mesh. We found that reading 
this compact animation data from disk combined with evaluating a rig for each character was faster than 
reading cached geometric point data from disk by a ratio of more than 1000:1. We built support for rendering 
the crowd characters as either geometry or as voxels. Us­ing a sparse voxel octree, our voxel data requires 
less memory than geometry. Moreover, when rendering the crowd as voxels we are able to entirely eliminate 
geometric data from main memory while ray-tracking. This is because our voxel data can be deformed di­rectly 
by the rig. Overall, this integrated render-time deformation system allowed us to streamline both the 
rendering and the pipeline of our crowd assets for Epic. *E-mail: {thierry,davidg,marka,justinb}@blueskystudios.com 
Permission to make digital or hard copies of part or all of this work for personal or classroom use isgranted 
without fee provided that copies are not made or distributed for commercial advantage and that copies 
bear this notice and the full citation on the first page. Copyrights for third-party components of thiswork 
must be honored. For all other uses, contact the Owner/Author.SIGGRAPH 2013, July 21 25, 2013, Anaheim, 
California. 2013 Copyright held by the Owner/Author.ACM 978-1-4503-2261-4/13/07 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2013</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	</section>
	<section>
		<section_id>2504465</section_id>
		<sort_key>60</sort_key>
		<section_seq_no>2</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Getting riggy with it]]></section_title>
		<section_page_from>2</section_page_from>
	<article_rec>
		<article_id>2504466</article_id>
		<sort_key>70</sort_key>
		<display_label>Article No.</display_label>
		<display_no>5</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Pixels to parks]]></title>
		<subtitle><![CDATA[new animation techniques for fantasyland]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2504459.2504466</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2504466</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4193576</person_id>
				<author_profile_id><![CDATA[81414592674]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Akhil]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Madhani]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4193577</person_id>
				<author_profile_id><![CDATA[82458877857]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Justin]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Walker]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4193578</person_id>
				<author_profile_id><![CDATA[82459126457]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Gene]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lee]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4193579</person_id>
				<author_profile_id><![CDATA[82459131857]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Aaron]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Adams]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4193580</person_id>
				<author_profile_id><![CDATA[82459212457]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Alexis]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wieland]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4193581</person_id>
				<author_profile_id><![CDATA[82458813657]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Evan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Goldberg]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2013</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>2504467</article_id>
		<sort_key>80</sort_key>
		<display_label>Article No.</display_label>
		<display_no>6</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Mixing dynamics and blend shapes for the Hulk]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2504459.2504467</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2504467</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4193582</person_id>
				<author_profile_id><![CDATA[82459188957]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Julien]]></first_name>
				<middle_name><![CDATA[Cohen]]></middle_name>
				<last_name><![CDATA[Bengio]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4193583</person_id>
				<author_profile_id><![CDATA[82459170357]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Chase]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Cooper]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4193584</person_id>
				<author_profile_id><![CDATA[81100302526]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[John]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Doublestein]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2013</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>2504468</article_id>
		<sort_key>90</sort_key>
		<display_label>Article No.</display_label>
		<display_no>7</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Simplicial interpolation for animating the Hulk]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2504459.2504468</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2504468</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4193585</person_id>
				<author_profile_id><![CDATA[82459188957]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Julien]]></first_name>
				<middle_name><![CDATA[Cohen]]></middle_name>
				<last_name><![CDATA[Bengio]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4193586</person_id>
				<author_profile_id><![CDATA[81100201570]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Rony]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Goldenthal]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2013</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>2504469</article_id>
		<sort_key>100</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>8</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[BlockParty 2]]></title>
		<subtitle><![CDATA[visual procedural rigging for film, TV, and games]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2504459.2504469</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2504469</url>
		<abstract>
			<par><![CDATA[<p>At Lucasfilm, riggers create thousands of rigs each year to support films, television shows, and video games. These rigs vary greatly in complexity and type, ranging from simple props to giant multi-limbed creatures. <i>BlockParty 2</i> (BP2), a new Maya-based procedural rigging system named after its predecessor [Smith and White 2006], allows expert and novice riggers to build sharable rigs visually using expertly-designed rig pieces.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4193587</person_id>
				<author_profile_id><![CDATA[82459255657]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Rachel]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Rose]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Industrial Light & Magic/Lucasfilm]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4193588</person_id>
				<author_profile_id><![CDATA[82459197357]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Mike]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Jutan]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Industrial Light & Magic/Lucasfilm]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4193589</person_id>
				<author_profile_id><![CDATA[81100302526]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[John]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Doublestein]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Industrial Light & Magic/Lucasfilm]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1179993</ref_obj_id>
				<ref_obj_pid>1179849</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Smith, J., and White, J. 2006. BlockParty: Modular Rigging Encoded in a Geometric Volume. In <i>ACM SIGGRAPH 2006 Sketches (SIGGRAPH '06)</i>, Article 115. ACM, New York, NY.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 BlockParty 2: Visual Procedural Rigging for Film, TV, and Games biped.1.edited.jpg spider.1.edited.jpg 
car.1.edited.jpg horse.1.edited.jpg ui.1.edited.jpg Rachel Rose Mike Jutan John Doublestein Industrial 
Light &#38; Magic / Lucasfilm    Figure 1. The BlockParty 2 visual rigging interface for four characters: 
a biped, a spider, a car, and a horse. 1. Introduction At Lucasfilm, riggers create thousands of rigs 
each year to support films, television shows, and video games. These rigs vary greatly in complexity 
and type, ranging from simple props to giant multi-limbed creatures. BlockParty 2 (BP2), a new Maya-based 
procedural rigging system named after its predecessor [Smith and White 2006], allows expert and novice 
riggers to build sharable rigs visually using expertly-designed rig pieces. BP2 has two defining characteristics 
that make it uniquely well-suited for Lucasfilm s varied rigging needs: . Rig pieces establish contracts 
that describe what information is needed from the Maya scene and how other rig pieces can be connected; 
enforcement of these contracts makes rig pieces flexible for many use cases. . Rig pieces are presented 
in a discoverable, user-friendly UI for visually constructing, customizing, and reproducing rigs.  2. 
Contractual Block Design BP2 is built around the concept of a block, or a rig piece that has a high-level 
purpose (e.g. an arm or hinge). Blocks can be self-contained in that they work without other blocks (e.g. 
a finger), or they can be an add-on block that extends existing blocks (e.g. a block that adds a twist 
rig to joints from another block). Multiple blocks can be connected and exported to create a compound 
block (e.g. a full limb or an entire character); compound blocks can be used to create new rigs just 
like other blocks. The challenge is to make each block in the system as reusable as possible. The key 
to the flexibility of the blocks in BP2 is that the system provides standardized mechanisms for connecting 
them. Block connections come in two forms: plug/socket connections (a concept expanded upon from the 
original BlockParty system) and hook/handle connections. When a plug from one block is connected to a 
socket on another, a parenting relationship is formed in the rig. Similarly, blocks can expose specific 
Maya nodes as handles; when a hook is connected to a handle, the block with the hook gains access to 
the exposed node. While plugs and hooks are often designed to be paired with specific sockets and handles, 
respectively, the BP2 system enforces the contractual obligation between any two connected blocks; this 
standardization of interactions allows any block to be connected to any other. For instance, a hind-leg 
s plug can be connected to a clavicle s socket to create a clavicle-leg; or a double-jointed knee hook 
can be connected to an elbow joint to create a double-jointed elbow. Another way that BP2 encourages 
reuse is by limiting which nodes are accessible in the existing Maya scene. Each block advertises a list 
of required signposts, or 3D positions, orientations, and/or scale values that the block needs in order 
to place Maya nodes during rigging (e.g. signposts for a spine include the positions at the bottom and 
top of the spine). BP2 passes only this information from the Maya scene to the block when rigging occurs. 
The end result is that blocks can be more easily re-purposed by assigning and repositioning signposts. 
3. Visual Rig Construction      Figure 2. BlockParty 2 User-Interface. Left: Library of available 
blocks. Right: In-place signpost assignment panel. Since Lucasfilm produces so many rigs, it is important 
that the rigging interface for BP2 be intuitive for riggers and non-riggers. So, unlike other procedural 
rigging systems, rigging in BP2 is achieved via a visual, 2D interface. The UI allows users to easily 
instantiate blocks by dragging them from a block library onto a canvas and connecting them visually (see 
Figure 1). A user modifies settings in-place for each block by expanding the block s visual representation 
in the UI; this expanded view of a block includes several task-specific panels. For instance, one panel 
allows a user to assign required signposts for a block to existing nodes in the Maya scene; the panel 
can also import a template for all of the required signposts that can be resized/repositioned in the 
3D view using Maya's normal transformation tools (see Figure 2). This visual approach to procedural rigging 
allows riggers of all skill levels to spend more time conceptually designing a rig and less time navigating 
nodes in the Maya scene or scripting. The BP2 UI presents users with available blocks in a simple to 
navigate library panel (see Figure 2). Tagged blocks from all of Lucasfilm are submitted to the same 
database. The library comes with a number of predefined, filtered views of the tagged blocks in the system, 
but users can also create customized views for specific production needs or personal preference. The 
BP2 system has proven to scale well across a wide variety of rig complexities. Props, bipeds, multi-limbed 
creatures, and mechanical characters have all been created with BP2. At Lucasfilm, the ease-of-use and 
flexibility of BP2 has swiftly led to its adoption for films, television shows, and video games. References 
SMITH, J., AND WHITE, J. 2006. BlockParty: Modular Rigging Encoded in a Geometric Volume. In ACM SIGGRAPH 
2006 Sketches (SIGGRAPH 06), Article 115. ACM, New York, NY. 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2013</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>2504470</article_id>
		<sort_key>110</sort_key>
		<display_label>Article No.</display_label>
		<display_no>9</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>5</seq_no>
		<title><![CDATA[Enhanced dual quaternion skinning for production use]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2504459.2504470</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2504470</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4193590</person_id>
				<author_profile_id><![CDATA[82459126057]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Gene]]></first_name>
				<middle_name><![CDATA[S.]]></middle_name>
				<last_name><![CDATA[Lee]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4193591</person_id>
				<author_profile_id><![CDATA[82459088457]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Andy]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4193592</person_id>
				<author_profile_id><![CDATA[82459263457]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Matt]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Schiller]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4193593</person_id>
				<author_profile_id><![CDATA[82459016557]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Scott]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Peters]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4193594</person_id>
				<author_profile_id><![CDATA[81504687154]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Mark]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[McLaughlin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4193595</person_id>
				<author_profile_id><![CDATA[81458652386]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Frank]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hanner]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2013</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	</section>
	<section>
		<section_id>2504471</section_id>
		<sort_key>120</sort_key>
		<section_seq_no>3</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Catching the eye]]></section_title>
		<section_page_from>3</section_page_from>
	<article_rec>
		<article_id>2504472</article_id>
		<sort_key>130</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>10</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Near-eye light field displays]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2504459.2504472</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2504472</url>
		<abstract>
			<par><![CDATA[<p>We propose a light-field-based approach to near-eye display that allows for thin, lightweight head-mounted displays capable of depicting accurate accommodation, convergence, and binocular disparity depth cues. Our near-eye light field displays depict sharp images from out-of-focus display elements by synthesizing light fields corresponding to virtual scenes located within the viewer's natural accommodation range. While sharing similarities with existing integral imaging displays and microlens-based light field cameras, we optimize performance in the context of near-eye viewing. Near-eye light field displays support continuous accommodation of the eye throughout a finite depth of field; as a result, binocular configurations provide a means to address the accommodation-convergence conflict occurring with existing stereoscopic displays. We construct a binocular prototype and a GPU-accelerated stereoscopic light field renderer.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[accommodation-convergence conflict]]></kw>
			<kw><![CDATA[head-mounted displays]]></kw>
			<kw><![CDATA[light field displays]]></kw>
			<kw><![CDATA[microlens arrays]]></kw>
			<kw><![CDATA[virtual reality]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4193596</person_id>
				<author_profile_id><![CDATA[81319495323]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Douglas]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lanman]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[NVIDIA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4193597</person_id>
				<author_profile_id><![CDATA[81100131290]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[David]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Luebke]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[NVIDIA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Ng, R., Levoy, M., Br&#233;dif, M., Duval, G., Horowitz, M., and Hanrahan, P. 2005. Light field photography with a hand-held plenoptic camera. Tech. Rep. CTSR 2005-02, Stanford.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2185577</ref_obj_id>
				<ref_obj_pid>2185520</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Pamplona, V. F., Oliveira, M. M., Aliaga, D. G., and Raskar, R. 2012. Tailored displays to compensate for visual aberrations. <i>ACM Trans. Graph. 31</i>, 4, 81:1--81:12.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Near-Eye Light Field Displays Douglas Lanman David Luebke NVIDIA  Figure 1: Enabling thin, lightweight 
near-eye displays using light .eld displays. (Left) Our binocular near-eye display prototype is shown, 
comprising a pair of OLED panels covered with microlens arrays. This design enables a thin head-mounted 
display, since the black box containing driver electronics could be waist-mounted with longer ribbon 
cables. (Right) Due to the limited range of human accommodation, a severely defocused image is perceived 
when a bare microdisplay is held close to the eye (here simulated as a close-up photograph of an OLED). 
Conventional near-eye displays require bulky magnifying optics to facilitate accommodation. We propose 
near-eye light .eld displays as thin, lightweight alternatives, achieving comfortable viewing by synthesizing 
a light .eld corresponding to a virtual scene located within the accommodation range (here implemented 
by viewing a microdisplay, depicting interlaced perspectives, through a microlens array). Abstract We 
propose a light-.eld-based approach to near-eye display that allows for thin, lightweight head-mounted 
displays capable of de­picting accurate accommodation, convergence, and binocular dis­parity depth cues. 
Our near-eye light .eld displays depict sharp im­ages from out-of-focus display elements by synthesizing 
light .elds corresponding to virtual scenes located within the viewer s natural accommodation range. 
While sharing similarities with existing in­tegral imaging displays and microlens-based light .eld cameras, 
we optimize performance in the context of near-eye viewing. Near-eye light .eld displays support continuous 
accommodation of the eye throughout a .nite depth of .eld; as a result, binocular con.gura­tions provide 
a means to address the accommodation-convergence con.ict occurring with existing stereoscopic displays. 
We con­struct a binocular prototype and a GPU-accelerated stereoscopic light .eld renderer. Keywords: 
light .eld displays, head-mounted displays, microlens arrays, accommodation-convergence con.ict, virtual 
reality 1 Overview To be of practical utility, a near-eye display should provide high­resolution, wide-.eld-of-view 
imagery with compact, comfortable magnifying optics. However, current magni.er designs typically require 
multiple optical elements to minimize aberrations, leading to bulky eyewear with limited .elds of view. 
We consider a simple alternative: placing a light .eld display directly in front of a user s eye (or 
a pair of such displays for binocular viewing). As shown in Figure 1, sharp imagery is depicted by synthesizing 
a light .eld for a virtual display (or a general 3D scene) within the viewer s un­aided accommodation 
range. We demonstrate this design enables thin, lightweight head-mounted displays (HMDs) with wide .elds 
of view and addresses accommodation-convergence con.ict; how­ever, these bene.ts come at a high cost: 
spatial resolution is re­duced with microlens-based designs, although with commensurate gains in depth 
of .eld and in accurate rendering of retinal defocus. Through this work, we demonstrate how to mitigate 
resolution loss. 2 Related Work As characterized by Ng et al. [2005], light .eld cameras share many 
of the properties of our near-eye display: extending depth of .eld and .eld of view, at the cost of decreased 
spatial resolution. Sev­eral recent works have proposed placing integral imaging displays close to the 
eye (described in detail in the supplementary support­ing document). Pamplona et al. [2012] apply related 
integral imag­ ing displays to correct optical aberrations. In comparison to this closely-related work, 
we emphasize that our efforts differ in scope; we target general-purpose 3D display, rather than estimation 
and correction of refractive errors. Furthermore, we are the .rst to op­timize the optical design and 
underlying rendering algorithms to enable thin, lightweight HMDs, while minimizing resolution loss. 
3 Implementation As shown in Figure 1, a binocular prototype was constructed us­ ing components from 
a Sony HMZ-T1 personal media viewer. The case and eyepieces were removed, exposing two OLED microdis­plays. 
Fresnel Technologies #630 microlenses were af.xed to each panel. We estimate each modi.ed eyepiece achieves 
a spatial reso­lution of 146×78 pixels and .eld of view of 29×16 degrees. Each modi.ed eyepiece measures 
42×31×10 mm with a 0.7 gram mi­crolens array (an unmodi.ed HMZ-T1 eyepiece is 43×31×37 mm with a 57.7 
gram lens). Real-time stereoscopic light .eld rendering was achieved by modifying the NVIDIA OptiX GPU-accelerated 
ray tracing engine to support quad buffering in OpenGL.  References NG, R., LEVOY, M., BR ´ EDI F, M., 
DUVA L , G., HOROW I TZ , M., AND HA N RA H A N , P. 2005. Light .eld photography with a hand­held plenoptic 
camera. Tech. Rep. CTSR 2005-02, Stanford. PA MPLONA , V. F., OLI V E I R A , M. M., AL I AG A , D. G., 
A N D RA S K A R , R. 2012. Tailored displays to compensate for visual aberrations. ACM Trans. Graph. 
31, 4, 81:1 81:12. Permission to make digital or hard copies of part or all of this work for personal 
or classroom use isgranted without fee provided that copies are not made or distributed for commercial 
advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party 
components of thiswork must be honored. For all other uses, contact the Owner/Author.SIGGRAPH 2013, July 
21 25, 2013, Anaheim, California. 2013 Copyright held by the Owner/Author.ACM 978-1-4503-2261-4/13/07 
 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2013</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>2504473</article_id>
		<sort_key>140</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>11</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Survey and evaluation of tone mapping operators for HDR video]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2504459.2504473</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2504473</url>
		<abstract>
			<par><![CDATA[<p>This work presents a survey and a user evaluation of <i>tone mapping operators</i> (TMOs) for <i>high dynamic range</i> (HDR) video, i.e. TMOs that explicitly include a temporal model for processing of variations in the input HDR images in the time domain. The main motivations behind this work is that: robust tone mapping is one of the key aspects of HDR imaging [Reinhard et al. 2006]; recent developments in sensor and computing technologies have now made it possible to capture HDR-video, e.g. [Unger and Gustavson 2007; Tocci et al. 2011]; and, as shown by our survey, tone mapping for HDR video poses a set of completely new challenges compared to tone mapping for still HDR images. Furthermore, video tone mapping, though less studied, is highly important for a multitude of applications including gaming, cameras in mobile devices, adaptive display devices and movie post-processing.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4193598</person_id>
				<author_profile_id><![CDATA[82459287257]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Gabriel]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Eilertsen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Link&#246;ping University, Sweden]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[gabriel.eilertsen@liu.se]]></email_address>
			</au>
			<au>
				<person_id>P4193599</person_id>
				<author_profile_id><![CDATA[81100120690]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Jonas]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Unger]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Link&#246;ping University, Sweden]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4193600</person_id>
				<author_profile_id><![CDATA[81488669556]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Robert]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wanat]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Bangor University, United Kingdom]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4193601</person_id>
				<author_profile_id><![CDATA[81100175469]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Rafa&#322;]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Mantiuk]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Bangor University, United Kingdom]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1208706</ref_obj_id>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Reinhard, E., Ward, G., Pattanaik, S., and Debevec, P. 2006. <i>High Dynamic Range Imaging -- Acquisition, Display and Image-Based Lighting</i>. Morgan Kaufmann, San Francisco, CA.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1964936</ref_obj_id>
				<ref_obj_pid>2010324</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Tocci, M. D., Kiser, C., Tocci, N., and Sen, P. 2011. A Versatile HDR Video Production System. <i>ACM Transactions on Graphics (TOG) (Proceedings of SIGGRAPH 2011) 30</i>, 4.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Unger, J., and Gustavson, S. 2007. High-dynamic-range video for photometric measurement of illumination. <i>Proc. of SPIE 6501</i>, 65010E.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Survey and Evaluation of Tone Mapping Operators for HDR Video Gabriel Eilertsen1* Jonas Unger1 Robert 
Wanat2 Rafal Mantiuk2 1Link¨2Bangor University, United Kingdom oping University, Sweden  Figure 1: Left: 
The HDR-video sequences used in our evaluation exhibit strong local contrast changes, rapid variations 
in local and global intensity as well as spatio-temporal variations. Right: the plot illustrates how 
different a set of representative TMOs from our evaluation, see legend, responds to the same HDR input. 
The plot shows the log luminance of the temporal variation of a pixel under the green square as the light 
bulbs are turned on and off. Note that the plots are, for presentation purposes, shifted along the y-axis 
to start at the same value. 1 Introduction This work presents a survey and a user evaluation of tone 
mapping operators (TMOs) for high dynamic range (HDR) video, i.e. TMOs that explicitly include a temporal 
model for processing of variations in the input HDR images in the time domain. The main motivations behind 
this work is that: robust tone mapping is one of the key as­pects of HDR imaging [Reinhard et al. 2006]; 
recent developments in sensor and computing technologies have now made it possible to capture HDR-video, 
e.g. [Unger and Gustavson 2007; Tocci et al. 2011]; and, as shown by our survey, tone mapping for HDR 
video poses a set of completely new challenges compared to tone mapping for still HDR images. Furthermore, 
video tone mapping, though less studied, is highly important for a multitude of applications includ­ing 
gaming, cameras in mobile devices, adaptive display devices and movie post-processing. Our survey is 
meant to summarize the state-of-the-art in video tone­mapping and, as exempli.ed in Figure 1 (right), 
analyze differences in their response to temporal variations. In contrast to other stud­ies, we evaluate 
TMOs performance according to their actual in­tent, such as producing the image that best resembles the 
real world scene, that subjectively looks best to the viewer, or ful.lls a certain artistic requirement. 
The unique strength of this work is that we use real high quality HDR video sequences, see Figure 1 (left), 
as opposed to synthetic images or footage generated from still HDR images. 2 Evaluation of TMOs for 
video From our survey of TMOs, we have selected eight representative TMOs that perform temporal processing, 
and thus are aimed at HDR-video input. For the evaluation, we divide the TMOs into two classes based 
on their intent: Visual System Simulators (VSS), that include the temporal domain by simulating the limitations 
and properties of the human visual system, and Best Subjective Quality (BSQ) operators, that are designed 
to produce the most preferred, or best looking, images or video in terms of subjective preference or 
artistic goals. The evaluation, including 36 participants, was car­ried out using pairwise comparisons 
between a set of HDR-video sequences processed using the selected TMOs. The input video se­quences exhibited 
up to 24 f -stops of dynamic range. For the VSS TMOs, the participants were, in each trial, asked to 
judge which of the two video clips that looked most like what they would expect the real scene to look 
like. For the BSQ operators the participants were asked to select the video clip that subjectively looked 
best. For a fair comparison, the parameters for each TMO used in the evalua­tion were determined by an 
expert user in a pre-study. *e-mail:gabriel.eilertsen@liu.se 3 Results The results of the study, shown 
in Figure 2, revealed that many complex and in particular local video operators introduce artifacts: 
.ickering, ghosting, over-saturated colors. Therefore, unexpectedly, less sophisticated global operators 
(ferwerda 96 and pattanik 00) received higher scores in the comparison of the VSS operators. This shows 
importance of testing video operators with challenging sequences, which contain both high spatial and 
temporal contrast changes. The results for BSQ operators revealed another important issue, most often 
disregarded by operators: the subjective quality is much penalized if tone-mapping reveals noise and 
other artifacts of camera HDR capture. Therefore, the operators need to take into ac­count the quality 
of input HDR sequence. Overall, the study showed that the problem of video tone mapping is not by far 
fully solved and identi.ed the areas that need further improvement. Figure 2: Results of the evaluations 
for VSS (left) and BSQ (right) TMOs. The results are scaled in JND units (the higher, the better) under 
Thurstone Case V assumptions, where 1 JND corresponds to 75% discrimination threshold. Note that absolute 
JND values are arbitrary and only relative differences are meaningful. The error bars denote 95% con.dence 
intervals computed by bootstrapping. Acknowledgments This project was funded by the Swedish Foundation 
for Strategic Research through grant IIS11-0081, Link ¨oping University CENIIT, and COST Action IC1005 
on HDR video. References REI N H AR D , E., WA R D, G., PATTA NA I K , S., A N D DEB E V E C , P. 2006. 
High Dynamic Range Imaging Acquisition, Display and Image-Based Lighting. Morgan Kaufmann, San Francisco, 
CA. TO C C I, M. D., KI S E R , C., TO CC I , N., A ND SE N, P. 2011. A Versatile HDR Video Production 
System. ACM Transactions on Graphics (TOG) (Proceedings of SIGGRAPH 2011) 30, 4. UN G E R , J., A N D 
GU S TAVSO N , S. 2007. High-dynamic-range video for photometric measurement of illumination. Proc. of 
SPIE 6501, 65010E. Permission to make digital or hard copies of part or all of this work for personal 
or classroom use isgranted without fee provided that copies are not made or distributed for commercial 
advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party 
components of thiswork must be honored. For all other uses, contact the Owner/Author.SIGGRAPH 2013, July 
21 25, 2013, Anaheim, California. 2013 Copyright held by the Owner/Author.ACM 978-1-4503-2261-4/13/07 
 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<article_sponsors>
			<funding_agency>Linkoping University CENIIT</funding_agency>
			<grant_numbers>
				<grant_number></grant_number>
			</grant_numbers>
		</article_sponsors>
		<article_sponsors>
			<funding_agency>Swedish Foundation for Strategic Research</funding_agency>
			<grant_numbers>
				<grant_number>IIS11-0081</grant_number>
			</grant_numbers>
		</article_sponsors>
		<article_sponsors>
			<funding_agency>COST Action IC1005 on HDR video</funding_agency>
			<grant_numbers>
				<grant_number></grant_number>
			</grant_numbers>
		</article_sponsors>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2013</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>2504474</article_id>
		<sort_key>150</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>12</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Coded exposure HDR light-field video recording]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2504459.2504474</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2504474</url>
		<abstract>
			<par><![CDATA[<p>Capturing exposure sequences for computing high-dynamic range (HDR) images causes motion blur in case of camera movements. This also applies for light-field cameras, such as camera arrays. Images composed from multiple blurred HDR light-field perspectives are also blurred.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4193602</person_id>
				<author_profile_id><![CDATA[81486655936]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[David]]></first_name>
				<middle_name><![CDATA[C.]]></middle_name>
				<last_name><![CDATA[Schedl]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Johannes Kepler University Linz]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[David.Schedl@jku.at]]></email_address>
			</au>
			<au>
				<person_id>P4193603</person_id>
				<author_profile_id><![CDATA[81503672096]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Clemens]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Birklbauer]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Johannes Kepler University Linz]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[Clemens.Birklbauer@jku.at]]></email_address>
			</au>
			<au>
				<person_id>P4193604</person_id>
				<author_profile_id><![CDATA[81100622976]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Oliver]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bimber]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Johannes Kepler University Linz]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[Oliver.Bimber@jku.at]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Coded Exposure HDR Light-Field Video Recording David C. Schedl, Clemens Birklbauer, and Oliver Bimber* 
Johannes Kepler University Linz  Figure 1: HDR light-.eld recording during camera movement: Instead 
of recording complete exposure sequences per perspective of a camera array (e), we encode exposure times 
in each repeating camera quad-tuple (a,f). By computing a composite depth map from all exposures, segmenting 
it into depth layers, and tracking the optical .ow of scene features in each depth layer (c), we derive 
depth-individual PSFs that are used for local motion deblurring (b,d). Finally, we interpolate deblurred 
exposure images that have not been recorded from particular perspectives. Combining them results in HDR 
images for each camera perspective. These images can then be used for tone-mapped light-.eld rendering 
(g-l). In contrast to recording regular exposure sequences (g,i,k), our approach (h,j,l) reduces motion 
blur. Figures g-l compare tone-mapped wide-aperture (i.e., shallow depth-of-.eld) light-.eld renderings 
for different focus settings (indicated with the green arrows). Note, that remaining artifacts in out-of-focus 
regions are due to directional under-sampling (i.e., too few cameras in the array). Our Approach Capturing 
exposure sequences for computing high-dynamic range (HDR) images causes motion blur in case of camera 
movements. This also applies for light-.eld cameras, such as camera arrays. Im­ages composed from multiple 
blurred HDR light-.eld perspectives are also blurred. To reduce motion blur, we encode four exposure 
times in each re­peating camera quad-tuple of a camera array (assuming relatively small camera-baselines): 
Instead of recording all exposures sequen­tially for all perspective cameras (cf. .g. 1e), we apply the 
spatio­ temporal exposure pattern shown in .gures 1a,f. This reduces the overall recording interval, 
but also prevents from recording all ex­posures at all perspectives, and therefore constrains the smallest 
synthetic aperture to be at least 2 × 2. Within each camera quad­tuple, the shortest and longest exposures 
are captured only from one perspective each (1 and 4, respectively), while the two medium ex­posures 
are recorded interleaved from two perspectives (2 and 3). For each of the four different exposures (cf. 
.g. 1b) that are recorded at varying perspectives, we compute a depth map. Since these depth maps vary 
locally in quality (due to motion blur for the higher exposures, and due to low SNR for the lower exposures), 
we compile them to a single composite depth map based on a depth con.dence criteria (cf. .g. 1c). This 
composite depth map is then segmented into different depth layers. For each depth layer we track features 
in the low exposure images. The optical .ow of these fea­tures allows us to determine the local point-spread 
function (PSF) that causes motion blur in higher exposures. These PSFs are used for motion deblurring 
(i.e., deconvolution) of each recorded expo­sure image at all camera perspectives (cf. .g. 1d). By shifting 
the PSFs before deconvolution (as illustrated by the ar­rows in .gure 1f), we can receive three subframes 
for each record­ ing interval at all perspectives of each camera quad-tuple. Thus, for subsequent recording 
intervals of an HDR light-.eld video (coded exposures and deconvolution shifts as illustrated in .gure 
1f), we obtain the following exposure images at times 0, 4, 8: 1/N for per­spective 1, 1/(2 × N) and 
1/(4 × N) for perspectives 2 and 3, and 1/(8 × N) for perspective 4. Compared to classical exposure se­quencing 
(cf. .g. 1e), this leads to a ×3.75 higher frame rate. With the recorded and deblurred exposure images 
we .nally compute an enhanced composite depth map to interpolate deblurred expo­sure images for all camera 
perspectives that have not been directly recorded. Figures 1g-l illustrate tone-mapped, wide-synthetic-aperture 
(i.e., shallow depth-of-.eld) images rendered from a light .eld that was recorded during camera motion. 
Figures 1g,i,k show the results at different focus settings (green arrows indicate selected in-focus 
re­gion) with regular exposure sequences for each camera perspective (cf. .g. 1e). Figures 1h,j,l show 
the same images recorded with our coded exposures (cf. .g. 1f) and computed with the technique outlined 
above. *.rstname.lastname@jku.at Permission to make digital or hard copies of part or all of this work 
for personal or classroom use isgranted without fee provided that copies are not made or distributed 
for commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights 
for third-party components of thiswork must be honored. For all other uses, contact the Owner/Author.SIGGRAPH 
2013, July 21 25, 2013, Anaheim, California. 2013 Copyright held by the Owner/Author.ACM 978-1-4503-2261-4/13/07 
 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2013</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>2504475</article_id>
		<sort_key>160</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>13</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[On-set depth capturing for VFX productions using time of flight]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2504459.2504475</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2504475</url>
		<abstract>
			<par><![CDATA[<p>Pixel accurate depth estimation of a dynamic scene, in addition to conventional recording on a film set, opens up new opportunities in modern filmmaking. Depth maps can be used for Keying, Set Reconstruction as well as for subsequent Depth Grading in Post Production [1].</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4193605</person_id>
				<author_profile_id><![CDATA[82459208657]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Simon]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Spielmann]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Filmakademie Baden-Wuerttemberg]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[simon.spielmann@filmakademie.de]]></email_address>
			</au>
			<au>
				<person_id>P4193606</person_id>
				<author_profile_id><![CDATA[81100178077]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Volker]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Helzle]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Filmakademie Baden-Wuerttemberg]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[volker.helzle@filmakademie.de]]></email_address>
			</au>
			<au>
				<person_id>P4193607</person_id>
				<author_profile_id><![CDATA[82458714857]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Rahul]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nair]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[HCI Heidelberg University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[rahul.nair@iwr.uni-heidelberg.de]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Patel, m. 2009. The New Art of Virtual Moviemaking. Audodesk Whitepaper.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>598475</ref_obj_id>
				<ref_obj_pid>598429</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Scharstein A., Szeliski D. 2002. A taxonomy and evaluation of dense two-frame stereo correspondence algorithms. IJCV 47]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2404900</ref_obj_id>
				<ref_obj_pid>2404898</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Nair, R. 2012. High Accuracy TOF and Stereo Sensor Fusion at Interactive Rates. <i>LNCS Vol. 7584</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2356595</ref_obj_id>
				<ref_obj_pid>2355573</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Park, J., Kim, H., Tai, Y., Brown, S., Kweon, I. 2011. High Quality Depth Map Upsampling for 3D-TOF Cameras. <i>ICCV</i>]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Mendiburu, B. 2009. 3D Movie Making: Stereoscopic Digital Cinema from Script to Screen. <i>Focal Press</i> N.26]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[ECMA International. 2008. High Rate Ultra Wideband PHY and MAC Standard. <i>Standard ECMA-368</i>, ISO/IEC 26907.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 On-Set Depth Capturing for VFX Productions using Time of Flight Simon Spielmann Volker Helzle Rahul 
Nair Filmakademie Baden-Wuerttemberg Filmakademie Baden-Wuerttemberg HCI Heidelberg University simon.spielmann@filmakademie.de 
volker.helzle@filmakademie.de rahul.nair@iwr.uni-heidelberg.de 1. Introduction Pixel accurate depth 
estimation of a dynamic scene, in addition to conventional recording on a film set, opens up new opportunities 
in modern filmmaking. Depth maps can be used for Keying, Set Reconstruction as well as for subsequent 
Depth Grading in Post Production [1]. Traditional methods using stereo image pairs to determine depth 
often provide inadequate results [2]. Structured backgrounds, object edges and fine details are the biggest 
issues for procedures that operate on stereo image pairs [2-4]. These approaches need at least a minimum 
of disparity to keep the calculation error at a reasonable level. The lack of information in homogeneous 
or regular structured images will lead to errors within the calculated depth map. These problems particularly 
arise in film and media productions. Due to the increasing usage of blue and green screens, the amount 
of usable information in the raw picture material is more and more reduced. Moreover, on account of the 
small stereo basis used in movie productions [5] and the artistic use of motion blur, the needed disparity 
is often lower than the available resolution. Within this talk, we present a use case of a time of flight 
(TOF) assisted method [3], which decisively improves the quality of the estimated depth maps. This method 
has been developed in the joint research project on exploration of innovative software solutions for 
stereoscopic content creation1 . We also show the differences between controlled-and real live conditions 
on a film set, and suggest workflows to combine hardware and algorithms to optimize the quality of depth 
estimation. 2. On set depth capturing The referenced approach has been tested under real conditions 
in a film production2 . In addition to the stereo cameras, a TOF camera system3 was mounted to a stereo 
mirror rig4 . In comparison with the two RGB cameras5 and the stereo rig, the utilized TOF system is 
small and lightweight. Therefore, the additional effort to integrate the camera into the rig is manageable. 
Finally, the camera was mounted on a mechanical linkage under the compendium of the mirror box. The recording 
of the TOF data was achieved by a wired external PC. By using the already existing cable links from the 
RGB camera to the video village, the amount of work and time is minimal. Another solution could be the 
use of radio transmitters [6], which would allow wireless recording. 3. Stereo Conversion Classical 
stereoscopic production have some drawbacks, often related to the need of a second camera and the bulky, 
additional 1 http://research.animationsinstitut.de/161.0.html 2 How To get a Girl in 60 seconds, Johannes 
Peter, Filmakademie 2013 3 pmd[vision]® CamCube 3.0 4 Screen Plane® Kite-Rig 5 Arri® ALEXA equipment. 
Longer times are required for the technical preparation, since the two cameras and their lenses must 
be precisely calibrated to each other. Also, an additional processing of the depth perception in Post 
Production is limited as well as time and cost-intensive. The only practical applicable possibilities 
are manual-or algorithmic approaches [2-4]. In addition to the quality and resolution enhancement, the 
novelty of our work is the estimation of a high resolution depth map out of a low-res TOF depth map and 
a single high-res RGB image in a real live production environment. Figure 1. Take Me Out to the Ballgame. 
 3. Results Figure 1 shows a comparison between TOF, a stereo matching­ [2] and the introduced technique 
[3].With our approach typical artifacts like reconstruction errors in fine details, edge bleeding and 
noise are nearly eliminated. The resulting depth maps have the same resolution as the production plates 
and are less noisy than pure TOF images. During production some system related restrictions occurred. 
Because of the functional principle of a TOF system, through phase correlation, the maximal measurable 
distance is limited by the modulation frequency. In our camera this is around 7-8 meters. Furthermore 
synchronization between the RGB and the TOF camera is necessary. This can be achieved by using dedicated 
hardware like a Genlock, or in Post, by warping and interpolating the footage. 4. Acknowledgements This 
project is joint research with the Heidelberg Collaboratory for Image Processing (HCI). We thank the 
ministry of economy Baden-Württemberg and our partner companies Unexpected, Pixomondo, ScreenPlane, Bewegte 
Bilder and Tridelity for its funding under grant number 2-4225.16/380. References [1]PATEL, M. 2009. 
The New Art of Virtual Moviemaking. Audodesk Whitepaper. [2]SCHARSTEIN A., SZELISKI D. 2002. A taxonomy 
and evaluation of dense two-frame stereo correspondence algorithms. IJCV 47 [3]NAIR, R. 2012. High Accuracy 
TOF and Stereo Sensor Fusion at Interactive Rates. LNCS Vol. 7584. [4]PARK, J., KIM, H., TAI, Y., BROWN, 
S., KWEON, I. 2011. High Quality Depth Map Upsampling for 3D-TOF Cameras. ICCV [5]Mendiburu, B. 2009. 
3D Movie Making: Stereoscopic Digital Cinema from Script to Screen. Focal Press N.26 [6]ECMA INTERNATIONAL. 
2008. High Rate Ultra Wideband PHY and MAC Standard. Standard ECMA-368, ISO/IEC 26907. Permission to 
make digital or hard copies of part or all of this work for personal or classroom use isgranted without 
fee provided that copies are not made or distributed for commercial advantage and that copies bear this 
notice and the full citation on the first page. Copyrights for third-party components of thiswork must 
be honored. For all other uses, contact the Owner/Author.SIGGRAPH 2013, July 21 25, 2013, Anaheim, California. 
2013 Copyright held by the Owner/Author.ACM 978-1-4503-2261-4/13/07  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2013</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	</section>
	<section>
		<section_id>2504476</section_id>
		<sort_key>170</sort_key>
		<section_seq_no>4</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Effects omelette]]></section_title>
		<section_page_from>4</section_page_from>
	<article_rec>
		<article_id>2504477</article_id>
		<sort_key>180</sort_key>
		<display_label>Article No.</display_label>
		<display_no>14</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Man of steel]]></title>
		<subtitle><![CDATA[procedural city building and destruction]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2504459.2504477</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2504477</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4193608</person_id>
				<author_profile_id><![CDATA[82459058257]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Eug&#233;nie]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[von Tunzelmann]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2013</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>2504478</article_id>
		<sort_key>190</sort_key>
		<display_label>Article No.</display_label>
		<display_no>15</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Simulating fluids using a coupled voxel-particle data model]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2504459.2504478</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2504478</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4193609</person_id>
				<author_profile_id><![CDATA[81488650461]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Dan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bailey]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4193610</person_id>
				<author_profile_id><![CDATA[81488672204]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Ian]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Masters]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4193611</person_id>
				<author_profile_id><![CDATA[81488644719]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Matt]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Warner]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4193612</person_id>
				<author_profile_id><![CDATA[82458622157]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Harry]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Biddle]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2013</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>2504479</article_id>
		<sort_key>200</sort_key>
		<display_label>Article No.</display_label>
		<display_no>16</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Jack's frost]]></title>
		<subtitle><![CDATA[controllable magic frost simulations for Rise of the Guardians]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2504459.2504479</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2504479</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4193613</person_id>
				<author_profile_id><![CDATA[82459074757]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[David]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lipton]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4193614</person_id>
				<author_profile_id><![CDATA[81424592239]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Ken]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Museth]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4193615</person_id>
				<author_profile_id><![CDATA[81319501982]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Ben]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sutherland]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2013</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>2504480</article_id>
		<sort_key>210</sort_key>
		<display_label>Article No.</display_label>
		<display_no>17</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[Bubbles and foam in Partysaurus Rex]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2504459.2504480</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2504480</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4193616</person_id>
				<author_profile_id><![CDATA[81100329027]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Adam]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Harder]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4193617</person_id>
				<author_profile_id><![CDATA[82459237457]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Chris]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Mangnall]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2013</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	</section>
	<section>
		<section_id>2504481</section_id>
		<sort_key>220</sort_key>
		<section_seq_no>5</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[A cloud of shadows]]></section_title>
		<section_page_from>5</section_page_from>
	<article_rec>
		<article_id>2504482</article_id>
		<sort_key>230</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>18</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Imperfect voxelized shadow volumes]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2504459.2504482</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2504482</url>
		<abstract>
			<par><![CDATA[<p>Voxelized shadow volumes (VSVs) [Wyman 2011] are a discretized view-dependent shadow volume representation, but are limited to point or directional lights. We extend them, allowing dynamic volumetric visibility from area lights using <i>imperfect shadow volumes</i>. As with imperfect shadow maps [Ritschel et al. 2008], area lights can use coarser spacial sampling without significantly degrading quality. Combining coarser resolution with a parallel shadow volume construction enables interactive rendering of dynamic volumetric shadows from area lights in homogeneous single-scattering media, at around 4x the cost of hard volumetric shadows.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[area lights]]></kw>
			<kw><![CDATA[participating media]]></kw>
			<kw><![CDATA[shadows]]></kw>
			<kw><![CDATA[voxelization]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4193618</person_id>
				<author_profile_id><![CDATA[81100265704]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Chris]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wyman]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[NVIDIA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[chris.wyman@acm.org]]></email_address>
			</au>
			<au>
				<person_id>P4193619</person_id>
				<author_profile_id><![CDATA[81500645454]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Zeng]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Dai]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Iowa]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[zeng-dai@uiowa.edu]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Heckbert, P., and Herf, M. 1997. Simulating soft shadows with graphics hardware. Tech. Rep. CMU-CS-97-104, CMU.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1409082</ref_obj_id>
				<ref_obj_pid>1409060</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Ritschel, T., Grosch, T., Kim, M., Seidel, H.-P., Dachsbacher, C., and Kautz, J. 2008. Imperfect shadow maps for efficient computation of indirect illumination. <i>ACM Transactions on Graphics 27</i>, 5, 1--8.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2018329</ref_obj_id>
				<ref_obj_pid>2018323</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Wyman, C. 2011. Voxelized shadow volumes. In <i>High Performance Graphics</i>, 33--40.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Imperfect Voxelized Shadow Volumes Chris Wyman* Zeng Dai NVIDIA University of Iowa Figure 1: (Left) 
A video-illuminated polytope with no media, unshadowed homogeneous media, shadowed media. (Center) A 
dragon in the Crytek Sponza with a moving light. (Right) A hairball inside the Sponza, with and without 
volumetric shadows. All run interactively. Voxelized shadow volumes (VSVs) [Wyman 2011] are a discretized 
view-dependent shadow volume representation, but are limited to point or directional lights. We extend 
them, allowing dynamic vol­umetric visibility from area lights using imperfect shadow volumes. As with 
imperfect shadow maps [Ritschel et al. 2008], area lights can use coarser spacial sampling without signi.cantly 
degrading quality. Combining coarser resolution with a parallel shadow vol­ume construction enables interactive 
rendering of dynamic volu­metric shadows from area lights in homogeneous single-scattering media, at 
around 4x the cost of hard volumetric shadows. Keywords: shadows, area lights, participating media, voxelization 
1 Introduction Voxelized shadow volumes rely on a spacial discretization based on an angular, epipolar 
sampling. This sampling s key advantage is a structure where grid axes lie parallel to view and light 
rays, allowing cache-aligned visibility lookups along camera rays and a parallel scan along light rays 
to quickly compute dense shadow samples. A VSV retains volumetric visibility inherent in geomet­ric shadow 
volumes, without many of the costs (exorbitant .ll rate consumption and need to identify silhouette edges). 
Unfortunately epipolar sampling, by design, depends heavily on the epipole connecting the camera and 
point light, complicating exten­sions to area lighting. We take a common approach, sampling an area light 
as numerous point lights (e.g., Heckbert and Herf [1997]). But this scales linearly with light samples; 
sampling dense enough for good quality can require over a second per frame. But computing a naive VSV 
for each light sample is overkill. We explored the VSV resolution needed when sampling VPLs from an area 
source. Given averaging from accumulating multiple lights, we found sampling visibility 64x coarser than 
needed for a single point source feasible (e.g., 1283 instead of 5123 or larger). 2 Imperfect Shadow 
Volumes In the context of shadow maps, Ritschel et al. [2008] observed not only coarse visibility, but 
partially incorrect visibility could gener­ate realistic shadows from area lights. Given human perception 
is *e-mail:chris.wyman@acm.org e-mail:zeng-dai@uiowa.edu tuned more to detecting surfaces rather than 
thin media, we hypoth­esized artifacts in volumetric visibility would be still less visible. To create 
our imperfect VSVs, we used imperfect shadow maps to populate epipolar space with occluders (separately 
for each sam­ple on the area light) and performed a scan to extrude occlusions away from the light (to 
form shadow volumes). As these imperfect volumes are low resolution, we can .t hundreds of them in a 
sin­gle render target. This allows a single parallel scan over one buffer, greatly reducing GPU overhead 
compared to hundreds of scans. Interpolation. To reduce aliasing artifacts arising from course sam­ples, 
we introduce an interpolation scheme in epipolar space. Ex­panding the Riemann sum used to integrate 
scattering, terms cancel, allowing us to avoid costly trilinear lookups at each step. Instead we need 
to interpolate at shadow boundaries, and due to VSV view dependency this can occur in screen space. We 
found a single bi­linear interpolation suf.ces (for each VPL), and due to our epipolar representation 
this requires exactly two texture lookups. Results. We implemented imperfect VSVs in OpenGL, and com­pare 
with a naive, brute force application of VSVs on a per-VPL basis. The table below compares performance 
(from Figure 1, left) when naively applying VSVs for 256 VPLs with both the naive method and our new 
imperfect VSV algorithm at a coarser, more appropriate sampling density for area lights. Voxelize Scan 
to VSV Final Gather Naive, at 5123 [Wyman 2011] 250 ms 1400 ms 175 ms Naive, at 1283 128 ms 124 ms 99 
ms Imperfect VSVs, at 1283 7 ms 5 ms 15 ms  References HEC K BE RT, P., A ND HE RF, M. 1997. Simulating 
soft shadows with graphics hardware. Tech. Rep. CMU-CS-97-104, CMU. RI T S CHEL , T., GROSC H , T., KI 
M , M., SE ID EL , H.-P., DACHS-BAC H E R , C., A N D KAU T Z, J. 2008. Imperfect shadow maps for ef.cient 
computation of indirect illumination. ACM Trans­actions on Graphics 27, 5, 1 8. WY MAN , C. 2011. Voxelized 
shadow volumes. In High Perfor­mance Graphics, 33 40. Permission to make digital or hard copies of part 
or all of this work for personal or classroom use isgranted without fee provided that copies are not 
made or distributed for commercial advantage and that copies bear this notice and the full citation on 
the first page. Copyrights for third-party components of thiswork must be honored. For all other uses, 
contact the Owner/Author.SIGGRAPH 2013, July 21 25, 2013, Anaheim, California. 2013 Copyright held by 
the Owner/Author.ACM 978-1-4503-2261-4/13/07  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2013</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>2504483</article_id>
		<sort_key>240</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>19</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Sub-pixel shadow mapping]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2504459.2504483</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2504483</url>
		<abstract>
			<par><![CDATA[<p>The limited resolution of shadow maps may result in erroneous shadowing, yielding artificially jagged edges (Figure 1) and temporally crawling shadows even using perspective optimization techniques. Dai <i>et al</i>. [2008] propose an explicit storage of geometry within shadow map texels to avoid aliasing. Each texel stores the coordinates of the closest triangle only, potentially leading to false negatives in the intersection computation while incurring large memory consumption. These artifacts are reduced by intersecting the triangles stored in numerous neighboring texels, resulting in significant performance hit while still missing some intersections.</p> <p>We introduce <i>Sub-Pixel Shadow Maps</i> (SPSM) for real-time shadow mapping with sub-pixel precision. Our technique is based on the storage of a fixed-size partial representation of the scene geometry using conservative rasterization, combined with an original reconstruction of shadow edges.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4193620</person_id>
				<author_profile_id><![CDATA[81487641482]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Pascal]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lecocq]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Technicolor]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[pascal.lecocq@technicolor.com]]></email_address>
			</au>
			<au>
				<person_id>P4193621</person_id>
				<author_profile_id><![CDATA[81100339836]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Pascal]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gautron]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Technicolor]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[pascal.gautron@technicolor.com]]></email_address>
			</au>
			<au>
				<person_id>P4193622</person_id>
				<author_profile_id><![CDATA[81100029219]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Jean-Eudes]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Marvie]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Technicolor]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[jean-eudes.marvie@technicolor.com]]></email_address>
			</au>
			<au>
				<person_id>P4193623</person_id>
				<author_profile_id><![CDATA[81481650476]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Gael]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sourimant]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Technicolor]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[gael.sourimant@technicolor.com]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1357016</ref_obj_id>
				<ref_obj_pid>1342250</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Dai, Q., Yang, B., and Feng, J. 2008. Reconstructable geometry shadow maps. In <i>Proceedings of I3D</i>, 4:1--4:1.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Hasselgren, J., Akenine-M&#246;ller, T., and Ohlsson, L. 2005. Conservative rasterization. In <i>GPU Gems 2</i>, M. Pharr, Ed. Addison-Wesley, 677--690.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Sub-Pixel Shadow Mapping Pascal Lecocq Pascal Gautron Jean-Eudes Marvie Gael Sourimant Technicolor * 
 Figure 1: Our technique provides sub-pixel accuracy to shadow maps, avoiding aliasing artifacts while 
preserving the rendering speed (left). We reconstruct shadow edges by combining an estimate of the shadow 
tangent (right) with partial geometry information. The limited resolution of shadow maps may result in 
erroneous shadowing, yielding arti.cially jagged edges (Figure 1) and tempo­rally crawling shadows even 
using perspective optimization tech­niques. Dai et al. [2008] propose an explicit storage of geome­try 
within shadow map texels to avoid aliasing. Each texel stores the coordinates of the closest triangle 
only, potentially leading to false negatives in the intersection computation while incurring large memory 
consumption. These artifacts are reduced by intersecting the triangles stored in numerous neighboring 
texels, resulting in sig­ni.cant performance hit while still missing some intersections. We introduce 
Sub-Pixel Shadow Maps (SPSM) for real-time shadow mapping with sub-pixel precision. Our technique is 
based on the storage of a .xed-size partial representation of the scene ge­ometry using conservative 
rasterization, combined with an original reconstruction of shadow edges. The Sub-Pixel Shadow Map Storing 
triangle information within the shadow map introduces two main challenges. First, most hardware rasterizers 
generate frag­ments only if the center of the fragment is covered by the consid­ered triangle. A geometry-based 
shadow map [2008] may thus lack information in the texels corresponding to shadow edges, where precision 
is crucial. The second challenge is the presence of sev­eral triangles rasterized at a same texel position. 
Ideally, each texel should store an arbitrarily large list of overlapping triangles. We .rst enforce 
the generation of fragments for the entire coverage of the triangles using Conservative Rasterization 
[Hasselgren et al. 2005]. We then generate the SPSM by rendering the scene from the light viewpoint, 
storing vertex information for the closest triangle only. We pack the coordinates in SPSM space within 
128-bit using a novel compression scheme for compact storage of depth deriva­tives (Figure 2). At the 
rendering stage we project each visible point p in light space and determine whether p lies on the triangle 
stored in the corre­sponding SPSM texel. If so, conservative rasterization guarantees that p is not occluded. 
Fully lit zones are then rendered using a single SPSM lookup. Otherwise, we intersect the light ray from 
p with the triangle. While .nding an occlusion ensures shadow­ing, the lack of intersection may be due 
to missing geometry. We then intersect the light ray with the triangles stored in the 3 × 3 neighborhood 
of the texel. If needed, we compensate for occlusion incertainty by analytically reconstructing the shadow 
edge. Even though our representation is partial, an accurate shadow edge can be obtained by linking edge 
vertices. Considering the result of depth comparisons for the point p in the 3 × 3 neighborhood, we .rst 
deduce an approximate light-space tangent to the shadow *{pascal.lecocq, pascal.gautron, jean-eudes.marvie, 
gael.sourimant}@technicolor.com Figure 2: If point p is not covered by the triangles stored within a 
close neighborhood (left), their vertices are used to deduce an accurate shadow edge (right). edge (Figure 
1). We then project the vertices whose bisector faces towards the approximate tangent onto that tangent, 
and determine their projection distance. Vertices occluded by projected edges are discarded to avoid 
reconstructing an erroneous edge. Comparing the projection distance dpof p on the tangent with the projection 
distances of neighboring vertices determines whether p is lit or shadowed (Figure 2). Results Many techniques 
reduce aliasing and bias issues by projection opti­mization, each introducing typical artifacts. To avoid 
confusion we compare SPSM with classical uniform shadow maps, although our approach can also bene.t from 
these optimizations. We implemented SPSM on a nVidia GeForce GTX580. Compar­ing 5122 uniform and sub-pixel 
shadow maps, our technique effec­tively avoids aliasing while introducing an average computational overhead 
of only 14%. The storage of the SPSM uses a single 128­bit RGBA buffer, yielding a total memory occupancy 
of 4MB. While bringing signi.cant quality improvements, the storage of a single triangle per texel may 
result in occluder fusion during edge reconstruction. Future work will consider heuristics for en­hanced 
edges, typically using a combination of conservative and non-conservative rasterization. Sub-Pixel Shadow 
Maps are a simple and inexpensive alternative to classical shadow mapping, bringing high quality shadowing 
capa­bilities into highly demanding applications such as video games, in which neither image quality 
nor speed could be compromised. References DAI , Q., YA NG, B., A N D FE N G , J. 2008. Reconstructable 
geom­etry shadow maps. In Proceedings of I3D, 4:1 4:1. HA S S E LG R E N , J., AK E N I NE -M ¨ OLL E 
R, T., A N D OH LSS O N , L. 2005. Conservative rasterization. In GPU Gems 2, M. Pharr, Ed. Addison-Wesley, 
677 690. Permission to make digital or hard copies of part or all of this work for personal or classroom 
use isgranted without fee provided that copies are not made or distributed for commercial advantage and 
that copies bear this notice and the full citation on the first page. Copyrights for third-party components 
of thiswork must be honored. For all other uses, contact the Owner/Author.SIGGRAPH 2013, July 21 25, 
2013, Anaheim, California. 2013 Copyright held by the Owner/Author.ACM 978-1-4503-2261-4/13/07  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2013</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>2504484</article_id>
		<sort_key>250</sort_key>
		<display_label>Article No.</display_label>
		<display_no>20</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Lighting technology of the last of us]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2504459.2504484</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2504484</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4193624</person_id>
				<author_profile_id><![CDATA[81442616814]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Michal]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Iwanicki]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2013</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>2504485</article_id>
		<sort_key>260</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>21</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[Interactive indirect lighting computed in the cloud]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2504459.2504485</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2504485</url>
		<abstract>
			<par><![CDATA[<p>In this talk we present two different ways to map indirect lighting computation onto the new "pipeline" represented by the Cloud. Some of this new pipeline is familiar, but some is quite different. Given the vast space of possible hardware configurations and the many known algorithms for light transport, we explore two quite different approaches to indirect lighting and empirically examine their performance on the Cloud. As this examination is largely empirical, we use test scenes with significant geometric and texture complexity. We present detailed optimizations and measurements of bandwidth and latency performance. We further examine the amortization of indirect lighting across multiple users, a key potential benefit of Cloud-based systems.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4193625</person_id>
				<author_profile_id><![CDATA[81421599021]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Cyril]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Crassin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[NVIDIA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4193626</person_id>
				<author_profile_id><![CDATA[81100131290]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[David]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Luebke]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[NVIDIA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4193627</person_id>
				<author_profile_id><![CDATA[81549502856]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Michael]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Mara]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[NVIDIA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4193628</person_id>
				<author_profile_id><![CDATA[81327490282]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Morgan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[McGuire]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[NVIDIA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4193629</person_id>
				<author_profile_id><![CDATA[82459130757]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Brent]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Oster]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[NVIDIA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4193630</person_id>
				<author_profile_id><![CDATA[81477640569]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Peter]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Shirley]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[NVIDIA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4193631</person_id>
				<author_profile_id><![CDATA[81100524617]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>7</seq_no>
				<first_name><![CDATA[Peter-Pike]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sloan]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[NVIDIA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4193632</person_id>
				<author_profile_id><![CDATA[81100265704]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>8</seq_no>
				<first_name><![CDATA[Chris]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wyman]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[NVIDIA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Crassin, C., Neyret, F., Sainz, M., Green, S., and Eisemann, E. 2011. Interactive indirect illumination using voxel cone tracing. <i>Computer Graphics Forum 30</i>, 7.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Martin, S., and Einarsson, P. 2010. A real-time radiosity architecture for video games. <i>SIGGRAPH 2010 courses</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[McTaggart, G. 2004. Half-life 2/Valve Source shading. In <i>Direct3D Tutorial Day, Game Developers Conference</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2348825</ref_obj_id>
				<ref_obj_pid>2348816</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Shi, S., Nahrstedt, K., and Campbell, R. 2012. A real-time remote rendering system for interactive mobile graphics. <i>ACM Trans. Multimedia Comput. Commun. Appl. 8</i>, 3s.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Interactive indirect lighting computed in the Cloud Cyril Crassin David Luebke Michael Mara Morgan McGuire 
Brent Oster Peter Shirley Peter-Pike Sloan Chris Wyman NVIDIA  Figure 1: Direct-only vs. both direct 
and indirect in two systems. Cloud indirect computed and encoded in (left two) lightmaps and (right two) 
voxels. Please see the video for dynamic lighting. Abstract. In this talk we present two different ways 
to map indi­rect lighting computation onto the new pipeline represented by the Cloud. Some of this new 
pipeline is familiar, but some is quite different. Given the vast space of possible hardware con.gura­tions 
and the many known algorithms for light transport, we explore two quite different approaches to indirect 
lighting and empirically examine their performance on the Cloud. As this examination is largely empirical, 
we use test scenes with signi.cant geometric and texture complexity. We present detailed optimizations 
and mea­surements of bandwidth and latency performance. We further ex­amine the amortization of indirect 
lighting across multiple users, a key potential bene.t of Cloud-based systems. The Cloud. The evolution 
of distributed computing has a long and complex history. The Cloud is not so much a technolog­ical change 
as a change in scope and expectations: a variety of clients including phones, tablets, and PCs should 
be supported, and a certain level of seamlessness is expected. The presence of mo­bile clients introduces 
an asymmetry in the relative computational power of the client and server. Games. The symbiotic relationship 
between gamers, hardware vendors, and game developers has driven a co-evolution of better imagery on 
better hardware. Recent games have added indirect lighting as part of this improvement in imagery and 
it is an active area of development in the games industry [Martin and Einarsson 2010]. The challenge 
of providing users with this improved visual quality despite the limitations of low-power mobile clients 
suggests a distributed solution: compute indirect lighting on a server. Many modern multi-player game 
architectures already include a server as part of the system architecture. The technological landscape 
of games in the Cloud is summarized nicely in the recent article by Shi et al. [2012]. A light-map based 
approach. Our .rst system uses irradiance lightmaps [McTaggart 2004] computed dynamically and progres­sively 
on the server, and these lightmaps are sent across the network to the client. We take care to make these 
lightmaps compatible with the H.264 compression standard in order to use dedicated hardware on both client 
and server, greatly improving the speed and latency of lightmap transmission. The client computes the 
direct lighting interactively and composes this with whatever lightmaps it has at the time. Because the 
direct and indirect lighting are decoupled, the client experiences no stalls due to latency, but it instead 
has a lag between direct and indirect lighting. We will demonstrate this system with different amounts 
of lag, and we measure this lag on real systems in the wild (e.g., an internet connection across a signi.cant 
geographic distance). We also examine the bandwidth requirement of our system. A voxel-based approach. 
Our second system uses a voxel-based approach [Crassin et al. 2011] to compute and store the indirect 
lighting. This is a larger data structure than lightmaps, but does not require a UV-parameterization 
of the scene. Because of its larger size, this data structure is not suitable for transmission to the 
client. Instead the .nal images are computed on the Cloud and are streamed as video to the client. This 
has the advantages of be­ing able to support very low-power clients, as well as being able to take advantage 
of multiple and diverse nodes within the server. It has the disadvantage of requiring lower latency for 
acceptable user experience. This system is also demonstrated under various latency conditions. Summary. 
We will provide what is to our knowledge the .rst demonstration of interactive graphics with indirect 
lighting com­puted on the Cloud. This involves many technical details that do not come up in traditional 
systems, and provides an empirical demon­stration of of the perceptual effect of latency between direct 
and indirect lighting. We hope this will inspire much discussion and interest in both the research and 
production communities. References CRA SSIN ,C.,NEY R ET,F.,SAIN Z ,M., GRE EN ,S., AN D EIS EMANN , 
E. 2011. In­ teractive indirect illumination using voxel cone tracing. Computer Graphics Forum 30, 7. 
MARTIN ,S., AN D EINAR SS ON , P. 2010. A real-time radiosity architecture for video games. SIGGRAPH 
2010 courses. MCTAG G A RT, G. 2004. Half-life 2/Valve Source shading. In Direct3D Tutorial Day, Game 
Developers Conference. SHI,S .,NAH RST ED T,K., AN D CAMPB EL L , R. 2012. A real-time remote rendering 
system for interactive mobile graphics. ACM Trans. Multimedia Comput. Commun. Appl. 8, 3s. Permission 
to make digital or hard copies of part or all of this work for personal or classroom use isgranted without 
fee provided that copies are not made or distributed for commercial advantage and that copies bear this 
notice and the full citation on the first page. Copyrights for third-party components of thiswork must 
be honored. For all other uses, contact the Owner/Author.SIGGRAPH 2013, July 21 25, 2013, Anaheim, California. 
2013 Copyright held by the Owner/Author.ACM 978-1-4503-2261-4/13/07  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2013</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	</section>
	<section>
		<section_id>2504486</section_id>
		<sort_key>270</sort_key>
		<section_seq_no>6</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Multi-disciplinary collaboration in education]]></section_title>
		<section_page_from>6</section_page_from>
	<article_rec>
		<article_id>2504487</article_id>
		<sort_key>280</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>22</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Exploring the intersection between art, music, and technology]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2504459.2504487</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2504487</url>
		<abstract>
			<par><![CDATA[<p>This talk will present the results of a collaborative learning experience between students from the visual arts and computing sciences exploring the intersection between art, music, and interactive technology. Our goal is to provide an experiential learning environment that models industry in an experimental setting; fostering innovation and advancing the students skills while exposing them to technologies from other disciplines. It is our goal that the experience will assist them in developing critical problem solving approaches to project management.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4193633</person_id>
				<author_profile_id><![CDATA[82459226557]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Susan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lakin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[School of Photography, Rochester Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[srlpph@rit.edu]]></email_address>
			</au>
			<au>
				<person_id>P4193634</person_id>
				<author_profile_id><![CDATA[81100612714]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Joe]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Geigel]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Rochester Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[jmg@cs.rit.edu]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Exploring the Intersection Between Art, Music, and Technology Susan Lakin* Joe Geigel School of Photography 
Department of Computer Science Rochester Institute of Technology Rochester Institute of Technology Figure 
1: Examples of student projects 1 Introduction This talk will present the results of a collaborative 
learning experi­ence between students from the visual arts and computing sciences exploring the intersection 
between art, music, and interactive tech­nology. Our goal is to provide an experiential learning environment 
that models industry in an experimental setting; fostering innova­tion and advancing the students skills 
while exposing them to tech­nologies from other disciplines. It is our goal that the experience will 
assist them in developing critical problem solving approaches to project management. 2 Course Mechanics 
We chose to offer a pair of courses in different colleges, one in the College of Imaging Arts and Sciences 
and the other in the Golisano College of Computing and Information Sciences, to run simultane­ously and 
in partnership. Drawing upon a rich music scene in our community we chose the music video as our course 
subject, a genre that is evolving with technology and becoming more interactive. Together, both classes 
entered the course topics through the his­tory of the music video genre, exploring the connection between 
the visual arts and music, and examining how interactive technol­ogy is changing the way we experience 
music. Breaking down the academic walls between disciplines, we found it necessary to ex­pose the students 
to each others .eld of study. Grouped together by schools, the students are assigned to introduce themselves, 
their discipline, and present the work of professionals in their .eld. Be­fore embarking on the .nal 
group project, we meet separately with the students from our respective colleges to study new technology 
from their discipline. We brought both classes back together and assigned small interdisciplinary teams 
to create a test project meant to help the students discover the organization and cooperative skills 
needed for the production of an interdisciplinary artwork. From the beginning of the course, musicians 
are recruited from the com­munity and .nally interdisciplinary teams are assigned with indi­vidual roles 
clearly de.ned for the interactive .nal course projects. Each team must meet with their chosen musicians, 
identify a song, and develop a concept for their project. In a cooperative class­room environment, each 
team presents their project ideas in a work­ *e-mail:srlpph@rit.edu e-mail:jmg@cs.rit.edu in-progress 
critique to engage the entire class in developing their concept. Afterwards, each team creates a storyboard 
and sets a timetable with key deadlines through to post production and .nal presentation. 3 Results 
The students gain expertise in their own area via application in a larger problem and are exposed to 
diverse disciplines and new tech­nology. By reaching out to the music community, we expand the boundaries 
of the university and provide valuable experience for the students to work with professional musicians. 
The classroom at­mosphere nurtures innovation in an experimental environment and stimulates self-discipline. 
The students sharpen their career build­ing skills in collaboration, cooperation, compromise, and effective 
group dynamics. In a vigorous setting they produce a creative and innovative original work that blends 
aesthetics with technical func­tionality in a polished interactive artwork. Stills from the .nal course 
projects are illustrated in Figure 1. 4 Conclusion The course provides a collaborative, cross-college 
and shared learn­ing experience for students studying art and technology. The classes are structured 
to bring together different disciplines and prepare the students to work both independently and cooperatively 
in develop­ing a concept and solving problems related to the application of technology and imaging. Placing 
the students on interdisciplinary teams, they are exposed to different ways of thinking, a variety of 
personalities, and diverse skills; all which foster communication, compromise, and shared responsibility. 
The students are exposed to new technology and learn how to apply conventional approaches to visual communication 
into new forms of interactive media. They embraced the potential of the latest technology and developed 
in­novative approaches to achieve their desired outcomes. The .nal projects are made public via the Internet, 
exhibition, and/or through public performance. We hope the courses will serve as a model for future interdisciplinary 
curriculum collaborations. Permission to make digital or hard copies of part or all of this work for 
personal or classroom use isgranted without fee provided that copies are not made or distributed for 
commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights 
for third-party components of thiswork must be honored. For all other uses, contact the Owner/Author.SIGGRAPH 
2013, July 21 25, 2013, Anaheim, California. 2013 Copyright held by the Owner/Author.ACM 978-1-4503-2261-4/13/07 
 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2013</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>2504488</article_id>
		<sort_key>290</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>23</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Arts/tech collaboration with embedded systems and kinetic art]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2504459.2504488</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2504488</url>
		<abstract>
			<par><![CDATA[<p>The definition of "computer graphics" as used by artists in new media and kinetic areas of the arts is much more expansive than simply rendering to a screen. A visit to the SIGGRAPH art gallery, for example, will showcase a wide variety of uses of computing, embedded control, sensors, and actuators in the service of art. Kinetic art using embedded control is a marriage of art and technology. Artistic sensibility and creativity are required for concept and planning, and computer science and engineering skills are required to realize the artistic vision [Candy and Edmonds 2002]. However, these different skills are often taught in extremely different parts of a university campus.</p> <p>As an attempt to bridge this gap, we describe a cross-disciplinary collaborative course that pairs computer science students with art students to engage in joint engineering design and creative studio projects. These projects combine embedded system design with sculpture to create kinetic art. We believe that this is a natural pairing of two disparate disciplines, and one that provides distinct educational benefits to both groups of students [Brunvand and Stout 2011].</p> <p><i>Kinetic art</i> contains moving parts or depends on motion, sound, or light for its effect. The kinetic aspect is often regulated using micro-controllers connected to motors, actuators, transducers, and sensors that enable the sculpture to move and react to its environment [Malina 1974]. But, distinct from other types of computer art, the computer itself is usually not visible in the artwork. It is a behind the scenes controller. An <i>embedded system</i> is a special-purpose computer system (microcontroller) designed to perform one or a few dedicated functions, often reacting to environmental sensors. It is embedded into a complete device including hardware and mechanical parts rather than being a separate computer system.</p> <p>In the project-based semester-length class we describe in this talk computer science students work together with art students to build collaborative kinetic art pieces. Students explore interfacing of embedded systems with sensors and actuators of all sorts, along with real-time/interactive programming techniques and interrupt driven system design. By requiring that the project groups include both engineers and artists, the students contribute to their own learning and creative growth through peer teaching. Learning to communicate across disciplines, and perhaps just as importantly respect each other's skills and contributions, is vitally important for successful collaboration. The students also explore physical and conceptual aspects of machine-making as a fine-art sculpture process. The resulting artworks often make marks (produce physical "computer graphics") as a part of their artistic function.</p> <p>Our collaborative course builds on the powerful connection between embedded control and kinetic art. This pairing seems like a natural fit, and one with high potential for intriguing results. Engineers are rarely taught to think about artistic, conceptual, and aesthetic outcomes, and artists are not usually taught to think about engineering issues in creating an artistic artifact. The studio model is an intriguing model for more general CS education [Barker et al. 2005], but it is perhaps best experienced in a true studio course. A focus on design thinking also seems to us to be a natural complement to computational thinking.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4193635</person_id>
				<author_profile_id><![CDATA[81100034829]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Erik]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Brunvand]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Utah]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[elb@cs.utah.edu]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1047482</ref_obj_id>
				<ref_obj_pid>1047344</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Barker, L. J., Garvin-Doxas, K., and Roberts, E. 2005. What can computer science learn from a fine arts approach to teaching? In <i>SIGCSE '05</i>, ACM, New York, NY, USA.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1953263</ref_obj_id>
				<ref_obj_pid>1953163</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Brunvand, E., and Stout, P. 2011. Kinetic art and embedded systems: A natural collaboration. In <i>SIGCSE '11</i>, ACM, New York, NY, USA, ACM.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>772186</ref_obj_id>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Candy, L., and Edmonds, E. 2002. <i>Explorations in art and technology</i>. Springer-Verlag, London, UK.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Malina, F. 1974. <i>Kinetic Art: Theory and Practice</i>. Dover Publications, Inc.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Arts/Tech Collaboration with Embedded Systems and Kinetic Art Erik Brunvand* School of Computing University 
of Utah  Figure 1: Examples from the collaborative Embedded Systems and Kinetic Art class at the University 
of Utah. From the left they are: Suspended Drawing Machine (plastic, timing belt, pen, motors, computer 
control), Homespun Technology (conductive paint, LEDs, batteries, and IC), Drawbot (motor, battery, piano 
wire, graphite powder),Underwood 1910 (metal, typewriter, pneumatic actuators, computer control), and 
Cars (radio-controlled cars, white board, dry-erase markers, computer control) 1 Abstract The de.nition 
of computer graphics as used by artists in new me­dia and kinetic areas of the arts is much more expansive 
than simply rendering to a screen. A visit to the SIGGRAPH art gallery, for ex­ample, will showcase a 
wide variety of uses of computing, embed­ded control, sensors, and actuators in the service of art. Kinetic 
art using embedded control is a marriage of art and technology. Artistic sensibility and creativity are 
required for concept and planning, and computer science and engineering skills are required to realize 
the artistic vision [Candy and Edmonds 2002]. However, these differ­ ent skills are often taught in extremely 
different parts of a university campus. As an attempt to bridge this gap, we describe a cross-disciplinary 
collaborative course that pairs computer science students with art students to engage in joint engineering 
design and creative studio projects. These projects combine embedded system design with sculpture to 
create kinetic art. We believe that this is a natural pair­ing of two disparate disciplines, and one 
that provides distinct ed­ucational bene.ts to both groups of students [Brunvand and Stout 2011]. Kinetic 
art contains moving parts or depends on motion, sound, or light for its effect. The kinetic aspect is 
often regulated using micro­controllers connected to motors, actuators, transducers, and sensors that 
enable the sculpture to move and react to its environment [Ma­ lina 1974]. But, distinct from other types 
of computer art, the com­ puter itself is usually not visible in the artwork. It is a behind the scenes 
controller. An embedded system is a special-purpose com­puter system (microcontroller) designed to perform 
one or a few dedicated functions, often reacting to environmental sensors. It is embedded into a complete 
device including hardware and mechan­ical parts rather than being a separate computer system. In the 
project-based semester-length class we describe in this talk computer science students work together 
with art students to build *e-mail:elb@cs.utah.edu collaborative kinetic art pieces. Students explore 
interfacing of em­bedded systems with sensors and actuators of all sorts, along with real-time/interactive 
programming techniques and interrupt driven system design. By requiring that the project groups include 
both engineers and artists, the students contribute to their own learning and creative growth through 
peer teaching. Learning to commu­nicate across disciplines, and perhaps just as importantly respect each 
other s skills and contributions, is vitally important for suc­cessful collaboration.The students also 
explore physical and con­ceptual aspects of machine-making as a .ne-art sculpture process. The resulting 
artworks often make marks (produce physical com­puter graphics ) as a part of their artistic function. 
Our collaborative course builds on the powerful connection be­tween embedded control and kinetic art. 
This pairing seems like a natural .t, and one with high potential for intriguing results. Engi­neers 
are rarely taught to think about artistic, conceptual, and aes­thetic outcomes, and artists are not usually 
taught to think about engineering issues in creating an artistic artifact. The studio model is an intriguing 
model for more general CS education [Barker et al. 2005], but it is perhaps best experienced in a true 
studio course. A focus on design thinking also seems to us to be a natural comple­ment to computational 
thinking. References BA R K ER , L. J., GA RVI N -DOXA S , K., A N D RO B ERT S, E. 2005. What can computer 
science learn from a .ne arts approach to teaching? In SIGCSE 05, ACM, New York, NY, USA. BRU N VA ND, 
E., A ND STO U T, P. 2011. Kinetic art and embedded systems: A natural collaboration. In SIGCSE 11, ACM, 
New York, NY, USA, ACM. CA N DY, L., A N D EDM O N D S , E. 2002. Explorations in art and technology. 
Springer-Verlag, London, UK. MA L INA , F. 1974. Kinetic Art: Theory and Practice. Dover Pub­lications, 
Inc. Permission to make digital or hard copies of part or all of this work for personal or classroom 
use isgranted without fee provided that copies are not made or distributed for commercial advantage and 
that copies bear this notice and the full citation on the first page. Copyrights for third-party components 
of thiswork must be honored. For all other uses, contact the Owner/Author.SIGGRAPH 2013, July 21 25, 
2013, Anaheim, California. 2013 Copyright held by the Owner/Author.ACM 978-1-4503-2261-4/13/07  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2013</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>2504489</article_id>
		<sort_key>300</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>24</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Creating a nimble new curriculum for digital media artists]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2504459.2504489</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2504489</url>
		<abstract>
			<par><![CDATA[<p>Emerging technologies, digital media environments, and mobile media are rapidly changing the landscape of learning required for digital media artists. The author will discuss the theory and practice of immersion applied in the undergraduate digital media arts program and in the Master of Arts in Learning and Emerging Technologies (MALET) at SUNY Empire State College. These new programs allow established and emergent digital media artists to collaborate on interdisciplinary, immersive media arts or design projects within virtual environments while pursuing individualized degrees. Our nimble approach to learning design includes mentoring intensive models, fully online courses, mobile learning, prior learning assessment, and totally individualized learning on subjects proposed by students.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4193636</person_id>
				<author_profile_id><![CDATA[82458977857]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Nicola]]></first_name>
				<middle_name><![CDATA[Marae]]></middle_name>
				<last_name><![CDATA[Allain]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[SUNY Empire State College, Saratoga Springs, NY]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[nicola.allain@esc.edu]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Nelson, Brian C., and Benjamin E. Erlandson. 2012. <i>Design for learning in virtual worlds</i>. New York: Routledge.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Wankel, Charles, and Patrick Blessinger. 2012. <i>Increasing student engagement and retention using immersive interfaces: virtual worlds, gaming, and stimulation</i>. Bingley: Emerald.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Creating a Nimble New Curriculum for Digital Media Artists Nicola Marae Allain, Ph. D. SUNY Empire State 
College Saratoga Springs, NY 12866 USA nicola.allain@esc.edu 1. Introduction Emerging technologies, 
digital media environments, and mobile media are rapidly changing the landscape of learning required 
for digital media artists. The author will discuss the theory and practice of immersion applied in the 
undergraduate digital media arts program and in the Master of Arts in Learning and Emerging Technologies 
(MALET) at SUNY Empire State College. These new programs allow established and emergent digital media 
artists to collaborate on interdisciplinary, immersive media arts or design projects within virtual environments 
while pursuing individualized degrees. Our nimble approach to learning design includes mentoring intensive 
models, fully online courses, mobile learning, prior learning assessment, and totally individualized 
learning on subjects proposed by students. 2. Exposition Students co-create at a distance in collaborative 
virtual environments as part of their learning. This immersion includes the integration of a complex 
set of skills habituation to being within an avatar embodiment, habile navigation, communication etiquette, 
and orienting oneself to the environment. This develops a sense of community and team-building that provides 
essential skills for 21st Century artists. In immersive and mobile learning situations, students must 
also interact with, and create, a variety of digital media tools in interdisciplinary contexts. 3. Elaboration 
Faculty teaching in the program are scholar/artists in multiple digital media arts, with strong training 
in media theory and research practice in immersive learning design. Participating students have advanced 
knowledge of one or more of the following areas: digital art and design, computer arts, video, electronic 
music, digital storytelling, filmmaking, game design, animation, visual effects, motion graphics, animation 
art and design, digital photography, 3D virtual worlds, digital performance, mobile media design, and 
audio production. Peers provide expertise from different artistic genres within the emerging digital 
media arts fields. In collaborative settings, they also coordinate the complex logistics of teamwork 
and content creation as they master the new environment. For example, in a Center for Distance Learning 
Media Arts course, students co­create interactive media works while working at a geographical distance. 
The work culminates in a juried Media Arts Festival hosted in a virtual world. Presentation of their 
piece for the festival requires a high level integration of immersive literacy while building a strong 
sense of community. Figure 1. SUNY Empire State College Virtual Campus 4. Results Digital media arts 
students have completed impressive short films created across time and distance. Graduate students are 
designing immersive learning experiences. Students in mobile media design mobile applications using mobile 
devices. Talented students have completed independent studies on creating worlds, games as interactive 
storytelling, writing for games, virtual worlds, and other creative topics. Degree plans have been approved 
in digital art and design, digital storytelling and media arts, and other areas reflecting new directions 
in the curriculum. 5. Conclusion These studies engage highly skilled artists from different genres in 
the creation of digital stories, films, interactive web media, visual narratives, games, and mixed media. 
They come together in experimental environments to develop full-fledged projects in virtual creative 
teams , and showcase their work in immersive Media Arts Festivals or Design Showcases. Advanced students 
practice a deep analysis of artistic processes and possibilities while pursuing pathways in digital media 
arts genres. These nimble new approaches provide opportunities for real life learning and the sophisticated 
digital skills required of 21st Century digital media artists. References Nelson, Brian C., and Benjamin 
E. Erlandson. 2012. Design for learning in virtual worlds. New York: Routledge. Wankel, Charles, and 
Patrick Blessinger. 2012. Increasing student engagement and retention using immersive interfaces: virtual 
worlds, gaming, and stimulation. Bingley: Emerald. Permission to make digital or hard copies of part 
or all of this work for personal or classroom use isgranted without fee provided that copies are not 
made or distributed for commercial advantage and that copies bear this notice and the full citation on 
the first page. Copyrights for third-party components of thiswork must be honored. For all other uses, 
contact the Owner/Author.SIGGRAPH 2013, July 21 25, 2013, Anaheim, California. 2013 Copyright held by 
the Owner/Author.ACM 978-1-4503-2261-4/13/07  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2013</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>2504490</article_id>
		<sort_key>310</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>25</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[Best practices in short animation production in private/public partnerships]]></title>
		<subtitle><![CDATA[an agile approach]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2504459.2504490</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2504490</url>
		<abstract>
			<par><![CDATA[<p>Since 2004, Seneca College in Toronto, Canada, has partnered with animation production studios and government agencies to produce a number of highly successful short animated films directed by A-list directors. Beginning with Chris Landreth's Academy Award winning film <i>Ryan</i>, The National Film Board of Canada (NFB) and Copperheart Productions worked with Seneca in developing a production management method the leveraged the strengths of each organization by making the economics of the production manageable for each partner while simultaneously delivering unique benefits to each.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4193637</person_id>
				<author_profile_id><![CDATA[82458693957]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Mark]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Jones]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[School of Creative Arts & Animation, Seneca College, Toronto, Canada]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[mark.jones@senecacollege.ca]]></email_address>
			</au>
			<au>
				<person_id>P4193638</person_id>
				<author_profile_id><![CDATA[82459136457]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Sean]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Craig]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Seneca College, Toronto, Canada]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[sean.craig@senecacollege.ca]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Best Practices in Short Animation Production in Private/Public Partnerships: An Agile Approach Mark 
Jones Chair, School of Creative Arts &#38; Animation Seneca College, Toronto, Canada mark.jones@senecacollege.ca 
 Since 2004, Seneca College in Toronto, Canada, has partneredwith animation production studios and government 
agencies toproduce a number of highly successful short animated films directed by A-list directors. Beginning 
with Chris Landreth s Academy Award winning film Ryan, The National Film Board ofCanada (NFB) and Copperheart 
Productions worked with Senecain developing a production management method the leveraged thestrengths 
of each organization by making the economics of the production manageable for each partner while simultaneouslydelivering 
unique benefits to each. Out of this model Seneca created its Production Project inAnimation Summer Institute, 
a graduate-level single semesterspecialty program that brings together recent graduates from animation 
programs with professional studios and public funders to support the production of short films of high 
quality that goonto distribution and festival circuit success. Since the creation of this Summer Institute, 
Seneca has partnered on films such as The Spine (NFB, 2008), Ormie (ARC Productions, 2010), Lovebirds 
(ARC Productions, unreleased), Drawing from Life (NFB, 2010), and In Search of Blind Joe Death: The Story 
of John Fahey(Tamarak Productions, 2012). In 2013, the NFB will release Subconscious Password, the thirdChris 
Landreth collaboration between The National Film Board of Canada, Copperheart Productions, and Seneca 
College.Throughout the production Seneca had fifteen students participatein areas of concept design, 
modeling, texturing, animation, rigging, lighting and compositing as well as pipelinedevelopment; and 
five college professors working in the areas ofcharacter design supervision, lighting and rendering supervision,animation 
supervision, effects supervision, pipeline development and compositing supervision. The NFB provided 
senior artists tohelp with rigging, texturing, animation, lighting and effects, aswell as technical support 
at the Montreal location. Copperheartprovided additional production support in the way of audio recording 
and mastering, and live action sequence production. With Subconscious Password, there were unique challenges 
that required that each of the partners break new ground. These challenges were outside the scope of 
creative work, and weremore about infrastructure, networking, file transfer/management,and studio space: 
1. The animation crew was split over two locations in Torontoand Montreal, demanding a high-speed network 
and efficientsynchronized pipeline be used. Here, Seneca was able to leverage use of the CANARIE network, 
a high performancehybrid network that supports data-intensive research and innovation across a range 
of private and public sector users. This network enabled the project to transfer files at up to 600Mbps. 
Sean Craig Professor/Program Coordinator, 3D Animation Seneca College, Toronto, Canada sean.craig@senecacollege.ca 
 2. Rendering was also distributed across the two remote locations. Here, Seneca developed a specific 
pipeline to anticipate problems that would occur at render. Once animation was finished, nCloth and Hair 
simulations were run, and the characters were GEO Cached out to render scenes. Every shot was entirely 
rebuilt in lighting and rendering. This allowed us to work in parallel on look development as animation 
was ongoing, and also ensured that we would not be rendering full character rigs. All renderlayers, Mattes, 
light rigs, and pass contribution maps werealso scripted. The process of rebuilding a shot in lighting 
was streamlined to take about 20 minutes per shot. 3. This new film was stereoscopic, a first for Chris 
Landrethand Seneca, requiring additional considerations be built intothe pipeline.  The production pipeline 
was built as the partners worked throughthe film, as there was not adequate time or resources to developthe 
pipeline prior to starting, reflecting an Agile Development model of production execution. The talk will 
present the methods in which the production used an Agile approach to execution, along with the innovations 
implemented in terms of network and render pipeline creation. Finally, the presenters will review the 
business and organizationalmodel used in the Seneca Animation Summer Institute Program asa best practice 
for production partnerships between education,industry, and government. Permission to make digital or 
hard copies of part or all of this work for personal or classroom use isgranted without fee provided 
that copies are not made or distributed for commercial advantage and that copies bear this notice and 
the full citation on the first page. Copyrights for third-party components of thiswork must be honored. 
For all other uses, contact the Owner/Author.SIGGRAPH 2013, July 21 25, 2013, Anaheim, California. 2013 
Copyright held by the Owner/Author.ACM 978-1-4503-2261-4/13/07 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2013</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	</section>
	<section>
		<section_id>2504491</section_id>
		<sort_key>320</sort_key>
		<section_seq_no>7</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Put that in your pipe!]]></section_title>
		<section_page_from>7</section_page_from>
	<article_rec>
		<article_id>2504492</article_id>
		<sort_key>330</sort_key>
		<display_label>Article No.</display_label>
		<display_no>26</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[<i>TidScene</i>]]></title>
		<subtitle><![CDATA[Pixar's pipeline backplane]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2504459.2504492</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2504492</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4193639</person_id>
				<author_profile_id><![CDATA[82458679857]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Arun]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Rao]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2013</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>2504493</article_id>
		<sort_key>340</sort_key>
		<display_label>Article No.</display_label>
		<display_no>27</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Lurch!]]></title>
		<subtitle><![CDATA[Interactive rendering pipeline automation]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2504459.2504493</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2504493</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4193640</person_id>
				<author_profile_id><![CDATA[81100359188]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Alexander]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kolliopoulos]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2013</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>2504494</article_id>
		<sort_key>350</sort_key>
		<display_label>Article No.</display_label>
		<display_no>28</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[ReviewTool]]></title>
		<subtitle><![CDATA[a database-driven visual effects editing application]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2504459.2504494</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2504494</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4193641</person_id>
				<author_profile_id><![CDATA[81466647316]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Damien]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Fagnou]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4193642</person_id>
				<author_profile_id><![CDATA[82458821557]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Christopher]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Cameron]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4193643</person_id>
				<author_profile_id><![CDATA[82458905357]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Adam]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Valdez]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2013</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>2504495</article_id>
		<sort_key>360</sort_key>
		<display_label>Article No.</display_label>
		<display_no>29</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[Pronto]]></title>
		<subtitle><![CDATA[scheduling the un-schedulable]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2504459.2504495</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2504495</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4193644</person_id>
				<author_profile_id><![CDATA[81421597655]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Hannes]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ricklefs]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2013</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	</section>
	<section>
		<section_id>2504496</section_id>
		<sort_key>370</sort_key>
		<section_seq_no>8</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Game cinematics & stereoscopic]]></section_title>
		<section_page_from>8</section_page_from>
	<article_rec>
		<article_id>2504497</article_id>
		<sort_key>380</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>30</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Zerg rush hour]]></title>
		<subtitle><![CDATA[simulating swarms for <i>StarCraft 2</i> cinematics]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2504459.2504497</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2504497</url>
		<abstract>
			<par><![CDATA[<p>In <i>Starcraft 2: Heart of the Swarm's</i> introductory cinematic, tens of thousands of alien creatures known as the Zerg descend upon and infest a human city. To animate them all, a procedural particle-based crowd system was created to simulate their movement and behavior. Artist-friendly controls direct the swarm to move, attack, climb, and leap using volume primitives in a lightweight implementation, providing fast turnaround for modifying and executing new iterations.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4193645</person_id>
				<author_profile_id><![CDATA[82459281457]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Matt]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Cordner]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Blizzard Entertainment]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[mcordner@blizzard.com]]></email_address>
			</au>
			<au>
				<person_id>P4193646</person_id>
				<author_profile_id><![CDATA[81100426409]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Bill]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[La Barge]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Blizzard Entertainment]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[blabarge@blizzard.com]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[H. K&#252;ck, C. Vogelgsang., and G. Greiner, 2002. Simulation and Rendering of Liquid Foams. Graphics Interface (GI).]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Zerg Rush Hour: Simulating Swarms for StarCraft 2 Cinematics Matt Cordner* Bill La Barge Blizzard Entertainment 
 1 Introduction In Starcraft 2: Heart of the Swarm s introductory cinematic, tens of thousands of alien 
creatures known as the Zerg descend upon and infest a human city. To animate them all, a procedural particle­based 
crowd system was created to simulate their movement and behavior. Artist-friendly controls direct the 
swarm to move, attack, climb, and leap using volume primitives in a lightweight implemen­tation, providing 
fast turnaround for modifying and executing new iterations. 2 Fast Simulations Using Particles The Zerglings 
were required to move through the city in an insect­like swarm, crawling over each other and around obstacles 
in their path. They needed to avoid certain areas, navigate around corners, attack targets, and react 
to explosions. Fast simulations and intu­itive controls were also desired. We began with a simple Houdini 
particle setup and created a single VEX node to implement most of the swarming behavior. The VEX code 
relied on point cloud methods, volumes, and a zstate variable to determine each individual Zerg creature 
s movement capabili­ties and desired actions. Some examples are whether the creature was climbing, scrambling 
over other Zerglings, attacking, being crushed, blocked, overcrowded, or had enough support to propel 
forward, and in which direction. Once a target velocity vector was determined, a simple algorithm adapted 
from K ¨uck s bubbles simulation was applied. In this method, the total force acting on the bubble is 
a summation of re­pulsive and attractive forces, friction, and gravity. + Fra Vi = 1 (kvVi + kof Vo i 
i + Fg) (1)kv + kof + kair The properties of bubbles .t the swarm pro.le excellently: the Zerg moved 
.uidly, staying close to one another without much interpen­etration, and could climb over obstacles and 
each other rather eas­ily. The simulation would query the state of the previous frame by treating it 
as a point cloud to .nd each Zerg s nearest neighbors, their zstates and velocities. The target velocity 
was worked into the above equation by applying it as a frictional force, to simulate the Zergling propelling 
itself forward by pushing off of whatever it is currently supported by (e.g., ground, obstacles, other 
Zerglings). 3 Using Volumes for Crowd Simulations Volumes are ideally suited for particle simulations, 
as they have a low overhead for accessing data such as velocity vectors and signed distance .elds. In 
our case of animating the swarm, volumes were used for the same reasons. Instead of de.ning a list of 
objects that the swarm would interact with and iterating over those elements to .nd their relative locations 
and distances to each individual crea­ture, we employed volumes to represent them all. For the general 
direction of movement, we created a velocity .eld that de.nes the path of the swarm through streets and 
around build­ *mcordner@blizzard.com blabarge@blizzard.com Figure 1: The Zerg swarm .lls a city street. 
c @Blizzard Entertainment, Inc. All rights reserved. ings. For objects that the Zerglings could climb 
over, a signed dis­tance .eld was suf.cient for level ground, ramps, rubble, tanks, and other larger 
creatures such as the Nydus Worm that breaches the plaza. A fast sample of these volumes would return 
the distance to the object, gradient and the normal of that element. With that information, we could 
determine what the Zergling is encounter­ing and whether or not it should climb over, go around or attack 
it. Avoidance volumes were created to have the smaller Zerglings avoid being stomped on by the much larger 
Ultralisk creatures. An­other set of volumes were turned on and off in concert with many of the explosions, 
which scatter the creatures nearest to them in all directions. When we reached the .nal shot, we noticed 
that Zerglings were running through the ranks of enemy marine formations. We again used volumes to de.ne 
kill zones, to identify where Zerglings were coming under .re from the marines ri.es and the intensity 
of that .re. Each Zergling inside the kill zone would check a random function see if it should be killed 
and play a death animation. 4 Rendering The Swarm Each particle from the simulation represented a single 
creature. For visualization in Houdini, we instanced cached animations of a run­ning Zerg creature onto 
each particle. To render .nal frames, the particle data was written to RIB .les and then parsed to insert 
the appropriate geometry procedural calls and shaders. Loading geom­etry at render time through the use 
of a procedural kept our RIB data small, and provided .exibility to utilize different LOD models. 5 
Conclusion For animating a swarm of insect-like Zerg, we were able to create a very fast, large-scale 
simulation system that demonstrated complex behavior but was still easy to manipulate and augment by 
the artist. Our use of volumes was instrumental in gathering the data neces­sary to determine the creatures 
actions and method of interaction with the scene. References H. K ¨ U CK , C. VO G E LG S A N G ., A 
ND G. GR E I NE R, 2002. Simula­tion and Rendering of Liquid Foams. Graphics Interface (GI). Permission 
to make digital or hard copies of part or all of this work for personal or classroom use isgranted without 
fee provided that copies are not made or distributed for commercial advantage and that copies bear this 
notice and the full citation on the first page. Copyrights for third-party components of thiswork must 
be honored. For all other uses, contact the Owner/Author.SIGGRAPH 2013, July 21 25, 2013, Anaheim, California. 
2013 Copyright held by the Owner/Author.ACM 978-1-4503-2261-4/13/07  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2013</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>2504498</article_id>
		<sort_key>390</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>31</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Blizzard entertainment]]></title>
		<subtitle><![CDATA[Diablo 3 cinematics wing effects]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2504459.2504498</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2504498</url>
		<abstract>
			<par><![CDATA[<p>For Blizzard Entertainment's <i>Diablo 3</i> cinematics, the archangels <i>Imperious</i> and <i>Tyreal</i> possessed wings that were an extension of each character's actions and mood. The fire and celestial wing effect concepts augmented the animation of the wings and reflected each angel's personality. A system was designed to allow for minimal use of disk space, simulation cycles, and artist time. <i>Imperious</i> alone had 44 individual feathers that comprised his wings.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4193647</person_id>
				<author_profile_id><![CDATA[82458888957]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Christopher]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Blizzard Entertainment]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4193648</person_id>
				<author_profile_id><![CDATA[82458863457]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Hosuk]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Chang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Blizzard Entertainment]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4193649</person_id>
				<author_profile_id><![CDATA[81100426409]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Bill]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[La Barge]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Blizzard Entertainment]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4193650</person_id>
				<author_profile_id><![CDATA[82458833457]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Jeremy]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Pilgrim]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Blizzard Entertainment]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4193651</person_id>
				<author_profile_id><![CDATA[82459220857]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Jason]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Burton]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Blizzard Entertainment]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Blizzard Entertainment Diablo 3 Cinematics Wing Effects  Christopher Yang, Hosuk Chang, Bill La Barge, 
Jeremy Pilgrim, Jason Burton Blizzard Entertainment  Figure 1: Diablo 3 stills of Imperious (left) and 
Tyreal (right) Introduction For Blizzard Entertainment's Diablo 3 cinematics, the archangels Imperious 
and Tyreal possessed wings that were an extension of each character's actions and mood. The fire and 
celestial wing effect concepts augmented the animation of the wings and reflected each angel s personality. 
A system was designed to allow for minimal use of disk space, simulation cycles, and artist time. Imperious 
alone had 44 individual feathers that comprised his wings. From Concept to Production The need for 
predictable fire effects required a more traditional particle and geometry centered solution, thus avoiding 
simulated fire elements. Pre-baked out sequences of particles were used in lieu of running costly fire 
simulations. For Tyreal s 16 blue celestial wing feathers, each was a combination of 12 passes. Between 
each undulating feather segment, there needed to be a thin veil webbing to augment the look and presence 
of the wings. Preserving the Animation Both angels sets of wings were animated within Maya and those 
resultant polygonal structures were procedurally converted to nurb surfaces and curves within Houdini. 
The surfaces and lines became the foundation for all the subsequent effects passes. The surface provided 
a normalized domain of 0 to 1, correlating from root to tip of each wing feather segment. This surface 
mimicked the original animation and allowed a defined zone for the baked-out particles to traverse. 
Millions of Particles For Imperious, a particle sequence with interesting ebb and flow motion was generated 
moving through a velocity field using FumeFX. These particles were deformed from their original path 
to move along the shape of the wing from root to tip. The particles served as the base element along 
with 13 other elements for each feather to generate the final look. When composited in Nuke, the particles 
and curves combined, forming a fiery look that was sought after in the concept art. To cover each of 
Imperious s feathers with particles, the particle count ranged from 1 million for the shortest feathers, 
to over 5 million for the longer, hero feathers. Copies of the original particle sequence were placed 
along each feather span at render time. Each copy sampled different starting frames of the original sequence, 
negating the need to run individual simulations for each feather.  Figure 2: Imperious wing element 
sample passes. From Pre-Baked to Sim As the production evolved, the Imperious wing effects were required 
to flex and react to movement. By factoring in the velocities of the original wing animation, the pre-baked 
particles were allowed to flex along their path to mimic lag and follow through. The pre-baked particles 
were also converted on a point by point basis from a fixed translation to become part of a fully dynamic 
simulation. Each particle was able to disassociate itself from the pre-baked geometry sequence depending 
upon the velocities applied to that particle from the motion of the wing feathers. Forces such as wind 
and gravity and collisions were then applied to the once pre-baked particles. This added a dynamic feel 
to the once choreographed particle movement. The original particle count was preserved and the tear-off 
flow created a look of connectivity between the wing elements and the environment. As these newly dynamic 
particles flowed away from the wing, they were acted upon by a velocity field that included the character 
motion and environment collision objects to achieve a more plausible look of interaction. Celestial 
Smooth Flow The ethereal look of Tyreal s wings was achieved through a custom shader that provided passes 
of the wing geometry s undulating surface base on its surface curvature. The webbing between feather 
strands was only generated if they were a certain length in camera space view.  Figure 3: Tyreal wing 
element sample passes. Conclusion These procedural systems allowed the artists to achieve the director 
s goals with predictable and repeatable results. We have shown that this method was a viable alternative 
to solely relying on fluid or gas simulations for fire type effects. 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2013</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	</section>
	<section>
		<section_id>2504499</section_id>
		<sort_key>400</sort_key>
		<section_seq_no>9</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Face the facts]]></section_title>
		<section_page_from>9</section_page_from>
	<article_rec>
		<article_id>2504500</article_id>
		<sort_key>410</sort_key>
		<display_label>Article No.</display_label>
		<display_no>32</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[A deformer-based approach to facial rigging]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2504459.2504500</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2504500</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4193652</person_id>
				<author_profile_id><![CDATA[82458999657]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[John]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kahwaty]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4193653</person_id>
				<author_profile_id><![CDATA[82459126257]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Gene]]></first_name>
				<middle_name><![CDATA[S.]]></middle_name>
				<last_name><![CDATA[Lee]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4193654</person_id>
				<author_profile_id><![CDATA[81504685734]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Greg]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Smith]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4193655</person_id>
				<author_profile_id><![CDATA[82459088257]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Andy]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4193656</person_id>
				<author_profile_id><![CDATA[82459263457]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Matt]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Schiller]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2013</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>2504501</article_id>
		<sort_key>420</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>33</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Driving high-resolution facial blendshapes with video performance capture]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2504459.2504501</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2504501</url>
		<abstract>
			<par><![CDATA[<p>We present a technique for creating realistic facial animation from a set of high-resolution static scans of an actor's face driven by passive video of the actor from one or more viewpoints. We capture high-resolution static geometry using multi-view stereo and gradient-based photometric stereo [Ghosh et al. 2011]. The scan set includes around 30 expressions largely inspired by the Facial Action Coding System (FACS). Examples of the input scan geometry can be seen in Figure 1 (a). The base topology is defined by an artist for the neutral scan of each subject. The dynamic performance can be shot under existing environmental illumination using one or more off-the shelf HD video cameras.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4193657</person_id>
				<author_profile_id><![CDATA[81421597317]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Graham]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Fyffe]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[USC Institute for Creative Technologies]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4193658</person_id>
				<author_profile_id><![CDATA[81100556708]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Andrew]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Jones]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[USC Institute for Creative Technologies]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4193659</person_id>
				<author_profile_id><![CDATA[81458649961]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Oleg]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Alexander]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[USC Institute for Creative Technologies]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4193660</person_id>
				<author_profile_id><![CDATA[81414619646]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Ryosuke]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ichikari]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[USC Institute for Creative Technologies]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4193661</person_id>
				<author_profile_id><![CDATA[81504686804]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Paul]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Graham]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[USC Institute for Creative Technologies]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4193662</person_id>
				<author_profile_id><![CDATA[82458991657]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Koki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nagano]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[USC Institute for Creative Technologies]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4193663</person_id>
				<author_profile_id><![CDATA[81440600975]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>7</seq_no>
				<first_name><![CDATA[Jay]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Busch]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[USC Institute for Creative Technologies]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4193664</person_id>
				<author_profile_id><![CDATA[81504685651]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>8</seq_no>
				<first_name><![CDATA[Paul]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Debevec]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[USC Institute for Creative Technologies]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[gl@usc.ict.edu]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>2024163</ref_obj_id>
				<ref_obj_pid>2024156</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Ghosh, A., Fyffe, G., Tunwattanapong, B., Busch, J., Yu, X., and Debevec, P. 2011. Multiview face capture using polarized spherical gradient illumination. In <i>Proceedings of the 2011 SIGGRAPH Asia Conference</i>, ACM, New York, NY, USA, SA '11, 129:1--129:10.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Werlberger, M. 2012. <i>Convex Approaches for High Performance Video Processing</i>. PhD thesis, Institute for Computer Graphics and Vision, Graz University of Technology, Graz, Austria.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Driving High-Resolution Facial Blendshapes with Video Performance Capture Graham Fyffe Andrew Jones 
Oleg Alexander Ryosuke Ichikari Paul Graham Koki Nagano Jay Busch Paul Debevec * USC Institute for Creative 
Technologies Figure 1: (a) Geometry and re.ectance data from multiple static scans are combined with 
(b) dynamic video frames to recover (c) animated high resolution geometry that can be (d) relit under 
novel illumination. This example is recovered using only a single camera viewpoint. We present a technique 
for creating realistic facial animation from a set of high-resolution static scans of an actor s face 
driven by passive video of the actor from one or more viewpoints. We cap­ture high-resolution static 
geometry using multi-view stereo and gradient-based photometric stereo [Ghosh et al. 2011]. The scan 
set includes around 30 expressions largely inspired by the Facial Action Coding System (FACS). Examples 
of the input scan geom­ etry can be seen in Figure 1 (a). The base topology is de.ned by an artist for 
the neutral scan of each subject. The dynamic perfor­mance can be shot under existing environmental illumination 
using one or more off-the shelf HD video cameras. Our algorithm runs on a performance graph that represents 
dense correspondences between each dynamic frame and multiple static scans. An ideal performance graph 
will have edges that are sparse and well distributed as sequential frames are similar in appearance and 
any single dynamic frame only spans a small subset of facial expressions. Alternatively, facial deformation 
may be local (only affecting part of the face), for example the eyebrows may be raised while mouth remains 
neutral. Theoretically, all performance frames should lie within the domain of scanned FACS expressions 
allowing us to minimize temporal drift. We developed an ef.cient scheme for selecting a subset of possible 
image pairs for reduced optical .ow computation. We prune the graph based on a quarter-resolution GPU 
optical .ow [Werlberger 2012] between each static frame and each dynamic frame on a sin­gle frontal camera 
view. If necessary, we can rerender the static frames to match the lighting and rough head pose in the 
dynamic scene. We then compute normalized cross correlation between the warped dynamic frame and each 
original static expression and av­erage over twelve facial regions. We developed a iterative greedy voting 
algorithm based on per-region con.dence measure to iden­tify good edges. In each iteration we search, 
over all static expres­sions and all sequence frames, to identify the frame whose region has the highest 
con.dence match to a static expression. We add that dynamic to static edge to the performance graph and 
compute high-resolution .ow for that image pair. We then adjust the remain­ *e-mail:gl@usc.ict.edu ing 
con.dence weights by subtracting a hat function scaled by the con.dence of the selected expression in 
each region and centered around the last chosen frame. The hat function suppresses other fa­cial regions 
that are satis.ed by the new .ow. We continue to iterate until the maximum con.dence value falls below 
a threshold. Based on the pruned performance graph, we apply a novel 2D drift­free vertex tracking scheme 
leveraging temporal optical .ow and multiple texture targets. We track each mesh vertex as a 2D path 
in each camera image both forward and backwards in time. We com­bine .ow vectors as a weighted sum of 
bidirectional temporal .ows and nearby static-to-dynamic .ows. Similar to the voting scheme, we weigh 
each static-to-dynamic .ow vectors as a hat function that decays over temporal .ows. We compute the 3D 
position of each vertex in the face mesh independently for each frame of the per­formance by triangulating 
the 2D projected vertex locations. Novel to our triangulation is a least-squares formulation penalizing 
the squared distance from the ray emitted from each camera at the 2D projected location. We regularize 
the triangulation using a small penalty on the distance along the ray to a hand-placed proxy face mesh. 
This regularization improves the stability of our solution, and at the same time enables the use of monocular 
data for triangu­lation. We simultaneously enforce a multi-target Laplacian shape regularization term 
within the same least-squares framework. In contrast to previous work, we consider the three-dimensional 
cou­pling between the shape term and triangulation terms, thereby ob­taining a robust estimate that gracefully 
.lls in missing or unreliable triangulation information using multiple target shape exemplars. References 
GH O S H , A., FY FF E , G., TU N WATTA NAPON G , B., BU SC H , J., YU, X., AN D DEB E V EC , P. 2011. 
Multiview face capture using polarized spherical gradient illumination. In Proceedings of the 2011 SIGGRAPH 
Asia Conference, ACM, New York, NY, USA, SA 11, 129:1 129:10. WE R L BER G E R , M. 2012. Convex Approaches 
for High Perfor­mance Video Processing. PhD thesis, Institute for Computer Graphics and Vision, Graz 
University of Technology, Graz, Aus­tria. Permission to make digital or hard copies of part or all of 
this work for personal or classroom use isgranted without fee provided that copies are not made or distributed 
for commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights 
for third-party components of thiswork must be honored. For all other uses, contact the Owner/Author.SIGGRAPH 
2013, July 21 25, 2013, Anaheim, California. 2013 Copyright held by the Owner/Author.ACM 978-1-4503-2261-4/13/07 
 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2013</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>2504502</article_id>
		<sort_key>430</sort_key>
		<display_label>Article No.</display_label>
		<display_no>34</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Hair growth by means of sparse volumetric modeling and advection]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2504459.2504502</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2504502</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4193665</person_id>
				<author_profile_id><![CDATA[82459039757]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Ashraf]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ghoniem]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4193666</person_id>
				<author_profile_id><![CDATA[81424592239]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Ken]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Museth]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2013</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>2504503</article_id>
		<sort_key>440</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>35</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[Incendiary reflection]]></title>
		<subtitle><![CDATA[evoking emotion through deformed facial feedback]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2504459.2504503</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2504503</url>
		<abstract>
			<par><![CDATA[<p>Incendiary reflection aims to create computer-generated emotion by letting people recognize pseudo-generated facial expressions as changes to their own facial expressions (Figure 1, left).</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4193667</person_id>
				<author_profile_id><![CDATA[81555914056]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Shigeo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yoshida]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[shigeodayo@cyber.t.u-tokyo.ac.jp]]></email_address>
			</au>
			<au>
				<person_id>P4193668</person_id>
				<author_profile_id><![CDATA[81490640954]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Sho]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sakurai]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[sho@cyber.t.u-tokyo.ac.jp]]></email_address>
			</au>
			<au>
				<person_id>P4193669</person_id>
				<author_profile_id><![CDATA[81331500452]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Takuji]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Narumi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[narumi@cyber.t.u-tokyo.ac.jp]]></email_address>
			</au>
			<au>
				<person_id>P4193670</person_id>
				<author_profile_id><![CDATA[81100172183]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Tomohiro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tanikawa]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[tani@cyber.t.u-tokyo.ac.jp]]></email_address>
			</au>
			<au>
				<person_id>P4193671</person_id>
				<author_profile_id><![CDATA[81100257385]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Michitaka]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hirose]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[hirose@cyber.t.u-tokyo.ac.jp]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[James, W., The principles of psychology, vol. 2. Dover Publications, New York, 1950.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Tomkins, S., Affect, imagery, and consciousness: The Positive affects; Springer, New York, vol.1, 1962.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1141920</ref_obj_id>
				<ref_obj_pid>1141911</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Shaefer, S. et al., Image deformation using moving least squares, ACM Trans. Graph, vol.25, no.3, pp.533--540, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2459243</ref_obj_id>
				<ref_obj_pid>2459236</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Yoshida, S. et al., Manipulation of an Emotional Experience by Real-time Deformed Facial Feedback, In Proc of the 4th Augmented Human International Conference, pp.35--42, 2013.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Incendiary reflection: evoking emotion through deformed facial feedback Shigeo Yoshida, Sho Sakurai, 
Takuji Narumi, Tomohiro Tanikawa, and Michitaka Hirose The University of Tokyo {shigeodayo, sho, narumi, 
tani, hirose}@cyber.t.u-tokyo.ac.jp  Figure 1: Left: Incendiary reflection. Center: Deformation of the 
two types of facial expressions: Smiley Face and Sad Face . Right: User s emotional state is influenced 
by the feedback of deformed facial expressions. 1. Introduction Incendiary reflection aims to create 
computer-generated emotion by letting people recognize pseudo-generated facial expressions as changes 
to their own facial expressions (Figure 1, left). With conventional human-computer interactions, the 
manipulation subjective elements of experienced emotion is not possible. Thus, emotional experience may 
not be properly conveyed in such contexts. Emotion is assumed to result from perceiving stimulation from 
the external environment, such as behaviors or situations, and handling this stimulation internally. 
Bodily responses, such as heart rate and facial expressions, have been thought to consequently change 
via an evoked emotion. However, the internal processing mechanisms for evoking an emotion by a relevant 
stimulus have not been fully clarified. Therefore, evoking emotions by reproducing this process through 
engineering techniques is extremely difficult. However, in the field of cognitive science, some researchers 
argue that recognition of changes within bodily responses unconsciously evokes an emotion. William James 
best expressed this phenomenon: We don t laugh because we re happy -we re happy because we laugh. [James. 
1950] For example, the facial feedback hypothesis [Tomkins. 1962] indicates that changes to facial expressions 
affect emotional experience: smiling enhances pleasant feelings while attenuating unpleasant feelings. 
We focus on the effect of facial expressions on evoked emotion. We propose a method for manipulating 
emotional states through feedback of deformed facial expressions in real time. 2. Incendiary reflection 
Incendiary reflection consists of a camera, and a display. The camera is used to capture and track a 
user's face. This system gives feedback of a deformed facial expression by using a mirror-like display. 
We developed a method for deforming a user s face and transforming a user s facial expression in real 
time, using an image-processing technique [Schaefer et al. 2006]. Using this method, we easily and naturally 
deformed the appearance of a user s face. We generated two facial expressions Smiley Face and Sad Face 
which represent the positive-negative affect dimension (Figure 1, center). Furthermore, we conducted 
a user study to evaluate the effectiveness of deformed facial feedback. Our results showed that this 
type of feedback could change emotional states; not only positive affect and negative affect but also 
preference decision [Yoshida et al. 2013]. This suggested that we could artificially manipulate emotional 
states. Unlike a mirror, user's gaze direction and his/her gaze direction shown on the display weren't 
correct exactly in this system configuration. Therefore, this system corrects a user's gaze direction 
by image processing, and displays it. 3. Visitor Experience We observed visitors behavior at the experimental 
exhibition on December 6th to 10th. About 300 people experienced it (Figure 1, right). There were few 
audiences who feel uncomfortable to deformed facial expressions. Moreover, many audiences commented that 
they seemed to be influenced by the changes of their facial expression. Thus we believe that our method 
for deforming facial expressions was natural enough for audiences to recognize the changes to their expressions, 
and artificial changes in facial expression can influence the emotional state. 4. Conclusion We presented 
the Incendiary reflection, a system for evoking emotion through deformed facial feedback. This system 
uses a visual feedback of facial expression that is simple but powerful enough for determining one s 
emotional state. The result of user study and demonstration showed that this system could influence the 
emotional states. We also think that our system could be applied to entertainment technology, such as 
movies, amusement attractions, museum exhibitions, and games, as a direct way to evoke emotion. Moreover, 
we believe this system could be used to manipulate impressions on consumer products. For example, if 
this system was installed in a clothing store fitting room, someone could use this system while trying 
on clothes; s/he might think that certain clothes are more attractive if s/he is experiencing Smiley 
Face feedback. References JAMES, W., The principles of psychology, vol. 2. Dover Publications, New York, 
1950. TOMKINS, S., Affect, imagery, and consciousness: The Positive affects; Springer, New York, vol.1, 
1962. SHAEFER, S. et al., Image deformation using moving least squares, ACM Trans. Graph, vol.25, no.3, 
pp.533-540, 2006. YOSHIDA, S. et al., Manipulation of an Emotional Experience by Real-time Deformed Facial 
Feedback, In Proc of the 4th Augmented Human International Conference, pp.35-42, 2013. Permission to 
make digital or hard copies of part or all of this work for personal or classroom use isgranted without 
fee provided that copies are not made or distributed for commercial advantage and that copies bear this 
notice and the full citation on the first page. Copyrights for third-party components of thiswork must 
be honored. For all other uses, contact the Owner/Author.SIGGRAPH 2013, July 21 25, 2013, Anaheim, California. 
2013 Copyright held by the Owner/Author.ACM 978-1-4503-2261-4/13/07  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2013</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	</section>
	<section>
		<section_id>2504504</section_id>
		<sort_key>450</sort_key>
		<section_seq_no>10</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Complete fabrication]]></section_title>
		<section_page_from>10</section_page_from>
	<article_rec>
		<article_id>2504505</article_id>
		<sort_key>460</sort_key>
		<display_label>Article No.</display_label>
		<display_no>36</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[D-tech me]]></title>
		<subtitle><![CDATA[fabricating 3D figurines with personalized faces]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2504459.2504505</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2504505</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4193672</person_id>
				<author_profile_id><![CDATA[81323496927]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[J.]]></first_name>
				<middle_name><![CDATA[Rafael]]></middle_name>
				<last_name><![CDATA[Tena]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4193673</person_id>
				<author_profile_id><![CDATA[81466643116]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Moshe]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Mahler]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4193674</person_id>
				<author_profile_id><![CDATA[81466647897]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Thabo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Beeler]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4193675</person_id>
				<author_profile_id><![CDATA[82458728957]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Hengchin]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yeh]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4193676</person_id>
				<author_profile_id><![CDATA[81490677861]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Max]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Grosse]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4193677</person_id>
				<author_profile_id><![CDATA[81100545736]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Iain]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Matthews]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2013</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>2504506</article_id>
		<sort_key>470</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>37</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Sketch-based pipeline for mass customization]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2504459.2504506</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2504506</url>
		<abstract>
			<par><![CDATA[<p>We present a novel application workflow to physically produce personalized objects by relying on the sketch-based input metaphor. This is achieved by combining different sketch-based retrieval and modeling aspects and optimizing the output for 3D printing technologies. The workflow starts from a user drawn 2D sketch that is used to query a large 3D shape database. A simple but powerful sketch-based modeling technique is employed to modify the result from the query. Taking into account the limitations of the additive manufacturing process we define a fabrication constraint deformation to produce personalized 3D printed objects.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4193678</person_id>
				<author_profile_id><![CDATA[81309509166]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Kristian]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hildebrand]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[TU Berlin]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[kristian.hildebrand@tu-berlin.de]]></email_address>
			</au>
			<au>
				<person_id>P4193679</person_id>
				<author_profile_id><![CDATA[81508689354]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Marc]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Alexa]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[TU Berlin]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>2185527</ref_obj_id>
				<ref_obj_pid>2185520</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Eitz, M., Richter, R., Boubekeur, T., Hildebrand, K., and Alexa, M. 2012. Sketch-based shape retrieval. <i>ACM Transactions on Graphics (Proceedings SIGGRAPH) 31</i>, 4, 31:1--31:10.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1411916</ref_obj_id>
				<ref_obj_pid>1411846</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Zimmermann, J., Nealen, A., and Alexa, M. 2008. Sketch-based interfaces: Sketching contours. <i>Comput. Graph. 32</i>, 5 (Oct.), 486--499.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Sketch-Based Pipeline for Mass Customization Kristian Hildebrand * Marc Alexa TU Berlin TU Berlin Figure 
1: We present a sketch-based pipeline guiding the process of manufacturing. We start by sketch-based 
retrieval of a user sketch in a large 3D model database. We show the possibilities of sketch-based modeling 
to customize the results intuitively. Customized 3D shapes are manufactured using a 3D printer. Abstract 
We present a novel application work.ow to physically produce per­sonalized objects by relying on the 
sketch-based input metaphor. This is achieved by combining different sketch-based retrieval and modeling 
aspects and optimizing the output for 3D printing tech­nologies. The work.ow starts from a user drawn 
2D sketch that is used to query a large 3D shape database. A simple but powerful sketch-based modeling 
technique is employed to modify the result from the query. Taking into account the limitations of the 
additive manufacturing process we de.ne a fabrication constraint deforma­tion to produce personalized 
3D printed objects. 1 Introduction The evolution from handcrafted personalized manufacturing to in­tuitive 
and easily accessible mass customization is one of the cen­tral challenges for the digital manufacturing 
age. Successful im­plementation of this process is dependent upon the development of high quality interfaces 
for end users. Sketching, because of its abil­ity to act as a common means of visual communication, provides 
an exceptional input tool for searching large image and 3D shape databases, as well as modeling 3D surfaces. 
This work contributes, for the .rst time, a closed work.ow from a user drawn 2D sketch to a 3D printed 
personalized object by taking manufacturing limitations into account. This is achieved by com­bining 
different sketch-based retrieval [Eitz et al. 2012] and model­ ing aspects [Zimmermann et al. 2008], 
enabling the user to control the process with a sketch-based input metaphor The components of our approach 
include: 1. A simple 2D view of a 3D shape is sketched by the user and used as a query image for a large 
3D model database. The retrieval system returns a set of matching 3D models. 2. Users select one model 
of the retrieval set, and modify it using simple strokes along the shape s silhouette. 3. The .nal modi.ed 
object is printed using a standard off-the­shelf 3D printer.  2 Shape Retrieval The presented retrieval 
method is based on a preprocess over the 3D models in the database. Important features of the models 
are *e-mail: kristian.hildebrand@tu-berlin.de extracted to match the sketched input images with the 3D 
shapes. To this end, non-photorealistic rendering algorithms are employed to render the object with selected 
feature lines from several virtual viewpoints. This results in the creation of a set of line renderings 
for each model. An image descriptor based on Gabor .lters, which is speci.cally designed to match the 
sketch-based user input and rendered images is then used to generate a bag-of-features repre­sentation. 
Thereby the user is able to search the shape database quickly and accurately even if the database contains 
millions of 3D models and the sketch is drawn by a non-expert user. 3 Shape Modeling and Fabrication 
The sketch-based modeling process enables the user to modify the model by sketching new silhouettes of 
the mesh. For each stroke drawn by the user, a corresponding feature-preserving deformation is computed. 
This deformation, however, might introduce changes that cannot be manufactured with a 3D printer, such 
as, thin and fragile structures, which produce unstable results,  self-intersections,  meshes that 
exceed the boundaries of the available printing  volume. We avoid these production dif.culties by simulating 
the additive manufacturing process during the modeling process. To this end, a set of slices along the 
printing direction of the deformed shape is generated. These slices are tested whether they violate the 
above constraints, and the deformation is reduced stepwise using linear interpolation until the constraints 
are satis.ed. The demonstration of our system uses a database of several thou­sand 3D models, creating 
a variety of personalized 3D printed ex­amples.  References EIT Z , M., RIC H T E R , R., BO U B EKEUR 
, T., HI LD EB RAN D , K., AND ALEXA , M. 2012. Sketch-based shape retrieval. ACM Transactions on Graphics 
(Proceedings SIGGRAPH) 31, 4, 31:1 31:10. ZIMM ER M A N N, J., NEALE N, A., AND ALEXA , M. 2008. Sketch­based 
interfaces: Sketching contours. Comput. Graph. 32, 5 (Oct.), 486 499. Permission to make digital or hard 
copies of part or all of this work for personal or classroom use isgranted without fee provided that 
copies are not made or distributed for commercial advantage and that copies bear this notice and the 
full citation on the first page. Copyrights for third-party components of thiswork must be honored. For 
all other uses, contact the Owner/Author.SIGGRAPH 2013, July 21 25, 2013, Anaheim, California. 2013 
Copyright held by the Owner/Author.ACM 978-1-4503-2261-4/13/07 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2013</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>2504507</article_id>
		<sort_key>480</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>38</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Isosurface stuffing improved]]></title>
		<subtitle><![CDATA[acute lattices and feature matching]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2504459.2504507</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2504507</url>
		<abstract>
			<par><![CDATA[<p>Tetrahedral mesh generation is an important tool in graphics, both for discretizing dynamics in animation and in providing a domain for geometric algorithms. The search for faster methods which produce higher quality tetrahedra continues. Here we present two modifications of Labelle and Shewchuk's isosurface stuffing [2007], which is exceptionally fast, provides good quality tetrahedra (with strong bounds), and has a simple implementation --- with the caveat it only applies to smooth geometry.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4193680</person_id>
				<author_profile_id><![CDATA[82458899257]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Crawford]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Doran]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of British Columbia]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[cdoran@cs.ubc.ca]]></email_address>
			</au>
			<au>
				<person_id>P4193681</person_id>
				<author_profile_id><![CDATA[82458741257]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Athena]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Chang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of British Columbia]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[athena.me@gmail.com]]></email_address>
			</au>
			<au>
				<person_id>P4193682</person_id>
				<author_profile_id><![CDATA[81100248660]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Robert]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bridson]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of British Columbia]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[rbridson@cs.ubc.ca]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1276448</ref_obj_id>
				<ref_obj_pid>1276377</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Labelle, F., and Shewchuk, J. 2007. Isosurface stuffing: fast tetrahedral meshes with good dihedral angles. <i>ACM Transactions on Graphics (TOG) 26</i>, 3, 57.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Shephard, M. S., and Georges, M. K. 1991. Automatic three-dimensional mesh generation by the Finite Octree technique. <i>Int'l. J. Num. Meth. Eng. 32</i>, 709--749.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[&#220;ng&#246;r, A. 2001. Tiling 3D Euclidean space with acute tetrahedra. In <i>Proc. Canadian Conference on COmputational Geometry</i>, 169--172.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Williams, B. 2008. Fluid surface reconstruction from particles. <i>M.Sc Thesis, University Of British Columbia</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Isosurface Stuf.ng Improved: Acute Lattices and Feature Matching Crawford Doran* Athena Chang Robert 
Bridson University of British Columbia University of British Columbia University of British Columbia 
  Figure 1: A model with sharp edges and corners is meshed with our variant of isosurface stuf.ng, achieving 
excellent dihedral angles (shown in the histogram) while also precisely matching the input surface (sharp 
features emphasized). Tetrahedral mesh generation is an important tool in graphics, both for discretizing 
dynamics in animation and in providing a domain for geometric algorithms. The search for faster methods 
which pro­duce higher quality tetrahedra continues. Here we present two mod­i.cations of Labelle and 
Shewchuk s isosurface stuf.ng [2007], which is exceptionally fast, provides good quality tetrahedra (with 
strong bounds), and has a simple implementation with the caveat it only applies to smooth geometry. 
We posit that isosurface stuf.ng s improved element quality over earlier octree methods (e.g. [Shephard 
and Georges 1991]) is at least in part due to its use of the BCC lattice of tetrahedra as input to the 
cutting and warping stage, rather than post-tetrahedralizing cut and warped octree cells. We take this 
a step further, using an even higher quality lattice of strictly acute tetrahedra, the modi­.ed A-15 
tile identi.ed by Ung¨ ¨ or [2001], and previously used by Williams for isosurface extraction due to 
its favourable valences [2008]. All dihedral angles begin between 53. and 79., affording signi.cantly 
better output elements and/or allowing a greater de­gree of warping before quality is degraded too far 
(see .gure 1 for example dihedral angle statistics). This .exibility allows us to recover some of the 
earlier octree meth­ods ability to capture sharp features such as edges and corners in the input, even 
without re.nement. We added a feature-matching stage to the mesh generator after the usual stuf.ng. For 
each fea­ *e-mail:cdoran@cs.ubc.ca e-mail:athena.me@gmail.com e-mail:rbridson@cs.ubc.ca Figure 2: Without 
feature-matching, the isosurface stuf.ng ap­proach rounds off edges and corners when applied to non-smooth 
inputs. Our variant doesn t yet guarantee every feature is captured, but is effective enough for many 
models of interest in graphics. ture point (e.g. corner or endpoint of a curve) we snap the posi­tion 
of the closest boundary vertex to it, breaking ties to the clos­est feature when con.icted. Then, for 
each feature curve (whose endpoints now have snapped vertices) we .nd a path through the boundary mesh 
that runs as close as possible to the curve using Di­jkstra s algorithm, subject to avoiding excessive 
deformations, and snap the vertices of the path to lie on the feature curve. In dif.cult situations, 
such as high valence features and features underresolved w.r.t. the lattice spacing, not all features 
are matched but such cases are localized and don t detract from mesh quality. Especially with a standard 
smoothing postprocess to optimize vertex locations, this produces extremely high quality meshes for inputs 
of general interest to graphics, yet is still fast and simple to implement. References LA BEL L E , F., 
A N D SH EW C H U K , J. 2007. Isosurface stuf.ng: fast tetrahedral meshes with good dihedral angles. 
ACM Transac­tions on Graphics (TOG) 26, 3, 57. SH E P H AR D , M. S., AN D GE OR G E S , M. K. 1991. 
Automatic three-dimensional mesh generation by the Finite Octree tech­nique. Int l. J. Num. Meth. Eng. 
32, 709 749. ¨ UN G ¨ O R, A. 2001. Tiling 3D Euclidean space with acute tetrahe­dra. In Proc. Canadian 
Conference on COmputational Geome­try, 169 172. WIL L IAM S , B. 2008. Fluid surface reconstruction from 
particles. M.Sc Thesis, University Of British Columbia. Permission to make digital or hard copies of 
part or all of this work for personal or classroom use isgranted without fee provided that copies are 
not made or distributed for commercial advantage and that copies bear this notice and the full citation 
on the first page. Copyrights for third-party components of thiswork must be honored. For all other uses, 
contact the Owner/Author.SIGGRAPH 2013, July 21 25, 2013, Anaheim, California. 2013 Copyright held by 
the Owner/Author.ACM 978-1-4503-2261-4/13/07  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2013</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>2504508</article_id>
		<sort_key>490</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>39</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[constructable]]></title>
		<subtitle><![CDATA[interactive construction of functional mechanical devices]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2504459.2504508</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2504508</url>
		<abstract>
			<par><![CDATA[<p>constructable is an interactive drafting table based on a laser cutter that produces precise physical output in every editing step. Users interact by drafting directly on the workpiece using a hand-held laser pointer. The system tracks the pointer, beautifies its path, and implements its effect by cutting the workpiece using a fast high-powered laser cutter. constructable achieves precision through tool-specific constraints, user-defined sketch lines, and by using the laser cutter itself for all visual feedback, rather than using a screen or projection.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4193683</person_id>
				<author_profile_id><![CDATA[81548921556]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Stefanie]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Mueller]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Hasso-Plattner-Institute, Potsdam, Germany]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4193684</person_id>
				<author_profile_id><![CDATA[81557182156]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Pedro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lopes]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Hasso-Plattner-Institute, Potsdam, Germany]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4193685</person_id>
				<author_profile_id><![CDATA[81470652750]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Konstantin]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kaefer]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Hasso-Plattner-Institute, Potsdam, Germany]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4193686</person_id>
				<author_profile_id><![CDATA[81557092256]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Bastian]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kruck]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Hasso-Plattner-Institute, Potsdam, Germany]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4193687</person_id>
				<author_profile_id><![CDATA[81548004881]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Patrick]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Baudisch]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Hasso-Plattner-Institute, Potsdam, Germany]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>2380191</ref_obj_id>
				<ref_obj_pid>2380116</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Mueller, S., Lopes, P., Baudisch P. 2012. Interactive Construction: Interactive Fabrication of Functional Mechanical Devices. In <i>Proceedings of UIST '12</i>, 599--606.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1935716</ref_obj_id>
				<ref_obj_pid>1935701</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Willis, K. D. D., Xu, C., Wu, J. K., Levin, G., Gross, M. D. 2011. Interactive fabrication: new interfaces for digital fabrication. In <i>Proceedings of TEI '11</i>, 69--72.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 constructable: Interactive Construction of Functional Mechanical Devices Stefanie Mueller Pedro Lopes 
Konstantin Kaefer Bastian Kruck Patrick Baudisch Hasso-Plattner-Institute, Potsdam, Germany Abstract 
constructable is an interactive drafting table based on a laser cut­ter that produces precise physical 
output in every editing step. Users interact by drafting directly on the workpiece using a hand­held 
laser pointer. The system tracks the pointer, beautifies its path, and implements its effect by cutting 
the workpiece using a fast high-powered laser cutter. constructable achieves precision through tool-specific 
constraints, user-defined sketch lines, and by using the laser cutter itself for all visual feedback, 
rather than using a screen or projection. 1 Introduction The common workflow of personal fabrication 
tools places com­puter-aided design software at the front-end. The use of CAD provides three main benefits 
over traditional woodworking tools, such as saws and wood chisels: (1) interacting in software is faster 
than operating a mechanical tool, (2) users can undo mistakes, and (3) construction aids allow for high 
precision. On the flipside, all editing is now done on a computer screen, which removes users from the 
workpiece and prevents users from refining their design interactively. Interactive fabrication systems 
address this by letting users once again work directly with the workpiece [WILLIS, 2011]. A key element 
of interactive fabrication systems is that they provide output after every editing step. This allows 
users to (1) validate their designs earlier and (2) build on the result of earlier steps. However, editing 
now becomes slower, because users have to repeatedly wait for the fabrication engine to finish. Second, 
users lose the precision required to create functional devices. With constructable [MUELLER, LOPES, &#38; 
BAUDISCH, 2012], we attempt to put these qualities back into interactive fabrication, moving it in the 
direction of what we call interactive construction. 2 constructable interactive construction constructable 
is a drafting table that produces physical output in every step. As illustrated by Figure 1, all interaction 
in construc­table takes place on the work-piece, mediated through low-power hand-held laser pointers, 
which we call proxy lasers or simply tools. In the shown example, the user uses the finger joint tool 
to add finger joints between two pieces by crossing the two involved edges. Proxy lasers are too weak 
to affect the work piece. There­fore, constructable tracks proxy laser interactions using a camera mounted 
above, reconstructs the tool s path, transforms it using a constraint set defined by the current tool, 
and implements the effect using its high-powered cutting laser. Since all elements were constructed using 
constraints, we obtain functional mechani­cal devices, such as the gearbox in Figure 1d. Each proxy laser 
features three barrel buttons. While held depressed, the middle button activates the beam, allowing the 
user to determine a starting point with precision. The other two buttons trigger the tool s two modes 
of operation. The cut button allows cutting a tool-specific shape, such as a circle for the circle tool. 
The sketch line button creates the same shape, but etches it as a shallow dashed line into the surface 
of the material. Sketch lines have no direct impact on the mechanics of the workpiece, but instead serve 
as alignment aids that attract sub-sequent cuts. constructable users create objects by using a sequence 
of tools, each of which embodies a different constraint set. Figure 2 shows the sequence for creating 
a simple booklet.  Figure 2: Creating a booklet using the polyline, round and bend tool. 3 Conclusions 
constructable is an interactive system that enables users to create functional mechanical devices. The 
key behind constructable is that it, unlike previous work in interactive fabrication, allows users to 
create precise output using tools-based constraints, sketch lines for alignment, and the laser cutter 
itself for precise output. References MUELLER, S., LOPES, P., BAUDISCH P. 2012. Interactive Construc­tion: 
Interactive Fabrication of Functional Mechanical Devices. In Proceedings of UIST 12, 599 606. WILLIS, 
K.D.D., XU, C., WU, J.K., LEVIN, G., GROSS, M.D. 2011. Interactive fabrication: new interfaces for digital 
fabrication. InProceedings of TEI 11, 69 72. Permission to make digital or hard copies of part or all 
of this work for personal or classroom use isgranted without fee provided that copies are not made or 
distributed for commercial advantage and that copies bear this notice and the full citation on the first 
page. Copyrights for third-party components of thiswork must be honored. For all other uses, contact 
the Owner/Author.SIGGRAPH 2013, July 21 25, 2013, Anaheim, California. 2013 Copyright held by the Owner/Author.ACM 
978-1-4503-2261-4/13/07  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2013</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	</section>
	<section>
		<section_id>2504509</section_id>
		<sort_key>500</sort_key>
		<section_seq_no>11</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Rendering grab bag]]></section_title>
		<section_page_from>11</section_page_from>
	<article_rec>
		<article_id>2504510</article_id>
		<sort_key>510</sort_key>
		<display_label>Article No.</display_label>
		<display_no>40</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[SnugBlur!]]></title>
		<subtitle><![CDATA[Constraint-preserving motion blur]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2504459.2504510</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2504510</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4193688</person_id>
				<author_profile_id><![CDATA[81488649994]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Brandon]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kerr]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4193689</person_id>
				<author_profile_id><![CDATA[82459119057]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[David]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ryu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2013</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>2504511</article_id>
		<sort_key>520</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>41</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Pencil tracing mirage]]></title>
		<subtitle><![CDATA[principle and its evaluation]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2504459.2504511</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2504511</url>
		<abstract>
			<par><![CDATA[<p>To generate realistic representation of the nature scene is one of the most challenging areas in the computer graphics community. Ray tracing[1] is the most well-known technique to synthesize a realistic image. Since ray tracing is the most suitable method for simulating reflection and refraction of the light, it has been used for simulating atmospheric optical phenomena due to reflection and refraction of the light. The mirage is a kind of atmospheric optical phenomenon. Therefore, it is possible to synthesize mirages in 3DCG by simulating or modeling condition of the air. We focus on pencil tracing technique[2] that is an extention of conventional ray tracing technique based on the paraxial approximation theory. Our simple method based on pencil tracing can efficiently generate an appearance of mirage without any complex thermodynamic simulation.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4193690</person_id>
				<author_profile_id><![CDATA[81474687456]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Katsuhisa]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kanazawa]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tokyo Healthcare University, Setagaya-ku Tokyo and Tokyo Denki University, Adachi-ku Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[kanazawa@vcl.im.dendai.ac.jp; k-kanazawa@thcu.ac.jp]]></email_address>
			</au>
			<au>
				<person_id>P4193691</person_id>
				<author_profile_id><![CDATA[82458789357]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Yuma]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sakato]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tokyo Denki University, Adachi-ku Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[sakato@vcl.im.dendai.ac.jp]]></email_address>
			</au>
			<au>
				<person_id>P4193692</person_id>
				<author_profile_id><![CDATA[81319502327]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Tokiichiro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Takahashi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tokyo Denki University/UEI Research, Senju Asahi-cho, Adachi-ku Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[toki@vcl.im.dendai.ac.jp]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>358882</ref_obj_id>
				<ref_obj_pid>358876</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Whitted, T, 1980. An Improved Illumination Model for Shaded Display. Comm. ACM. 23, 6, June 1980, 343--349.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>37408</ref_obj_id>
				<ref_obj_pid>37402</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Shinya, M., Takahashi, T., Naito, S, 1987. Principles and Applications of Pencil Tracing. <i>SIGGRAPH Comput. Graph</i>. 21, 4 (August 1987), 45--54. DOI=10.1145/37402.37408]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Pencil Tracing Mirage: Principle and its Evaluation Katsuhisa Kanazawa Yuma Sakato Tokiichiro Takahashi 
* Tokyo Healthcare University Tokyo Denki University/*UEI Research Setagaya 3 11 3 Setagaya-ku Tokyo 
Senju Asahi-cho 5 Adachi-ku Tokyo k-kanazawa@thcu.ac.jp {kanazawa, sakato, toki}@vcl.im.dendai.ac.jp 
 1. Introduction To generate realistic representation of the nature scene is one of the most challenging 
areas in the computer graphics community. Ray tracing[1] is the most well-known technique to synthesize 
a realistic image. Since ray tracing is the most suitable method for simulating reflection and refraction 
of the light, it has been used for simulating atmospheric optical phenomena due to reflection and refraction 
of the light. The mirage is a kind of atmospheric optical phenomenon. Therefore, it is possible to synthesize 
mirages in 3DCG by simulating or modeling condition of the air. We focus on pencil tracing technique[2] 
that is an extention of conventional ray tracing technique based on the paraxial approximation theory. 
Our simple method based on pencil tracing can efficiently generate an appearance of mirage without any 
complex thermodynamic simulation. 2. Proposed Method We extend the pencil tracing method to synthesize 
mirages. Our idea is to introduce a special kind of object in scene, and integrate a perturbation factor 
into image generation process of the pencil tracing. Our method is useful because it can treat perturbation 
of the air as simply matrix product. 2.1 Pencil Tracing The pencil tracing is a method for accelerating 
image generation of the ray tracing method by tracing a pencil (or bundle) of rays instead of an individual 
ray. Thus, a ray is not traced per a pixel or a sub-pixel, it is traced per domain of pixels. An axial 
ray (center ray of domain) is traced in the same manner as ray tracing, but paraxial rays (surrounding 
rays of the axial ray) are omitted to be traced by using result of tracing their axial ray. Therefore, 
pencil tracing is faster than the conventional ray tracing. In this method, an optical system affecting 
a path of the axial ray is represented as 4x4 matrix called system matrix, and a ray is represented as 
4D vector called ray-vector. Paraxial rays changed by the optical system are approximated by the transformation 
of their ray-vector representation with the system matrix. This is formulated as follows: l'=Tl Here, 
l,l'are ray-vectors of an original ray and of a transformed ray, respectively. Tis a system matrix. 
 2.2 Perturbation Object and Matrix Most of mirages have fluctuated appearance due to perturbation of 
the air. It is one of the most important factors of the mirage appearance. To synthesize this, we introduce 
a perturbation object. This is a special kind of scene object that adds positional and directional noise 
to a ray. To integrate this operation of addition into the form of matrix product, we extend a system 
matrix to 5x5. Extended system matrix and one for the perturbation TPis as follows:  Figure 2. Image 
Generation Time. T'=(T o), TP=(e tE) t t o1o1Here Tis a 4x4 system matrix and T'is its equivalent extended 
matrix, eis a 4x4 identity matrix, tEis noise ray-vector, and ois 4D column zero vector. 3. Results As 
shown in Fig.1, images of square sun mirage are generated by our method. These images are a part of the 
short video (450 frames). Compared to the actual video (see the supplemental video), realistic images 
are successfully generated. All the images of Fig.1 are generated by the following computer environments: 
CPU -Intel Core i7-920 (2.66GHz), system memory -16GB. We measured the image generation time of a variety 
of sizes of images and sizes of domains. Figure 2 shows the generation time of 450 images of square sun 
mirage. Measurement results verified that our method can generate images about 1.5-5 times as fast as 
the conventional method. 4. Conclusions We proposed a pencil tracing technique for synthesizing mirages. 
Our method generated images of square sun mirage fast and efficiently. References [1] WHITTED, T, 1980. 
An Improved Illumination Model for Shaded Display. Comm. ACM. 23, 6, June 1980, 343-349. [2] SHINYA, 
M., TAKAHASHI, T., NAITO, S, 1987. Principles and Applications of Pencil Tracing. SIGGRAPH Comput. Graph. 
21, 4 (August 1987), 45-54. DOI=10.1145/37402.37408 Permission to make digital or hard copies of part 
or all of this work for personal or classroom use isgranted without fee provided that copies are not 
made or distributed for commercial advantage and that copies bear this notice and the full citation on 
the first page. Copyrights for third-party components of thiswork must be honored. For all other uses, 
contact the Owner/Author.SIGGRAPH 2013, July 21 25, 2013, Anaheim, California. 2013 Copyright held by 
the Owner/Author.ACM 978-1-4503-2261-4/13/07  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2013</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>2504512</article_id>
		<sort_key>530</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>42</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Screen-space curvature for production-quality rendering and compositing]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2504459.2504512</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2504512</url>
		<abstract>
			<par><![CDATA[<p>Surface curvature is a measure commonly employed in Computer Graphics for a vast range of applications: for modeling purposes of course, but also to drive texture generation, or to produce exaggerated or stylized shading results (see Figure 1).</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4193693</person_id>
				<author_profile_id><![CDATA[81508705983]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Nicolas]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Mellado]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Inria - Univ. Bordeaux - IOGS - CNRS]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[nicolas.mellado@inria.fr]]></email_address>
			</au>
			<au>
				<person_id>P4193694</person_id>
				<author_profile_id><![CDATA[81314487621]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Pascal]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Barla]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Inria - Univ. Bordeaux - IOGS - CNRS]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4193695</person_id>
				<author_profile_id><![CDATA[81375601661]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Gael]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Guennebaud]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Inria - Univ. Bordeaux - IOGS - CNRS]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4193696</person_id>
				<author_profile_id><![CDATA[81508701608]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Patrick]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Reuter]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Inria - Univ. Bordeaux - IOGS - CNRS]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4193697</person_id>
				<author_profile_id><![CDATA[82458877157]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Gregory]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Duquesne]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Luxology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1944772</ref_obj_id>
				<ref_obj_pid>1944745</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Griffin, W., Wang, Y., Berrios, D., and Olano, M. 2011. GPU curvature estimation on deformable meshes. In <i>ACM I3D '11</i>, 159--166.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2346805</ref_obj_id>
				<ref_obj_pid>2346796</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Mellado, N., Guennebaud, G., Barla, P., Reuter, P., and Schlick, C. 2012. Growing least squares for the analysis of manifolds in scale-space. <i>CGF 31</i>, 5, 1691--1701.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1531331</ref_obj_id>
				<ref_obj_pid>1531326</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Vergne, R., Pacanowski, R., Barla, P., Granier, X., and Schlick, C. 2009. Light warping for enhanced surface depiction. <i>ACM Trans. on Graph. 28</i>, 3.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Screen-Space Curvature for Production-Quality Rendering and Compositing * Nicolas Mellado1 Pascal Barla1 
Gael Guennebaud1 Patrick Reuter1 Gregory Duquesne2 1 -Inria -Univ. Bordeaux -IOGS -CNRS 2 -Luxology 
Figure 1: A subdivision surface character (courtesy of Peter Eriksson) is animated and rendered using 
Modo cThree poses are shown @. in the middle: our screen-space mean curvature (top: concave regions in 
red, convex regions in blue) is used to modulate a basic shading (bottom-left) yielding an exaggerated 
appearance (bottom-right). Right: another shading modulation based on our offset t . 1 Introduction Surface 
curvature is a measure commonly employed in Computer Graphics for a vast range of applications: for modeling 
purposes of course, but also to drive texture generation, or to produce exagger­ated or stylized shading 
results (see Figure 1). Curvature is a differential property expressed at each surface point. When working 
with meshes, it can be ef.ciently computed per ver­tex using GPU-based techniques [Grif.n et al. 2011]. 
However, this approach becomes impractical when dealing with production quality scenes that involve various 
representations (e.g., subdivi­sion surfaces, implicit and procedural geometry, displacement and bump 
maps) and/or deformable objects. This severely limits its use in real-time editing (as in sculpting) 
or for shading animated scenes. 2 Screen-Space Curvature Our approach may be applied either in ray-tracing 
or compositing contexts. For each sample (ray or pixel), we start by collecting neighbor samples in screen-space 
that are closer than a threshold distance t. We also reject neighbors that are farther than a thresh­old 
relative depth d, to avoid collecting samples across occluding contours. We then recover a normal vector 
and a 3D position for each neighbor sample. The resulting local point cloud is .t with an algebraic sphere 
where each sample is weighted by a compactly­supported function parametrized by t. Our .tting is performed 
using the method of Mellado et al. [2012], which has the bene.t of characterizing the resulting sphere 
with geometrically-meaningful parameters. This provides us not only with a mean curvature estimate ., 
but also with an offset t that identi.es local relief and a smooth reconstructed normal vector . . With 
this approach, t controls the scale of the local reconstruction. As shown in Figure 2, our approach produces 
an accurate approxi­mation of object-space mean curvature, which is due to the local na­ture of differential 
properties. There is a notable difference though: farther objects exhibit less details than closer ones 
with the screen­space approach, which provides for automatic simpli.cation and reduced aliasing artifacts. 
An alternative to our screen-space curvature would be to apply 2D derivative .lters on normals [Vergne 
et al. 2009]. However, this *e-mail:nicolas.mellado@inria.fr approach neglects 3D positions, and raises 
issues nearby occluding contours, whereas our .tting remains well-de.ned.  Figure 2: Comparison between 
mean curvature estimated in object-(left) and screen-space (right), shown at 2 different scales. 3D model 
courtesy of AIM@SHAPE Library. 3 Applications We have implemented SSC in Modo c @ on top of its ray-tracing 
en­gine, and in CUDA for post-processing, as shown in the accompa­nying video. An example animation is 
depicted in Figure 1, where we visualize our mean curvature estimate . with a color code, along with 
a simple curvature-based shading result. Our method is ap­plied at each frame with a negligible performance 
overhead com­pared to rendering (for both pixel-and ray-based implementations), and it exhibits natural 
temporal coherence. In future work, we plan to use SSC during sculpting to enhance small (yet important) 
surface details, and for non-photorealistic ren­dering to guide stylization. In both cases, we believe 
that not only . but also t and . will prove to be valuable control parameters. References GRIFFIN, W., 
WANG, Y., BERRIOS, D., AND OLANO, M. 2011. GPU curvature estimation on deformable meshes. In ACM I3D 
11, 159 166. MELLADO, N., GUENNEBAUD, G., BARLA, P., REUTER, P., AND SCHLICK, C. 2012. Growing least 
squares for the anal­ysis of manifolds in scale-space. CGF 31, 5, 1691 1701. VERGNE, R., PACANOWSKI, 
R., BARLA, P., GRANIER, X., AND SCHLICK, C. 2009. Light warping for enhanced surface depic­tion. ACM 
Trans. on Graph. 28, 3. Permission to make digital or hard copies of part or all of this work for personal 
or classroom use isgranted without fee provided that copies are not made or distributed for commercial 
advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party 
components of thiswork must be honored. For all other uses, contact the Owner/Author.SIGGRAPH 2013, July 
21 25, 2013, Anaheim, California. 2013 Copyright held by the Owner/Author.ACM 978-1-4503-2261-4/13/07 
 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2013</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>2504513</article_id>
		<sort_key>540</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>43</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[Discrete texture design using a programmable approach]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2504459.2504513</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2504513</url>
		<abstract>
			<par><![CDATA[<p>Many rendering methods use discrete textures (planar arrangements of vector elements) instead of classic bitmaps. Discrete textures are resolution-insensitive and easily allow to modify the elements' geometry or spatial distribution. However, manually drawing such textures is a time-consuming task. Automating this production is a long-time studied subject. The methods designed for this purpose deal with a difficult tradeoff between the reachable variety of textures and the usability for a community of users.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4193698</person_id>
				<author_profile_id><![CDATA[82458730857]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Hugo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Loi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Inria-LJK (U. Grenoble, CNRS)]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4193699</person_id>
				<author_profile_id><![CDATA[81388592789]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Thomas]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hurtut]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[LIPADE - U. Paris Descartes]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4193700</person_id>
				<author_profile_id><![CDATA[81421593758]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Romain]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Vergne]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Inria-LJK (U. Grenoble, CNRS)]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4193701</person_id>
				<author_profile_id><![CDATA[81466645622]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Jo&#235;lle]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Thollot]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Inria-LJK (U. Grenoble, CNRS)]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1731056</ref_obj_id>
				<ref_obj_pid>1731047</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Grabli, S., Turquin, E., Durand, F., and Sillion, F. X. 2010. Programmable rendering of line drawing from 3D scenes. <i>ACM Trans. Graph. 29</i>, 2, 1--20.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Discrete Texture Design Using a Programmable Approach Hugo Loi1., Thomas Hurtut2, Romain Vergne1, Jo¨ 
elle Thollot1 1Inria-LJK (U. Grenoble, CNRS) 2LIPADE -U. Paris Descartes Figure 1: In our approach each 
texture is the result of a simple program. Based on a set of combinable operators, a large variety of 
distributions can be intuitively described and produced. See the scripts corresponding to these images 
in supplemental material. 1 Introduction Many rendering methods use discrete textures (planar arrangements 
of vector elements) instead of classic bitmaps. Discrete textures are resolution-insensitive and easily 
allow to modify the elements ge­ometry or spatial distribution. However, manually drawing such textures 
is a time-consuming task. Automating this production is a long-time studied subject. The methods designed 
for this purpose deal with a dif.cult tradeoff between the reachable variety of tex­tures and the usability 
for a community of users. Numerous models have been proposed for representing discrete tex­tures. These 
methods can be roughly classi.ed into two categories: local models, that constrain the neighborhood structure 
of elements, and parametric models, that describe the global statistics of ele­ments spatial distribution. 
In order to be usable by artists, these approaches are mainly designed to allow by-example texture syn­thesis. 
However, the variety of textures they can achieve is limited by the dif.culty for parametric models to 
represent local structures, or by the low ability for local models to capture large scale effects. In 
this talk, we show that considering discrete textures as programs allow for a larger variety of textures 
than relying on a given model. These programs combine operators that distribute points, curves or regions 
on the plane. We propose a small set of such operators and we let the user write the texture program 
by combining them, such as in [Grabli et al. 2010] for stylized line drawing. The variety of textures 
reachable by this representation is then bounded by the combinatorial of the proposed operators. In return, 
we adress a different community of users, such as technical directors, due to the required use of a programmable 
interface. A larger audience can be attended by hiding the programming langage with a graph­based visual 
notation. Contributions We propose a small set of atomic operators that distribute points, curves and 
regions on the plane, and we show their ef.ciency for representing a large variety of discrete textures. 
 We factorize the redundant concepts between classic element distribution algorithms such as dart throwing 
and Lloyd s re­laxation, by expressing them with our atomic operator set. This allows us to design a 
large number of variants of these algorithms that share the same factorized representation.  * This 
work has been sponsored by the ANR MAPSTYLE #12-CORD­0025 and ANR SPIRIT #11-JCJC-008-01. 2 Programs 
and Operators We provide a set of operators for manipulating scalars (boolean, integer or real values) 
or elements (points, curves and regions in the 2D plane), and a system drawn from functionnal programming 
for combining these operators as functors. We propose a generic and factorized formulation of greedy 
(without backtrack) element distribution algorithms: while (loop condition) do {-pinning: create a new 
point p -shaping(p): create a region r given p -checking(r): decide to keep r or not} Each of the four 
bold terms above denotes a user-chosen opera­tor. Figure 1a shows a classic anisotropic dart throwing 
obtained with our generic algorithm when the user chooses a maximum iter­ation number as loop condition, 
a random point pinning operator, a hatch shaping operator and a non-overlap checking operator. Figure 
1b shows a result with same choices but a checking operator that keeps regions contained in rectangles 
previously distributed regu­larly with the same algorithm. Figure 1c shows the combination of 1b and 
another distribution whose checking operator keeps the re­gions outside the rectangles. Figure 1d differs 
from Figure 1a only on its shaping operator, which combines a random rotation and a continuous shrink 
of a user-drawn spiral element. We propose a similar formulation with shaping and pinning oper­ators 
for region-based relaxation methods, including Lloyd s algo­rithm. Figure 1e shows a greedily-distributed 
set of regions, moved by our generic relaxation method and .nally reshaped with a trans­formation of 
their Vorono¨i cells. Figure 1f shows a similarly dis­tributed and relaxed set of circle regions. It 
is combined with the same algorithm than for 1a, but with a shaping operator that creates stylized stream 
lines around the distribution of circles. Figure 1g in­terlocks greedy distributions with pinning along 
curves and stream line shaping on the basis of a relaxed distribution of circles. Our operator set is 
highly extensible and combinable, allowing a better adaptation to user s needs and further distribution 
concepts. See our supplementary document for our operators reference and the programs corresponding to 
Figure 1. References GR A B LI , S., TU R QU IN, E., DU RAN D , F., A N D SIL L IO N , F. X. 2010. Programmable 
rendering of line drawing from 3D scenes. ACM Trans. Graph. 29, 2, 1 20. Permission to make digital or 
hard copies of part or all of this work for personal or classroom use isgranted without fee provided 
that copies are not made or distributed for commercial advantage and that copies bear this notice and 
the full citation on the first page. Copyrights for third-party components of thiswork must be honored. 
For all other uses, contact the Owner/Author.SIGGRAPH 2013, July 21 25, 2013, Anaheim, California. 2013 
Copyright held by the Owner/Author.ACM 978-1-4503-2261-4/13/07  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2013</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	</section>
	<section>
		<section_id>2504514</section_id>
		<sort_key>550</sort_key>
		<section_seq_no>12</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[A corps of cores, of course!]]></section_title>
		<section_page_from>12</section_page_from>
	<article_rec>
		<article_id>2504515</article_id>
		<sort_key>560</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>44</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Embree ray tracing kernels for CPUs and the Xeon Phi architecture]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2504459.2504515</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2504515</url>
		<abstract>
			<par><![CDATA[<p>Modern CPUs achieve high computational throughput by implementing increasingly wide SIMD vector units (such as 8-wide AVX or 16-wide SIMD for the Xeon Phi instructions). Achieving optimal performance on these architectures requires leveraging these wide SIMD vector units effectively. We present Embree [Ernst and Woop 2011], an open source ray tracing library developed to show performance-focused graphics programmers how to take full advantage of multiple cores and wide SIMD units in the context of ray tracing. Embree features spatial acceleration structures and traversal algorithms that are optimized for CPUs and the Intel Xeon Phi architecture. In particular, Embree supports hybrid ray packet/single ray traversal algorithms---optimized for both CPUs and Xeon Phi---that are designed to handle both coherent and incoherent workloads efficiently [Benthin et al. 2012]. While a first version of Embree originally focused only on single ray traversal on SSE- or AVX-enabled CPUs, this talk specifically covers the upcoming Embree 2.0 release that explicitly also supports the Xeon Phi architecture, adds support for packet tracing, two level hierarchies, partial scene updates, dynamic content, and virtual intersectors for user defined primitives.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4193702</person_id>
				<author_profile_id><![CDATA[81100618274]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Sven]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Woop]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Intel Labs]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[sven.woop@intel.com]]></email_address>
			</au>
			<au>
				<person_id>P4193703</person_id>
				<author_profile_id><![CDATA[82459031057]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Louis]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Feng]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Intel Corporation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[louis.feng@intel.com]]></email_address>
			</au>
			<au>
				<person_id>P4193704</person_id>
				<author_profile_id><![CDATA[81100041422]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Ingo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wald]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Intel Labs]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[ingo.wald@intel.com]]></email_address>
			</au>
			<au>
				<person_id>P4193705</person_id>
				<author_profile_id><![CDATA[81100370453]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Carsten]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Benthin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Intel Labs]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[carsten.benthin@intel.com]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>2361037</ref_obj_id>
				<ref_obj_pid>2360755</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Benthin, C., Wald, I., Woop, S., Ernst, M., and Mark, W. R. 2012. Combining single and packet-ray tracing for arbitrary ray distributions on the intel mic architecture. <i>IEEE Transactions on Visualization and Computer Graphics 18</i>, 9, 1438--1448.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Ernst, M., and Woop, S., 2011. Embree: Photo-Realistic Ray Tracing Kernels. http://software.intel.com/en-us/articles/embree-photo-realistic-ray-tracing-kernels, June.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Pharr, M., and Mark, W. 2012. ISPC: A SPMD Compiler for high-performance CPU Programming. In <i>Innovative Parallel Computing (InPar), 2012</i>, 1--13.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Embree Ray Tracing Kernels for CPUs and the Xeon Phi Architecture Sven Woop* Louis Feng Ingo Wald Carsten 
Benthin§ Intel Labs Intel Corporation Intel Labs Intel Labs 1 Introduction Modern CPUs achieve high computational 
throughput by imple­menting increasingly wide SIMD vector units (such as 8-wide AVX or 16-wide SIMD for 
the Xeon Phi instructions). Achieving opti­mal performance on these architectures requires leveraging 
these wide SIMD vector units effectively. We present Embree [Ernst and Woop 2011], an open source ray 
tracing library developed to show performance-focused graphics programmers how to take full advantage 
of multiple cores and wide SIMD units in the con­text of ray tracing. Embree features spatial acceleration 
structures and traversal algorithms that are optimized for CPUs and the Intel Xeon Phi architecture. 
In particular, Embree supports hybrid ray packet/single ray traversal algorithms optimized for both CPUs 
and Xeon Phi that are designed to handle both coherent and in­coherent workloads ef.ciently [Benthin 
et al. 2012]. While a .rst version of Embree originally focused only on single ray traversal on SSE-or 
AVX-enabled CPUs, this talk speci.cally covers the upcoming Embree 2.0 release that explicitly also supports 
the Xeon Phi architecture, adds support for packet tracing, two level hierar­chies, partial scene updates, 
dynamic content, and virtual intersec­tors for user de.ned primitives. Xeon Phi is a powerful platform 
for rendering because of its high computational capabilities, fast memory system including large caches, 
and .exible programming model that supports complex algorithms. The Xeon Phi coprocessor provides 60 
cores, each core with a 512-bit wide vector unit and four concurrent hyper­threads. Embree contains highly 
optimized data structure builders and single-ray kernels that achieve best performance on this archi­tecture. 
Porting an existing renderer to Xeon Phi can be as easy as replacing the ray tracing core with these 
kernels and recompiling the system. While this approach of porting to Xeon Phi is straight­forward and 
gives good results, it would not fully leverage compu­tational capabilities of Xeon Phi because the non-traversal 
render­ing code might not make good use of the vector units. To better leverage the wide vector units 
of Xeon Phi throughout the entire renderer, Embree 2.0 additionally provides a tight inte­gration with 
the Intel SPMD Program Compiler (ispc) [Pharr and Mark 2012]. ispc is a Single Program, Multiple Data 
(SPMD) vectorizing language similar to OpenCL. In addition to SPMD, it also supports scalar data and 
control .ow and allows for a tight in­tegration with existing C++ code. Using ispc makes it possible 
to have a single implementation of a rendering system, partly written in C++, that can transparently 
leverage the SSE, AVX, and Xeon Phi instruction sets throughout the entire rendering system without having 
to write any IA-speci.c code at all. While most of the ren­derer is written in high-level ispc code, 
we exploit ispc s tight coupling to C++ code to switch over to special high-performance traversal kernels 
(implemented in C++ and intrinsics) whenever the renderer casts a ray; Embree automatically uses different 
traversal codes specialized for each instruction set, without the user having to care about this. *e-mail:sven.woop@intel.com 
e-mail:louis.feng@intel.com e-mail:ingo.wald@intel.com §e-mail:carsten.benthin@intel.com Embree provides 
only a minimal API to the acceleration structure builders and traversal algorithms, which gives the user 
all the .exi­bilities for implementing their own rendering system. Embree also provides an example renderer 
for CPUs written in C++ and a second implementation written in ispc that performs best on Xeon Phi. We 
have seen many vendors and users successfully integrate the Embree kernels into their own projects. For 
example, DreamWorks Animation has showcased a lighting tool prototype utilizing Em­bree 2.0 at Super 
Computing 2012. Figure 1: Models rendered with the Embree ray tracing kernels using the example path 
tracer. The crown model is provided by Martin Lubich, http://www.loramel.net. 2 Presentation In this 
presentation, we will discuss the design challenges, new fea­tures, and our experiences with the development 
of Embree 2.0. For potential users of Embree, we will show how to use the Embree API from a C++ or ispc 
application. For attendees interested in im­plementation details, we will describe the SIMD friendly 
ray trac­ing algorithms for CPUs and the Xeon Phi hardware architecture including performance numbers 
for non-trivial path tracing work­loads for both architectures. In particular, we will present the single 
ray traversal kernels which achieve best performance on CPUs and our hybrid ray packet/single ray approach 
that gives optimal perfor­mance on Xeon Phi. We will also talk about our impressions on the potential 
and challenges of using Xeon Phi for ray tracing. References BEN TH I N , C., WA L D , I., WO OP, S., 
ER N S T, M., A N D MA R K, W. R. 2012. Combining single and packet-ray tracing for arbi­trary ray distributions 
on the intel mic architecture. IEEE Trans­actions on Visualization and Computer Graphics 18, 9, 1438 
1448. ERNS T, M., A ND WO OP, S., 2011. Embree: Photo-Realistic Ray Tracing Kernels. http://software.intel.com/en­us/articles/embree-photo-realistic-ray-tracing-kernels, 
June. PH AR R , M., A N D MA RK , W. 2012. ISPC: A SPMD Compiler for high-performance CPU Programming. 
In Innovative Parallel Computing (InPar), 2012, 1 13. Permission to make digital or hard copies of part 
or all of this work for personal or classroom use isgranted without fee provided that copies are not 
made or distributed for commercial advantage and that copies bear this notice and the full citation on 
the first page. Copyrights for third-party components of thiswork must be honored. For all other uses, 
contact the Owner/Author.SIGGRAPH 2013, July 21 25, 2013, Anaheim, California. 2013 Copyright held by 
the Owner/Author.ACM 978-1-4503-2261-4/13/07  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2013</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>2504516</article_id>
		<sort_key>570</sort_key>
		<display_label>Article No.</display_label>
		<display_no>45</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Parallel JavaScript]]></title>
		<subtitle><![CDATA[bringing the compute power of multi-core CPUs and GPUs to the world of web graphics]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2504459.2504516</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2504516</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4193706</person_id>
				<author_profile_id><![CDATA[81392604753]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Stephan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Herhut]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4193707</person_id>
				<author_profile_id><![CDATA[81100566849]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Richard]]></first_name>
				<middle_name><![CDATA[L.]]></middle_name>
				<last_name><![CDATA[Hudson]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4193708</person_id>
				<author_profile_id><![CDATA[81100439172]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Tatiana]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Shpeisman]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4193709</person_id>
				<author_profile_id><![CDATA[81338490999]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Jaswanth]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sreeram]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2013</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	</section>
	<section>
		<section_id>2504517</section_id>
		<sort_key>580</sort_key>
		<section_seq_no>13</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Movie sampler]]></section_title>
		<section_page_from>13</section_page_from>
	<article_rec>
		<article_id>2504518</article_id>
		<sort_key>590</sort_key>
		<display_label>Article No.</display_label>
		<display_no>46</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Oz]]></title>
		<subtitle><![CDATA[the great and volumetric]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2504459.2504518</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2504518</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4193710</person_id>
				<author_profile_id><![CDATA[81100426615]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Magnus]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wrenninge]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4193711</person_id>
				<author_profile_id><![CDATA[81100127436]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Chris]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kulla]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4193712</person_id>
				<author_profile_id><![CDATA[82459303657]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Viktor]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lundqvist]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2013</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>2504519</article_id>
		<sort_key>600</sort_key>
		<display_label>Article No.</display_label>
		<display_no>47</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Rendering fur in Life of Pi]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2504459.2504519</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2504519</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4193713</person_id>
				<author_profile_id><![CDATA[81100390217]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Ivan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Neulander]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4193714</person_id>
				<author_profile_id><![CDATA[82458900157]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Toshi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kato]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4193715</person_id>
				<author_profile_id><![CDATA[81502707426]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Kevin]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Beason]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2013</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>2504520</article_id>
		<sort_key>610</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>48</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[BSSRDF importance sampling]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2504459.2504520</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2504520</url>
		<abstract>
			<par><![CDATA[<p>Light propagation within translucent materials can be described by a BSSRDF [Jensen et al. 2001]. The main difficulty in integrating this effect lies in the generation of well-distributed samples on the surface within the support of the rapidly decaying BSSRDF profile. Jensen suggested that these points could be importance sampled but did not provide implementation details. More recently, Walter et al. [2012] and Christensen et al. [2012] proposed other sampling methods which can still suffer from excessive variance.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4193716</person_id>
				<author_profile_id><![CDATA[82459169657]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Alan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[King]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Solid Angle]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4193717</person_id>
				<author_profile_id><![CDATA[81100127436]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Christopher]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kulla]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Sony Pictures Imageworks]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4193718</person_id>
				<author_profile_id><![CDATA[81466647402]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Alejandro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Conty]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Sony Pictures Imageworks]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4193719</person_id>
				<author_profile_id><![CDATA[81488655123]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Marcos]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Fajardo]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Solid Angle]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>2343108</ref_obj_id>
				<ref_obj_pid>2343045</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Christensen, P. H., Harker, G., Shade, J., Schubert, B., and Batali, D. 2012. Multiresolution radiosity caching for global illumination in movies. <i>SIGGRAPH 2012 Talks</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1964951</ref_obj_id>
				<ref_obj_pid>1964921</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[D'Eon, E., and Irving, G. 2011. A quantized-diffusion model for rendering translucent materials. <i>SIGGRAPH 2011</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383319</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Jensen, H. W., Marschner, S. R., Levoy, M., and Hanrahan, P. 2001. A practical model for subsurface light transport. SIGGRAPH 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2185555</ref_obj_id>
				<ref_obj_pid>2185520</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Walter, B., Khungurn, P., and Bala, K. 2012. Bidirectional lightcuts. <i>SIGGRAPH 2012</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 BSSRDF Importance Sampling Alan King Christopher Kulla Alejandro Conty Marcos Fajardo Solid Angle Sony 
Pictures Imageworks Sony Pictures Imageworks Solid Angle (a) (b) (c) (d) Figure 1: Simple BSSRDF without 
(a) and with (b) axis MIS (25 spp). Complex BSSRDF without (c) and with (d) weight MIS (16 spp). Light 
propagation within translucent materials can be described by a BSSRDF [Jensen et al. 2001]. The main 
dif.culty in integrating this effect lies in the generation of well-distributed samples on the surface 
within the support of the rapidly decaying BSSRDF pro.le. Jensen suggested that these points could be 
importance sampled but did not provide implementation details. More recently, Walter et al. [2012] and 
Christensen et al. [2012] proposed other sampling methods which can still suffer from excessive variance. 
 Figure 2: Geometric setup for Equation 1.  Disk based sampling Our approach uses a disk distribution 
of samples which we project against the surface geometry using probe rays. We bound the radial term of 
the BSSRDF with a maximum distance Rm to de.ne a bounding sphere around the shading point P. We cast 
probe rays against this sphere along an axis V perpendicular to the disk and compute the incoming irradiance 
at all intersection points we .nd inside the sphere. The contribution of each point is modulated by: 
Rd (IPhit - PI) 1 pdf disk (r) |V · Nhit| (1) which accounts for the probability of generating the point, 
the change in differential area measure and the BSSRDF itself. We omit the view-dependent terms here 
for simplicity. We choose the pdf to be proportional to Rd, as for .at surfaces this results in perfect 
importance sampling. We have developed equations for both cu­bic and gaussian pro.les. For a single planar 
gaussian de.ned as -1 -r Rd(r) = (2pv)e 2/2v, we use the following normalized pdf and warping equation 
to get r . [0, Rm) from a random sample . . [0, 1): -R2 /2v m (r) = Rd (r) /1 - e pdf disk  -R2 /2v 
r(.) =-2v log1 - .1 - em The angle around the disk is chosen uniformly. We found Rm = T v/12.46 to be 
a reasonable compromise between accuracy and speed. Axis and Weight Multiple Importance Sampling Equation 
1 will lead to high variance in regions of high curvature, or around sharp corners, because the dot product 
in the denominator can be­come arbitrarily small. In fact areas perpendicular to the disk may not be 
hit at all! To compensate for this, we randomly choose the axis V among the three coordinate axes of 
the local shading frame: N, B, T. When computing the contribution of each hit point, we account for the 
probability of having found the same point along the two other axes by multiple importance sampling. 
Since the three axes form an orthonormal basis, the denominator can never become small which ensures 
more uniform variance over the surface. The resulting improvement is shown in Figure 1(b). For realistic 
material modelling, the BSSRDF is usually a weighted sum of multiple gaussians with unique variances 
and weights per color channel [D Eon and Irving 2011]. In skin for example, red light scatters much further 
than in the other channels. We apply importance sampling to combine all gaussian distributions per color 
channel by their weight via Veach s one-sample MIS model, which signi.cantly reduces the noise of complex 
pro.les. This improve­ment is shown in Figure 1(d). As our method is dominated by the cost of tracing 
the probe rays, the increased complexity of the weight calculation does not affect the overall speed. 
Discussion We implemented this technique in the Arnold ren­derer where it has replaced a much more complicated 
implementa­tion based around point clouds. The new algorithm is much easier to control for artists, has 
no memory overhead, is trivially parallelizable, and handles recursive effects without the need for multiple 
passes. Another bene.t is that tight diffusion kernels can be used without penalty, while point cloud 
based methods must ensure a minimum spacing close to the mean-free path or suffer loss of detail. References 
CH R I ST E N S E N, P. H., HAR K E R, G., SHA DE, J., SC HU B E RT, B., A N D BATA L I , D. 2012. Multiresolution 
radiosity caching for global illumination in movies. SIGGRAPH 2012 Talks. D EO N , E., AN D IRV IN G, 
G. 2011. A quantized-diffusion model for rendering translucent materials. SIGGRAPH 2011. JE NS EN, H. 
W., MARSCH N E R, S. R., LE VOY, M., A ND HA N R A-H A N , P. 2001. A practical model for subsurface 
light transport. SIGGRAPH 2001. WALT E R, B . , KHU N G U RN , P. , AN D BA LA, K . 2012. Bidirectional 
lightcuts. SIGGRAPH 2012. Permission to make digital or hard copies of part or all of this work for 
personal or classroom use isgranted without fee provided that copies are not made or distributed for 
commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights 
for third-party components of thiswork must be honored. For all other uses, contact the Owner/Author.SIGGRAPH 
2013, July 21 25, 2013, Anaheim, California. 2013 Copyright held by the Owner/Author.ACM 978-1-4503-2261-4/13/07 
 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2013</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>2504521</article_id>
		<sort_key>620</sort_key>
		<display_label>Article No.</display_label>
		<display_no>49</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[A monster's guide to cheating in GI class]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2504459.2504521</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2504521</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4193720</person_id>
				<author_profile_id><![CDATA[81448593522]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Beth]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Albright]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4193721</person_id>
				<author_profile_id><![CDATA[81430603549]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Byron]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bashforth]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4193722</person_id>
				<author_profile_id><![CDATA[82459013257]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Jonathan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hoffman]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4193723</person_id>
				<author_profile_id><![CDATA[82459002657]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[George]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nguyen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2013</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>2504522</article_id>
		<sort_key>630</sort_key>
		<display_label>Article No.</display_label>
		<display_no>50</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>5</seq_no>
		<title><![CDATA[Creating a Monster]]></title>
		<subtitle><![CDATA[artistic and technical challenges]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2504459.2504522</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2504522</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4193724</person_id>
				<author_profile_id><![CDATA[82458990657]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Michael]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Honsel]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2013</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>2504523</article_id>
		<sort_key>640</sort_key>
		<display_label>Article No.</display_label>
		<display_no>51</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>6</seq_no>
		<title><![CDATA[Crowds at Monsters University]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2504459.2504523</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2504523</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4193725</person_id>
				<author_profile_id><![CDATA[82459098757]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Hemagiri]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Arumugam]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4193726</person_id>
				<author_profile_id><![CDATA[82459155457]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Michael]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Frederickson]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4193727</person_id>
				<author_profile_id><![CDATA[82458730357]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[J.]]></first_name>
				<middle_name><![CDATA[D.]]></middle_name>
				<last_name><![CDATA[Northrup]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2013</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>2504524</article_id>
		<sort_key>650</sort_key>
		<display_label>Article No.</display_label>
		<display_no>52</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>7</seq_no>
		<title><![CDATA[Vegetation on Monsters University]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2504459.2504524</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2504524</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4193728</person_id>
				<author_profile_id><![CDATA[82458887457]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Antony]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Carysforth]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4193729</person_id>
				<author_profile_id><![CDATA[82458916457]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Omar]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Elafifi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4193730</person_id>
				<author_profile_id><![CDATA[82458809357]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Nathan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Fariss]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4193731</person_id>
				<author_profile_id><![CDATA[82459191357]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Henry]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Garcia]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4193732</person_id>
				<author_profile_id><![CDATA[82459243457]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Edgar]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Rodriguez]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4193733</person_id>
				<author_profile_id><![CDATA[82458998457]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Christine]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wagonner]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2013</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>2504525</article_id>
		<sort_key>660</sort_key>
		<display_label>Article No.</display_label>
		<display_no>53</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>8</seq_no>
		<title><![CDATA[Lighting the blue umbrella]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2504459.2504525</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2504525</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4193734</person_id>
				<author_profile_id><![CDATA[82458631957]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Brian]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Boyd]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2013</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>2504526</article_id>
		<sort_key>670</sort_key>
		<display_label>Article No.</display_label>
		<display_no>54</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>9</seq_no>
		<title><![CDATA[Rainy rain raining]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2504459.2504526</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2504526</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4193735</person_id>
				<author_profile_id><![CDATA[82458859557]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Michael]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[O'Brien]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4193736</person_id>
				<author_profile_id><![CDATA[81466648726]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Allen]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hemberger]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4193737</person_id>
				<author_profile_id><![CDATA[82458944057]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Amit]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Baadkar]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4193738</person_id>
				<author_profile_id><![CDATA[81320490563]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Cody]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Harrington]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2013</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	</section>
</content>
</proceeding>
