<?xml version="1.0" encoding="ISO-8859-1"?>
<!DOCTYPE proceeding SYSTEM "proceeding.dtd">
<proceeding ver="6.0" ts="04/10/2010">
<conference_rec>
	<conference_date>
		<start_date>07-14-1980</start_date>
		<end_date>07-18-1980</end_date>
	</conference_date>
	<conference_loc>
		<city><![CDATA[Seattle]]></city>
		<state>Washington</state>
		<country>USA</country>
	</conference_loc>
	<conference_url></conference_url>
</conference_rec>
<series_rec>
	<series_name>
		<series_id>SERIES382</series_id>
		<series_title><![CDATA[International Conference on Computer Graphics and Interactive Techniques]]></series_title>
		<series_vol></series_vol>
	</series_name>
</series_rec>
<proceeding_rec>
	<proc_id>800250</proc_id>
	<acronym>SIGGRAPH '80</acronym>
	<proc_desc>Proceedings of the 7th annual conference</proc_desc>
	<conference_number></conference_number>
	<proc_class>conference</proc_class>
	<proc_title>Computer graphics and interactive techniques</proc_title>
	<proc_subtitle></proc_subtitle>
	<proc_volume_no></proc_volume_no>
	<isbn>0-89791-021-4</isbn>
	<issn></issn>
	<eissn></eissn>
	<copyright_year>1980</copyright_year>
	<publication_date>07-14-1980</publication_date>
	<pages>336</pages>
	<plus_pages></plus_pages>
	<price><![CDATA[]]></price>
	<other_source>ACM Order Number: 428800</other_source>
	<publisher>
		<publisher_id>PUB27</publisher_id>
		<publisher_code>ACMNY</publisher_code>
		<publisher_name>ACM</publisher_name>
		<publisher_address>2 Penn Plaza, Suite 701</publisher_address>
		<publisher_city>New York</publisher_city>
		<publisher_state>NY</publisher_state>
		<publisher_country>USA</publisher_country>
		<publisher_zip_code>10121-0701</publisher_zip_code>
		<publisher_contact>Bernard Rous</publisher_contact>
		<publisher_phone>212 869-7440</publisher_phone>
		<publisher_isbn_prefix></publisher_isbn_prefix>
		<publisher_url>www.acm.org/publications</publisher_url>
	</publisher>
	<sponsor_rec>
		<sponsor>
			<sponsor_id>SP932</sponsor_id>
			<sponsor_name>ACM Special Interest Group on Computer Graphics and Interactive Techniques</sponsor_name>
			<sponsor_abbr>SIGGRAPH</sponsor_abbr>
		</sponsor>
	</sponsor_rec>
	<categories>
		<primary_category>
			<cat_node>I.3</cat_node>
			<descriptor/>
			<type/>
		</primary_category>
	</categories>
	<ccs2012>
		<concept>
			<concept_id>0.10010147.10010371</concept_id>
			<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
			<concept_significance>100</concept_significance>
		</concept>
		<concept>
			<concept_id>0.10010147.10010371</concept_id>
			<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
			<concept_significance>500</concept_significance>
		</concept>
	</ccs2012>
	<general_terms>
		<gt>Theory</gt>
	</general_terms>
	<chair_editor>
		<ch_ed>
			<person_id>PP40041667</person_id>
			<author_profile_id><![CDATA[81100411412]]></author_profile_id>
			<orcid_id></orcid_id>
			<seq_no>1</seq_no>
			<first_name><![CDATA[James]]></first_name>
			<middle_name><![CDATA[J.]]></middle_name>
			<last_name><![CDATA[Thomas]]></last_name>
			<suffix><![CDATA[]]></suffix>
			<affiliation><![CDATA[]]></affiliation>
			<role><![CDATA[Chairman]]></role>
			<email_address><![CDATA[]]></email_address>
		</ch_ed>
		<ch_ed>
			<person_id>PP39093322</person_id>
			<author_profile_id><![CDATA[81100164786]]></author_profile_id>
			<orcid_id></orcid_id>
			<seq_no>2</seq_no>
			<first_name><![CDATA[Robert]]></first_name>
			<middle_name><![CDATA[A.]]></middle_name>
			<last_name><![CDATA[Ellis]]></last_name>
			<suffix><![CDATA[]]></suffix>
			<affiliation><![CDATA[]]></affiliation>
			<role><![CDATA[Chairman]]></role>
			<email_address><![CDATA[]]></email_address>
		</ch_ed>
		<ch_ed>
			<person_id>PP39099457</person_id>
			<author_profile_id><![CDATA[81332510007]]></author_profile_id>
			<orcid_id></orcid_id>
			<seq_no>3</seq_no>
			<first_name><![CDATA[Harvey]]></first_name>
			<middle_name><![CDATA[Z.]]></middle_name>
			<last_name><![CDATA[Kriloff]]></last_name>
			<suffix><![CDATA[]]></suffix>
			<affiliation><![CDATA[]]></affiliation>
			<role><![CDATA[Chairman]]></role>
			<email_address><![CDATA[]]></email_address>
		</ch_ed>
	</chair_editor>
	<ccc>
		<copyright_holder>
			<copyright_holder_name>ACM</copyright_holder_name>
			<copyright_holder_year>1980</copyright_holder_year>
		</copyright_holder>
	</ccc>
</proceeding_rec>
<content>
	<article_rec>
		<article_id>807460</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-1980</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Special session on effective information presentation techniques]]></title>
		<page_from>1</page_from>
		<doi_number>10.1145/800250.807460</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=807460</url>
		<abstract>
			<par><![CDATA[<p>Are computer graphics displays as effective as they should be? Are they as appealing as they could be? How can displays of information acquire more impact? How can they be made more memorable. Questions like these are being asked frequently in the computer graphics community.</p> <p>Because of advanced display equipment, lower costs, effective communication networks, and increasingly sophisticated user groups, computer graphics is entering a new stage in its development. Visualized information will reach more people in this decade than ever before. Computer graphics displays will become centrally involved with concept formation and decision making on a mass scale. For this reason computer graphics systems should incorporate more successfully the knowledge of visual communication professionals who have developed valid, effective principles for conveying facts and concepts through typography, color, symbols, motion, photography, etc.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP39049611</person_id>
				<author_profile_id><![CDATA[81100583778]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Aaron]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Marcus]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Meta-Graphics Design Studio, Berkeley, california]]></affiliation>
				<role><![CDATA[Chairman]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P332421</person_id>
				<author_profile_id><![CDATA[81100593844]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Mervyn]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kurlansky]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P333889</person_id>
				<author_profile_id><![CDATA[81100583396]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Susan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Marcus]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P331158</person_id>
				<author_profile_id><![CDATA[81100095514]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Jack]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Reineck]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Reineck and Reineck Design Studio, San Francisco, California]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P330438</person_id>
				<author_profile_id><![CDATA[81100095517]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Gay]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Reineck]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 SPECIAL SESSION ON EFFECTIVE INFORMATION PRESENTATION TECHNIQUES Chairman Aaron Marcus Department 
of Computer Science and Applied Mathematics Lawrence Berkeley Laboratory Berkeley, California 94720 
 Susan Marcus Jack and Gay Reineck Mervyn Kurlansky Pentagram Design Studio Meta-Graphics Design Studio 
Reineck and Reineck Design Studio London, England Berkeley, california San Francisco, California Are 
computer graphics displays as effective as they should be? Are they as appealing as they could be? How 
can displays of information acquire more impact? How can they be made more memorable. Questions like 
these are being asked frequently in the computer graphics community. Because of advanced display equiptment, 
lower costs, effective communication networks, and in- creasingly sophisticated user groups, computer 
graphics is entering a new stage in its development. Visualized information will reach more people in 
this decade than ever before. Computer graphics dis- plays will become centrally involved with concept 
formation and decision making on a mass scale. For this reason computer graphics systems should incor- 
porate more successfully the knowledge of visual communication professionals who have developed valid, 
effective principles for conveying facts and concepts through typography, color, symbols, motion, photography, 
etc. Graphic designers who are involved in the creation of symbols, tables, charts, maps, and diagrams 
can be useful in this collaboration. They have much to say that is relevant to current computer graphics 
displays that are often characterized by over-use of color, cluttered composition, and crude typographic 
hierarchies. By paying more attention to basic graphic design principles, more effective communica- tion 
is possible. This panel session will enable graphic designers to address the computer graphics community 
with ideas, processes, and issues of informational graphic design. They will discuss charts, maps, and 
diagrams in a variety of contexts including computer graphics media but also their counterparts in conventional 
display media. The presentations are intended to set the stage for future dialogue and interaction between 
professionals from the world of computer graphics and graphic design. Aaron Marcus: Visualizing Global 
Interdependencies An experimental informational graphics narrative will be shown and discussed. This 
prototype sequence represents a signage system for a conceptual landscape. Its symbols, maps, and diagrams 
are in- tended to communicate complex information about global energy interdependence. By commenting 
on its development, the graphic design process will be clearer to computer graphics professionals. Jack 
and Gay Reineck: Map Making Map making, as a form of symbolization, is particu- larly effective in transmitting 
complex information about the environment. Historically, cartographers have been concerned primarily 
with the technical aspects of making maps; obtaining geographic data, then mechanically and stylistically 
representing the physical world. Remarkably little thought has ever been given to the effectiveness of 
the map and its ability to communicate clearly. Today, special purpose maps are required for communicating 
specific information about people, places, events, and services, particularily in the fields of tourism 
and public transit. The production of an effective special purpose map involves a complex design process 
and requires a thorough understanding of the graphic elements which promote clear visual communication. 
The presentation will outline the design process, draw comparisons between existing maps and describe 
some of the techniques and methods used to produce special purpose maps. Susan Marcus: On Diagrams and 
Complexity Numerous professions face a critical need for a viable, non-verbal means for distilling significance 
from expansive, complex, technical systems that contain dynamic, cyclical, and hierarchical struc- tures. 
Bringing the principles of design to the world of scientific inquiry and bridging communica- tion gaps 
between disparate yet interdependent groups through a visual medium defines a major area for interaction 
between computer graphics and graphic design. An experimental research effort by a mathematician, a graphic 
designer, and two com- puter scientists will be discussed. The team is translating the mathematical mechanisms 
of three global simulation computer models into simple elegant diagrammatic form. The project is a use- 
ful heuristic for computer graphics professionals seeking to create more effective displays of complex 
systems and processes. Mervyn Kurlansky: Viewdata Graphics In designing frames of information for Prestel, 
the British viewdata system, some guidelines emerge for readibility, color use, organization, and routing 
that may be useful for a wider context of computer graphics displays. the title of the publication and 
its date appear, and notice Pe~mlssion to copy without fee all or part of this material is is given 
that copying is by permission of the Association for granted provided that the copies are not made or 
distributed  for direct comznerclal advantaEe, the ACM copyrIEht notice and &#38;#169;]980 ACM 0-8979]-02]-4/80/0700-000] 
$00.75  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1980</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>807461</article_id>
		<sort_key>2</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-1980</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[The theory, design, implementation and evaluation of a three-dimensional surface detection algorithm]]></title>
		<page_from>2</page_from>
		<page_to>9</page_to>
		<doi_number>10.1145/800250.807461</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=807461</url>
		<abstract>
			<par><![CDATA[<p>In many three-dimensional imaging applications the three-dimensional scene is represented by a three-dimensional array of volume elements, or voxels for short. A subset Q of the voxels is specified by some property. The objects in the scene are then defined as subsets of Q formed by voxels which are &#8220;connected&#8221; in some appropriate sense. It is often of interest to detect and display the surface of an object in the scene, specified say by one of the voxels in it.</p> <p>In this paper, the problem of surface detection is translated into a problem of traversal of a directed graph, G. The nodes of G correspond to faces separating voxels in Q from voxels not in Q. It has been proven that connected subgraphs of G correspond to surfaces of connected components of Q (i.e., of objects in the scene). Further properties of the directed graph have been proven, which allow us to keep the number of marked nodes (needed to avoid loops in the graph traversal) to a small fraction of the total number of visited nodes.</p> <p>This boundary detection algorithm has been implemented. We discuss the interaction between the underlying mathematical theory and the design of the working software. We illustrate the software on some clinical studies in which the input is computed tomographic (CT) data and the output is dynamically rotating three-dimensional displays of isolated organs. Even though the medical application leads to very large scale problems, our theory and design allows us to use our method routinely on the minicomputer of a CT scanner.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Visible line/surface algorithms</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>G.2.2</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372.10010377</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering->Visibility</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003624.10003633</concept_id>
				<concept_desc>CCS->Mathematics of computing->Discrete mathematics->Graph theory</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP39049692</person_id>
				<author_profile_id><![CDATA[81100586176]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Ehud]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Artzy]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Amiad, 12335, Israel]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP43128668</person_id>
				<author_profile_id><![CDATA[81342494824]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Gideon]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Frieder]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Medical Image Processing Group, Department of Computer Science, State University of New York at Buffalo, 4226 Ridge Lea Road, Amherst, New York]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP43125635</person_id>
				<author_profile_id><![CDATA[81339504502]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Gabor]]></first_name>
				<middle_name><![CDATA[T.]]></middle_name>
				<last_name><![CDATA[Herman]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Medical Image Processing Group, Department of Computer Science, State University of New York at Buffalo, 4226 Ridge Lea Road, Amherst, New York]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Artzy, E. Display of three-dimensional information in computed tomography. Comm. Grph. and Image Proc. 9, (Feb. 1979), 196-198.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>988465</ref_obj_id>
				<ref_obj_pid>988460</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Artzy, E. and Herman, G.T., Boundary detection in 3-dimensions with a medical application. Comp. Graph., to appear.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Coin, CG., Herman, G.T., Huck-Folliss, A., Jacobson, S., Pernink, M., and Reranen, V. Computed tomography of disc disease. Scientific exhibit at the Radiological Soc. of North Amer., Atlanta, Ga., Nov. 1979.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>359846</ref_obj_id>
				<ref_obj_pid>359842</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Fuchs, H., Kedem, Z.M., and Uselton, S.P. Optimal surface reconstruction from planar contours. Comm. ACM 20, 10 (Oct. 1977), 693-702. (Also presented at SIGGRAPH'77.)]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Gordon, R., Herman, G.T. and Johnson, S.A. Image reconstruction from projections. Sci. Amer. 233 (Oct. 1975), 56-68.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Greenleaf, J.F., Tu, J.S., and Wood, E. H. Computer generated three-dimensional oscilloscopic images and associated techniques for display and study of the spatial distribution of pulmonary blood flow. IEEE Tran. Nucl. Sci. NS-17, (June 1970), 353-359.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Harary, F. Graph Theory. Addison-Wesley Publishing Company, (1972).]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Herman, G.T. and Liu, H.K. Three-dimensional display of human organs from computed tomograms. Comp. Graph. and Image Processing 9, (Jan. 1979), 1-21.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_obj_id>260999</ref_obj_id>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Knuth, D.E. The Art of Computer Programming Volume 1 Fundamental Algorithms. Addison-Wesley Company, Reading, Mass., (1968).]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Liu, H.K. Two and three dimensional boundary detection. Comp. Graph. and Image Processing 6 (Apr. 1977), 123-134.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807435</ref_obj_id>
				<ref_obj_pid>800249</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Rhodes, M.L. An algorithmic approach to controlling search in three-dimensional image data. SIGGRAPH'79 Proceedings, Chicago, Ill. (Aug. 1979), 134-142.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_obj_id>563318</ref_obj_id>
				<ref_obj_pid>563274</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Robb, R.A., Ritman, E.L., Greenleaf, J.F., Sturm, R.E., Herman, G.T., Chevalier, P.A., Liu, H.K., and Wood, E.H., Quantitative imaging of dynamic structure and functions of the heart, lungs and circulation by computerized reconstruction and subtraction techniques. Proceedings of the 3rd Annual Conf. on Comp. Grph., Interactive Techniques and Image Processing (SIGGRAPH'76), Philadelphia, Pa. (July 1976), 246-256.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_obj_id>321570</ref_obj_id>
				<ref_obj_pid>321556</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Rosenfeld, A. connectivity in digital pictures. J. Assoc, Computing Mach. 17 (Jan. 1970), 146-160.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_obj_id>578095</ref_obj_id>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Rosenfeld, A., and Kak, A.C. Digital Picture Processing. Academic Press, New York, N.Y., (1976).]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807390</ref_obj_id>
				<ref_obj_pid>800248</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Sunguroff, A., and Greenberg, D. Computer generated images for medical application, SIGGRAPH'78 Proceedings, Atlanta, GA. (Aug. 1978), 196-202.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 The Theory, Design, Implementation and Evaluation of a Three-Dimensional Surface Detection Algorithm 
 by Ehud Artzy , Gideon Frieder and Gabor T. Herman Medical Image Processing Group, Department of Computer 
Science State University of New York at Buffalo 4226 Ridge Lea Road, Amherst, New York I~226 USA Present 
address: Amiad, 12335, Israel CR Categories: 8.2, 3.34, 5.32 ABSTRACT In many three-dimensional imaging 
traversal) to a small fraction of the to- applications the three-dimensional scene tal number of visited 
nodes. is represented by a three-dimensional array This boundary detection algorithm has of volume elements, 
or voxels for short. been implemented. We discuss the inter- A subset Q of the voxels is specified by 
action between the underlying mathemati- some property. The objects in the scene cal theory and the design 
of the working are then defined as subsets of Q formed by software. We illustrate the software on voxels 
which are "connected" in some some clinical studies in which the input appropriate sense. It is often 
of interest is computed tomographic (CT) data and the to detect and display the surface of an output 
is dynamically rotating three-di- object in the scene, specified say by one mensional displays of isolated 
organs. of the voxels in it. Even though the medical application leads In this paper, the problem of 
to very large scale problems, our theory surface detection is translated into a and design allows us 
to use our method problem of traversal of a directed routinely on the minicomputer of a CT graph, G. 
The nodes of G correspond scanner. to faces separating voxels in Q from voxels not in Q. It has been 
proven that I. ~ntroduction connected subgraphs o£ ~ correspond to surfaces of connected components In 
many three-dimensional imaging of Q (i.e., of objects in the scene). applications the three-dimensional 
scene Further properties of the directed is represented by a three-dimensional ar- graph have been proven, 
which allow us ray of volume elements, or voxels for to keep the number of marked noaes short. A subset 
Q of the voxels is speci- (needed to avoid loops in the graph fied by some property. The objects in 
the scene are then defined as subsets of Q formed by voxels which are "connected" in some appropriate 
sense. It is often of Permission to copy without fee all or part of this material is granted provided 
that the copies are not made or distributed interest to detect and display the surface for direct co~maercial 
advantage, the A(24 copyright notice and of an object-~-~e scene, specified say the title of the publication 
and its date appear, and notice by one of the voxels in it. is given that copying is by permission 
of the Association for Computing Machinery. To copy otherwise, or to republish, The current interest 
in this topic requires a fee and/or specific permission. has been reflected by articles presented at 
the recent SIGGRAPH meetings [4, 11, 12, 15] and elsewhere. 01980 ACM 0-89791-021-4/80/0700-0002 $00.75 
Here we present an entirely new approach (based on the ideas of the pre- liminary report by Artzy and 
Herman [2]). The problem of surface detection is trans- lated into a problem of traversal of a directed 
graph, G. The nodes of G corres- pond to faces separating voxels in Q from voxels not in Q. We have proven 
that con- nected subgraphs of G correspond to sur- faces of connected components of Q (i.e., of objects 
in the scene). This proves what was an unproven conjecture in [2]. We have proven further properties 
of the directed graph, which allow us to keep the number of marked nodes (needed to avoid a loop in 
the graph traversal) to a small fraction of the total number of visited nodes. Furthermore, the output 
of the surface detection algorithm is appropriate for surface display. This boundary detection algorithm 
has been implemented. We discuss the inter- action between the underlying mathematical theory and the 
design of the working soft- ware. We illustrate the software on some clinical studies. In these studies 
the in- put is computed tomographic (CT) data [5] and the output is dynamically rotating three-dimensional 
displays of isolated organs. Even though the medical application leads to very large scale problems, 
our theory and design allows us to use our method routinely on the minicomputer of a CT scanner (an 
Eclipse S/200). One of our illustrations is a display of the bony structures in the region of the orbits 
 and sinuses, with data taken by a high resolution CT scanner. The total number of voxels in the input 
three-dimensional scene in this case is 1,198,080, the number of voxels within-the organ of interest 
is 153,547 and the nubmer of faces enclosing the organ (the number of nodes of the connected subgraph) 
is 112,130. (Nevertheless, the number of marked nodes never exceeds 1,744; i.e., it is less than!1.6~ 
of the total number of visited nodes.) The low ratio of surface area to volume is an indication of 
the extreme intricacy of the detected surface. It is our claim that previously reported computer programs 
would either fail to detect such complex structures or would do so at a computer cost which is at 
least an order of magnitude greater than required by our software. In the next section we give an 
outline of our approach and discuss its implementation, emphasizing those theo- rectical properties 
which allow execution of what initially appear to be extremely large problems on a minicomputer in 
a surprisingly short time. The performance of the algorithm on actual clinical ex- amples is illustrated 
in Section 3. We state our conclusions in Section 4. Due to space restrictions of the SIGRAPH proceedings, 
we had to omit details of our mathematical definitions and theorems. For these details we refer the 
reader to Technical Report No. MIPG43, published by the Medical'Image Processing Group, Depart- ment 
of Computer Science, State University of New York at Buffalo. (The report has the same authors and title 
as the present article.)-We now summarize the three sections of:the technical report which had to be 
omitted in the proceedings. In the first omitted section we discuss two- dimensional notions which are 
analogous to some of the three-dimensional concepts of our theory. This is done to provide for an easier 
understanding of the reasons behind our three-dimensional definitions; either by showing that they are 
analogous to well-accepted definitions in the two- dimensional case, or by pointing out the necessity 
for their essentially three- dimensional nature when there are no two- dimensional analogs. The second 
omitted section is devoted to a precise discussion of our three-dimensional theory based on the directed 
graph model. The three-dimen- sional boundary detection algorithm is restated (in terms of this precise 
model) and its correct behavior is established in the last of the omitted sections. Two final comments 
in this intro- duction. First, in the terminology of some workers, what we call "boundary detec- tion" 
wouldbe called "boundary tracking"; they would consider that the boundary is "detected" by the initial 
thresholding. Second, it is worth pointing out at this early stage that our basic contribution is the 
extension to the three-dimensional case of the "hand on the wall strategy" which has been demonstrated 
to work well in the plane [13, 14]. While this analogy is significant, there are some essentially three-dimensional 
problems to be solved; as it is carefully illustrated in the technical report referred to above. 2. 
Informal Description and Implementational Considerations The following discussion is inten- ded to 
familiarize the reader with the basic problem. In this discussion we sacrifice mathematical rigor for 
simple presentation. For rigorous definitions, including a careful elaboration of the meaning of boundaries, 
see the technical report referred to in Section I. As mentioned in the introduction, we start with the 
assumption that a sub- set Q of voxels in the three-dimensional scene have been specified and we are 
interested in detecting the boundary of an object in the scene. The object is defined as the set of all 
voxels in Q, "connected" in some appropriate sense to a given voxel in Q. An example is shown in Figure 
I. On the top we show a single slice (size 180x128) of a three-dimensional scene (size 180x128x52), 
which will be discussed in more detail in Section 3. The greyness associated with each voxel represents 
a property (similar to density) of tissue in a human head at a corresponding position. The set of voxels 
Q containing "bone" can be specified by selecting an appropriate grey value and thresholding the scene. 
This has been done, and £he result of the process for the single slice in the scene is shown at the bottom 
of Figure I. The following is our major moti- vation for seeking a three-dimensional boundary detection 
algorithm different from those published in prior literature: looking at a single slice of a complex 
scene, such as shown in Figure I, it is impossible to tell which voxels in Q belong to the same object. 
Object con- nectivity is an essentially three-dimen- sional property. If the scene is simple, it is often 
possible to identify (based on a priori knowledge) voxels in a slice which belong to a certain object. 
Thus two-dimensional boundaries can be drawn and then connected to form the three- dimensional surface. 
Such an approach has been taken, for example, in [4,6,15]. Our claim is that for some applications this 
approach is infeasible and truly three- dimensional surface detection is called for. While such algorithms 
have also been reported in the literature [10,11], they tend to be rather time-consuming for scenes as 
complex as the one illustrated in Figure I. We claim that the algorithm described below is substantially 
faster than previously reported truly three- dimensional boundary detection algorithms. We now state 
the idea on which our algorithm is based. Consider a single voxel (see Figure 2a). For any one of its 
faces, we select two out of the four edges and call them the outgoing edges. The other two are called 
the incoming edges. When done properly, this procedure covers the six faces and the twelve edges of a 
cube without clashes; i.e., each edge will be an outgoing edge of one face and an incoming edge of an 
adjacent face. If each face is viewed as a node and each edge as an arc, the boundary will be represented 
by a digraph (directed graph; see, e.g. [7]) in which each node has indegree two (corresponding to the 
incoming edges) and an outdegree two (corresponding to the outgoing edges); see Figure 2b. Objects in 
a scene consist of voxels; surfaces of objects consist of faces of voxels. What we have shown is teat 
the notion of a digraph whose nodes are faces and whose arcs are edges can be generalized from a single 
voxel to arbitrary scenes in such a way that a "connected" part of the surface of an object gives rise 
to a connected component of the digraph and vice-versa. Hence, finding a boundary surface specified by 
one of its elements corresponds to traversing a connected component of a digraph. Our algorithm (and 
its implementation) makes essential use of the fact that every node in the digraph has outdegree two 
and in- degree two. First of all, every connected com- ponent of such a digraph is strongly con- nected. 
This combined with the fact that every node has outdegree two, implies that for every node there is a 
binary tree rooted at the node which is a subgraph of the digraph and spans the connected com- ponent 
containing the given node. This implies that standard procedures for tra- versing binary spanning trees 
(see, e.g., [9]) can be applied to our problem. How- ever, there is a practical difficulty in doing 
so, caused by the size of our pro- blem. In the introduction we gave an example (for further details 
see Section 3), where the number of nodes in the con- nected component (equivalently, in the binary spanning 
tree) is over 100,000. For the following discussion the reader should bear in mind that while a binary 
spanning tree exists, the information that is stored is the original graph. In practice, the spanning 
tree gets specified during the graph traversal process. The problem is that in general there is no restriction 
on the maximum indegree of a node in a strongly connected digraph in which every node has outdegree two. 
Hence.a general binary spanning tree tra- verasal algorithm has to "mark" all the nodes which it has 
visited. In order to avoid infinite loops, whenever an arc leads from an already visited node to a "new" 
node, the "new" node has to be checked against the set of marked nodes. Therefore, it is important that 
the number of those nodes be kept low. This we do by using the fact that our digraph has both indegree 
and outdegree two and by traver- slng the digraph direclty. We give now a simple example, which demonstrates 
both our procedures and our techniques. Figure 3 shows a body made up from three voxels, where each 
face is identified by a number. Marking each edge on the body as an incoming or outgoing edge, we can 
create a boundary digraph (see Figure 4) in which each node represents a face and each arc represents 
a connection between a face and one of its neighbors. Note that each node of the digraph is both of indegree 
and of outdegree two. We make use of this fact in the following way. When a node is visi- ted, we check 
it against the list of marked nodes. If it is not on the list we mark it (add it to the list). If it 
is on the list, we remove it, since we know it will be n~ver visited again. This way we keep the list 
of marked nodes to a computationally acceptable size. For the example stated in the introduction, the 
list of marked nodes is never longer (and is usually much shorter) than 1744, which is less than 1.6~ 
of the total number of nodes in the connected subgraph. This has been found typical for our application 
area which is the detection of organ sur- faces from CT data. The reader now is encouraged to follow 
the boundary detection algorithm Step-by-step using the following table. We use the FIFO queue discipline 
to select the next node from which the tlaversal should continue. Node Queue of Considered nodes to 
be List of marked nodes Output list considered {I} {1,1} {I} I {6,2} {1,1,6,2} {I,6,2} 6 {2,5,8} {I,1,6,2,5,8} 
{1,6,2,5,8} 2 {5,8,7,10} {1,1,6,2,5,8,7,10} {1,6,2,5,8,7,10} 5 {8,7,10,4} {1,6,2,5,8,7,10,4} {1,6,2,5,8,7,10,4} 
8 {7,10,4} {1,6,2,8,7,1~} {I,6,2,5,8,7,10,4} 7 {I0",4,9} {1,2,8,7,10,9} {1,6,2,5,8,7,10,4 ,~} 10 {4,9) 
{1,2,8,10} {1,6,2,5,8,7,10,4 ,9} 4 {9,13} {2,8,10,13} {1,6,2,5,8,7,10,4 ,9,13} 9 {13,15} {2,10,13,12} 
{I,6,2,5,8,7,10,4 ,9,13,12} 13 {12,3,14} {2,10,13,12,3,14} {1,6,2,5,8,7,10,4,9,13 ,12,3,14} 12 {3,14} 
{2,10,13,12,3} {1,6,2,5,8,7,10,4,9,13 ,12,3,14} 3 {14,11} {10,12,3,11} {1,6,2,5,8,7,10,4,9,13 ,12,3,14,11} 
14 {11} {10,12} {1,6,2,5,8,7,10,4,9,13 ,12,3,14,11} 11 ~ {1,6,2,5,8,7,10,4,9,13 ,12,3,14,11} Note that 
in the traversal pro- cedures, we started at an arbitrary node and followed one outgoing edge, remem- 
bering the "neglected" node by queueing it, and noting the traversed nodes by marking them (storing them 
on the list). The binary spannlng tree was not directly used. However, it is of interest to note that 
in the FIFO queue discipline that we used, the binary spannzng tree rooted in node I (and illustrated 
in Figure 5) is traversed breadth first. We now look at the computational complexity of our algorithm. 
Note that the controlling parameter in the al- gorithm is the number of faces on the boundary (equivalently, 
the number of nodes in the binary spanning tree). Each face is accessed twice (due to the nodes having 
indegree two). Both times a face is accessed, the program has to check whether it is on the list of marked 
nodes; this is potentially where the greatest amount of computing effort is spent. We devised the data 
structure for storing the marked faces with the specific aim of reducing this work. Each face is specified 
by a voxel coordinate and a direction (one of six). If suffi- ciently large memory is available, the 
faces can be stored in a four-dimensional array, and the search time is therefore Constant. Hence, from 
the theoretical standpoint, which usually neglects details of memory, our algorithm is linear with the 
number of faces. However, as one of our main goals is to design the algorithm to work efficiently within 
the confines of a minicomputer, we cannot neglect the effect of memory. In order to keep the algorithm 
as close to linearity as possible, we devised the data structure for storing the marked faces with both 
the confines of memory and the restrictions on working time in mind. Let A be the maximal number of slices 
in our scene; e.g., in our 180x128x52 example, A is 180. The faces are stored in 6A interlaced linked 
lists, one associated with each direction in each slice. Space is allocated and de- allocated dynamically. 
This way, both the search time and memory are minimized. Again~ following our previously introduced example, 
while the peak number of faces stored in the lists is 1744, the maximum length that a sublist achieved 
is-'~.'---- We computed the total time per face detection using some worst case allocations (i.e., 
creation of long sublists). Our measurements show that the time per face is 3.3 msec with a search length 
of 30 to 8.78 msec with a search length of 300. As mentioned earlier, the lower search length is more 
typical. This discussion indicates a basic difference between our approach and that of the also truly 
three-dimensional boundary detection method of Rhodes [11]. Rhodes finds the surface of an object in 
 the scene by first finding the object by three-dimensional region growing. This means that the controlling 
parameter is the number of voxels in the object, rather than the number of faces in the boundary. Since 
the number of voxels in any medical object is likely to be greater than the number of faces, one would 
expect the Rhodes algorithm to be slower than ours. This is confirmed by the computer times reported 
in [11]. The only algorithm (besides the one described "here) that we are aware of in the l~terature 
which is both truly three- dimensional and is controlled by the faces in the boundary (rather than by 
the voxels in the object) is the one due to Liu [10]. Liu's algorithm has a more general appli- cability 
(it works on the original scene, rather than on a subset Q of voxels), but for situations where both 
algorithms are applicable, ours is faster, due mainly to our capability of keeping the number of marked 
faces down to a small fraction of the total number of visited faces. This has been confirmed by experiments, 
showing an order of magnitude speed increase over Liu's algorithm. Another implementational consider- 
ation is the storage of the scene. We specify the set Q by a binary array (asso- ciating I with voxels 
in Q and 0 with voxels not in Q), and use bfts to store the binary array. Thus our 180x128x52 example 
requires 74,880 16 bit words. Since we are using a 32K curd unit~ we had to implement a softwarc virtual 
memory management system for handling the scene. This increases our computer time require- ments for 
the larger scenes. In the many applications of our programs to CT data we found that computer time on 
the Eclipse S/200 varied from 2 milliseconds per face (surface of a sphenoid bone consisting of 21,324 
faces) to 10 milliseconds per face (surface of bony structures around and behind the sinuses and orbits 
consisting of 112,130 faces). In what appears to be the clinically most significant studies so far (those 
involving the ventricular system in the ~rain and the spinal column) our timing Varied between 4 and 
10 milliseconds per face. These times include the virtual memory management overhead. 3. Experimental 
Results Our alqorithm has been applied to detect the surfaces of a large variety of organs inside the 
human body from CT data. For example, in [2] we reported on the detection and display of the ventricular 
system inside the human brain based on CT scans of patients at the Millard Fillmore Hospital in Buffalo, 
New York. Another example is [3] where we have reported on a study aimed at determining the clinical 
efficacy of three-dimensional boundary detection and display for the diagnosis and treatment of spine 
disease; especially disc herniations. Here we report on two recent studies both of which involve rather 
large problems. The voxels in the raw CT data are not cube shaped. In all the experiments reported, 
the CT voxels had square shaped cross-sections of 0.8mm x 0.8mm. In the ~o experiments they were respectively 
Imm and 1.5mm long. In both cases we have used linear interpolation to esti- mate what the CT data would 
have been like if the voxels were cube shaped with all sides 0.8mm long. The data were collected by the 
GE CT/T scanner at different hos- pitals, and boundary detection and dis- play were done on the m~inicomputer 
of this scanner; namely, the Eclipse S/200. The data for the first experiment have been briefly described 
in previous sections; see in particular Figure I. Originally 40 CT slices Imm apart gave rise after linear 
interpolation to 52 slices 0.8mm apart. Corresponding 180x128 voxel subregions were selected in each 
slice (containing the organ of interest), and a range of CT numbers (greyness) was specified to identify 
those voxels in the scene which contain bone (see Figure I). Our algorithm has been applied, producing 
a list of faces, which were then displayed in a rotating movie-like mode on the screen of a Comtal Graphic 
Display System using the techniques described in [1,8]. Figure 6 shows two frames of this movie. It 
should be noted that the output of our algorithm is just what is needed as input by the display algorithms 
of [1,8], since those display algorithms require in- put which is in the form of a set of voxel faces. 
 The next example is related to the spine: it is a case of spinal fracture. Prior to going into details, 
we wish to emphasize that the original CT scans have been taken in the natural course of diag- nostic 
evaluation and surgical planning. They are typical of standard clinical practice, which was not in any 
way changed to accomodate our boundary detection and display algorithms. Thus our methods are applicable 
without increasing radiation dose to the patient. Figure 7 shows two combined views of the left and 
right halves of the fra- ctured part of the paine of a patient at the Medical College of Wisconsin, in 
Milwaukee. From the original 40 CT slices at 1.Smm apart we have created 74 esti- mated slices 0.8mm 
apart. Two separate scenes were built up for the left and right halves of the spine, each 64x128x74 voxels. 
Separate applications of our algorithm yielded two surfaces, one with 59,090 faces (enclosing a volume 
of 140,950 voxels) and the other with 43,592 faces (enclosing a volume of 107,131 voxels). The two surfaces 
are displayed as if the spine was cleaved in the middle and then are rotated in opposing direc- tions 
to allow the surgeon to compare the nature of the damage in the two halves. Two frames of this movie 
are shown in Figure 7.  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1980</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>807462</article_id>
		<sort_key>10</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-1980</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Polygon comparison using a graph representation]]></title>
		<page_from>10</page_from>
		<page_to>18</page_to>
		<doi_number>10.1145/800250.807462</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=807462</url>
		<abstract>
			<par><![CDATA[<p>All of the information necessary to perform the polygon set operations (union, intersection, and difference) and therefore polygon clipping can be generated by a single application of a process called <italic>polygon comparison.</italic> This process accepts two or more input polygons and generates one or more polygons as output. These output polygons contain unique homogenous areas, each falling within the domain of one or more input polygons. Each output polygon is classified by the list of input polygons in which its area may be found. The union contour of all input is also generated, completing all of the information necessary to perform the polygon set operations.</p> <p>This paper introduces a polygon comparison algorithm which features reduced complexity due to its use of a graph data representation. The paper briefly introduces some of the possible approaches to the general problem of polygon comparison including the polygon set and clipping problems. The new algorithm is then introduced and explained in detail.</p> <p>The algorithm is sufficiently general to compare sets of concave polygons with holes. More than two polygons can be compared at one time; all information for future comparisons of subsets of the original input polygon sets is available from the results of the initial application of the process.</p> <p>The algorithm represents polygons using a graph of the boundaries of the polygons. These graphs are imbedded in a two dimensional geometric space. The use of the graph representation simplifies the comparison process considerably by eliminating many special cases from explicit consideration.</p> <p>Polygon operations like the ones described above are useful in a variety of application areas, especially those which deal with problems involving two dimensional or projected two dimensional geometric areas. Examples include VLSI circuit design, cartographic and demographic applications, and polygon clipping for graphic applications such as viewport clipping, hidden surface and line removal, detailing, and shadowing.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Area coherence]]></kw>
			<kw><![CDATA[Clipping]]></kw>
			<kw><![CDATA[Edge intersections]]></kw>
			<kw><![CDATA[Euler graphs]]></kw>
			<kw><![CDATA[Graphs]]></kw>
			<kw><![CDATA[Polygon clipping]]></kw>
			<kw><![CDATA[Polygon comparison]]></kw>
			<kw><![CDATA[Polygon set operations]]></kw>
			<kw><![CDATA[Winged-edge structures]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>E.2</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor>Display algorithms</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002951.10003152.10003161.10003162</concept_id>
				<concept_desc>CCS->Information systems->Information storage systems->Record storage systems->Record storage alternatives</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P161253</person_id>
				<author_profile_id><![CDATA[81100217340]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Kevin]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Weiler]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Institute of Building Sciences, Carnegie-Mellon University, Pittsburgh, PA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Braid, Hilyard, and Stroud. Stepwise Construction of Polyhedra in Geometric Modelling. Tech. Rept. 100, CAD group, University of Cambridge Computer Laboratory, October, 1978.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Baumgart. A Polyhedron Representation for Computer Vision. National Computer Conference, 1975, pp. 589-596.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>891970</ref_obj_id>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Baumgart. Winged Edge Polyhedron Representation. Tech. Rept. CS-320, Stanford Artificial Intelligence Laboratory, October, 1972.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Eastman and Weiler. Geometric Modeling using the Euler Operators. First Annual Conference on Computer Graphics in CAD/CAM Systems, May, 1979, pp. 248-259.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Eastman and Yessios. An Efficient Algorithm for Finding the Union, Intersection, and Differences of Spatial Domains. Tech. Rept. 31, Institute of Physical Planning, Carnegie-Mellon University, September, 1972.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>360802</ref_obj_id>
				<ref_obj_pid>360767</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Sutherland and Hodgman. "Reentrant Polygon Clipping." CACM 17, 1 (January 1974), 32-42.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_obj_id>356626</ref_obj_id>
				<ref_obj_pid>356625</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Sutherland, Sproull, and Schumacker. "A Characterization of Ten Hidden Surface Algorithms." Computing Surveys 6, 1 (March 1974).]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Turner, J. An Algorithm for Doing Set Operations on Two and Three Dimensional Spatial Objects. Architecture Research Laboratory, University of Michigan, 1977.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_obj_id>563896</ref_obj_id>
				<ref_obj_pid>563858</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Weiler and Atherton. Hidden Surface Removal using Polygon Clipping. SIGGRAPH 77 Proceedings, SIGGRAPH, Summer, 1977, pp. 214-222.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Weiler. Hidden Surface Removal using Polygon Clipping. Master Th., Cornell University, January 1978.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Polygon Compa rison using a Graph Representation Kevin Weiler Institute of Building Sciences Carnegie-Mellon 
University Pittsburgh PA 15213 keywords: polygon comparison, polygon set operations, polygon clipping, 
clipping, graphs,euler graphs, wihged-edge structures, area coherence, edge intersections CR categories: 
8.1,8.2 1. Abstract All of the information necessary to perform the polygon set operations (union, intersection, 
and difference) and therefore polygon clipping can be generated by a single application of a process 
called polygon comparison. This process accepts two or more input polygons and generates one or more 
polygons as output. These output polygons contain unique homogenous areas, each falling within the domain 
of one or more input polygons. Each output polygon is classified by the list of input polygons in which 
its area may be found. The union contour of all input is also generated, completing all of the information 
necessary to perform the polygon set operations. This paper introduces a polygon comparison algorithm 
which features reduced complexity due to its use of a graph data representation. The paper briefly introduces 
some of the possible approaches to the general problem of polygon comparison including the polygon set 
and clipping problems. The new algorithm is then introduced and explained in detail. The algorithm is 
sufficiently general to compare sets of concave polygons with holes. More than two polygons can be compared 
at one time; all information for future comparisons of subsets of the original input polygon sets is 
available from the results of the initial application of the process. The algorithm represents polygons 
using a graph of the boundaries of the polygons. These graphs are imbedded in a two dimensional geometric 
space. The use of the graph representation simplifies the comparison process considerably by eliminating 
many special cases from explicit consideration. Permission to copy without fee all or part of this material 
is granted provided that the copies are not made or distributed for direct co~ercial advantage, the ACM 
copyright notice and Polygon operations like the ones described above are useful in a variety o! application 
areas, especially those which deal with problems involving two dimensional or projected two dimensional 
geometric areas. Examples include VLSI circuit design, cartographic and demographic applications, and 
polygon clipping for graphic applications such as viowporl clipping, hidden surface and line removal, 
detailing, and shadowing. 2. Introduction Polygon comparison is the process where several input polygons 
are compared and output polygons are generated such that the area of each output polygon is unique (non-overlapping) 
and completely homogenous in terms of the number and identity of the input polygons which contain the 
output area (see figure 2-1). These results provide six common polygon manipulation operations including 
the polygon set operations: union, intersection, and difference (see figures 2-2a through 2-2d), and 
the polygon clipping operations (see figures 2-2e and 2-2f). Polygon clipping, as normally used in computer 
graphics, is a combination of the difference and intersection of the input polygons. This paper introduces 
a polygon comparison algorithm which represents an advance over its predecessors, particularly in the 
area of reduced complexity. The paper briefly introduces some of the possible approaches to the general 
problem including the polygon set and clipping problems as well as polygon comparison. The new algorithm 
is then introduced and explained in detail. The algorithm operates by first merging the graphs of all 
of the input polygons together into a single graph. All of the output polygons are already imbedded in 
the graph at this point. This new graph is then traversed; information accumulated during traversal enables 
the identification of the owning areas of each output polygon, in most cases without further processing. 
Finally, the output can be stored or results can be selected from the output based on whether union, 
intersection, difference, or clipping was requested. the title of the publication aria its date appear, 
and notice is given that copying is by permission of the Association for Computing Machinery. To copy 
otherwise, or to republish, O]980 ACM 0-8979]-021-4/80/0700-0010 $00.75 10 requires a fee and/or specific 
permission. New polygons created by the comparison process are the result of the recombination of exisling 
boundary segments of the input polygons; no new or extended edges need to be created. This has the advantage 
that only the minimum necessary number of output polygons are created. The term polygon comparison is 
used here to distinguish this process which solves for all of the polygon set operations at once from 
other similar processes which solve for individual polygon set operations at a time. A contour is a single 
closed loop boundary which encloses an area. Polygons as used here refer to one or more contours which 
enclose an area. Polygons may be concave and may contain holes. Contour ownership is defined as the set 
of input polygons which completely contain the area enclosed by the contour. This may also be referred 
to as the history of the contour. An edge-side refers to one side of an edge, since in the connected 
graph which is a product of the polygon comparison processboth sides of each edge are used by different 
contours to contain different areas. An entry point is a pointer which gives access to the circular list 
of edge-sides which make up the closed contour loop. Note that an entry point into a contour must record 
not only the edge but also the side of the edge used by the contour. Coincident edges are edges which 
are colinear and overlap for some portion of their length. A graph is a set of edges connected at their 
vertices to form two or more complete contours. A restriction assumed throughout the following discussions 
is that algorithms of interest to this paper use a representational method which models polygons by their 
closed contours (boundaries) and that the contours themselves are modeled using discrete parts (such 
as edges and/or vertices). 3. Background There are several basic variations on how the polygon comparison, 
set, and clipping problems can be approached. The approaches can be classified into three groups ranging 
from the simplistic to more sophisticated approaches with better ouput characteristics: 1. split environment 
cutting approach: treat every edge of all input contours as a clipping plane which cuts the environment 
in half. This will appropriately decompose the input so that all output areas will be unique homogenous 
areas, but will excessively fragment the output and increase the number of comparisons performed. (could 
be implemented using modified Sutherland-Hodgman clipper [6]). 2. split and join: as above but with an 
additional post-processing step which rejoins adjacent areas of common ownership which were split unecessarily 
in the above process. group 2 group 3 group 4 group I Figu re 2-1 : outpul from the polygon comparison 
process a. _uni_cn b. intersection  c. a-b difference d. b-a difference  clippi~_a_-b f. clipping 
b-_a. inside: ~ inside : ~ Figu re 2-2: operations solved with output from the polygon comparison process 
3. unified: generates the output contours by recombining existing contours without adding new segments. 
The number of output areas is thereby minimized automatically. This approach in general offers the most 
flexibility, and most polygon comparison, set, and clipping algorithms to date have favored this basic 
approach [5], [8], [10]. The polygon comparison, set, and clipping problems in general involve both 
geometric and topological as well as local and global types of information. All algorithms which solve 
problems in this area must have the capacity to resolve geometric relationships by intersecting the input 
contours with each other as well as the capacity to characterize the output so that its exact relationship 
to the input is known. The algorithm designer must choose from many different alternatives and combinations 
of techniques when developing algorithms in this area. These choices determine the final characteristics 
of the solution in terms of complexity, efficiency, and flexibility. The algorithm which follows solves 
the polygon comparison problem in a monolithic fashion rather than solving for a single polygon set operation 
at a time. Thus the results of any of the six polygon operations described earlier are a subset of the 
results obtained by the polygon comparison process. 4. The Algorithm Polygon comparison processes must 
in general resolve two basic spatial relationships between the contours of the input polygons: intersection 
relationships must be determined  enclosing relationships must be determined  All output contours 
from the process must also be characterized such that the exact extent of any output area including its 
holes is fully described, and each output area must know which input polygons own its area. Once these 
objectives have been met, the final solution for any of the six operations can be determined by a simple 
selection process. The polygon comparison process can be accomplished by using the following four step 
algorithm: 1. contour merge: the contours are merged into a single graph structure whenever they intersect. 
This can be accomplished by contour edge comparisons to discover the intersections. The contours are 
then joined together at each intersection. At the end of this step all of the output contours are imbedded 
in the graph, but relevant global information about them still needs to be obtained. 2. traversah all 
of the contours are now traversed; entry points to the contours can be found by using a combination of 
the original data structure entry points and the new intersection points. A contour is traversed until 
its entry point has been reached again by a complete circuit of the contour. The traversal serves to 
collect enough information about the contour to be able to classify the contents of tbe contour's area. 
When all of the entry points in the initial entry point list have been used by the traversal process 
the traversal step is complete. An additional purpose of the traversal is to select a unique entry point 
for each of the output contours (since the union of the initial entry point list and the intersection 
point list usually'contains several entry points to lhe same contours). Tbe traversal step can also convert 
the output contours to another format if that is required, although characterization information still 
needs to be associated with it. 3. disjoint contours: additional information must still be collected 
to complete the characterization process of the contours. If there are contours which are disjoint, or 
if there are several disjoint graphs of contours, then additional tests are made. The tests must place 
all contours or contour graphs into the proper spatial relationship with the other contours in the data 
structure. Note that this cannot be done efficiently until after the traversal has established which 
contour entry points are unique. Hole (inner) contours are associated with their outer contours by this 
process. The basic polygon comparison process is complete at this point. selection of output: the output 
contours can be grouped according to the common sets of owners which contain their area. When the contours 
have been grouped, results can be chosen by a simple selection process to supply the solutions to the 
polygon set or clipping operations. The following section explains the graph data structure used by 
the algorithm; the following four sections give detailed explanations of each of the four steps listed 
above. 5. Graph Representation Data Structure The data structure used to model the polygons represents 
not only the geometric relationships between contours, but also embodies many of the topological relationships 
as well. The data structure includes a top level structure to reflect the relationships between the individual 
contours in the database. The two possible geometric relationships between any two contours in this representation 
are: one of the contours lies inside of the area of the other. neither contour lies within the area of 
the other; they coexist within the same area. These contour relationships are modeled by a binary tree 
structure: one branch can be used to get to all contours that coexist within the same area and a downward 
branch can be used to get to all contours contained within the contour. Two-dimensional space is classified 
into unique and non-overlapping areas by this tree structure (see figure 5-1). Figure 5-1: tree structure 
of contours dividing space into non- overlapping areas Z coexlstinBcontours  edge side A ~edse side 
B ~ edge side C entrypoln=return_ptr vertex i Figu re 5-2: the euler graph data structure showing 
the contour tree structure and one edge-side loop. Note that only one edge-side loop of the graph is 
shown for simplicity. A graphical drawing of the contour is also shown side 2 side l el Figu re 5-3: 
the winged-edge structure The contours themselves are defined in this algorithm by a structure which 
is reminiscent of the Baumgart wingedged or euler graph structure used for polyhedron representation 
in the geometric modeling field [2].. There are a set of graph manipulation operations called the euler 
operators which operate on this kind of structure and which can guarantee consistency of the graph after 
each modification. These operators can be used to advantage in implementing the graph manipulation functions 
necessary in the contour merge part of the polygon comparison process. There are some differences between 
the graph structure used here and the original euler graph structure. The graphs are geometric as well 
as topological structures since their vertices have fixed coordinates anchoring the structure in two-dimensional 
space. Comparisons made between these graph structures are therefore on a geometric as well as topological 
basis. It is the geometric aspect which allows the merge process in step one of the algorithm to compare 
different contours, while later steps of the algorithm take advantage of the topological relationships 
to characterize the ownership of the areas contained in the graph structure. The top level tree structure 
explained above extends the euler graph structure to include disjoint contours and their spatial relationships. 
The structure itself consists of contour, edge, and vertex elements which are linked together in a way 
that embodies many adjacency relationships. A visual representation of this basic structure can be seen 
in figure 5-2. The detailed relationships available from this form of the euler graph data structure 
are shown in figure 5-3, and a Pascal approximation of the relevant data object types including the tree 
structure is in figure 5-4. Some key relationships represented by the structure are: downward relationships 
from contours to the edge-sides which define the loop (the entry points into the contour), and from the 
edges to the vertices which bound them.  upward relationships from the edge-sides to the contours which 
have entry points using those edge-sides. Note that each edge has two sides and therefore must have two 
of these pointers. All edge-sides not pointed at by entry points have null pointers for this relationship. 
 history relationships. Each edge-side history defines the set of input polygons whose contour loops 
geometrically coincide with the edge-side. An edge-side may also be thought of as '.'facing" the area 
it bounds in a contour loop. Thus Ihe history of an edge-side tells the set of input polygons whose area 
it faces. Contours also have a history relationship which, after the algorithm has completed, contains 
the set of input polygons which own the area enclosed by the contour. cross relationships from one edge-side 
to the next to define the edge-side loops of the contours. These are dual direction linkages, and both 
sides of the edge have their own linkages making a total of four edge links per edge. Traversing contours 
created from these structures is straightforward since all linkages are specified by the structure and 
do not have to be computed. While every edge in the structure shown has two vertices, each vertex is 
associated with a particular side of the edge. A convention of use is then made as to whether this vertex 
represents the first or last vertex passed over when traversing the edge-side in a clockwise or counter-clockwise 
direction. This is a space optimization of the example data structure which is made so that the edge-side 
links only need to record the edge and not the edge-side pointed to. More complete descriptions of the 
theory behind the original winged-edge data structure, the euler operators, and their use in the geometric 
modeling field is contained in [3], [4] and [1]. const {Implementation defined llmtt} max_number of tnput_polys 
-16 type {declare supporting types} stde-1..2; {each edge has two sides} direction -(CW, CCW); {two 
traversal directions}  {declare htstory attribute types} owners 1 .. max_number_of_tnput_polys; history 
= sot of owners; {declare pointer types} cptr tcontour; {pointer to contour object} eptr -+edge; {pointer 
to edge object} vptr = +vertox; {potnter to vertex object} {declare the object types} contour -record 
entrypotnt: record {entry to edgemstde loop} entry_edge: eptr; entry_side: stde end; coexisting_contours: 
cptr; {binary tree} contained_contours: cptr; contour_history: history; {ownership attribute} end;  
 edge -record vertices: array [stde] of vptr; {down to verts} edgeltnks: array [side,direction] of epic; 
{pointers to CW and CCW edge of stdet, pointers to CW and CCW edge of stde Z} ontrypotnt return_ptr: 
array [side] of cptr; {above set only tf entrypotnt uses side} edge_history: array [side] of history; 
{attribute} end;  vortox -rocord x,y: integer {geometric vertex coords} end;  6. Merge Process The 
merge process primarily involves finding intersections between contours; when an intersection is found 
the two geometrically identical points on the corresponding graphs are merged together to reflect the 
intersection. Additional entry points into the resulting graph are also generated during this process. 
6.1. Intersecting Contours The merge intersection process must find the intersections between the contours 
and then update the graph data structure to appropriately represent the intersection. Intersections between 
contours are found by comparing each contour of one input polygon against the contours of all other polygons 
using one of the several available techniques for contour and edge intersection finding. Many efficiency 
advantages can be gained in these operations by using hierarchical boxing tests and by using the hierarchical 
nature of the input appropriately. When these higher level tests cannot rule out an actual intersection 
of contours, then the edge of the contour is compared against the edge of the other contour for intersection. 
While the exact technique used for intersection detection does not matter functionally, more generality 
will be found with techniques which can determine the intersections in such a way as to be able to handle 
general graphs as well as simple one contour graphs. This allows the general comparison of two or more 
graphs as well as polygons. While this paper assumes that the edges lie on straight lines, curved edges 
would require only a slightly different intersection finding process and the rest of the algorithm would 
remain the same. The algorithm presented requires only local operations to process intersections and 
relies on the graph data structure to maintain the global relationships. Those relationships are picked 
up more efficiently during the traversal step. Alternative algorithms often need to ascertain global 
information during the intersection phase to determine whether contours actually cross when they intersect 
or merely graze each other. Particularly troublesome in this regard are coincident intersections which 
may be preceded and followed by other coincident intersections; a possibly large amount of global information 
is necessary in this case to determine whether a true crossing occurs or not. Not only does this cause 
additional searching and traversing, but also increases the number of special cases required to handle 
the intersection process. Figu re 5-4: Pascal examples of a modified winged-edge data The algorithm presented 
eliminates any need to determine true structure also showing history attributes contour crossings and 
thereby eliminates most special cases of i A !    i><. A. MIDPOINT-MIDPOINT B. ENDPOINT-MIDPOINT 
-----' !] e C. ENDPOINT-ENDPOINT  Figu re 6-1 : three edge-vertex intersection cases intersection 
from consideration. This simplifies the intersection process not only because of reduction in the number 
of special cases to be distinguished, but also because only a few different types of geometric computations 
oeed to be performed. For the purposes of graph modi|ication in this algorithm, only three types of intersections 
need 1o be distinguished: midpoint-midpoint intersections (see figure 6-1a); the simplest of the intersection 
cases.  endpoint-nfidpoint intersections (see figure 6-1b); a vertex of one of the edges tested is involved 
in the intersection.  endpoint-endpoint intersections (see figure 6-1c); a vertex of one edge coincides 
with a vertex of the other edge. The exact steps used to merge the data structure in these three intersection 
cases are are described below. A utility which simplifies this process is a merge operation which, given 
two edges and an endpoint vertex for each, will use geometric information to take the second edge and 
insert its named vertex into the graph of the first edge at its named vertex, adjusting all edge links 
appropriately. The vertex of the second edge is deleted in the process (see figure 6-2). midpoint-midpoint 
intersection: both edges are split at the intersection location; each half of one of the split edges 
is then inserted into the graph of the other split edge using the merge process. This insertion process 
can be made more efficient than the general insertion process if desired. (see figure 6-3a). Figure 6-2: 
merge of edgel-vertexA and edge2-vertexB is performed by the merge process using geometric information 
A. M'IDPO INT-HIDPOINT B. ENDPOINT-MIDPOINT C. ENDPOINT-ENDPOINT 1) 2) Figure 6-3: merging edges at 
intersections BEFORE AFTER i t  ~s d bd Figu re 6-4: coincident edges and history merge process, showing 
history fields of the edge-sides before and after the history merge process  endpoint midpoint intersection: 
the edge which is intersected by the endpoint is split. Each of the edges touching the intersection vertex 
are inserted into tl~e graph at the new vertex using the insertion proceSs. (see figure 6-3b)  endpoint-endpoint 
intersection: Each of the adjacent edges using the intersection vertex as an endpoint are inserted into 
the graph at the vertex of the intersected endpoint. (see figure 6-3c).  While the end conditions of 
coincident edges are caught and properly handled by the intersection operations, an additional step must 
be taken whenever it is determined that an intersection which was just merged into the graph is one of 
two coincident edges. This step is a history merge of the two edges whioh are coincident (see figure 
6-4); it deletes one of the edges after doing a union of the history attributes of the two edges. Each 
side of the surviving edge then carries the. union of the histories of the two edges for that side. The 
purpose of edge histories is explained in section 7. 6.2. Additional Entry Points Additional entry points 
are generated during the merge process for use in the following traversal step. This is necessary because 
new contours which do not already have entry points may be formed in the graph that results from the 
merge process. These entry points make the traversal process more efficient. All edge-sides which have 
any of their edge links changed by intersection are treated as entry points and are placed on a new 
entry point list since they are potentially unique entries into edge-side loops. A simple criterion such 
as this creates redundant entry points which are easily culled out during the traversal process. 7. Traversal 
of the Graph Once the contours have been merged, they must be traversed to collect all of the global 
information about the output contours. Contours are traversed by starting at the edge-side specified 
by the entry point and following the edge-side links until a complete circuit is made by arriving back 
at the entry point. The traversal is used not only to characterize the ownership of each contour, but 
also to determine which of the entry points into each of the output contours are unique. Note that all 
of the output contours are already complete and are imbedded in the graph created by the merge process. 
No "choices" are necessary to follow along the boundary of an output contour during traversal since all 
edge links have already been completely determined by the merge process. 7.1. Characterization of Contour 
Ownership Many techniques can be used to discover the owner(s) of the area within a given output contour. 
Considerable savings in complexity can result however, if advantage is taken of the principle of area 
coherence [7]. The history information maintained so far in the polygon comparison process is essential 
to the characterization step described below. Two principles which remain true throughout the polygon 
comparison process are: 1. All output contours differing from the input contours because of intersection 
are formed by combining portions of all of the original contours which directly enclose their area. 
2. The history of an edge-side is never contradicted; if one side of an edge once faced the interior 
of an input contour it will always face the interior of that input contour regardless of how it may be 
contained in any new output contour. This is the principle of area coherence. The output of the polygon 
comparison process is always characterized in terms of the areas of the original input.  Combir~ing 
these two principles gives rise to the following principle: 3. The ownership or history of any given 
output contour is the set which is the union of the histories of all of the edge-sides which make up 
a contour. This last principle explains why coincident edges discovered in the merge process must be 
coalesced into a single edge containing the union of the histories of the two edges, rather than attempting 
to keep only one or even both of the edges. All edge history information must be preserved in the merge 
process; while @ =ersed £mput.: out ut contours:   ~uCuCuCu~u¢uCu¢=¢ bUaUaUb=ab < bUbUbU U~U¢=b aUaUCUOUaUa=a 
Figu re 7-1 : deriving ownership from history unions we can be guaranteed that every owner of the area 
of the output contour has at least one edge somewhere on the contour, we are not guaranteed that more 
than one of these edges from different owners will not map into the same geometric edge during the merge 
process. An implementation of this history union technique becomes simple in Pascal; every edge-side 
would have a Pascal set to represent its history (see the attributes of the data objects described in 
figure 5-4, which shows how a history set attribute can be easily represented in the objects). A history 
set can contain any, all, or none of the possible members which designate each input polygon. Every contour 
element in the data structure also has a history set to collect the results of the characterization performed 
by the traversal. Before traversal the history set of a contour would be the null set. During traversal 
the history of each edge is unioned with the current contour history and that value becomes the new contour 
history. At the end of the traversal the contour history reflects all of the input contours which directly 
own the area of the output contour (see figure 7-1). Note that the "outside" areas not contained by any 
input polygons are represented in the history attribute of the edge sides which face them as a null set. 
Every input edge loop consists of two contours; one faces the inside while the other faces to the outside 
of the area of interest and is made up of edge-sides with null history sets. The only output contours 
with a null set history after the polygon comparison process are those describing the union contours 
of all input. In summary, while the graph data structure allows us to merge the contours using only local 
information, we must still make a global traversal of the output contours to determine their history. 
7.2. Determination of Unique Entry Points Traversal starting points are selected from a list of entry 
points generated by combining all of the original entry points with all of the new entry points found 
during the merge intersection process. The only difficulty is that some of the entry points are redundant 
and lead into the same contour. For the efficiency of the next step of the algorithm and for the final 
output it is desirable to have a series of entry points into unique contours without duplicates, particularly 
since the tree contour structure is eventually built from this same list. A list of entry points can 
be implemented in a simple fashion using the data structures described in section 5 (see fig. 5-4). The 
list can be constructed from contour records which are linked together using one of the contour pointers 
contained in the record. Only the entrypoint field is actually used for data. When an addition is made 
to this entry point list, the entry -point -return field of the edge-side being pointed at by the entrypoint 
is made to point back to the contour record containing the entry point. Only edge sides which are directly 
pointed at by the entry pointers have valid return pointers; all others have null pointers. An entry 
point is used to give a starting point for a traversal of an output contour. During traversal every edge-side 
that is used has its entry -point -return field checked to see if it is valid. If it is valid, then that 
entry point (contour element) is removed from the entry point list (unless it was used as the starting 
point). This eliminates the possibility of using more than one entry point into the same contour and 
tidies up the list into a unique entry point list at the same time. 8. Disjoint Contours The traversal 
and history collection described in the previous section is not sufficient to completely characterize 
all possible ownership properties of the finished product. While several contours may intersect to form 
a single intersection graph, there may be contours or intersecting groups of contours which are disjoint 
from the remaining contours after the merge step has completed. It must be established if these disjoint 
contours contain any other contours or intersecting group of contours within their area. Traversal is 
not sufficient to discover these relationships since it can only discover local topological properties 
of the output. A special range of tests must be performed to find these global geometric spatial relationships 
between disjoint contours. They are made by comparing each output contour (or contour graph) against 
the others to determine whether one is inside the area of the other or if they coexist within the same 
area. This process continues until all contour relationships have been discovered and recorded in the 
hierarchical tree structure of contours. The process in this way distinguishes contours which are "hole" 
or interior contours of polygons from those which are main or outer contours. Relationships available 
from the input can be used to eliminate some of the tests between contours owned by the same input set. 
Again, the contour surrounding an intersection graph can also be used to consolidate some of the tests 
into a single test instead of testing each of the constituent contours individually. A simple algorithm 
which can be used to discover all of these inside/outside relationships is the following four step process: 
1. From a list of contours, take the first contour and compare it for an inside/outside relationship 
with all of the other contours on the fi91I. 2. If the contour is inside one and only one contour on 
the list it belongs to that contour as a contained contour; remove it from the list and place it inside 
that contour (if that contour already contains other contours then further tests must be performed to 
find exactly where in that contour tree structure it should be placed). 3. If a contour is contained 
by more than one other contour on the list, then move on to the next contour on the list and try to place 
it inside the remaining contours on the list by repeating the steps above. At each complete pass through 
the contour list at least one contained contour can be placed. This will eventually enable all of the 
contained contours to be properly placed. 4. When a complete pass has been made through the list and 
no contours have been found to be contained by other members of the list, then the process is complete. 
 While this simple algorithm illustrates the basic technique, many modifications can be made to increase 
its efficiency and eliminate duplicate testing. 9. Selection of Output The output contours can be grouped 
according to the set of owners which contain their area. The results can then be selected to supply the 
polygon set or clipping operations. For example, the possible groups resulting from a comparison of two 
polygons named A and B (see figure 2-1) are: 1. outer contours 2. contours enclosing only A area 3. 
contours enclosing only B area 4. contours enclosing both A and B area  The six possible results of 
polygon comparison of two input polygons can be obtained by making the following selections from these 
four groups: 1. union - use group 1 above 2. intersection - use group 4 above 3. A-B difference - use 
group 2 above 4. B-A difference - use group 3 above 5. A clipped by B -  area of A inside B - use 
group 4  area of A outside B - use group 2  6. B clipped by A -  area of B inside A - use group 4' 
 area of B outside A - use group 3   Examples of the results for the six operations are-shown in 
figure 2-1. 10. Specific Advantages The algorithm presented provides several advantages over alternate 
approaches to the problem: 1. concave polygons with holes may be handled. 2. only the minimum necessary 
number of output polygons are generated. 3. most special intersection cases are eliminated due to the 
graph representation. 4. the output includes results for any or all of the six' polygon comparison operations. 
 5. more than two polygons can be compared at once. 6. one application of the comparison process yields 
all information necessary for the result of the six operations on any subset of the original input ,(although 
advantage 2 can not be guaranteed in this case). 7. the algorithm will not create degenerate output 
such as zero area contours or coincident edges, and in fact can correct input containing such degeneracies. 
  The primary advantage, representing a significant advance over the work from which this algorithm 
is derived [10], [9] , is the reduction in the number of special edge intersection cases necessary, as 
well as a significant simplification of the 11. Conclusion A notable feature of the algorithm presented 
is its representation of data in a graph format. The information embodied in the graph widens the application 
areas in which polygon comparison and similar processes can be used. While the output from this process 
can be converted into olher formats, it can also be a valuable forn?at in its own right because of the 
kinds of topological information explicitly available. An example relevant to the graphics field would 
be using the graph to examine the adjacencies between contours in the graph in order to perform anti-aliasing 
on an image in a frame buffer. Manipulation of partially solved hidden surface (or line) scenes also 
become easier in a graph format, allowing polygons to added and compared to graphs, graphs added and 
compared to graphs, etc. This situation occurs when there are one or more stationary objects in the scene. 
The polygon comparison algorithm solves the polygon set and polygon clipping problems with a single process. 
The algorithm provides a significant reduction in complexity over alternative techniques, while offering 
a great deal of flexibility in the form and amount of input processed at one time and increasing its 
usability in a variety of application areas. 1. Braid, Hilyard, and Stroud. Stepwise Construction of 
Polyhedra in Geometric Modelling. Tech. Rept. 100, CAD group, University of Cambridge Computer Laboratory, 
October,. 1978. 2. Baumgart. A Polyhedron Representation for Computer Vision. National Computer Conference, 
1975, pp. 589-598.  3. Baumgart. Winged Edge Polyhedron Representation. Tech. Rept. CS-320, Stanford 
Artificial Intelligence Laboratory, October, 1972. 4. Eastman and Weiler. Geometric Modeling using the 
Euler Operators. First Annual Conference on Computer Graphics in CAD/CAM Systems, May, 1979, pp. 248-259. 
 5. Eastman and Yessios. An Efficient Algorithm for Finding the Union, Intersection, and Differences 
of Spatial Domains. Tech. Rept. 31, Institute of Physical Planning, Carnegie-Mellon University, September, 
1972. 6. Sutherland and Hodgman. "Reentrant Polygon Clipping." CACM 17, 1 (January 1974), 32-42.  7. 
Sutherland, Sproull, and Schumacker. "A Characterization of Ten Hidden Surface Algorithms." Computing 
Surveys 6, 1 (March 1974). 8. Turner, J. An Algorithm for Doing Set Operations on Two and Three Dimensional 
Spatial Objects. Architecture Research Laboratory, University of Michigan, 1977. 9. Weiler and Atherton. 
Hidden Surface Removal using Polygon Clipping. SIGGRAPH 77 Proceedings, SIGGRAPH, Summer, 1977, pp. 214-222. 
 characterization process. 10. Weiler. Hidden Surface Removal using Polygon Clipping. Master Th., Cornell 
University, January 1978.  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1980</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>807463</article_id>
		<sort_key>19</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-1980</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[Creating polyhedral stellations]]></title>
		<page_from>19</page_from>
		<page_to>24</page_to>
		<doi_number>10.1145/800250.807463</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=807463</url>
		<abstract>
			<par><![CDATA[<p>A process for creating and displaying stellations of a given polyhedral solid is described. A stellation is one of many star-like polyhedra which can be derived from a single solid by extending its existing faces. A program has been implemented which performs the stellation process on an input object and generates a 3-dimensional image of the stellated object on a computer graphics display screen. Pictures of icosahedron and rhombictriacontahedron stellations generated by the program are included in the paper.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Complete face]]></kw>
			<kw><![CDATA[Computer graphics]]></kw>
			<kw><![CDATA[Interactive graphics]]></kw>
			<kw><![CDATA[Polygon modelling]]></kw>
			<kw><![CDATA[Polyhedra]]></kw>
			<kw><![CDATA[Raster graphics]]></kw>
			<kw><![CDATA[Stellations]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Hierarchy and geometric transformations</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010240.10010244</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision representations->Hierarchical representations</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP15020776</person_id>
				<author_profile_id><![CDATA[81100057436]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Kathleen]]></first_name>
				<middle_name><![CDATA[R.]]></middle_name>
				<last_name><![CDATA[McKeown]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Department of Computer and Information Science, The Moore School/D2, University of Pennsylvania, Philadelphia, PA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP15038212</person_id>
				<author_profile_id><![CDATA[81452608047]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Norman]]></first_name>
				<middle_name><![CDATA[I.]]></middle_name>
				<last_name><![CDATA[Badler]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Department of Computer and Information Science, The Moore School/D2, University of Pennsylvania, Philadelphia, PA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>563900</ref_obj_id>
				<ref_obj_pid>563858</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Blinn, J. F. A homogeneous formulation for lines in 3 space. SIGGRAPH Proceedings 11, 2 (1977), 237-241.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Bruckner, M. Vielecke und vielflache. (Leipzig, 1900).]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Coxeter, H. S. M., Du Val, P., Flather, H. T., Petrie, J. F. The fifty-nine icosahedra. The Univ. of Toronto press, (1938).]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>810125</ref_obj_id>
				<ref_obj_pid>800178</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Fuchs, H. and Kedem, Z. M. The "highly intelligent" tablet as an efficient pointing device for interactive graphics. Proceedings ACM National Conference (1978), 765-769.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Kepler, J. Harmonia mundi. Opera Omnia, Vol. V (1864).]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[LISP.3D.SYSTEM. Computer Graphics User's Guide, Moore School Computing Facility, Univ. of Penn. (1978).]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Pawley, G. S. The 227 triacontahedra. Geometriae Dedicata 4 (1975), 221-232.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Poinsot, Cauchey, Bertrand, and Cayley. Abhandlungen uber die regelmassigen sternkoper. (Leipzig 1906).]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Wheeler, A.H. Certain forms of the icosahedron and a method for deriving and designating certain higher polyhedra. Proceedings International Mathematics Congress, 1 (1924).]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Whorf, R., P. The twenty-five triakistetrahedra. unpublished paper (1978).]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Creating Polyhedral Stellations Kathleen R. McKeown and Norman I. Badler Department of Computer and 
Information Science The Moore School/D2 University of Pennsylvania Philadelphia, PA. 19104 ABSTRACT: 
A process for creating and displaying stellations of a given polyhedral solid is described. A stellation 
is one of many star-like polyhedra which can be derived from a single solid by extending its existing 
faces. A program has been implemented which performs the stellation process on an input object and generates 
a 3-dimensional image of the stellated object on a computer graphics display screen. Pictures of icosahedron 
and rhombictriacontahedron stellations generated by the program are included in the paper. KEY WORDS 
AND PHRASES: computer graphics, polyhedra, stellations, complete face, interactive graphics, polygon 
modelling, raster graphics. CR CATEGORIES: 3.15, 8.2 i. Introduction A process for creating and displaying 
stellations of a given polyhedral solid is described. Stellation is a geometric transformation which 
creates a finite family of polyhedral solids from a single solid. The number of stellations which can 
be obtained from a single solid depends on the complexity of the object. For example, the 12-sided dodecahedron 
has only 3 possible stellations, while the 20-sided icosahedron yields 59 different stellations. Because 
of the complex yet symmetric forms that can be obtained, mathematicians have long been intrigued by 
the stellation process and the striking polyhedra which result. The stellation transformation is achieved 
by replacing each face of the original polyhedron with a new face which lies in the same plane and is 
a collection of planar figures. The figures are finite regions selected from the complete face of the 
given object: the 2-dimensional configuration that is formed by intersecting the infinite plane of one 
face of the object with the infinite planes of every other face. Figure 1 shows the complete face of 
the icosahedron. Permission to copy without fee all or part of this mterial is granted provided that 
the copies are not made or distributed for direct co~m~erclal advantage, the A(~f copyright notice and 
the title of the publication and its date appear, and notice is given that copying is by permission of 
the Association for Computing Machinery. To copy otherwise~ or to republlshp requires a fee and/or specific 
permission. 01980 ACM 0-89791-021-4/80/0700-0019 $00.75 A program has been implemented which performs 
the stellation process on an input object and generates a 3-dimensional image of the stellated object 
on a computer graphics display screen. Any solid* consisting of identical polygonal faces can form the 
base polyhedron. An interactive graphics package was used for easy creation of the polyhedron as a list 
of vertices for each face (LISP.3D.SYSTEM 1978). In combination, the two programs allow for experimentation 
with the stellation process on objects whose stellations have not previously been seen or enumerated. 
Moreover, stellations are easier and faster to produce when the program is used than when drawn by hand. 
 The program was run with the icosahedron, a regular 20-sided object, as input (see Figure 4). Many of 
the examples and several of the stellations within the paper originate with the icosahedron. The presentation 
will have pictures of rhombictriacontahedron stellations which had not, until now, been drawn or modelled. 
 2. History Stellation is not a new concept. The stellations of the octahedron and the dodecahedron 
(the smallest of the Platonic solids which have stellations) were examined by Kepler around 1619 (KEPLER 
1864). A combination of people discovered the stellations of the icosahedron, the * If the dihedral 
angles of the input object are not obtuse, stellation will result in only one object: the same that was 
input. 19 next simplest of the solids which can be stellated. In 1809, Poinsot discovered the great 
icosahedron (POINSOT 1906), and in the early 20th century, several more stellations were discovered by 
Bruckner (BRUCKNER 1900) and A.H. Wheeler (WHEELER 1924). But it was not until 1938 that a complete 
enumeration (along with drawings) of the icosahedron stellations was obtained (COXETER 1938). A number 
of mathematicians continue to work on enumerating, drawing, and modelling stellations of new objects. 
Among them are Whorf (the triakistetrahedron (WHORF 1978)) and Pawley (the rhombictriacontahedron (PAWLEY 
1975)). 3. Overview of the Stellation Process To create stellations of a polyhedral solid, the program 
is executed on a file which contains the vertices of each face of the object. The first phase of processing 
constructs the complete face of the given polyhedron. The resulting 2-dimensional configuration is drawn 
on the computer graphics display screen. The user constructs the new face for stellation by selecting 
polygons from the complete face using a cursor. Each polygon selected is outlined in pink by the program 
so that the user can keep track of the selections. Figure 5 shows a new face that could be used for stellating 
the icosahedron; the new face is that part of the complete face outlined id pink. Note that there is 
no adjacency requirement for selection. Thus, the stellation formed from the face shown in Figure 5 will 
have 20 "faces" like the icosahedron but each "face" will actually consist of three separate regions 
lying in the same plane. The stellated polyhedron is created by replacing each face of the original 
object with the new face selected by the user. After the user specifies certain parameters for the display 
(for example, the viewline and the colors to be used), the stellation is displayed either as a wire-frame 
drawing or as a solid shaded object using a hidden surface elimination routine. The stellation formed 
from the new face in Figure 5 is shown in Figure 6. Different stellations of the same object may be created 
by selecting a different new face from the complete face. 3.1 Creating the Complete Face: The complete 
face of a polyhedral solid is created by extending the plane of one face of the object. The other faces 
are then extended until the plane of the first face is intersected. The 2-dimensional figure formed by 
the lines of intersection is called the complete face. The complete face of the icosahedron is shown 
in Figure with Coxeter's numbering scheme (COXETER 1938). The first step in creating the complete 
face is to rotate the object so that one face lies in the z=0 (viewing) plane. Then, for each face in 
the object, the plane column vector* for the face is used with the plane column vector for the z=0 plane 
to form the line matrix for the line of intersection between the two planes. The line matrix is computed 
by taking the cross-product between the two plane column vectors. Arbitrary points along the resulting 
vector are stored. Since the lines of intersection all lie within the z=0 plane, the z-coordinate can 
be discarded and the face drawn using 2-dimensional graphics routines. Note that the intersection of 
two planes forms an infinite line; the line matrix contains no information on appropriate endpoints for 
the line. In order to draw the complete face on the screen, some additional computation must be done 
to determine where to start and stop drawing each line and to determine the window which just contains 
the face. To this end, each line is intersected with every other line in the face and the points of intersection 
sorted using a parameterized representation of the line. Each line is drawn from its minimum point of 
 intersection to its maximum. Figure 1 * The plane column vector is determined by the first three points 
in the face and is the equation for the plane determined by those points. The first three elements in 
the vector describe the normal to the plane and the fourth indicates where the plane lies in space. The 
vector is computed using homogeneous coordinate techniques (BLINN 1977). 20 3.2 Selecting the New Face: 
After the complete face is drawn, the user of the program manipulates a cursor to select the polygons 
which will form the new face. The only restriction on the selection of polygons is that the group selected 
have the same rotational symmetry as the original face. The program, in its current state, does not do 
any intelligent error checking on the selection process. It is up to the user to select a face that has 
the same rotational symmetry as the original face. For example, when stellating the icosahedron, the 
user must select a set of polygons that has triangular symmetry. Figure 1 illustrates this requirement 
with Coxeter's numbering scheme. If the user chooses one of the polygons labeled i, he must also select 
the other two polygons labeled i. The major problem for the program during this stage is determining 
the vertices of the polygon within which the cursor lies (see (FUCHS and KEDEM 1978)). The method used 
involves probing out from the cursor until a side of the polygon is touched and then tracing around the 
polygon, saving vertices, until the side originally touched is reached again. The following algorithm 
describes the method (references are made to Figure 2 which illustrates the first two steps in the tracing 
process): i. Construct horizontal line from cursor to window edge (line CW, Figure 2). 2. Intersect 
that line with every line in complete face  3. Select minimum* point of intersection (Point I, Figure 
2).  4. Select vertex-i of polygon:  On intersected line (line AB, Figure 2), vertex-i is the next 
greatest** point of intersection.  5. n=l  6. Select next vertex:  * The line is parameterized from 
C to W and the minimum of all intersection points selected on that basis. ** For each line in the complete 
face, the points of intersection with other lines in the face are available in sorted order (these were 
computed and stored when constructing the complete face). Thus, to determine the next greatest point, 
the point of intersection with the cursor line (point I, Figure 2) is parameterized on the line (line 
AB, Figure 2) and the storage array searched for the parameterized point just greater than that point. 
 A. On lines intersecting old line at vertex-n (lines BE, BD, and BF intersect line AB at B, Figure 2) 
select nearest intersection points on same side of old line as cursor (points F,D, E, Figure 2). B. 
Next vertex = intersection point on that line which makes the smallest angle with old line (line BE 
makes smallest angle with line AB, Figure 2, so next vertex = E). C. n=n+l D. Repeat A-D until next 
vertex = vertex-1 c ~ .~..-Y-/I ~ w AB = first line intersected by cursor line B = closest point of 
intersection on AB F,D,E = points on the same side of AB as cursor 0<s<£ so AE is the next side of the 
triangle Figure 2 3.3 Forming the Stellated Object: The stellated object is formed by replacing each 
face in the original polyhedron with the new face selected by the user. This is achieved by performing 
 a series of transformations on the new face: rotation, translation, and orientation. For each face in 
the original input object, a copy of the new face is rotated to lie in the same, or a parallel, plane 
as that of the old face. To determine the axis of rotation, the cross-product is taken between the normal 
to the z=0 plane and the plane of the old face. The angle of rotation is equivalent to the arc-cosine 
of the third component of the normalized normal to the old face. After rotation, the distance between 
the center of the old face and the center of 2] the rotated new face is determined and the rotated 
new face is then translated by that distance. A third and final transformation must be done to align 
the new face with the old. When replacing the old face of the polyhedron with the new face, the new face 
is "overlaid" on the old by matching the center polygon of the complete face (as if it were part of the 
new face) with the old face of the polyhedron. The first two transformations put the polygons of the 
new face in the same plane as the old face. However, if the center polygon of the complete face were 
present in the new face, it would not necessarily be aligned with the old face (as in Figure 3 BEFORE, 
where triangles A and B are askew). The orientation of the new face is achieved by performing the rotation 
and translation transformations on the center polygon as well as on the new face. To correctly orient 
the new face, the vertices of the transformed center polygon (triangle B in Figure 3) must be lined 
up with the vertices of the face to be replaced (triangle A in Figure 3). A line is constructed between 
the center of the old face and one vertex of the center polygon (line cb, Figure 3). Another is constructed 
between the center and the corresponding vertex of the old face (line ca, Figure 3). The new face (polygons 
C,D,E, Figure 3) is then rotated by the angle between the two lines (angle s, Figure 3). Note that only 
the new face polygons are affected. / 8 ~'~ BEFORE A = old face B = center polygon of the complete 
face C,D,E = new face polygons Figure 3 With the completion of the orientation transformation, the 
transformed coordinates of the new face are stored. Note that each polygon of the new face is treated 
as a separate face in the stellated object. Although the original vertices of the polygons are transformed 
 as a group, the new polygon are stored duplicate occurrences removed, the polyhedron the display routine. 
 4. Results The program was tested vertices for each separately. After of vertices are data is passed 
to on the icosahedron, a 20-sided figure having an equilateral triangle as its face (Figure 4). The 
 icosahedron has 59 different stellations (including the icosahedron itself);' some of these are extremely 
unusual, having holes through the center and/or components connected along an edge or at a vertex. 
 Seven computer-generated images of the stellations with hidden surfaces removed are shown in Figures 
6-12. These pictures were photographed from a Ramtek GX-100B raster display and color video monitor 
 that has 240x320 spatial resolution and 12 bits of color. Although all stellations for the icosahedron 
have been enumerated and drawn, a number of objects exist whose stellations have not been visualized. 
The rhombictriacontahedron is one of these. Pawley (PAWLEY 1975) has identified 227 stellations by describing 
the faces, but many of these stellations have not been drawn or modelled. Moreover, his restrictions 
on what counts as a valid stellation are stringent, thus limiting the number of stellations he obtains. 
Several rhombictriacontahedron stellations will be shown at the conference session. 5. Extensions Some 
extensions can be made to improve ease of use and computational efficiency of the program. These extensions 
involve rotational symmetry, numbering, and merging of the new face polygons. The program could use the 
rotational symmetry of the face to decrease the number of cursor entries the user must make when selecting 
the new face. Thus, for the icosahedron, if the user selected one of the polygons labelled "i" (see Figure 
i), the other two polygons would be selected automatically. Alternatively, the user could be allowed 
to completely bypass the selection of polygons using the cursor. Instead, a numbering scheme could be 
used to uniquely identify the different combinations of polygons having the same rotational symmetry 
as the original face. In the current program, computation in the Watkins hidden surface routine can 
be extensive. Since each polygon in the new face is treated as a distinct face of the stellated object, 
faces quickly become 22  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1980</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>807464</article_id>
		<sort_key>25</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-1980</article_publication_date>
		<seq_no>5</seq_no>
		<title><![CDATA[Acoustic radar graphic input device]]></title>
		<page_from>25</page_from>
		<page_to>31</page_to>
		<doi_number>10.1145/800250.807464</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=807464</url>
		<abstract>
			<par><![CDATA[<p>Accurate X,Y position determining digitizers presently require a precision array of wires embedded in the tablet. In rear-projection tablets these wires tend to limit the optical quality of images projected on the screen. Principles of acoustic radar are applied in a device which achieves better resolution, at a lower cost and without the use of wires, either in a tablet or a cursor. The function of acoustic reflectors for automatic calibration is described, achieving an accuracy of + mn; 0.01% of the distance to the sensors. In the case of small tablets, a hand-held stylus may be tracked at the rate of 900 locations a second.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Acoustic radar]]></kw>
			<kw><![CDATA[Back-projection]]></kw>
			<kw><![CDATA[Cursor]]></kw>
			<kw><![CDATA[Digitizer]]></kw>
			<kw><![CDATA[Finger-pointing]]></kw>
			<kw><![CDATA[Graphic input]]></kw>
			<kw><![CDATA[Position measurement]]></kw>
			<kw><![CDATA[Retro-reflector]]></kw>
			<kw><![CDATA[Self-calibrating]]></kw>
			<kw><![CDATA[Sonar]]></kw>
			<kw><![CDATA[Stylus]]></kw>
			<kw><![CDATA[Tablet]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.1</cat_node>
				<descriptor>Input devices</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.4.7</cat_node>
				<descriptor>Projections</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>B.4.1</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003752.10010061.10010068</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures->Random projections and metric embeddings</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010583.10010588</concept_id>
				<concept_desc>CCS->Hardware->Communication hardware, interfaces and storage</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010391</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Graphics input devices</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP39040463</person_id>
				<author_profile_id><![CDATA[81100379833]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[P.]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[de Bruyne]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Institute for Communication Technology, Federal Institute of Technology, CH8092 ZUrich, Switzerland]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Brenner, A.E., de Bruyne, P. SONIC PEN: A digital stylus system. IEEE Transactions on Computers, V. C 19, No 6 (1970) pp. 546-548. |he device is made by Science Accessories Corp. of Southport, Conn.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Philipow, E. Taschenbuch der Elektrotechnik V.3 Nachrichtentechnik p. 1382.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Mason, W.P. Physical Acoustics V. 1 Part A (1964) pp. 169-270, Academic Press N.Y.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Sell, H. Eine neue Kapazitive Methode zur Umwandlung mechanischer Schwingungen in elektrische und umgekehrt. Z. f. technische Physik (1937) 3.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Anke, D. Luftschallwandler nach dem Sell-Prinzip f&#252;r Frequenzen von 50 kHz bis 100 kHz. Acustica V. 30 (1974) pp. 30 - 39.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Skolnik, M. I. Radar Systems, McGrawhill Book Co. N.Y. (1962) p. 43.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[de Bruyne, P. Authentication of Handwriting with SONAR. Proceedings of the 1980 International conference "Security through Science and Engineering" Berlin ( to be published Sept. 1980).]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[de Bruyne, P. Developments in Signature Verification. Security Management Vo1. 22, No. 6 (1978) pp. 57 - 61.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Acoustic Radar Graphic Input Device P. de Bruyne Institute for Communication Technology Federal Institute 
of Technology CH8092 ZUrich, Switzerland ted by designs, all incorporating wires accurately ABSTRACT 
embedded in the tablet. Pulses in these wires are Accurate X,Y position determining digitizers presen-sensed 
by a coil or electrode embedded in the cursor, tly require a precision array of wires embedded in When 
the wires are accurately spaced, such as on a the tablet. In rear-projection tablets these wires printed 
board, an accuracy of ~ 5 mil. is realised. tend to limit the optical quality of images projec- A completely 
wireless tablet, cursor and stylus is ted on the screen. Principles of acoustic radar are desired by 
some users. Such a device has been built applied in a device which achieves better resolution, at our 
Institute, utilizing the principles of Sonar. at a lower cost and without the use of wires, either It 
may be used without a screen or cursor, using in a tablet or a cursor. The function of acoustic reflectors 
for automatic calibration is described, the finger as a pointer if desired. achieving an accuracy of 
~ 0.01% of the distance to the sensors. In the case of small tablets, a hand- Acoustic radar (SONAR) 
Electronic devices using SONAR to accurately deter- held stylus may be tracked at the rate of 900 loca- 
mine the range of an object in air are relatively tions a second. recent. The Polaroid Sonar Autoranging 
camera is an example. Although the "object" has undefined acous- Keywords and phrases: Graphic input, 
Digitizer, So-tic properties and shape, this acoustic radar device nar, Acoustic radar, Finger-pointing, 
Tablet, Back- projection, Retro-reflector, Cursor, Position mea-has proved to be accurate and reliable. 
surement, Stylus, Self-calibrating. Application to 9raphic input devices. In the case CR Classifications: 
6.35, 3.70, 6.4 (Pat.pending). of SONAR operated graphic input tablets this "object" is cooperative. 
A cylindrical cursor, for instance, INTRODUCT ION ensures that the reflected signal has a predictable 
Acoustic ranging in air for graphic input devices was waveform and amplitude. It thus becomes possible 
to pioneered at Harvard University in 1966 [Ref.l] The measure the time elapsed between transmission 
and first device utilized a small spark at the tip of a hand-held stylus. The resulting acoustic wavefront 
reception of the pulse with high accuracy. Planar electrostatic transducers similar to those used with 
the "Sonic Pen" data tablet may also be used as a was picked up by linear Sell-type electrostatic mic- 
rophones positioned at rightangles along two edges transmitter of very short pulses. They generate a 
 of the working area. This "Sonic Pen" digitizer, as planar acoustic wavefront which is reflected by 
ob- it was called, became the leading product in its jects in its path. If an object has part of its 
sur- field and is still available today in a practically face approximately parallel to the transducer, 
part unchanged form. The high accuracy requirements of cursor digitizers could, however, not be met with 
of the reflected wavefront will return parallel to the transducer and cause a transient voltage to ap-the 
"Sonic Pen" device. This market remained domina- Permission to copy without fee all or part of this material 
is the title of the publication and its date appear, and notice granted provided that the copies are 
not made or distributed is given that copying is by permission of the Association for for direct commercial 
advantage, the ACM copyright notice and Computing Machinery. To copy otherwise, or to republish, requires 
a fee and/or specific permission. Ol980 ACM 0-89791-021-4/80/0700-0025 $00.75 25 I I I-'='-I ! % % 
I i I d ! i I Fig. I. Reflected wavefront returning to its source. pear at its terminals. Fig. 1 shows 
that the first sound to return to the transducer follows the short- est path with a distance d, The value 
of d may be calculated from the transit time of the pulse and from the known velocity of sound, The distance 
of the centre of the cursor to the transducer may then be found by adding the known radius of curvature 
to the distance d. By alternately pulsing the X and Y transducers, which are located in planes perpendicu- 
lar to each other and to the plane of the tablet, the coordinates of the centre of the cursor may be 
tracked with respect to fixed reference axes. PHYSICAL PROPERTIES The velocity of ultrasound is independent 
of fre-quency. Losses for distances of 2 metres are so small, that a transient containing frequencies 
up to 200 kHz remains unchanged when propagated over this distance. The waveform of an acoustic transient 
will be retai- ned even after reflection from the surface of a cy- lindrical cursor. The propagation 
velocity is howev-er dependent on temperature [Ref. 2]. It may thus be desirable to compensate for temperature 
changes. This may be achieved by measuring both the transit time of a pulse reflected from the cursor 
at the unknown po- sition and also the transit time of this same pulse reflected from a calibration reflector 
placed at an accurately known position, as shown in Fig. 2. By calculating the ratio of these two values 
and multi- plying with a constant, the range to the transducers may be obtained with an accuracy of + 
0.01%, One of the properties of radar concerns its insensi- i/H/HH/H/////H/////HH//J < T ra ns duc er 
I t Calibration ,~ I reflector I " I < Working ~,I I area I ~ Cursor .1' , 1 Fig. 2. Digitizer with 
calibration reflectors. tivity to motion of the medium. Air draughts of uni-form temperature are found 
to have very little eff-ect on the accuracy. If the air velocity v is per-pendicular to the transducer 
plane, the transit time of a pulse to and from an object at range d is: d d = + (1) t c-v c+v where c 
is the velocity of sound, For a velocity of 8 ft/sec. the error in the time measurement is less than 
0.01%. If the velocity is in a different direction the er-ror is even less. In case of tablets with a 
sparking pen, the sound travels in only one direction and such draughts may produce errors of nearly 
I%.  DESIGN The hardware for the acoustic radar graphic input tablet consists of two transducers and 
the control electronics. A cursor or stylus and a plane on which graphic data is presented may be freely 
chosen to suit the application, They are low cost items. Transducer technology The smallest generator 
of a high intensity acoustic transient is a spark [Ref. I]. It is however a one way transducer and therefore 
not compatible with acoustic radar,where the outward going and returning wavefront are to follow a nearly 
identical path. More promising is the linear piezo-electric transdu- cer [ReK 3]. These transducers require 
a substantial period, after transmitting a pulse, before they may be used as a receiver of returning 
signals. This is a result of the large mass of the moving parts. 26 F1 Fig. 3. Separate transmit, and 
receiving transducers. This recovery problem may be solved by using separa- te transmitting and receiving 
transducers, as shown in Fig.3. The receiving transducer is shown mounted above the planar transmitting 
transducer and is sli-ghtly curved in order to have part of the surface perpendicular to the direction 
of the returning sig- nal. The cursor must be similarly curved. A short risetime of the received pulse 
is thus assured. Cur-ved surfaces however, disperse the energy of the wa-vefront and the sensitivity 
therefore falls off ra-pidly with an increase in the tablet size. The recei- ved signals have a magnitude 
which typically is an order of magnitude less than when planar transducers are used. We have found that 
Sell-type electrostatic transdu- cers permit bidirectional operation and thus may be planar. This type 
of transducer consists of a thin metallized membrane which is movable with respect to a stationary back-electrode 
[Ref. 4]. The membrane may have a very low mass of about 1 mg/cm 2 and hence does not store much kinetic 
energy. Rapid damping of the membrane motion permits it to function as a sen- sitive receiver within 
200 ~sec after transmitting a pulse. Such a planar transducer is naturally alig- ned with the wavefront 
which has been reflected by a suitable cursor. With careful design the transmit- ted and received waveform 
are nearly identical in sha-pe. A risetime of 2 ~sec may be obtained, which typi- cally permits timing 
of the peak of the pulse with an accuracy of + I00 nanoseconds. This corresponds to a resolution of + 
0.5 mil in the position of the cur- sor. In magnetostrictive devices the speed of sound is more than 
I0 times its value in air, hence a com- parable resolution is not easily achieved, The requi- red clock 
frequency would exceed 300MHz~ The mass of the transducer membrane, although small 27 is still several 
times greater than that of the air which is moved by, or moving it, hence an impedance mismatch causes 
reflections to occur. In our trans-ducers the power losses in receiving and transmittT ing are found 
to be less than 30 dB each. Practical transducer design The dimensions of the transducers are determined 
by the required tablet size. We have found that for an acceptable signal-to-noise ratio of 20 dB that 
the cursor may be quite small with respect to the area of the transducer. The effective retro-reflective 
cross sectional area of the cursor or stylus may be as low as 0.2% of the transducer area to meet this 
requirement. The height h of the transducer and of the cursor is determined by the required maximum ra- 
nge d, It is desirable to design for a fairly uni-form sensitivity over the working area of the tablet 
and it may be shown that this is achieved when:  h>{d; (2) whereL is the wavelength of a frequency matching 
the risetime of the acoustic pulse. Account has here been taken of the fact that the tablet or screen 
surface reflects a wavefront of grazing incidence so as to double the effective height of the transducer 
and cursor. Mechanical construction. The best overall performan-ce was achieved with an electrostatic 
transducer constructed as shown in Fig. 4. The back-electrode S is covered with plastic microspheres 
having a dia- meter between 5 and 25 ~m, each sphere having a S Fig. 4. Exploded view of the Sell-type 
transducer  Fig. 6. Handheld stylus and cylindrical cursor may be realised by using a thicker membrane 
and an increased distance between it and the back-electrode. Retro-reflecting cursor. At large working 
distances (20" to 70") from a long transduceG it may be desi- rable to employ a larger retro-reflective 
cross sec-tional area than can be obtained with a small cylin-drical cursor. From eq. (3) it is seen 
that to doub-le this area, the radius of the cursor must be inc- reased 4 times. It is clear that cylindrical 
cursors of large diameter are appreciably less efficient than a well designed retro-reflecting cursor. 
It is well known from optics that retro-reflectors have the property of reflecting a wavefront without 
disper-sing the direction. The wavefront is reflected in the exact direction from which it originated. 
If we consider a simple corner reflector of height h and aperture w as depicted in Fig. 7, the retro-refl-ective 
area Ae is given by: Ae = h . w (cosa- sina) (5) for 0<a<45 ° 0 *45 Fig. 7. 0ff-axis effective reflective 
area of a cor- ner reflector, as a function of the angle of incidencea, for a wavelength ~<< w. where 
a is the angle of the incident beam with re-spect to the axis of symmetry. Singularity reflect-ions exist 
at angles a = + 45 o because of direct mi-rror reflections by the surfaces of the retro-refle-ctor. This 
equation does not include this effect or the effect of reflections from the tablet surface. These reflections, 
as mentioned before, double the effective height of the cursor at long ranges. The equation~Jrther assumes 
that 2h>>~. By combining 8 of these corner reflectors, an omnidirectional retro-cursor is obtained as 
shown in Fig. 8. Two levels of 4 retro-reflectors mounted one above the other are used so that the diameter 
becomes V~" w. The ref- lective cross sectional area is practically indepen-dent of the angle of incidence 
except for the sing-ularity reflections which occur every 45 o . The effe- ctive aperture area is half 
that of the maximum of a single corner reflector given in equation (5), becau-se the height is shared 
between the upper and lower sections. If the cursor diameter is 5 cm and the 2 height 1.5 cm, this amounts 
to 5.3 cm as compared to 1.16 cm 2 obtained with a cylindrical cursor of the same dimensions. The retro-reflective 
cursor is thus more efficient by a factor depending on its ,'° -" \"-- Upper  ~ section r kllloo I 
I IJ 0.4 02 Ae I ~ I ~ I L .... -180 = -90 ° 0 Q 90 ° *180 ° Fig. 8. Simple omni-directional retro-reflecting 
cur-sor with effective aperture Ae as a function of the angle of incidence a. 29 Lower section \I 'g. 
9. " ied retro-reflective cursor diameter and on the wavelength of the reflected ul- trasound. Practical 
retro-reflective cursor. The cursor of Fig. 8 should have infinitely thin walls, because front and rear 
surfaces must coincide and intersect in a vertical line at the centre. It is hence not practi-cal. Another 
shortcoming is that the centre area should be available for viewing a crosswire. The apex of each corner 
reflector should therefore be removed. A solution to both problems is found by employing wedge shaped 
tapered walls and is shown in Fig. 9. The wedge shaped walls force the individual corner reflectors to 
be oriented at an angle greater than exactly 90 °, as in Fig. 8.This does not affect the action of the 
retro-reflector as the corners remain at go °, Additional advantages of this arrangement are that the 
singularity reflections are split into two parts, each having half the amplitude. The re- flection diagram 
is thus less affected by the singu- larities. All reflecting surface planes intersect in a vertical line 
through the centre of the cursor and hence the phase of the reflected waves is indepen-dent of rotation 
of the cursor. This fulfils a req- uirement for accurate measurements. The section of the device which 
incorporates the handle is never or- iented towards the transducers, hence does not have 90 o aligned 
surfaces. The angles of this section are less than 90 o . The top section of the cursor is made twice 
as high as the bottem section. This is because of the fact that the top section does not benefit appreciably 
from reflections in the tablet plane, which double the effective height of the bottem sec- tion at the 
maximum range from the transducer. This practical retro-reflective cursor realises a gain of 3.5 in area 
over a cylindrical cursor of equal height (1.6 cm) and diameter (5 cm) when used with pulses of 2 ~sec 
risetime at a distance of 75 cm from the transducer. It can be used at a range of 5 ft and more. Calibration 
reflectors It was mentioned in a previous section that the pla- nar calibration reflectors shown in Fig. 
1 are used to compensate for differences in the speed of sound at different ambient temperatures and 
levels of hu-midity. The dimensions required were found experime- ntally. An area of 1 square inch is 
satisfactory. The size of the tablet for a given active area of working surface need not be affected 
by the presence of the reflectors. Shading effects caused by extre- me positions of the cursor are recognized 
and hand- led inte~igently by the microprocessor. The two ca-libration reflectors incorporate a redundancy 
which is used to advantage. Control electronics The tablet is connected to the control electronics by 
two coaxial wires, one for X and one for Y. The electronics consists of two receivers, a timer con- troller 
and a data buffer with microprocessor. All these functions are easily implemented on a single printed 
circuit board. Transceiver. The transducers are operated with a pulse of 150 V of about 4 ~sec duration. 
For a tablet of 22" x 22" active area the capacitive load is ap-proximately 7 nF. and Darlington switching 
transis-tors with a voltage step-up transformer are used in the driver. The driver power requirements, 
even for such a large tablet is a modest 0.2 W at its maxi-mum pulsing rate. The transmitter and receiver 
which are connected to the transducer by the same wire, are decoupled from each other with diodes. The 
receiver noiselevel of 4 ~V is 155 dB below that of the transmitted pulse and a rapid decay of transients 
is thus required. This has been attained without special measures and 30 in practice the position of 
objects as near as 1 cm to the transducer have been measured with a resolu- tion of 5 microns. At distances 
of 40 cm the limit-ing resolution using a 2" cursor is 75 n~nosec. This accurate timing is achieved over 
a dynamic range of signal amplitudes of 20 : 1 using amplitude tracking peak detectors. A clock frequency 
of 13.5 MHz pro- vides a resolution of 0.5 mil for tablets with a di- gitizing area up to 20" x 20". 
Timer controller. A divider of the clock frequency provides trigger and control pulses for th~transmi-tters, 
detector logic and multiplexer. The maximum trigger rates are dependent on the size of the tab- let. 
For small tablets this may be up to 900 Hz. The speed of sound limits the rate to 250 coordinates a second 
in the case of a tablet of 20" x 20". Microprocessor. The microprocessor is an 8080A and is interfaced 
with input and output buffers. It mo-nitors the calibration signals and status switches, subtracts the 
coordinates of the user defined origim adds the radius of the cursor and thus obtains the output data 
in the correct format. This output is a-vailable at a standard RS 232 interface. A parallel interface 
and a display is presently being designed. This system will be compatible with a PDP II compu-ter and 
will be used in the development of an on- line verification system of handwritten signatures. [Ref, 7 
and 8]. APPLICATIONS The advantages in accuracy, resolution and transpa- rency of the tablet are of interest 
in applications where graphic input digitizers are presently used. As an example, a sketch of a rear-projection 
digi- tizer is given in Fig. I0. The projected image is completely free from obstructions because no 
wires are used in the tablet or in the cursor. The slide projector is mounted to the side of the projection 
screen for ease of access. The concept also permits an L-shaped transducer as- sembly to be used without 
a screen or a cursor. The finger itself may then be used as a pointer-reflec- tor within the active area. 
This is useful in data file access and other applications which normally require the use of a light pen. 
Fig. I0. Rear-projection digitizer ACKNOWLEDGENENT The development of these graphic input devices was 
made possible by kind permission of Professor Dr. P. Leuthold, director of this Institute. Mr. R. Da- 
nieli and Mr. A. RUckstein are thanked for their ef-forts in constructing the prototype equipment. REFERENCES 
I. Brenner, A.E., de Bruyne, P. SONIC PEN : A digital stylus system. IEEE Tra~actio~ on Compu~er~,V. 
C 19, No 6 (1970) pp. 546-548. The device is made by Science Accessories Corp. of Southport, Conn. 2. 
Philipow, E. Ta~chenbuch der Elektrotechnik V.3 Nachrichtentechnik p. 1382. 3. Mason, W.P. Physical 
Acoustics V. 1 Part A (1964) pp. 169-270, Academic Press N,Y. 4. Sell, H. Eine neue Kapazitive Methode 
zur Umwan-dlung mechanischer Schwingungen in elektrische und umgekehrt. Z. f. technische Physik (1937) 
3. 5. Anke, D. Luftschallwandler nach dem Sell-Prinzip fUr Frequenzen von 50 kHz bis I00 kHz. Acu~tica 
 V. 30 (~974) pp. 30 -39. 6. Skolnik, M. I. Radar Systems,McGrawhill Book Co. N.Y. (1962) p. 43. 7. 
de Bruyne, P. Authentication of Handwriting with SONAR. Proceedings of the 1980 International conference 
"Security through Science and Engi- neering" Berlin ( to be published Sept.1980). 8. de Bruyne, P. Developments 
in Signature Verifi- cation. SecuRity Management VoI. 22, No. 6 (1978) pp. 57 -61.  31  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1980</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>807465</article_id>
		<sort_key>32</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-1980</article_publication_date>
		<seq_no>6</seq_no>
		<title><![CDATA[Movie-maps]]></title>
		<subtitle><![CDATA[An application of the optical videodisc to computer graphics]]></subtitle>
		<page_from>32</page_from>
		<page_to>42</page_to>
		<doi_number>10.1145/800250.807465</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=807465</url>
		<abstract>
			<par><![CDATA[<p>An interactive, dynamic map has been built using videodisc technology to engage the user in a simulated &#8220;drive&#8221; through an unfamiliar space. The driver, or map reader, is presented with either sparsely sampled sequences of images taken by single frame cameras that replicate actual imagery from a space, or with computer synthesized replicas of those images. The reader may control the speed, route, angle of view and mode of presentation of this information and may thus tour the area. In addition, he may access spatially stored ancillary data stored in the buildings or in locales in the environment. This basic map is being enhanced to provide topographic views, and to incorporate optical and electronic image processing to provide a more responsive, visually complete representation of an environment.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Anamorphic imagery]]></kw>
			<kw><![CDATA[Computer generated imagery]]></kw>
			<kw><![CDATA[Image processing]]></kw>
			<kw><![CDATA[Interactive systems]]></kw>
			<kw><![CDATA[Optical videodisc]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>H.2.4</cat_node>
				<descriptor>MAPS</descriptor>
				<type>P</type>
			</primary_category>
			<other_category>
				<cat_node>H.5.1</cat_node>
				<descriptor>Video (e.g., tape, disk, DVI)</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010225.10010230</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision tasks->Video summarization</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245.10010248</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems->Video segmentation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002951.10002952.10003190</concept_id>
				<concept_desc>CCS->Information systems->Data management systems->Database management system engines</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Management</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP39075243</person_id>
				<author_profile_id><![CDATA[81100315933]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Andrew]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lippman]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Research Associate, Architecture Machine Group, Massachusetts Institute of Technology, Cambridge, Massachusetts]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Simulation of wrinkled surfaces, James Blinn, SIGGRAPH 1978.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[An improved illumination model for shadow display, Turner Whitted, SIGGRAPH 1979.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Ibid.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Movie-Maps: An Application of the Optical Videodisc to Computer Graphics Andrew Lippman Research 
Associate Architecture Machine Group Massachusetts Institute of Technology Cambridge, Massachusetts 
02139 ABSTRACT An interactive, dynamic map has been built using videodisc technology to engage the 
user in a simulated "drive" through an unfamiliar space. The driver, or map reader, is presented with 
either sparsely sampled sequences of images taken by single frame cameras that replicate actual imagery 
from a space, or with computer synthesized replicas of those images. The reader may control the speed, 
route,, angle of view and mode of presentation of this information and may thus tour the area. In addition, 
he may access spatially stored ancillary data stored in the buildings or in locales in the environment. 
This basic map is being enhanced to provide topographic views, and to incorporate optical and electronic 
image processing to provide a more responsive, visually complete representation of an environment. Key 
Words: optical videodisc; computer generated imagery; anamorphic imagery; image processing; interactive 
systems. Category Numbers: 3.80, 8.1, 8.2. The work reported herein has been supported by the Cybernetics 
Technology Division of the Defense Advanced Research Projects Agency, under Contract No. MDA-903-78-C-0039. 
 1.0 Introduction In the past, research in computer graphics has centered on better, more rapid techniques 
for modeling and synthesizing realistic appearing images. Starting from calligraphic displays, in which 
the world was depicted as line segments, computer graphics grew to include the solids, textures, and 
colors possible with raster scan television systems, and developments in modeling have resulted in 
acceptably realistic and dynamic animation systems and flight simulators. Recent work has con- centrated 
on the addition of synthesized texture (i), multiple illumination sources (2), and computer refraction 
(3), but pictures generated with these features must be done "off-line." The dynamics of flight simulator-type 
systems remains too costly for most applications.  Simultaneously, the television industry has begun 
to absorb the techniques of digital signal processing into their transmission systems, but with the different 
goal of real-time picture manipulation, and with a far less comprehensive set of modeling features. Framestore 
synchronizers, the broadcast industry analogue to raster scan computer displays, emphasize speed of operation, 
and implement image synchroniza- tion, smooth scaling and merging of pictures at the standard television 
rate of thirty images per second. These systems also use feedback to generate effects, but as a rule, 
accept real images as their data source, and provide real images as their output. Control rather than 
synthesis is stressed. These two apparently disparate approaches to picture interaction and processing 
now have the capability to be merged by use of the optical videodisc. The manipulation expertise of the 
television industry can be combined with the modeling and inter- action expertise of the computer graphics 
industry in systems that have the responsiveness and controllability of computer systems, but use the 
visually complete and detailed imagery of the television world. Further, these systems are available 
at costs that permit them to Permlsslon to copy without fee all or part of this material is the title 
of the publication and its date appear, and notice  granted provided that the copies are not made or 
distributed is given that copying is by permission of the Association for  for direct commercial advantaEe 
, the ACM copyright notice and Computing Machine~. To copy otherwise, or to republish,  G1980 ACM 0-8979]-02]-4/80/0700-0032 
$00.75 32 requires a fee and/or specific permission. serve a great many applications. Unlike graphics 
systems which often are designed to meet certain performance goals at any price and are then developed 
into afford- able products, the videodisc has progressed from an inexpensive movie player to an interfaced 
computer peripheral that provides random access to over 50,000 frames of television information. A system 
that executes this merger, and at the same time explores its limits is the Movie-Map. At its simplest 
level, it may be regarded as a dynamic replacement for a topographic paper map: it can familiarize a 
user, or map reader, with a spatial environment. This familiarization, however, is accomplished by quite 
different means: the "map reader" explores space by participating in a simulated drive through it, seeing 
filmed sequences that replicate the actual views he would have were he in the space, driving. The experience 
of driving is made more intensive and involv- ing through interaction: the user deter- mines routes, 
turns, speeds, and points of view. He may also select the season, via a "season knob," and the visual 
mode of the tour: a photo, sketch, or animation (illustration i). Thus the system does not simply repeat 
a guided tour, but allows a person to freely explore, at his own rate, via his own path, and with either 
photographic or detailed computer synthesized visuals. A second purpose of the Movie-Map is to provide 
a data access and management system. The data stored may take the form of text, still pictures, synthesized 
or recorded sound, dynamic animation and cinema. It is characterized by being accessed spatially, where 
the particular organization corresponds to the physical layout of real space. In fact, this data is stored 
"behind" the facades of buildings, or in locales, and is retrieved by"driving" to it. Often the data 
relates exactly to the building which houses it: behind the front of a restaurant may lie the menu, or 
a tour of the kitchen. Other times, it may relate to the entire space and be located in an expected edifice: 
the telephone directory may be contained in the telephone exchange; the town's vital statistics are 
stored, naturally enough, at town hall. A third function of the Movie-Map is to explore the interface 
between real images and computer graphics. The particular image recording and processing techniques used 
to create the map are uncommon, involving anamorphic lenses, travelling animation cameras, and three 
dimensional projection screens. Optical and electrical processing of the images, both prior to the recording 
on the disc, and during viewing are freely interchanged. With minimal real distortion and, ideally, 
no conceptual distortion, the user may place himself anywhere in the streets of the  town, continuously 
alter his point and angle of view, and smoothly travel between actual camera positions. The ranges of 
realism necessary are being determined, and the means to achieve tham are being developed. Two types 
of movie-maps will be discussed. The first, or the basic map, contains the elements necessary to implement 
surrogate travel and spatial data access. Travel is somewhat discontinuous, occurring in discrete movements 
along streets, and two views other than straight down the road are provided. This is the system on which 
many of the filming and mapping concepts were developed. It is characterized by being a single image 
display system, and no processing of the image occurs: appropriate images are selected from one optical 
videodisc at a time. This will lead to the discussion of the anamorphic map, currently under development. 
This map is characterized by the addition of smooth travel between filmed images and continuous view 
alteration. The control systems for this map will prove to be very different, and the display system 
will use multiple screens, and curved screens. The images from the disc will be processed versions of 
the original film, and that processing will occur both before recording and during use.   2.0 The Basic 
Mapping System The basis of the movie-map is the optical videodisc. The capability of this device to 
store and randomly access 54,000 frames of television, and to sequence through them in a controlled manner 
is exploited heavily in the design of the system (see Appendix i) . Simply put, the map simulates travel 
by displaying to a user (driver) controlled rate sequences of indivisual television frames taken at periodic 
intervals along a particular street in a town. These frames originate as filmed views that correspond 
to what one would see were they actually in the town, driving along that street. Their rate of playback, 
or alternatively, the number of times each video frame is displayed before the next one is shown, determines 
the apparent speed of travel. To allow the route to deviate from straight paths down each street, separate 
sequences that depict all possible turns through all intersections are likewise filmed, and interspersed 
with the straight line travel as required. Thus, it is possible, for example, to drive three blocks along 
one street, at a set speed, then turn onto a crossing street and continue travelling. Instantaneous 
freedom of choice in route selection is permitted by the use of two videodiscs. While one is playing 
the sequence that corresponds to travel along a street, the second is automatically 33 positioned at 
the start of the sequences for turns from that street at each approach- ing intersection (figure 2). 
When a turn is requested, the signal from this second disc is made visible, and the turn is played. At 
the end of the turn, the first disc, having readied the sequence for the new street, continues the journey. 
This alternation between "visible" disc and "look-ahead" disc is the means by which all user choices 
in the system are executed. Note that due to the disc's ability to play backward as Well as forward 
with equal facility, both a left and a right-hand turn may be queued on the look-ahead disc simultaneously. 
They are merely laid out "head-to-head," with one in reverse order: searching to their junction and playing 
forward displays one; playing backward displays the other (figure 3). The configuration to realize basic 
surrogate travel and data access is shown in figure 4. A single, 32 bit minicomputer controls the operation 
of the map, accesses the data base, and manages the user inter- action. Display is on a nineteen inch 
television screen, with a touch sensitive panel overlaid in front; images originate either in the videodiscs 
or in a frame buffer display, and are mixed, switched, and matted together by an interfaced video switcher. 
In the basic map, sound consists solely of synthesized speech, implemented by a Votrax, phonemic synthesizer. 
The user controls consist of the touch panel or a specially built "joystick." During a drive through 
the town, a set of driving controls and indicators is over- laid onto the disc image, as shown in the 
 figures. These pictograms define buttons and indicate the places on the screen which when touched cause 
a change in the action of the map. Centrally located is a stop button, which halts all travel. It is 
 flanked by two colored bars that change either forward or reverse speed. When a speed is selected, 
a "tick mark" moves along the arrow to a point that is Straight Turns T.--~ Disc 1 Disc 2 ~ ; T; . 
Disc 1 Disc 2 Disc player alternation scheme. While disc 1 is playing a sequence corresponding to travel 
along a street, disc 2 searches for the turns approaching at the next intersection. (top). As disc 1 
passes the intersection, disc 2 moves on to the next turns. (bottom). Figure 2 Disc layout. A straight 
segment is followed by all of the turns from that street, in order. From a single point on the disc, 
playing forward will show a right turn; playing backward will show the corresponding Straight 1st 2nd 
3rd Segment ~ Turns Turns Turns .. ... ,,, --\~,  N"N' 'N Figure 3 J Basic Map Reading System: (1) 
User station with monitor; transparent touch panel; joystick; loudspeakers (2) Video generation, two 
disc players; Ramtek graphic display, video switch (3) Sound Generation, Votrax speech synthesizer 
(4) System management, Interdata 7/32 processor  Figure 4 34  proportional to the actual speed chosen. 
 When the user stops, that tick mark remains to facilitate resumption of motion with the same speed. 
At the extreme edges of the screen are two arrows which are abstractions of international road sign 
symbols for allowable turns. When touched, they change into green arrows to indicate that a turn at 
the next possible intersection will occur; when no turn is possible, a diagonal red bar is drawn through 
the turn indicator to replicate the "no left/right turn" international sign. Likewise, when forward 
 travel is not possible, as in the case of a "T" intersection, the stop sign is replaced by an international 
"do not enter" sign. Immediately to the inside of the turn indicators are two "eyeballs" by which a 
driver may request a change in direction of view. In the basic map, two alternative views exist: a full 
right or full left view; these are selected by the appropriate button. In the anamorphic map, the view 
rotates either clockwise or counter- clockwise. All of the options available by touching the screen 
are similarly available by action on the joystick. Forward pressure controls forward speed, likewise 
for reverse, and sideways pressure indicates the desire to turn. Separate buttons radially spaced around 
the stick control are used for view selection. When driving with the joystick, there are two options 
available for speed control other than simple force-to-speed translation. The stick may be made to respond 
as an accelerator, in which case the rate of travel will increase in proportion to the time that the 
stick is pressed either forward or backward, as well as pressure, or the stick may be made to operate 
as a speed controller with coast. In this case, absolute speed is proportional to pressure, but when 
the stick is released, speed remains constant. When other than simple speed is the desired control, immediate 
stop can be effected by depression of the bottom button on the joystick, and gradual slowing can be accomplished 
by the reverse of the actions required to speed up. In all cases, the indicators on the screen function 
to confirm that the system has received the command in question. Thus it is not necessary to hold the 
left turn indicator (for example) until a turn is begun, but only until the arrow responds. The next 
available turn will be made. 2.1 The visuals of driving The town was mapped by mounting four cameras 
aimed at 90° intervals around a horizontal circle and taking one frame every ten feet along each street. 
Stability was maintained with a trailing "fifth wheel" that generated the shutter actuation pulses (figure 
5). To minimize discontinuities such as lighting changes, abrupt shadow movement, and drastic alter- 
 ations in the environment, filming was restricted to midday, and a path was developed that would completely 
record the images for each section of town in the least time. The entire town was recorded in both 
fall and winter; selection of image is done by a user "season" control. The visual result of this filming 
technique is that in driving, one has the ability to stop every ten feet along the street, and proceed 
at rates that are independent of the rate at which the town was filmed. The actual speeds correspond 
to frame presentation rates that are multiples of 33 milliseconds, the normal video rate. When each frame 
is presented once, for 33 milliseconds, travel occurs at 300 feet per second, slightly above 200 miles 
per hour. In the actual system, frame rates that vary from ten per second down are allowed, and the speed 
range is from 68 miles per hour to a complete stop, with a granularity that is proportional to speed. 
 A corollary of the sparse sampling of images along the streets is that the image will often be viewed 
at faster rates than those at which it was taken (approximately two frames per second). This has the 
effect of translating orientation changes of the camera up in frequency, and thus amplifying their disquieting 
effect: a slow rotation will become a jarring bump. Also, since the motion is inherently abrupt, all 
other sources of abruptness must be eliminated if possible. This includes lighting and color changes, 
and shadow motions. If too many things vary between one frame and the next, a user can easily become 
lost travelling in a straight line! As part of the work in developing the map- ping technique, paths 
through the town were developed that approximated what filmmakers would call a "one take" film. The entire 
town was traversed in one pass, with the camera in use continuously from late morning until late afternoon. 
Edges of town were filmed either early or late, and north-south streets as closely as possible to their 
east-west cross streets. Several different methods of filming turns through intersections were tested 
and incorporated into experimental versions of the map. They varied between "twirls" where the camera 
rotated through 360° horizontally in the center of each inter- section, to complex paths that simulated 
the view a normal driver takes as he drives around a corner. In the one adopted, the camera was fixed 
to the film- ing truck and simply driven through all turns. While this requires eight passes through 
each intersection to record all possible street to street turns, efficient paths were developed to make 
this filming 36 possible, and comparable in time to filming the straight sections.  2.2 The sound 
 During driving, all sound is generated from a speech synthesizer, and it is informational in content. 
The range of verbal accompaniment includes the following: -Street one is on, approaching, or turning 
onto -Compass directions -Confirmation of turns, commands -Spatial relationship to relevant, obvious 
landmarks -Distance from known points In addition, the verbalization of the above messages can be a 
function of the following: -Real time since,the last related message -Real distance -Number of previous 
repetitions of the message -Intervening messages -Intent in using the map The program that controls 
the speech forms an ordered list of possible messages, then tests the entries in sequences and plays 
the first one that meets the weighted verbalization criteria. A user control varies the verbosity of 
the system.  2.3 Ancilliary data access All data not related to actual driving is literally stored 
behind the facades of buildings and spaces. The images for these facades were recorded with specially 
calibrated single frame cameras that per- mitted accurate framing and seasonal registration between fall 
and winter. Thus the facades form a special set of pictures where the building to be examined is well 
centered in the frame and prominent, and where operation of the "season knob" results in a purely seasonal 
change; there is no movement in the image other than the disappearance of leaves and the appearance of 
snow (figure 6). Since the facades are so evidently different from the normal travelling footage, they 
form a natural interface between driving and using the system as a means for accessing spatially stored 
data. To enter the data access mode, one touches the building in question during the drive. The normal 
driving sequence halts; the image of the facade is recalled from the disc. Simultaneously, the address 
or name of the building just touched is spoken by The synthesizer, and the driving controls are replaced 
by a set of indicators that inform the user of the name of the place and content of the available data. 
This new set of pictograms are touch sensitive and are used to select and control presentation of this 
data. For most data access, the disc alternation scheme used in travel suffices. The facade image remains 
visible until the selected set of images is ready; then they are made visible. Two special cases are 
worthy of note. When the data requested consists of movies, the disc is used in normal play mode, and 
the movie can contain synchro- nously recorded sound. This is the only occasion in the basic mapping 
system where the discs play at normal rates and thus the only occasion where videodisc sound can be played 
without the use of a third player. When the data consists of textual information, that text may be generated 
from the stored data base and displayed via the graphics display. In that case, the discs are idle. 
Data access mode is the least explored use of the movie-map, and as such, only a few examples of its 
operation are included in the system. Its controls are likewise primitive. The user may vary the rate 
and duration of a particular presentation in a linear fashion only, and stop it at any time to return 
either to the facade, or to the drive. All investigations of more sophisticated visual data control that 
might occur in the future are envisioned to use more than the two disc players that the basic map requires, 
and will allow continuous sound and more general perusal of ancilliary data.  2.4 Animation Computer 
generated images provide the ability to drive in places where it is not possible to film, to generate 
an interactive overview of the town, and to remove excess detail that might confuse a newcomer. In one 
sense animation can be thought of as another season, and the selection of the animated footage is made 
by the appropriate operation of the "season knob." The animated tour is stored in the same manner as 
the filmed tour: with straight sections and turns, and operated likewise, with the standard driving controls. 
The difference is in the image content. The approach taken to computer generated images is novel in 
two respects: it is created "o%ff-line '' and made "real-time" by use of the videodisc; it contains a 
mixture of two-dimensional and three-dimensional, synthesized images. The system used to display the 
pictures operates from a full 3-D data base of the town, in fact the same data base that provides the 
informa- tion necessary to allow one to touch buildings en route. It portrays each edifice as a simple 
rectangular solid, and its sorting is optimized by use of the block organization of the town. A depth 
 sort is used, and the image is simply written to the display from far to near. In the course of writing 
the image, digitized facades are wrapped onto the surface of the solids, those facades 37  being the 
same used in the real image sequences, and appear as "billboards." Distant mountains are treated similarly 
and are thus perspective renderings of two-dimensional images (figure 7). In this manner, detail is 
added to the resultant image without incurring the overhead of a full three dimensional model of the 
building in question. Although the result is a scene that is not analytically correct, the effect is 
startling: even buildings with marquees that jut out into the street are readily identifiable and distinguishable. 
Further, the addition of mountains in the background serves as a valuable orientation aid and makes 
the image visually complete. It should be noted that the animated version of the drive through the town 
is used in exactly the same manner as the real-image drive. Animation consists of forward travel segments 
and segregated turn sequences. They are concatenated into a continuous movie by the same disc- stepping 
and alternation scheme used with the photographed images. Thus animation that is created at less than 
real-time rates is used in an interactive, real-time manner. Two types of animated drives are used in 
the mapping system. In one, the animation is literally a computer generated version of the real footage. 
Its point and angle of view duplicate it exactly. It is thus simply an alternative season and is used 
as such. In a second use, the point of view is elevated to make visible several adjacent streets. The 
drive is more like a helicopter tour. Under investigation is the use of the animated images to provide 
intermediate vertical travel between a set of aerial photographs and the street. The procession might 
be as follows: one starts at the streets, travels upward to aerial height viewing computer generated 
images, then continues with photographic images taken from an aerial map. These allow the driver to ascend 
to heights from which it is possible to see the entire town and its surroundings, then return. 3.0 Anamorphic 
Mapping The utility of the basic map rests upon two presumptions: that the impression of sufficiently 
smooth travel can be accomplished by the successive presentation of spatially sparse images, and that 
one can integrate those images to form a conceptual map of the town. Those assump- tions can be thought 
of as goals, and the basic map is an attempt to make a minimal system to meet them. It uses only two 
videodiscs, and executes no image processing functions other than selection and sequencing. Investigation 
into anamorphic mapping extends these goals. The object is to make a system so immersive, so visually 
clear and simple, that the map will substitute for the first visit. A user will be able to form such 
a complete model of the town that he will arrive a native. This will be accomplished through enhanced 
responsiveness and compelling visuals that allow no doubts and misinterpretations. The work being done 
falls into two general categories: optical processing and video rate electronic processing. All is being 
done within the context of available videodisc hardware; future work postulates modification of the disc 
itself to better integrate it into the processing environ- ment. Initial experiments have been completed 
in the optical domain, and the first panoramic map exists along with a projection-based panoramic viewer. 
Additionally, processed images derived from the photographic footage and used to digitally synthesize 
turns is included in the system.  3.1 Optical processing A tour through the town was filmed with a 
special panoramic filming lens that captured a 360° field of view with a single camera, and recorded 
that view on a single c frame of 35mm film. The specific lens is related to a super-wide "fish-eye" 
type, with significant differences. A fish-eye lens accepts such a wide angle of view that it can "see" 
behind the camera. The standard of the industry is the Nikon 220° fish-eye. It maps an area larger than 
a hemisphere onto a circle on the film. When this lens is aimed upwards, a full 360° panorama is taken, 
from directly above to 20° below the horizon. However, most of the film area is occupied with the sky, 
and the buildings and streets all appear in a narrow ring around the outer edge. The actual lens used 
in the mapping eliminates the central sky area, compressing it into a smaller circle in the center of 
the frame, and maps approximately a 30° below horizontal and 30° above cylinder onto an annulus. Thus 
it is similar to a 240° fish-eye with the central 120 ° removed. Images taken with this lens can be 
used in a variety of ways. The film can be "straightened" optically by projecting it back through the 
lens and re-photographing it with standard reproduction cameras, or linear panorama cameras. When used 
in this manner, the lens defers the decision about display camera angles to a later time or simply compensates 
for problems with moving lens panorama equipment. A more interesting case obtains when the images are 
directly recorded from the film and viewed through a special viewer (see figure 8). With this viewing 
apparatus, 39  each frame is similar to the anamorphic art works that enjoyed popularity in the renaissance. 
A distorted, incomprehensible view is rendered clear by the use of a conical mirror inverted above a 
horizontal display screen. Two benefits accrue. The user may "look to the side" by merely doing so; no 
digital controls are necessary, and the image is somewhat spatially faithful to the original scene. Further, 
the image is virtual, appearing in space and divorced from the screen, with all its fiducials and implied 
flatness; inherent spatiality is enhanced.  3.2 Electronic processing of the image The addition of 
real-time video processing hardware between the disc and the viewing screen allows a plethora of enhancements 
to either the panoramically filmed map, or maps filmed with standard flat lenses. The range of complexity 
of this equipment is likewise variable. The first step is to consider ways to present alternate views 
without dis- orienting the map reader. In the basic map, two 90° side views are instantly available, 
but to see them, an abrupt "cut" is made. In one frame the forward view is shown, in the next a sideways 
shot. This can foster some disorientation. A simple answer to this is to "slide" the side view across 
the screen, in the same manner as occurs during a video wipe. The effect of this will be a clear distortion 
in the imagery: the same building in both pictures will be simultaneously shown with grossly different 
perspectives. However, this joint presentation may will be conceptually clearer than the cut. The smoothness 
of the change will offset its inaccuracies. In this manner, a trade has been made between the physical 
error and the conceptual error. The actual hardware needed to implement this type of transformation 
is relatively simple. A video delay with only a single line of storage will suffice. Two images are written 
into different parts of the lines, stored and read out as one. This is realizable digitally with little 
difficulty; the hardest part is the synchronization of the two videodiscs. Digital simulations of this 
technique are included on the disc. An extension of this approach is to actually perform the perspective 
correction as the turn to the side view is done. To under- stand this, it is necessary first to under- 
stand that four cameras equipped with 90° horizontal field of view lenses correctly positioned can record 
all the data necessary to reconstruct that scene from any angle of view. To do so, four projectors are 
arranged in the same orientation as the cameras, and the images are projected onto screens that form 
the sides of a box. When a camera is position- ed at the center, the perspective seen is an exact replication 
of what the camera would record were it in the original scene. It follows that a camera so positioned 
could re-record any angle of view, or an electronic processor can generate it from the original images. 
 It is therefore possible to extend the technique of wiping the side view into place with the technique 
of accurately rotating it into position. This requires an extension of the hardware to store more 
than one line of video information, and for it to be able to cons%ruct elements "in between" actual 
elements stored. It must now interpolate. Several examples of this "synthesized twirl" are used in 
the system (figure 9). They are created off-line and recorded with the animation camera. One more 
extension to the use of digital hardware will now be made, capitalizing upon that interpolation capability. 
When the system can interpolate between picture elements and lines, it now can scale an image by an 
arbitrary factor. Thus it can perform the electronic equivalent to an "optical zoom." This technique 
can be used to smoothly interpolate between the successive frames taken at ten foot intervals to allow 
the user to position himself between actual filming points. A sequence of travel along a street can 
now be generated as if frames were shot at a spacing corresponding to that of travel. This is another 
case of an actual perspec- tive distortion that may result in a better conceptual presentation. The frames 
inter- polated between the camera positions will not, strictly speaking, be accurate. This may be more 
than compensated for by the smoothness introduced into the act of traveling. This has not yet been attempted. 
 One final note on the use of digital hard- ware: the same processing equipment used to control rotations 
to side views may also be used to synthesize turns. A turn can be created that consists of a forward 
 motion combined with a rotation of the camera. The camera continuoualy rotates from the straight ahead 
position until it points directly down the object street. Travel then proceeds in the new direction. 
 This "side effect" of image processing greatly simplifies the on site work necessary to create a movie-map. 
 4.0 Conclusions Although originally intended for the con- sumer market, the videodisc has proven to 
 be a useful addition to the hardware of computer graphics and interactive systems. When regarded as 
a device to store 54,000 possibly unrelated single images rather than one half hour of continuous video, 
 4]   
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1980</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>807466</article_id>
		<sort_key>43</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-1980</article_publication_date>
		<seq_no>7</seq_no>
		<title><![CDATA[High-performance raster graphics for microcomputer systems]]></title>
		<page_from>43</page_from>
		<page_to>47</page_to>
		<doi_number>10.1145/800250.807466</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=807466</url>
		<abstract>
			<par><![CDATA[<p>A frame buffer architecture is presented that reduces the overhead of frame buffer updating by three means. First, the bit-map memory is (x,y) addressable, whereby a string of pixels can be accessed in parallel. Second, the pixel-change operation is performed by hardware in a single read-modify-write cycle. Third, multiple objects in the frame buffer are addressable simultaneously by a set of address registers. The remaining task of generating (x,y) addresses and providing new data can be managed rapidly by current microprocessors or DMA-devices.</p> <p>With a modest expenditure of hardware, this architecture eliminates all the bit-shifting, bit-masking, and bit-manipulation conventionally associated with frame buffer graphics, while retaining the full generality of user-programmable control. The particular implementation described allows raster manipulation at full bit-map memory bandwidth. It can paint a 16&#215;16 pixel character into the frame buffer in 16 microseconds and can modify a 1024&#215;1024 pixel raster in 64 milliseconds.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Bit-map]]></kw>
			<kw><![CDATA[Frame buffer]]></kw>
			<kw><![CDATA[Functional memory]]></kw>
			<kw><![CDATA[Raster operation]]></kw>
			<kw><![CDATA[Raster-scan graphics]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.1</cat_node>
				<descriptor>Raster display devices</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor>Bitmap and framebuffer operations</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>C.5.3</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010476.10011187.10011191</concept_id>
				<concept_desc>CCS->Applied computing->Computers in other domains->Personal computers and PC applications->Microcomputers</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372.10010373</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering->Rasterization</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010583.10010588.10010591</concept_id>
				<concept_desc>CCS->Hardware->Communication hardware, interfaces and storage->Displays and imagers</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Performance</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP31049649</person_id>
				<author_profile_id><![CDATA[81100606986]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Andreas]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bechtolsheim]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Computer Systems Laboratory, Stanford University, Stanford, California]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39046513</person_id>
				<author_profile_id><![CDATA[81100515727]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Forest]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Baskett]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Computer Systems Laboratory, Stanford University, Stanford, California]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>5532</ref_obj_id>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[W.M. Newman and R.F. Sproull, Principles of Interactive Computer Graphics, second edition, McGraw Hill, 1979.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[C.P. Thacker, et al., "Alto: A personal Computer", in D. Siewiorek, C.G. Bell and A. Newell, Computer Structures: Readings and Examples, second edition, McGraw Hill, 1980.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[I.E. Sutherland, R.F. Sproull, S. Gupta, A. Thompson, personal communication.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[R.F. Sproull "Raster Graphics for Interactive Programming Environments", Xerox PARC Report CSL-79-6, June 1979.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 High-Performance Raster Graphics for Microcomputer Systems Andreas Bechtolsheim and Forest Baskett Computer 
Systems Laboratory Stanford University Stanford, California 94305 A frame buffer architecture is presented 
that reduces the overhead of frame buffer updating by three means. First, the bit-map memory is (x,y) 
addressable, whereby a string of pixels can be accessed in parallel. Second, the pixel-change operation 
is performed by hardware in a single read-modify-write cycle. Third, multiple objects in the frame buffer 
are addressable simultaneously by a set of address registers. The remaining task of generating (x,y) 
addresses and providing new data can be managed rapidly by current microprocessors or DMA-devices. With 
a modest expenditure of hardware, this architecture eliminates all the bit-shifting, bit-masking, and 
bit-manipulation conventionally associated with frame buffer graphics, while retaining the full generality 
of user-programmable control. The particular implementation described allows raster manipulation at full 
bit-map memory bandwidth. It can paint a 16×16 pixel character into the frame buffer in 16 microseconds 
and can modify a 1024×1024 pixel raster in 64 milliseconds. Keywords: Raster-Scan Graphics, Frame Buffer, 
Bit-Map, Raster Operation, Functional Memory 1. Introduction The raster-scan graphics system described 
in this paper is part of a workstation being dcsigned at Stanford University to support projects in design 
automation (VLSI) and advanced text processing (TEX). In brief, the workstations contains a powerful 
16-bit microprocessor, a substantial main memory, and a local network interface for accessing centralized 
file servers or remote large-scale computing resources. Each station also supports one or more high-resolution 
displays, Using a comntercial 16-bit microprocessor as the main processor of the workstation, it is difficult 
to achieve adequate frame buffer manipulation speed. Tile microcomputer processing bandwidth is simply 
insufficient to deal with the number of bits in a high-resolution frame buffer which is organized as 
a conventional memory. Our approach to this problem is to organize the frame buffer in a different way. 
This architecture treats frame buffer updates as a a function with five parameters: (X, Y, Width, Operation, 
Data), whereby X and Y are the address of a rectangle of size WidthX1, to which the Data is applied with 
the specified Operation (see Figure 1). X + Width This work has been supported in part by DARPA-MDA-903-79-C- 
0680, by Stanford Artificial Intelligence Laboratory, and by Stanford [-----~ --3it field to which I 
 Linear Accelerator Center. i Operation and Data is applied Permission to copy without fee all or part 
of this mterial is granted provided that the copies are not made or distributed Figure 1. Functional 
Frame Buffer for direct co~aerclal advantage, the A(~ copyright notice and the title of the publication 
and its date appear, and notice is given that copying is by permlsslon of the Association for In our 
implementation, width can range from 1 to 16 bits and the Computlng Machinery. To copy otherwise, or 
to republlsh, requires a fee and/or specific permission. operation is a bit-wise boolean function. These 
five parameters are maintained in separate registers. X and Y are loaded by referencing 01980 ACM 0-89791-021-4/80/0700-0043 
$00.75 windows in the main processor's address space; Width, Operation, and Data are loaded from the 
data bus. In a single memory reference operation, the main processor then can provide a new address <x> 
or <y>, access a string of size <width> at location <x,y>, and perform the previously selected <operation> 
on the frame buffer with the new <data> provided. This functional franae buffer architecture performs 
in hardware all bit-shifting, bit-masking, and bit-manipulation usually associated with frame buffers 
graphics. The only remaining task for the processor (or a DMA controller) is to generate the <x,y> addresses 
and transfer data to and from the frame buffer. Using the frame buffer function as a primitive, operations 
on rasters of arbitrary height and width can be implemented easily, such as RasterOP [1]. In our implementation, 
the frame buffer is a 64k by 16 bit (1024×1024) dual-port memory with sufficient bandwidth to allow one 
frame buffer modification (read-modify-write) cycle every microsecond. Any processor that can provide 
a new address and data at that rate fully utilizes the available frame buffer memory bandwidth. This 
means that: ---painting a 16X16 pixel character takes 16 microseconds; ---vectors are drawn at a rate 
of 1 pixel per microsecond; ---a 1024X768 raster can be modified in 48 milliseconds; ---scrolling a 1024×768 
rectangle in any direction takes 96 milliseconds. The speed of these operations eliminates special-purpose 
hardware for scrolling, vector generation, and cursor motion. Instead, a functional frame buffer architecture 
allows generalized manipulation of raster images in a truly interactive fashion. 2. The Problem: Bandwidth 
 A flame buffer display system typically consists of an application program, a frame buffer, and a video 
monitor (Figure 2). The frame buffer contains the image in a raster (bit-pattern) representation. Two 
processes work on the frame buffer, one updating the frame buffer to change the image it represents, 
the other refreshing the video monitor. The bandwidth of the refresh process is proportional to the number 
of pixels displayed, the bits per pixel, and the refresh rate, whereas the bandwidth of the update process 
depends on the response-time requirements of ~e application. As frame buffer sizes grow because of decreasing 
memory costs, the bandwidth requirements will increase proportionally. For example, a high-resolution 
monochrome monitor with a display area of 1024 by 768 one-bit-pixels and 60 Hz non-interlaced refresh 
demands a bandwidth of approximately 64 Mbit/sec; the same bandwidth is necessary for a standard video-monitor 
with a display area of 640 by 480, four bits per pixel, and interlaced 60 Hz refresh. The frame buffer 
memory has to accomodate both processes. If the bit-map memory has an overall bandwidth of, say, 80 Mbit/sec, 
then the refresh process leaves 16 Mbit/sec for update purposes.  I uo, a e "ere'h t V0e° The ratio 
of refresh over update bandwidth indicates the number of display (refresh) times necessary to modify 
an entire frame buffer; in our case, four refresh times or 66 milliseconds. As this ratio gets larger, 
the interactiveness of the display decreases, limiting the usefulness of the display. Also, a slower 
update uses up a larger portion of the overall system resources, depleting the processing cycles remaining 
for the application program and other tasks. Ideally, one would like to update the frame buffer at the 
full available bit-map memory bandwidth. However, it is difficult to provide such a high update rate. 
Each frame buffer operation is accompanied by the overhead of generating the frame buffer address, performing 
the raster operation, masking bits that shall remain unchanged, and loop control. The effective bandwidth 
of a processor performing all of these operations thus needs to be considerably higher than the update 
bandwidth alone, and far exceeds the bandwidth of currently available microcomputers. The problem thus 
is how to achieve fast frame buffer manipulation with a processor that is limited in speed. 3. Conventional 
Frame Buffer Graphics Architectures Two flame buffer architectures can be distinguished: Those in which 
the frame buffer is part of main memory and those where it isn't. In both cases, the frame buffer is 
typically a dual port memory with one port dedicated for refresh to unload the refresh bandwidth from 
the memory bus. ~System  Bus~ Figure 3.1. Frame Buffer in Memory t :~Of [ Figure 3.2. Frame Buffer 
Isolated Program Process Process Monitor Figure 2. Model of a Frame Buffer Graphics System The frame 
buffer in main memory architecture (Figure 3.1) makes the frame buffer directly accessible to the main 
processor. However, it is almost always inefficient to manipulate the frame buffer with the main processor, 
except with processors that have special instructions for frame buffer manipulation, such as BitBlt in 
the Alto computer [1]. Thus one usually needs a peripheral "graphics processor" for high-speed frame 
buffer manipulation, which receives commands from the main processor. Because there is only a single 
memory bus, contention between the main processor and the graphics processor can result. Typically a 
significant portion of the system resources must be dedicated for frame buffer manipulation in this architecture. 
The frame buffer not in main memory architecture isolates the frame buffer from the system by placing 
the frame buffer under exclusive control of a dedicated graphics processor (Figure 3.2). The graphics 
processor provides a standard set of frame buffer manipulation functions, receiving commands and data 
from the main processor. An independent graphics subsystem has a number of advantages. First, with a 
sufficient rich set of graphics primitives, demands on system resources are greatly reduced. Second, 
it is straightforward to meet given update bandwidth requirements with a specially-designed controller. 
Third, a self-contained graphics subsystem can be interfaced to a variety of host computers. For all 
of these reasons, most commercial high-performance raster-scan systems have adopted this architecture. 
A difficulty introduced by the memory partitioning is the limited memory space in the graphics subsystem. 
Almost all frame buffer applications require memory space in addition to the visible bit-map, to store 
character sets, graphical objects, and so forth. If main memory were used for this purpose, data has 
to be moved across the graphics processor interface, putting load back on the main processor system. 
This problem can partly be solved by making the frame buffer larger than the visible display area, and 
to use the excess, invisible part as a software controlled cache for graphical objects such as the currently 
used character fonts. With the cache, communication bandwidth between host and graphics processor is 
reduced to the remaining miss-rate. In the context of a low-cost microcomputer system, both architectures 
have the disadvantage to require a graphics processor, typically a bit- slice, microprogrammable machine. 
Another, more subtle, problem introduced by a graphics processor is the constraint of predefined frame 
buffer operations. It will be difficult, if not impossible, for users to extend the graphics processor 
firmware. This is less a problem for well-defined applications such as vector drawing, but it limits 
the exploration of novel frame buffer operations (for example area fill, greyscale backgrounds, and brushes), 
as discussed in Newman and Sproull [1]. 4. The Functional Frame Buffer Architecture An alternative approach 
to speed up frame buffer updating is to provide a more appropriate representation for raster manipulation 
at the hardware level. 4.1. Pixel Addressing If the bit-map memory is organized just as other memory 
into physical memory words, the task of mapping the pixel-addressing into physical words has to be done 
in software or firmware, a constant overhead for every frame buffer access. This overhead can be avoided 
if the frame buffer is (x,y) addressable. Since the desired frame buffer manipulation bandwidth can only 
be reached by working on multiple pixels in parallel, we shall access as many bits as the data-paths 
in the system can transfer, and have a facility to specify fewer bits if so desired. In our implementation, 
a 1×16 rectangle can be accessed at an arbitrary (x,y) location. An alternative solution, implemented 
by Sutherland and Sproull, is to access 8×8 rectangles [3]. 4.2. Pixel Operation For a one-bit per pixel 
frame buffer, a small set of boolean operations suffices for the actual frame buffer bit-change operation: 
Bit-Move, Bit-Or, Bit-And, and Bit-XOR, whereby the new data can be optionally inverted [1,2]. Executing 
this operation locally to the frame-buffer in a single read-modify-write memory cycle gains at least 
a factor of two in bandwidth over doing so in the main processor with separate read-write cycles. The 
source data is either supplied from the main processor or is accessed in an earlier frame buffer memory 
cycle. 4.3. Control Control of frame buffer manipulation is retained by the main processor in a completely 
user-programmable fashion. When the raster operation and the width field have been set, the only remaining 
task is to provide (x,y) addresses and to transfer data to or from the fiame buffer. Making the (x,y) 
address registers independent windows in the processors address space, rectangular graphical objects 
can be accessed using autoincrementing addressing modes or DMA devices. 5. Implementation We have implemented 
a functional frame buffer architecture with a frame buffer size of 1024X1024 Bits or 128 kBytes. The 
aspect ratio can be reconfigured for a variety of visual display sizes such as 1170×896 or 1024×768. 
In the latter case, invisible frame buffer space can be use for font storage. Memory planes can be stacked 
to provide up to eight bits per pixel.  5.1. Memory Organization The 1024X1024 bits of memory are stored 
in 64 16k dynamic RAMs. This memory supports two organizations: for refresh purposes it consists of 16k 
64-bit physical memory words which are aligned along horizontal scan lines. For update purposes, each 
64-bit physical memory word is fiarther divided into four 16-bit logical memory words, since the data 
paths are only 16 Bits wide (Figure 5.1.a). For refresh cycles, all 64 RAM chips are read out in parallel 
into a 64 Bit buffer, whose content is shifted out to the video monitor (Figure 5.Lb). The timing of 
video refresh cycles depends on video bandwidth: high-resolution non-interlaced monitors require one 
refresh cycle every microsecond. Note that dynamic RAM refresh is done automatically by the video refresh. 
I °, ....... ,,,,,,;sl Figure 5.1 .a I I I I I I I I I I I I I IL 2 ............ , Memory Organization 
 [4~ i i i i I I | i i i i i I Figure 5.1.b Refresh Cycle   IflllllllllllllllllllllllllllllllllitNi 
I I ! f I I I I I I f I I 161 Figure 5.1 .c I1,~ .... flllllllllllllllllfllllllllllllllll~fli Intraword 
Access HIIIIIIIIIIIIIIIIIII,,,, .... ,~i I I I I I ] I I I I I I I I P, =1 ~lllll$111111111111111Ullllllllllllllll,,, 
1t I17 .... ,, , ..... 3,11 Figure 5.1 .d Interword Access ~11 I I" I I.....I I l I  illll,imlll l 
 5.2. Update Cycles Update cycles are read or read-modi~-wtite cycles that alternate with refresh cycles. 
For bit-string addressing, 16 consecutive bits must be accessed starting on any bit boundary. Memory 
accesses can thus cross logical and physical word boundaries. The data is then aligned with a 16-bit 
shifter to appear normalized at the processor interface. To access 16 bits within a physical word (Figure 
5.1.c), the appropriate memory chips are selected individually by controlling the row-address-strobe 
of each chip a function of the bit-address. All RAM chips receive the subsequent column-address-strobe, 
but only those selected with RAS will be enabled. The data-out from all logical words is ORed together 
before it is put on the data bus. If an update crosses a physical word boundary (Figure 5.1.d), the memory 
chips in the next physical memory word must receive an address one higher than the other RAM chips. This 
is implemented by dividing the memory in two banks with separate address drivers (Figure 5.2). An adder 
increments the address for the lower bank by one if the access starts in the higher bank. Adder Mux/Driver 
Memory Word Address<O:13> Bank 0 RASO..31 I ~ Bank I 1 Decoder I RAS 32..63 I Bit Address<O:5> )1 IT"AS 
0..63 Figure 5.2. Memory Addressing 5.3. Function Unit The function unit performs the actual bit-change 
operation in a single read-modify-write cycle. The bit-change operation is applied bit-wise to the source 
data and the old destination value to yield a new destination value. The source data can be either supplied 
by the main processor or by the frame buffer from a previous read cycle. The set of functions are: Clear 
Dst ~-0 Copy Dst q-Src Paint Dst ,~ Dst OR Src Erase Dst ~-Dst AND NOT Src Invert Dst ~-Dst XOR Src Clear/ 
Dst q-1 Copy/ Dst *-NOT Src Paint/ Dst ~-Dst OR NOT Src Erase/ Dst ~-Dst AND Src Invert/ Dst * Dst XOR 
NOT Src The function unit is implemented as a programmable logic array (partitioned into four chips). 
The function has one more input, MASK, that inhibits the modification of the corresponding destination 
bits, causing these bits to be rewritten with their old value. This is necessary if we desire to change 
fewer than 16 Bits at one time, for example when drawing vectors. The MASK is computed from the value 
of the WIDTH register and the destination address. The relation of the function unit to the rest of the 
graphic system is shown in Figure 5.3. Data-Out Video (16) aq t64! f Bit Address Address Bus ~ Word-Adrs. 
Data Bus (16) Sync ) Figure 5.3. Data Paths in the Functional Frame Buffer   
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1980</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>807467</article_id>
		<sort_key>48</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-1980</article_publication_date>
		<seq_no>8</seq_no>
		<title><![CDATA[Simulation and expected performance analysis of multiple processor Z-buffer systems]]></title>
		<page_from>48</page_from>
		<page_to>56</page_to>
		<doi_number>10.1145/800250.807467</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=807467</url>
		<abstract>
			<par><![CDATA[<p>The results of expected performance analysis and simulation of three multiple processor Z-buffer architectures are presented. These architectures have been proposed as approaches to applying many processors, working in parallel, to the task of rapidly creating shaded raster images. These architectures are attractive since they offer potentially high performance, in terms of image update rate, at modest cost. All three approaches make use of multiple instances of identical processor modules.</p> <p>The analyses and simulations indicate that substantial gain is possible by applying multiple processors to the task. But, as more and more processors are added, additional gains in performance become smaller and smaller. This result suggests optimal system sizes. The performance of these architectures depends on the processors used, the number of processors, and certain characteristics of the image environments. Each architecture has its own performance characteristics and limitations.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.3</cat_node>
				<descriptor>Bitmap and framebuffer operations</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>D.4.8</cat_node>
				<descriptor>Simulation</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>C.4</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10002944.10011123.10011674</concept_id>
				<concept_desc>CCS->General and reference->Cross-computing tools and techniques->Performance</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341.10010370</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Simulation evaluation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10011007.10010940.10010971.10010972.10010978</concept_id>
				<concept_desc>CCS->Software and its engineering->Software organization and properties->Software system structures->Software architectures->Simulator / interpreter</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010520.10010575</concept_id>
				<concept_desc>CCS->Computer systems organization->Dependable and fault-tolerant systems and networks</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003033.10003079</concept_id>
				<concept_desc>CCS->Networks->Network performance evaluation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944.10011123.10011674</concept_id>
				<concept_desc>CCS->General and reference->Cross-computing tools and techniques->Performance</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Performance</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP14111766</person_id>
				<author_profile_id><![CDATA[81100304539]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Frederic]]></first_name>
				<middle_name><![CDATA[I.]]></middle_name>
				<last_name><![CDATA[Parke]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Computer Engineering, Case Institute of Technology, Case Western Reserve University, Cleveland, Ohio]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>356626</ref_obj_id>
				<ref_obj_pid>356625</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[I. E. Sutherland, R. F. Sproull, R. A. Schumacker, "A Characterization of Ten Hidden Surface Algorithms," Computing Surveys, March 1974.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>5532</ref_obj_id>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[W. M. Newman and R. F. Sproull, Principles of Interactive Computer Graphics, McGraw-Hill, 2nd Ed., 1979.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>810237</ref_obj_id>
				<ref_obj_pid>800179</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[H. Fuchs, "Distributing a Visible Surface Algorithm Over Multiple Processors," Proc. ACM '77, Seattle, Oct. 1977.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>802893</ref_obj_id>
				<ref_obj_pid>800090</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[H. Fuchs and B. Johnson, "An Expandable Architecture for Video Graphics," Proc. 6th Symp. on Comp. Arch., April 1979.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[F. I. Parke, "A Parallel Architecture for Shaded Graphics," Tech. Rep., Computer Engineering Dept., Case Western Reserve Univ., Jan. 1979.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>360802</ref_obj_id>
				<ref_obj_pid>360767</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[I. E. Sutherland and G. W. Hodgeman, "Reentrant Polygon Clipping," CACM, June 1974.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_obj_id>359869</ref_obj_id>
				<ref_obj_pid>359863</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[F. C. Crow, "The Aliasing Problem in Computer Synthesized Shaded Images," CACM, Nov. 1977. Also Tech. Rep. UTEC-CSc-76-015, Univ. of Utah, March 1976.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[F. I. Parke, "Performance Analysis of Z-buffer Convex Tiler Based Shaded Image Generation," Tech. Rep. CES 79-15, Computer Engineering, Case Institute of Technology, Oct. 1979.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_obj_id>811762</ref_obj_id>
				<ref_obj_pid>800292</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[F. I. Parke, "An Introduction to the N.mPc Design Environment," Proc. 16th Design Automation Conf., San Diego, June 1979.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 SIMULATION AND EXPECTED PERFORMANCE ANALYSIS OF MULTIPLE PROCESSOR Z-BUFFER SYSTEMS Frederic I. Parke 
 Computer Engineering Case Institute of Technology Case Western Reserve University Cleveland, ABSTRACT 
-The results of expected perfor- mance analysis and simulation of three multiple processor Z-buffer architectures 
are presented. These architectures have been proposed as approaches to applying many processors, working 
in parallel, to the task of rapidly creating shaded raster images. These architectures are attrac- tive 
since they offer potentially high performance, in terms of image update rate, at modest cost. All three 
ap- proaches make use of multiple instances of identical processor modules. The analyses and simulations 
indicate that substantial gain is possible by ap- plying multiple processors to the task. But, as more 
and more processors are ad- ded, additional gains in performance be- come smaller and smaller. This result 
suggests optimal system sizes. The per- formance of these architectures depends on the processors used, 
the number of proces- sors, and certain characteristics of the image environments. Each architecture 
has its own performance characteristics and limitations. CR Index -8.2, 8.1, 6.22 Introduction An increasingly 
popular approach to shaded graphics is the image or frame buffer. The frame buffer system typically consists 
of a large memory, to hold the intensity information for the pixels comprising the image, and the necessary 
logic and circuitry to scan this memory repeatedly to drive standard video devices such as television 
monitors. The popular- ity of these systems is directly related to cost. As the cost of large memories, 
based on integrated circuit technology, Permission to copy without fee all or part of this material 
is granted provided that the copies are not made or distributed for direct.commercial advantage, the 
ACM copyright notice and the title of the publication and its date appear, and notice is given that copying 
is by per~Isslon of the Association for Computing Machinery. To copy otherwise, or to republish, requires 
a fee and/or specific permission. 01980 ACM 0-89791-021-4/80/0700-0048 $00.75 Ohio 44106 has decreased, 
the cost of frame buffe~ systems has also decreased and continues to decrease. The frame buffer decouples 
the gen- eration of shaded images from the display of the images. Frame buffer systems usu- ally rely 
on a host computer system to execute software implemented algorithms to compute the desired pixel values 
using little or no specialized computational hardware. These systems are very flexible and cost effective 
for many applications, but offer low performance for applications requiring rapid image update. The 
frame buffer concept can be ex- panded to include depth or Z information for each picture element. This 
expanded form is usually referred to as a Z-buffer. Use of a Z-buffer eliminates the need for explicit 
visible surface algorithms [i] when dealing with three-dimensional en- vironments. As pixel values are 
written into the Z-buffer, their depth values are compared with the current depth values in the buffer. 
If the depth value of the pixel being written is less than (closer than) the current buffer value, the 
buffer value is replaced with the new value. In most instances, the Z-buffer is interfaced directly 
to a host computer system as shown in Figure i. All computa- tion necessary to determine the required 
pixel values is done by programs executing on the host machine. This computation typically includes accessing 
data bases, user interaction, coordinate system transformations, clipping, and scan conversion [2]. Scan 
conversion is the process of determining the pixel values to be written into the Z-buffer based on the 
transformed, clipped environment descrip- tion primitives. The description primi- tives may be polygons, 
surface patches, etc. The scan conversion process will likely require a significant amount of computation 
on the host machine due pri- marily to the large number of pixel values that must be determined. The 
scan conversion task can be off- loaned from the host processor by includ-  ing as part of the Z-buffer 
system a dedi- cated process which accepts description primitives from the host machine, does the scan 
conversion, and outputs pixel values to the Z-buffer memory as shown in Figure 2. The addition of the 
dedicated proces- sor can significantly increase the perfor- mance of the Z-buffer system in terms of 
image update rate. Cost considerations provide a strong temptation to implement this dedicated processor 
as a microproces- sor. However, the performance of current microprocessors, relative to needed per- formance, 
leaves something to be desired. The combination of cost considerations and performance objectives motivate 
the inves- tigation of implementing the scan conver- sion process as a set of microprocessors operating 
in parallel. Use of Multiple Microprocessors There are two basic concepts underly- ing the application 
of multiple processors to the scan conversion process. The cen- tral idea is to effectively distribute 
the scan conversion task across the set of processors. Figure 3 illustrates this basic concept. The host 
system provides, as input, description primatives, such as polygons, which have been clipped and transformed 
into the screen coordinate system. This input is "distributed" to a set of identical image processors 
which perform the scan conversion. Output from the image processors, pixel values, is written to the 
Z-buffer memory. The pri- mary question of interest is how best to accomplish this distribution. Three 
ap- proaches to this distribution problem are described below. In fact, the main dis- tinction between 
these architectures is the way in which each handles this distri- bution. The use of multiple image 
processors introduces a potential problem. If a number of processors are attempting to access a single 
memory, in this case the Z-buffer memory, there is likely to be memory contention. The approach taken 
by each of these architectures is to avoid this memory contention problem bv seament-  ing the Z-bu£fer 
memory. Each segment of the Z-buffer is associated with a single image processor. Each processor accesses 
only the memory segment associated with it as shown in Figure 4. This pushes the memory contention problem 
back to the video generator. However, this is not a problem since the video generator accesses only a 
single pixel at a time and makes these accesses in a regular, predictable sequence. Descriptions of 
the Three Architectures The architectures considered are one proposed by Fuchs[3,4], one proposed by 
Parker5], and a hybrid of these two. The name associated with each reflects the distribution mechanism 
used. There are a number of similarities common to these architectures. All distribute the scan conversion 
task across a set of proces- sors. All make use of the segmented Z- buffer memory concept. All can use 
essen- tially the same image processor. The differences between approaches center on how the scan conversion 
process is distri- buted and on the details of Z-buffer memory segmentation. Central Broadcast Controller 
-This approacS, propose~-6y-FucSs, uses a cen- tral broadcast controller to distribute the input data. 
The Z-buffer memory is segmented in an interlace pattern. Several possible interlace patterns are shown 
in Figure 5. The information describing each image primitive, e.g. polygon, is broadcast to all image 
proces- sors via a common bus from the central broadcast controller. The controller waits until each 
image processor has com- pleted processing for that image primitive and then broadcasts the next image 
primi- tive. This process continues until the image is complete. The interlace pattern assures that each 
image processor will produce pixel values for each image primi- tive broadcast. The structure of a four 
image processor broadcast system is shown in Figure 6. ACCESSING DATA BASES, USER I ACCESSING DATA 
BASES, USER 1 HOST INTERACTION, COORDINATE INTERACTION, COORDINATE HOST IT~NSEOR~TIONS, ETC. 1 TRANSFORMATIONS, 
CLIPPING, ETC. CPU CLIPPING,  4 ~ ROST J Distribution P ...... J I ~ I DEDICATED SCAN CONVERSION PROCESS 
SCAN CONVERSION PROCESS PROCESSOR Memory I-- Z -DUFFER MEMORY 1 ,ideo Geo. I--~<~ GENERATOR C VIDEO 
GENERATOR Figure 3 Figure l Figure 2 HOST [ Otstribution P ...... I i Figure 4 Splitter Tree -This approach, 
pro-  posed~Hy--P~r~ uses a tree of region splitters to distribute the input data to the various image 
processors. For polygo- nal environments, each region splitter is a slightly modified single plane 
polygon clipper of the variety proposed by Suther- land and Hodgman [6]. Each splitter divides its 
input region in half, output- ing image primitives to one half or the other and in some cases splitting 
them to go to each half. The output from each half is treated as input to the next level of splitters. 
The effect is to subdivide the screen space into regions, the content of each region forming the input 
to ~ sin- gle image processor. The Z-buffer memory is similarly divided into segments, each segment 
corresponding to a region of the screen. The structure of a four image processor splitter system is 
shown in Fig- ure 7. The number of image processors to be employed determines the number of splitters 
and splitter tree levels neces- sary. In general 2^n image processors will require 2^n-i splitters arranged 
in n levels. Each splitter executes exactly the same algorithm so all splitters can be implemented as 
identical hardware modules. H[brid -This is a combination of the splitter and broadcast approaches. 
Input data is first divided into screen regions using a splitter tree. Output from the lowest level of 
the splitter tree for each region is distributed to a set of image processors for that region as in the 
broadcast approach. A four image proces- sor structure of this type is shown in Figure 8. Ex~££t£d Performance 
Analysis Before implementing a system based on any of these approaches, an analysis of expected performance 
is in order. These architectures are not fundamentally res- tricted in the type of image primitive they 
can handle, however, the performance analysis and subsequent simulations are for environments described 
in terms of convex polygons. Two Processors Three Processors Eight Processors alblalb a d h c a a e 
g b . . . . . . . glalelclg]a .... I lelt C C b f b f m=4 ----m=4 d d g c g C b b ~hd~- e a i- -111 
Figure 5 -Interlace Patterns C Eltpped Polygon Source vertex data  1 ,roa0cast cootro.er I done signal 
 , 1 T II I Vide° Generator Figure 6 -Fuchs Approach In order to do a performance analysis, we need 
a model for the image processors used. The same image proces- sor, with minor changes, may be used in 
all three architectures. We make the assumption that each image processor is executing a scan conversion 
algorithm similar to one described by Crow [7]. For each polygon, the vertex with maximum Y value is 
found. From this ver- tex the right edge and left edge of the polygon are determined. From scan line 
to scan line the left and right edge values are computed incrementally. Across each scan line, from left 
edge to right edge, the pixel values are computed incremental- ly. If the scan conversion is done using 
only a single processor, the total time, Tt, to scan convert an entire image is given by the following 
equation. Tt = Pn*Pt + Sn*St + En*Et + Gn*Gt where Pn = number of pixels computed Sn = number of scan 
line segments En = number of polygon edges Gn = number of polygons  clipped polygon source 1 L clipped 
polygon so.rce 1 vertexd a t.L..__ vertex d a t.~.~._ broadcast C~O litter br°adcast 1!o~ IN-'I S 
P °11erJ   ~:~>L~_~ntr Figure 7 -Splitter Approach and Pt = computation time per pixel St = computation 
time per segment Et = computation time per edge Gt = computation time per polygon Pt, St, Et, and Gt 
are determined by the basic computational speed of the processor and details of the scan conversion algo- 
rithm while Pn, Sn, En, and Gn are deter- mined by the image environment. Pn = Xres * Yres * Dc where 
Xres and Yres specify the size of the pixel array and Dc is the average depth complexity of the image, 
i.e. how many times, on the average, each pixel is written. For 4-sided polygons Sn = Oc * Gn * v~ 
 and En = 4 * Gn where n = Pn/Gn and Oc is an orientation correction term estimated to be 1.2. Making 
the appropriate substitutions gives the following expression for total scan conversion time. Tt = Pn*Pt+l.2* 
~/~* v~*St+Gn*(4*Et+Gt) Gn and Dc are the independent vari- ables which characterize the image en- vironment. 
Therefore, for a given algo- rithm executing on a given processor, the total scan conversion time depends 
only on Gn and Dc. A more detailed discussion of this analysis is presented in [8]. What are the values 
of Pt, St, Et, and Gt? Analysis of a typical scan conversion algorithm indicates the follow- ing values: 
 ' Video Generator " ~ Figure 8 -Hybrid Approach Pt = 9 St = 61 Et = 62 Gt = 71  where the values refer 
to the number of basic machine operations such as add, sub- tract, increment, test, etc. Operations such 
as multiply and divide are assumed to take 10 times as long as the basic operations. Actual execution 
times depend on how fast a given processor executes. Given this analysis of a single pro- cessor scan 
converter, what is the expect- ed performance of multiple processor sys- tems? As we shall see, the answer 
depends on the specific structure of the system. Central Broadcast Controller -The total scan convers{on 
time-is the maximum of the scan conversion times of the indi- vidual image processors. For each proces- 
sor, the scan conversion time is computed as for a single processor version except that the values for 
Pn and Sn are--dr{= ferent. Note that En and Gn are the same as for the single processor case. Pn depends 
on the number of processors and Sn depends on the number of scan lines in the interlace pattern. Pn(np) 
= Pn/np Sn(m) = Sn/m  where np is the number of image processors and m is the number of scan lines 
in the interlace pattern. The pixel computations are distributed approximately equally across the processors. 
The segment compu- tations are distributed based on the image interlace pattern. If interlace depends 
only on column position, the segments do not get distributed, each processor deals with all segments. 
If, however, interlace depends entirely on row position maximum segment distribution occurs. Splitter 
Tree -Neglecting for a moment-- the--sp~{~ing computations, the total scan conversion time is the maximum 
of the scan conversion times for the indi- vidual image processors. The time for each processor is computed 
as for the sin- gle processor case except that the values for Pn, Sn, En, and Gn are different. The values 
for Pn, Sn, En, and Gn depend on the number of levels in the splitting tree. Pn(1) = Pn/(2^levels) 
Gn(1) = N*(2^(-levels) + 2*Ew) En(1) = 4 * Gn(1) Sn(1) = Oc * v~6(1) * where N is the number of polygons 
Input to the splitter tree. Ew is a term which takes into account the additional polygons created by 
the splitting process. Ew is the expected polygon width (Ew = ~/D6/v~). Actually, the total scan conversion 
time is the maximum of the image processor times and the splitter tree computation time. Since the first 
splitter in the tree deals with the most complex environ- ment, its performance determines th% com- putation 
time for the splitter tree. The computation time for the first splitter depends on the speed of the splitter 
pro- cessor and the overall image environment. For an environment consisting of uniformly distributed 
4-sided polygons, splitter time is expressed as Splitter Time = 4*Pn*(Vt÷Ew*IVt) where Pn is the total 
number of polygons in the image, Vt and IVt are the vertex handling and "induced" vertex computation 
times of the splitter. Plot 7 shows the relationship between Pn, Dc, and splitter time where the values 
of Vt and IVt are 50 microseconds and 200 microseconds. This implies that for a given environment there 
is a crossover point, in terms of the number of image processors, beyond which the splitter tree time 
dominates the scan conversion time. Plot 8 illustrates this crossover which also applies to the hybrid 
 systems described below. Hybrid System -The hybrid approach has cHa{acte--~[s{[cs of both the splitter 
and broadcast systems. Pn(r) = Pn/(2^levels) Pn(h) = Pn(r)/NPR Gn(h) = N* (2^ (-levels) + 2*Ew) En(h) 
= 4 * Gn(h) Sn(h) = (Oc * ~/P~) * ~/~(h))/m where NPR is the number of image proces- sors per region, 
Pn(r) is the number of pixels per region, and m is the number of scan lines in the region interlace 
pat- tern. Given this analysis and the perfor- mance characteristics of a given image processor it 
is possible to predict the performance of multiple image processor systems. Note that the foregoing 
analysis assumes that the polygons are uniformly distributed throughout the screen space. Simulations 
 In order to gain additional ihsight into the operation of these systems, to validate the performance 
analysis, and to obtain realistic performance data, simula- tions of several multiple image processor 
systems have been developed. These simu- lations were developed using the N.mPc design environment [9]. 
This environment allows simulation and performance evalua- tion of systems consisting of multiple communicating 
processors. The developed simulations include all required interpro- cessor control signals, data transfers, 
and execution timing. The simulated image processors are modeled as 16-bit microprocessors of the Z8000, 
I8086 class executing a slightly different scan conversion algorithm than the one outlined above. One, 
two, and four image processor systems for both the broadcast and splitter architectures have been simulated. 
The results of these simulations are summarized in Tables 1 thru 3. Table 1 lists the parameters of 
five test cases used as input to the simula- tions. Using these parameters and the resulting total scan 
conversion times from a single processor simulation the follow- ing values for Pt, St, Et, and Gt were 
determined. Pt = 20 microseconds St = 225 microseconds Et = 250 microseconds Gt = 30@ microseconds 
Table 2 shows the scan conversion times for the five test cases for various broadcast system configurations. 
The column headings indicate the interlace pattern used. The first number refers to the number of rows 
in the pattern while the second refers to the number of columns in the pattern. The product of rows and 
columns is the number of image processors used. Table 3 lists the scan conversion times using splitter 
systems. The 1 level version splits about a vertical splitting  Table l Test Case Dc Gn En Sn Pn Total 
Time (millisec) .18 l 3 40 1496 41.4 .66 3 lO 144 5399 145.2 1.18 18 64 350 9670 295.2 .50 16 56 333 
4068 177.0 .13 5 20 80 I030 46.6 Table 2 Test Case interlace pattern Ixl Ix2 2xl Ix4 2x2 4xl 41.4 26.9 
22.4 18.2 13.7 11.5 145.2 93.5 77.3 64.2 48.1 39.9 295.2 202.7 163.3 149.7 llO.4 91.5 177.0 141.2 I03.7 
I15.5 78.1 59.6 46.6 37.4 28.4 31.1 22.2 17.8 (time in millisec) Table 3 splitter levels Test Case 0 
l(v) 2(vh) 41.4 27.7 15.5 145.2 91.9 48.4 295.2 190.3 78.7 177.0 90.7 46.5 46.6 28.6 19.2 (time in millisec) 
 plane. The 2 level system splits first vertically and then horizontally. The 64 x 128 pixel images 
produced by the simulations for four of the test cases are shown labeled as Test Cases 1 thru 4. Results 
 Based on the performance values ob- tained from the simulations and on the performance analysis presented 
above, plots i thru 8 indicate expected perfor- mance parameters for systems with large numbers of image 
processors. Plot 1 shows the relationship between total pixel time, depth complexity and the number 
of image processors used. The number of pixels computed depends only on image depth complexity and image 
resolu- tion. The time scale on the left side is for 640x480 images. The time scale on the right side 
is for 320x240 images. The insert shows the data for 16, 32, 64, and 128 processor systems with an expanded 
scale. Doubling the number of image pro- cessors halves the pixel time. This plot applies to broadcast, 
splitter, and hybrid systems. Plot 2 shows the relationship between total segment time, depth complexity, 
and the number of image processor for splitter systems. As for the previous plot, the left side times 
are for 640x480 images and the right side times are for 320x240 im- ages. Plot 3 shows the relationship 
between  total segment time, depth complexity, and scan lines in the interlace pattern for broadcast 
architectures. Plot 4 shows the relationship between total polygon time, depth complexity, and the number 
of image processors for splitter architectures. The polygon time shown here includes the edge time. For 
Plots 2, 3, and 4, the image is composed of 1024 uniformly distributed polygons. Plot 5 shows the total 
scan conver- sion time for splitter systems. The times shown are simply sums of the component times shown 
in Plots i, 2, and 4. The performance difference between a single processor system and an 8 or 16 processor 
system is quite significant. However, the difference between a 16 processor system and a 128 processor 
system is not nearly as significant unless the performance of the 128 processor system is required. 
Plot 6 shows the total scan conver- sion time for broadcast systems superim- posed with the scan conversion 
time for splitter systems. The broadcast times are for systems with approximately square interlace patterns. 
The polygon overhead base line shown is dependent on the number of polygons and edges in the image. This 
polygon overhead accounts for much of the expected performance difference between broadcast and splitter 
systems with large numbers of image processors. Plot 8 shows the expected performance relationship between 
broadcast, broadcast with "smart" central broadcast controller, splitter, and hybrid systems. The hybrid 
systems have 4 image processors for each region resulting from the splitting opera- tions. For a 4 processor 
system, the broadcast and hybrid systems are the same. As the number of image processors is in- creased 
the hybrid system performance approaches that of the splitter system. Limitations The broadcast approach 
suffers from the fact that each processor must deal with every polygon and every edge. This results in 
performance which approaches a polygon-edge overhead value as additional processors are added rather 
than approach- ing zero. The overhead value depends on the number of polygons and edges in the image 
environment (see Plot 7). 640x480 320x240 640x480 320x240   I! 6- DC=4 3 20' \ Pt = 2O~.~sec 1.5, 
\ l-# .4 .50 .25 5 Dc=l .25 15" l.O ~4 '2 Dc=2 .2 -3 ~g 8 ~ lo- .5 'IDc=l i 16 32 64 128 1 2 4 8 
16 32 64 128 l 2 ~ ~ l~ 32 6A ~28 number of processors number of image processors lO-1.5  \ I024 polygons 
"4" ~ i l.O ~6 Polygon and Edge Time for 0c= 1 Splitter Architecture ~4 .5 ¸ 1024 four sided Dc=4 
1 2 A 8 16 32 64 128 l Z 4 8 16 32 64 128 Number of scan lines in the interlace pattern (m) Number of 
image processors Oo~A\ 2-\\ \ 2 \\ zg o 20 I0 ~\ \ x~ \ I "-~"-.. "l.gs \ \ S 640"60 \ ~~ Total Time 
for F uchs I024 polygons \~ \" ~._~ and Splitter \ ~,~ Architectures polyglnilerhead ~ ~a~u~e ~  I 
2 4 8 16 32 64 128 Number of image processors 1 2 4 8 16 32 64 128 Number of image processors 2.0- \ 
l 1°tsJ Time in Seconds \\\ \\ \ TotoITioe 102Apol.ons 6AOx480 Nuobe~ of p .......... 4" 21ove~ l.O Oc=l 
x~~~Splitter Time 500 1060 2000 Number of Polygons 5000 Spl~er o~eo/''------____-'~8~brid , , , , . 
4 8 16 32 64 128 256 Number of image processors , 512 One way to reduce this overhead limi- tation is 
to move the polygon and edge computations from the image processors into the central broadcast controller. 
For this "smart" central broadcast con- troller version, performance is determined by the maximum of 
the broadcast controller and the image processor times. The result, for fairly complex environments, 
is increased performance for systems with few image processors but only slightly increased performance 
for systems with many image processors. The controller time is constant for a given environment while 
the image processor time depends on the number of processors working. As the number of image processors 
is increased, a cross over point is reached where the con- troller time dominates the image processor 
times. This is illustrated in Plot 8. The splitter architecture depends on a uniformly distributed image 
environment. If the environment is not uniformly dis- tributed, it is possible to saturate a branch of 
the splitter tree while the oth- er branches are idle. The worst case is when all image description primitives 
lie in a single region. For this case only one image processor will be active, the rest remaining idle. 
 These limitations of the broadcast and splitter approaches motivated the hybrid approach. The hybrid 
approach lowers the number of polygons and edges broadcast to a given image processor which therefore 
lowers the overhead value. Also, the hybrid approach retains the broadcast approach immunity to the en- 
vironment distribution problem within each region. Care must be exercised in choosing  the interlace 
pattern used with the broad- cast architecture. If one chooses a pat- tern that is based only on column 
posi- tion, the only term in the total time expression that decreases as processors are added is the 
pixel term. If one chooses a pattern that is based only on row position, then as processors are added 
 the segment time term will have maximum reduction in addition to pixel time term reduction. However, 
choosing an interlace pattern based only on column position or based only on row position is less than 
 optimal. The broadcast approach attempts to equally distribute the pixel computa- tions across the 
image processors. This is best accomplished by using an interlace pattern that depends both on row 
and column position. Z-buffer systems suffer from a funda- mental inability to deal with the aliasing 
problem [7]. The systems outlined above are no exception. The various image arti- facts due to aliasing 
will be present in the images produced by these systems. Test Case l Test Case 2 ! ii iii iiiiilRii?...: 
 iiiiiiiiiiiiiiiiiiiiiiiiiiiiiii!iiiii!iiiiiii i Test Case 3 7 Test Case 4 Conclusion The expected 
performance plots indi- cate that, for the microprocessor based image processors simulated, optimal system 
size is somewhere between 16 and 64 image processors. This assumes 640x480 pixel images with °a depth 
complexity of 1 com- posed of about a 1000 polygons. For lower resolution, fewer polygons, or lower depth 
complexity, fewer processors might be suf- ficient. For higher resolution, more polygons, or higher depth 
complexity, more processors might be justified. An important consideration in choos- ing the number 
of image processors is overall system balance. Ideally, scan conversion performance should match the 
performance of the host machine. Scan conversion should be as rapid as the host can supply image primitives. 
However, providing scan conversion performance which is faster than the host machine is not justified. 
 The hybrid approach is perhaps the optimal architecture. It uses a shallow splitter tree to reduce the 
number of polygons and edges each image processor handles while it maintains the environment independence 
of the broadcast approach within each region. Ac kn owl ed~ement s The assistance of the following 
individuals is greatly appreciated: Prof. Henry Fuchs at the University of North Carolina, Jay Beck at 
Tektronix Labs, Mark Boenke and Greg Ordy at CWRU. References [1] I. E. Sutherland, R. F. Sproull, 
R. A. Schumacker, "A Characterization of Ten Hidden Surface Algorithms," Computin~ Su[ve[s, March 1974. 
 [2] W. M. Newman and R. F. Sproull, Principles of Interactive Computer [3] H. Fuchs, "Distributing 
a Visible Surface Algorithm Over Multiple Pro- cessors," Proc. ACM "77, Seattle, Oct. 1977. [4] H. Fuchs 
and B. Johnson, "An Expand- able Architecture for Video Graph- ics," Proc. 6th Symp. on Comp. Arch., 
April 1979. [5] F. I. Parke, "A Parallel Architecture for Shaded Graphics," Tech. Rep., Compoter Engineering 
Dept., Case Western Reserve Univ., Jan. 1979. [6] I. E. Sutherland and G. W. Hodgeman, "Reentrant Polygon 
Clipping," CACM, June 1974. [7] F. C. Crow, "The Aliasing Problem in Computer Synthesized Shaded Images," 
CACM, Nov. 1977. Also Tech. Rep. UTEC-CSc-76-015, Univ. of Utah, March 1976. [8] F. I. Parke, "Performance 
Analysis of Z-buffer Convex Tiler Based Shaded Image Generation," Tech. Rep. CES 79-15, Computer Engineering, 
Case Institute of Technology, Oct. 1979. [9] F. I. Parke, "An Introduction to the N.mPc Design Environment," 
Proc. 16th Design Automation Conf., San Diego, June 1979.  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1980</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>807468</article_id>
		<sort_key>57</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-1980</article_publication_date>
		<seq_no>9</seq_no>
		<title><![CDATA[Trends in high performance graphic systems(Panel Session)]]></title>
		<page_from>57</page_from>
		<doi_number>10.1145/800250.807468</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=807468</url>
		<abstract>
			<par><![CDATA[<p>Accompanying the rapid development of integrated circuit fabrication technology has been a parallel, but slower, development of IC design techniques and systems. Recent approaches to IC design enable individual designers to consider developing their own VLSI circuits. Such capability may open the door to a more varied set of system designs than could previously be considered in most design environments.</p> <p>The panel members are all currently designing graphic systems oriented towards VLSI implementation. Each will give a short presentation describing one of their system designs. Following these presentations, three members of the previous session (Forest Baskett, Andreas Bechtolsheim, and Fred Parke) will discuss common issues, problems, and future prospects.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.2</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010520.10010521.10010542.10011714</concept_id>
				<concept_desc>CCS->Computer systems organization->Architectures->Other architectures->Special purpose systems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010520.10010521.10010542.10011714</concept_id>
				<concept_desc>CCS->Computer systems organization->Architectures->Other architectures->Special purpose systems</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Performance</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP39076519</person_id>
				<author_profile_id><![CDATA[81339500019]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Henry]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Fuchs]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of North Carolina at Chapel Hill]]></affiliation>
				<role><![CDATA[Chairman]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14061541</person_id>
				<author_profile_id><![CDATA[81413600319]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[D.]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Cohen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Southern California]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P31333</person_id>
				<author_profile_id><![CDATA[81100302448]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Bob]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sproull]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Carnegie-Mellon University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39074901</person_id>
				<author_profile_id><![CDATA[81332493735]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Jim]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Clark]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Stanford University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14111765</person_id>
				<author_profile_id><![CDATA[81100304539]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Fred]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Parke]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Case Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 TRENDS IN HIGH PERFORMANCE GRAPHIC SYSTEMS Panel Introduction Accompanying the rapid development of 
integrated circuit fabrication technology has been a parallel, but slower, development of IC design tech- 
niques and systems. Recent approaches to IC design enable individual designers to consider developing 
their own VLSI circuits. Such capability may open the door to a more varied set of system designs than 
could previously be considered in most design environments. The panel members are all currently designing 
graphic systems oriented towards VLSI implementation. Each will give a short presentation describing 
one of their system designs. Following these presentations, three members of the previous session (Forest 
Baskett, Andreas Bechtolsheim, and Fred Parke) will discuss common issues, problems, and future prospects. 
Chairman: Henry Fuchs, University of North Carolina.at Chapel Hill Panelists: D. Cohen, University of 
Southern California Bob Sproull, Carnegie-Mellon University Jim Clark, Stanford University Fred Parke, 
Case Institute of Technology 57
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1980</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>807469</article_id>
		<sort_key>58</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-1980</article_publication_date>
		<seq_no>10</seq_no>
		<title><![CDATA[Atlas supplement to the 1972 County and City Data Book]]></title>
		<page_from>58</page_from>
		<page_to>62</page_to>
		<doi_number>10.1145/800250.807469</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=807469</url>
		<abstract>
			<par><![CDATA[<p>A series of maps presenting the spatial distribution of the tabular data from the <underline>1972 County and City Data Book</underline> is discussed. Using an automated mapping procedure developed on a minicomputer by two non-computer scientists, 196 choropleth maps of county-level data for the State of Washington were prepared. The maps provide governmental decision-makers and planners a means to quickly comprehend patterns in the data that are not readily noticeable in the tabular presentation of the data.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Computer cartography]]></kw>
			<kw><![CDATA[Geographic displays]]></kw>
			<kw><![CDATA[Interactive computer graphics]]></kw>
			<kw><![CDATA[Proportional shading]]></kw>
			<kw><![CDATA[Spatial information]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.2.1</cat_node>
				<descriptor>Cartography</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010476.10010479</concept_id>
				<concept_desc>CCS->Applied computing->Computers in other domains->Cartography</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P332110</person_id>
				<author_profile_id><![CDATA[81100587469]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Lucky]]></first_name>
				<middle_name><![CDATA[M.]]></middle_name>
				<last_name><![CDATA[Tedrow]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Demographic Research Laboratory, Western Washington University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P330175</person_id>
				<author_profile_id><![CDATA[81100424318]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Eugene]]></first_name>
				<middle_name><![CDATA[A.]]></middle_name>
				<last_name><![CDATA[Hoerauf]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Department of Geography, Western Washington University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>807448</ref_obj_id>
				<ref_obj_pid>965103</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Dalton, J., Billingsley, J., Quann, J., and Bracken, P. "Interactive Color Map Displays of Domestic Information." SIGRAPH-ACM Computer Graphics, 13(2), (1979), pp. 226-233.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Jenks, G. F. and Caspall, F. C. "Error on Choropleth Maps: Definition, Measurement, Reduction." Annals of the Association of American Geographers, 61, (1971), pp. 217-44.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Schmid, Calvin F. and Schmid, S. E. Handbook of Graphic Presentation (Second Edition), Ronald Press Company, New York, (1979).]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Tedrow, Lucky M. and Hoerauf, E. Washington County Atlas Supplement to the County and City Data Book, 1972. In press, Western Washington University, (1980).]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Tobler, W. R. "Choropleth Maps Without Class Intervals?", Geographical Analysis, 5, (1973), pp. 262-5.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[United States Bureau of the Census. County and City Data Book, 1972. (A Statistical Abstract Supplement). Washington, D. C.: U.S. Government Printing Office, (1973).]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[United States Bureau of the Census. County and City Data Book, 1977. (A Statistical Abstract Supplement). Washington, D. C.: U.S. Government Printing Office, (1978).]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[United States Bureau of the Census and Man-power Administration. Urban Atlas. Washington, D. C.: U.S. Government Printing Office, (1974).]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 ATLAS SUPPLEMENT TO THE 1972 COUNTY AND CITY DATA BOOK Lucky M. Tedrow Demographic Research Laboratory* 
Western Washington University and Eugene A. Hoerauf Department of Geography** Western Washington University 
 Abstract  A series of maps presenting the spatial dis- tribution of the tabular data from the 1972 
County and City Data Book is discussed. Using an auto- mated mapping procedure developed on a minicomput- 
er by two non-computer scientists, 196 choropleth maps of county-level data for the State of Washing- 
ton were prepared. The maps provide governmental decision-makers and planners a means to quickly comprehend 
patterns in the data that are not readi- ly noticeable in the tabular presentation of the data. Key 
Words and Phrases: - interactive computer graphics -geographic displays - proportional shading 
- spatial information -computer cartography CR categories: 8.2, 3.37, 3.53 *Mailing address: Demographic 
Research Laboratory Department of Sociology Western Washington University Bellingham, Washington 98225 
 **Mailing address: Department of Geography and Regional Planning Western Washington University Bellingham, 
Washington 98225 Permission to copy without fee all or part of this material is granted provided that 
the copies ere not made or distributed for direct commercial advantage, the A~I copyright notice and 
the title of the publication and its date appear, and notice is given that copying is by permission of 
the Association for Computing Machinery. To copy otherwise, or to republish, requires a fee and/or specific 
permission. 01980 ACM 0-89791-021-4/80/0700-0058 $00.75 Introduct ion Vast quantities of tabulated 
statistical data are made available annually by federal, state, and local government agencies. The U.S. 
Bureau of the Census alone produces a wide array of data in tab- ular form on an annual basis in addition 
to vol- umes of data from censuses taken at regular inter- vals. Much of the data are not used to their 
po- tential due to the inability of users to quickly interpret the data. This paper describes a com- 
puter-produced Atlas Supplement to the 1972 County and City Data BooR4). The project was~d~- ta~.en 
wi-----t~-~e spe-----cific intention of making the county-level data of the 1972 ~ and City Data Book 
(6) more readily inte~etable to-~sers. As Schmid and Schmid (3) have noted, "Neither columns and rows 
of statistics in their tabular arrangement nor the seemingly endless listings of figures in textual form 
can possess the clarity, appeal, or meaningfulness of a well-designed chart." The large amounts of money 
spent on gathering and analyzing data may often be wasted if the potential market of users is unable 
to translate those data into information, that is, if the user cannot reduce the data to comprehensible 
form, either for purposes of simple description or as a basis for decision-making and planning. ~xcellent 
examples of the Census Bureau's response to the need for graphic display of data to aid user interpretation 
are the Urban Atlas (8) and the Domestic Information Display System pro- ject by NASA/Goddard Space Flight 
Center and the U.S. Bureau of the Census (I). These two projects involved not only the technical personnel 
of the Census Bureau, but included the technical exper- tise and hardware resources of the Lawrence Berke- 
ley Laboratory and the NASA/Goddard Space Flight Center. Such opportunities seldom exist at the state 
or local planning level. The enormity of these projects are simply beyond the resources of a small academic 
department, research laboratory or planning agency. The 1972 Co~and City Data Book (CCDB) presents ~nety 
oi-~-cio~mo~p~-statistical information for cities, counties, standard metro- politan statistical areas, 
urbanized areas snd un- incorporated areas. The Atlas Supplement t_o_the CCDB makes available a choropleth 
map of Washing- ton counties for each of the 196 county-level statistical items in Table 2 of the CCDB. 
Most of the statistical items were derived from the latest censuses of population, housing, governments, 
man- ufactures, business, mineral industries, and agri- culture. The general headings from the 1972 CCDB 
 58 (see Appendix A) describe the statistical data for Washington counties found on pages 510-521 of 
that publication. The Atlas Supplement to the Co~ and City Data Bookp~ents'a compu~r~oduced map for each 
of the 196 statistical items. Equipment The automated mapping procedure, developed in BASIC, uses the 
equipment shown in Figure I. Some of the equipment was purchased by a grant from the National Science 
Foundation (HES75-12384) to the Sociology Department, Western Washington Univer- sity (WWU). Additional 
equipment, including graphic terminals, remote incremental plotters and a digitizor, have been purchased 
by a combination of departmental and university funds. The auto- mated mapping routines were developed 
on ~M Ter- minal System (~S), which combines computer- assisted instruction (CAI), remote programming, 
scientific programming, text editing and data pro- cessing functions. WTS was developed at WWU and has 
been in operation since 1974. Figure 1 is one of six independent, but similarly configured mini- computer 
systems at ~U. These systems provide both academic and administrative computing at the University. The 
other minicomputer systems do not have the graphic peripherals displayed in Figure i. These other systems, 
each capable of support- ing approximately 25 terminals, are used primarily for CAI and interactive progra~ing. 
 Mapping Procedure and Discussion Figures 2:A and 2:B are maps showing the spa-  tial distribution 
in Washington of Items No. 46 and No. 87 from the 1972 CCDB. The outline of the counties was prepared 
using the 30" by 40" Tektronix graphics tablet digitizor shown in Fig- ure i. The "x,y" coordinates 
were stored and a "points dictionary" was used to permit the plot- ting of county boundaries in the 
form of polygons using identical coordinates along contiguous bor- ders. This technique prevents the 
appearance of gaps between county borders; a problem which oc- curs when each county is digitized independently. 
 The original "points" file included approximately 1,000 "x,y" coordinates; and the "points diction- 
 ary" linked together nearly 1,600 "x,y" coordinate pairs to produce the county outlines in these maps. 
 Each of the choropleth maps requires approxi-  mately an hour of connect time on the Interdata 7/32 
minicomputer syst~a shown in Figure i. The time necessary to initiate a plot is minimal and requires 
only a few interactive commands. The user starts the mapping process by telling the first program, 
FLAG, which of the 196 CCDB items is to be mapped, and the plotter to which the out- put is to be directed. 
FLAG reads the CCDB data file, selects the appropriate thirty-nine county values, and determines the 
shading pattern for each county based on its position in the value range. FLAG also stores the county 
values in the lettering file for later plotting within the county areas. FLAG chains into the plotting 
pro- grams which proceed through line, polygon and let- tering files to produce the finished plot. 
Before shading a polygon, a routine adds additional points to the polygon to provide a clear window 
 for the later inclusion of the county value. The shading routine shades each polygon separately; the 
polygon is rotated to make the shading lines horizontal, X-intercepts are calculated and then rotated 
back to the original angle before being sent to the plot routine. After drawing and shad- ing all the 
counties, the lettering and gray scale legend are added to complete the map. The series of programs are 
executed in a 16 K byte partition; this partition holds the plotter driving routines, the character-plot 
definition files, and the map- ping programs. This has required a series of mod- ules with automatic 
chaining (with or without keep- ing variables and values) the segments together. Thus, several of the 
modules may be entered direct- ly to complete an interrupted plot. A unique feature of these maps is 
the use of proportional shading instead of class intervals. This technique was advocated by Tobler (5) 
but has not been readily accepted or used by all carto- graphers. This method of presentation considera- 
bly reduces the time needed to manually determine special class intervals for each map. Some devel- opment 
in automated class interval selection by Jenks and Caspall (2) has made progress in further reducing 
this time-consuming step in map prepara- tion. Theoretically, using the proportional shading  technique 
employed here, one has available an in- finite number of class intervals for graphic display on a choropleth 
map. In actuality, one is limited by the resolution of the plotter (.01" in our case) and the plot 
size of the smallest geo- graphic unit on the map. By plotting these maps at the same size required 
for printing, we have eliminated a costly reduction step, but conse- quently are limited to 22 class 
intervals ranging from .01" to .22". The use of proportional shad- ing presents no problem in determining 
the rela- tive values for different counties on the maps be- cause the actual values are placed in 
windows within each county. The gray scale legend is in- cluded to indicate the maximum and minimum 
of the data and the approximate pattern for intermediate values. ~difications and Extensions The 
recent availability of the 1977 County  and Ci~Data Book (7) in machine readable form an~the ac~si~n 
of a Tektronix 4662 plotter have prompted changes in our automated mapping routines to more efficiently 
produce an atlas sup- plement to the 1977 CCDB. Two modifications to the graphics system that are already 
implemented both speedup the mapping and improve the quality of the maps for the 1977 atlas supplement. 
The graphics system used to prepare the 1972 atlas supplement was restricted toplotting one map at 
a time. The plotter was idle during the time that one map was completed and the next map was initi- 
 ated. ~difications to the mapping routines now allow the production of up to six maps in succes- sion 
with a few interactive co,ands at the start. The other improvement was made possible by  the acquisition 
of the Tektronix plotter. The quality of the lettering fonts on the Tektronix plotter are superior 
to those currently available on the Zeta plotter. ~difications to the graph- ics system now permit 
the county outlines and shading patterns to be produced on the Zeta plot- ters and the lettering to 
be completed on the Tek- tronix plotter. Because of the particular equip- ment available, separating 
the lettering and line drawing functions onto different plotters enhances 59 the lettering quality 
and decreases the time re- quired to complete a map. In producing lines for the outlines and shading, 
the Tektronix is some- what faster than the Zeta, requiring about two- thirds the time for the same 
plot. The fact that the Zeta requires software character generation makes it very slow at lettering 
compared to the Tektronix plotter with its built-in character generator. Using two Zeta plotters to 
plot the line images and the Tektronix to do all the let- tering will be using the available resources 
to best advantage. Register marks plotted outside the map area permit alignment on the Tektronix plotter 
for lettering after the shaded portion of the map has been completed on the Zeta plotter. The ~ and 
City Data Book provides coun- ty-level~--Eo~-all co~{ies-i~the United States. A county-level ailas supplement 
could be prepared for any state using this data source and similar mapping procedures employed in preparing 
the Washington atlas supplement. Polygon shading routines are quite conmnonly used at universities and 
laboratories elsewhere. By incorporating pro- portional shade routines - thus eliminating the time consuming 
manual class-interval selection process -maps could be produced for other states. This would permit the 
statistical data from the Co~and Ci~Data Book to be more readily in- terpreta-~e by de--ion-makers, planners 
and other users of this valuable reference source. References i. Dalton, J., Billingsley, J., Quann, 
J., and Bracken, P. "Interactive Color ~p Displays of Domestic Information." SIGRAPH-AO4 Com- puter Graphics, 
13(2), (1979), pp. 226-233. 2. Jenks, G. F. and Caspall, F. C. "Error on Choropleth Maps: Definition, 
Measurement, Reduction." Annals of the Association of American Geographers, 61, (1971), pp. 217-44. 
 3. Schmid, Calvin F. and Schmid, S. E. Handbook of Graphic Presentation (Second Edition~, Ronald Press 
Company, New York, (1979).  4. Tedrow, Lucky M. and Hoerauf, E. ~ashington County Atlas Supplement to 
the County and City Data Book, 1972. In press, Western Washington University, (1980).  5. Tobler, W. 
R. "Choropleth Haps Without Class Intervals?", Geographical Analysis, 5, (1973), pp. 262-5.  6. United 
States Bureau of the Census. County and City Data Book, 1972. (A Statistical Abstract Supplement). Washington, 
D. C.:  U.S. Government Printing Office, (1973).  7. United States Bureau of the Census. County and 
City Data Book, 1977. (A Statistical Abstract Supplement). Washington, D. C.:  U.S. Government Printing 
Office, (1978).  8. United States Bureau of the Census and Man- power Administration. Urban Atlas. ~Vashing-ton, 
D. C.: U.S. Government Printing Office,  (1974). Appendix A Presented below are the general headings 
which appear in the Co~andCi~Data Book, 1972 (6). These headings de~ribe the Sta~tical aa~ available 
for Washington counties in Table 2 of that publication (pages 510-521). The Atlas Supplement to the ~ 
and Ci~Data Book pre- sents a com~t~pro~d-~p for each ~ these 196 items. Items 1-15 AREA, POPULATION 
Items 16-32 POPULATION, BIRTH AND DEATH RATES, EDUCATION Items 33-49 LABOR FORCE Items 50-67 FAMILIES, 
INCOME Items 68-84 SOCIAL SECURITY, PUBLIC ASSISTANCE, HOUSING Items 85d01 HOUSING Itemsl02-117 PRESIDENTIAL 
VOTE, GOVF~ENT Item~l18431 BANKIng, ~iANUFACTURES Items132q48 RETAIL TRADE Items149q63 SELECTED SERVICES, 
WHOLESALE TRADE Items164479 MINERAL IFDUSTRIES, FARM POPULATION, AGRICULTURE Items180496 AGRICULTURE 
 FIGURE l INTERDATA TELEFILE 7/32 -DCI6C DISK -- MINCOMPUTER CONTROLLER C ___~TEKTRONIX ~ [TEKTRONIX 
 ~----~4954 \4010 CRT / [DIGITIZER OTHER SYSTEM  USERS  60 ILl []  _1 R E Z U H E LL [] E °, Lfl 
Ln N E DU Z ELjLJ F~ E LdLj  _J EL Z Ld o. [.~ ft. bd rfi r,- LA-444+t~tNIIm [] p-DR HLO ._H dl EL HU 
ZiI L~EE ZLO HZ LHZ [] RW LAW H~ ZB U [] 62 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1980</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>807470</article_id>
		<sort_key>63</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-1980</article_publication_date>
		<seq_no>11</seq_no>
		<title><![CDATA[A prototype Spatial Data Management System]]></title>
		<page_from>63</page_from>
		<page_to>70</page_to>
		<doi_number>10.1145/800250.807470</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=807470</url>
		<abstract>
			<par><![CDATA[<p>Spatial Data Management is a technique for organizing and retrieving information by positioning it in a spatial framework. Data is accessed in a Spatial Data Management System (SDMS) via pictorial representations which are arranged in space and viewed through a computer graphics system. These pictures can be created by an interactive graphical editor, allowing an SDMS to serve as a personal repository of diagrams, text, and photographs. Pictograms can also be generated from data in a symbolic database management system, allowing SDMS to be used as an interface to large, shared databases.</p> <p>A prototype SDMS has been constructed which employs a set of color, raster scan displays driven by a large minicomputer. The user can create and examine data surfaces which are larger than the display screen, traversing a surface and zooming in and out to control the level of detail displayed. The prototype system provides a uniform mechanism for accessing a wide variety of data types in a manner which does not require the use of a formal command or query language.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Computer graphics]]></kw>
			<kw><![CDATA[Database query languages]]></kw>
			<kw><![CDATA[Graphics languages]]></kw>
			<kw><![CDATA[Man-machine interaction]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.2</cat_node>
				<descriptor>SDMS</descriptor>
				<type>P</type>
			</primary_category>
			<other_category>
				<cat_node>H.2.3</cat_node>
				<descriptor>Query languages</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.6</cat_node>
				<descriptor>Languages</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003120.10003121.10003128</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction techniques</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002951.10002952.10003197</concept_id>
				<concept_desc>CCS->Information systems->Data management systems->Query languages</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010520.10010521.10010542.10011714</concept_id>
				<concept_desc>CCS->Computer systems organization->Architectures->Other architectures->Special purpose systems</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Languages</gt>
			<gt>Management</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P47676</person_id>
				<author_profile_id><![CDATA[81100434357]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Christopher]]></first_name>
				<middle_name><![CDATA[F.]]></middle_name>
				<last_name><![CDATA[Herot]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Computer Corporation of America, 575 Technology Square, Cambridge, MA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P241584</person_id>
				<author_profile_id><![CDATA[81100431212]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Richard]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Carling]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Computer Corporation of America, 575 Technology Square, Cambridge, MA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39037176</person_id>
				<author_profile_id><![CDATA[81100311571]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Mark]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Friedell]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Computer Corporation of America, 575 Technology Square, Cambridge, MA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P329779</person_id>
				<author_profile_id><![CDATA[81100095620]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[David]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kramlich]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Computer Corporation of America, 575 Technology Square, Cambridge, MA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>320253</ref_obj_id>
				<ref_obj_pid>320251</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Hendrix, G., Sacerdoti, E., Sagalowicz, D., Slocum, J., "Developing a Natural Language Interface to Complex Data", ACM Transactions on Database Systems 3:2, June, 1978.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Woods, W., Kaplan, R., Nash-Webber, B., The Lunar Sciences Natural Language Information System, Bolt Beranek and Newman, Cambridge, Mass., June, 1972.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Zloof, M., "Query by Example", Proc. AFIPS 1975 NCC, Vol 44, AFIPS Press, Montvale, N.J.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[McDonald, N., Stonebraker, M., "CUPID: The Friendly Query Language", Proc. ACM Pacific Conference, San Francisco, April, 1975.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807391</ref_obj_id>
				<ref_obj_pid>800248</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Donelson, W., "Spatial Management of Information", Proc. ACM SIGGRAPH 1978, Atlanta, Georgia.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Bolt, R., Spatial Data Management, Architecture Machine Group, Massachusetts Institute of Technology, Cambridge, Massachusetts, 1979.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Robertson, G., McCracken, D., Newell, A. The ZOG Approach to Man-Machine Communication, technical report CMU-CS-79-148, Department of Computer Science, Carnegie Mellon University, Pittsburgh, PA, October, 1979.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_obj_id>1500029</ref_obj_id>
				<ref_obj_pid>1499949</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Stonebraker, M., Held, G., Wong, E., "INGRES - A Relational Data Base System", Proc. AFIPS, Volume 44.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Shoup, R., "Superpaint..the Digital Animator", Datamation, May, 1979.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Smith, A. Paint, Tech. Memo No. 7, Computer Graphics Lab, New York Institute of Technology, Old Westbury, NY, July, 1978.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Herot, C., Carling, R., Friedell, M., Kramlich, D., and Thompson, J., "Spatial Data Management System Semi-Annual Technical Report," Technical Report CCA-79-25, June, 1979.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_obj_id>563309</ref_obj_id>
				<ref_obj_pid>563274</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Weller, D., Williams, R., "Graphic and Database Support for Problem Solving", Proc ACM SIGGRAPH 1976, Philadelphia.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_obj_id>361061</ref_obj_id>
				<ref_obj_pid>361011</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Ritchie, D., Thompson, K., "The UNIX Time Sharing System", Communications of the ACM, 17:7, July 1974.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
    displays out of its own 480x640 frame buffers. The SDMS system runs in several parallel processes 
which communicate through shared memory and pipes (a low-speed interprocess channel which appears to 
a user program as an i/o device). The prototype configuration supports a single user. Although general 
purpose time sharing can be run simultan- eously with SDMS, memory address space limitations preclude 
supporting more than one SDMS user simul- taneously. Data surfaces are stored on a moving-head disk 
as bit arrays known as image-planes. An image plane is partitioned into tiles which are rectangles of 
64 lines, each containing 128 pixels. Thus each tile occupies 8K bytes, which is equal to the page size 
of the PDP-Ii/70. The ships database, shown earlier, consists of four image planes (to support zooming 
to four levels of detail) and occupies 36 megabytes. A simplified process structure appears below. All 
of the processes shown run in the 11/70. The various input devices connect directly to the 11/70 where 
they are read by processes indicated on the diagram. ........................................... Figure 
17. SDMS Process Structure The staKer is actually two processes which allow the user to move the display 
window over the data surface. One of these processes reads the bit- array representation of the data 
surface into the main memory of the PDP-II/70. This process en- deavors to keep in memory all tiles currently 
on the screen plus sufficient extra tiles to maintain the user's current direction of motion. A second 
process sends portions of these tiles to the dis- play~ once again endeavoring to maintain a suitable 
margln of data just off-screen to allow for motion across the data surface. When the user presses on 
the joy stick, indicating a desire to move in a given direction, this process first checks to make sure 
that sufficient picture data is loaded into the display to start moving in that direction. If so, the 
display is commanded to begin its screen refresh at a new location, causing the picture to scroll. The 
program then determines whether new data must be sent to the display to maintain the margin around the 
displayed picture. This may require that additional data be read in from the disk as well. This process 
continues as long as the user holds down the joy stick or until an edge of the data surface is reached. 
This same program is responsible for updating the global variables describing the user's current position. 
 These two programs are also involved in zooming the display. When the user twists the joy stick, the 
display is commanded to change its scale factor. If the user is over a port or in a data surface which 
has multiple image planes, the stager must prepare to display the new~ more detailed image plane. Fortunately, 
zoomlng the display reduces the storage required to contain the current image. The memory which remains 
is filled with the new im- age plane in anticipation of the user's continued twisting of the joy stick. 
Finall~, if the user does continue to twist the joy stlck, the next im- abe plane, which has already 
been loaded into the dlsplay memory, appears on the screen. The naviKational aids process is responsible 
for displaying the current world-view map or the data surface hierarchy map. It alters the position of 
the highlighted rectangle as the global values of the user s current coordinates change. This process 
also listens to the touch sensitive digi- tizer mounted in front of the navigational aid dis- play and 
directs the stager to perform a rapid transit operation if the user touches the screen. The database 
interface is responsible for creating data surfaces in response to user commands and for blinking selected 
icons when so requested. It is also responsible for maintaining the consistency between the symbolic 
and graphical representations in the database. To this end, it monitors updates to the symbolic database. 
If such an update changes the value of an attribute which was used to create an icon, that icon is recreated 
with the new data. The graphical editor allows the user to modify the graphical data space. It operates 
upon the same in-core tiles used by the stager. It also manipu- lates the system's description of the 
graphical data space, allowing the user to delete icons, designate pictures as templates, and create 
ports. 5. CONCLUSIONS The initial informal evaluations by users who have used the system in our laboratory 
have been enthu- siastic. Most people learn to use the controls in a matter of seconds and can create 
pictures almost i~nediately. Much of this success can be attri- buted to the small number of simple controls 
and the furnishing of i~nediate feedback to every user action. The touch sensitive digitizers simplify 
the process of directing user input to the appro- priate display. They would have been used for all three 
displays (replacing the use of the data tab- let in the graphical editor) if they had sufficient resolution 
and a means of sensing finger position (for intermediate feedback) before the user pressed on the screen. 
 The SDMS method of accessing data seems to be most appropriate for those situations where the user needs 
to browse through a database. Since the data to be retrieved need not be specified precisely, the user 
does not need to learn a formal query language or possess an intimate knowledge of the structure and 
contents of the database. On the other hand, SDMS is not especially well suited to those circumstances 
which requlre locating some number of entities in the database which meet some precisely stated criterion. 
 As was anticipated, the least successful aspects of the system are those which require keyboard input. 
The selection of entities which are to appear on a data surface requires the use of a database query 
language which is awkward at best. Likewise, the icon class descriptions require the services of 69 
 someone who not only knows the organization of the database and the various commands but has the tal- 
 ent to design a data surface which is useful and aesthetically pleasing. A more rigorous evaluation 
of the system will take place during the coming year when the system is in- stalled at several selected 
sites where it can be used under conditions more closely approximating those of the ultimate operational 
environment. This process will involve loading the system with real databases and adding process ports 
for some previously implemented decision analysis programs. 5.1 Future Work  The usefulness of the 
system could be greatly in- creased if graphic representations of databases could be created with less 
human intervention. An approach outlined in [Ii] would use profiles of users and their applications together 
with descrip- tions of the semantics conveyed by various repre- sentations to select the most appropriate 
represen- tation for any particular circumstance. The composition of complex queries could be accom- 
plished by allowing the user to manipulate the graphical representations, creating a picture of the desired 
result in a form of graphical query-by- example. This might make the requisite logical thinking less 
painful for the user. In terms of implementation details, two very speci- fic goals will be pursued: 
creating icons more quickly and storing graphical data surfaces more compactly. A much faster icon creation 
wou~d allow the icons to be generated on the fly as the user scrolled over the data surface. This would 
greatly reduce the storage requirements of the system and also eliminate the problems of maintaining 
the con- sistency of the two different methods of storage currently employed. One approach which will 
be at- tempted in the short term is to split the process of icon creation into two stages. The retrieval 
from the symbolic database will produce a "com- piled" icon description. This description will be expanded 
as the user traverses the data surface. This approach would allow several users to examine the same graphical 
data space in an efficient and economical manner. References 7. Robertson, G., McCracken, D., Newell, 
A. The ZOG Approach to Man-Machine Comm-Qic~- tion, technical report CMU-CS-79-148, De- partment of Computer 
Science, Carnegie Mellon University, Pittsburgh, PA, October, 1979. 8. Stonebraker, M., Held, G., Wong, 
E., "INGRES - A Relational Data Base System", Proc. AFIPS, Volume 44. 9. Shoup, R., "Superpaint...the 
Digital Animator", Datamation, May, 1979. I0. Smith, A. Paint, Tech. Memo No. 7, Com- puter Graphics 
Lab, New York Institute of Technology, Old Westbury, NY, July,1978. ii. Nerot, C., Carling, R., Friedell, 
M., Kramlich, D., and Thompson, J., ~'Spatial Data Management System Semi-Annual Technical Report," Technical 
Report CCA-79-25, June, 1979. 12. Weller, D., Williams, R., "Graphic and Da-  tabase Support for Problem 
Solving", Proc ACM SIGGRAPH 1976, Philadelphia.  13. Ritchie, D., Thompson, K., "The UNIX Time  Sharing 
System", Communications of the ACM, 17:7, July 1974. i. Hendrix, G., Sacerdoti, E., Sagalowicz, D.~ 
Slocum, 3., "Developing a Natural Language Interface to Complex Data", ACM Transactions on Database Systems 
3:2, June, 1978. 2. Woods, W., Kaplan, R., Nash-Webber, B., The Lunar Sciences Natural Language Informati~ 
System, Bolt Beranek and Newman~ Cambridge, Mass., June, 1972.  3. Zloof, M., "Query by Example", Proc. 
AFIPS 2975 NCC, Vol 44, AFIPS Press, Montvale,  N.J.  4. McDonald, N., Stonebraker, M., "CUPID: The 
 Friendly Query Language", Proc. ACM Pacific Conference, San Francisco, April, 1975.  5. Donelson, 
W., "Spatial Management of Infor-  mation", Proc. ACM SIGGRAPH 1978, Atlanta, Georgia.  6. Bolt~ R., 
Spatial Data Mana~ement~ Architec- ture Machine Group, Massachusetts Institute of Technology, Cambridge, 
Massachusetts,  1979. 70 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1980</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>807471</article_id>
		<sort_key>71</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-1980</article_publication_date>
		<seq_no>12</seq_no>
		<title><![CDATA[ATLAS]]></title>
		<subtitle><![CDATA[A geographic database system data structure and language design for geographic information]]></subtitle>
		<page_from>71</page_from>
		<page_to>77</page_to>
		<doi_number>10.1145/800250.807471</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=807471</url>
		<abstract>
			<par><![CDATA[<p>The design concepts and languages of a geographic information system ATLAS (<underline>A</underline>dministration and <underline>T</underline>otal <underline>L</underline>anduse <underline>A</underline>nalysis <underline>S</underline>upport system) are proposed.</p> <p>The database structure is designed based on the geographic information structure concepts which contain semantic structure, topological structure and location structure. For a flexible user interface, the system provides two languages) IGL (<underline>I</underline>nteractive <underline>G</underline>eographic <underline>L</underline>anguage) and GDDL (<underline>G</underline>eographic <underline>D</underline>ata <underline>D</underline>efinition <underline>L</underline>anguage), whose functions are designed based on the geographic information structure concepts. IGL is an interactive end user language, which facilitates information retrieval under geographic and/or statistic conditions and thematic map production. GDDL is a language for a system manager who constructs and maintains the database.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Cartography]]></kw>
			<kw><![CDATA[Database]]></kw>
			<kw><![CDATA[Geographic information structure]]></kw>
			<kw><![CDATA[Geographic information system]]></kw>
			<kw><![CDATA[Graphic language]]></kw>
			<kw><![CDATA[Regional analysis]]></kw>
			<kw><![CDATA[Regional planning]]></kw>
			<kw><![CDATA[Thematic map]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>H.2.8</cat_node>
				<descriptor>Spatial databases and GIS</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.2.1</cat_node>
				<descriptor>Cartography</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.6</cat_node>
				<descriptor>Languages</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003120.10003121.10003128</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction techniques</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010476.10010479</concept_id>
				<concept_desc>CCS->Applied computing->Computers in other domains->Cartography</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002951.10003227.10003236</concept_id>
				<concept_desc>CCS->Information systems->Information systems applications->Spatial-temporal systems</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003145.10003147.10010887</concept_id>
				<concept_desc>CCS->Human-centered computing->Visualization->Visualization application domains->Geographic visualization</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Languages</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P333985</person_id>
				<author_profile_id><![CDATA[81100287702]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Tateyuki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tsurutani]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Central Research Laboratories, Nippon Electric Co., Ltd.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P308416</person_id>
				<author_profile_id><![CDATA[81100348847]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Yutaka]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kasahara]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Central Research Laboratories, Nippon Electric Co., Ltd.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P332376</person_id>
				<author_profile_id><![CDATA[81100155907]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Masaru]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Naniwada]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Central Research Laboratories, Nippon Electric Co., Ltd.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>563873</ref_obj_id>
				<ref_obj_pid>563858</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Brassel, K. E., Utano, J.J. and Hanson, P.O. "The Buffalo Crime Mapping System: A Design Strategy for the Display and Analysis of Spatially Referenced Crime Data" Proc. SIGGRAPH'77 pp78-85, 1977]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Brassel, K.E. "A Topological Data Structure for Multi-element Map Processing" First International Advanced Study Symposium on Topological Data Structures for Geographic Information Systems vol.4 Dutton, G. (Ed) 1978]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Carlson, E.D., etal. "The Design and Evaluation of an Interactive Geo-data Analysis and Display System" Proc. IFIP'74 , 1974]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>563885</ref_obj_id>
				<ref_obj_pid>563858</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Edson, D.T. and Lee, G.Y. "Ways of Structuring Data within A Digital Cartographic Data Base" Proc. SIGGRAPH'77 pp148-157, 1977]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Go, A., Stonebaker, M. and Williams, C. "An Approach to Implementing A Geo-data System" Proc. SIGGRAPH &amp; SIGMOD Workshop '75 pp67-77, 1975]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Kasahara, Y., Tsurutani, T. and Naniwada, M. "Geographic Information Concepts" Proc. 20th National Conf. of Japan Information Processing Soc. 1979 (in Japanese)]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Liskov, B.H. "A Design Methodology for a Reliable Software System", FJCC, p191, 1972.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Mantey, P.E. and Carlson, E.D. "Integrated Data Base for Municipal Decision-Making" Proc. NCC'75, 1975]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_obj_id>356777</ref_obj_id>
				<ref_obj_pid>356770</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Nagy, G. and Wagle, S. "Geographic Data Processing" Computing Surveys, Vol.11, No.2, 1979]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Takasaki, M. "Introduction to Map" NHK BOOKS, 1977 (In Japanes)]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_obj_id>563304</ref_obj_id>
				<ref_obj_pid>563274</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Tuerke, K. "A System For Interactive Acquisition and Administration of Geometric Data for Thematic Map production" Proc. SIGGRAPH '76 pp154-162, 1976]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Wangth, T.C. "A Parameter Driven Language for Use in Application Systems" First International Advanced Study Symposium on Topological Data Structures for Geographic Information Systems vol.7 Dutton, G.(Ed) 1978]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 ATLAS: A Geographic Database System ---Data Structure and Language Design for Geographic Inlormation--- 
 Tateyuki Tsurutani) Yutaka Kasahara and Masaru Naniwada CENTRAL RESEARCH LABORATORIES NIPPON ELECTRIC 
CO. LTD. ABSTRACT The design concepts and languages of a geographic information system ATLAS (Administration 
and Total L_anduse A_nalysis Support system~ are proposed. The database structure is designed based on 
the geographic information structure concepts which contain semantic structure, topolog!cal structure 
and location structure. For a flexible user interface, the system provides two languages) IGL (Interactive 
G__eographic Lanuguage) and GDDL (Geographic Data Definition ~anguage)) whose functions are design~ based 
on the geographic information structure concepts. IGL is an interactive end user language, which facilitates 
information retrieval under geographic and]or statistic conditions and thematic map production. GDDL 
is a language for a system manager who constructs and maintains the database. Key Words and Phrases: 
geographic information system, regional planning) regional analysis, cartography) thematic map, database) 
graphic language) geographic information structure. CR categories : 3.35) 3.53, tt.33, 4.34 1. INTRODUCTION 
The general planning section of a local government in Japan deals with various regional and urban planning 
problems) which requires a large amount of geographic and statistical data. Computer technology will 
assist handling such data in planning process) especially in retrieving and displaying adequate geographic 
and statistical data. Therefore) it is desirable for the planning section to use a geographic information 
system which provides an efficient means of geographic and statistical information retrieval and thematic 
map production. Permission to copy without fee all or part of this ~terial is granted provided that 
the copies are not made or distributed for direct commercial advantage, the ACM copyright notice and 
 the title of the publication and its date appear, and notice is given that copying is by permission 
of the Association for Computing Machinery. To copy otherwise, or to republish, requires a fee and/or 
specific permission. ~980 ACM 0-89791-021-4/80/0700-0071 $00.75 The information needed for planning 
varies from problem to problem. For example) road network data are indispensable for a transportation 
planning problem but are not so important for a water resources problem. Therefore) the geographic information 
system for the general planning section must be able to handle various kinds of geographic information) 
such as administrative area, school district) road, river) etc., each of which has its own geographic 
and statistical characteristics. In recent years) geographic information systems) which store, process 
and display geographic information) have been developed [1) g) 10 ]. However) to support a regional planning 
section for a local government) difficulty arises) because most of these systems are designed for rather 
specific problems and might lack flexibility and extendability in geographic data handling. The first 
problem for such a geographic information system is to clarify geographic information structure concepts 
which provide a design basis for a unified and flexible geographic information system. The other major 
problem is the user interface to the geographic information system. In general) planners want to take 
their choices of data at both the category level and the detail level, when they construct or use their 
systems. This paper describes an outline for a geographic information system ATLAS (Administration and 
Total Landuse Ainalysis _Support s~tem)supporting regmonal planning. The geographic information structrue 
concepts and ATLAS database structure are proposed in Section 2. To realize a flexible user interface) 
ATLAS has two languages; IGL (Interactive Geographic Language) and GDDL (G_eograph~ D_ata p efi~tion 
L angua-ge), which are discussed in Sections 3 and 4, respectively. IGL is an interactive language for 
an end user who is not an EDP specialist. GDDL is for a system manager who constructs and maintains the 
database. 2 DATA STRUCTURE This section discusses ATLAS geographic informa- tion structure concepts and 
the database structure based on the concepts. Before the discussion) basic terminology is introduced. 
2.1 Basic terminology. Geographic element 71 A geographic entity, such as a city) a road) a river Geographic 
relationship etc.) is called a geographic element. A geographic element has its own name (or code), location, 
statistical data and topological attribute, whose value is an area, a line or a point. A collection of 
geographic elements may also be a geographic element (an aggregate geographic element), for example, 
a prefecture which is an areal geographic element is regarded as a set of cities, towns and villages. 
A line geographic element may also be an aggregate geographic element. For example, a road is regarded 
as a collection of links between two adjacent interstections (road segments). Geographic element class 
 A geographic element class is a set of geographic elements which has the same characteristics. It has 
a unique name (or code). For example, a collection of all cities becomes a geographic element class city. 
Geographic relationship A geographic relationship is the relation between two geographic elements and 
is naturally extended to a relation between two geographic element classes. These relationships define 
geographic information structures. Segment and node A segment is a line segment defined by a boundary 
line of an areal geographic element or by a line geographic element. A node is defined by a point geographic 
element or by an end of a segment (see Fig. I) [1~ ]. 2.2 Geographic information structure Geographic 
information structL~re for the geographic information system is defined by geographic relationships, 
each of which reflects the relationship between geographic entities in the real world and is to be realized 
in the system for important applications. 0 node ~ ~ , . 2 Figure I Segment and node example Following 
geographic relationships are introduced in ATLAS. (i) Semantic relationship The relationship between 
an aggregate geographic element and its segregated geographic elements is called a semantic relationship. 
From a geometrical viewpoint) the relationship is similar to location containment relationship, which 
is described later. However, emphasis is not on the geometrical view but on the application field view. 
For example) the relation between a prefecture and an included city is considered as a semantic relation) 
while the relation between a prefecture and a river basin is not, even if they have a specific location 
containment relation. (ii) Area adjacency relationship This relationship means that two areal geographic 
elements in the same geographic element class have a common area boundary line) i.e.) a common segment. 
(iii) Line adjacency relationship This relationship means that two line geographic elements which belong 
to the same geographic element class have a common node. (iv) Boundary relationship This is a relation 
between a line geographic element and an areal geographic element or between a line geographic element 
and a point geographic element, which means that these two geographic elements have a common segment 
or a common node. (v) Location containment relationship  This relationship means that two geographic 
elements have a common place or location. When the relationship concerns a line and an areal geographic 
element, it is called a passing-through relationship. (vi) Distance Distance is calculated according 
to locations and separation between two geographic elements. (vii) Direction Self explanatory. Geosraphic 
information structure concepts Along with the traditional structures, topological and geometrical structures, 
the semantic structure is introduced in the ATLAS geographic information structure concepts. Therefore, 
a hierarchical structure can be constucted among geographic elements. Moreover, the structure corresponds 
to the information processing methods in many application fields. For example, statistical data, such 
as that concerning population and housing, are gathered and processed in a bottom-up manner, from the 
block level to the national level. In the case of a water quality management problem, a river is regarded 
as an aggregation of sections, in which water quality data is monitored and detail characteristics data 
are investigated. The ATLAS geographic information structure concept has three strucutres. (i) Semantic 
structure This structure is a hierarchical structure, defined by semantic relationship (see Fig. 2). 
It reflects a semantic property in geographic informa- tion. 72 (il) Topological structure This structrue 
is defined by area adjacency) line adjacency and boundary relationships (see Fig. 3), It reflects a topological 
property in geographic information. (iii) Location structure This structure is defined by location containment, 
distance and direction relationships (see Fig. 4). It reflects a metric property in geographic information. 
I r ............................ i-............ I  I I I ] I I ° =II SEGMENT I I l.=,l Figure Z Semantic 
structure example I ..EPECTURE J ROW  I .RSEC o I , , Areaadjacency IL I crrv / I DIS'tRIer i J Line 
adjacency Figure 3 Topological s'tructure example / C~osrapmcA r ................. L ........... T .............. 
l i DISTRICT DISTRICT I ROAD SEGMENT I I 'i I  BLOCK Figure ~ Locazion structure example 2.3 Data 
base structure ATLAS database structure design is based on the geographic information concepts. According 
to the levels of abstraction concepts [7], it is composed of three data levels; geographic element level, 
segment level and coordinate level (see Fig, 5). Geographic element level This level realizes the semantic 
structure) the topological structure and a part of the location structure. Geographic elements are data 
records in this level. Geographic relationships are the chains between data records, except for distance 
and direction relationships. These chains are generated in the system, according to the lower level data 
i.e. segment data and coordinate data. However, semantic relationships should be specified explicitly 
in user input data. Every geographic element data record also has chains to their statistical data records 
and the lower level data. This data structue allows users to retrieve geographic and statistical information 
according to geographic conditions and/or statistical conditions. Segment level In this level, a data 
record is a segment related to one or several geographic elements in the upper level, and has a unique 
segment name. User input data for a geographic element contains a geographic element name, its segment 
names and coordinate data. When a new geographic element is entered, the segment name is tested and topological 
relationships in the upper level are generated, if any exist. Coordinate level In this level, a data 
record is point coordinate data or grid cell data. Point coordinate data is the grid cell code, X coordinate, 
Y coordinate format. Location containment relationship in the geographic element level is generated 
automatically, if common grid cells exist between two geographic elements. User retrieval inquiry, using 
distance or direction relationships, which are not realized explicitly in the geographic element level, 
is processed according to this level data. Geographic element level Segmen1~ level Coordinate level 
 Figure 5 Data base s~rucxure  73 3 INTERACTIVE GEOGRAPHIC LANGUAGE (IGL) IGL is an end user oriented 
command language, which facilitates information retrieval, thematic map production, map modification 
etc. Typical IGL statements are shown in Table 1. Two kinds of files, map file and table file, are used 
in this system. The former contains geographic information, which corresponds to a map sheet. The latter 
contains statistical data in matrix form. Geographic and statistical operations specified with most IGL 
statements are processed as file operations, file creation, file modification, etc. by the IGL interpreter. 
3.1 Information retrieval The information retrieval function, which IGL provides, reflects the geographic 
information structure concepts. A user can specify the data by information retrieval conditions using 
geographic relationships. The IGL interpreter analyzes a user's inquiry, finds the navigation route in 
the database and retrieves the target data, which are stored in a map file or a table file. MTCREATE 
statement is an information retrieval statement, which is followed by a map definition block and/or a 
table definition block enclosed by MDF and END statements, or by TDF and END statements, respectively. 
Map and table definition blocks are used to specify: Geographic and/or statistic information retrieval 
conditions -Table file format, i.e., contents of rows and columns In these blocks, information retrieval 
condition statements; GELM, GCLS, GCND and SCND statements are applicable. Syntax of these statements 
are shown in Fig. 6. In GCND statement, geographic operators are available, each of which corresponds 
to a specific geographic relationship in the data base. Therefore, the GCND statement represents the 
sequence of geographic relationships between the entry geographic element and the target geographic element 
class in the data base. The geographic operators are shown in Table 2. According to these statements, 
users can express compound conditons, for example, cities with more than 100000 population, through which 
Sagami river flows, is represented as follow. GCLS CITY GCND RIVER SAGAMI CNTN CITY SCND CITY, POPULATION 
>100000 SMCREATE - SDF SG EL DITM SSIZ Other statements MDISP TDISP SMDISP Table I Typical IGL statements 
Information retrieval statements MTCTREATE-map/table create instruction MDF declaration of map definition 
block TDF -declaration of table difinition block END end of map/table definition block GELM specifies 
target geographic elements GCLS specifies target geographic element classes GCND specifies a condition 
using geographic relationships SC ND specifies a condition using statistical data TROW declaration of 
table row TCLM declaration of table column SITM specifies target statistical items Thematic map production 
statements thematic map create instruction declaration of thematic map "definition block specifies geographic 
elements to be used for a thematic map specifies statistical items to be expressed on a thematic map 
specifies the size of figures in a thematic map map display instruction table display instruction thematic 
map display instruction (GELM st.~ :: = GELM <geographic element class> ~geographic element> ... <GCL5 
st.) :: = GCL$ <geographic element class> .... <GCND st~ :: = GCND <target geographic element class~/~entry 
geographic element class~ <entry geographic elemen~ ~geographlc operator~ [<geographic element class>~'geographic 
operator> J. OCND st.> :: = 5CND <geographic element class>/<logical expression> Figure 6 IGL retrieval 
condition statement syntax 74 Table 2 SEMT for AAD3 for LAD3 for BUND for CNTN for DIST for DIRT for 
IGL geographic operators semantic relationship area adiacency relationship line adjacency relationship 
boundary relationship . .  location containment relationship distance direction In Fig. 7A, the map 
definition block specifies geographic element Kawasaki city by the GELM statement and all wards in Kawasaki 
city by the GCLS and the GCND statements. The GCND statement expresses the geographic condition for geographic 
element class 'ward' in the GCLS statement just previous to the GCND, namely, all wards semantically 
related to Kawasaki city. In Fig. 7B, the table definition block specifies the contents of rows and columns. 
The TROW statement represents the beginning of the row definition and the following GCLS and GCND statements 
specify the row, i.e. all wards in Kawasaki city. The TCLM statement represents the beginning of the 
column definition. The following SITM statements specifies statistical items, total population, 0-14 
years old, 15-64 years old and over 65. Figures 7C and 7D show the contents of the resulting files; MFILE 
1 and TFILE 1. 3.2 Thematic map production Figure gA shows an example of thematic map production. In 
Fig. SA, SMCREATE statement specifies a map file MFILE 1 and a table file TFILE 1 (see Fig. 7C, 7D). 
SGEL statement specifies the table rows, which correspond to the geographic elements. DITM statement 
specifies the table columns which contains statistical items to be expressed on the thematic map. SCRL 
statement specifies cartographic method "segmented circle map" (see Fig. 8B). IGL has several statements 
for cartographic methods, choropleth map, dot map, segmented rectangle map etc [1, 10].   3.3 Map Modification 
IGL provides the following map modification facilities to; (i) Modify map scale and move user's window 
on a displayed map (ii) Modify and move legend on a map  (iii) Modify and move statistical figures 
on a map To ease these operations, these IGL commands are provided as a menu. The user can invoke these 
functions through cursor on CRT display terminal. Figures 8B and 8C show the menu and reformed statistical 
map. To modify the thematic map in Fig. 8B, SASL and SGMV commands are used; SASL for modifying the size 
of all statistical figures and SGMV for moving the figures. 3.4 Other functions In addition to the facilities 
discussed previously, IGL has the following functions for the end user. (i) Statistical data handling 
Users can treat statistical data in tabular (matrix) form. MTCREATE MFILE I MDF GELM CITY KAWASAKI GCLS 
WARD GCND WARD / CITY KAWASAKI SEMT END Figure 7A Map creation program example MTCREATE TFILE I TDF TROW 
GCLS WARD GCND WARD / CITY KAWASAKI SEMT TC LM 51TM POPTOTAL, POP 0-1% POP 15-6% POP 6J-END  Figure 
7B Table creation program example ~. "~ ......"9~~(Th.~TSU ward) "\. .~:'~_ "7" L.~iX.~l] "N .... ":~,T~k99(NRKAHARR 
ward) ~.~,..,..,..-', ~. .7'~,l"b 4 ~ ( $ O]..IJ.~ I ward) ~.f~ ..~-~ ".,~  ~9~$9(KAgASAKI City) ~. 
.j~. %; Figure 7C Map example (KAWASAKI city) i TOTAL 0-14 i 15-64 65-IKAgASAKI 216456 476821 1572081 
11566 iSAIgAI 148687 349901 107319 6378 iNAKAHARA 196760 419271 147019 8444 !TAKATSU 249271 686711 1719901 
8610 iTAMA 202388 514341 1425201 84341 Figure 7D Table example (POPULATION of KAWASAKI) 75 (ii) IGL program 
 A sequence of IGL statements may be stored as a program. A user can make IGL programs for his standard 
jobs and activate them by EXEQ statement. (iii) Text macro Text macro variable is applicable in any 
IGL statement. In the system, it is substituted for by a character string (value) which is provided by 
LET statement or keyboard input. SMCREATE MFILE I,TFILE I SDF SGEL R DITM C, 2,3, g SSIZ I, 5 END  Figure 
gA Thematic map produc'tion program example AMOV ASCL AROT AFRM SASL  SGDL / s Nx .1 SQMu " .~ Figure 
iB Seqmented circle map example tt GEOGRAPHIC DATA DEFINITION LANGUAGE (GDDL) As mentioned before, GDDL 
is a language for a system manager to use in constructing a database suitable for his system objectives. 
He can define the following items by GDDL: (i) Geographic element classes to be stored in the database 
(ii) Geographic relationships betweeneachpair of geographic element classes (iii) Statistical items for 
each geographic element class That is, he can specify the categories and details of geographic and statistical 
information and geographic information structure; semantic structure, topological structure and location 
structure. Figure 9 shows a part of a GDDL program for the geographic information structures shown in 
Figs 2, 3 and 4. A GDDL program is compiled and stored in the database. ATLAS data input program refers 
to this information and generates geographic relation chains between geographic element records, if specified 
in the GDDL program. The IGL interpreter also refers to this information to find navigation routes in 
the database and to detect er-'-rs in IGL statements. CLASS PREFECTURE AREA CITY AREA POLICE-DISTRICT 
AREA SCHOOL-DISTRICT AREA BLOCK AREA ROAD. LIN E ROAD-SEGMENT LINE INTERSECTION POINT END_CLASS RELATION 
SEMANTIC PREFECTURE CITY SEMANTIC PREFECTURE SCHOOL-DISTRICT SEMANTIC CITY WARD SEMANTIC ROAD ROAD-SEGMENT 
AADJACENCY CITY AADJACENCY SCOOL-DISTRICT LADJACENCY ROAD-SEGMENT BOUNDARY ROAD-SEGMENT INTERSECTION 
BOUNDARY BLOCK ROAD-SEG M ENT LOCATION CITY SCHOOL-DISTRICT END-R ELATION STATISTICS CITY POPULATION-TOTAL, 
POP 0-1# POP 1.5-6% POP 6.5... END-STATISTICS Figure ~C Rdormed segmented circle map example Figure 9. 
GDDL program example 76 5. CONCLUSION The design concepts and languages of a geographic information 
system ATLAS for supporting regional analysis and regional planning are proposed. The database is based 
on geographic information concepts concerning semantic structure, topological structure and location 
structure. It can store various sorts of geographic and statistics data. ATLAS provides two languages, 
IGL and GDDL, for a flexible user interface. IGL is an interactive end user language, and facilitates 
information retrieval, map production and map modification. Especially, it can express information retrieval 
conditions which use the relationships in geographic information structure concepts. GDDL facilitates 
defining the database, i.e., geographic data, categories statistical data items and geographic information 
structure. ATLAS prototype system has been developed on ACOS SERIES 77 NEAC SYSTEM 700 Time sharing system. 
The database is based on CODASYL type DBMS called ADBS. After implementing the database and language 
processors stage, functional and performance evaluation phases will follow. It is desired that these 
phases be not restricted to the physical evaluation of the system, but include the evaluation of the 
concepts in actual regional environment. g 9 10 11 12 Mantey, P.E. and Carlson, E.D. "Integrated Data 
Base for Municipal Decision-Making" Proc. NCC'75, 1975 Nagy, G. and Wagle, S. "Geographic Data Processing" 
Computing Surveys, Voll.11, No.2, 1979 Takasaki, M. "Introduction to Map" NHK BOOKS, 1977 (In 3apanes) 
Tuerke, K. "A System For Interactive Acquisition and Administration of Geometric Data for Thematic Map 
production" Proc. SIGGRAPH '76 pplStt-162, 1976 Wangth, T.C. "A Parameter Driven Language for Use in 
Application Systems" First International Advanced Study Symposium on Topological Data Structures for 
Geographic Information Systems vol.7 Dutton, G.(Ed) 1978 ACKNOWLEDGMENT The authors want to express their 
thanks to Dr. T. Mikami for his encouragement. They also thank all ATLAS development members. REFERENCES 
Brassel, K. E., Utano, 3.3. and Hanson, P.O. "The Buffalo Crime Mapping System : A Design Strategy for 
the Display and Analysis of Spatially Referenced Crime Data" Proc. SIGGRAPH'77 pp7g-85, 1977 Brassel, 
K.E. "A Topological Data Structure for Multi-element Map Processing" First International Advanced Study 
Symposium on Topological Data Structures for Geographic Information Systems vol.# Dutton, G. (Ed) 1978 
Carlson, E.D., etal. "The Design and Evaluation of an Interactive Geo-data Analysis and Display System" 
Proc. IFIP'7tt , 1974 Edson, D.T. and Lee, G.Y. "Ways of Structuring Data within A Digital Cartographic 
Data Base" Proc. SIGGRAPH'77 ppltt8-157, 1977 Go, A., Stonebaker, M. and Williams, C. "An Approach to 
Implementing A Geo-data System" Proc. SIGGRAPH &#38; SIGMOD Workshop '75 pp67-77, 1975 Kasahara, Y., 
Tsurutani, T. and Naniwada, M. "Geographic Information Concepts" Proc. 20th National Conf. of 3apan Information 
Processing Soc. 1979 (in 3apanese) 7 Liskov, B.H. "A Design Methodology for a Reliable Software System", 
F3CC, pl91, 1972. 77   
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1980</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>807472</article_id>
		<sort_key>78</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-1980</article_publication_date>
		<seq_no>13</seq_no>
		<title><![CDATA[Computer graphics moves into the business world (Panel Session)]]></title>
		<page_from>78</page_from>
		<doi_number>10.1145/800250.807472</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=807472</url>
		<abstract>
			<par><![CDATA[<p>Computer graphics has become an everyday tool for an engineer, scientist, and technician. Only within the last year or so has the business community demonstrated a real interest in computer graphics for their requirements. While the technically trained person had no difficulty accepting that a picture was worth a thousand words, the businessman often didn't trust pictures and felt more confident with his tabular data. Many of these attitudes are changing with the introduction of both hardware and software oriented towards business requirements. This panel will discuss a number of topics relating to computer graphics and business, hardware, software, system, and user attitudes.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>J.1</cat_node>
				<descriptor>Business</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010406</concept_id>
				<concept_desc>CCS->Applied computing->Enterprise computing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010406.10010412</concept_id>
				<concept_desc>CCS->Applied computing->Enterprise computing->Business process management</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP40027810</person_id>
				<author_profile_id><![CDATA[81100488149]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Carl]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Machover]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Machover Associates]]></affiliation>
				<role><![CDATA[Chairman]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P333892</person_id>
				<author_profile_id><![CDATA[81332531159]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Susan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Taylor]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[IBM]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14203119</person_id>
				<author_profile_id><![CDATA[81332525980]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Allan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Schmidt]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Harvard University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P332815</person_id>
				<author_profile_id><![CDATA[81100141112]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Paul]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Birkhahn]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[American Can Company]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P331696</person_id>
				<author_profile_id><![CDATA[81100268001]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[June]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hament]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Insurance Company of North America]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P331547</person_id>
				<author_profile_id><![CDATA[81100210855]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[John]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Quann]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[NASA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1980</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>807473</article_id>
		<sort_key>79</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-1980</article_publication_date>
		<seq_no>14</seq_no>
		<title><![CDATA[Integrating solid image capability into a general purpose calligraphic graphics package]]></title>
		<page_from>79</page_from>
		<page_to>85</page_to>
		<doi_number>10.1145/800250.807473</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=807473</url>
		<abstract>
			<par><![CDATA[<p>Raster scanned graphics terminals provide several features not found in standard line drawing displays. Among them are area fill and an extensive color palette. Hardware support for such functions is becoming cost effective and available in a variety of forms. What is now needed is high level, device independent software that assists in the generation of and interaction with these terminals. To accomplish this, a project has been undertaken jointly by Sandia Laboratories, Purdue University, and MEGATEK Corporation to integrate solid image capabilities into the Graphics Compatibility System, GCS. GCS is a widely used, general purpose calligraphic graphics package. Due to the similarity of the GCS model to that of the GSPC Core System, GCS provides a timely test bed for solid image extensions to a line drawing package. This paper will summarize the capabilities being implemented, the method and motivation behind each function, and suggestions for further work.</p> <p>The heart of the extensions is a hidden surface algorithm similar to Myers' approach which combines the simplicity of a Z buffer with the efficiency of a Watkin's algorithm. Facilities will be provided for hidden line removal as well as hidden surface processing in order to satisfy the basic GCS design goal of device independence. In this way, random vector terminals as well as raster devices will benefit.</p> <p>If a user does not require hidden object processing then very little additional overhead will remain in those routines which have been called. Hidden surface processing may also be intermixed with wire frame drawings and text.</p> <p>Two remaining issues are the user interface for 3D object definition and the specification of lighting models. Since the Myers algorithm requires N sided polygons as input, a basic call has been provided for such definitions. Higher level constructs, called &#8220;shells&#8221; and &#8220;faces&#8221;, which facilitate creation of complex 3D bodies, have been implemented.</p> <p>The specification of lighting models is an area of considerable debate. It has been concluded that a choice of a few common models will be made available to the user. These include interpolated and noninterpolated shading (smooth and flat). Objects may be color-shaded or black-and-white. A hook is provided for user definition of more exotic lighting models.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.4</cat_node>
				<descriptor>Graphics packages</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.1</cat_node>
				<descriptor>Raster display devices</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Hidden line/surface removal</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010583.10010588.10010591</concept_id>
				<concept_desc>CCS->Hardware->Communication hardware, interfaces and storage->Displays and imagers</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372.10010373</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering->Rasterization</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP14109251</person_id>
				<author_profile_id><![CDATA[81365595937]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[G.]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Laib]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP31090861</person_id>
				<author_profile_id><![CDATA[81100117553]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[R.]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Puk]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P330396</person_id>
				<author_profile_id><![CDATA[81332530147]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[G.]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Stowell]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>563893</ref_obj_id>
				<ref_obj_pid>965141</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Blinn, J.F., "Models of Light Reflection for Computer Synthesized Pictures," Computer Graphics, Vol. 11, No. 2, pp. 192-198, Summer 1977.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Foley, J.D., Templeman, J., Dastyar, D., "Raster Graphics Extensions to the Core Graphics System," Final Report, Contract DACA39-78M0073, Weterways Experiment Station, U.S. Army Corps of Engineers, August, 1978.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Gabriel, H, et.al., "Introduction to the Graphics Compatibility System," NTIS No. AD-778 750, U.S. Department of Commerce, March 1974.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>988498</ref_obj_id>
				<ref_obj_pid>988497</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[ACM-SIGGRAPH, "Status Report of the Graphics Standards Planning Committee," Computer Graphics, Vol. 13, No. 3, August 1979.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Myers, Allan J., "An Efficient Visible Surface Program", NSF Report for Grant #UCR 74-00768 A01, 1975.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>360839</ref_obj_id>
				<ref_obj_pid>360825</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Phong, Bui-Tuory, "Illumination for Computer Generated Images," CACM 18, pp. 311-317, June 6, 1975.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Puk, Richard F., "Graphics Compatibility System Reference Manual", U.S. Army Waterways Experiment Station, Vicksburg, MS, October, 1977.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_obj_id>360802</ref_obj_id>
				<ref_obj_pid>360767</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Sutherland, I.E. and Hodgman, G.W., "Reentrant Polygon Clipping," CACM, January 1974.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 INTEGRATING SOLID IMAGE CAPABILITY INTO A GENERAL PURPOSE CALLIGRAPHIC GRAPHICS PACKAGE by G. Laib, 
R. Puk, G. Stowell* ABSTRACT Raster scanned graphics terminals provide several features not found in 
standard line drawing displays. Among them are area fill and an extensive color palette. Hardware support 
for such functions is becoming cost effective and available in a variety of forms. What is now needed 
is high level, device independent software that assists in the generation of and interaction with these 
terminals. To accomplish this, a project has been undertaken Jointly by Sandia Laboratories, Purdue University, 
and MEGATEK Corporation to integrate solid image capabilities into the Graphics Compatibility System, 
GCS. GCS is a widely used, general purpose calligraphic graphics package. Due to the similarity of the 
GCS model to that of the GSPC Core System, GCS provides a timely test bed for solid image extensions 
to a line drawing package. This paper will summarize the capabilities being implemented, the method and 
motivation behind each function, and suggestions for further work. The heart of the extensions is a 
hidden surface algorithm similar to Myers' approach which combines the simplicity of s Z buffer with 
the efficiency of a watkin's algorithm. Facilities will be provided for hidden line removal as well as 
hidden surface processing in order to satisfy the basic GCS design goal of device indepenOence. In this 
way, random vector terminals as well as raster devices will benefit. If a user does not require hidden 
object processing then very little additional overhead will remain in those routines which have been 
called. Hidben surface processing may also be intermixed with wire frame drawings and text. Two remaining 
issues are the user interface for 30 object definition and the specification of lighting models. Since 
the Myers algorithm requires N sided polygons as input, a basic call has been provided for such definitions. 
Higher level constructs, called "shells" and "faces", which facilitate creation of complex 3D bodies, 
have been implemented. The specification of lighting models is an area of considerable debate. It has 
been concluded that a choice of a few common models will be made available to the user. These include 
interpolated and noninterpolated shading (smooth and flat). Objects may be co/or-shaded or black-and-white. 
A hook is provided for user definition of more exotic lighting models. Introduction For many years, 
the generation of solid image shaded color pictures has required an intimate knowledge of highly specialized 
software packages which could only be used on unique and expensive hardware. With the advent of lower 
cost raster devices and with the development of more efficient hidden object removal algorithms, the 
ability to preduce a user-oriented general-purpose, bevice-independent, solid-image graphics package 
was recognized. Coupled with a requirement for such a package at Sandia Laboratories -AlbuquerQue, a 
project was established which combined expertise in graphics software available at Sandia with hldden-object 
software already developed at Purdue University and with support from the GC5 Distributor, U.S. Azmy 
 Waterways Experiment Station. The Graphics Compatibility System (GCS), [Puk77], was chosen as a vehlcle 
for this project for several reasons. First, it has an TTI~IT-werk was supported by funds provided by 
the U.S. Department of Energy and Sandla Laboratories. Permission to copy without fee all or part of 
thls mterlal is granted provided that the copies are not made or dlstribuced for direct co~erclal advantage, 
the ACM copyright notice and ~980 ACM 0-8979]-02]-4/80/0700-0079 $00.75 79 active scientific user community. 
The principal developer of GCS was employed by Sandia Laboratories. Also, GCS was already available and 
being utilized for its calligraphic cepabllities at both Purdue and Sandia. Finally, It is also considered 
to be an implementation of the GSPC Core System [GSPC79]. Many of the concepts embodied in this work 
were interectively discussed and derived from the wark at George Washington University under the direction 
of James Foley which resulted in a report to the GCS Distributors on possible extensions [FOLEY78]. This 
report also described an implementation of raster extensions to the GSPC Core System underway at George 
Washington University. The [GSPC?9] report on raster extensions was also used for reference. However, 
the GCS solid image facility did not attempt to previde compatibility with these raster extensions. 
Rather the GCS solid image facility should be considered an alternate approach to raster extensions. 
General Goals The fundamental theme is to provide the user with a powerful, yet simple-to-use graphics 
package which supports the definition of arbitrary scenes consisting of both planar and linear primitives, 
appearance (color, patterns, etc.) attributes, lighting model definition and control, hidden object removal, 
and target device independence. It was deemed important that the addition of new functions preserve the 
basic syntax, semantics, and conventions that have been inherent in GCS since its inception. Picture 
segmentation functions, modeling transforms, viewing operations, and interactive procedures already implemented 
for line drawing should have the same effect on any raster, solid image display. The addition of routines 
for hidden object processing brings with it substantial execution and memory space overhead. Since not 
all users of GCS need these facilities, a method of minimizing the impact of these extensions on line 
drawing users was sought. It was not considered satisfactory, for example, to include the hidden object 
routines in a portion of GCS which would necessarily be loaded if only line drawing was to occur. However, 
it was considered imperative that no design restrictions exist on the complexity of a scene. Any implementation 
should permit arbitrarily dense pictures to be defined and processed. Only execution time should be affected 
since memory utilization is already substantial even without the hidden object/raster routines. Thus, 
collection of scene contents has been designed to occur on s direct access file. An important attribute 
of GCS has been the simplicity of its calling sequences. It was felt to be desirable to provide a mechanism 
whereby the existing GCS commands could be used in the specification of solid objects. Thus a general 
facility for producing complex closed polygons for the hidden object removal process was designed in 
which the definition of a single face of reasonably regular solid objects could result in the automatic 
definition of all faces of the object. Moreover, individual lines not part of faces are themselves treated 
as closed polygons. In this manner, solid objects can partially obscure line drawings. An important 
aspect of solid object generation is the specification of both the appearance of an object and the specification 
of the lighting conditions under which the object is viewed. There are two factors to be considered. 
First, the objects within the scene have appearance attributes. For example, the objects may vary in 
color, surface texture, or surface pattern. Secondly, the scene must be illuminated to be viewed. Both 
of these requirements are specified by GCS functions which provide for specification of the necessary 
information in a natural manner. Yet, full control is avail'able the title of the publication and its 
date appear, and notice is given that copying is by permission of the Association for Computing Machinery. 
To copy otherwise, or to republish, requires a fee and/or specific permissiou. for users who have the 
sophistication and desire for more intimate access. Differing applications require different types of 
hidden object removal. In fact, not removing the hidden objects may be necessary. Therefore, the hidden 
object removal process has been designed to produce wire-frame images (no hidden object removal), hidden-llne 
removal where the scene consists of lines, ghost-lina mode where the hidden lines are rendered as dashed 
lines, and hidden surface removal where the interior of faces will be shaded according to the lighting 
model in effect. This latter capability will only be possible on those devices which can support this 
capability. A final goal is that support should be provided for all types of raster devices~ This can 
be accomplished by generating as output soma form of compact raster representation such as run codes 
at the lowest device-independent level and with post-processing in the device-dependent routines to the 
required format. Memory and I/O requirements are then optimized for each specific device type. In summary, 
the general goals for the GCS solid image enhancements are these: Be consistent with GCS syntax, semantics, 
and conventions. Minimize additional overhead for non-hidden object users. Provlde handling of arbitrarily 
complex pictures. Provide good human engineering for the specifications of solid objects and their appearance. 
Support all types of raster and calligraphic devices efficiently. Solid Image Enhancements In this section, 
the extensions to GCS which have been implemented or are under development will be discussed. Included 
will be a description of the methodology behind each extension, the problems that were encountered, and 
the solutions that were used. The functions of these extensions have been divided into four categories: 
control, specification of the scene, specification of surface appearance, and hidden object removal. 
Control All parameters and control variables are set through calls to the two GCS mode and option setting 
routines, USET and UPSET. USET sets discrete modes mmemanicly while UPSET sets parameter values by specifying 
the name of the parameter and its continuous value. This helps to eliminate confusion in specifying the 
many options used by the hidden object removal routines as they are specified using English terms. This 
also results in programs which are more self-docummnting. As for all GCS modes and options, suitable 
defaults are chosen. With all hidden object processors, all data to be processed must be available at 
the time that the hidden object removal operation is invoked. That is, the data must be "collected" as 
the scene is defined. A control variable, set by the user, indicates whether data is to be collected 
or not. If data is to be collected, the data structure for the hidden object removal routines is also 
initialized and placed in mass storage. Thus users not interested in hidden object processing need not 
be burdened with overhead created by collection and data structure initialization. A final command is 
required to initiate the hidden object processing routines. Upon a call to UHIOE, all data "collected" 
will be processed and the resulting output sent to the device specified. Specification of the Scene 
 As mentioned previously, all objects to be processed by the hidden object routines must be constructed 
of closed polygons. Functions have been added to ~CS to allow user specification of closed polygons through 
the "face" facility. In addition, the side faces of objects may be automatically defined using the CCS 
"shell" facility. Faces are defined to be the area specified by the edges of a closed polygon. There 
are two types of faces: simple faces and complex faces. In both cases, no assumption is made concerning 
the convexity of the faces. Simple faces are composed of an initial "move" followed by one or more "draws". 
The resulting polygon will be closed by GCS if not closed by the user. Complex faces consist of a simple 
face followed by another "move" followed by "draws". Additional "move/draws" groupings may be added. 
Complex faces provide for the specification of holes within faces. An example might be the specification 
of a wall with windows. The outer edges of the wall are the simple face with each window being represented 
by a hole. Holes are totally transparent; i.e., holes do not obscure anything. Faces are specified in 
GCS by either of two mechanisms. The first method consists of a simple subroutine call of the form: 
CALL U3POLY (X_ARRAY,Y_ARRAY,Z_ARRAY,COONT) where X ARRAY Y--ARRAY are arrays containing the X,Y,Z coordinates 
ZARRAY of each vertex COUNT is the number of vertices in the polygon. Note that each call to I~POLY can 
only generate a single simple face. Each face will consist of a solid color with uniform blending options 
for all edges (see specification of surface appearance below). The second method for defining faces 
consists of two subroutines which indicate the start and end of a face definition. These two subroutines 
group line drawing commands which define a face. An example of this method is: CALL UIEGFC(XO, YO, Z0) 
CALL U)PE]4 (Xl, Yi, Zl) CALL U3PEN (XN,YN,ZN) CALL UENDFC Complex feces may be generated by interspersing 
U3MOVE commands between groups of U3PEN commands. With this method, it is possible to alter the surface 
appearance attributes which are associated with each edge and vertex. All faces will automatically be 
closed by GCS if the last user-speclfled vertex is not the same as the first. Using either method, it 
is possible to define non-planar feces. The hidden object routines have been designed to process such 
faces although glitches in the picture may result. As mentioned earlier, all text and nonpolygonal llnewerk 
is converted to degenerate 2-vertex polygons which allows them to be included in scenes processed by 
the hidden object routines. Greatly increased ease in generating the description of solid objects is 
provided by the "shell" facility. In GCS, the modelling transformation facility provides for the definition 
of user coordinate systems. If the first face of an object is assumed to lle in the X-Y plane of the 
user coordinate system, a simple projection of each llne in the face along the Z-direction of the coordinate 
system defines a four-sided polygon which can also be collected. The distance of the projection is user 
ad- Justable. For example, the following sequence of commands will generate a tube of length i0 units 
and diameter 2 units. The center of the front opening of the tube is at the origin of the user coordinate 
systems: CALL UPSET ("OEPTH",iO.) CALL USET ("9-ELL") CALL UCRCLE (0.,0.,2.) If the example code is 
enclosed within face definition brackets, the tube can be capped on its ends: CALL USET("FBF~£E") CALL 
UBEGFC(O. ,0. ,0. ) CALL UPSET ("DEPTH",iO.) CALL USET ("SHELL") CALL UCRCLE(O. ,0. ,2. ) CALL UE~IDFC 
The "FBFACE" parameter specifies that both a front and a back face to the tube is to be produced. All 
combinations of front and back faces are specifiable by suitable USET options and apply to all face construction. 
The example above takes advantage of a feature whereby two-dimensional figures such as circles, arcs, 
and software characters are always generated in the current X-Y plane. An interesting example in which 
the shell facility could be beneficial is the generation of demographic maps in which map sections have 
height based on the value of the data. Generation of shells is not restricted to extrusion from the 
current X-Y piane. In the general case, whenever shell mode is in effect, each end of the line is extruded 
by the "depth" in effect when each end point was specified. Shell mode is terminated by a CALL USET ("NOSHELL"). 
 80 Specification of Surface Appearance The first step in specifying the appearance of a polygon or 
face is to choose how each poiygon is to be colored. All colors are specified by an index into a color 
iook-up table. On devices in which the color look-up table is modifiabie, suitable routines are provided 
to load the table using any one of four color spaces: red/green/blue, magenta/yellow/cyan, grey-scale, 
and hue/saturation/intensity. The current setting of the color index is assigned to each vertex as it 
is input. In this way, faces of solid or variable color may be generated. An alternative color specification 
technique being investigated is to define a pixel array of color indices that are to be mapped onto a 
given polygon. This alternative would faciiitate the generation of painted surfaces as proposed by Foley 
[FO_EY78]. For devices which support intensity variation as wei1 as color variation, an intensity must 
be specified or calcuiated to fully define the appearance of the face. The intensities may be calculated 
by a lighting model implemented in GCS. The current lighting model is similar in form to that described 
in [BLINN77]: Z COMPUTE CONTRIBUTION OF EACH LIGHT SOURCE DO 100 1 = 1,NLITS COSei ="~"" "~i if ( 
COSe i < ~.~ ) THEN ei>90.O i nt (A) i = ,~'~ else int(A) I = LSi(i),Ii. 0 -TAMBN.__~_T.~,cesne NLITS 
J I I~ LSI (j)/ j=l 100 CONTINUE COMPUTER TOTAL INTENSITY NLfTS int(A) -E int(A)j + TAMBNT j=1 BYMBOLOGY: 
NLITS = NUMBER OF LIGHT SOURCES int(A) i = intensity at Location A due to Light Source i N = Normal at 
Location A L i = Vector to Light Source i FIGURE 1 -TAMBNT is the percentage of ambient light present 
and may be set by the following command. Default value is 50~. CALL UPSET('AMBIENT,,value) value is in 
range 0.0 -100.0 -n is proportional to the reflectivity of the face and is set indirectly by the UPSET 
option. Default = 50 percent of the range. CALL UPSET('REFLECTIVITY',VBiUe) value is percentage reflectivity 
-(0.0 - i00.0) value = 0.0 is a very dull surface and may also be obtained by CALL USET('MATTE') while 
lO0.O is a very shiny surface and may also be obtained by CALL USET ( "SHINY"). -LSI(i) is the intensity 
of light source i. As a polygon is being generated, an intensity value for each vertex is calculated. 
Multiple light sources and their locations may be specified. The routine ULIGHT defines the intensity 
and location (in user coordinates) for a particular light. If the light source is behind the face, no 
contribution is accumulated for that light source. Using the above methods, a wide variety of surface 
appearances may be defined. A polygon may be specified to be uniform or varying in color and/or intensity. 
If the color or intenslty varies, linear interpolation is used to determine the values at any location 
on the polygon. In GCS, "intensity" is considered to be an overali master brightness control which works 
in conjunction wlth the intensities which are a natural part of a color definition. It is often desirable 
to smooth the transition between polygons for the representation of continuous surfaces. Several options 
have been designed for GCS to provide for blending of polygons. The blending options include: no blending, 
biebding of color oniy, blending of intensity only, and blending of both color and intensity. Investigation 
of the best approach to implementing this feature is still underway. The background of any scene may 
also be assigned a color index and an intensity. This allows defining a background without the necessity 
for constructing a plane piaced behind all objects in the scene. Hidden Object Removal Hidden object 
removal occurs when the routine UHIDE is invoked. There are numerous algorithms available to perform 
hidden object removal. For this version, UHIDE is an implementation of a raster-scan class hidden surface 
removal algorithm, similar to one described by Myers [MYERS75]. This algorithm was chosen because of 
its combination of efficient performance and straightforward (and concise) internal logic. Myers' algorithm 
allows the number of pixels used in generating a picture to be modified. This is important since execution 
time of the algorithm is dependent on the resolution compiled as well as the scene complexity. A user 
may thus specify the number of pixels which will control the precision, and hence the execution time, 
required to generate it. For this project, the number of pixels used is set by first choosing the number 
of pixels in the vertical and horizontal directions spanning the entire display surface. The actual number 
of pixels used by the hidden object routines is then automatically calculated to be proportional to the 
size of the current viewport (see Figure i). Suitable defaults are provided for each device supported. 
 As in most raster scan hidden object algorithms, one plxel row is processed completely before proceeding 
to the next. But contrary to some implementations, all intensity and color values for every pixel of 
the current scan line are saved. If a visible polygon is encountered, the saved values are replaced by 
new ones. Other implementations output the intensity and color to the device for each pixel every time 
a visible polygon appears at that pixel. The "immediate output" method saves memory at the expense of 
I/O; however, it cannot be used on film recorders which add colors as they are received. Overwriting 
individual pixels would also add to the difficulty of processing partially transparent surfaces as they 
are rendered by adding fractional intensities of all surfaces which can be "seen" at a given pixel. 
Input to this type of algorithm must be in the form of closed polygons. Because of this restriction, 
the line clipper in GCS was replaced with a polygon clipper similar to one described by Sutherland [SUTH74]. 
The requirement of closed polygons also forced all linework and text to be converted to 2-vertex degenerate 
polygons. This conversion is performed only if data is being "collected" for hidden object processing. 
 The initial integration of this algorithm limited picture complexity to lO0 polygons. To process larger 
numbers of polygons, the picture area would be subdivided until less than lO0 polygons were present in 
the current division. Thls method required that much of the data be sent through the clipper at least 
as many times as there were subdivisions of the picture area. There were also cases where the subdivision 
method would fail, such as any scan line intersecting more than i00 polygons. The major problem with 
this method, though, was that even with a modest limit of I00 polygons, the hidden object data structure 
required six times the memory to load as did the hidden object 81 code itself. Even for simple pictures 
of only a few polygons, the turn around time was poor due to the large amount of computer memory required. 
To solve these problems, it was decided to "page" the data in and out as required using a random file 
scheme. A data buffer of only a few hundred words is now needed to process any number of polygons. However, 
users willing to trade increased memory utilization for execution efficiency may specify a larger buffer 
than the minimum.  Four modes of object processing are being implemented. Wire frame mode which bypasses 
all hidden object processing permits a "quick look" for interactive graphic users. Often this quick look 
will be used to decide if the image is worthy of further processing. Less interactive modes supported 
are hidden surface, hidden line, and "ghost-line" computations. When invoked, those latter modes produce 
solid image or random vector pictures from all output primitives "collected" by GCS. Hidden line removal 
operations will be performed by post-processing the scan-line output of the Myers Algorithm. Additional 
information is kept in the data structure which allows edges to be reconstituted as lines. The lines 
may be then directly drawn as solid lines or dashed lines (for ghost-line generation). Certain raster 
devices such as the Tektronix 4027, require that the lines be regrouped into polygons so that the automatic 
area fill avaiiable in those devices can be utilized. Implementation of the hidden line removal postprocessor 
is still underway. Example ProQrams Two examples of GCS solid-image output are contained in Figures 
2 and 3. Figure 2a displays a block with a hole in it produced using faces and shells. Figure 3a combines 
the face and shell facility with the use of the GCS Graphical Data Structures facility to produce the 
Sandla Laboratories logo. The programs which generated those examples on a raster display are shown in 
Figures Figures 2b and 3b. The logo structure was produced using llne drawing only versions of GCS and 
consists of eight calls to UARC for the edge of the logo and several calls to U3DRAW to produce the thunderbird. 
 Implementation is accomplished by dot density control using an algorithm within the device driver routines. 
No simple one-to-one correspondence exists between a pixel of some grey level produced by a graphics 
package and the presence or absence of a dot on the resulting plot. Pictures so processed ere crude but 
inexpensive and rapid to produce. Several low-cost color raster terminals have entered the market place 
which satisfy most general plotting, charting and text presentation applications. They are not intended 
for image processing, smooth shaded pictures or grayscale productions. Most are typicaily connected to 
a host through a standard RS-232 asynchronous serial line which is not suitabie for transfer of point 
plot (frame buffer) formats. Instead, a local processor expands highly compact commands thus reducing 
I/O requirements. The most notable command pecuiiar to those terminals is an "area fili" instruction 
which shades a portion of the display surface outiined by a ciosed polygon. Raster-oriented hidden object 
processors do not readily lend themselves to such output primitives but it appears possible to postprocess 
the data to handle them. It should be clear that a general purpose, device-iqdependent graphics package 
for production of rasterized images can not use any feature of a raster device to simpIify its task unless 
it is common to all devices. For exampie it is tempting to use the overwrite characteristics of a frame 
buffer to implement temporal priority hidden surface removal; however, the picture produced on a film 
recorder will be incorrect. It is more logical to implement a general hidden object processor at the 
device-independent ievel and display format and conversion procedures at the device-dependent level. 
In this way, hidden object scenes will appear similar no matter what type of target device is employed. 
The consequences of temporal ordering of output data on a particular device is left to the user who merely 
passes the output primitives through to the device driver. An alternate approach to insuring that a 
correct picture will appear on all devices is to incorporate a collection of aigorithms. Selection of 
the particular algorithm to be used could be accomplished by querying the device-dependent routines to 
identify the capabilities of the current device. While correct output can be accomplished by this scheme, 
widespread use can greatly increase the size of the graphics package thus reducing its computer-system 
independence.  There are several aspects of the implementation of the GCS solid-image enhancements which 
are of interest. Described in this section are considerations of raster device characteristics, an estimate 
of resource utilization, and a report on some of the difficulties encountered. Raster Device Characteristics 
 It is often overlooked that raster graphics devices are not alike in some very important respects. The 
most popular device is probabiy the bit map or frame buffer driven CRT. An element of refresh memory 
is assigned to each pixel on the dlspiay. Both color and/or intensity is specified by this combination 
of bits, usually after being mapped through a look-up table which expands the range of possible appearances. 
An important characteristic is that new information written into a pixel storage iocation completely 
overwrites any previous data. Hidden object processors may use this to advantage by outputting the scene 
in reverse order of depth. Objects which are nearer the observer overlay background objects aiready stored 
in the bit map. Handling transparent objects is somewhat more difficult since any opaque background objects' 
appearance should be merely tinted by that of a transparent surface and not overwritten. Aiso capable 
of driving a CRT from bistable memory are run-coded display processors. In contrast to the pixel map, 
run-coded displays encode a scan segment and not an individual pixel as its basic entity. For the majority 
of solid image scenes this format produces a significant reduction in display memory requirements. The 
cost of the display is not oniy less but real time "movie" playback is possible because less information 
must be trgnsferrad to update the image. Smooth shaded and extremely complex scenes may not be practical 
with this type of dispiay, however. Another common raster device is a film recorder. Unlike either the 
frame buffer or run-coded display, a COM recorder adds appearance data when writing any part of the image 
area. HO'~ it is relatively simple to handle transparent objects but it is more difficult to handie temporal 
priority hidden surface schemes. The color, intensity, and coordinate range of COM devices usually far 
exceeds any other raster device which places extreme performance demands on driving software packages. 
 Dot matrix printers, are widely used for alphanumerics and graphics production. Random vector and shaded 
graphics are possible but in contrast to the devices previously outlined, overstrike is not possible. 
For solid image production, shading Resource Requirements While the subroutines which perform the hidden 
line removal are rather large (about 2000 decimal words), those are not loaded uniess the user includes 
an invocation to the UHIDE routine which initiates the hidden object removal process. Overhead which 
must be ioeded supports the collection of polygons and recursive buffers for the polygon clipper. These 
routines and the default buffer add approximately another 2000 decimai words). These estimates do not 
include the hidden line postprocessor which wii1 probably add another 1000 words. The amount of execution 
time varies according to the complexity of the scene, the scene precision specified by the user, and 
the resolution of the display surface. Only a limited number of programs have been measured for execution 
time. Of the two example programs previously described, the first measured execution required approximately 
i6 seconds of CPU time and the second about 64 seconds. For timing runs, the output device was a Tektronix 
4014 with special point plot mode used to produce the images. The computer was a CDC6600. Much of the 
time was spent computing the special point plot mode parameters. However, no apparent delay resulted 
before the beginning of picture generation. Encountered Difficulties In four major areas, difficulties 
were encountered which are worthy of note. These areas are the pipeline structure, random file efficiency, 
iighting model application, and run code gener- ation. Each of those are described below. It shouid aiso 
be noted that many of the problems were the result of technical imcompatibilities between the way features 
were implemented in GCS and the way they had been previously impiemented at Purdue. The actual design 
amalgamation presented few problems. This is perhaps due to the desire to produce a stravm~an software 
package for evaluation. Pipeline The major difficu]tles within the pipeline consisted of two types. 
In the first type, the requirement to perform poIygon clipping as opposed to line clipping necessitated 
the replacement of the GCS iine ciipper with a polygon clipper derived from that of Sutherland and Hodgman 
[SUTH 74]. In the GCS line clipper, the pipeline ]ine drawing routine GCSLIN would call the ciipper with 
two end points and would be returned two (possibly clipped)  82 PROGRRfl SORCUR C C NOTEs NO SUFFER 
IS ALLOCATED IN THIS EXANPLE SO THE C DEFAULT BUFFER SIZE OF sos MILL DE USED. C C DEFINE COLOR LOOKUP 
TABLE PRRRRETERS C DIREHSION CORP(3),COMPE(3) DATA coHP,COflP24.,e.°e.°o.oe.,leS./ c c INITIALIZE GCS 
C CALL USTRRT C C SET MINDOU AHD UIEU C CALL UUIEM(a.,3.,4.°O.°O.oO.) CALL U3UHDO(-3.°3.o-3.°3.°-3.,3.) 
 C C SET UZEUPORT--USE PERCENT UNITS C CALL USET('PERCEHT UHITS') CALL UUUPRT(O.olOB.°O.,1SS.) C C 
SET FILE FOR H]DDEH OBJECT REROUAL ROUTINES C CALL UPSET('XPRECISIOH'oSIa.) CALL UPSET('VPRECISIOH'°51E.) 
 C C DEFIHE LIGHTING RODEL C LIGHT SOURCE I IS Iee~ BRIGHT AT (1B.°S.°15.) C CALL UPSET('ARDIEHT LIGHT 
PERCEHTAGE'olO.) CALL ULIGHTII.°IOS.°lO.oR.~I5.) C C SET SURFACE APPEARANCE FOR INTENSITY AND COLOR 
C TO UARY ACROSS FACE C CALL USET(*CHANGIRG') C C LOAD COLOR LOOKUP TABLE MITH SHADES OF DLUE C CALL 
UCLRTD('RGB'°I.,S.,CORP°CORP2) C C SPECIFY HIDDEN SURFACE REROUAL C CRLL USET('SURFRCE REHOURL') C C 
COLLECT DATA BUT DON'T DRAU IT C CALL USET('COLLECT') CALL USET('HODRAU') C C SPECIFY DRCKGROUHD C 
CALL USET('BRED') CALL UPSET('DZNTEHSITY'°ISS.) C C SPECIFY GENERRTIOH OF SHELL FACES C CALL USET('SHELL') 
C C SPECIFV GEHERATION OF FRONT AND BACK FACES C CALL USET('FDFACE') C C SPECIFY DEPTH OF SHELL C 
CALL UPSET('DEPTH'°-I.) C C DRAU PERIRETER OF FRONT FACE*-COLOR INDEX MILL DE 3. C CALL UPSET('COLOR 
INDEX'c3.) CALL USEGFC(1.°I.ol.) CALL U3DRAU(-I.°I.°I.) CALL U3DRAU(-I.°-I.°I.) CALL U3DRAU(1.o-I.,I.) 
CALL U3DRAM(I*oI.°I.) C C XHSERT A flOUR TO THE HOLE C CALL U3ROUE(S.S,S.5°I.) C C DRAU OUTLXNE OF 
THE HOLE C CALL U3DRAU(-O.SoO.5ol.) CALL U3DRAU(-S.5,-O.5,1.) CALL U3DRAU(B.5°-S.S,1.) CALL U3DRAU(D.5°S.5°I.) 
C C TERRIHATE THE FACE C CALL UEHDFC C C SET GLOBAL IHTEHSXTY C CALL UPSET('DRZSHTHESS'olBS.) C C GENERATE 
H;DDEH SURFACE REROUED XNRG[ C CRLL UHIDE C C TERRZNATE ¢ CALL UEND STOP END PROGRRfl TSZRD C C NOTEs 
NO DUFFER 1$ ALLOCATED IH THIS EXRflPLE SO THE C DEFAULT DUFFER SIZE OF See WILL DE USED* C C DEFINE 
COLOR LOOKUP TABLE PRRARETER$ C DZHEHSIOH CORP(3),CORPE(3) DATA CONP°COflPE/S.,S.,S.,ISS.oS.,D./ C 
C IHITZAL~ZE OC$ C CALL USTRRT C C SET FILE FOR HIDDEN OSJECT REHOUAL ROUTINES C CALL UPSET('H|DFILE'°?.S) 
C C SET PRECISION TO RE USED C CALL UPSET(°XPRECZSIOH'°51Eo) CALL UPSET('VPRECISIOH'.SIE.) C C DEFIHE 
LIGHTING MODEL C LIGHT SOURCE 1 IS ISeX BRIGHT AT (IS.,e.,15.) C CALL UPSET('RNBIEHT LIGHT PERCEHTRGE',IO°) 
 CALL ULIGHT(I.°ISS.,IS.°S.°I5.) C C SET SURFACE APPEARANCE FOR IHTEHSITY AND COLOR C TO URRY ACROSS 
FACE C CALL USET(°CHRNQIHO ' ) C C LORD COLOR LOOKUP TABLE UITH SHADES OF RED CALL UCLRTD('RGD°°I*oS.,COflP°COflPE) 
¢ C SPECIFY HIDDEN SURFACE REflOVRL ¢ CALL USET('SURFRC£ REHOURL') C C COLLECT DATA BUT DON'T DRAM IT 
C CALL USET('COLLECT') CALL USET('HODRRU') C C SPECIFY BACKGROUND C CALL USET('SBLUE') CALL UPSET('BIHTEHSITV°°ISS.) 
C C SPECIFY GENERATION OF SHELL FACES C CALL USET('SHELL') C C SPECIFY GEHERRT|OH OF FRONT AND SACK 
FACES C CRLL USET('FSFRCE') C C SPECIFY DEPTH OF SHELL C CALL UPSET('DEPTH'°IS.) C C SPECIFY GCS LIBRARY 
UORK F|LE C CALL UPSET(°LZRRRRY UORK FZLE'oI*) C C LOAD STRUCTURE LIBRARY INTO UORK FILE C CALL UTILTY('LORD'°E.) 
 ¢ C DEFINE USER COORDINATE $YSTEfl FOR DRRMZHG THUNDERBIRD C --AT ORIGIN° NO SCALE CHANGE° ROTATED 
3e DEGREES C RROUHD THE PREUIOU$ V-AXIS. C C CALL U3CSYS(S.°S.,O.,I.,I.,I.oS.,3S.°O.) C DRRU PERIRETER 
OF FRONT FACE--COLOR INDEX MILL IE 3. C CALL UPSET('COLOR INDEX',3.) CALL U|EOFC(S.°D.,D.) C C XHUOK[ 
THE THUNDERBIRD SYNSOL--SYRDOL CONSISTS OF C S ARCS, R ROVE. AND DRRUS TO OUTLINE THE THUNDERIZRD. C 
 C THE OTHER PRRRRETERS TRRHSFORN THE THUNDERBIRD C COORDINATE SYSTER TO THE USER COORDINATE SYSTER. 
 C C CALL U3CALLlll*,IS*°S.,D.8°S.R°I.,e.,S.,S.°,TSIRD,) C TERflZHRTE THE FACE C CALL UEHDFC C C SET 
GLOBAL INTENSITY C CALL UPSET(°|RZGHTHESS, IDD,) C C GENERATE HIDDEN SURFACE RENOUED INROE C CALL UHIDE 
C C TERRIHRTE C CALL UEHD STOP END 83  end points. Then GCSLIN would look at a notation flag to determine 
how to display the result. The reentrant polygon clipper is o true pipeline component which accepts lines 
at one end and transmits visible lines at the other. It also keeps track of the beginning and ending 
of polygons so that closed polygons ore guaranteed. This required that the GCSLIN routine be split into 
two parts with the polygon clipper in between. Since only visible polygons need be collected for hidden 
object processing, the collection process was placed at the output of the clipper. The second pipeline 
problem is concerned with the manner in which collection groups are associated with picture segments. 
Ideally, the hidden object processing should be the last operation before display of the picture. However, 
this would require considering the entire hidden object mechanism as device-dependent. For compatibility 
with the currently defined device-independent/ device-dependent interface of GCS, polygons are collected 
separately from segmentation. This means that, in the current implementatlons, a line drawing of a picture 
can be placed in one or more segments while the output of the hidden object removal must be placed in 
a single segment. Recently, a technique has been identified which may allow the reg1~uping of visible 
surfaces into their original segments. This would require maintaining segment membership information 
in the hidden object removal data structure. Until this has been investigated further, the impact on 
performance con not be known. Random File Efficiency Much of the time involved in implemenLing the solid-lmage 
extensions to GCS was spent in designing both the random file hondling routines and the paging data structure 
for greater effi- clency. Many of the problems encountered were the result of constraints placed on the 
GCS random file handling facility to retain its computer system-independent aspects. Examples of these 
constraints are the use of fixed length records and conservation of disk storage by reusing discarded 
records. Additlonal work was required to reimplement the computer-dependent random file focility for 
the computer being used to remove some operating system required operotions. Lighting Model Applicotions 
 In this area, there are still some unresolved problems which are encountered when trying to implement 
support for more sophisticoted lighting models. An example is control of the blending of surface appearance 
over edges and vertices to provide o smooth appeorance. Presently, support for blending is still being 
investigated. Another example is the implementation of support for highlights within faces. These highlights 
are a result of reflections of the light source towards the viewer. While it appears possible to provide 
this kind of output, additional work is required. Run Cede Generation The output from Myers' hidden 
surface removal routine is o scan line buffer containing the color ond intensity ot each plxel. To provide 
support for devices which con directly occept run code input, the scon line buffer must be post-processed 
to generate these run-codes. A devlce-dependent run-cede processing routine has been added to GCS which 
either sends the run codes to the device or simulotes the run codes using point-plotting and/or line 
drawing. The post processing of the scan llne buffer requires that changes in color and intensity be 
detected. The most efficient tolerance for these changes is still being investigoted. Summary Preliminary 
use of the prototype implementation of the GCS solid-image enhancements indicates that GCS is an apprppriote 
environment for providing these capabilities. Not only is solid-image GCS fully upwards compatible from 
the current wire frame GCS, but the face and shell facilities have proven to be an innovative ond powerful 
new tool in scene definition. However, much work still remoins in completing the development. Not yet 
implemented ore such features as hidden line removal, polygon blending, highlighted faces, and transparency. 
 Acknowledgements An initial impetus for this project came from work underway at George Washington University 
under the direction of James Foley. Additional impetus was provided by the availability of a working 
implementotlon of the Myers' algorithm at Purdue University. Finally, appreciation is given to Harold 
Spohr, Scot Flshburn, David Darsey, Randall Simons, ond I. Miehelle Kiueiewicz of Sondia Laboratories 
-Albuquerque and James Jones of the U.S. Army Waterways Experiment Station for their support and constructive 
ideas. REFERENCES [BLINN77] [FOLEY78] [GAaR74] [GSPC79] [MYERS75] [PHONG75] [PUK77] [SUTH74] Bllnn, 
J.F., "Models of Light Reflection for Computer Synthesized Pictures," Computer Graphics, Vol. ll, No. 
2, pp. 192-198, Summer 1977. Foley, J.D., Templemon, J., Dastyor, D., "Roster Graphics Extensions to 
the Core Graphics System," Final Report, Contract DACA39-78MO073, Weterwoys Experiment Stotion, U.S. 
Army Corps of Engineers, August, 1978. Gobriel, H, et.ol., "Introduction to the Graphics Compotibility 
System," NTIS No. AD-778 750, U.S. Deportment of Commerce, Morch 1974. ACM-SIGGRAPH, "Stotus Report 
of the Graphics Standards Planning Committee," Computer Graphics, Vol. 13, No. 3, August 1979. Myers, 
Allan J., "An Efficient Visible Surface Program", NSF Report for Grant #UCR 74-00768 AO1, 1975. Phong, 
Bui-Tuory, "Illumination for Computer Generated Imoges," CACM 18, pp. 311-317, June 6, 1975. Puk, Richord 
F., "Graphics Compatibility System Reference Monuol", U.S. Army Waterwoys Experzment Stotion, Vicksburg, 
MS, October, 1977. Sutherlsnd, I.E. and Hodgman, G.W., "Reentrant Polygon Clipping," CACM, Jonuary 1974. 
 85  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1980</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>807474</article_id>
		<sort_key>86</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-1980</article_publication_date>
		<seq_no>15</seq_no>
		<title><![CDATA[QUADRIL]]></title>
		<subtitle><![CDATA[A computer language for the description of quadric-surface bodies]]></subtitle>
		<page_from>86</page_from>
		<page_to>92</page_to>
		<doi_number>10.1145/800250.807474</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=807474</url>
		<abstract>
			<par><![CDATA[<p>Most man-made objects can be closely approximated by bodies whose surfaces are composed of portions of second-order (quadric) surfaces. These surfaces include elliptic, hyperbolic, and parabolic cylinders, as well as quadric cones, paraboloids, hyperboloids, ellipsoids, and pairs of planes. Simple planes (first-order surfaces) may be included as degenerate quadric surfaces.</p> <p>Because these quadric-surface bodies are so useful for modelling man-made objects, it is important that any Computer-Aided Design (CAD) system be able to work with such bodies. The &#8220;QUADRIL&#8221; language described here was designed to accept descriptions of quadric-surface bodies in character-string form.</p> <p>QUADRIL has a mixture of English-like and algebraic syntax. It may be used to specify quadric-surface bodies and then to display them on various media.</p> <p>QUADRIL will accept descriptions of quadric-surface bodies either as &#8220;volumetric&#8221; combinations of basic bodies, or as boolean functions of bounding surfaces. English-like syntax is used for specifying what surfaces and basic bodies are used, while algebraic syntax is used to transform the canonical forms of the surfaces or bodies into the shape, position, and orientation that the user desires.</p> <p>Volumetric combination of bodies involves the operations of union (+), intersection (*), and subtraction (-). Boolean specification of volumes is in terms of a boolean tree with the bounding surfaces as leaf nodes. The tree is expressed as a character string.</p> <p>QUADRIL permits using user-created &#8220;STRUCTURES&#8221; as component bodies (&#8220;OBJECTS&#8221;) in greater STRUCTURES.</p> <p>The display of the quadric-surface bodies may also be specified in QUADRIL. The user is considered fixed in space, while the body is transformed to give the desired view.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Computer languages]]></kw>
			<kw><![CDATA[Geometric modelling]]></kw>
			<kw><![CDATA[Quadric surfaces]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.5</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Geometric algorithms, languages, and systems</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061.10010063</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures->Computational geometry</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Languages</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP14143052</person_id>
				<author_profile_id><![CDATA[81408594433]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Joshua]]></first_name>
				<middle_name><![CDATA[Zev]]></middle_name>
				<last_name><![CDATA[Levin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Boeing Computer Services Co.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Roberts, L.G. "Machine Perception of Three-Dimensional Solids" Technical Report No. 315, Lincoln Lab., Massachusetts Institute of Technology, Cambridge, Massachusetts.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>321330</ref_obj_id>
				<ref_obj_pid>321328</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Weiss, R. "BE VISION, a Package of IBM 7090 Programs to Draw Orthographic Views of Combinations of Planes and Quadric Surfaces" Jour. ACM 13:2 (April 1966) pp. 194-204]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>889976</ref_obj_id>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Coons, S.A. "Surfaces for Computer-Aided Design of Space Forms" MAC-TR-41, N.T.I.S., Springfield, Virginia, 1967]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>321468</ref_obj_id>
				<ref_obj_pid>321466</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Comba, P.G. "A Procedure for Detecting Intersections of Three-Dimensional Objects" Jour. ACM 15:3 (July 1968) pp. 354-366]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Woon, P.Y. "A Computer Procedure for Generating Visible-Line Drawings of Solids Bounded by Quadric Surfaces" Technical Report 403-15, Department of Electrical Engineering, School of Engineering and Science, New York University, November 1970 (AD-724 744)]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Woon, P.Y., and Freeman, H. "A Computer Procedure for Generating Visible-Line Projections of Solids Bounded by Quadric Surfaces" Information Processing 71, Vol. 2, North-Holland Publishing Co., Amsterdam, 1971, pp. 1120-1125]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Mahl, R. "Visible Surface Algorithm for Quadric Patches" IEEE Trans/Comp C-21 (1) (January 1972) pp. 1-4]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Catmull, E. "A Subdivision Algorithm for Computer Display of Curved Surfaces" UTEC-CSc-74-133, University of Utah, Salt Lake City Utah, December 1974]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Engeli, M., and Hrdliczka, B. EUKLID, eine Einfuerung" Fides Treuhand-Vereinigung, Zurich, Switzerland, 1974]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_obj_id>360727</ref_obj_id>
				<ref_obj_pid>360715</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Braid, I.C. "The Synthesis of Solids Bounded by Many Faces" Comm. ACM 18:4 (April 1975) pp. 209-216]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_obj_id>360839</ref_obj_id>
				<ref_obj_pid>360825</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Bui-Tuong Phong "Illumination for Computer Generated pictures" Comm. ACM 18:6 (June 1975) pp. 311-317]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Levin, J.Z. "A Parametric Algorithm for Drawing Pictures of Solid Objects Bounded by Quadric Surfaces" Technical Report CRL-46 Rensselaer Polytechnic Institute, Troy, N.Y. March 1976 (AD-A032 921)]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_obj_id>360355</ref_obj_id>
				<ref_obj_pid>360349</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Levin, J. "A Parametric Algorithm for Drawing Pictures of Solid Objects Composed of Quadric Surfaces" Comm. ACM 19:10 (October 1976) pp. 555-563]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_obj_id>356750</ref_obj_id>
				<ref_obj_pid>356744</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Carlbom, I., and Paciorek, J. "Planar Geometric Projections and Viewing Transformations" ACM Computing Surveys 10 (4) (December 1978) pp. 465-502.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Levin, J.Z. "Mathematical Models for Determining the Intersections of Quadric Surfaces" Technical Report CRL-61, Rensselaer Polytechnic Institute, Troy, New York, 1978]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Levin, J.Z. "Mathematical Models for Determining the Intersections of Quadric Surfaces" Journal of Computer Graphics and Image Processing, Vol 11 (1979), pp. 73-87.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Levin, J.Z. "QUADRIL: A Computer Processor for the Design and Display or Quadric-Surface-Bodies", Technical Report IPL TR-80-003, Rensselaer Polytechnic Institute, Troy, New York, April 1980]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 QUADRIL: A Computer Language for the Description of Quadric-Surface Bodies by 3oshua Zev Levin Boeing 
Computer Services Co. ABSTRACT Most man-made objects can be closely approximated by bodies whose surfaces 
are composed of portions of second-order (quadric) surfaces. These surfaces include elliptic, hyperbolic, 
and parabolic cylinders, as well as qua- dric cones, paraboloids, hyperboloids, ellipsoids, and pairs 
of planes. Simple planes (first-order surfaces) may be included as degenerate quadric surfaces. Because 
these quadric-surface bodies are so useful for modelling man-made objects, it is important that any Computer-Aided 
Design (CAD) system be able to work with such bodies. The "QUADRIL" language described here was designed 
to accept descriptions of quadric-surface bodies in character-string form. QUADRIL has a mixture of English-like 
and algebraic syntax. It may be used to specify quadric-surface bodies and then to display them on various 
media. QUADRIL will accept descriptions of quadric-surface bodies either as "volumetric" combinations 
of basic bodies, or as boolean functions of bounding surfaces. English-like syntax is used for specifying 
what surfaces and basic bodies are used, while algebraic syntax is used to transform the canonical forms 
of the surfaces or bodies into the shape, position, and orientation that the user desires. Volumetric 
combination of bodies involves the operations of union (+), intersection (*), and subtraction (-). Boolean 
specification of volumes is in terms of a boolean tree with the bounding surfaces as leaf nodes. The 
tree is expressed as a character string. QUADRIL permits using user-created "STRUCTURES" as component 
bodies ("OB3ECT5") in greater STRUCTURES. The display of the quadric-surface bodies may also be specified 
in QUADRIL. The user is considered fixed in space, while the body is transformed to give the desired 
view. Permission to copy without fee all or part uf this material is granted provided that the copies 
are not made or distributed for direct co1~erclal advantage, the ACM copyright notlce and the title of 
the publicatlon and its date appear, and notice is given that copying is by per~L~sslon of the Association 
for Computlng Machinery. To copy other~se, or to republish, requires a fee and/or specific permission. 
 01980 ACM 0-89791-021-4/80/0700-0086 $00.75 Key Words and Phrases: Quadric Surfaces, Geometric Modelling, 
Computer Languages. CR Categories: 3.21, 3.23, 3.26, 3.29, 4.13, 4.22, 8.2 ACKNOWLEDGMENTS: The author 
wishes to thank Professor Herbert Freeman of Rensselaer Polytechnic Institute for the suggestion of this 
research topic and for the continued guidance of the research. This work was partially supported by the 
National Science Foundation, Automation, Bioengineering, and Sensing Program, under Grant No. ENG 79-04821. 
GLOSSARY Basic Body: An intrinsically-defined quadric-surface body. Body: A solid bounded by quadric 
surfaces. Object: A structure or basic quadric-surface body used as part of a larger structure. Quadric 
Surface: A second-order surface; the locus of points in three-space for which a second-order function 
in three variables evaluates to zero. QUADRIL: (QUADRIc-surface body Language): The textual language 
described in this paper. QUISP: (QUadric-surface InterSection Processor): The program that supports 
QUADRIL, as well as interactive quadric-surface body specification. Quadric-Surface Body: See Body. Structure: 
A quadric-surface body built (or being built) by QUISP. Chapter 1. INTRODUCTION Chapter 2. LANGUAGE 
SPECIFICATION 1.1 Statement of the Problem The specification and display of three-dimensional bodies 
and their two-dimensional pictures are major problems in computer graphics. Specification of a body may 
be done in many different ways. The user may specify the equations of the surfaces bounding the body, 
or specify points on the body's surface; but these approaches are both tedious and error-prone. A powerful 
descriptive language for specifying surfaces and bodies is preferable since it is easier for the user 
to learn and use, is less error-prone, and makes it easier :[or one user to understand another user's 
specifications. A quadric-surface body may be expressed as a boolean tree. The leaf nodes of this tree 
are elementary boolean functions that take the value TRUE if the point being tested is inside the specified 
surface, and FALSE if it is outside. A point is inside the body if the boolean tree evaluates to TRUE, 
and it is outside if it is FALSE. The quadric-surface body is the locus of points satisfying the boolean 
tree. It is possible to write a text string representing such a boolean tree, and thereby represent the 
body. However, in many cases this is not suitable. It is usually easier to think of a body as being constructed 
from elementary quadric-surface bodies by the processes of intersection, union, and intersection with 
complement. 1.2 Work Done by Others There are three approaches to describing and displaying curved surfaces. 
One is to approximate them by many small planar faces, so that planar-face body algorithms may be used 
to specify and display curved bodies. These have been available since Roberts (I963). The chief disadvantages 
of this approach are the large number of planar faces that must be processed, implying large core-memory 
and time requirements, and roughness (discontinuities) in the pictures produced. Bui-Tuong Phong (1975b) 
produced smoothly shaded pictures using planar-faced bodies, but the silhouettes were still rough. Another 
approach is to use high-order parametric patches, as introduced by Coons (1967). Each point on the surface 
is determined by equations for x, y, and z in two parameters (u and v), for its own local patch. Catmull 
(197/~a) dealt with bi-cubic patches, producing excellent shaded pictures. The third approach is to use 
quadric surfaces directly. This has been done by Weiss (1966), Mahl (1972), and Woon (1971a, 1971b). 
Comba (1968) also dealt with some quadric surfaces, but not all. The author has also developed a quadric-surface 
algorithm (1976a, 1976b, 1978b, 1979, 1980). The language EUKLID (1974b) permits taking two bodies and 
getting their union, intersection, or difference, but only for planar=faced bodies. Braid (1975a) permits 
adding some planar-faced bodies together with circular cylinder=faced bodies to create three-dimensional 
objects. QUADRIL is a language for specifying quadric-surface bodies and drawing pictures of them. It 
has English-like syntax and vocabulary to make it easy to learn and easy to use. It is possible to have 
QUISP, QUADRIL's host, support other input languages, such as an extended version of EUKLID (197ttb), 
perhaps called "EUQLID". The following discussion is an informal description of QUADRIL. A more formal 
exposition of the language, including Backus-Naur Form (BNF) grammar, may be found in the author's doctoral 
dissertation (Levin 1980). 2.1 Basic Grammatical Rules and Delimiters A QUADRIL "text" consists of the 
specifications of one or more quadric-surface bodies, plus commands to control the generation of pictures, 
listings, and mass-storage files. The text is divided into "sentences" or "statements", each one of which 
terminates either in a semicolon (";"), or in a period followed by a blank space (,,. ,,). A sentence 
consists of a string of identifiers and numbers, separated by delimiters. These delimiters are arithmetic 
signs, punctuation marks, and blanks. The identifiers are keywords, which have fixed meanings; and element 
names, which are made up by the user, and refer to structures, objects, and quadric surfaces. Naturally, 
the element names cannot duplicate keywords, nor can they duplicate names of other currently used elements. 
Comments are enclosed in square brackets ("[", "]"). They are totally ignored by the QUADRIL processor. 
Comments may be nested. A right square bracket ("]") will not end the comment string unless it matches 
the leftmost left square bracket ("["). This is so that the user may use comment brackets to effectively 
elide part of the text, even if it contains comments already. Many operations are specified by having 
a keyword followed by a number, optionally separated by an equal's ("=") sign. A "number" is any decimal 
floating-point number, or a simple expression of such numbers, using the operations of addition (+), 
subtraction (-), multiplication (*), and division (/). Multiplication and division have greater binding 
strength than addition and subtraction. Other than that, evaluation is performed left-to-right, unless 
modified by parentheses. 2.2 Algebraic, Boolean, and CombinedNotations There are two different ways 
to describe a quadric-surface body; as a combination of basic quadric-surface bodies; or as a locus of 
points satisfying a boolean :[unction whose leaf nodes are the insides and outsides of quadric surfaces. 
Once a "structure" has been established by combining basic bodies, it may be used as an "object" and 
combined with other basic bodies and objects to form a higher-level structure. A structure is represented 
in QUISp's tables as a boolean tree of surfaces and objects. The user may represent structures either 
as boolean trees, or in an structure, but also sets up some important parameters. B notation, it also 
defines the structure's Boolean tree. In algebraic notation that is converted into a boolean tree by 
QUADRIL. Structure Parameters With the A-(algebraic) notation, the user can put together objects in 
terms of an algebra-like string of objects and the three basic operators. This is equivalent to the "volumetric" 
approach of Braid (1975a). The B (Boolean) notation describes a quadric-surface body as the locus of 
points satisfying a Boolean tree whose leaf nodes are the insides and outsides of quadric surfaces and 
quadric-surface bodies. The B notation is generally more difficult to use, but is more general and allows 
the user to do more. The C (combined) notation is a combination of the A and B notations, combining generality, 
flexibility, and ease of use. See section 2.6 for details. 2.3 Transformations Geometric transformations 
are used in so many places that they are discussed separately here. The following transformation specifications: 
displace- ment, rotation, scaling, and skewing, are used to perform three-dimensional transformations 
on quadric surfaces, basic bodies, and objects. When more than one transformation is specified, they 
are done sequentially and cumulatively. Displacement transformations may be in the x-(X or DX), y- (Y 
or DY), or z- (Z or DZ) directions. To displace a body by five units in the x-direction and two units 
in the -z-direction, one may use: "DX -- 5, Z -- -2". A rotation transformation may be about either the 
x-(ROTX, TW or TWIST), y-(ROTY, ELEV, or ELEVATION), or z-(ROTX, AZIM, or AZIMUTH) axis, in a counter-clockwise 
sense. To rotate the y-axis of a body into the z-axis (about the x-axis), one uses the transformation: 
"ROTX = 90". To rotate about the y-axis so that the x-axis moves 60 ° towards the z-axis, one may use: 
"ELEV = -60". A scaling transformation may be specified for one (X, Y, or Z), two (XY or YX, YZ or ZY, 
or ZX or XZ), or all three (XYZ) directions, as: "SCALE/dir = num" or "SCL/dir = hum". To scale a body 
along the y- and z- axes by one-third simultaneously, one may use: "SCL/ZY = 1/3". The skew transformation 
will tilt one of the AXES in a specified DIRECTION. As an example, "SKEW X/Z = 0.5" will cause the body 
to be transformed so that its old x-axis is tilted upwards (positive z) by an angle of Arctan 0.5. 2 
.# Specifying Structure Names and Parameters The STRUCTURE is the most important organizational unit 
in QUISP. It represents both a physical structure (the body being modelled) and a memory structure. The 
structure declaration sentence not only sets up a The structure definition can specify several useful 
parameters which help determine various tolerances and sizes. The RMAX parameter is the maximum distance 
from from the origin to any point in the structure. The SCALE gives the length of one length unit, either 
defined in terms of inches, centimeters, or millimeters, or in terms of the current length unit. The 
RESOLUTION (or RES) is the minimum distance that two points can be from each other and be considered 
distinct for many of the sub- processes. If the parameter name is specified with no number, the system 
default value is used. If a parameter is not specified at all, the parameter value from the previous 
structure is used, except for the first structure, for which the system default value is used. The Boolean 
Tree The Boolean tree is used to describe a structure in B notation. Any point for which the Boolean 
tree evaluates to TRUE is inside the structure, whereas any point for which it is FALSE is outside the 
structure. The leaf nodes of the Boolean tree are objects or surfaces. The first leaf node must have 
either INSIDE or OUTSIDE. If neither is present for any subsequent leaf node, the last one used is taken. 
As an example, the string: "(INSIDE ALBERT OR BOOMER) AND CHARLY, OR OUTSIDE DURWOOD OR EDUARDO" is equivalent 
to the tree: <or> / \ <and> OUTSIDE OUTSIDE / \ DURWOOD EDUARDO INSIDE <or> INSIDE INSIDE ALBERT BOOMER 
 2.5 Specifying Objects and Quadric Surfaces The statements described in this section specify the components 
of quadric-surface bodies --the quadric surfaces and the objects. In each statement, the user specifies 
the name of the element, what surface or object it is, and (optionally) its spatial transformation, its 
Boolean bounds, and what other surfaces and objects it intersects with. One may also specify a many-fold 
body, which is the union of several objects of the same structure, with a specified transformation from 
each object to the next.   Basic Quadric-Surface Bodies Figures 1 through 9 contain pictures of the 
basic quadric-surface bodies. The equations given are those of the main surfaces. The equation of the 
ordinary surface The SIMPLE, DOUBLE, and INVERTED attributes areis usually that of the DOUBLE body, unless 
as noted. mutually exclusive. When using the B-Notation, the user may deal with the An auxiliary surface 
(qualifiers: AUXIL or mathematically-defined quadric surfaces, not the basic AUXILIARY) is usually strictly 
a bounding surface, or is quadric-suriace bodies or other objects. These are (with for drawing meridians 
and parallels on surfaces and the exception of the ellipsoid) infinite along at least one objects to 
make them more visible during interactive axis. processing. A gridded surface or object (qualifier: GRIDDED) 
is   Names of Quadric Surfaces and Bodies displayed (when indicated) with dashed curves wherever it 
intersects its canonical u-v, v-w, and u-w planes, to makeA quadric surface or a quadric surface body 
is the surfaces more visible for interactive processing orspecified as the name of a quadric-surface, 
preceded by display. one or more qualifiers. QUADRIL gives the user the option of explicitlyAn ordinary 
surface (no qualifiers) is the mathematical specifying the ten coefficients of the quadratic-equationsurface. 
With the exception of the ellipsoid, these are form of a quadric surface, rather than specifying the 
 infinite in spatial extent. surfacers name. An inverted surface (qualifier: INV or INVERTED) is the 
same, but with all elements of the equation multiplied The RATIO Specification by -1. The center of 
an ELLIPSOID is INSIDE the surface. The center of an INVERTED ELLIPSOID is OUTSIDE the If the surface 
of body is a hyperboloid or a hyperbolic surface. cylinder, one may specify a RATIO number. This number 
is the ratio between the closest approach of the two arms A double object (qualifier: DOUBLE) is bounded 
by of a hyperbolic cross-section of the body, and the body's the ordinary surface with the same specification 
name, maximum dimension. To get a cooling tower whose waist plus a few planes. A double object usually 
fits within a is 6096 of its diameter, use a "DOUBLE HYPERBOLOID two-by-two-by-two-unit box which is 
centered on the OF ONE SHEET WITH RATIO : 3/5". origin. A simple object (qualifier: SIMPLE) is like half 
of Specifying Surface Intersections double object, usually within a one-by-one-by-one-unit box whose 
bottom is centered on the origin. For the Surface intersections may be declared simply byHYPERBOLOID 
OF ONE SHEET and for the stating that a surface MEETS (or IS MEETING) another HYPERBOLIC CYLINDER, the 
maximum dimension may be larger by a factor of the square root of two. DOUBLE CONE: ¢2+y2-z2 = 0 IHPLE 
CONE: 4x2+4~2-(1-~) 2 = 0 DOUNLE NYPERBOLOID OF ONE SREET: z Ratio unspeclfled: z2+ 2-z2-1 = O " specified: 
z2+y2-(1-r~)a 2-r2 ffi 0 I Ratio unspec: 4x2+fy2-fa-1) 2-1 = O ............ DOUBLE PARABOLIC CYLINDER: 
¢2_y . 0 ...... i~ ..... 'tspec : 4~2+4y2-(1-p2),(~-lJ2-r2~ =0 ~~ SIHPLE PARABOLIC CYLINDER: 2x2-~ O 
7 If ratio not spec., DOUBLE body ££ts tn ~---I~. I Sf,PLE .... y body ms t~ t~/ f~ x f~ x t box. z 
/ ] HYPERBOLOID Pig. 7 CONE FIE. ~ PARABOLIC x/~--¥ OF ONE SHEET z~ CYLINDER DOUBLE ELLIPTIC PARAROLOIDt 
z HYPERROLOID OF TNO SHEETS: a z2+yl-z = 0 Ratio unspecified: ¢2+~/2-z2+1 = O | SITIPLE ELLIPTIC PANASOLOIO: 
 DOUBLE ELLIPTIC CYLINOER: J DODNLE liYPERBOLOIU OF T~IO SBEETS:* ~1--7"..-,,..~ Z2÷92-I 0 ~I~[ ,Iz2+4lyz~f2-z) 
= 0 = (1-p2)(:C2+V2)-Z2÷p2 = O SIHPLE ELLIPTIC CYLINDER: I/ 4Z2÷dy2-/ = 0 ~ ............ SIIIPLE HYPERUOLOID 
OF TI#O SHEETS:* --- Pig. 8 ELLIPTIC 4(1-rZJ (¢2+92)- (,1z-2)2÷~2 ffi 0 # .... /y Z}\ " specified. r~clo 
, " Fig. 5 ELLIPTIC CYLINDER X/ OF TNO SHEETS ~/ z/ z DOUBLE ELLIPSOID: z2÷¥2÷mz-/ -0 HYPERBOLIC PARABOLOIB: 
~ ~ SIHPLE ELLIPSOIDt z2+y2÷12-1/4 -0 ".poo..d, EIIIPLE IIYPERBOLIC CTLIUDENt Upper surface: x2-y2+a-Z 
-0 I \ / / ] Ratio unspcctfiedl 4x2-4 2+I 0 --~--Fig , 9 ELLIPSOID Lower surfacer z2-y2+a÷l -0 ~/~t --~ 
 DOUBLE ltYPERBOLIC PARADOLOID: I ~'~"*~f~Y [ SINPLE BTBERSOLIC PARASOLOID: i t 4z2-4y2+2.~-2 = 0 I[ 
Ca:tO IS not I I z .... ,d,,eo.ooI'i°d" rL~ [:::l°i: 1_'_q/ ,o ..t~t. ,Si.L.o. rig. 3 ttYPERBOLIC X~ 
V PARABOLOID z/ CYLINDER z/ surface. One purpose of declaring an intersection is so that intersection 
curves on a smooth surface will be displayed, in addition to those on edges. 2.6 Algebraic Combinations 
of Bodies This section describes how to use the "handier" algebra-like approach to building quadric-surface 
bodies. The A-Notation In the A-(algebraic) notation, objects may be combined using these three operations: 
The union (+) of two bodies is the locus of all points in either of the two bodies. The intersection 
(*) of two bodies is the locus of points in both of the bodies. The intersection with complement or subtraction 
(-) of two bodies is the locus of points in the first body, but not in the second. Evaluation of an expression 
is done from left-to-right, unless modified by parentheses. All three operators have equal precedence. 
As an example, a cylinder surmounted by a cone may be represented as: HUT IS A STRUCTURE WITH RMAX = 
2. WALL IS A SIMPLE CYLINDER. ROOF IS A SIMPLE CONE WITH DZ = I, HUT = WALL + ROOF,  The C-Notation 
The C (combined) notation is a combination of the A and B notations. Using the C-notation, one can put 
this example in a more compact form: HUT (A STRUCT WITH RMAX = 2) = WALL (SIMPLE CYLINDER) + ROOF (A 
SIMPLE CONE WITH DZ : I). The C-notation allows one to set up a single sentence with algebraic syntax 
of the A-notation, while allowing all the special features of the B-notation. 2.7 DISPLAY Commands The 
display commands create two-dimensional pictures on a graphics terminal screen, or on a specified device. 
The user can specify the following properties: Type of Picture There are three main types of picture: 
A FULL picture shows all curves. A DASHED=HIDDEN-CURVE picture shows all hidden curves as being dashed. 
A VISIBLE-CURVE picture shows only the visible curves, eliding all those which are invisible from the 
vantage point. A GRIDDED picture shows the dashed grid=lines of gridded surfaces and objects, which are 
not normally displayed. Display Parameters The picture parameters control the setup of a picture. The 
view parameters control the drawing of a particular "view" of a picture. The ORIGIN parameter indicates 
perspective view, and specifies the distance to the origin. The STEREO parameter indicates that a stereo-optic 
pair of pictures is to be produced. The FINENESS (or FN) parameter indicates the maximum area between 
a curve and the straight line segment approximating it. The SCAN parameter indicates the width of the 
scan-line for raster-scan output devices. Finally, the DASH parameter indicates the length of the dashes 
in dashed curves. If the parameter name is specified with no number, the system default value is used. 
If the view parameter is not specified at all, the parameter value from the structure is used, Picture 
Transformation The object may be transformed by any transformation prior to its display. QUISP uses a 
fixed viewpoint, with the body being moved, not the observer. The observer is assumed to be on the positive 
x-axis, facing the origin, and the picture plane is the y-z plane.  View Transformation After the picture 
has been set up, it may be transformed in two dimensions by using a subset of the standard transformations. 
Scalings and displacements are permitted in all three dimensions, but those in the x-direction (line-of-sight) 
will be ignored. Rotations about the x-axis and skewings involving y and z are also permitted. 2.8 Miscellaneous 
Commands The LIST and DUMP commands cause a listing of QUISP's common tables to be made. LIST gives a 
structured, interpreted listing, which may be very useful to a user. The DUMP command puts out the tables 
in raw form, which is most useful for the programmer in detecting bugs. The LOOK command enters QUISP's 
interactive mode, displaying the structure on the screen if called from a graphics terminal. The SAVE 
command stores a mathematically-processed structure, and the structures it references, on a mass-storage 
device. The RECALL command retrieves such stored structures. Chapter 3. ILLUSTRATIVE ~EXAMPLES Screw 
This test object includes only planar curves, and is written in B-notation exclusively. The helical screw 
thread is simulated with a set o:E fifteen hemi-ellipses. [ THE SCREW BODY: ] SCREW IS A STRUCTURE WITH 
RMAX = 7.25 AND RES = .005, INSIDE HEAD, BEVEL, AND HEAD.PLANE, AND (INSIDE LEFTSLOT OR RIGHTSLOT OR 
SLOTBOT), OR INSIDE SLOTBOT, SHAFT, AND TIP. HEAD IS A SPHERE WITH SCALE = 5. BEVEL IS A CONE WITH SCALE/Z 
= /4/3. LEFTSLOT IS A PLANE WITH ROTX = -82. RIGHTSLOT IS A PLANE WITH ROTX = 82. SLOTBOT IS A PLANE 
WITH DZ = 4.25. HEAD.PLANE IS AN INVERTED PLANE WITH DZ = 3. SHAFT IS A CYLINDER. TIP IS A PARAB WITH 
SCALE /Z = 2 AND DZ = -7. [ AUXILIARY PLANES: ] PX IS A PLANE WITH ROTY = 90, MEETING SLOTBOT, LEFTSLOT, 
RIGHTSLOT, HEAD, BEVEL, SHAFT, HEAD.PLANE, AND TIP. PY IS A PLANE WITH ROTX = 90, MEETING SHAFT, TIP, 
BEVEL, HEAD, SLOTBOT, AND HEAD.PLANE. [ THE SCREW THREAD: ] SHAFT MEETS T1, T2, T3, T4, TS, T6, T7, TS, 
Tg, T10, TII, AND TI2. TIP MEETS T12, Ti3, TI4, AND T15. T1 IS A PLANE WITH ROTX = +20 AND DZ = +2.65, 
OUTSIDE PX. T2 IS A PLANE WITH ROTX = -20 AND DZ = +1.93, INSIDE PX. T3 IS A PLANE WITH ROTX = +20 AND 
DZ = +1.21, OUTSIDE PX. T4 IS A PLANE WITH ROTX = -20 AND DZ = +0.49, INSIDE PX. T5 IS A PLANE WITH ROTX 
= +20 AND DZ = -0.23, OUTSIDE PX. T6 IS A PLANE WITH ROTX = -20 AND DZ = -0.95, INSIDE PX. T7 IS A PLANE 
WITH ROTX = +20 AND DZ = -1.67, OUTSIDE PX. T8 IS A PLANE WITH ROTX = -20 AND DZ = -2.39, INSIDE PX. 
T9 IS A PLANE WITH ROTX = +20 AND DZ = -3.11, OUTSIDE PX. T10 IS A PLANE WITH ROTX = -20 AND DZ = -3.83, 
INSIDE PX. TI1 IS A PLANE WITH ROTX = +20 AND DZ = -4.55, OUTSIDE PX. T12 IS A PLANE WITH ROTX = -20 
AND DZ = -5.27, INSIDE PX. TI3 IS A PLANE WITH ROTX = +20 AND DZ =-5.895. OUTSIDE PX. TI4 IS A PLANE 
WITH ROTX = -20 AND DZ = -6.38, INSIDE PX. T15 IS A PLANE WITH ROTX = +20 AND DZ = -6.72, OUTSIDE PX. 
 Non-Planar Surface The purpose of this test object is to demonstrate how non-planar curves are handled. 
This example has one non- planar quadric-surface intersection curve. CYLBODY IS A STRUCTURE WITH RMAX 
= 4, INSIDE LEFT, RIGHT, AND MBODY, OR SBODY, TOP, AND FLAT. MBODY IS A CYLINDER WITH ROTX = 90 AND SCALE 
= 1.5, MEETING LEFT, RIGHT, AND SBODY. SBODY IS A CYLINDER WITH DY = 2, MEETING TOP. TOP IS A PLANE WITH 
DZ = 2.5. LEFT IS A PLANE WITH ROTX = 90 AND DY = -3. RIGHT IS A PLANE WITH ROTX = -90 AND DY = 3. FLAT 
IS AN AUXILIARY INVERTED PLANE. Chapter to. CONCLUSIONS   to. 1 General The QUADRIL language is entirely 
original, and deliberately so. I could have implemented a dialect of EUKLID or of some other extant language, 
but chose to be original instead because: l) I wanted a language such that the body specifications would 
naturally serve as their own documentation. (Of course, the user is still free to add comments.) Such 
a language can have very verbose input texts, but I also allow for more compact forms to be used. Such 
a language should be based on a natural language, not some arcane mathematical notation. 2) I wanted 
to completely avoid positional notation, whereby the user may specify a transformation in all three directions 
in one call, with the x-transformation positioned first, then y's, and lastly z's. This may lead to confusion, 
especially if the transformations are not commutative. Instead, every transformation is noted as to what 
it is, and transformations are applied in strict left-to-right order. 3) I wanted to experiment. I fully 
realize that QUADRIL might be inferior for most purposes than, say, a dialect of EUKLID. However, it 
might also be much better, and the only way to find out for sure is to try it. to.2 Specific Contributions 
The QUADRIL language is entirely original. It is designed to be easy to communicate with, both in terms 
of the user conveying his geometric design to the processor, and one user conveying the design to another 
user through QUADRIL. Both natural-language-like and algebra-like syntax are used to facilitate this. 
The algebraic syntax is used where it is best --for algebraic expressions and "volumetric" combinations 
of surfaces and objects; while the natural-language-like syntax is used for other purposes, so it can 
act as self-documentation. tO. 3 Direction of Future Research There are several ways the QUADRIL language 
may be expanded. One way would be to allow the user to express a quadric surface as the surface defined 
by ten or more points in three-space. Using this, ten sample points may be used to define a quadric surface 
in a model of a real physical body. Another possible expansion of syntax would be to allow the DISPLAY 
command to contain specification of projections using terms that draftsmen are familiar with, as outlined 
in (1978a). REFERENCES (1963) Roberts, L.G. "Machine Perception of Three-Dimensional Solids" Technical 
Report No. 315, Lincoln Lab., Massachusetts Institute of Technology, Cambridge, Massachusetts. (1966) 
Weiss, R. "BE VISION, a Package of IBM 7090 Programs to Draw Orthographic Views of Combinations of Planes 
and Quadric Surfaces" 3our. ACM 13:2 (April 1966)pp. 194-204 (1967) Coons, S.A. "Surfaces for Computer-Aided 
Design of Space Forms" MAC-TR-41, N.T.I.S., Springfield, Virginia, 1967 (1968) Comba, P.G. "A Procedure 
for Detecting Intersections of Three-Dimensional Objects" 3our. ACM 15:3 (3uly 1968) pp. 354-366 (1971a) 
Woon, P.Y. "A Computer Procedure for Generating Visible-Line Drawings of Solids Bounded by Quadric Surfaces" 
Technical Report 403-15, Department of Electrical Engineering, School of Engineering and Science, New 
York University, November 1970 (AD-724 744) (1971b) Woon, P.Y., and Freeman, H. "A Computer Procedure 
for Generating Visible-Line Projections of Solids Bounded by Quadric Surfaces" Information Processing 
71, Vol. 2, North-Holland Publishing Co., Amsterdam, 1971, pp. 1120-1125 (1972) Mahl, R. "Visible Surface 
Algorithm for Quadric Patches" IEEE Trans/Comp C-21 (1) (3anuary 1972) pp. 1-4 (197toa) Catmull, E. "A 
Subdivision Algorithm for Computer Display of Curved Surfaces" UTEC-CSc-74- 133, University of Utah, 
Salt Lake City Utah, December 1974 (197tub) Engeli, M., and Hrdliczka, B. EUKLID, eine Einfuerung" Fides 
Treuhand-Ver einigung, Zurich, Switzerland, 1974 (1975a) Braid, I.C. "The Synthesis of Solids Bounded 
by Many Faces" Comm. ACM 18:4 (April 1975) pp. 209-216 (1975b) Bui-Tuong Phong "Illumination for Computer 
Generated pictures" Comm. ACM Ig:6 (3une 1975) pp. 311-317 (1976a) Levin, :I.Z. "A Parametric Algorithm 
for Drawing Pictures of Solid Objects Bounded by Quadric Surfaces" Technical Report CRL-46 Rensselaer 
Polytechnic Institute, Troy, N.Y. March 1976 (AD-A032 921) (1976b) Levin, 9. "A Parametric Algorithm 
for Drawing Pictures of Solid Objects Composed of Quadric Surfaces" Comm. ACM 19:10 (October 1976) pp. 
555-563 (1978a) Carlbom, I., and Paciorek, 9. "Planar Geometric Projections and Viewing Transformations" 
ACM Computing Surveys 10 (4) (December 1978) pp. 465- 502. (1978b) Levin, 3.Z. "Mathematical Models 
for Determining the Intersections of Quadric Surfaces" Technical Report CRL-61, Rensselaer Polytechnic 
Institute, Troy, New York, 1978 (1979) Levin, J.Z. "Mathematical Models for Determining the Intersections 
of Quadric Surfaces" 3ournal of Computer Graphics and Image Processing, Vol 11 (1979), pp. 73-87. (1980) 
Levin, J.Z. "QUADRIL: A Computer Processor for the Design and Display or Quadric-Surface-Bodies", Technical 
Report IPL TR-80-003, Rensselaer Polytechnic Institute, Troy, New York, April 1980  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1980</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>807475</article_id>
		<sort_key>93</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-1980</article_publication_date>
		<seq_no>16</seq_no>
		<title><![CDATA[PICTUREBALM]]></title>
		<subtitle><![CDATA[A LISP-based graphics language system with flexible syntax and hierarchical data structure]]></subtitle>
		<page_from>93</page_from>
		<page_to>99</page_to>
		<doi_number>10.1145/800250.807475</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=807475</url>
		<abstract>
			<par><![CDATA[<p>PICTUREBALM is a portable, interactive, LISP-based language system for graphics applications programming. PICTUREBALM's design and initial experimental implementation is described from the point of view of both the user and the language system implementor. The approach of extending a LISP-based language by adding graphical operations was chosen because many of the recognized requirements for graphics programming languages are standard features of LISP-like systems. Future work is proposed.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[BALM]]></kw>
			<kw><![CDATA[Data structures]]></kw>
			<kw><![CDATA[Geometric modeling]]></kw>
			<kw><![CDATA[Graphics languages]]></kw>
			<kw><![CDATA[LISP]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.6</cat_node>
				<descriptor>Languages</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.6</cat_node>
				<descriptor>Graphics data structures and data types</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.2.5</cat_node>
				<descriptor>LISP</descriptor>
				<type>P</type>
			</other_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010394</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Graphics file formats</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10011007.10011006.10011050</concept_id>
				<concept_desc>CCS->Software and its engineering->Software notations and tools->Context specific languages</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061.10010063</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures->Computational geometry</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003128</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction techniques</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Languages</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P94016</person_id>
				<author_profile_id><![CDATA[81100558697]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Gary]]></first_name>
				<middle_name><![CDATA[B.]]></middle_name>
				<last_name><![CDATA[Goates]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Computer Science Department, University of Utah, Salt Lake City, Utah]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP31080057</person_id>
				<author_profile_id><![CDATA[81100145121]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Martin]]></first_name>
				<middle_name><![CDATA[L.]]></middle_name>
				<last_name><![CDATA[Griss]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Computer Science Department, University of Utah, Salt Lake City, Utah]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P94067</person_id>
				<author_profile_id><![CDATA[81100342033]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Gary]]></first_name>
				<middle_name><![CDATA[J.]]></middle_name>
				<last_name><![CDATA[Herron]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Boeing Computer, Services Co., Mail Stop 73-03, P. O. Box 24346, Seattle, Washington]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Baer, A.; Eastman, C.; and Henrion, M. "Geometric Modelling : A Survey." Computer-Aided Design 11, 5 (September 1979), 253-272.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>360354</ref_obj_id>
				<ref_obj_pid>360349</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Clark, J. H. "Hierarchical Geometric Models for Visible Surface Algorithms." Comm. ACM 19, 10 (October 1976), 547-554.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Goates, G. B.; Griss, M. L.; and Herron, G. J. PICTUREBALM: A LISP-Based Graphics Language System with Flexible Syntax and Hierarchical Data Structure. Technical Report UUCS-80-118, University of Utah Computer Science Department, February, 1980.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Griss, M. L. BIL: A Portable Implementation Language for LISP-Like Systems. Utah Symbolic Computation Group Operating Note 36, University of Utah, 1977.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>698991</ref_obj_id>
				<ref_obj_pid>646670</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Griss, M. L.; Kessler, R. R.; and Maguire, G. Q. Jr. TLISP - A Portable LISP Implemented in P-code. Proceedings of EUROSAM 79, ACM, June, 1979, pp. 490-502.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807038</ref_obj_id>
				<ref_obj_pid>800232</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Harrison, M. C. A Language Oriented Instruction Set for BALM. Proceedings of SIGPLAN/SIGMICRO 9, ACM, 1974, pp. 161.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_obj_id>953999</ref_obj_id>
				<ref_obj_pid>953997</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Marti, J. B., et al. "Standard LISP Report." SIGPLAN Notices 14, 10 (October 1979), 48-68.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 J PICTUREBALM: A LISP-BASED GRAPHICS LANGUAGE SYSTEM WITH FLEXIBLE SYNTAX AND HIERARCHICAL DATA STRUCTURE 
 Gary B. Goates and Martin L. Griss Computer Science Department and Gary J. Herron Mathematics Department 
 University of Utah Salt Lake City, Abstract  PICTUREBALM is a portable, interactive, LISP-based language 
system for graphics applications programing. PICTUREBALM's design and initial experimental implementation 
is described from the point of view of both the user and the language system implementor. The approach 
of extending a LISP-based language by adding graphical operations was chosen because many of the recognized 
requirements for graphics programing languages are standard features of LISP-like systems. Future work 
is proposed. Key Words: BALM, data structures, geometric modeling, graphics languages, LISP CR Categories: 
4.13, 4.22, 4.34, 8.2 I. Introduction BALM [6] is a portable, easily extensible, interactive, LISP-based 
language system. BALM's data structures, dynamic storage management, and resident incremental compiler 
are essentially those of LISP [7], but BALM is ALGOL-like in its syntax. The close match between these 
features and the requirements for good graphics programing environments motivated extending BALM into 
a graphics language. The Utah version of BALM is implemented in terms of a pseudo-code, or "P-code", 
 abstract stack machine, called MTLISP [5]. MTLISP is written in the highly portable BIL language [4] 
 which is translated into FORTRAN or target machine assembly language. PICTUREBALM was developed by 
 adding about a dozen low-level graphics functions as new MTLISP P-code operations, and then making 
 them easily accessible by means of the extension mechanisms built into the BALM system. In particular, 
PICTUREBALM's parsing was extended by adding new operators for graphics. Utah 84112 The hierarchical 
structure and implementation of the PICTUREBALM system are designed to support both the beginning and 
the expert user well. The sophisticated PICTUREBALM user can utilize low level primitive operations to 
support customized modeling, syntax or device environments; yet the beginner need not know how to use 
these features. PICTUREBALM usage typically consists of creating, modifying, and requesting the display 
of graphical objects, called "Models". The usage adopted in this paper is that the names of non-terminal 
symbols of the grammar of Models are capitalized, so as to emphasize that the term is being used in 
a technical sense. A Model is a three dimensional representation of the spatial, topological and graphical 
features of an object, similar to the "image model" described by Baer, Eastman and Henrion [I]. Models 
can contain any number of primitives, which can generally be in any order. PICTUREBALM Model primitives 
include: Point Sets, which might be interpreted as polygons, connected line segments, curve control points, 
etc.; transformations; color or appearance attributes; named references to other Models to be displayed 
as if they were part of the current Model; and procedure calls. Model primitives need not be the same 
as the hardware primitives available. Allowing Models to contain references to other Models facilitates 
dynamic displays and allows the user to structure his data in Clusters in a meaningful manner. Sub-Models 
may be shared among a number of Models. Allowing procedure calls to be imbedded within Models provides 
the user with a mechanism which can easily effect arbitrary displays, transformations, parameterized 
models or other functions that may be required by a specific application; in some cases, it is essential 
to represent objects by algorithms or procedural Permission to copy without fee all or part of this 
material is granted provided that the copies are not made or distributed for direct commercial advantage, 
the A(~ copyright notice and the title of the publication and its date appear, and notice is given that 
copying is by permission of the Association for Computing Machinery. To copy otherwise, or to republish, 
requires a fee and/or specific permission. ~980 ACM 0-89791-021-4/80/0700-0093 $00.75 This work supported 
in part by the National Science Foundation under Grant Numbers MCS-78-01966, MCS-78-04853 and MCS-76-15035. 
 Author' s current address : Boeing Computer Services Co., Mail Stop 73-03, P. O. Box 24346, Seattle, 
Washington 98124. model s. BALM's rich, hierarchical and easy-to-manipulate data structure will be a 
treat to work with for anyone used to writing FORTRAN routines to manipulate "lists" of graphical data. 
 The PICTUREBALM system includes operators and syntactic extensions which are designed to make PICTUREBALM 
commands look "natural" to the user. Points are constructed by using curly brackets, e.g. {x, y}. The 
" " operator is used to make Point Sets; e.g. it ~an be used to make polygons out of Points. The "@" 
operator is used to locate text displays at some Point on the screen. Transforms change the view or the 
position of objects which they are linked to by either of the "<" or ">" operators. For example, the 
following PICTUREBALM command will assign to the identifier "Rotated.triangle" the Model which represents 
a triangle rotated 45 degrees about the z axis: Rotated.triangle = {0,0} {1,1} {2,0} {0,0} > ZROT (45~; 
-- Note that the ">" and "<" operators have lower precedence than " " Display is accomplished by passing 
Models to the PICTUREBALM interpreter for the desired output device. Each PICTUREBALM interpreter knows 
of the existence of all legal Model primitives and each does something appropriate with any primitives 
which are not directly relevant to their device, e.g. an interpreter for a storage tube device might 
ignore a color attribute. Also, some Model interpreters must place constraints on what Models they consider 
to be validly formed. The intent of this project was not to produce a production quality graphi@s system, 
but rather to explore the features that LISP-like languages offer for graphics. While neither the design 
nor the implementation of PICTUREBALM is completed, the success of the current experimental implementation 
illustrates the benefits of LISP-like languages in many graphics programing activities. 2. Selecting 
BALM as the Base Language to Extend The basic unit of data in BALM is called an "item". An item can 
be a pair, a list, a vector, a string, an integer, a procedure or an identifier. The elements of a pair, 
a list or a vector can be any item of any type. Each item contains a run-time tag specifying its type, 
in the sense of its being one of the above types; variables may change types dynamically. Unlike PASCAL, 
there are no user-definable data types nor is there compile-time type checking. All identifiers have 
both a value item and a "property list" item, which is typically a list of attribute-value pairs. Each 
top level BALM command is terminated by a semicolon. Typically such commands are assignment statements 
or procedure invocations. As soon as a semicolon is encountered on an input line, the command is interpreted. 
If the right hand side of an assignment statement starts with the "PROC" operator, then the procedure 
which follows is compiled to MTLISP machine P-codes by the run-time resident compiler. If the procedure 
contains no syntax errors, then the value of the identifier on the left side of the assignment becomes 
the compiled procedure. After each command is interpreted, the language system fetches the next command 
from the active input stream, which can easily be switched between the user's terminal and files. All 
BALM language statements "return" a value. The value item returned by a procedure can be specified by 
an explicit RETURN statement. BALM procedures can accept a variable number of arguments; they can determine 
at run time the namber and the type of their arguments. BALM uses BEGIN-END blocks for determining the 
scope of identifiers, and it has a number of high-level control structures, such as WHILE and FOR. Most 
graphics applications would benefit from the following PICTUREBALM features, which are attributes shared 
by many LISP-like systems: - Interactive program definition and execution. - Efficient and natural 
manipulation of complex, hierarchical, data structures. - Run-time type checking. - Dynamic storage 
management. - Run-time symbol table. - Availability of language features at run time. BALM has several 
features which are important for graphics that are not shared by all LISP-based languages, viz: -ALGOL 
or PASCAL-like syntax and control structures. - Extensible syntax. - An abstract machine pseudo-code 
implementation. -A highly portable implementation. Thus BALM, of the available LISP-based languages, 
 is particularly well suited for extension to graphics applications. 3. FICTUREBALM Graphical Objects 
and Their Interpretation PICTUREBALM Models are created by combining primitive objects, modifiers, and 
references to other Models. Models are then displayed by means of the prefix operator SHOW, which passes 
the Model to whichever PICTUREBALM interpreter is currently selected. One goal of the PICTUREBALM design 
is to keep Models as device independent as possible. However, some Model interpreters must place constraints 
on what Models they consider to be validly formed. For example, the current implementation of a Model 
interpreter for line drawing devices will accept any arbitrary Point Set which it interprets as a connected 
set of line segments, but a surface rendering interpreter would have to use only Point Sets which describe 
surfaces of specified types. PICTUREBALM does not use the approach of constraining all Models so as to 
be interpretable on every device because this restricts Models to only that small set of primitives common 
to all devices. PICTUREBALM supports the creation and manipulation of Models both by means of built-in 
procedures for the various primitives and by means  of syntactic extensions, i.e. operators which construct 
Models out of primitives. PICTUREBALM contains seven operators designed to make graphics programs easy 
to read and write. They are denoted by the following special characters: {, }, _, @0 &#38;, < and >. 
 interpreted as curve or surface control Points or vectors. - Text Specifications. - Transforms, which 
represent transformations of Model Objects in three dimensional space, clipping windows or perspective 
transformations. Points are constructed by using curly brackets. One to four numbers can be used to 
specify Points. For example, {a} denotes the Point a on the x axis, {x, y} denotes the point (x, y, 
O) in three dimensional space, and {x, y, z} denotes the point (x, y, z). The use of curly brackets 
to denote Points was prefered over the standard parentheses notation because of the ambiguity of the 
latter with function calls. The " " operator constructs Point Sets out of Points. --It can be used, 
for example, to form a Point Set representing a line out of two Points, or one representing a set of 
connected line segments out of more than two Points. The sequence "A B C D", where A, B, C and D are 
identifiers of Points, creates a Model which when SHOW'n moves the display beam to the point represented 
by A, draws to B, to C, and then to D. The "@" operator is used to locate a text display at some location 
on the screen, e.g. "Hi There!" @ {10} denotes a Model which displays a string that starts at 10 on the 
x axis. Models may have modifiers and Transforms concatenated onto them to form new Models using either 
the "<" operator or the ">" operator. They are equivalent semantically but they differ in the order 
in which their arguments are given. The ">" view expresses the transformations which may be successively 
applied to an object relative to the global coordinate system. The "<" view expresses transformations 
relative to the object's own local coordinate system; it can be thought of as moving the object's coordinate 
system relative to a static observer. In both views, the transformations are thought of as occurring 
in order from left to right and the observer remains stationary with respect to a global coordinate 
system. The "&#38;" operator is used to group Models into larger Models. For example, the command sequence 
 Two.things = ABC > XROT(45) &#38; XYZ > YMOVE(6); SHOW Two.things; will draw Model ABC rotated by 
45 degrees around the X axis and will also draw Model XYZ moved by 6 units in the Y direction. Models 
are simply BALM items; they can be assigned as the value of any identifier and they can serve as operands 
in commands. Also modifiers such as Transforms can be applied to objects either at the time that a 
Model is built, or at the time it is SHOW'n. Models can contain any n~nber of the following types of 
primitives: - Point Sets, i.e. an ordered set of Points.  - Point Set Modifiers, which could be used 
to specify that Point Sets are to be  -Appearance Attributes, such as color or intensity. -Repeat 
Specifications, which instantiate sub-Models under a specified Transform which is repeatedly applied. 
 -Clusters, i.e. named references to other Models. - Calls to arbitrary PICTUREBALM procedures. Model 
primitives can generally be combined in any order to form a Model. A more detailed description of PICTUREBALM 
as well as some comparisons with previous work is presented in [3]. One notable feature of PICTUREBALM 
is that most graphics functions are represented as primitives to be included in Models; unlike graphics 
subroutine packages, few functions are implemented as procedures which are intended to be called directly 
by the user. Clustering, i.e. allowing Models to contain named references to other Models, is of great 
utility in facilitating dynamic displays. It also enables the user to structure data so as to be meaningful 
to the programer or the program user. Clusters can be used by the program to simplify processing, as 
Clark has suggested [2]. Clustering is useful in both its "quoted" and "unquoted" forms. The unquoted 
form of clustering facilitates forming large static Models from simpler and easier to think about sub-Models. 
Quoting consists of simply preceding the cluster's identifier by the LISP quote operator, "'". Quoting 
the reference to a Cluster has the effect of making the display of the Model dependent on the value of 
the sub-Model at the time that the Model is SHOW'n, rather than on its value at the time when the Model 
is formed. This is analogous to a "call by name" rather than a "call by current value" type of reference. 
Quoted~ clustering is very useful for dynamic views of static Models as well as for displaying dynamic 
objects; dynamic attributes of Models can be isolated in a quoted Cluster. When a Model interpreter 
encounters a procedure imbedded within a Model, the procedure is called. The current cLm~ulative transformation 
matrix and the current beam position are available to such procedures in global variables. This provides 
the PICTUREBALM user with a mechanism for effecting arbitrary displays, transformations, functions or 
parameterized geometric Models which may be useful (or even essential) for specific applications. The 
following are some specific examples of how this capability might be used: -Supporting user-defined 
Model primitives. -Displaying complex scenes efficiently by means of algorithmic modeling. -Performing 
non-linear transformations on objects. Procedures which return a legal Model as their value can be used 
either by calling them at Model-building time or by imbedding them in a Model to be called at SHOW time. 
If there are any primitives in the Model below an imbedded Procedure, then the procedure is passed a 
single argument which is the sub-Model below it. An interesting case occurs whenever a procedure both 
returns a Model as its value and accepts any Model as its only argument. Such a procedure becomes a new 
Model primitive in the sense that it can be placed anywhere within a Model. The ability to represent 
graphical data in arbitrarily complex hierarchical structures is important in geometric modeling [I]. 
Operating on such complex, hierarchical data structures is greatly facilitated by the following attributes 
of the PICTUREBALM language: - A Model is merely a BALM data item and thus it can be assigned or reassigned 
as the value of any identifier. - Models can be clustered, i.e. they can contain named references to 
sub-Models. - BALM possesses a rich data structure and  flexible run-time type checking. While it is 
left to the applications programer to design the desired data hierarchies and Models, PICTUREBALM provides 
very powerful tools to manipulate such structures conveniently. 4. Examples of PICTUREBALM Commands 
 In the following examples, anything following a % on the same line is a comment. Commands are terminated 
with a semicolon. % % PICTUREBALM Commands to SHOW Lots of Cubes % % % Outline is a Point Set defining 
the 20 by 20 % square which will be part of the Cube.face % Outline = { 10, 10} {-10, 10} {-10,-10} 
{ 10,-10} {10, 10}; % % Cube.face will also have an Arrow on it % Arrow = {0,-I} {0,2} &#38; {-1,1} 
{0,2} {I,1}; % % We are ready for the Cube.face % Cube.face = (Outline &#38; Arrow) > 'Tran.z; % 
 % Note the use of static clustering to keep objects % meaningful as well as the quoted Cluster % to 
the as yet undefined transformation Tran.z, which will result in its evaluation being % deferred until 
SHOW time % % and now define the Cube % Cube = Cube.face &#38; Cube.face > XROT (180) % 180 degrees 
&#38; Cube.face > YROT (90) &#38; Cube.face > YROT (-90) &#38; Cube.face > XROT (90) &#38; Cube.face 
> XROT (-90); % % Set up initial Z Transform for each cube face % Tran.z = ZMOVE (10); % 10 units 
out % % % Set up the desired perspective with a clipping % window centered at the origin and of half 
size % 60, and the observer at -200 on the z axis % GLOBAL.TRANSFORM = WINDOW (60, -200); % % Now 
draw cube % SHOW Cube; \ \ I i I $ I  'I I I I J / % % Draw it again rotated and moved 
left % SHOW Cube > XROT(20) > YROT(30) > ZROT(10); % % Dynamically expand the faces out % Tran.z 
= ZMOVE (12); % SHOW Cube ; % Now show 5 cubes, each moved further right by 35 % SHOW Cube > XMOVE(-70) 
> REPEATED (5, XMOVE(35)); % % Define a procedure which returns a Model of % a cube when passed the 
face to be used % Build.cube = PROC (Facet, Face &#38; Face > XROT(180) &#38; Face > YROT(90) &#38; 
Face > YROT(-90) &#38; Face > XROT(90) &#38; Face > XROT(-90) END; % just return the value of the one 
statement % % Use this procedure to display 2 cubes, with and % without the Arrow -first do it by calling 
% Build.cube at time the Model is built % P = Build.cube (Cube.face) > XMOVE(-15) &#38; Build.cube (Outline 
> 'Tran.z) > XMOVE(15); SHOW P; f  !B f I f  % Second do it by inserting the procedure into the 
% Model to be called at SHOW time Tran. z = XMOVE(10); % Reset to closed cube Q = Cube.face > Build.cube 
> XMOVE(-15) &#38; Outline > 'Tran.z > Build.cube > XMOVE(15); SHOW Q; % Now define a procedure which 
returns a Model of % a cube when passed the half size parameter % Cube.model : PROC (H.Size), { H.Size 
H.Size H.Size} {-H. Size H. Size H. Size} {-H. Size -H. Size H. Size} { H.Size -H.Size H.Size} { H. 
Size H. Size H. Size} { H.Si ze H. Size -H. Size} {-H. Size, H. Size, -H. Size} {-H. Size, -H. Size, 
-H. Size} { H.Size, -H.Size, -H.Size} { H.Size, H.Size, -H.Size} {-H.Size, H.Size, -H.Size} {-H.Size, 
H.Size, H.Size} {-H. Size, -H. Size, -H. Size } {-H. Size, -H. Size, H. Size} { H.Size, -H.Size, -H.Size} 
{ H.Size0 -H.Size, H.Size}  END ; % Imbed the parameteri zed cube in some Models % His.cube = Cube.model 
 < 'His.size; Her.cube = Cube.model < 'Her.size; R = His.cube > XMOVE (20) &#38; Her.cube > XMOVE (-20) 
; % % Set up some sizes and SHOW them % His.size = 7; Her.size = 12~ SHOW R; N  % % Set up some different 
sizes and SHOW them again % His.size = 18; Her.size = 13; SHOW R; 11 % Note that the distinction between 
the two uses of % Build.cube is procedure call syntax versus Model % building syntax % Define a procedure 
that returns a point linearly % interpolated between a sine and a square wave Waveform.Pt = PROC (Interpol, 
Omega, Time), BEGIN (two.y, temp) , temp = SIN (Omega * Time), two.y = Interpol * temp + (I -Interpol) 
* SIGN(temp), RETURN ( {Time, two.y / 2} ) END END; % % Define another procedure which constructs a 
Point % Set describing two full cycles of the % parameteri zed waveform % Waves = PROC (Interpel), 
 BEGIN (two .cycles), two .cycles = {0}, % use 720 line segments and Omega = I FOR I : (-360, 360) REPEAT 
 two.cycles = two.cycles Waveform.Pt (Interpel, I, I), RETURN (two .c y~les) END END; % % Use these 
procedures to draw three waveforms - % a sine wave, a square wave and one half way % between the two 
 % SHOW Waves(0) &#38; Waves(I/2) &#38; Waves(1); % % Define a procedure that makes a wheel with 8 
% spokes rotated around the z axis % Wheel = PROC (Spoke), Spoke > REPEATED (8, ZROT(45)) END; % % Draw 
a wheel consisting of 8 cubes % Cube.On.Spoke = Outline > ZMOVE(IO) > Build.cube; Eight.Cubes = Cube.On.Spoke 
> XMOVE(25) > Wheel; SHOW Eight.Cubes;  % Draw a cube where each face consists of just % a Wheel of 
8 Outlines % Flat.Spoke = Outline > XMOVE(25); A.Fancy.Cube = Flat.Spoke > Wheel > ZMOVE(50) > > Build.cube; 
SHOW A.Fancy.Cube; % % Redraw the fancy cube, after changing perspective % by moving the observer farther 
out along z axis % GLOBAL.TRANSFORM = WINDOW (60, -500); % SHOW A.Fancy.Cube; 1 t % Note the flexibility 
resulting from the fact that % both Build.Cube and Wheel simply take or return % any Model as their arg~nent 
or value % The reader may wish to consider what would be required to accomplish these same displays, 
modeling and syntax using a graphics subroutine package called from a language such as FORTRAN. Q O 
O O Q Q  Re ferences  5. Directions for Future Research The intent of this project was not to produce 
a production quality graphics system, but rather to empirically explore the advantages that LISP-like 
languages offer for graphics applications. Nevertheless, the success of the current limited and experimental 
PICTUREBALM implementation supports the hypothesis that LISP-like languages have a significant contribution 
to make to many graphics programing activities. This result raises many potentially productive areas 
for further work, such as: - Developing a "production quality" PICTUREBALM implementation. -Writing 
applications programs in PICTUREBALM. - Extending PICTUREBALM into a layout modeling language for .integrated 
circuit design. - Integrating graphics, pattern matching and computer algebra in a environment for computer-aided 
geometric design. - Optimizing PICTUREBALM's implementation. Acknowledgments We would like to thank 
Brian Barsky, Paul Drongowski, Russ Fish and Rich Riesenfeld who offered this project their time, support, 
 suggestions and clarifying insights. Also Robert Kessler, Gerald Q. Maguire, Jr. and Jed Marti commented 
on drafts of this paper. I. Baer, A.; Eastman, C.; and Henrion, M. "Geometric Modelling : A Sur~,ey." 
Computer-Aided Design 11, 5 (September 1979), 253-272. 2. Clark, J. H. "Hierarchical Geometric Models 
for Visible Surface Algorithms." Comm. ACM 19, 10 (October 1976), 547-554.  3. Goates, G. B.; Griss, 
M. L.; and Herron, G. J. PICTUREBALM: A LISP-Based Graphics Language System with Flexible Syntax and 
Hierarchical Data Structure. Technical Report UUCS-80-118, University of Utah Computer Science Department, 
  February, 1980. 4. Griss, M. L. BIL: A Portable Implementation Language for LISP-Like Systems. Utah 
Symbolic Computation Group Operating Note 36, University of Utah, 1977. 5. Griss, M. L.; Kessler, R. 
R.; and Maguire, G. Q. Jr. TLISP -A Portable LISP Implemented in P-code. Proceedings of EUROSAM 79, 
ACM, June, 1979, PP. 490-502. 6. Harrison, M. C. A Language Oriented Instruction Set for BALM. Proceedings 
of SIGPLAN/SIGMICRO 9, ACM, 1974, pp. 161.  7. Marti, J. B., et al. "Standard LISP Report." SIGPLAN 
Notices 14, 10 (October 1979), 48-68.  ~e8 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1980</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>807476</article_id>
		<sort_key>100</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-1980</article_publication_date>
		<seq_no>17</seq_no>
		<title><![CDATA[A structure from manipulation for text-graphic objects]]></title>
		<page_from>100</page_from>
		<page_to>107</page_to>
		<doi_number>10.1145/800250.807476</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=807476</url>
		<abstract>
			<par><![CDATA[<p>The general purpose graphics systems of the future will need a simple logic for visual objects&#8212;one structure underlying both text and graphics. As an experiment, perhaps the immediate handling of visual objects by the user can provide the starting point for developing that structure. This paper describes the PAM graphics system, in which <italic>the structure of text-graphic objects arises directly out of manual manipulation.</italic> The needs of manual manipulation determine the <italic>text-graphic pattern</italic> as the simplest organizing structure for images; PAM stands for <italic>P</italic>Attern <italic>M</italic>anipulating. The PAM system is designed for the agile manipulation of text-graphic patterns&#8212;first manually, and then, later, programmatically.</p> <p>Starting from this strict 'front-in' viewpoint&#8212;where immediate manipulation (hand powered animation) was to be the primary application&#8212;a 'manipulative grammar' was evolved to give the user a simple yet powerful handle on text-graphic images. This grammar turned out to be a generalization of LISP syntax from textual symbolic expressions to text-graphic forms, structuring such forms as trees and then offering:</p> <p>@@@@spatial GRABBING of objects into attention</p> <p>@@@@tree guided attention shifters like FIRST, REST, NEXT, and UP</p> <p>spatial &amp; tree manipulations on any object in attention.</p> <p>@@@@The resulting structures also offer surprising computational power (in a manner directly analogous to the way the basic list structures and functions of LISP give rise to the flexibility and power of a full blown LISP system, McCarthy and Talcott [1]), leading finally to <italic>computing with text-graphic forms.</italic> Consequently, a <italic>semantic</italic> function is added to supplement the basic manipulative grammar:</p> <p>@@@@evaluation of object in attention, result displayed at the cursor. Evaluation supports facilities like naming (and thus saving) visual objects, programming, and creation of menus (patterns of evaluatable function objects).</p> <p>An experimental version of the PAM system has been implemented in MACLISP at the Stanford Artificial Intelligence Lab.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Computing with text-graphic forms]]></kw>
			<kw><![CDATA[Front-in design]]></kw>
			<kw><![CDATA[Graphics command language]]></kw>
			<kw><![CDATA[Graphics programming language]]></kw>
			<kw><![CDATA[Hand powered animation]]></kw>
			<kw><![CDATA[Interactive computer graphics]]></kw>
			<kw><![CDATA[LISP]]></kw>
			<kw><![CDATA[Man-machine interface]]></kw>
			<kw><![CDATA[Manipulative grammar]]></kw>
			<kw><![CDATA[Phenomenology]]></kw>
			<kw><![CDATA[Text-graphic objects]]></kw>
			<kw><![CDATA[Visual linguistics]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.2</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.6</cat_node>
				<descriptor>Interaction techniques</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.5.2</cat_node>
				<descriptor>Pattern analysis</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10010391</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Graphics input devices</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061.10010063</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures->Computational geometry</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010257</concept_id>
				<concept_desc>CCS->Computing methodologies->Machine learning</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010520.10010521.10010542.10011714</concept_id>
				<concept_desc>CCS->Computer systems organization->Architectures->Other architectures->Special purpose systems</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Performance</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P330295</person_id>
				<author_profile_id><![CDATA[81100287526]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Fred]]></first_name>
				<middle_name><![CDATA[H.]]></middle_name>
				<last_name><![CDATA[Lakin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Xerox Palo Alto Research Center, 3333 Coyote Hill Road, Palo Alto, CA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[McCarthy, John and Talcott, Carolyn, LISP Programming and Proving, Class notes CS 206, Computer Science Dept., Stanford University, 1979.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Sutherland, Ivan E., "Computer Graphics: Ten Unsolved Problems", Datamation, pages 22-27, May 1966.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807365</ref_obj_id>
				<ref_obj_pid>800248</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Futrelle, R.P. and Barta, G., "Towards the Design of an Intrinsically Graphical Language", SIGGRAPH '78 Proceedings, pages 28-32, Aug 1978.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Engelbart, D.C., "Augmenting Human Intellect: A Conceptual Framework," SRI International, Menlo Park, California, Oct 1962.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[English, W.K., Engelbart, D.C., and Berman, M.L., "Display-Selection Techniques for Text Manipulation", IEEE Trans. on Human Factors in Electronics, Vol. HFE-8, No. 1, March 1967.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Sacerdoti, Earl, "Planning in a Hierarchy of Abstraction Spaces", Adv. Papers 3rd Intl Conf. on Artificial Intelligence, Stanford University, August 1973.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Teitelman, Warren, InterLISP Reference Manual, Xerox Palo Alto Research Center, 1978.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Sutherland Ivan E., "Sketchpad: A Man-Machine Graphical Communication System", Proceedings&#8212;Spring Joint Computer Conference, pages 329-346, 1963.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Martin, Paul A., DIP: A Program to Understand Diplomacy Dialogs, PhD thesis, Stanford University, forthcoming.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Defanti, Tom, "The Digital Component of the Circle Graphics Habitat", Proceedings National Computer Conference, 1976.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Sibbet, David, "Introduction to Group Graphics", The Correspondent, CORO Foundation North. Cal. Public Affairs Quarterly, Summer 1976.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Lakin, Fred, Design for a Working Group Display, manuscript 1974.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Ball, Geoffrey H. and Gilkey, James Y., "Facilitation and Explicit Group Memory&#8212;Their Application in Education", SRI International IR&amp;D No. 183531-409, Menlo Park, California, Dec 1971.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Pferd, William, Peralta, L.A. and Predergast, F.X., "Interactive Graphics Teleconferencing", IEEE Computer, Nov. 1979.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_obj_id>540414</ref_obj_id>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Winograd, Terry, Understanding Natural Language, Academic Press, 1972.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Natanson, Maurice, Editor, Essays in Phenomenology, Martinus Nijhoff, The Hague, 1966.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_obj_id>802795</ref_obj_id>
				<ref_obj_pid>800087</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Lakin, Fred, "Computing with Text-Graphic Forms", submitted to the LISP Conference at Stanford University, August 1980.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[Lakin, Fred, "Indigenous Graphics for LISP", submitted to the LISP Conference at Stanford University, August 1980.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 A Structure from Manipulation for Text-Graphic Objects Fred H. Lakin * Stanford Artificial Intelligence 
Laboratory  Abstract The general purpose graphics systems of the future will need a simple logic for 
visual objects -one structure underlying both text and graphics. As an experiment, perhaps the immediate 
handling of visual objects by the user can provide the starting point for developing that structure. 
This paper describes the PAM graphics system, in which the structure of text-graphic objects arises directly 
out of manual manipulation. The needs of manual manipulation determine the text-graphic pattern as the 
simplest organizing structure for images; PAM stands for PAttern Manipulating. The PAM system is designed 
for the agile manipulation of text-graphic patterns -first manually, and then, later, programmatically. 
Starting from this strict 'front-in' viewpoint -where immediate manipulation (hand powered animation) 
was to be the primary application -a 'manipulative grammar' was evolved to give the user a simple yet 
powerful handle on text-graphic images. This grammar turned out to be a generalization of LISP syntax 
from textual symbolic expressions to text-graphic forms, structuring such forms as trees and then offering: 
l, spatial GnABBING of objects into attention tree guided attention shifters like FlaST, nEST, NEXT, 
and UP 1' spatial &#38; tree manipulations on any object in attention. The resulting structures also 
offer surprising computational power (in a manner direcdy analogous to the way the basic list structures 
and functions of LISP give rise to the flexibility and power of a full blown LISP system, McCarthy and 
Takott [1]), leading finally to computing with text-graphic forms. Consequendy, a semantic function is 
added to supplement the basic manipulative grammar: 1, evaluation of object in attention, result displayed 
at the cursor. Evaluation supports facilities like naming (and thus saving) visual objects, programming, 
and creation of menus (patterns of evaluatable function objects). An experimental version of the PAM 
system has been implemented in MACLISP at the Stanford Artificial Intelligence Lab. Key words and phrases: 
text-graphic objects, front-in design, hand powered animation, manipulative grammar, LISP, computing 
with text-graphic forms, phenomenology, interactive computer graphics, man-machipe interface, graphics 
command language, graphics programming language, visual linguistics. CR Categories: 3.36, 3.41, 4.22, 
8.2. * now at Xerox Palo Alto Research Center, 3333 Coyote Hill Road, Palo Alto, CA 94304. Permission 
to copy without fee all or part of this material is granted provided that the copies are not made or 
distributed for direct commercial advantage, the ACM copyright notice and the title of the publication 
and its date appear, and notice is given that copying is by permission of the Association for Computing 
Machinery. To copy otherwise, or to republish,  requires a fee and/or specific permission. &#38;#169;]980 
ACH 0-8979]-02]-4/80/0700-0]00 $00.75  Sections Model of User on the Electric Blackboard (handPAM) 
Selective Manipulation Leads to Structure A Structure from Manipulation for Text-Graphic Objects ']'he 
Separation of Space &#38; Tree Handling Space &#38; Tree Structure The 4 Levels ofhandPAM A Typical Blackboard 
Image Ambiguities Resolved by Manipulative Grammar Relation ofhandPAM to writtenPAM Manipulative Grammar 
is Pre-semantic Applications Phenomenology and Graphic Interface Design Notes on Illustrations Acknowledgements 
References Model of User on the Electric Blackboard (handPAM) text-graphic \ ~ ~.:~°:#~ controls object 
of A Phenomenology of Text-Graphic Manipulation The human uses the hand controls to manipulate his object 
of attention on the text-graphic display. Electric Blackboard Implementation The text-graphic handle 
monitors the movements oft.he keyset, keyboard and mouse, and uses them as commands to manipulate the 
objatn in bigpat. Figure 1 Selective Manipulation Leads to Structure Given selective manipulations (drag, 
rotate, scale, erase) on atomic visual objects, users will naturally and soon want a way of structuring 
for group manipulation. The tree structure of PAM patterns is one of the simplest possible grouping devices 
which clears up some of the ambiguities in text-graphic manipulation (for example, as defined by Suthefland 
[2] and Futrelle [3] in a later section). Before discussing the powers of PAM's manipulative grammar, 
we must first see how it arises. The user's initial experience of the PAM system is handPAM, a manual 
manipulator for text-graphic images which functions as an 'Electric Blackboard'. handPAM is designed 
to support the spontaneous, general purpose writing-and-drawing activity which is often performed on 
a blackboard. The actual behavior of handPAM is defined by its top-level function called hand/e Oust 
as the LISP editor is defined by edit, and LISP itself by evaO. Figure 1 shows a user in the handPAM 
environment. The sketch depicts only the important features according to the front-in or phenomenolgical 
approach, so it's actually a model. In implementation the object of attention becomes the objatn, a global 
variable which is in turrl a member of bigpat, the largest pattern made up of everything on the display. 
The human's purpose in this environment is to make the text and graphics do what he wants, as ideas come 
to him. It is within the organizing effect of this purpose that the Levels of handPAM progressively unfold 
themselves (see figure 5). At Level 1, simple objects (drawlines and textlines) are introduced, along 
with spatial manipulations (DRAG,ROTATE, SCALE and ERASE) which can be applied to any object selected. 
An object is selected by spatially pointing it out, thus GRABBING it into attention (whereupon it becomes 
the objatn, indicated by thickening); only when an object is in attention can it be manipulated. And 
it is the power of selective manipulations -within the driving purpose of his desire to perform blackboard 
type text-graphic activity -which naturally leads the user to want and acquire the pattern structuring 
operations of Levels 2 and 3. Manipulative grammar, then, is a way of dealing with the forms and structures 
of visual objects and their customary arrangement in simple and complex patterns (so far, paraphrasing 
Webster's definition for verbal grammar) which is organized by the in-order.to of spontaneous manipulation. 
A consequence of the design goal of supporting blackboard type activity is 'visual unobtrusiveness', 
meaning that no extra objects appear on the display to distract the viewers from the actual text- graphic 
image under manipulation. Instead of using menus or other graphic devices added-on-just-for-control, 
handPAM relies on: straightforward spatial manipulations; simple rules of tree structure for applying 
them collectively to groups of objects; and finally a supple control system connecting the human's hands 
to the space &#38; tree manipulations so that within a short time mouse movements and keyset chording 
will be second nature. handPAM is actually a generalization of Warren Teiteiman's LISP editor to text-graphic 
patterns on a static display; Teitelman [7] credits Peter Deutsch for the original idea of a structure 
editor. Deutsch first implemented one at the University of California at Berkeley in 1965. The non-specific 
'hand controls' of the phenomenological description will be implemented as the mouse and keyset of English, 
Engelbart and Berman [5], along with an alphanumeric keyboard. The version of handPAM currently running 
has only a keyboard, using typed commands to simulate mouse and keyseL  A Structure from Manipulation 
for Text-Graphic Objects A te×t.graphic object is either a line or a pattern. A line is a drawline or 
a character or a textline. A draw/ine is a line drawn through none or more locations. A character is 
one or more drawlines.  L T K ~ A textline is one or more characters. GEORGE A pattern is a group of 
none or more lines and/or patterns. El. -1 T ---~' - ]EORGE Figure 2 The first step in the creation 
of handPAM was the analysis of text-graphic performances using more conventional media, including blackboards, 
paper and pencil, and overhead projectors. In common to all these media is the selective creation, moving 
and erasing of text-graphic objects. Applying such manipulations selectively requires some way of structuring 
complex objects and their parts; trees are a simple and obvious way to structure visual objects, e.g. 
, ,. ~," Complex objects with such a tree structure  O (3 (shown by the thin line network) are called 
text-graphic patterns. Figure 2 delineates the basic structures for objects in handPAM, Note the three 
levels of atoms (atomic objects are all called lines in PAM): drawlines, characters, and textlines. Although 
this may seem illogical, it makes sense from a user's ordinary orientation, in which a line of text and 
a drawn line are both indivisible atomic objects for manipulation (later very important when GRABBtNG 
objects into attention). For instance, the human in figure 1 has constructed the pattern george as having 
seven atomic members, one of which is the texthne GEORGE, another of which is the drawline that is the 
leR ear, another of which is the drawline that is the outline of the head, and so on. A rough metaphor 
expressing this orientation is that there is as much information in each character in a fine of text 
as carried by each location in a drawn line. Of course, other manipulative orientations are possible 
in which one would want to edit the locations in a drawline or the characters in a textline, but these 
are viewed as secondary 'modes' which one enters and exits from the basic structures pictured in figure 
2. Note also the ways in which patterns follow LISP exactly: patterns can be empty, in which case they 
are represented by the ground-sign character nitpat; otherwise a pattern is an ordered list of members, 
each of which is either a line or a pattern (no recursive patterns are shown in figure 2). The Separation 
of Space &#38; Tree And note finally the way in which patterns are displayed, which can differ radically 
from the way LISP lists are represented for human viewing. This difference is the 'separation of space 
&#38; tree' which PAM maintains but LISP does not (and can not because order of members in a printed 
list must be denoted by spatial ordering). This separation is not an issue in figure 2 because all the 
patterns were carefully constructed to be ordered from left to righL The point is that PAM patterns can 
be constructed so that members have other spatial orderings than left-to-right. In PAM patterns, spatial 
structure and tree structure are completely independent; the spatial arrangement of objects in a pattern 
does not necessarily express or imply any particular tree structure. It is this complete separation 
of space &#38; tree which requires the use of additional thin black lines to denote the tree structure 
of patterns; i.e. the thin tree networks which show the order of members in a pattern and at the same 
time show if any members are patterns themselves. Perhaps not so obviously, it is only the complete separation 
of space &#38; tree which permits handPAM to support non-specific blackboard type text-graphic activity 
(such as represented by figure 6, where no simple spatial parsing rule like left-to-right will group 
objects in a way natural for human manipulation). To illustrate the freedom obtained by divorcing space 
&#38; tree, a parallel can be drawn between left-to-right ordering in PAM and alphabetical ordering in 
LISP. Imagine a LISP input reader which always rearranged atoms in lists to alphabetical order -clearly 
this would be useless for most purposes. Instead, atoms in lists can have any order, and then if we want 
alphabetical we write a sort function. Likewise, spatial positions of PAM objects necessitate no particular 
tree ordering, and then we can write functions which sort and/or parse depending upon specific applications 
(for example LeftToRightLayout, DirectionOrder, or the spatial parser for SIBTRAN in figure 6). Figure 
3 shows the pattern george with two different tree orderings and three different spatial arrangements. 
a. c.  ~EDF~E b. d.  t ~ED~gE " EEDF~gE Figure 3 george b has right-to-leR ordering, while the other 
three have left- to-right ordering. Trees have been shown in all cases, because without trees george 
a and george b both appear the same. Only the tree structure of george b has been reversed; the spatial 
location of each of his members is unchanged Therefore the reversal is only revealed when the trees are 
shown, or when george b is laid out spatially by a 'pretty positioner' (georged). george c and george 
d have both been laid-out spatially from left to right by that function - LeftToRightLayout -which uses 
tree structure to determine spatial position in the same way that LISP printers always must. Handling 
Space &#38; Tree Structure GEORGE " tree Spatial Manipulations are performed by the right hand using 
the mouse: DRAWING a line with the cursor, locating a new line of TEXT, GRABBING the object nearest the 
cursor into attention; and then DRAGGING, SCALING, ROTATING or ERASING the object in attention. Tree 
Manipulations are performed by the left hand on the keyset: Directing attention along the tree structure 
to the FIRST object in a pattern, or to the RESTof the objects, or UP one level to the pattern of which 
the current object of attention is a member; changing tree structure by COLLECTING new patterns, or INSERTING 
objects into existing paUerns or EXTRACTING objects from them. Figure 4 Figure 4 shows the manipulative 
grammar in action as the user is ready to spontaneously modify both space &#38; tree structure of objects 
on the display. The alphanumeric keyboard is not shown to emphasize the clean separation of functions 
between the mouse and keyset. The mouse is used for Level 1 operations (see figure 5), all of which have 
a spatial component -DRAWING, locating TEXT, GRABBING, e~ --and so it's fit and proper that they should 
all be operated from the spatial (analog) input device in handPAM. In a sense, the mouse is Level 1. 
Tree manipulations, Levels 2 and 3, arc performed on the keyset. Manipulative grammar really consists 
of an objatn in a space &#38; tree structure; it is not simply trees for visual objects, but rather the 
dynamic interplay between a manipulable ob]atn and the use of space &#38; tree structure for attention 
shifting (i.e. changing the objatn) and object construction. It is this dynami 9 interplay which characterizes 
the tree structures in the handPAM environment. Many graphics systems employ trees (i.e. Defanti[10]), 
but the structure of PAM patterns are experiential' trees -the user actively grasps the structures as 
he uses them to. drive attention around among the objects on the display. The 4 Levels of handPAM Line 
~.~11" Manipulation ~. .~..OIZ~E j~tll- /:. scarf r o'~,.f~ Simple Pattern Manipulation -F -F Recursive 
Pattern Manipulation ?,~tv~ u9 re~'t -I-°;- + + -F Evaluation of Names and Function Calls SETNAME G G 
ZS NOW A NAME t -I- ooooo0ooO0O ooeoo t~d~,.o.tz object of ~t:e.,,C,.a~,,., ~eU..,t ~,,.k,t~ ~ ~Jor GGoRGE 
e~_tL, o..tT,..bj~t ~ =t~,.-t/.u:,% reX*~ ,,ab,¢ ~'t o,rs~ Figure 5 Figure 5 shows the 4 Levels ofhandPAM. 
Level 1 provides creation of atomic objects, spatial GRABBING of objects into attention, and spatial 
manipulations. Anything which can be put into attention -made the obiatn -can be manipulated. Levels 
2 and 3 permit the manipulation of tree structure, and allow the user to 'drive' attention around among 
the objects on the display using tree guided attention shifters (FIRST, NEXT, UP, etc.). And in Level 
4 evaluation is added to the operations that can be performed on the objatn; evaluation permits naming 
of an object and then later retrieving of an object by evaluating the visual entity that is its name. 
The point should be repeated that it is the oh]atn along with the tree structure of PAM objects which 
together make up the 'manipulative grammar' that is at the heart of handPAM; tree structure alone is 
some kind of grammar for visual objects, but having an objatn climb around that tree on a static (non-scrolling) 
interactive display is what gives the grammar its manipulative quality. It is interesting to note the 
likeness between what is here called manipulative grammar and what Sutherland calls demonstrative language. 
In Sketchpad, demonstrative language seems to be ways Of pointing at objects using the lightpen, both 
objects actually visible and objects or parts topologically related to visible objects (for instance, 
the end point of a line where only some middle segment of the line can be seen) Sutherland [8]. A further 
characterization of this class of techniques is that it is both non-textual (no typed developmentally 
unfold themselves, etc. commands) and structurally knowledgeable (including about structures not directly 
visible). If these conclusions are correct, then manipulative grammar as here defined may well be a demonstrative 
language: hand motions with the mouse and keyset are used to determine and manipulate the (not discernible 
from passive observation) tree structure of patterns. A similar concept is Futrelle's notion of deixis 
: "This is the act of referring to an item in context without having to + -;- name it. Pointing at an 
object is such a deictic reference," Futrelle [3]. This certainly links up with Suthefland's ideas, as 
deictic is defined as: gi,'~ 41~t 1. in grammar, pointing out; demonstrative: as, 'that' is a deictic 
pronoun. The manipulative grammar used in handPAM is deictic in both space &#38; tree; that is, an object 
can be pointed out in its spatial context by putting the cursor on it and grabbing it, or it can be pointed 
out in its tree context by guiding attention to it along the tree structure of or ~-~j~s~ -~' the larger 
pattern of which it is a member (using FIRST, NEXT, BACK, UP GEORGE and REST). More details on the workings 
of handPAM - how the Levels - must be deferred to a future paper. The purpose of the present paper is 
merely to present a tree based manipulative grammar and show how it arises from and facilitates manual 
manipulation. Likewise, further explanation about the evaluation of the objatn which can occur in Level 
4 (writtenPAM) will be covered in another paper, Lakin [17] (although a brief summary of writtenPAM is 
provided in a later section). However, the possibEity of evaluating text-graphic objects is important 
to this paper. It means t_hat writtenPAM possesses the processing power for doing linguistic analysis 
of text-graphic discourse (ie. the user interaction) in at least the same way that LISP facilitates such 
computation on textual interactions (Winograd's SHRDLU [15], Martin's DIP [9], etc.). In PAM as in LISP, 
processing power means providing atomic objects, ways of structuring them into complex objects, and equality 
tests for both atomic and complex objects. A visual linguistics made possible by such processing could 
be useful in several ways. First, knowledgeable manipulation assistants could be built to increase the 
speed and dexterity of manual image handling; creation/modification of text-graphics at blackboard speeds 
is a very challenging application and help will be needed. And then, once good agility has been achieved 
in the manual handPAM medium, more complex machine understandings could be attempted of discourse taking 
place in that medium. A Typical Blackboard Image rr--V-I F I --if- ............ " "~--~AP'PL~CATION5 
L skNatr'e* I ~,n~ t-- T - _  .... ~_ _ -___~ ,SENSOr-MOTOR BAS(1) ~MAWTIC$ i d I I + Figure 6 Figure 
6 shows a more complex blackboard-type image which might be constructed and manipulated in the handPAM 
environment. The graphic devices used in this example (hollow arrows, spirals, etc) are part of the vocabulary 
of SIBTRAN. SIBTRAN is a simplified version of a visual language invented by David Sibbet [11], a graphic 
artist in San Francisco. Sibbet uses his visual language in the course of drawing on large sheets of 
paper to facilitate the problem defining and solving processes of various groups. The kind of work Sibbet 
does can be called 'group graphics', Lakin [12]. This service is based on the fact that writing and drawing 
together often help a group think about something. The term 'explicit group memory' was first coined 
by Ball and Gilkey [13] to describe this phenomenon and document its usefulness. But blackboards in seminar 
rooms and scnools have been silent testimony to its efficacy for over a hundred years. Lately, the uses 
of computer displays for explicit group memory have begun to be observed in conjunction with computer 
teleconferencing (Pferd, Peralta and Prendergast [14]; although there limited to architectural floor 
plans). And perhaps the general use of text-graphic display to facilitate cognition was first examined 
by Doug Engelbart in his "Augmenting Human Intellect: A Conceptual Framework" [4]. The spontaneous manipulation 
of images like figure 6 -in order to facilitate the cognitive activity of a working group -has been the 
driving application of the PAM system ever since its very inception (Lakin [12]). Rigorous scientific 
appraisal of these media is still in the future; the goal of the present paper is simply to describe 
one underlying logic for the construction of such tools. Finally, before leaving this figure, it should 
be mentioned (as an example of linguistic processing of text-graphic objects) that a spatial parser has 
been written (but not yet implemented) for SIBTRAN forms. Spatial parsers take as input a simple pattern 
of drawlines and textlines. They then use spatial information and the syntax of the visual target language 
to reorganize the lines into a complex pattern structured so as to facilitate further processing of the 
form. Assuming that the input pattern is properly formed according to the rules of SIBTRAN, the spatial 
parser for SIBTRAN then proceeds in three passes to finally return a complex pattern structured like 
figure 6. Before parsing can begin, however, it is first necessary to order all of the objects from the 
upper left by using the function DirectionOrder. Then TextGroup, the first pass of parser, collects stacks 
of textlines into patterns, and also puts underlined textlines into patterns with the underline first. 
Enclose, the second pass, is next; it handles enclosure forms such as Rectangles, HollowArrows and Ellipses. 
Enclose collects the inside objects into a pattern with the enclosing graph as first (calling itself 
on each of the inside objects as it collects them from up-left). And finally the third pass, Connectors, 
constructs patterns in which a connector such as a SingleArrow, DoubleArrow, HollowArrow or StraightLine 
is first and its objects are rest (ordered in the direction of the connector if it has one or from up-left 
in the case of DoubleArrows and Straightlines). Connectors then calls itself recursively on each object 
of a parsed connector. It was noted earlier that the needs of manipulating blackboard figures led to 
the complete separation of space &#38; tree in PAM patterns. In support, it is claimed that groupings 
like those diagrammed in figure 6 are 'natural' to a human who is changing and moving the objects, and 
yet cannot be expressed by any simple, fixed space-to-tree correspondence (hence the need for a parser). 
Ambiguities Resolved by Manipulative Grammar Fourteen years ago Ivan Sutherland wrote an article in which 
he outlined 10 unsolved problems in computer graphics [2]. One of these he characterized as the Structure 
of Drawings Problem. Sutherland's example is a graphic editing situation in which there are five similar 
boxes, each with a dot in it (the situation portrayed at the top of figure 7). Some pointing device is 
positioned as shown and a DELErE command is given. "The correct computer response depends upon the real 
underlying structure of the drawing." For instance, the user could mean delete the dot in that box, delete 
the whole box, or perhaps delete the group of two boxes of which that box is a member! Figure 7 presents 
different structural representations for five boxes with dots. Ambiguous reference // mi: \ / Partial 
clarification [ r, F ~r,. ''it" handPAM solution Figure 7 Sutherland goes on to say that "... we must 
have languages powerful enough to explicitly represent the structure of pictures. Moreover, the languages 
must represent the structure naturally enough so that it doesn't get in our way. We must represent the 
structure so users can see it. In [the middle object in figure 7] a dotted line around the groupings 
shows the structure, but that would not be appropriate in every case. We must learn how to represent 
structure for human consumption. We must also build languages which can construct structure." handPAM 
offers a way of representing the structure of the boxes with dots that would be appropriate in this and 
many other cases. Yet this structure notation and the need for it arises out of a different viewpoint 
than Sutherland's, as evidenced by one other remark he makes: "In fact, we usually do not even think 
explicitly about the underlying structure." Because the user initially experiences the PAM system as 
a manual graphics editor providing selective manipulations on visual objects, the question of structuring 
visual objects in order to manipulate them is naturally with the user from the very first time she wants 
to handle a group of atomic objects. Structure is not some programmatic issue that comes up down the 
road a ways -it arises immediately out of the user's desire to group and manipulate objects. The user 
will continually be thinking explicitly about the structure of objects. In handPAM, the user can directly 
manipulate this underlying structure of each individual object just as she does its visual properties, 
and she can make the structure visible by using the system function ShowTree. The bottom object in figure 
7 shows how Snowrree would represent the structure indicated by Sutherland for the group of five boxes. 
And that part which has been singled out for manipulation -the obiatn in handPAM -is indicated by being 
thickened (although blinking, colored halo or other devices could be used). Futrelle [3] raises two important 
ambiguities which manipulative grammar seems to resolve. One is the relation of text and graphics: "It 
is natural to want to design a 'language' in which the program text and graphical objects are treated 
on an equal footing, a language in which the coding is done by typing text and by drawing pictures. Such 
a fully integrated language we call an Intrinsically Graphicial Language or IGL. A direct attack on this 
problem fails because text and pictures are of two different modalities so that the relations between 
them are not readily perceptible to a human being." By having the three levels of atoms -drawlines, characters 
and textlines --handPAM establishes a readily discernible and useful relation between text and graphics. 
Text is just a way of thinking about graphics, with certain rigid rules for laying out combinations of 
26 little pictures (and each of the 26 little pictures is really just a pattern of drawlines). Experience 
with handPAM has shown these relations quite handy for manual manipulation, providing good agility as 
one composes text-graphic images (and sub-atomic editing for drawlines, characters and textlines is provided). 
These basic structures also provide a means for programmatic manipulation of text-graphic images, thus 
leading to writtenPAM programs which both are text-graphic forms and do computing with text-graphic forms 
(see the membership test defining form in figure 8; hence writtenPAM may be an IGL). Put another way, 
the structures in manipulative grammar are a way around the impasse "...which results because pictorial 
information does not map onto code in any obvious way." And in fact such a mapping must be found if one 
wants to deal with pictorial symbol systems like Chinese characters, which was one of the defining applications 
for handPAM (the last character example in figure 2). Figure 8 shows various objects which illustrate 
the relation between text and graphics in PAM. The top of figure 8 demonstrates the result of trying 
to show the trees for the three different kinds of atomic objects; in the middle is the textline with 
its tree shown, possible only because flags have been reset to permit disclosure of the 'sub-atomic' 
structures; and on the bottom is an example of computing with text-graphic forms, presenting a graphic 
synonym for one definition of the membership test in LISP. FOOPLEDA~B 5ETO OUOTE ×Y Figure 8 Clearing 
up the relation between text and graphics also partly solves another problem that Futrelle sees, namely 
the "... careful and efficient disambiguation of deictic references .... If a visible object appears 
on the screen, pointing to it may be an ambiguous reference .... Some ambiguities which occur with spatial 
deictic reference alone are resolved in handPAM by the three levels of atoms and pattern structures. 
That is, when the cursor is on the crossbar in the E in GEORGE in the pattern george, which object is 
being pointed at? Or more concretely, which object will a user get when she does a GRAB? The answer to 
these questions is provided first by the convention that textlines are atomic to GRABBING, SO the only 
remaining choice is whether just the atom GEORGE will become the Objatn or whether it will be the whole 
pattern george (ears, head, name, etc). And that choice is made by the user when she sets the GrabMode, 
picking either GRABLINE or GRABTOP (the latter grabs the largest pattern of which the result of GRABUNE 
is a member). But finding a way to m/~p graphics onto text only solves some of the ambiguities of deictic 
reference. "If the programmer points to a certain part of a picture it is not obvious which attribute, 
the color, line width, region, position, etc. is intended, and furthermore whether reference is being 
made to the generic attribute, the value(s) bound to it, or what." One way to think of this problem is 
as the manipulation of unobservable attributes. That is, in handPAM the location, rotation, scale and 
appearance (vectors in the drawlines) of objects are directly observable and manipulable. Tree structure, 
on the other hand, is not directly observable and yet the user is able to knowingly modify it using tree 
manipulation operations like INSERT, EXTRACT, and PATTERN COLLECT. He can do so because handPAM supplies 
him with two ways of observing this 'unobservable' attribute. The first is the tree traversing attention 
shifters -F~RST, NEXT, UP, etc. --which offer an experiential way of discovering a pattern's structure 
by climbing around it. And the second is the handPAM operation SHOWTREE, which calls the system function 
ShowTree to draw the tree of the objatn (or, for atomic objects, label what level atom it is; see figure 
8). Another unobservable attribute for atomic handPAM objects is value; the object (if any) that a line 
is bound to is discovered by putting it into attention and performing the handPAM operation EVALUATe. 
Values can be set by the operation NN~E. The observation and manipulation of other unobservable attributes 
has been handled on an ad hoc basis thanks to the flexibility of the LISP implementation environment. 
For instance, the vectors in objects can have a thickness, but the resolution of the CRT is not good 
enough to show it; so the LISP function call (Thicken 1.0) thickens the objatn by one pixel on the paper 
output devices, and (GetThickness objatn) returns the status of this attribute. The ShowTree solution 
to displaying pattern tree structure is an example of a classic approach for viewing unobservable attributes. 
This approach consists of finding a general graphic representation of the attribute, and then having 
the system automatically generate and display the proper specific representation for the status of a 
particular object's attribute whenever the user asks. Sutherland first used this approach for showing 
constraints in Sketchpad. Futrelle uses a similar device he calls a 'Graphic Aid' for visible and not-so- 
visible attributes of general classes he calls Objects. In both cases, the additional wrinkle is offered 
that the user can then edit the representation of the attribute and have the attribute itself change. 
This wrinkle could be implemented quite easily for object attributes in handPAM; it would be particularly 
nice to edit a shown tree and have the pattern's tree structure change correspondingly. (The point should 
be made that in PAM, unlike in Sutherland's and Futrelle's systems, there are no 'masters' or 'classes' 
who have 'instances'; in PAId all objects are unique and individual.) Figure 9 shows three examples of 
editable representations for not-so-observable attributes: Sutherland's constraint representation in 
Sketohpad; Futrelle's Graphic Aids; and a proposed editor for showntrees in handPAM. CENT L -~ Constraint 
representation Graphic Aid ShownTree Editor Figure 9 In the case of Sutherland's diamond-headed arrow 
on the left, the user can manipulate the constraints for this defintion picture through their displayed 
forms, including the attacher-point constraints (shown with code T), non-rotation (code E) and constant 
size (code F). The Graphic Aid for Futrelle's Object of class Bow in the middle allows the user to change 
the Width, Length or Center (location) of the Bow by interacting with the textual representation of these 
qualities. And on the right, the user is preparing to insert the HollowArrow (in attention) into the 
pattern by indicating the place on the pattern's showntree where the new member is to be inserted. Relation 
of handPAM to writtenPAM Structuring and Manipulating Text-Graphic Images  / PAM \ user entre~ =~. 
handPAM writtenPAM handPAM's4 Levels are designed to unfold themselves developmentally according to the 
logic of text-graphic manipulation. The 4th Level of handPAM is actually writtenPAM, where text- graphic 
patterns are evaluated and the result is displayed on the screen. By building up his operational experience 
of pattern structure and manipulation, handPAM easily leads the user to a basic understanding of the 
evaluation function which defines writtenPAM. In the phenomenolgical view, interface design is seen as 
a continuum running from the sensory-motor to the cognitive. writtenPAM is a generalization of LISP from 
computing with textual symbolic expressions to computing with text-graphic forms. writtenPAM allows people 
to talk directly about text-graphic objects and manipulations on them, as for example in setting the 
value of constructing patterns PATCON$ QUOTE taking them apa~ FIRST~ ' ~'~' = GEORGE GEOR&#38;E and 
making equality tests I --~-----~-r--~' ~' E.QUAL? = FALSE In short, writtenPAM's functions and predicates 
operate on visual objects and return them as values. As a matter of fact, this language may be 'intrinsically 
graphic' (Futrelle [3]) and only later textual, but we still speak of writing programs, so we call it 
writtenPAM for convenience. Manipulative Grammar is Pre-semantic Note the size of the class of possible 
semantics that can fit onto the Levels of handPAM in figure 5. The only syntax so far is trees -and code-objects 
manipulated by the user could easily have other syntaxes; and the only semantic commitments are the functions 
NAME and EVALUATE --and the actual rules used by the evaluation function could vary widely from system 
to system. For example, a PASCAL version of handPAM might look the same to the user in the first 3 Levels 
(i.e. manual manipulation of text-graphic objects). But the 4th Level -using text-graphic objects as 
instructions to a computer -would have the functions COMPILE and RUN instead of EVALUATE. Thus an executable 
object would be constructed, put into attention, and the COMPILE command given. The result of compilation 
would be a text pattern returned at the cursor consisting of the name of the runable file and/or a list 
of error messages. If the compilation was successful, then the name of the runable file could be put 
into attention and the RUN command given, with the result of the execution returned at the cursor. And 
an APL version of handPAM would be simply a matter of having different rules for the interpreter -perhaps 
APLEVALUATE. And, actually, there is no reason why writtenPAM, PASCAL and APL need be confined to separate 
systems; a user-set system variable could determine which rule system was invoked when the evaluation 
chord on the keyset was struck. Handling Text-Graphic Objects Computing with Text-Graphic Forms PAttern 
Manipulation includes both manual handling and computational processing of text-graphic images. Applications 
handPAM is not an application, it is a context for applications -a general purpose environment for manipulating 
text and graphics. And then specialized application programs can be written which live within this environment; 
for example, data plotting, program dynamics diagrammers, circuit design aids, text editors, graphic 
programming notations, etc. One advantage of specialized applications taking place within a general purpose 
environment is that all visual objects returned by specialized functions are built as if they had been 
constructed by hand and are therefore subject to further modification by hand. Thus the definable, repeatable 
parts of applications can be done automatically, and the rest left to in- session user intervention. 
The converse of this feature is that the programmer can first manually simulate any new specific application, 
and then incrementally write programs which take over easily definable sub-functions in that particular 
domain. Another advantage of a general graphic context is that a function built for one purpose (like 
LeffToRightLayout, for spacing characters in textlines) can usually be applied outside its original application 
and thus becomes one more general utility function (see figure 3 for LeftToRightLayout applied to george). 
To use an analogy, specialized applications are like trees and handPAM is a nursery, providing the common 
ground wherein different trees can be grown and work together in an interdependently functioning forest.. 
At the moment, five different groups of specialized application functions are being assembled (Lakin 
[18]): 1) OrawPlan, a function which draws a diagram of the plans created by Sacerdoti's planning program 
[6]. 2) FinderMap, a group of functions which dynamically display finder activity as nanoKRL programs 
execute, Martin [9]). 3) DataPIot, a technical illustration package for experimental results. 4) FlowCharter, 
being simply the functions box, connect and ArrowEdit for boxing objects, connecting them by arrows, 
and editing arrows when the automatic routing is unsatisfactory. 5) OUTLINE, a visual synonym system 
for LISP code using graphic 'nameshapes' instead of textual function names (see figure 8). However, now 
that the above has been said, the truth is that handPAM is also itself an application: the Electric Blackboard. 
A 'smart blackboard' could be useful any time writing and drawing together would help a group to think 
more effectively. And because all the specific application functions live within the general device, 
whenever a discussion lead into some specific topic like circuit design or data analysis, the same medium 
could still support the group's thinking wherever it went. Finally, if necessary, new functions could 
even be written on the spot to add some new feature so the discussion could continue more easily. Phenomenolgy 
and Graphic Interface Design Careful description of something without concern for its reality yields 
a phenomenology - an exposure and delineation of the logic of the phenomenon. A phenomenon is literally 
a 'that which appears', an appearing to a consciousness; phenomenology provides a way to give a precise 
account of such an appearing (Natanson [16]). Taking manipulation of text-graphic images as a phenomenon, 
this viewpoint allows us to describe it while for a time disregarding any hardware or software 'behind 
the scenes' which 'really' causes the existence and behavior of the images. What we have left is the 
human, the text-graphics, and his manipulation of them,., which is the starting point for this paper. 
Designing a graphics system in the method used for handPAM -starting with data structures and operations 
which arise from text-graphic manipulation itself -is thus a phenomenological approach. The design grows 
out of the logic of the phenomenon of manipulation as experienced from the human's point of view. Another 
way to think of this approach is as front-in design -starting with the human and the graphics, and then 
(slowly!) moving toward the computer. It cannot be emphasized too strongly how important this phenomenological 
principle was in creating the PAM system: the project was born out of it and returned to it many times 
for renewal and guidance. Notes on the Illustrations Figures 2, 3, 7 and 8 were constructed in the experimental 
handPAM environment at SAIL. Figure 3 was printed on a Xerox Graphics Printer, while figures 2, 7 and 
8 were printed double size on a Varian States and then photographically reduced. The remaining illustrations 
are pen and ink sketches. Annotative text in figures was done on the Xerox Graphics Printer and then 
pasted up. Acknowledgements Special thanks to the people without whom this project could not have happened. 
To John McCarthy, for inventing LISP and giving me access to the computer facilities at SAIL. To Jonathan 
King, Allan Hayes and Ben Laws for long discussions on the pro-computer PAM system. To Jim Collatz for 
actually learning handPAM before it was implemented. To Hans Moravec and Rodney Brooks for providing 
the basic graphics tools at SAIL: and to Dick Gabriel and Paul Martin for showing me the finer points 
of MACLISP. References [1] McCarthy, John and Talcott, Carolyn, LISP Programming and Proving, Class 
notes CS 206, Computer Science Dept., Stanford University, 1979. [2] Suthefland, Ivan E., "Computer Graphics: 
Ten Unsolved Problems", Datamation, pages 22-27, May 1966. [3] Futrelle, R.P. and Barta, G., "Towards 
the Design of an Intrinsically Graphical Language", SIGGRAPH 78 Proceedings, pages 28-32, Aug 1978. [4] 
Engelbart, D.C., "Augmenting Human Intellect: A Conceptual Framework," SRi International, Menlo Park, 
California, Oct 1962. [5] English, W.K., Engelbart, D.C., and Berman, M.L., "Display-Selection Techniques 
for Text Manipulation", IEEE Trans. on Human Factors in Electronics, Vol. HFE-8, No. 1, March 1967. [6] 
Sacerdoti, Earl, "Planning in a Hierarchy of Abstraction Spaces", Adv. Papers 3rd IntL Conf. on Artificial 
Intelligence, Stanford University, August 1973. [7] Teitelman, Warren, InterLIgP Re~rence Manual, Xerox 
Pain Alto Research Center, 1978. [8] Sutherland" Ivan E., "Sketchpad: A Man-Machine Graphical Communication 
System", Proceedings-Spring Joint Computer Conference, pages 329-346, 1963. [9] Martin, Paul A., DIP: 
A Program to Understand Diplomacy Dialogs, PhD thesis, Stanford University, forthcoming. [10] Defanti, 
Tom, '"Eae Digital Component of the Circle Graphics Habitat", Proceedings National Computer Conference, 
1976. [11] Sibbet, David, "Introduction to Group Graphics", The Corospondent, CORn Foundation North. 
Cal. Public Affairs Quarterly, Summer 1976. [12] Lakin, Fred, Design for a Working Group Display, manuscript 
1974. [13] Ball, Geoffrey H. and Gilkey, James Y., "Facilitation and Explicit Group Memory --Their Application 
in Education", SRI International IR&#38;D No. 183531-409, Menlo Park, California, Dec 1971. [14] Pferd, 
William, Peralta, L.A. and Predergast, F.X., "Interactive Graphics Teleconferencing", IEEE Computer, 
Nov. 1979. [15] Winograd, Terry, Understanding Natural Language, Academic Press, 1972. [16] Natanson, 
Maurice, Editor, Essays in Phenomenology, Martinus Nijhoff, The Hague, 1966. [17] Lakin, Fred, "Computing 
with Text-Graphic Forms", submitted to the LISP Conference at Stanford University, August 1980. [18] 
Lakin, Fred, "Indigenous Graphics for LISP", submitted to the LISP Conference at Stanford University, 
August 1980.  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1980</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>807477</article_id>
		<sort_key>108</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-1980</article_publication_date>
		<seq_no>18</seq_no>
		<title><![CDATA[Stochastic modeling in computer graphics]]></title>
		<page_from>108</page_from>
		<doi_number>10.1145/800250.807477</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=807477</url>
		<abstract>
			<par><![CDATA[<p>A recurrent problem in generating realistic pictures by computers is to represent natural irregular objects and phenomena without undue time or space overhead. We develop a new and powerful solution to this problem by modeling objects as sample paths of stochastic processes. Of particular interest are those stochastic processes which previously have been found to be useful models of the natural phenomena to be represented. One such model applicable to the representation of terrains, known as &#8220;fractional Brownian motion&#8221;, has been proposed by B. Mandelbrot.</p> <p>The value of a new approach to object modeling in Computer Graphics depends largely on the efficiency of the techniques used to implement the model. We introduce here a new, efficient algorithm for rendering realistic surfaces defined using the stochastic process mentioned above. A major advantage of this technique is that it allows us to compute the surface to arbitrary levels of detail without increasing the database. Thus objects with complex appearance can be displayed from a very small database. The character of the surface can be controlled by merely modifying a few parameters. A similar change allows complex motion to be created inexpensively.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Brownian motion]]></kw>
			<kw><![CDATA[Computer graphics]]></kw>
			<kw><![CDATA[Fractal surfaces]]></kw>
			<kw><![CDATA[Fractional brownian motion]]></kw>
			<kw><![CDATA[Hierarchical models]]></kw>
			<kw><![CDATA[Motion modeling]]></kw>
			<kw><![CDATA[Object modeling]]></kw>
			<kw><![CDATA[Stochastic modeling]]></kw>
			<kw><![CDATA[Stochastic surfaces]]></kw>
			<kw><![CDATA[Terrain modeling]]></kw>
			<kw><![CDATA[Texture]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.5</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>G.3</cat_node>
				<descriptor>Stochastic processes</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10002950.10003648.10003700</concept_id>
				<concept_desc>CCS->Mathematics of computing->Probability and statistics->Stochastic processes</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061.10010063</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures->Computational geometry</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP39076356</person_id>
				<author_profile_id><![CDATA[81100345881]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Alain]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Fournier]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14201762</person_id>
				<author_profile_id><![CDATA[81100584372]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Don]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Fussell]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Programs in Mathematical Sciences, The University of Texas at Dallas, Richardson, Texas]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 STOCHASTIC MODELING IN COMPUTER GRAPHICS* Alain Fournier** and Don Fussell Programs in Mathematical 
Sciences The University of Texas at Dallas Richardson, Texas 75080 ABSTRACT A recurrent problem in generating 
realistic pictures by computers is to represent natural irregular objects and phenomena without undue 
time or space overhead. We develop a new and powerful solution to this problem by modeling objects as 
sample paths of stochastic processes. Of particular interest are those stochastic processes which previously 
have been found to be useful models of the natural phenomena to be represented. One such model applicable 
to the representation of terrains, known as "fractional Brownian motion", has been proposed by B. Mandelbrot. 
The value of a new approach to object modeling in Computer Graphics depends largely on the efficiency 
of the techniques used to implement the model. We introduce here a new, efficient algorithm for rendering 
realistic surfaces defined using the stochastic process mentioned above. A major advantage of this technique 
is that it allows us to compute the surface to arbitrary levels of detail without increasing the database. 
Thus objects with complex appearance can be displayed from a very small database. The character of the 
surface can be controlled by merely modifying a few parameters. A similar change allows complex motion 
to be created inexpensively. As examples of applications of the method, we show an artificial planet 
viewed at various levels of detail, and a piece of paper being uncrumpled. KEY WORDS AND PHRASES: Computer 
Graphics, object modeling, stochastic modeling, stochastic surfaces, texture, terrain modeling, motion 
modeling, Brownian motion, fractional Brownian motion, fractal surfaces, hierarchical models. CR CATEGORIES: 
5.12, 5.13, 5.5, 8.1, 8.2 *This research was supported in part by NSF Grant MCS-79-01-168 and was facilitated 
by the use of the Theory Net NSF Grant MCS-78-01689. **Current address: Department of Computer Science, 
University of Toronto. Permission to copy without fee all or part of this material is granted provided 
that the copies are not made or distributed for direct con~nercial advantage, the ACM copyright notice 
and the title of the publication and its date appear, and notice is given that copying is by permission 
of the Association for Computing Machinery. To copy otherwise, or to republish, requires a fee and/or 
specific permission. &#38;#169;1980 ACM 0-89791-021-4/80/0700-0108 $00.75 108
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1980</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>807478</article_id>
		<sort_key>109</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-1980</article_publication_date>
		<seq_no>19</seq_no>
		<title><![CDATA[Computer rendering of fractal curves and surfaces]]></title>
		<page_from>109</page_from>
		<doi_number>10.1145/800250.807478</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=807478</url>
		<abstract>
			<par><![CDATA[<p>Fractals are a class of highly irregular shapes that have myriad counterparts in the real world, such as islands, river networks, turbulence, and snowflakes. Classic fractals include Brownian paths, Cantor sets, and plane-filling curves. Nearly all fractal sets are of fractional dimension and all are nowhere differentiable.</p> <p>Previously published procedures for calculating fractal curves employ shear displacement processes, modified Markov processes, and inverse Fourier transforms. They are either very expensive or very complex and do not easily generalize to surfaces. This paper presents a family of simple methods for generating and displaying a wide class of fractal curves and surfaces. In so doing, it introduces the concept of statistical subdivision in which a geometric entity is split into smaller entities while preserving certain statistical properties.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Computer graphics]]></kw>
			<kw><![CDATA[Fractals]]></kw>
			<kw><![CDATA[Natural forms]]></kw>
			<kw><![CDATA[Random curves]]></kw>
			<kw><![CDATA[Terrain models]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Fractals</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Curve, surface, solid, and object representations</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010401</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Volumetric models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010399</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Parametric curve and surface models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P173071</person_id>
				<author_profile_id><![CDATA[81100040066]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Loren]]></first_name>
				<middle_name><![CDATA[C.]]></middle_name>
				<last_name><![CDATA[Carpenter]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Boeing Computer Services, Seattle, Washington]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 COMPUTER RENDERING OF FRACTAL CURVES AND SURFACES Loren C. Carpenter Boeing Computer Services Seattle, 
Washington ABSTRACT Fractals are a class of highly irregular shapes that have myriad counterparts in 
the real world, such as islands, river networks, turbulence, and snowflakes. Classic fractals include 
Brownian paths, Cantor sets, and plane-filling curves. Nearly all fractal sets are of fractional dimension 
and all are nowhere differentiable. Previously published procedures for calculating fractal curves employ 
shear displacement processes, modified Markov processes, and inverse Fourier transforms. They are either 
very expensive or very complex and do not easily generalize to surfaces. This paper presents a family 
of simple methods for generating and displaying a wide class of fractal curves and surfaces. In so doing, 
it introduces the concept of statistical subdivision in which a geometric entity is split into smaller 
entities while preserving certain statistical properties. KEY WORDS AND PHRASES: computer graphics, fractals, 
random curves, terrain models, natural forms. CR CATEGORIES: 3.41, 5.12, 5.13, 8.1, 8.2 Permission to 
copy without fee all or part of this material is granted provided that the copies are not made or distributed 
for direct co~nercial advantage, the ACH copyright notice and the title of the publication and its date 
appear, and notice is given that copying is by permission of the Association for Computing Machinery. 
To copy otherwise, or to republish, requires a fee and/or specific permission. 01980 ACM 0-89791-021-4/80/0700-0109 
$00.75 109
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1980</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>807479</article_id>
		<sort_key>110</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-1980</article_publication_date>
		<seq_no>20</seq_no>
		<title><![CDATA[A 3-dimensional representation for fast rendering of complex scenes]]></title>
		<page_from>110</page_from>
		<page_to>116</page_to>
		<doi_number>10.1145/800250.807479</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=807479</url>
		<abstract>
			<par><![CDATA[<p>Hierarchical representations of 3-dimensional objects are both time and space efficient. They typically consist of trees whose branches represent bounding volumes and whose terminal nodes represent primitive object elements (usually polygons). This paper describes a method whereby the object space is represented entirely by a hierarchical data structure consisting of bounding volumes, with no other form of representation. This homogencity allows the visible surface rendering to be performed simply and efficiently.</p> <p>The bounding volumes selected for this algorithm are parallelepipeds oriented to minimize their size. With this representation, any surface can be rendered since in the limit the bounding volumes make up a point representation of the object. The advantage is that the visibility calculations consist only of a search through the data structure to determine the correspondence between terminal level bounding volumes and the current pixel. For ray tracing algorithms, this means that a simplified operation will produce the point of intersection of each ray with the bounding volumes.</p> <p>Memory requirements are minimized by expanding or fetching the lower levels of the hierarchy only when required. Because the viewing process has a single operation and primitive type, the software or hardware chosen to implement the search can be highly optimized for very fast execution.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Computer graphics]]></kw>
			<kw><![CDATA[Hierarchical data structures]]></kw>
			<kw><![CDATA[Object descriptions]]></kw>
			<kw><![CDATA[Visible surface algorithms]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.6</cat_node>
				<descriptor>Graphics data structures and data types</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010394</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Graphics file formats</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP31094893</person_id>
				<author_profile_id><![CDATA[81100267814]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Steven]]></first_name>
				<middle_name><![CDATA[M.]]></middle_name>
				<last_name><![CDATA[Rubin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Bell Laboratories, Holmdel, New Jersey]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P285628</person_id>
				<author_profile_id><![CDATA[81100586999]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Turner]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Whitted]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Bell Laboratories, Holmdel, New Jersey]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>360354</ref_obj_id>
				<ref_obj_pid>360349</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Clark, J.H. Hierarchical geometric models for visible surface algorithms. CACM, 19-10, October 1976, pp. 547-554.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Reddy, D.R. and Rubin, S., Representation of three-dimensional objects, CMU-CS-78-113, Dept of Computer Science, Carnegie-Mellon University, April 1978.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Brooks, J. et al, An extension of the combinatorial geometry technique for modeling vegetation and terrain features, Mathematical Applications Group Inc., NTIS AD-782-883, June 1974.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807458</ref_obj_id>
				<ref_obj_pid>800249</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Csuri, C. et al, Towards an interactive high visual complexity animation system, SIGGRAPH '79 proceedings, August 1979, pp. 289-299.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>358882</ref_obj_id>
				<ref_obj_pid>358876</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Whitted, T., An improved illumination model for shaded display, to appear |CACM]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Kay, Douglas S., Transparency, refraction and ray tracing for computer synthesized images, M.S. Thesis, Cornell University, January 1979.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_obj_id>356626</ref_obj_id>
				<ref_obj_pid>356625</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Sutherland, I., Sproull, R., and Schumacher, R., A characterization of ten hidden surface elimination algorithms. ACM Computing Surveys, Vol. 6, No. 1, March 1974, pp. 1-55.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Forrest, A.R., On Coons and other methods for the representation of curved surfaces. Computer Graphics and Image Processing, vol 1, 1972.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_obj_id>906872</ref_obj_id>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Riesenfeld, R.F., Applications of b-spline approximation to geometric problem of computer aided design, PhD thesis, Syracuse University, 1972.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_obj_id>907365</ref_obj_id>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Newell, M., The utilization of procedure models in digital image synthesis, PhD Thesis, Computer Science, University of Utah, Salt Lake City, Utah, 1975.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_obj_id>578095</ref_obj_id>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Rosenfeld, A. and Kak, A.C., Digital Picture Processing, Academic Press, New York, 1976.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_obj_id>905703</ref_obj_id>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Kelly, M.D., Visual identification of people by computer, AIM-130, PhD Thesis, Computer Science, Stanford University, Stanford, Ca., July 1970.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Price, K. and Reddy, D.R., Matching segments of images, IEEE Trans. on Pattern Analysis and Machine Intelligence, 1 ,1, 1979, pp 110-116.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_obj_id>907242</ref_obj_id>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Catmull, E., A subdivision algorithm for computer display of curved surfaces, UTEC-CSc-74-133, PhD thesis, Computer Science Dept., Univ. of Utah, Dec. 1974.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_obj_id>358815</ref_obj_id>
				<ref_obj_pid>358808</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Lane, J.M., Carpenter, L.C., Blinn, J.F., and Whitted, T., Scan Line Methods for Displaying Parametrically Defined Surfaces. CACM, 23-1, January 1980, pp. 23-34.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 A 3-Dimensional Representation for Fast Rendering of Complex Scenes Steven M. Rubin Turner Whitted 
 Bell Laboratories Holmdel, New Jersey 07733 ABSTRACT Hierarchical representations of 3-dimensional 
objects are both time and space efficient. They typically consist of trees whose branches represent bounding 
volumes and whose terminal nodes represent primitive object elements (usually polygons). This paper describes 
a method whereby the object space is represented entirely by a hierarchical data structure con-sisting 
of bounding volumes, with no other form of representation. This homogeneity allows the visible sur- face 
rendering to be performed simply and efficiently. The bounding volumes selected for this algorithm are 
parallelepipeds oriented to minimize their size. With this representation, any surface can be rendered 
since in the limit the bounding volumes make up a point representation of the object. The advantage is 
that the visibility calculations consist only of a search through the data structure to determine the 
correspondence between terminal level bounding volumes and the current pixel. For ray tracing algorithms, 
this means that a simplified operation will produce the point of intersection of each ray with the bounding 
volumes. Memory requirements are minimized by expanding or fetching the lower levels of the hierarchy 
only when required. Because the viewing process has a single operation and primitive type, the software 
or hardware chosen to implement the search can be highly optimized for very fast execution. KEY WORDS 
AND PHRASES: computer graphics, visible surface algorithms, hierarchical data structures, object descriptions 
CR CATEGORIES: 8.2, 6.22 Permission to copy without fee all or part of this material is granted provided 
that the copies are not made or distributed for direct co~ercial advantage, the ACM copyright notice 
and the title of the publication and its data appear, and notice is given that copying is by permission 
of the Association for Computing Machinery. To copy otherwise, or to republish, requires a fee and/or 
specific permission. 01980 ACM 0-89791-021-4/80/0700-0110 $00.75 Introduction With increases in display 
resolution and processing power, computer generated images have become more complex. The state of the 
art has progressed far beyond "stick figure" representations of scenes to the point where details of 
realism are sought. In addition, the objects to be displayed are often complex, making the display process 
both time and space intensive. The most popular approach to complex image gen- eration has been to approximate 
surfaces with collec-tions of polygons. Generally, a good approximation requires an enormous number of 
polygons and a corresponding amount of display time. To help address the problem of complex object descriptions, 
alternative representations have been used. Complex surface representations such as quadric and cubic 
patches [8,9] are well suited to curved objects but are not general enough to model an arbitrary scene. 
Another alterna- tive, procedural representation [10], is completely gen- eral and relatively compact 
but still requires a mechan-ism for generating and displaying non-procedural primi- tive surface elements. 
Three dimensional "point" representations [4,6] are also completely general, rela-tively easy to display, 
but incredibly wasteful of memory. Hierarchical representations which decompose the object space into 
repeatedly simpler subspaces [1,2,3], are a promising form which allow arbitrary scene descriptions in 
an easily usable data structure. This paper will present a new twist on hierarchical representations 
of scenes that has many computational advantages. Clark [1] proposed the use of hierarchical geometric 
representations to speed both clipping and visibility cal- culations. Each level of the hierarchy consists 
of bounding volumes which enclose the lower levels. At the bottom of the tree, object representations 
are encoded in some conventional form such as polygons. He introduced the notions of "resolution clipping" 
and "graphical working set". According to these notions, levels of the data structure which describe 
detail at a greater resolution than can be resolved in the image are clipped from the current object 
description along with those portions of the scene which lie outside the view- ing area. In its "multi-stage 
combinatorial geometry model", MAGI [3] utilizes a tree structured object description. The branches of 
the hierarchy are arbitrarily oriented rectangular parallelepipeds that enclose subvolumes of the object 
(see Figure 1). It is again necessary to switch to an alternative form of representation at the bottom 
level in order to describe the object. The visible surface algorithm is a ray tracing technique in which 
the inverse of the coordinate transformation of each bounding box is applied to the ray at each node 
in the tree. The hierarchical structure reduces the number of candidate objects against which the ray 
must be tested for inter- section, and the successive transformations insure that the intersection calculations 
will be simple ones. Appropriately, the objects being displayed are trees (e.g. deciduous or coniferous 
instead of binary). Because of the enormous complexity of these objects, image gen- eration times are 
several hours. Reddy and Rubin [2] present three forms of hierarchical decomposition which, individually 
or in a combination of two, are sufficient to completely describe an object. At the initial levels of 
the hierarchy, rec-tangular parallelepipeds (like the MAGI system) parti- tion the object space. At lower 
levels of the hierarchy, one of two other representations can be used. The first low-level representation 
divides this sub-volume of the object space into eight equal sized subspaces by placing a partition in 
the middle of each axis. These eight sub- spaces are either empty, full, or further subdivided in the 
same binary manner (see Figure 2). The second low-level representation also divides the object subspace 
with partitions perpendicular to the axes. In this scheme, however, there can be multiple partitions 
along each axis at arbitrary locations (see Figure 3). Although the point accessing algorithm is more 
expensive in the second representation, less space is needed to represent an object due to the flexibility 
of the partitioning. The representation proposed in this paper con-structs the object space completely 
out of hierarchically structured subspaces. The terminal nodes do not con-tain another type of primitive, 
but are themselves displayable. Some of the advantages of this uniformity are an ability to examine the 
object at arbitrary magnification and the flexibility of combining common subspaces that share micro-descriptions. 
The subspaces that we have been using are rec-tangular parallelepipeds. This simple unit can be described 
and traversed with a single transformation matrix, thus lending the process well to easy and efficient 
hardware implementation. The next section discusses this representation in detail. Representation Conventional 
visibility calculations suffer a com-binatorial explosion when confronted with complex object descriptions 
[7]. Typical algorithms for hidden surface elimination require that each surface in the scene be sorted 
into a computationally effective order and then compared with a neighborhood of other sur-faces. As the 
scene becomes complex, the neighbor- Figure I: Arbitrarily oriented rectangular paral- lelepipeds modeling 
a hierarchically described object space. Figure 2: Binary subdivision of the object space along the X, 
Y, and Z axes. Figure 3: Unequal subdivision Of the object space with arbitrarily placed planes perpendicular 
to the X, Y, and Z axes. hood size becomes combinatorially unmanageable. A ray tracing algorithm, on 
the other hand, executes in a time that increases linearly with the number of objects in the scene. This 
method determines visibility by extending lines (rays) from the image plane into the object space. Whenever 
a ray intersects one or more objects, the nearest point of intersection is the visible one. In addition, 
it needs no explicit clipping operation and is the only algorithm suited to the global illumina- tion 
models of Kay [6] and Whitted [5]. A structured object space makes hidden surface elimination more efficient. 
By dividing the object into a small number of subspaces (perhaps ten) the ray traced from the screen 
can easily be checked for object inter- section at a macro level by comparing it with the simple subspaces. 
Each of these subspaces is an arbitrarily rotated, translated, and scaled rectangular parallelepiped 
(see Figure 4a). It is described with a four-by-four transformation matrix which transforms the ray in 
the object space into a ray within this subspace. All that is required to determine intersection is a 
vector transfor- mation and a comparison against the limits of the sub- space boundary. If the ray misses 
all of the paral-lelepipeds at the top level, then the corresponding screen pixel is "empty" (as is often 
the case in simple scenes). If it successfully penetrates a subspace, then the search proceeds at the 
lower level where all of the sub-subspaces of that subspace are examined in the same manner (see Figure 
4b). When the ray enters a parallelepiped that is not further subdivided, then it has reached a solid 
surface whose characteristics can be displayed. Coupled with the standard benefits of ray tracing are 
a new set of features that this hierarchy provides. Logarithmic access time (instead of polynomial time) 
stands out as the best feature. It typically takes only five or six levels of subdivision to represent 
complex scenes because each level contains about order of mag- nitude more detail. Thus, each linear 
increase in access time caused by an additional level of subdivision yields an exponential increase in 
resolution at that level. Object spaces are often shallow trees with a high branch- ing factor. Even 
with a shallow tree such as this it is possible to conserve storage at lower levels when there are com- 
mon object space features. Any subspaces in the entire tree that have the same detail, regardless of 
the environment, can share their descriptions since all environmental information (location, orientation, 
and scale) is ~ contained in the higher levels of the hierarchy. Most objects can take advantage of this 
feature because they have some common properties (usually surface details such as tree bark, windows 
in buildings, etc.) Thus, the hierarchy of subspaces is actually constructed as a graph instead of a 
tree. No efficiency is lost and much space is saved. Another advantage of this scheme is its total gen- 
erality. Any object can be represented since, in the limit, the parallelepipeds can form a point representa- 
tion. In the following sections we will show how planar polygons can be represented in terms of their 
bounding volumes, and how bi-parametric curved surfaces can be reduced to a point representation. Thus 
rectangular parallelepipeds are very simple objects with which to work. Since they provide uniformity 
of the object description they allow arbitrary magnification of the view to be done with no adjustment 
to the description or viewing algorithm. Figure 4a: The top level (box 20) of a hierarchi-cally structured 
object with 4 subspaces. Figure 4b: The second level (box 61) of the hierarchically structured object 
in Figure 4a. Input Creation of hierarchical databases is a non-trivial operation. The problem is that 
the partitioning at each level requires an understanding of the entire object space. When working with 
dynamically moving objects, an understanding of the motion is also needed to parti- tion the object correctly. 
Even static objects must be intelligently divided to make the viewing algorithm effective. This section 
will discuss a semi-automatic scheme for hierarchical decomposition and will propose some fully-automatic 
possibilities. All data for an object starts as a point or surface representation. This information 
comes from digitizers or algorithms and is rarely aggregated in any hierarchical form. Therefore, the 
first form of data is a two-level hierarchy where the top level represents the entire object space and 
the lower level is the complete scene description with thousands of parallelepipeds corresponding to 
the data points. In order to build a proper hierarchy, we have a structure editor which is able to introduce 
intermediate level spaces in the object. This editor allows random traversal of the object hierar- chy 
and arbitrary creation, deletion, and transformation of the object components. In addition, the editor 
can overlay a non-hierarchical description of an object (the typical initial form) with the hierarchy 
that is being edited to visually assist the human operator. Early experience shows that the editor is 
easy to use: a hierarchical description of the city of Pittsburgh, with hundreds of terminal nodes in 
the hierarchy, took only a day or two to enter. The human who assists the edi- tor in the hierarchy creation 
is looking for object coher- ence, tightness of fit within the domain of rectangular parallelepipeds, 
and dynamic motion consistency. In addition, the human must decide when to instruct the editor to combine 
similar low-level spaces that are to share descriptions. Automating of the hierarchy creation process, 
although not currently implemented, could be done by a number of methods. The simplest technique would 
look for clusters within the object space. It would view the object space as a three-dimensional histogram 
and select peaks which represent object coherence. There are a number of ways to extract multi-dimensional 
his- togram peaks which could be used [11]. One possibility is to reduce the resolution of the object 
space and find clusters in that. Lower resolution spaces are easier to deal with and accurately represent 
the original data [12,13]. For dynamic objects, an initial intermediate level of hierarchy could be built 
around the known degrees of freedom. The initial object space would then contain a top level which is 
"the world", a middle level for the moving units, and a low level for the scene detail. The automatic 
hierarchy routines would then work from there in the same manner. Automatic combining of common subspaces 
would be a pre-processing step that finds common detail in the initial data and merges the descriptions. 
This sort of pattern matching is a difficult problem which can suffer a combinatorial explosion. Reasonable 
solutions must use some pruning of the alternatives to arrive at an answer. It can be seen that the creation 
of a correct hierar- chy requires careful consideration. Sloppy hierarchies will cost dearly in the scene 
rendering stage but good hierarchies are hard to create automatically. In fact, it is possible to understand 
the benefits of rendering hierarchical scenes in terms of the expense of creating them. Simpler object 
representations need less work to create an object space but need more work to render a scene. Thus, 
this scheme derives many of its benefits from being able to off-load all of the combinatorial problems 
to the model creation stage. For many prob- lems of computer graphics this is a desirable tradeoff because 
the display must be as fast as possible. Display Display consists of two operations: visibility deter- 
mination and shading. The shader used with this algo- rithm is described elsewhere [5]. For ray tracing 
algo- rithms, visibility determination is essentially a process of intersecting each ray with a set of 
objects and chosing the nearest point of intersection (Figure 5). Figure 5: Determining correspondence 
between a pixel and visible surface by ray tracing. The structured database improves the efficiency 
of the operation by minimizing the object set to be tested against each ray. Equally important to the 
performance of the algorithm is the speed of the intersection mechanism. It is for this reason that rectangular 
paral- lelepipeds (RP's) are chosen as the only data elements in the object description. Each RP is centered 
in its own coordinate system with faces defined by X=+_Xb Y = ±Yb z ~ .~_ z b as illustrated in Figure 
6. A ray is defined parametri- cally by x = axt + bx y =ayt +by z = a zt + bz. After a ray is transformed 
into the coordinate system of the RP, to test a ray against the faces y = --+Yb one solves the equations 
t I = (Yb --by )/ay t 2 : (--Yb --by )/ay 113 The smaller of the two t values defines the nearer point 
of intersection. Then if --xb --< axt + bx <-- xb and --z b < azt + bz <_ z b the face is pierced by 
the ray. The tests to determine if the ray pierces the x = +--Xb and z = +--Zb faces are identi- cal. 
As the ray progresses deeper into the hierarchy, the dimensions of the RP's become progressively smaller 
until at the terminal level each box is essentially a point in three space. Associated with each terminal 
level box is a normal vector corresponding to the surface normal of the object which generated the box. 
The surface nor- mal is unrelated to the orientation of the box and is computed by the data generation 
procedure instead of the display procedure. It would be wasteful to represent planar polygons by collections 
of infinitesimally small bounding boxes. For the special case of rectangles, the terminal level bounding 
box can represent the polygon exactly if one of its three dimensions is equal to zero and the other two 
coincide with the dimensions of the rectangle. Arbitrary planar polygons can then be represented by the 
intersection of rectangles. Figure 7 shows a triangle represented by the intersection of three rectangles. 
The important feature of this representation is that the display procedure treats all cases the same 
whether they be described by collections of points or collections of polygons.  Data Management Simplicity 
of the display procedure, which is the key to the speed of this algorithm, is gained by transfer- ring 
much of the processing load to the data generation stage. In some cases the data generation can be per- 
formed off line either automatically or through the use of the structure editor. In many cases, however, 
data generation and display must occur concurrently. One example is the automatic expansion of curved 
surfaces into a hierarchical point representation. Subdivision algorithms for bi-parametric surfaces 
[14,15] are natural candidates for the task. We have augmented straight- forward subdivision with routines 
for generating bound- ing boxes for each subpatch and for maintaining the hierarchy at each step. Unnecessary 
processing is minimized by subdividing a patch or subpatch only if its bounding box is pierced. When 
the hierarchy is extended via subdivision, the new branches are retained from one pixel to the next to 
gain the benefits of object space coherence. In a similar fashion procedurally defined objects can be 
expanded into a point representa- tion, although we have not implemented such an expan- sion procedure. 
 Naturally, unlimited expansion of the object description may fill the memory available to the display 
process. Bounding boxes that are generated as a result of subdivision or some other procedure are labeled 
"temporary" when they are created. When the memory Figure 6: A rectangular parallelepiped defined by 
X=±Xb, Y=--+ Yb, and Z=--+ Zb. Figure 7: A triangle represented by the intersec- tion of three bounding 
boxes. becomes full, all "temporary" elements that were not visited during the previous pixel are deleted. 
 Implementation The choice of representation presented here and the attempts to simplify the display 
process are motivated by our desire to produce a technique that can be easily realized in hardware. However, 
two initial ver- sions of the display algorithm have been implemented in software. The original program, 
running on a PDP-11/4O j, incorporates the structure editor as an adjunct to the display routines. Using 
the editor, a detailed polygonal description of the city of Pittsburgh containing over 38,000 terminal 
nodes was created. By using instances of such items as windows on buildings, the number of terminal nodes 
actually stored is less than 600. This database was used to generate the images in Figures 8 i PDP and 
VAX are trademarks of the Digital Equipment Corporation  scenes because the initial stages of the display 
process consist of a logarithmic search through the object description. We have described a method by 
which the entire visibility calculation is reduced to this logarithmic search, both simplifying and improving 
the performance of the display processor. This simplicity is gained through the use of a single form 
of representation that is sufficiently general to accommodate a wide variety of object types. References 
 [1] Clark, J.H. Hierarchical geometric models for visi- ble surface algorithms. CACM ,19-10, October 
1976, pp. 547-554. [2] Reddy, D.R. and Rubin, S., Representation of three-dimensional objects, CMU-CS-78-113, 
Dept of Computer Science, Carnegie-Mellon Univcrsity, April 1978. [3] Brooks, J. et al, An extension 
of the combinatorial geometry technique for modeling vegetation and terrain features, Mathematical Applications 
Group Inc., NTIS AD-782-883, June 1974. [4] Csuri, C. et al, Towards an interactive high visual complexity 
animation system, SIGGRAPH '79 proceedings, August 1979, pp. 289-299. [5] Whitted, T., An improved illumination 
model for shaded display, to appear CACM [6] Kay, Douglas S., Transparency, refraction and ray tracing 
for computer synthesized images, M.S. Thesis, Cornell University, January 1979. [7] Sutherland, I., Sproull, 
R., and Schumacher, R., A characterization of ten hidden surface elimination algorithms. ACM Computing 
Surveys , Vol. 6, No. 1, March 1974, pp. 1-55. [8] Forrest, A.R., On Coons and other methods for thc 
representation of curved surfaces. C~mputer Graphics and Image Processing , vol 1, 1972. [9] Riesenfeld, 
R.F., Applications of b-spline approxi- mation to geometric problem of computer aided design, PhD thesis, 
Syracuse University, 1972. [10] Newell, M., The utilization of procedure models in digital image synthesis, 
PhD Thcsis, Computer Sci- ence, University of Utah, Salt Lake City, Utah, 1975. [11] Rosenfeld, A. and 
Kak, A.C., Digital Picture Pro-cessing ,Academic Press, New York, 1976. [12] Kelly, M.D., Visual identification 
of people by com- puter, AIM-130, PhD Thesis, Computer Science, Stanford University, Stanford, Ca., July 
1970. [13] Price, K. and Reddy, D.R., Matching segments of images, IEEE Trans. on Pattern Analysis and 
Machine Intelligence ,1,1, 1979, pp 110-116. [14] Catmull, E., A subdivision algorithm for computer display 
of curved surfaces, UTEC-CSc-74-133, PhD thesis, Computer Science Dept., Univ. of Utah, Dec. 1974. [15] 
Lane, J.M., Carpenter, L.C., Blinn, J.F., and Whit- ted, T., Scan Line Methods for Displaying Parametrically 
Defined Surfaces. CACM ,23-1, January 1980, pp. 23-34. 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1980</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>807480</article_id>
		<sort_key>117</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-1980</article_publication_date>
		<seq_no>21</seq_no>
		<title><![CDATA[A linear time exact hidden surface algorithm]]></title>
		<page_from>117</page_from>
		<page_to>123</page_to>
		<doi_number>10.1145/800250.807480</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=807480</url>
		<abstract>
			<par><![CDATA[<p>This paper presents a new hidden surface algorithm. Its output is the set of the visible pieces of edges and faces, and is as accurate as the arithmetic precision of the computer. Thus calculating the hidden surfaces for a higher resolution device takes no more time. If the faces are independently and identically distributed, then the execution time is linear in the number of faces. In particular, the execution time does not increase with the depth complexity.</p> <p>This algorithm overlays a grid on the screen whose fineness depends on the number and size of the faces. Edges and faces are sorted into grid cells. Only objects in the same cell can intersect or hide each other. Also, if a face completely covers a cell then nothing behind it in the cell is relevant.</p> <p>Three programs have tested this algorithm. The first verified the variable grid concept on 50,000 intersecting edges. The second verified the linear time, fast speed, and irrelevance of depth complexity for hidden lines on 10,000 spheres. This also tested depth complexities up to 30, and showed that perspective scenes with the farther objects smaller are even faster to calculate. The third verified this for hidden surfaces on 3000 squares.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Algorithms analysis]]></kw>
			<kw><![CDATA[Computational geometry]]></kw>
			<kw><![CDATA[Computer graphics]]></kw>
			<kw><![CDATA[Geometric intersections]]></kw>
			<kw><![CDATA[Hidden line]]></kw>
			<kw><![CDATA[Hidden surface]]></kw>
			<kw><![CDATA[Molecular models]]></kw>
			<kw><![CDATA[Space-filling]]></kw>
			<kw><![CDATA[Variable grid]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Hidden line/surface removal</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.1.2</cat_node>
				<descriptor>Analysis of algorithms</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003752.10010061.10010063</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures->Computational geometry</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010148.10010149</concept_id>
				<concept_desc>CCS->Computing methodologies->Symbolic and algebraic manipulation->Symbolic and algebraic algorithms</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10003809</concept_id>
				<concept_desc>CCS->Theory of computation->Design and analysis of algorithms</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP14033372</person_id>
				<author_profile_id><![CDATA[81100065460]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Wm]]></first_name>
				<middle_name><![CDATA[Randolph]]></middle_name>
				<last_name><![CDATA[Franklin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Electrical and Systems Engineering Dept., Rensselaer Polytechnic Institute, Troy, NY]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Bentley, J.L., Stanat, D.F., and Williams, E.H., Jr. The Complexity of finding fixed radius near neighbors. Info. Proc. Lett. 6, 6 (Dec. 1977), 209-212.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Bentley, J.L., Ottmann, T.A. Algorithms for reporting and counting geometric intersections. IEEE T. Comput. C-28, 9 (Sept. 1979), 643-647.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>358815</ref_obj_id>
				<ref_obj_pid>358808</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Blinn, J.F., Carpenter, L.C., Lane, J.M., and Whitted, T. Scan Line methods for displaying parametrically defined surfaces. Comm. ACM 23, 1 (Jan. 1980), 23-24.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>360353</ref_obj_id>
				<ref_obj_pid>360349</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Blinn, J.F., Newell, M.E. Texture and reflection in computer generated images. Comm. ACM 19, 10 (Oct. 1976), 542-547.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>563901</ref_obj_id>
				<ref_obj_pid>965141</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Crow, F.C. Shadow algorithms for computer graphics. Computer Graphics 11, 2 (Summer 1977), 242-248.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Franklin, W.R. VIEWPLOT summary, program logic manual, and user's manual. Harvard Lab for Computer Graphics, (July, Dec. 1976).]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Franklin, W.R. Combinatorics of hidden surface algorithms. Harvard University Center for Research in Computing Technology, TR12-78, (June 1978).]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Franklin, W.R. An Exact hidden sphere algorithm that operates in linear time. Computer Graphics and Image Processing, to appear.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Knowlton, K., and Cherry, L. ATOMS - a 3-D opaque molecular system. Computers and Chemistry 1, 3 (1977), 161-166.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807439</ref_obj_id>
				<ref_obj_pid>965103</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Max, N.L. ATOMLLL - ATOMS with shading and highlights. Computer Graphics 13, 2 (Aug. 1979), 165-173.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_obj_id>5532</ref_obj_id>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Newman, W., and Sproull, R.F. Principles of Interactive Computer Graphics, 2nd edition. McGraw-Hill, 1979.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_obj_id>63448</ref_obj_id>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Rogers, D.F. and Adams, J.A. Mathematical Elements for Computer Graphics. McGraw-Hill, 1976.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_obj_id>356626</ref_obj_id>
				<ref_obj_pid>356625</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Sutherland, I.E., Sproull, R.F., and Schumacker, R.A. A Characterization of ten hidden surface algorithms. Computing Surveys 6, 1 (Mar. 1974), 1-55.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_obj_id>563896</ref_obj_id>
				<ref_obj_pid>965141</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Weiler, K., and Atherton, P. Hidden surface removal using polygon area sorting. Computer Graphics 11, 2 (Summer 1977), 214-222.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 A LINEAR TIME EXACT HIDDEN SURFACE ALGORITHM Wm. Randolph Franklin Electrical and Systems Engineering 
Dept. Rensselaer Polytechnic Institute Troy, NY, 12181  ABSTRACT LIST OF SYMBOLS This paper presents 
a new hidden surface algo- rithm. Its output is the set of the visible pieces of edges and faces, and 
is as accurate as the arith- metic precision of the computer. Thus calculating the hidden surfaces for 
a higher resolution device takes no more time. If the faces are independently and identically distributed, 
then the execution time is linear in the number of faces. In par- ticular, the execution time does not 
increase with the depth complexity. This algorithm overlays a grid on the screen whose finenessdepends 
on the number and size of the faces. Edges and faces are sorted into grid cells. Only objects in the 
same cell can inter-sect or hide each other. Also, if a face com-pletely covers a cell then nothing behind 
it in the cell is relevant. Three programs have tested this algorithm. The first verified the variable 
grid concept on 50,000 intersecting edges. The second verified the linear time, fast speed, and irrelevance 
of depth complexity for hidden lines on I0,000 spheres. This also tested depth complexities up to 30, 
and showed that perspective scenes with the farther objects smaller are even faster to calculate. The 
third verified this for hidden surfaces on 3000 squares. Keywords: hidden surface, hidden line, com-putational 
geometry, geometric intersections, variable grid, computer graphics, molecular mod- els, space-fillinq, 
algorithms analysis CR categories: 8.2, 3.74, 5.31, 3.13. * This material is based upon work supported 
by the National Science Foundation under grant no. ENG-7908139. Permission to copy without fee all or 
part of this material is granted provided that the copies are not made or distributed for direct commercial 
advantage, the ACM copyright notice and the title of the publication and its date appear, and notice 
is given that copying is by permission of the Association for Computing Machinery. To copy otherwise, 
or to republish, requires a fee and/or specific permission. 01980 ACM 0-89791-021-4/80/0700-0117 $00.75 
1. F(N) = e(G(N)) means that for all O<a<b, there exists No such that N>No => aG(N) _< F(N) _< bG(N). 
 2. F(N)>8(G(N)) and G(N) <@(F(N)) mean that for all c > O, there exists N such that N > N o => F(N) 
>c G(N). o 3. F(N) > @(G(N)), equivalently G(N) < O(F(N)), meant F(N) = O(G(N)) or F(N) > 9(G(N)). 
 Informally, 1 means Fgrows asymptotically as fast as G, 2 means F grows faster, and 3 means F grows 
at least as fast. 4. Let Lx] = the largest integer < x (trunca-tion). 5. Let B be the fineness of the 
variable grid, that is the number of cells along one side of the screen. 6. Let C be an unspecified 
constant. 7. Let D be the depth complexity of the scene. 8. Let E be an edge. 9. Let F be a face. 
 I0. Let F b be the blocking face of a cell. II. Let G be a grid cell. 12. Let L be the length of a 
projected edge (assuming the screen is one-by-one).  13. Let N be the number of edges in the scene. 
 14. Let P be a point. 15. Let R be either a region of the screen cor- responding to the visible part 
of a face, or the radius for a projected sphere.   INTRODUCTION The hidden surface problem has been 
actively researched for 15 years. For an excellent summary as of 7 years ago, see Su~herland [13]. Lately, 
attention has turned to photographic quality out-put, Blinn [3,4], Crow [5], Newell [4], and Whitted 
[3]. The algorithm presented here falls in both object and image space in Sutherland's classification. 
Although it does many calcula- tions on the display screen, in contrast to imaqe space algorithms, its 
output is exact, and its time does not increase with depth complexity (the average number of projected 
faces on each point on the screen). Algorithms which compare all faces that cross a scan line take at 
least quad-ratic time in the depth complexity. In contrast to other object space algorithms, it takes 
linear time on a large class of input scenes. Intuitively, the reason that the depth com-plexity, which 
is related to the size of the faces, does not matter, is as follows: Either the faces are large or they 
are small. If they are large then they overlap so much that most of them are totally hidden. After these 
are quickly detected and deleted, the resulting problem is much simpler. On the other hand, if the faces 
are small, then each face overlaps few, if any, other faces. This, too, can be detected efficiently. 
Therefore, the size of the faces does not slow the algorithm. This algorithm is similar to that of Weiler 
and Atherton [14], except that it is faster, but does not yet texture the faces. Since the output of 
this algorithm has meaning, and is not just a set of pixel intensities, it could also accommo-date shadows 
and textures. This algorithm is similar in spirit to the computational geometry ideas of Bentley [12]. 
On the surface, this algorithm is siieilar to Warnock's. However, it has the following differences: I. 
Here the one level grid is a temporary scaf- folding and the visible pieces are returned whole, while 
in Warnock's algorithm the vis- ible pieces are returned cut up arbitrarily by the hierarchical grid. 
2. This algorithm produces exact results while Warnock's algorithm stops subdividing at about half a 
pixel, and doubling its resolu-tion doubles its time. This ~gorithm takes worse than linear time on scenes 
where the faces' positions are correlated so as to make them all intersect each other with each of them 
partly visible. However here the com-plexity of the output is >@(N), so the algorithm could not possibly 
be linear. Knowlton L9] and Max [I0] have a good hidden surface'algorithm for shading with space filling 
molecular models. An earlier version of this linear algorithm developed and implemented by the author 
in 1972-1976 is described in [6,7]. ASSUMPTIONS The speed of this algorithm depends on several assumptions 
about how the nature of the scene changes as N increases. a. The projected edges in any given scene 
are all the same length. This can be weakened to al-low the ratio of the average edge lengt h to the 
shortest length to be merely bounded.above since then all edges could be cut into pieces as long as the 
shortest edge without changing N by more than a constant factor. It might seem that this assumption rules 
out perspectively projected scenes since the fur- ther edges are smaller. However, in practice these 
scenes are faster to calculate, not slower. This is because in this case a few big faces in front totally 
hide many small faces in the rear, and this algorithm can quickly eliminate totally hidden faces. On 
the other hand, if the closer edges were smaller, then this algorithm would slow down. b. The faces' 
and edges' positions are independ-ently and uniformly distributed. This is never exactly true since otherwise 
the prob- ability of two faCes meeting at an edge would be zero. However, these effects become relatively 
smaller as N grows. Even if the 3-D scene is highly correlated, as in figure 3, the projected scene is 
less correlated. Finally, correlation of distant objects is irrelevant since it doesn't affect their 
prob-ability of intersecting, which remains zero. c. The objects in big scenes are "similar" to those 
in small scenes in the sense that the same number of edges meet at a vertex, that faces do not get longer 
and thinner, and so on. This appears to hold for realistic scenes. A scene of N faces with length one 
and width I/N would be very slow to calculate. d. Border effects at the edge of the screen are ignored. 
They become progressively less important as the objects get smaller, unless the depth complexity is very 
large, say I00.  A NAIVE ALGORITHM The new algorithm is a refinement on the fol- lowing naive algorithm 
operating in time T ~ 8(N2): I. Intersect all the projected edges pair by pair, to cut each edge into 
segments. Each segment is completely visible or else completely hidden. 2. Compare each segment against 
all the faces to see if it is visible. 3. Draw the visible segments and use them to di-vide the screen 
into regions, each corre- sponding to a visible part of a face. 4. Compare each region against all the 
faces to see which it corresponds to and shade it accordingly.  Note that not only any algorithm that 
compares all the edges will be too slow (i.e., worse than linear), but any algorithm that finds all the 
edge segments is too slow since the number of segments is @(N2L 2 + N) which can be > @(N). For example, 
let the scene be a cubical array of Cubes. If there are K 3 cubes, each of side I/K, then H=I2K 3 and 
L=I/K. so L=o(N-I/3). Then there are o(N 4/3) line segments, and testing their visibility takes time 
T=e(N7/3), which is worse than quadratic.  DATA STRUCTURES This algorithm has the following logical 
data structures: I. The array of faces. 2. The array of edges. 3. A B x B grid of square cells overlaid 
on the projected scene. Each cell has the following three elements which are initially empty:  3a. The 
number of the closest blocking face, F b, if any, of this cell. Fb'S projection com- pletely covers the 
cell so that nothing behind can be seen. 3b. The set of faces whose projections intersect the cell, and 
which are in front of F b. 3c. The set of edges whose projections intersect the cell, and which are in 
front of F b- 4. A set of pairs of intersecting edges. This set contains all the visible, and some of 
the hidden, intersections. 5. The set of visible line segments. 6. The set of visible regions, R. Each 
R is a connected visible part of one face.   THE LINEAR ALGORITHM I. Project and scale the scene to 
fit a 1 by 1 square on the plotter screen. These techniques are well known. They may be found in the 
text- books by Newman and Sproull [II], and Rogers and Adams [12]. 2. Let B = I~ rain (V~, L'I~, for 
0 <c <l. c, which is an unspecified constant, is used to fine-tune the algorithm. Fine-tuning is dis- 
cussed later. 3. Overlay a B by B grid of cells onto the scene 4. Initialize the grid cell data structures. 
 5. Repeat for each projected face F:  5a. Determine which cells, G, F falls partly or wholly in. This 
is not done by comparing each of the B 2 cells with F. Instead, the minimal box of cells enclosing F 
gives a good approximation. A more accurate determ-ination can be made by considering F's edges. If extra 
cells are included then the algor- ithm will later run slower but will still give the correct output. 
 5b. Repeat the following for each G that contains F: i. If G has a blocking face, F b, determine  
whether, throughout G, F b is always in front of F. False negatives are alright at this step. Thus a 
convenient way is to compare the distance of F from the viewpoint at the 4 corners of G with the If9 
distance of F b at these points. Since the faces are convex and do not inter-penetrate, if F is behind 
F b at those 4 point, it is always behind F b. (The con-verse is not always true.) If F is behind F b, 
do not consider this G further with F and go back to (i) to process the next G. Figure I: I0,000 spheres 
before and after removing the hidden lines ii. Otherwise, determine whether F is itself a blocking face 
of G. This is true if and only if G's 4 corners are inside F. If F does cover G, then update F b to be 
F. iii. Otherwise add F to the set of faces in G. 5c. If F is not added to any G, then to save space 
delete it and its edges from the arrays since it is certainly invisible. If there are many big faces, 
most of them may get deleted at this point. 6. Repeat for each cell, G: 6a. Compare the blocking face, 
Fb, against all the faces on G~s set. Delete from the set any that are behind F b within G. The method 
of 5a(i) can be used. The reason that there may be faces to delete is that F b may have been changed 
as the faces were processed. Thus a face, F, may have been added to G's list at a time when F was in 
front of the current F b. Then later F b was replaced with a closer face that was in front of F. F would 
be deleted in this step. Although we could clean out the list every time that we updated Fb, doing it 
all at once is faster. The initial check in 5a(i) was done to save space. 7. Repeat for each projected 
edge, E: 7a. Determine which cells, G, E passes through. Repeat for each G: i. Test whether E is behind 
F b. Do this by clipping E at the borders of G to E' and then testing whether the end-points of E' are 
both behind F b. ii. If not, then add E to the set of edges in G. Note that we add the complete E to 
the list, not its clipped version. 7b. If E is not added to any G, then delete it from the array of edges, 
since it is certainly invisible. However, note that a face may be partly visible even though none of 
its edges are. 8. Repeat for each cell, G: 8a. Repeat for each pair of edges, E l and E 2 in G's set 
of edges: i. Test whether they intersect. ii. If so, test whether the intersection is in G. (This catches 
duplications caused by the fact that the same pair of edges may pass through several cells). iii. If 
so, then add E l to the list of edges intersecting E2, and E 2 to the list of E l However, if only the 
visible lines are wanted, and not the visible surfaces, do the following steps instead of (iii): iv. 
Determine which one of E l and E 2 is in front of the other in 3-D at the point of intersection. Without 
loss of gener-ality, let it be E l . Figure 2:1,000 spheres in perspeclive, before and after removing 
hidden lines v. Add the intersection point to only E2's list. This is because the visibility of E l is 
not changed by E2's passing behind it. Nevertheless, we must still cut E l when we are shading if we 
want the visible regions to be properly defined. 9. Repeat for each edge, E: 9a. Sort the intersection 
points of E along E. A fast method that works regardless of E's slope and is stable against small floating-point 
errors in coordinates is this: i. Let the intersections be (X a, Ya ). ii. Sort them by ABS(X a) + ABS(Ya). 
9b. Use the sorted points to split E into seg-ments, S. An edge with no intersections forms one segment 
(unless it was previously deleted). 9c. Repeat for each segment, S: i. Find its midpoint, P. ii. Find 
which cell, G, contains P. iii. Compare P against all the faces, F, in G to see if any hide P. F hides 
P if it satisfies 2 conditions: P is behind the 3-D plane of F (possibly extended) and the projected 
P is inside the projected F. iv. If P is visible, then so is S, so draw S, and add it to the set of 
visible segments. I0. Determine the regions, R, of the planar graph formed by the visible segments, 
using standard algorithms. II. Repeat for each region, R: lla. Find a point, P, in it. The centroid will 
suffice except for some cases when R is concave. llb. Find which cell, G, contains P. llc. Compare P 
against the faces, F, in G. Find the closest of those faces whose projections contain P. The distance 
is measured by taking the length of a ray from the viewpoint towards P, possibly extended, until the 
3-D plane of F. Note that in general the ordering of 2 faces in G will depend on where P is, so it is 
impossible to presort them by dis- tance. lld. R corresponds to a visible part of the closest face, F, 
so shade R accordingly. If P does not fall in any face, then R corresponds to background, and can be 
appropriately shaded. There may be more than one region per face. Figure 3:1,470 spheres in a regular 
arrangement, with hidden lines removed TIMING There are 2 cases to be considered because of the definition 
of B = Lc min (v1~, L -I)] . = c min (v'~, L -I) since the floor function is asymptotically unim-portant. 
Case I: L ~ O(N -I/2) soB =-c . L Since the cell size is a constant fraction of the face size as N increases, 
the probability that a given face will completely cover a cell that it partly covers is a constant, say 
p. If the faces are square with sides parallel to the grid, then (c-112 P = \c+l] , c Z 1 Now as the 
number of faces in the cell increases to infinity, the expected number of the first blocking face is 
co q = E i p (l-p) i-I i=l l P c+l ~2 Thus, and this is crucial to the algorithm, al-though the expected 
number of faces per cell may grow to infinity, the expected number after faces hidden by the blocking 
face have been removed is bounded above by q. Let r = the expected number of cells a face falls in. Then 
r = (c+l) 2. number of faces Let s = N 121 Then the number of faces per cell before deletions Case 
2: L < B(N -I/2) is rsN B 2 of which q faces per cell are not deleted. Thus the fraction of faces left 
is qB 2 (c~l) 2 1 rsN = sL~N " Since the edges are distributed in 3-D as the faces are, this is also 
the fraction of edge seg- ments that will not be deleted for falling behind a blocking face. Let u = 
average number of cells an edge falls in = c+l . Then the average number of edges per cell is uN B 
2 Then the average number of edges per cell left after deletions is u N : (c+l I rsN B 2 rs (c-l) s 
 again a constant. Then intersecting the edges in the cell, pair by pair, takes constant time per cell. 
The total time is B 2. Since each cell has a constant average number of edges in it, the number of intersections 
found is 8(B 2) so the number of segments the edges are cut into is @(N + B2). Testing a segment for 
visibility takes time proportional to the number of faces in the cell that that segment's center falls 
in, which is constant. Thus the total time for the hidden line part of the algorithm, obtained by summing 
times for the operations given above, and adding e(N) for bookkeeping is T : B(N + B 2) : e(N) Thus 
the hidden line algorithm is linear. For the hidden surface algorithm, the number of visible segments 
is ~ the number of segments = e(N + B 2) = e(N). So the number of regions on the plotter screen is e(N). 
Determining them takes an expected time of 8(N). Finding which face a region corresponds to takes time 
proportional to the number of faces in the cell being used for that region, which is constant. Thus, 
the total time to identify the regions is O(N). The analysis is similar to Case 1 and gives the same 
result. Therefore, the complete hidden surface algorithm takes time T-- e(N) in either case.  IMPLEMENTATION 
 Three different programs have been written to test various aspects of this algorithm. All the implementations 
were on a Prime 500 midicomputer with a 16 bit wordlength and l MB of memory. The Prime performs a single 
precision floating multi- ply in about 5 microseconds. The results were plotted either on an Imlac or 
an IBM 3277 graphics attachment. The first was designed to test the concept of a variable grid. It calculated 
the intersec- tions among random edges. It was tested with different numbers of edges, edge lengths and 
grid sizes. For example, with lO,O00 edges of lengths about O.Ol (on a l by l square), and a lO0 by lO0 
grid, it took only 77 seconds to find all 1814 intersections out of the 50 million possibilities. The 
number of (edge,cell) pairs was 19931. The optimal grid size was determined experimentally, and within 
a factor of 2 made little difference. The algorithm behaved as expected as N and L varied, except that 
the optimal grid size was hard to predict since it depends on the relative speeds of various parts of 
the program. The second implementation, SPHERES, is de- scribed in detail in Franklin [8]. It uses the 
same concepts, but is a different algorithm designed to calculate hidden lines for scenes com- posed 
of spheres, as in a molecular model. SPHERES has been tested at depth complexities up to 30, and the 
time for fixed N even drops slightly as D increases at this point. For D = lO, SPHERES has been tested 
for N up to lO,O00. This case took only 383 seconds to calculate. Figure l shows lO000spheres packed 
lO deep before and after hidden lines are removed. Figure 2 shows lO00 spheres with D = lO in a perspective 
projection before and after. This case takes I/3 less time than N = lO00, D = lO with fixed R. Figure 
3 shows the hidden lines removed from a regular array of 1970 spheres. Table l shows how T varies with 
N if D = lO. If N is fixed while D varies from .l to 30 (by changing R), T is largest for D = 5, and 
declines slowly as D increases. The third implementation removes hidden sur- faces for random squares. 
Figure 4 shows 300 squares with the visible surfaces crosshatched at an angle proportional to their distance, 
This has been tested up to N = 3000 and table 2 shows that the time is still linear. This last case has 
D = 77. This algorithm is now being extended to hier-archical or rotating scenes and is being incorpo- 
rated into a general 3-D object manipulation package. EXTENSIONS It would be worthwhile to collect statistics 
on a scene before processing to determine the op- timal B. If the scene is inhomogeneous, a hier- archical 
grid may be faster, although there exist sequences of scenes that still execute slowly. ACKNOWLEDGEMENTS 
Leong Shin Loong and Abel Shi Lo, while Mechanical Engineering students at RPI, helped with these implementations. 
Their assistance is gratefully appreciated.  REFERENCES I. Bentley, J.L., Stanat, D.F., and Williams, 
E.H., Jr. The Complexi ty of finding fixed radius near neighbors. Info. Proc. Lett. 6, 6 (Dec. 1977), 
209-212 2. Bentley, J.L., Ottmann, T.A. Algorithms for reporting and counting geometric intersec-tions. 
IEEE T. Comput C-28, 9 (Sept. 1979),643-647. 3. Blinn, J.F., Carpenter, L.C., Lane, J.M., and Whitted, 
T. Scan Line methods for dis- playing parametrically defined surfaces. Comm. ACM 23, 1 (Jan. 1980), 23-24. 
 4. Blinn, J.F., Newell, M.E. Texture and reflection in computer generated images. Comm. ACM 19, I0 
(Oct. 1976), 542-547. 5. Crow, F.C. Shadow algorithms for computer graphics. Computer Graphics II, 2 
(Summer 1977), 242-248. 6. Franklin, W.R. VIEWPLOT summary, program logic manual, and user's manual. 
Harvard Lab for Computer Graphics, (July, Dec. 1976). 7. Franklin, W.R. Combinatorics of hidden sur- 
face algorithms. Harvard University Center for Research in Computing Technology, TR12-78, (June 1978). 
 8. Franklin, W.R. An Exact hidden sphere algorithm that operates in linear time. Computer Graphics and 
Imaqe Processing, to appear. 9. Knowlton, K., and Cherry, L. ATOMS-a 3-D opaque molecular system. Computers 
and Chemistry I, 3 (1977), 161-166.  I0. Max, N.L. ATOMLLL -ATOMS with shading and highlights. Computer 
Graphics 13, 2 (Aug. 1979), 165-173. II. Newman, W., and Sproull, R.F. Principles of Interactive Computer 
Graphics, 2nd edition. McGraw-Hill, 1979. 12. Rogers, D.F. and Adams, J.A. Mathematical Elements for 
Computer Graphics. McGraw-Hill, 1976. 13. Sutherland, I.E., Sproull, R.F., and Schumacker, R.A. A Characterization 
of ten hidden surface algorithms. Computin 9 Surveys 6, 1 (Mar. 1974), 1-55. 14. Weiler, K., and Atherton, 
P. Hidden surface removal using polygon area sorting. Computer Graphics II, 2 (Summer 1977), 214-222. 
 Table ~ : Times for hidden spheres calculations for D = I0 N R B T 30 0.326 7 4.1 I00 0.178 13 5.6 
300 0.03 23 10.3 1,000 0.0564 42 31.1 3,000 0.0326 72 97.5 I0,000 0.0178 142 383. Table 2: Times for 
hidden surface calculations for squares N L B D T 1,000 0.16 12 25.6 527 3,000 0.16 12 76.8 1792 Figure 
4:300 squares with visible surfaces shaded  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1980</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>807481</article_id>
		<sort_key>124</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-1980</article_publication_date>
		<seq_no>22</seq_no>
		<title><![CDATA[On visible surface generation by a priori tree structures]]></title>
		<page_from>124</page_from>
		<page_to>133</page_to>
		<doi_number>10.1145/800250.807481</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=807481</url>
		<abstract>
			<par><![CDATA[<p>This paper describes a new algorithm for solving the hidden surface (or line) problem, to more rapidly generate realistic images of 3-D scenes composed of polygons, and presents the development of theoretical foundations in the area as well as additional related algorithms. As in many applications the environment to be displayed consists of polygons many of whose relative geometric relations are static, we attempt to capitalize on this by preprocessing the environment's database so as to decrease the run-time computations required to generate a scene. This preprocessing is based on generating a &#8220;binary space partitioning&#8221; tree whose in order traversal of visibility priority at run-time will produce a linear order, dependent upon the viewing position, on (parts of) the polygons, which can then be used to easily solve the hidden surface problem. In the application where the entire environment is static with only the viewing-position changing, as is common in simulation, the results presented will be sufficient to solve completely the hidden surface problem.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Visible line/surface algorithms</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Hidden line/surface removal</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372.10010377</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering->Visibility</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Performance</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP39072004</person_id>
				<author_profile_id><![CDATA[81339500019]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Henry]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Fuchs]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of North Carolina at Chapel Hill]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P310588</person_id>
				<author_profile_id><![CDATA[81100539349]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Zvi]]></first_name>
				<middle_name><![CDATA[M.]]></middle_name>
				<last_name><![CDATA[Kedem]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Texas at Dallas]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P33653</person_id>
				<author_profile_id><![CDATA[81100625208]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Bruce]]></first_name>
				<middle_name><![CDATA[F.]]></middle_name>
				<last_name><![CDATA[Naylor]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Texas at Dallas]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Berman, G. and Fryer, K.D. Introduction to Combinatorics, (1972) Academic Press.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>802893</ref_obj_id>
				<ref_obj_pid>800090</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Fuchs, H. and Johnson, B. "An Expandable Multiprocessor Architecture for Video Graphics" Proc. 6th Annual Symp. on Computer Architecture, (1979) pp 58-67]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Schumaker, R.A., Brand, P., Gilliland, M. and Sharp, W. "Study for Applying Computer-Generated Images to Visual Simulation," AFHRL-TR-69-14, U.S. Air Force Human Resources Laboratory (1969)]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>356626</ref_obj_id>
				<ref_obj_pid>356625</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Sutherland, I.E., Sproull, R. F. and Schumaker, R.A. "A Characterization of Ten Hidden-Surface Algorithms", (1974) ACM Computing Surveys, 6 (1): 1-55]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 ON VISIBLE SURFACE GENERATION BY A PRIOri TREE STRUCTURES* Henry Fuchs University of North Carolina 
at Chapel Hill Zvi M. Kedem The University of Texas at Dallas Bruce F. Naylor The University of Texas 
at Dallas ABST£ACT This paper describes a new algorithm for solving the hidden surface (or line) 
problem, to more rapidly generate realistic images of 3-D scenes composed of polygons, and presents 
the development of theoretical foundations in the area as well as additional related algorithms. As 
 in many applications the environment to be displayed consists of polygons many of whose relative geometric 
relations are static, we attempt to capitalize on this by pre processing tile environment,s database 
so as to decrease the run-time computations required to generate a scene. This preprocessing is based 
on generating a "ninary space partitioning" tree whose inorder traversal of visibility priority at 
run-time will produce a lineaL" order, dependent upon the viewing position, on (parts of) the polygons, 
which can then be used to easily solve the hidden surfac6 problem. In the application where the entire 
environment is static with only the viewing-position changing, as is common in simulation, the results 
presented will be safficient to solve completely tlae llidden surface proulem. ;_N~a~OUCZZON One of 
the long-term goals of computer graphics has been, and continues to be, the rapid, possibly real-time 
generation of £ealistic images of simulated 3-D environments. "Real-time," in current practice, has come 
to mean creating an image in 1/30 of a second-- fast enough to continually generate images on a video 
monitor. With this fast image generation, there is no aiscernable delay between specifying parameters 
zor an image (using knobs, switches, or cockpit controls) and the *This research was partially supported 
by NSF under Grants MCS79-00168 and MC579- 02593, and was zacilitated by the use of Theory Net (NSF Grant 
MCS78-0|689). Permission to copy without fee all or part of this mterial is granted provided that the 
copies are not made or distributed for direct co~®~erclal advantage, the A(~4 copyright notice and the 
title of the publication and its date appear, and notice is given that copying is by permission of the 
Association for Computlng Machinery. To copy otherwise, or to republishj requires a fee and/or specific 
permission. 01980 ACM 0-89791-021-4/80/0700-0124 $00.75 image's appearance on the monitor's screen. Systems 
which can achieve this kind of performance are currently so expensive ($1M and up) that very few users 
can afford them. Users with more modest budgets have to be content with severely more limited performance--either 
a lower quality image (,wire frame" instead of solid-object modeling) or slower interaction (a time lag 
of several seconds to several minutes for a solid-object image).  PROBLEM STATEMENT The problem to 
be solved is: Given I. a data oase describing a 3-D environment in terms of, say, a few thousands tiles 
(polygons) describing the surfaces of the various objects in the environment, one or more light sources 
and 2. the (simulated) viewing position, orientation , and field of view, Generate a color video image 
of the environment as it would appear from the given viewing position and orientation. This image generation 
task consists, broadly, of the following three steps: I. transforming points into the image space, 
2. clipping away polygons outside the field of view,  3. generating the image from the polygons that 
remain. Generating the image consists of determining the proper color (intensities of red, green, and 
Olue) for each of perhaps 250,000 picture elements (approximately 500 rows of dots, with 500 dots in 
each row). For each picture element  ("pixel"), a) find the polygon closest to the viewing position. 
(This will be the visible polygon at this pixel, the  polygon which obstructs all others.) D) given 
tne visible polygon, determine the proper color for the pi~el by evaluating see, e.g., 1979) . a lighting 
(Newman model and formula, Sproull, PROPOSZD ~OLUTIO~ Since current moderately-priced ($~0- 80k) l eal-time 
line-drawing systems (e.g. Evans and Sutherland Picture System 2, ~ector General ~odel 3~0~) can easil~ 
perform steps 1 and 2, we shall concentrate on solutions to step 3. New solutions to this remaining step 
could then ~e combined with already available solutions to produce a complete system. Further, we believe 
step 3b can be effectively solved by distributing the individual pixel calculations among many small 
processors (Fuchs and Johnson, 1979). We thUS concentrate in this paper on step 3a, determinin 9 the 
visible polygon at each pixel. We propose an alternative solution to an approach first utilized a decade 
ago (Scaumaker et al., 19o9) nut due to a few difficulties, not widely exploited. The general approac~ 
is named on the observation that in a wide variety of applications many images are generated of the same 
environment with only a caange in the viewing position ancl orientation, but no change in the environment. 
For example, pilots in a simulator may practice many dit£erent landings at the same airport, with eac,l 
landing generating thousands of new images. Simiiarl y, an architect may "walk" thro,lgn a newly designed 
house or housing development; a bioc,~emist may rotate OK move a~out a complicated protein molecule. 
To take advantage ol such statlc environments, the data uase is ~eprocessed once (for all time, or until 
the data base is changed) before any images ale generated, in this pre processing stage, certain geometric 
relationJaips are extracted which can then be used to speeu up the visible polygon determination for 
each pi~el, for all possi~ le images. It is important to note that although the developmeLt hece is 
given oflly rigi~ oojects and environments, these concepts can be e xtende~l to handle environments wita 
some moving oojects. SOLUTION OVERVIEW In ouder to detecmine the visible surface at each pixel, traditionally 
tile distance from the viewillg position to each polygon whic~ maps onto that pixel is calculated. Most 
methods attempt to minimize the number of polygons to be ~o  considered. Our approach eliminates these 
distance calculations entirely. Rather, it transforms tae polygonal data base (splitting polygons when 
necessary) into a binary tree which can be traversed at image generation time to yield a vis~il~z priorit 
z value for each polygon. These visibility priorities are assigned in such a way that at each pixel the 
closest polygon to the viewing position will be the one with the highest wisioility priority. As we shall 
see, the visibility priorities are a function of the viewing position; they remain constant ~or all pixels 
in every image generated from the same viewing position. In cases for which these visibility priority 
numuers cannot De assigned to the original polygons (see, e.g., fig. 6) and some polygons need to be 
split, the splitting is done only once --during the p~eprocessing phase --never at image generation time. 
 ~hE~ROCESSING PHASE Let us now consider the set of polygons P = {pl,P2,°..,pn} which define the 3-D 
environment. Choose an arbitrary (ior noN) polygon Pk from this set. We note that the plane in-which 
this polygon lies partitions the rest of 3-space into two half-spaces--call these S k and S~. The two 
half-spaces are identified with the positive and negative sides of the polygon pk o If Pk was defined 
with a "front" slae, then that side is considered as tile positive one; otherwise, one of the sides is 
arDitrarily caosen at this time to be the positive side. What can we say about visibility priorities 
of these polygons? We know that if the viewing position is in one hall-space, say in Sk, that no polygon 
within S:K can obstruct either polygon Pk or any polygon in Sk(see figure. 1). Therefore, ~e split each 
of the polygons in P -{Pk} along the plane of Pk' putting tile polygons (or parts of t~em) waich lie 
in S k into one set and polygons which lie in S T into another set. ( Polygons coplanar with p~ can 
be put into eitaer set.) We can represent the results of this splitting process by a Uinary tree (we'll 
call it a Binary Space ?artitioning, or "BSP" tree) in which the root contains Pk and each branch,s subtree 
contains the set of polygons associated with one of the half-spaces (Fig. 2). We next consider one of 
the two new sets of polygons, say the one in S k. We remove a polygon, say p~ and split the remaining 
polygons in $ along the plane of Pi" putting those polygons (or parts tiler~of) lying on the positive 
side in one 125 set (Sk,j) and those lying on the negative side ±n anotaer set (Sk T)- The overall tree 
after this step is ~own in Fig. 3. To complete the construction of the BSP tree we continue splitting 
sets until no non-null sets remain. The entire preprocessin~ phase, then, consists of transforming the 
entire polygonal data base into a BSP tree by the following recursive procedure (stated in a simple pseudo-PASCAL): 
 PROC Make_tree (pl:polygon_list) : tree; BEGIN k=Select_polygon (pl) ; pos_list := null; neg_list 
:= null; /* pos ~efers to positive parts neg refers to negative parts ~/ FOR i := I TO Size_of(pl) DO 
 BEGIN IF i <> k THEN BEGIN Split_polygon (p1[i], pick], pos_parts, neg_par ts) ; Add (pos_parts, 
pos_list) ; Add (neg_parts, neg_list) END END;  RETURN Combine_tree (MaKe_tree (pos_list) , p1 (k) 
, [take_tree (neg_list))  END;  We note again that this process i~ only performed once for all possiul 
e images from all viewing positions; tae tree remains valid as long as the scene doesn't change. IMAGE 
GENERATION PHASE  Calculating the visibilit y priorities, once tae viewing position is known, is a variant 
of an in-order traversal of the environment's ~5P tree (traverse one subtree, visit the root, traverse 
the other subtree). We wish, for example, to ~ave an order of traversal that visits the polygons from 
those farthest away to those closest to t~e current viewing position. At any given node, there are two 
possibilities: positive side subtree, node, negative sid~ subtree or negative side suotree, node, positive 
side su~tree. We choose one o£ t~ese two orderings basea on tae relationship o£ the current viewing position 
to the node's L, oiygon. Specifically, we are intecested in the side (positive or negative) ot the node's 
polygon where the current viewing position is located. Let's call the two sides the "containing" side 
and the "other" side. The traversal for a back-to-front ordering is 1) the "other" side, 2) the node, 
and 3) the "containing" side. ( This side-of~ node-polygon determination is, of course, just a chec~ 
of the sign of the Z component of the node polygon's normal vector after the usual transformation to 
the screen coordinate system.) This notion of a traversal may be embodied in at least two different 
ways for visible surface image generation. One alternative is to assign priorities to f, olygons in the 
order that we visit taem. Using the traversal order just described we will get a low-to-high visibility 
priority assignment. These values can then De used within a conventional visible surzace display algorithm 
wherever visibility determinations need to De made. The other obvious alternative, which in fact is the 
one taut we nave implemented, does not assign explicit visibility priority values to polygons but uses 
the t~aversal to drive a "painter's" algorithm which paints onto the screen' s image ouffer each polygon 
as it is encountered in the traversal. Since higher priority polygons are visited later in the traversal 
and thus painter later, they will overwrite any overlapping polygons of lower priority. Tae following 
rec ursive procedure genera tes a visible surface image in the aoove-described manner. PROC Back to 
front(eye:viewing_position; t: BSP_tree) ; I~EGIN iF Not_null (t) THEN IF pos_side_of (root L t],eye) 
 THEN BZGiN pack to front (eye, neg_nranch [ t]); Display_polygon (root [ t]) ; mack to front (eye, 
pos_branch[t]) END ZLSE BEGIN Back to front (eye, i.os branch[ t ]) ; Display_polygon (root[ t]) 
; Back to front (eye, neg_branch£t]) VN9 E~,D Figures ~,5,and 6 illustrate this visible surlace algorithm. 
Since the display used had only one bit per pixel, the procedure Display_polygon painted the interior 
of the polygon the background saade and painted the outline of the polygon in tl~e otaer shade. The 
possiule weakness o£ this approach is that the number of polygons in the tree may increase sharply. (Recall, 
every root polygon splits all crossing polygons in its list in order to put any polygon in one or the 
other of its  subtrees. ) Ke **ave attempted to Limit this increase ~y selecting the root polygon at 
each stage to De the one whose plane splits the minimum number of polygons in its list. Table I indicates 
the performance of the system in limiting the number of polygons in the BSP tree. Figures 7 and 8 show 
~e BSP tree for the environments of Fxgures q and 6, respectively. No. of No. of PolyKons in Fig. no. 
Original PolyKons BSP Tree 4 ii ii 5 72 I00 6 3 5 Table I: Number or' polygons in tree versus original 
data base  We are currently examining a more sopaisticatea strategy for minimizing the number of polygons 
in the BSP tree. In addition to the just-descriued criterion of choosing a node polygon as one that minimizes 
the number of polygons that are split, a second criterion is also considered. This one maximizes the 
number ol "polygon conflicts" eliminat ee. We define a polygon conflict as an occurrence between two 
polygons in one list in whic,~ the plane of one pol~'gon intersects the other polygon. The hope is that 
these eliminated polygon conflicts will reduce the number oL polygons which will need to be cut in t,~e 
descendant suDtrees. ~ore precisely, if P is toe set of polygons, then form the sets S I' S 2 , S for 
each polygon p P as [ollows: 3 S 1 -= [q c P I g is entirely in the positive half space of p} S z [q 
e ~ [ q is intersected by the p~ane of p} S 3 ~ {q e P i q As entirely in the negative half-space of 
p] We define a function ; polygon sj and the plane f(si, sj) = of s i intersect ; otherwise and IO 
 = ~ ~ f(si, sj) Im'n si~Sm s 3.e~n we then select the p such that for Si(P) ,S2(P) ,S 3 (P) tee expresslon 
[Ii,3+ I3,1-(|$21~ weight)] which is maximal. FOHMAL DEVELOPSENT Let us now examine the nature of 
the uinary space-partitioning (BSP) tree more closely. The construction can be carried, in essentially 
identical manner, for any dimension; nonetheless, it is only the three-dimensional version that is of 
major interest to us here. However, it is easier to explain its nature in the two- dimensional setting, 
as the various geometric structures arising can be clearly drawn; thus the discussion of the properties 
of the tree will be presented ~ssuming a t~o-dimensional universe. Nonetheless, we encourage the reader, 
as the next section of the paper is reada to extrapolate the three-dimensional interpretation. In the 
latter portion of the paper, where combinatorial issues are examined, the results wall be given for both 
two and three dimensions, since combinatorial complexity is dimension dependent. We now begin with some 
(sligatly non-standard) terminology° Segment -an oriented closed convex subset of a line, i.e., a finite 
segment, a ray, or a line, with a direction associated with it. Region -a closed convex set of points 
of a plane. (A region is normally defined  as an open connected set.)* Extension of a Segment in a 
Region -given a segment s and a region R, define the extension of s in R to be the intersection of the 
line on which s lies with the region ~, obtaining the segment ~[ Assign to S R the direction induced 
x s (we indicate this by pointing an arrow to the ~ight) Note that a region can be unbounded (a plane) 
"partially bounded" (e.g., a half-plane) , or (completely) bounded (e.g., a finite polygon ). The motivation 
for defining regions and segments in this manner is that in general we have no interest in distinguishing 
between the bounded, partially bounded, and unbounded sets. The 3-space analogies to segments *A set 
is open if there is an "implicit boundary" which is not in the set. Formall~, a set Of points 8 in the 
plane is open if and only if 'V'K6R, e> 0 such that qPy[ix-1|<e => ySR]. k closed set is the complement 
of an open set (if bounded, the set inc£udes the boundary}. Formally, a set of points R in a plane is 
closed iff for every converging sequence -> x, Vn[x SE => x~,It]. 127 and regions are polygons (or alternately, 
regions) and sectors (o~ volumes) respectively. The orientation of the polygons corresponds to the usual 
notion of the front and back sides. We are now ready to examine the general algorithm for construction 
of a lapeled binary space-partitioning tree. Algorithm I: Construction of a (2-space) BSP tree Input 
- a region R and a set of segmentsZ lying in R Output - A BSP Tree Method -call t~e function, BSPT~ 
with R and Z as parameters and Z ÷ ~. Procedure - BSPT ( R:reglon; E:set of segments ) :node Begin 
 If E ~ ~ then besin A choose s c Z and form s R A PartltlOndefinedRandas: Z by s R into Rs 'R~, ERs 
, ZR~ A R s -={ p c R I P c s R or p lles to the right of SA R } A R~ --- { p e R [ P c s R or p 
lles to the left of SAR } Z R =-{ B n R s I B E Z-{s} } s   zz_ -= { S n R~ [ B E Z-{s} } 8 Create 
a new node v leftson(v) :=BSPT( R~,ER~ ) rightson(v) := BSPT (Rs,ER) s label(v) := s R return {v) 
 ~A  Create a leaf label (~) :=R return (~)  End BSPT Let us look at an example before examining 
t~e properties of this algorithm. Let R be a square and ~z{ a,b,c], as in figure 9a. If a is chosen 
first, we get figure 9D which creates figure 9c. If, next, b is R- a chosen before c, the final result 
will appear as in figure 10. A Consider now the set E of segments, which of course lies wholly within 
the original ~. It is easily seen that it partitions R into convex regions (polygons}. Each such region, 
together with its boundary, will be referred to as an area (volume foe 3- space). The set of all the 
areas created Dy the algorithm will be referred to as a tessellation. The areas may be thought of as 
the intersection of half-planes (half- spaces for 3-D) created by th 9 lines on whic~ the elements of 
z (or ~) lie. The purpose of orientation of the segments is to distinguish between the two half- planes. 
The subscripts of each region, generated by algorithm I, indicate the half-planes whose intersection 
forms the region. As an example, refer to figure 11 which is a BSP tree for the tessellation in fig. 
10 where parentheses are used to indicate subscripting of regions. It should be clear by now that the 
algorithm performs a recursive partitioning of the plane Dy the segments lying in it. However, oDserve 
that ~iven a set of segments , that more than one tessellations can be generated by the algorithm depending 
upon the order in which segments are selected. Observe that in fig. 9, had the order of selection been 
c, b, a, fig. 12 would have been produced, which not only looks different f~om fig. 10, but has four 
areas, as opposed to five. Since a tessellation is formed by the extended segments, as opposed to the 
segments themselves and the length of an extended segment is dependent on the size of the region containing 
it at the time it is extended, selecting segments in different orders produces different regions, and 
thus the dependence of the tessellation on the order of selection. It is also possible to have, for 
a given set of segments, more than one tree which describes the same tessellation. Assume that at some 
stage of the construction of the tree, we are examining the region R k and the associated set of segment%ER~ 
{s I, s 2 ..... sin}. If m A U=. Si=iUlS i wlth respect to P~, then e~e~y permutation ~ on i = |,2,.o.,m 
will result in a different subtree, where the  subtcee is generated oy selecting segments in the order 
s~(1) , s~(2), .,,s~ (m ~ Nonetheless, every s~Otree will describe the same tessellation of ~k" Conse 
~uen tl y, there ace distinct trees describing the same tessellation of the original region R. For example, 
in figure 13, either tree specifies the same tessellation. An important special case occurs when the 
initial set of segments is equivalent to the extended set, i.e., u{slseE} = u{~|~e~J. If in addition 
the initial region is a plane, all o~ the elements of z, would De lines. Since extension has no effect, 
the tessellation is fixed oefore the algorithm oegins. We call such a tessellation a maximum tessellation 
because ant set of segments lying on the same set ot lines can produce only tessellations whose areas 
ace the union of the areas o~ the maximum tessellation, as can be ~een by comparing figures 10 and 12 
with fig. I~. it follows that any set of segments has a corresponding maximum tessellation whose cardinal 
ity is the maximum of the number of areas produced by any tessellation resulting from the set. In general, 
the number of different tessellations that can be derived from a set Z i~, in some sense, the complement 
of the number of distinct trees which describe the same tessellations. A BSP tree constructed b¥ algorithm 
I contains nodes labeled with segments and nodes labeled wita areas. The segment nodes are exactly the 
interior nodes of the tree and the "area" nodes are the leaves. The algorithm can be thought of as first 
generating a binary tree composed of only the segment nodes.. There will then be segment nodes which 
have one or two empty sons. {Every node of a binary tree has potentially two sons, left and right. If 
a node does not have one or both sons# we refer to these as "empty sons.") At each empty son, an area 
node is added. The resulting tree is such that all segment nodes have both a left and a right son, either 
of which could be another segment node or an area node. Since binary trees of n nodes have n+1 empty 
sons, it follows that the number of area nodes~is one more than the number of segment nodes, thus a tree 
of 2n-I nodes is needed to represent a tessellation containing n areas. Each subtree of a BSP tree represents 
some region R I in the sense that the union of all the areas represented by the leaves o£ R i equals 
R i (the segments represented by the segment nodes of R i are thus, also included in this union). FOE 
notational purposes we will designate the region represented Dy the entire tree as Ro" This, of course, 
is the original reglon from which the tessellation is formed. The extension of the segment s represented 
 by the coot q of a subtree partitions a region ~i, and the regions represented by the two subtrees 
of ~ are the two half- spaces formed from Ri b~ ~ . If, upon traversing the tree one reaches q, then 
taking the left or right branch of g would have a geometric correspondence to selecting one of these 
two half-spaces. A path in the tree, then, reflects a successive selection of smaller and smaller portions 
of R o. In fact the region represented by a subtree is the intersection of the half-spaces with respect 
to R o formed by the extension of the segments which are on the path to the root of the subtree g {but 
not including q). it immediately follows that the area which is "added" at each empty son is exactly 
the intersection of the half-spaces with respect to R o formed by the extension of segments whose nodes 
are on the path to the son. It is easy to see how a BSP tree can be used to locate which area of the 
tessellation a point lies. Beginning at the coot, determine on which side of a segment the point lies 
and proceed to the son representing the half-space corresponding to that side {points on the line being 
assigned arbitrarily to one of the two half-spaces}. Repetition of this process will generate a path 
to a leaf node that represents the area in which the point lies, thus solving what might be called the 
"location problem" with respect to a tessellation.  B~ Tree use~_~..9.~...,£~¢oc¢~ o~d~ei,q The ability 
of a BSP tree to be used for the generation of a priority ordering is based upon the principle that given 
in which half-space lies the point to which the ordering is relative (usually thought of as the "eye" 
or viewing position), all points in this same half-space will have priority over all points in the other 
mall-space. Although this fact is fairly self evident for half-spaces, it is also true for any two convex 
regions. To obtain a priority ordering from the tree, an inorder traversal is performed. The choice 
of taking the left or right branch of a node q representing segment s is always made in favor of the 
subtree which represents the region that is contained in the same half-space that the viewing position 
is ~n, this half- space having been formed b X ~ with respect to R o. It is easy to see that such a policy 
will result in the first area node to be reached being the one in which the viewing position lies, i.e. 
the solution to the location problem mentioned earlier. Priority is assigned to a node upou backing-up 
from it during the traversal. 129 Thus for each node g, all nodes of the chosen subtree receive higher 
priorities than g, and similarly, all nodes of the remaining subtree obtain a lower priority than q. 
The entire traversal of the tree will then produce a total ordering of the nodes, and this is precisely 
the visibility priority of the elements represented by the nodes. Note that it is reguisite that R, be 
convex to guarantee this property. Since the partitioning of a convex object produces two convex objects, 
the convexity of H implies the sane property for a~l subseguent refinements of ~0 during the construction 
of the tree. Thus all areas are convex which is sufficient to guarantee the existence of a priority ordering 
of the areas. ConDarisoq of Uses of the BSP Tree The first appearance of a BSP tree An the general 
literature gas in Sutherland, st- el. (197~) zeviewlng the work of S~husaker, et aL (1969), although 
the tree gas n@t named and its ~general properties were not developed. The application presented gas 
that in which iavlslble "dividing planes" were introduced to the data base. The method Involved the designer 
of a siuulation scene manually positioning Ucl~sters" such as buildings, tDees, mountains, etco# so that 
vertioal dividing planes could be piaoed between the cluster.s to varying extents. This resulted, in 
terms of a BSP flee, la the generation of a tessellation of~the s~eface by the dividing places which 
are represented by segment nodes# and each cluster mas contained wholly within am area. Thus each cluster 
 corresponded to ordering could an area then node. a be obtained priority on the clusters. &#38;ddltional 
power is available if the tessellation is a maximum tessellation. In this case, it is possible to compute 
off-line the priority ordering for each case of the viewing position being in a different area. This 
follows from the fact that since the areas are formed by a maxiuuu tessellation, it is not possible for 
tgo different points in the sane area to be on different sides of the extension of a segment with respect 
to R 0 {since in a maximum tessellation all segments are egual to their extensions with respect to R~). 
Thus for each area the traversal of the tree is fixed. The Sutherland~ ~., presentation suggests taking 
advantage of this by pre-computing and storing for each area its inherent priority ordering on the clusters 
{since the dividing planes are not part of the scene they need not be included in the ordering). It was 
then sufficient tc solve the location problem in order to obtain the priority ordering. Since this method 
requires n 2 storage space (where n is the number of clusters) and the traversal of the tree is O(n), 
it is not clear whether this approach is advantageous. Also since a maximum tessellation is requited 
the tree will be the largest possible for a given set of clusters. The application of BSP trees introduced 
in this paper is something of a complement to that presented in Sutherland, et al. Here those objects 
 represented by the segment (or polygon} nodes constitute the visible data while the areas of the tessellation 
are of no importance. In fact, the function Hake_tree presented earlier produces only the segment 
nodes. The area nodes are only implied by the empty sons. Also, Sake tree forms a BSP tree for three 
 dimensions while the former method, although working in 3-D, forms a BSP tree for two dimensionsa 
and Make_tree's tessellation in general is not maximal. Clearly the aSP tree can be used with dividing 
planes to divide 3-space into volumes, and a hybrid of polygons and dividing planes could also be developed. 
 FoE instance, each area node of a tree constructed with dividing planes could be replaced with a BSP 
tree constructed of polygons for the cluster contained in the area. ~9~b~nato~s of the Bsp T~@e ge 
sill nee examine the size of the BSP trees. The previous discussion was presented, for slmplicity's sake, 
for the 2-D case; here we will derive some f.mules both for the 2-D and 3-D BSP trees. Although we are 
most interested in the 3-D case, 2-D is important in the special 3-D case in which all of the objects 
"sit" on the ground and can be separated by vertical planes. This is equivalent to a 2-D BSP tree corresponding 
to the 2-D scene obtained by projecting the objects and the separating plane on the ground plane. As 
noted previously, the BSP tree can be created by both infinite and finite objects= The infinite objects 
are planes for the 3-D case are lines for the 2-D case. The corresponding finite objects are non-intersec¢ing 
convex polygons and segments. Ue will examine these two extremal cases in turn.  A d-dimensional BSP 
tree partitions the d-dimensional space by (d-1)-dimensional objects. Je thus examine first, what is 
the maximum number fd (n) of volumes of a d-dimensional space that can be created by n (d-1)-dimensional 
planes. In the 2-D case we have been considering, this corresponds to the maximum tessellation of the 
plane using lines. The general formula is d (~) fd (n) =lEO As there is a one-to-one correspondence 
oetween the volumes and the leaves of tae binary BSP tree, the ;,umber of the nodes of the BSP tree is 
2f d (n)-1. How many (d- I) -dimensional regions created from (d- I) -dimensional planes under the assumption 
taat no 3 planes intersect along a single (d-2)-dimensional line~ It can be showa that the number is 
 d n z i Q). i=o In the other ext~emal case, where the  objects ~iven are n (d-1)-dimensional non- 
 interpenetrating convex pol)gons, we examine the worst case, namely compute the maximum number of 
the (d- I) -dimensional regions that are obtained from the polygons by the intersection of the n (d- 
 I)-dimensional planes on waich they lie. It can be shown that tae number is n nd_ 1  (2) + We summarize 
the results in Table 2 for the two interesting cases d=2 and d=3.  Volumes Unbounded Objects Bounded 
9bjects 2 2 n +n 2 n +n  2-D: --+ 1 n 2 2 3 3 2 n 3 n + 5n n -n + 2n + 3n 2 + 2n  3-D: --+ i 6 
2 6 Table ~: ~aximum possible nodes in BSP t~ee. Conclusion A solution has been presented to the visible 
surface problem which appears to De more e~ficient than previous solutions whenever many images are to 
be generated of tile same static environment° The algorithm is easy to implement since both phases, the 
prep~ocessing and the image 9eneLation, can each be succinctly stated in a short recursive proceduce. 
The major potential weakness, a large increase from the number of original polygons in the data base 
to the number in the BSP tree, has not occurred in any environment so far encountered. Acknowledq~m~ 
 We wish to thank Greg Abram for much needed and appreciated program development, Mike Cronin for numerous 
improvements to the narrative, and the referees for helpful and thorough reviews of the first draft of 
this paper. References Serman, G. and Fryer, K.D. Introduction !R Combinatori~§, (1972) Academic Press. 
 Fuchs, H. and Johnson, B. "An Expandable MultipEocessor Architecture for Video Graphics" ~rg~- 6~h 
A~nua! S m~. o__~ Co__m~.r Architecture , (1979) pp 58-67 Schumaker, E.A., Brand, R., Gilliland, M. 
and Sharp, W. "Study for Applying Computer-Generated Images to Visual Simulation," AFHRL-TR-69-1q, U.S. 
Air Force Human Resources Laboratory (1969) Sutherland, I.E., Sproull, R. Fo and Schumaker, R.A. "A 
Characterization of Ten Hidden-Surface Algorithms", {197q) ACM Computing Surveys, 6 (i): 1-55 I half-space/~/'' 
/ half-spa~e " ..-" "" half-space \", I Figure l: Environment split by plane of Pk  y ygon¢ set of 
polygons set of polygons Figure 2: Beginning of BSP tree construction [ po#ygon pk ] +/ X i [polygon 
pj ] J ~_ set of polygons / ~ inS set of polygons set of polygons in Sk, j in Sk, j Figure 3: BSP 
tree after two steps 131 Figure 4: Wire-frame and visible line/surface images of same environment (ii 
original polygons; ii in BSP tree) Figure 5: Wire-frame and visible (72 original polygons; line/surface 
i00 polygons images in BSP of same tree) environment A/ B\/c\ J ~ K ~ p , E K H\ I E\ F Figure 6: Visible 
line/surface image of simple object whose polygons cannot be directly assigned visibility priorities 
(some pol#gons here have been split during preprocessing) (left branches are positive) Figure 7: BSP 
tree (arrows on positive side of polygons) and polygons of Fig.4 B J\ A-top C-bottom C-top~ A-~bottom 
(left branches are positive; positive sides of all polygons are visible) Figure 8: BSP two tree and polygons 
of Fig. parts by plane of polygon 6 (A and B) C have each been split into 132 \ \ R J b b R a Figure 
9a Figure 9b Figure 9c 2 , , 3 a R VCT ~a v R(a(b)) CR(a(b)) R(a(b)) R(a(b)) 5 R(a(b(c))) c)) Figure 
i0 Figure ii Figure 12 a (R) /\ b(R(~)) b (R(a)) /\ /\ 3 1 4 b(R) a(e(b)) a(e(b)) \ The maximum tessellation 
for Figure 9 Figure 13 Figure 14 (end) 133 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1980</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>807482</article_id>
		<sort_key>134</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-1980</article_publication_date>
		<seq_no>23</seq_no>
		<title><![CDATA[Stereographic displays of atmospheric model data]]></title>
		<page_from>134</page_from>
		<page_to>139</page_to>
		<doi_number>10.1145/800250.807482</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=807482</url>
		<abstract>
			<par><![CDATA[<p>A system has been developed to display color stereographic time-lapsed sequences of outputs of three-dimensional atmospheric models. Contour lines and wind vectors derived from a model can be overlaid on a geostationary satellite image or displayed with map boundaries. The colors of these graphics can be changed interactively to highlight any desired feature or aid in interpreting relationships between parameters.</p> <p>The viewing portion of the system consists of a raster scan image display terminal with split screen capability and a stereoscopic viewing hood.</p> <p>The user of the system can observe model outputs in color and stereo as a function of time and in a manner which permits easy visualization and comparison.</p> <p>The system permits a meteorologist to visualize atmospheric parameters in a three-dimensional representation and correlate these parameters with information derived from satellite observations. This capability can significantly increase the user's ability to interact with and understand complex meteorological relationships.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Computer graphics]]></kw>
			<kw><![CDATA[Meteorology]]></kw>
			<kw><![CDATA[Modeling display]]></kw>
			<kw><![CDATA[Stereography]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.4.8</cat_node>
				<descriptor>Stereo</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>J.2</cat_node>
				<descriptor>Earth and atmospheric sciences</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.6.5</cat_node>
				<descriptor>Modeling methodologies</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010432.10010437</concept_id>
				<concept_desc>CCS->Applied computing->Physical sciences and engineering->Earth and atmospheric sciences</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341.10010342.10010343</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Model development and analysis->Modeling methodologies</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P332368</person_id>
				<author_profile_id><![CDATA[81100325069]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Mary]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[desJardins]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[NASA/Goddard Space Flight Center, Greenbelt, MD]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP31080853</person_id>
				<author_profile_id><![CDATA[81100350850]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[A.]]></first_name>
				<middle_name><![CDATA[Frederick]]></middle_name>
				<last_name><![CDATA[Hasler]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[NASA/Goddard Space Flight Center, Greenbelt, MD]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Billingsley, J. B., "Interactive Image Processing for Meteorological Applications at NASA/Goddard Space Flight Center," Proceedings of Seventh Conference on Aerospace and Aeronautical Meteorology Symposium on Remote Sensing from Satellites, Melbourne, Florida, November 1976.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Bracken, P. A., Dalton, J. T., Quann, J. J. and Billingsley, J. B., "AOIPS - An Interactive Image Processing System," National Computer Conference, 1978, pp. 159-171.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Butterfield, J. F., "Three-Dimensional Television," Proc. of 15th Annual SPIE Symposium, September 1970, pp. 3-9.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807448</ref_obj_id>
				<ref_obj_pid>800249</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Dalton, J. et al, "Interactive Color Map Displays of Domestic Information," Proc. ACM Siggraph '79, Computer Graphics 13,2, Aug. 1979, pp. 226-233.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Dalton, J. T., desJardins, M. L., Hasler, A. F., and Minzner, R. A., "Digital Cloud Stereography from Geostationary Orbit," Proc. of 13th International Symposium on Remote Sensing of Environment, April 1979.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Hasler, A. F., desJardins, M. L., and Shenk, W. E., "Four Dimensional Observations of Clouds from Geosynchronous Orbit Using Stereo Display and Measurement Techniques on an Interactive Information Processing System," Fourth NASA Weather and Climate Program Science Review, Jan. 1979, pp. 67-72.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807423</ref_obj_id>
				<ref_obj_pid>800249</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Roese, J. A. and McCleary, L. E., "Stereoscopic Computer Graphics for Simulation and Modeling," Proc. ACM Siggraph '79, Computer Graphics 13,2, Aug. 1979, pp. 41-47.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 STEREOGRAPHIC DISPLAYS OF ATMOSPHERIC MODEL DATA Mary desJardins A. Frederick Hasler NASA/Goddard 
Space Flight Center Greenbelt, MD 20771 ABSTRACT A system has been developed to display color stereographic 
time-lapsed sequences of outputs of three-dimensional atmospheric models. Contour lines and wind vectors 
derived from a model can be overlaid on a geostationary satellite image or dis- played with map boundaries. 
The colors of these graphics can be changed interactively to highlight any desired feature or aid in 
interpreting rela- tionships between parameters. The viewing portion of the system consists of a raster 
scan image display terminal with split screen capability and a stereoscopic viewing hood. The user of 
the system can observe model out- puts in color and stereo as a function of time and in a manner which 
permits easy visualization and comparison. The system permits a meteorologist to visual- ize atmospheric 
parameters in a three-dimensional representation and correlate these parameters with information derived 
from satellite observations. This capability can significantly increase the user's ability to interact 
with and understand com- plex meteorological relationships. Key Words and Phrases: computer graphics, 
stereography, modeling display, meteorology. CR Categories: 3.16, 4.41, 6.35, 8.1, 8.2. INTRODUCTION 
 The output from a mathematical model of the earth's atmosphere is a four-dimensional data set (three 
&#38;patial dimensions plus time). Visualiza- tion of and interaction with these data sets is necessary 
for rapid and accurate evaluation of modeling results. This requirement for rapid human interpretation 
of complex computational processes is found in many other applications involving com- puter simulation 
of dynamic physical processes. The Atmospheric and Oceanographic Information Processing System (AOIPS) 
at NASA's Goddard Space Flight Center is an interactive image processing and display facility used to 
support meteorological applications research. A desire to analyze model results and compare them with 
other data sources, particularly satellite data, led to an investiga- tion of techniques for producing 
interactive dis- plays rapidly. AOIPS SYSTEM CAPABILITIES The major components of the AOIPS system 
are two image analysis terminals controlled by a PDP- 11/70 minicomputer. The image analysis terminals 
are extremely flexible and perform a variety of image manipulations at TV-refresh rates. Each terminal 
consists of five digital refresh memories mapped through digital matrix switches to lookup tables and 
high resolution TV displays. Each refresh memory consists of 512 lines of 512 8-bit picture elements. 
One of the refresh memories is addressable as eight bit-planes which can be used for graphics displays. 
The TV monitors have an effective display size of 512 lines of 704 picture elements each. By configuring 
the matrix switches and other terminal registers under computer control, the image analysis terminals 
can be used to perform the following operations:  time lapse displays  image translation  split 
screen displays: rectangular subareas of different refresh memories can be composed into a single display 
 iterative processing: the output of any look- up table can be re-recorded back into a re- fresh memory 
for further processing graphic overlays: individual bit-planes can be displayed as overlays on the TV 
monitors and can be added to the contents of any re- fresh memory A schematic diagram of the terminal 
compo- nents is shown in Figure i. The AOIPS system and the image analysis terminals are described more 
fully in (1,2). Another application of the AOIPS is given in (4). Permission to copy without fee all 
or part of this material is granted provided that the copies are not made or distributed for direct commercial 
advantage, the An copyrlghz notice and the title of the publication and its date appear, and notice is 
given that copying is by permission of the Association for Computing Machinery. To copy otherwise, or 
to republish, 0]980 ACM 0-8979]-02]-4/80/0700-0]34 $00.75 ]34 requires a fee and/or specific permission. 
 ..... cr-~-~l ......... " ~ te~e~ ~ ................ i c3u~ ~ ~ ~ ~! ~! i~ I I I I I I iMaGe e~Ta ~ETUr~ 
TO compuTer oe ~E recoro ~~uu %om~1 e tl~,bNCsbGnALs HA~O,~ARE TO manIPuuaTqoN ~ CO.TaOt ,~.ur OUTPUt 
~ O~FSL, AmP to tv ~EF~ESH ~10~ ~ ,tot Jo~st4c~ , uIGHT ~'En .... ...,In .,oF L__- FIGURE I. Block 
Diagram of Image Analysis Terminal Components TWO-DIMENSIONAL METEOROLOGICAL DISPLAYS Meteorological 
data sets consisting of iso- pleth (contour) lines and wind vectors can be de- rived from two-dimensional 
cross-sections of atmos- pheric model data. Computer programs to generate graphics displays of these 
contour lines and wind vectors are available on the AOIPS. These displays can be combined with maps to 
provide earth location information or overlaid on geosynchronous satellite imagery. The direct visual 
comparison of model output data with satellite image data is useful in the interpretation and verification 
of the modeling results. Using the image terminal hardware, the contour line and wind vector graphics 
can be added to a satellite or map image and stored in a refresh mem- ory. A unique index parameter value 
is assigned to each graphic that is added to the image. By rout- ing this display through the hardware 
lookup tables to the color TV monitor, the graphics can be indi- vidually colored. Programs to change 
the colors rapidly and interactively under joystick control are also available on AOIPS. Using this technique, 
several individual graphic classes representing different features can be combined in one two-dimensional 
image, and the desired features can be highlighted in different colors. A third dimension can be added 
to the color display by using a time-sequenced loop of several of these two-dimensional displays. The 
loop can sequence images which differ in time or in the third spatial dimension. To display the relationships 
present in a four-dimensional data set (three spatial dimen- sions plus a time dimension), a stereographic 
sequence is used. Previous work using AOIPS to display stereographic meteorological data is described 
in the next section. STEREOGRAPHIC IMAGERY The AOIPS facility has been used for the dis- play of stereographic 
satellite imagery using images from two geosynchronous weather satellites with overlapping fields of 
view (5,6). Images from one satellite are mapped into the projection of the second satellite so that 
sea-level features coincide. The image pair is then displayed on the color TV by projecting one image 
in red and the other in blue/green. The combined image can be perceived in three dimensions by viewing 
through red and blue/green filters such as those used in glasses for viewing 3-D movies. The two satellites 
used for this display are in geosynchronous orbit over the equator, separ- ated in longitude by 60 ° 
 The true cloud height 135 is a function of the parallax, and this displace- ment is almost entirely 
along a raster line on the TV. This makes the stereo effect well pronounced and facilitates graphic 
labeling. Simulated stereo images can be created using the imagery from a single satellite. This is 
done by using information derived from different chan- nels (viz., visible and infrared). A shifted ster- 
eo partner to pair with the visible image is cre- ated by shifting each element of the visible image 
along its raster line, by an amount proportional to the cloud height as derived from the infrared measurement. 
Using this same technique, wind vec- tor or contour line graphics can be displayed in three dimensions 
by creating a pair for a graphics image, the shift between the images being propor- tional to the desired 
"height" of the graphic ele- ment. Figure 2 is a stereographic image with wind arrows displayed so that 
they appear at the height of the corresponding clouds. This picture has the right eye image displayed 
in red and the left eye image displayed in blue/green. It would be useful to be able to display wind 
vectors at different heights in different colors. However, these dis- plays preclude coloring of the 
graphics since the coloring capability is being used for the stereo- graphic presentation. Techniques 
for displaying stereographic images so that the graphics can be colored have been investigated. The 
curvature of the TV screen introduces a distortion resulting in false height information towards the 
edge of the screen. This effect can be minimized by centering the area of interest. The stereo hood 
has provided AOIPS with a low- cost method for viewing three-dimensional color stereographic imagery. 
No new hardware additions and only minor software modifications were neces- sary to produce these stereo 
displays. EXAMPLES OF STEREOGRAPHIC COLOR DISPLAYS The stereo hood, split screen system has been very 
useful in interpreting the results of mathe- matical atmospheric models. The images shown in this report 
were derived from the results of a medium scale model developed by Carl Kreitzberg of Drexel University. 
The model generated data at 29x35x15 grid points for 13 one-hour time steps. Geosynchronous satellite 
data from a single satel- lite was available for the same time period. Three-dimensional displays of 
specific humid- ity are shown in Figures 3 and 4. Isopleth lines were generated for seven values of specific 
humid- ity ranging from 2 to 14 gm/kg, at three of the heights available in the data set, viz., 0.375 
km, 1.25 km and 3.00 km. All of the contour lines were added to a map or projected onto a satellite 
image, each contour being assigned a unique index value. This image becomes the right eye image of a 
stereo pair. COLOR STEREOGRAPHIC DISPLAY TECHNIQUES Several methods of stereographic color dis- play 
were investigated, including mechanical and electro-optic shuttering devices (7)~ polarizing filters 
and a stereo optical hood (3). Most of these techniques turn out to have serious drawbacks, however. 
 Mechanical shuttering devices are clumsy and have mechanization problems. The electro-optical shuttering 
devices involve considerable expense and reduced color transmission. Additionally~ inter- leaving the 
lines from a stereo pair to create one image reduces the resolution of the imagery. The polarizing filter 
technique requires careful align- ment of two television pictures and a beam splitter and also reduces 
light transmission. The stereo hood, on the other hand, works quite well for displaying both stereographic 
images and graphics. The hood mounts directly on the television frame, allows for reasonable head movement 
and provides complete separation of the images (eliminating "ghosts" seen using the red/ green glasses). 
The hood, which can be swung away when not in use, requires that a stereo pair be displayed side by side 
on the color TV. This split screen display can be performed in the AOIPS system hardware. The effective 
display width of the color TV is 704 picture elements, meaning that 352 pixels from each image in the 
stereo pair can be displayed. The hardware translation feature allows varying portions of the 512 pixel 
images stored in the re- fresh memories to be selected for display. The corresponding stereo image is 
created using the height information and the translation hardware. This image becomes the left eye image. 
When viewed in three dimensions, the surfaces in the atmosphere separate, the height above the earth 
being exaggerated for display purposes. A time-lapsed loop of these images permits easy visualization 
of a parameter in a four-dimensional data set. Several coloring techniques are useful for analyzing 
the display. Figure 3 shows a stereo- graphic pair of images overlaid on a map back- ground. In this 
display two colors (red/blue) are used. The contour lines for 2 gm/kg at all three heights are red; contours 
for values greater than 2 gm/kg are blue. Using the joystick the user can move the boundary between the 
two colors to higher or lower humidity values. Color lookup tables are generated an~ loaded dynamically. 
Thus the user can interactively isolate the values of interest. Another effective display technique 
assigns a different color to each humidity value. Figure 4 shows the same graphics as those in Figure 
3, overlaid on a (non-stereographic) satellite image instead of a map. The colors of the humidity values 
along the top of the image are the same as the colors assigned to the corresponding contours. Using this 
type of display, the user can easily follow a particular value of humidity upward through the atmosphere. 
 A third type of color display technique has proven very effective for displaying wind vectors at different 
heights. Figure 5 shows a stereo pair of images displaying wind vectors at three 136  heights. The 
length of each arrow is proportional to the wind speed; the orientation of the arrow indicates the direction 
of the wind. In this dis- play the colors correspond to height. Low level winds are red, mid level winds 
are green and upper level winds are yellow. (All of the stereo pair figures may be separated and viewed 
by the reader using an optical stereoscope.) EXTENSIONS Another possibility for data display involves 
representing surface parameters as shaded areas and upper air parameters as contour lines. Attempts to 
date to display shaded areas in three dimensions have not been effective for model anal- ysis. In order 
to display the interactions at different heights above the earth's surface, all of the parameter isopleth 
lines must be present. Shaded area displays necessitate the removal of hidden areas and thus the interactions 
between levels cannot be seen. Displays using more than four or five height surfaces can also be produced. 
In this case, it is desirable to blank out some of the surfaces interactively. Also, displays of more 
than one parameter can be produced, using color to distin- guish each parameter. The system described 
can also be used to dis- play observational data as well as model results. The wind vectors shown in 
Figure 2 can be colored as a funcion of height or wind speed. Parameters obtained from conventional upper 
air balloon data or derived from satellite observations can be displayed and colored using these techniques. 
 CONCLUSIONS The techniques and display facilities des- cribed in this paper give meteorological scien- 
tists a significant new capability for visualizing four-dimensional data sets. In particular, the split 
screen, stereo hood technique provides a low-cost effective viewing system allowing inter- active coloring 
techniques to aid in four-dimen- sional feature analysis. ACKNOWLEDGMENTS The authors would like to 
thank Michael Forman, who investigated color stereographic dis- play techniques and was instrumental 
in setting up the experimental configuration. The authors would also like to acknowledge the support 
and encour- agement of John Dalton and John Quann. REFERENCES i. Billingsley, J. B., "Interactive Image 
Process- ing for Meteorological Applications at NASA/Goddard Space Flight Center," Proceedings of Seventh 
Con- ference on Aerospace and Aeronautical Meteorology Symposium on Remote Sensing from Satellites, Mel- 
bourne, Florida, November 1976. 2. Bracken, P. A., Dalton, J. T., Quann, J. J. and Billingsley, J. B., 
"AOIPS - An Interactive Image Processing System," National Computer Con- ference, 1978, pp. 159-171. 
 3. Butterfield, J. F., "Three-Dimensional Tele- vision," Proc. of 15th Annual SPIE Symposium, September 
1970, pp. 3-9.  4. Dalton, J. et al, "Interactive Color Map Dis-  plays of Domestic Information," 
Proc. ACM Siggraph '79, Computer Graphics 13,2, Aug. 1979, pp. 226- 233. 5. Dalton, J. T., desJardins, 
M. L., Hasler, A. F., and Minzner, R. A., "Digital Cloud Stereography from Geostationary Orbit," Proc. 
of 13th Interna- tional Symposium on Remote Sensing of Environment, April 1979.  6. Hasler, A. F., desJardins, 
M. L., and Shenk,  W. E., "Four Dimensional Observations of Clouds from Geosynchronous Orbit Using 
Stereo Display and Measurement Techniques on an Interactive Informa- tion Processing System," Fourth 
NASA Weather and Climate Program Science Review, Jan. 1979, pp. 67- 72. 7. Roese, J. A. and McCleary, 
L. E., "Stereoscopic Computer Graphics for Simulation and Modeling," Proc. ACM Siggraph '79, Computer 
Graphics 13,2, Aug. 1979, pp. 41-47. 139 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1980</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>807483</article_id>
		<sort_key>140</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-1980</article_publication_date>
		<seq_no>24</seq_no>
		<title><![CDATA[Application of graphical interaction to the analysis of radio astronomy data]]></title>
		<page_from>140</page_from>
		<page_to>146</page_to>
		<doi_number>10.1145/800250.807483</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=807483</url>
		<abstract>
			<par><![CDATA[<p>A highly interactive computer graphics system has been implemented for the display and analysis of high-resolution radio images which are produced by the Very Large Array radio telescope being built by the National Radio Astronomy Observatory. Some of the users of this system are in-house scientists who use the system routinely. However, many of the users are occasional visitors from other institutions who are not necessarily computer programming experts and who have little time or inclination to learn a complex data processing control language. In order to accommodate this mix of users, the system is controlled primarily through the use of a data tablet. Appropriate feedback to the user is provided by both a refreshed line drawing display and a color raster scan image display. Keyboard input is used only for specifying output file names and for specifying a few of the numerical parameters for processing operations. After the digital images are received from another computer system, the entire display and analysis system runs in a stand-alone PDP-11/40. The use of a line drawing display for user interaction and for displaying system status information has proven to be a very useful technique. Although the system is powerful and flexible, the user interface is friendly and can be quickly and easily learned.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>J.2</cat_node>
				<descriptor>Astronomy</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.4.1</cat_node>
				<descriptor>Radiometry</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.8</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010507</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Graphics recognition and interpretation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010432.10010435</concept_id>
				<concept_desc>CCS->Applied computing->Physical sciences and engineering->Astronomy</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Performance</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P133695</person_id>
				<author_profile_id><![CDATA[81100227855]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[James]]></first_name>
				<middle_name><![CDATA[M.]]></middle_name>
				<last_name><![CDATA[Torson]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[National Radio, Astronomy Observatory]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Ryle, M. and Hewish, A. The synthesis of large radio telescopes. Monthly Notices of the Royal Astronomical Society 120, (1960), 220.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Newman, W. E. and Sproull, R. F. Principles of Interactive Graphics. McGraw-Hill Book Company, New York, 1973, Chapter 9.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807430</ref_obj_id>
				<ref_obj_pid>965103</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Thornton, R. W. The Number Wheel: A tablet based valuator for interactive three-dimensional positioning. Computer Graphics 13, 2 (August 1979), 102-107.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Foley, J. D. and Wallace, V. L. The art of natural graphic man-machine conversation. Proceedings of the IEEE 62, 4 (Apr. 1974), 462-471.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 APPLICATION OF GRAPHICAL INTERACTION TO THE ANALYSIS OF RADIO ASTRONOMY DATA James M. Torson National 
Radio Astronomy Observatory* ABSTRACT A highly interactive computer graphics system has been implemented 
for the display and analysis of high-resolution radio images which are produced by the Very Large Array 
radio telescope being built by the National Radio Astronomy Observatory. Some of the users of this 
system are in-house scientists who use the system routinely. However, many of the users are occasional 
visitors from other institu- tions who are not necessarily computer programming experts and who have 
little time or inclination to learn a complex data processing control language. In order to accommodate 
this mix of users, the system is controlled primarily through the use of a data tablet. Appropriate 
feedback to the user is provided by both a refreshed line drawing display and a color raster scan image 
display. Keyboard input is used only for specifying output file names and for specifying a few of the 
numerical param- eters for processing operations. After the digital images are received from another 
computer system, the entire display and analysis system runs in a stand-alone PDP-11/40. The use of 
a line drawing display for user interaction and for displaying system status information has proven 
to be a very useful technique. Although the system is powerful and flexible, the user interface is 
friendly and can be quickly and easily learned. INTRODUCTION The National Radio Astronomy Observatory 
is cur- rently building a radio telescope called the Very Large Array (VLA) on the Plains of San Augustin 
in central New Mexico. This telescope is designed to produce "images" of radio intensity which have a 
resolution that is comparable to images produced by optical telescopes. The VLA consists of 27 anten- 
nas which are each 25 meters in diameter. The antennas are laid out along the three 20-kilometer long 
arms of a "Y" configuration. The VLA uses the *The National Radio Astronomy Observatory is oper- ated 
by Associated Universities, Incorporated, under contract with the National Science Founda- tion. Author's 
Address: NRAO, VLA Program, P O Box O, Socorro, NM 87801 Permission to copy without fee all or part 
of this material is granted provided that the copies are not made or distributed for direct commercial 
advantage, the ACM copyright notice and 0]980 ACM 0-8979]-02]-4/80/0700-0]40 $00.75 aperture synthesis 
technique originated by Ryle and Hewish [1] to produce images of radio intensity from the interferometric 
data that are measured. This basically involves doing a two-dimensional Fourier transform. Observations 
can be made within four wavelength bands: 20 cm, 6 cm, 2 cm, and 1.3 cm. Currently, the system is operating 
in contin- uum mode, which means that an observation results in an image, or radio "map", which corresponds 
to the total radio intensity which is received within the chosen bandwidth. In the future it will be 
possible to observe in spectral line mode, which will result in as many as 256 separate radio maps, each 
corresponding to a slightly different fre- quency channel within the observing bandwidth. In full operation, 
the raw data rate produced by the telescope (i.e., before Fourier transform) will be twelve million complex 
numbers per day for con- tinuum mode. For spectral line mode, this figure will be multiplied by the 
number of frequency channels, resulting in a maximum data rate of about three billion complex numbers 
per day. Figure 1 gives an overview of the VLA computer system. A set of six Modcomp II computers plus 
an array processor control the on-line operation of the antenna array and accept the data from the special-purpose 
correlator hardware. The raw data is currently passed on to the continuum data reduc- tion system, which 
consists of a DEC-10 and a PDP-11/70 with an array processor for doing the Fourier transforms. The finished 
maps can be displayed and analyzed by the PDP-II/40 Interactive Hap Processing System (IMPS), which is 
the subject of this paper. The spectral line data processing system will include a dedicated PDP-II/70 
sorting system plus an FFT system consisting of a PDP-II/40 with three array processors and a special 
transpose memory. There are two basic categories of VLA users. The first group consists of in-house 
scientists who routinely use the instrument and the data reduction software. The second group consists 
of visiting scientists from other institutions who only occa- sionally use the system. The IMPS system 
thus needs to be easy to learn for new users and yet convenient enough for frequent use by people who 
are very familiar with the system. These needs have led to the development of an image display and analysis 
system which makes heavy use of interac- tive line drawing graphics for the user control of the system. 
A data tablet is the main user input the title of the publication and its date appear, and notice is 
given that copying is by permission of the Association for Computing Machinery. To copy otherwise, or 
to republish, requires a fee and/or specific permission. 140 r i I CORRELATOR ___~. CONTROL INFORMATION 
'1 I I , (ONLY A FEW SHOWN) PRIMARY INFORMATION PATHS I ..... J MC ~ LAG TO ~1=,,,,, SECONDARY INFORMATION 
PATHS ARRAY and CORRELATOR 64 KW SPECTRUM i u i --~ HEAVY BOXES ARE DELIVERED CONTROL SYSTEM &#38;Ill 
CORA I r m ~=~ LIGHT BOXES ARE ON ORDER DASHED BOXES ARE TO BE ORDERED CORBIN MC'~ T MC 64 K~ 64 KW 
i I 1'I I I AP   I MONTY I BOSS MC ']~ MC:E I ~ GRIDDER ! 64 KW 64. KW I------"~ ~J I AP I  I COLUMN 
I DUAL ACCESSI 1 x FOaM ¢ ISCREEN DRVRI D,SK I Ij OIS SYSTEM DISK SYSTEM = X POSE I MC 76,0 j ~ II 500 
M_______L_ SO0 MB MEMORY T 4 MW ], -----Jl AP ~ , FHD I L .... ROW I ST I MB ] x FORM I I MAP I ~ ! 
HANDLER I SORTER 128 KW  I me Kw .._i_c-  N,[-' ---n ~ DEC-IO , [ 520 KW M 8o0 MB  DISKS I I D,SK 
l DISKS SORT I I J S 16 SCREENS I ,, 2 GB  Y , SYSTEM' I I ,:OMS I L--__ ] I I°A I I i t __J T E  
 M r-l AP MAP DISP. DISK 11/70 MAP MAKER PDP 11/40 250 MB KW CLEAN, ETC, 128 KW I I TV I ! FILM I 
DISK I DISPLAY I COMTAL WRITE R 250 MB I 256 LINE 5000 LINE SYSTEM I-L_ mmJ Figure I: NRAO-VLA Computer 
Block Diagram, November I, 1979. device. Appropriate feedback to the user is pro- IMPS HARDWARE AND 
OPERATING SYSTEM vided by a variety of interaction techniques in- volving both the refreshed line drawing 
display and The PDP-II/40 has 128-k words of core memory. User the raster scan image display. input devices 
include a Summagraphics data tablet and a freestanding keyboard. The graphic display RELATIONSHIP BETWEEN 
IMPS AND THE OTHER COMPUTERS devices include a DEC VT-II refreshed line drawing display and a Comtal 
raster scan image display, The radio maps are currently produced by either the which can display a 256x256 
gray-scale or pseudo- DEC-I0 or the PDP-II/70 and its array processor. color image. Hard copy output 
is provided by a The map data and associated header information are Versatec electrostatic printer/plotter 
and a then transferred to the disk of the IMPS system. Dicomed color film recorder. The main data storage 
After the data is put onto the IMPS disk, the IMPS in the system is a 250 megabyte disk pack. The software 
runs entirely in the PDP-II/40. (The connection to the DEC-10 system is through a DA-28, PDP-II/40 will 
be upgraded to a PDP-II/44 in the which provides a high-speed block transfer capa- near future.) IMPS 
is thus essentially a single-bility. The standard RSXII-M operating system is user stand-alone system. 
This is important for used. user satisfaction with the system because it pro- vides response times which 
are consistent and which Figure 2 shows the layout of the displays and input are rapid for most functions. 
devices. The VT-II screen is in the middle. The other two CRT's display the raster scan image. 141 
 Figure 3: Menu display for Comtal image loading functions. ENTERING PARAMETER VALUES FOR A FUNCTION 
 After a function has been selected from the menu displays, the user interacts with the system to supply 
any parameter values that are needed for that specific function. The interaction that takes place is 
different for each function. For example, suppose that the user picks menu item "2.3 Load Image into 
a Quadrant of the Screen." The system will then display a menu of available maps such as the list shown 
in Figure 4. Each item in the list is a one-line summary of the header information associated with a 
map. The user then picks the desired map in the same manner as he picked the desired function in the 
menu display. However, unlike the function menus which appear instantly on the screen, the list of available 
maps must be generated from information that is read from the disk. Each item is displayed as soon as 
the infor- mation is available rather than waiting to generate the entire list before showing anything 
to the  user. Thus, user panic is avoided since it is clear that the system is still alive and working 
properly [4]. Although the list generation doesn't take very long (the list in Figure 4 required about 
five seconds), the delay can become annoying for experienced users. Thus, the user may interrupt the 
list generation in two ways. If he sees the desired map in the list he may push down on the pen. This 
will immediately stop the list generation and display the cursor so that he can point to the desired 
map. Or, he may abort bhe list generation by typing a key on the keyboard (any key will do). The system 
will then ask him to type in the name which identi- fies the desired map. This may specify a map which 
hasn't yet been listed on the screen. If the specified map doesn't exist, an error message will be displayed. 
Pushing down on the pen or typing a key will then restart the map selection process. If the selected 
map is too large to fit within a quadrant of the image display screen, the system will next ask the user 
to specify the desired map subsection by giving him a display such as the one shown in Figure 5. The 
large solid rectangle represents the size of the selected map, which in this case is 512x512. The smaller 
square drawn with dashed lines represents the 128x128 subsection that will fit into a quadrant of the 
image display screen. Movements of the data tablet pen will now move the subsection square. When the 
square is in the desired location, the user pushes down on the pen. The subsection square is then intensified 
and the user may acknowledge or cancel the selection in the usual way. Note that the pixel coordinates 
of the corners and center of the entire input map are displayed. Also, the coordinates of the subsection 
are displayed and dynamically updated as the sub- section square is moved. This allows the user to pick 
a subsection at some exact pixel location if desired. Or, he may easily pick a subsection that is in 
some general area of the map without stopping to calculate the pixel coordinates. Also, there is never 
a problem with the user specifying a sub- section that is outside the bounds of the input map since the 
system doesn't allow moving the subsec- tion square outside the map rectangle. If the Figure 4: Menu 
display of available radio maps. Figure 5: Display for picking a map subsection. 143 entire input map 
will fit into a quadrant of the Also, the validity checking on the supplied values  screen, then the 
subsection selection is of course skipped. After the desired map subsection has been speci- fied, the 
system asks the user which quadrant is to be loaded by giving the display shown in Figure 6. If the function 
that loads the entire image display screen had been selected, this step would be omit- ted and the system 
would proceed directly to the loading of the image. Figure 6: Display for selecting quadrant of the 
Comtal screen to be loaded. Some functions have a numerical parameter which has a relatively small set 
of legal values. In this case, the user is presented with a menu that lists the legal values and he just 
picks the one that is desired. However, when a general numerical value or list of values is required, 
the system currently asks the user to type the value(s) on the keyboard. When a data processing operation 
produces a new output map, the system asks the user to type in the name to be used to identify the new 
map. These are the only cases where the user is required to enter values by typing on the keyboard. 
To a first approximation, the system is always doing one of three things: getting the specifica- tion 
of which function is desired, getting the parameters needed for the selected function, and executing 
the function. Of course the same three basic things are done by any system, even one which is oriented 
around having the user type text com- mands to the system. However, the graphical inter- action used 
by IMPS has important advantages over a system that uses textual interaction. For one thing, the user 
doesn't have to remember any names or codes for the available functions. Also, an important part of the 
IMPS design is the close interaction which takes place between the user and the system while the parameter 
values are being specified. The user doesn't need to remember which parameters are applicable to each 
function because the system will ask for the values that are needed. There are thus no errors resulting 
from unspecified parameters having undesirable default values. is for the most part done as each value 
is entered. The information supplied by previously specified parameters is used to check for errors or 
to pre- vent the entering of erroneous values where possi- ble. Thus, when the last parameter value is 
en- tered, the system is ready to execute the function. The user will never enter a list of parameter 
values and then find that he has to start over because an error in the first parameter prevents the system 
from executing the function. The IMPS system essentially gets parameter values by asking questions which 
are then answered by the user. Although it is well-known that such an approach works well for novice 
users, it frequently becomes too tedious for the experienced user in a system that uses textual interaction. 
However, this is not an important problem in IMPS since most questions are quickly and easily answered 
by a few simple movements of the data tablet pen. THE THREE TYPES OF FUNCTIONS There are three main 
types of functions in IMPS. The Comtal image loading functions described above are examples of the first 
type. With these func- tions, the system gets a set of parameter values, executes the function and then 
returns to the menu display so that the user can specify the next desired function. The second main 
type of function is similar to the first except that its execution does not involve the interactive displays. 
This category includes the data processing functions such as the function that averages groups of pixels 
in a map data file to produce a new output map which is smaller than the original map. The execution 
of these functions may also require a great deal more time than func- tions in the first category. IMPS 
currently makes no distinction between these two types of func- tions. However, in the future the system 
will be modified to add the capability of executing the long computational functions as "background" 
oper- ations. After the user has specified the param- eters for one of these functions, the system will 
add it to the background queue and then immediately return to the menu display so that the user can continue 
using the system by doing something inter- active or by queuing another background function. The third 
main type of IMPS function consists of functions which are highly interactive. These functions go into 
a tight loop where they get one or two parameter values from the user and then do some very simple and 
fast operation with these values. These functions just keep looping until the user indicates that he 
wants to exit from the function. THE HIGHLY INTERACTIVE FUNCTIONS Some of the functions which modify 
the way the map image is displayed on the Comtal screen are of the highly interactive type. When the 
image modifica- tion category is selected, the user is presented with the VT-11 display shown in Figure 
7. The upper part of the screen contains a menu of avail- able image modification functions. Some of 
these functions, such as inverting the image, do an 144 operation and then turn the cursor back on so 
that another function may be selected. The highly interactive functions include the functions that modify 
the transfer function which maps frame buffer pixel values into the displayed intensity values on the 
image display screen. For example, if the user picks the "Three Segment - Right Kink" function, the system 
first enters a "frozen" state. A push on the pen enters the "active" state. Movements of the pen across 
the tablet surface then change the shape of the image display transfer function. As it is changed, the 
transfer function plot on the bottom of the VT-II screen is dynami- cally updated. Also, the image on 
the Comtal screen is modified accordingly. This allows the user to dynamically alter the way that the 
map image is displayed so that he can more clearly see the features of interest. A push on the pen will 
enter the "frozen" state so that the image can be studied. Or, typing a key on the keyboard will exit 
from the function.  Another example of a highly interactive function is the cross-section plotting function. 
When this function is entered, a cursor is displayed on the Comtal screen and the user is presented with 
the VT-II display shown in Figure 8. As with the image modification functions, pushing on the pen toggles 
between the "active" and "frozen" states. In the "active" states movements of the pen move the Comtal 
cursor. As this is done, the plot on the VT-I1 screen is dynamically updated to show the intensity profile 
along the horizontal image line on which the cursor is sitting. The solid lines in the plot are adjusted 
to intersect on the dot which is plotting the pixel that the cursor is pointing to. Also, the X and Y 
coordinates of this pixel and the corresponding map data value are shown at the bottom of the VT-II 
screen. RECOVERING FROM ERRONEOUS USER INPUTS  As described above, the user is always provided with 
the opportunity to acknowledge or cancel a menu item selection. If acknowledgment of the selection leads 
to a lower level menu display, he may return to the previous menu display by either typing a key on the 
keyboard or selecting the last item (see Figure 3). At most times during the specification of parameters 
for an operation it is easy for the user to abort the parameter specifica- tion and return to the menu 
of functions. For example, if the user is being presented a menu of available input maps or a menu of 
legal parameter values, he may type a key on the keyboard to reject all of the menu items and exit from 
the function. If he is being asked to type in an output map name or a numerical parameter value, typing 
just a carriage return will abort the function. Some of the functions can be aborted even after they 
have started execution. For example, if the user sees that the image being loaded into the Comtal is 
the wrong one, he may abort the loading by typing a key. Thus, it is always easy for the user to change 
his mind and exert control over the system. Figure 7: Menu display and status display for image modification 
functions. Fisure 8: Display for plotting image intensity profiles. IMPLEMENTATION IMPS consists of 
about 20,000 lines of code, most of which is FORTRAN. Assembly language routines are used only in a few 
places for efficiency and for providing a more flexible disk I/O system. Although it is not apparent 
to the user, IMPS is implemented as many separate tasks. The main task, which is named IMPS, presents 
the menus and deter- mines which function is desired by the user. It then activates one of the second 
level tasks and exits. The second level task interacts with the user to determine the parameter values 
for the selected function. This passes control and a packet of parameter values to one of the third level 
tasks which actually executes the function. When the function is finished, control is passed back to 
the IMPS task. For some simple functions, such as the highly interactive functions, the second level 
task does the operation and returns control directly to the IMPS task. Also, this control flow will of 
course be modified somewhat when the background capability is added to the system. There are several 
advantages to having separate  145 tasks rather than a single task with overlays. First, system development 
is easier and quicker since only the task being worked on needs to be recompiled and linked. It is even 
possible for an ambitious user to implement a task that does some special function and then use it with 
the rest of the system. Also, the part of the system that interacts with the user is clearly separated 
from the part that does the data processing. The data processing tasks could thus be easily transported 
to another computer system, even if it had no interactive display. (Of course in this case, a text oriented 
control package for obtaining param- eter values would need to be implemented.) The standard DEC-supplied 
graphics subroutines are used for handling the VT-II display. The menu displays and most of the displays 
for interacting with the user are generated and stored in disk files by a special initialization program 
which is run only at system generation time. Thus, each task that uses the VT-II only needs to read in 
the appropriate display file and make calls to the routines that turn segments of the display file on 
and off. This provides faster response to user actions and also reduces the sizes of the tasks by eliminating 
the display file generation code. CONCLUSIONS The techniques of interactive line drawing graphics have 
proven to be very useful for the control of an image processing system. The use of a data tablet is sufficient 
for nearly all user control of the system. This results in a system which is very friendly and easy to 
learn for new users. At the same time, the system control is convenient enough to be routinely used by 
experienced users. REFERENCES 1. Ryle, M. and Hewish, A. The synthesis of large radio telescopes. Monthly 
Notices of the Royal Astronomical Society 12N~O, (1960), 220. 2. Newman, W. E. and Sproull, R. F. Principles 
of Interactive Graphics. McGraw-Hill Book Company, New York, 1973, Chapter 9.  3. Thornton, R. W. The 
Number Wheel: A tablet based valuator for interactive three-dimen- sional positioning. Computer Graphics 
~3, 2 (August 1979), 102-107.  4. Foley, J. D. and Wallace, V. L. The art of natural graphic man-machine 
conversation. Proceedings of the IEEE 62, 4 (Apr. 1974), 46Z-471.     
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1980</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>807484</article_id>
		<sort_key>147</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-1980</article_publication_date>
		<seq_no>25</seq_no>
		<title><![CDATA[Color graphics for remote teaching]]></title>
		<page_from>147</page_from>
		<page_to>153</page_to>
		<doi_number>10.1145/800250.807484</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=807484</url>
		<abstract>
			<par><![CDATA[<p>A system for preparation and manipulation of color graphics video frames is described. The system commands are designed for use by a class instructor (who is not a computer science person) in preparation and delivery of lectures to a remote site. The system can provide the types of images, overlays, and coloring that could be drawn using transparency sheets and color pens, or that could prepared as 35 mm technical slides (not photographs, but text and diagrams). The hardware consists of standardly available input, display, and output components. Several hardware alternatives are noted so that cost and capability tradeoffs can be compared. Communication between the instructor's site and the remote class site uses standard telephone lines for transmission of participants' voices and for digital graphics commands and compressed image data. The system software is yet in experimental form. It is in BASIC, but in modular form. While none of the individual components or graphics concepts in this system is in itself dramatically new, the integrated system demonstrates an important advancement in state-of-the-art for remote instruction.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Color graphics]]></kw>
			<kw><![CDATA[Teaching support]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Color, shading, shadowing, and texture</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>K.3.1</cat_node>
				<descriptor>Distance learning</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010489.10010495</concept_id>
				<concept_desc>CCS->Applied computing->Education->E-learning</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010489.10010494</concept_id>
				<concept_desc>CCS->Applied computing->Education->Distance learning</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010384</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Texturing</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P299133</person_id>
				<author_profile_id><![CDATA[81330492284]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[William]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hankley]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Department of Computer Science, Kansas State University, Manhattan, KS]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P291629</person_id>
				<author_profile_id><![CDATA[81100462954]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Virgil]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wallentine]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Department of Computer Science, Kansas State University, Manhattan, KS]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Association for Media-based Continuing Education for Engineers, Inc., Georgia Institute of Technology, Atlanta, GA.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Pettie, J. and Grace, D., "The Stanford Instructional Television Network," IEEE Spectrum, Vol. 7, May 1970, pp. 73-80.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Southworth, G., "Slow Scan TV Teleconferencing," (1978), and "Slow Scan TV Business Communications," (1979), Colorado Vid&#279;o, Inc., Boulder, CO.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Pinches and Liddell, "The Production of Programme Material for the Cyclops System," Proc. Conf. on Microprocessors in Automation and Communications, IEEE, No. 40, Sept. 1978.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Advent Corporation, 195 Albany St., Cambridge, MA, FTC Services, Inc., 25 Broad St., New York, NY.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Baer, W., "Telecommunications Technology in the 1980's," Chap. 2, G. Robinson (Ed.), Communications for Tomorrow: Policy Perspectives for the 1980's, Praeger, NY, 1978.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_obj_id>803470</ref_obj_id>
				<ref_obj_pid>952989</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Cheng, R., "On-Line Large Screen Display System for Computer Instruction," ACM SIGCSE, Vol. 8, #1, Feb. 1976, pp. 179-181.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Hiltz, S. R. and Turoff, M., "The Network Nation," Addison-Wesley Publishing Company, Inc., Reading, MA, 1978.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Hough, R. and Panko, R., "Teleconferencing Systems: A State-of-the-Art Survey and Preliminary Analysis," NTIS report number PB-268 455, 1977.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_obj_id>578546</ref_obj_id>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Johansen, R.; Vallee, J.; and Spangler, K., "Electronic Meetings," Addison-Wesley Publishing Company, Inc., Reading, MA, 1979.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_obj_id>803472</ref_obj_id>
				<ref_obj_pid>953026</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Kumar, V. and Rogers, J., "Instructional Uses of the Olin Experimental Classroom," ACM SIGCSE, Vol. 8, #1, Feb. 1976, pp. 189-191.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_obj_id>803473</ref_obj_id>
				<ref_obj_pid>952989</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Levine, D., "Computer-Controlled Display Demonstrations of Dynamic Concepts in Computer Science," ACM SIGCSE, Vol. 8, #1, Feb. 1976, pp. 192-199.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Martin-Vegue, C., "Technical and Economic Factors in University Instructional Television Systems," Proc. IEEE, Vol. 59, #6, June 1971, pp. 946-953.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Wallentine, V. and Hankley, W., "A Computer Based Remote Education Delivery System," Technical Report, Department of Computer Science, Kansas State University, 1979.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 COLOR GRAPHICS FOR REMOTE TEACHING William Hankley and Virgil Wallentine Department of Computer Science 
Kansas State University Manhattan, KS 66506 Abstract Use 9/i Disnlavs for Teaching A system for preparation 
and manipulation of color graphics video frames is described. The system commands are designed for use 
by a class instructor (who is not a computer science person) in preparation and delivery of lectures 
to a remote site. The system can provide the types of images, overlays, and coloring that could be drawn 
using transparency sheets and color pens, or that could be prepared as 35 mm technical slides (not photographs, 
but text and diagrams). The hardware consists of standardly available input, display, and output components. 
Several hardware alternatives are noted so that cost and capability tradeoffs can be compared. Communication 
between the instructor's site and the remote class site uses standard telephone lines for transmission 
of participants' voices and for digital graphics commands and compressed image data. The system software 
is yet in experimental form. It is in BASIC, but in modular form. While none of the individual components 
or graphics concepts in this system is in itself dramatically new, the integrated system demonstrates 
an important advancement in state-of-the-art for remote instruction. Kevwords: color graphics, teaching 
support Categories: 3.9, 8.2 Introduction The growth of off-campus enrollments in technical subjects, 
particularly computer science, prompts the need for mechanisms for remote delivery of instruction. For 
example, at KSU over 40 percent of our graduate enrollments in computer science are in cities located 
over 100 miles from the Manhattan campus. Nationally, several different types of delivery systems have 
been proposed and used: travel, video tape [I], live TV [2], slow-scan TV [3], and CAI [4]. This paper 
reports on the graphics component of a computer based instructional delivery system being developed at 
KSU. The system uses graphics displays and digital transmission of images. It supports (I) conducting 
of remote lectures using the graphics equivalent of color slides, (2) class interaction with the instructor, 
and (3) preparation and storage of slides. The "slides" referred to throughout the paper are always composed 
video images. The word "slide" is a carry-over from our prior use of transparency slides. Permission 
to copy without fee all or part of this material is granted provided that the copies are not made or 
distributed for direct com~erclal advantage, the ACM copyright notice and O]980 ACM 0-8979]-02]-4/80/0700-0]47 
$00.75 Computer science subjects are representative of technical subjects in that lectures depend heavily 
upon visual aids. This is characteristic of the abstractions within the subject material. At KSU, all 
of the instructors in computer science use diagrams to focus their lecture presentations and class discussions. 
Simple line diagrams with text are used to represent the structure of algorithms, data, data flow, and 
machines. (It is interesting to note that even blind programmers are taught to visualize diagrams as 
abstractions of programs and data.) About half of these instructors routinely use prepared transparency 
slides. A lecture would typieally cover ten 8 I/2" x 11" transparencies, but survey lectures cover many 
more slides. The slides are part static (prepared in advance) and part dynamic (developed using erasable 
color marking pens). The use of colors greatly enhances the presentation and reception of graphic material, 
but we have no speclfie economic justification for use of colors. Slides contain mixed text and diagrams. 
For visual clarity, text is limited to about twenty lines per page, and diagrams are about half the density 
of textbook diagrams. Commonly, black on white copies of the static subset of slides are sold in advance 
as an aid for students' note taking. In course reviews, students consistently emphasize that the advance 
preparation and distribution of the class slides is a major factor contributing to class organization 
and success. With an orientation towards use of visuals for classes, a significant amount of instructors' 
time is required for preparation of slides. This time is greater than that required for preparation of 
notes, because better organization and style are required. It is also greater than that required for 
in-class presentation of diagrams on a blackboard, since a much higher "frame rate" is achieved. As a 
class is taught, the slides leave a permanent and legible record of the course as well as a standard 
frame of notes for use in discussions outside of class. Based Delivery Svstem The functional components 
of a computer based education delivery system (called the DS) are represented in Figure I. The spokes 
are the subsystems available to users. These are described the title of the publicationand its date 
appear, and notice is given that copying is by permission of the Association for Computing Machinery. 
To copy otherwise, or to republish, requires a fee and/or specific permission. ]47 elsewhere [14]. 
We focus here on the teaching and "slide" preparation subsystems. These two are being implemented as 
a prototype two-node digital and audio network shown in Figure 2. Using the preparation system, an instructor 
is supported in the following: - input two-color 512 x 512 digitized images of existing diagrams create 
8 .color images composed of straight lines, dotted lines, free-hand curves, circles, arcs, ovals, rectangles, 
"bubble-clouds", arrows, and text; these may be different widths and sizes, filled, and blinking; more 
experienced persons can partition slides into window areas which can contain scrolled text or diagrams 
animated in discrete steps made of either a sequence of separate diagrams or driven by a mini BASIC program 
 - store sequences of slides - edit slides to delete and insert image components or superimpose slides 
 - transmit slides to the remote teaching site for temporary storage prior to presentation In the teaching 
mode, the instructor is linked to the remote class by (I) a digital link for transmission of slides and 
data and by (2) an audio link for normal class conversation. The class cannot see the instructor's face 
(except for possibly a still digitized image which might be transmitted), but experiences with teleconferencing 
show that facial cues are not essential. The instructor is supported in the following actions: select 
slides and display them at both locations; slides can be selected by name and number or by stepping forward 
or backward in the sequence - overlay slides - point to objects on the screen using a movable cursor 
 - highlight areas of a slide with any unused background color use any of the preparation functions; 
however, only free-hand drawings can be created fast enough for use during a class period - draw on 
slides without altering the originals allow students to perform all the operations above; for example, 
pointing at objects during a conversation give the class a hard copy of a slide; however, we have restricted 
remote copies to black and white - converse with students as in normal telephone conference - poll 
student responses to questions; for our prototype model, students are each restricted to binary responses 
 - receive student' s interrupt requests identified by student position number or name - peek at slides 
without transmitting them to the remote class The appendix provides a summary of instructor's commands. 
 GraPhics Components The configuration represented in Figure 3 provides the maximum capabilities that 
we judged practical for this nature delivery system. Tradeoffs of capabilities and costs are discussed 
later. Specific models are cited for example only. The components include: (HRD) 512 x 512 resolution 
8 color raster graphics display processor [Chromatics 1999] with keyboard and function key input and 
display for generation of high resolution slides" (FD) mini-floppy disks for temporary storage of slides 
 (TAB) digitizing tablet [Summagraphics BIT Pad] (CAM) 512 x 512 digitizing black/white TV camera [Dage 
camera, Colorado video digitizer interfaced to Chromatics display] (LRD) a low resolution (nominal 200 
x 200) 8 color raster graphics display [ISC Intereolor] with keyboard and light pen, for menu selection 
of operations commands and for display of text, and as an alternative low resolution display (HCU) hard 
copy device; options we considered were (a) low cost black/white dot matrix printer for remote class 
 (b) low cost color printer [TRILOG]  (c) high quality color printer [Xerox 6500]  (MON) monitors 
for classroom viewing; options were (a) RGB video monitors (CONRAC)  (b) RGB beam projector (ADVENT) 
  (SRU) student response units; minimally, a single binary switch unit for each class position. (ATC) 
audio teleconferencing system [Western Electric 50A] (CC) control computer, a general purpose processor 
for archiving of display slides (MOb) 4800 baud modems for use of commerical grade telephone lines 
Image Data Structures There are four types of image structures used for representation of images. The 
instructor must recognize the difference in these forms: 148 I. Image structure--conslsts of up to 512 
x site was configured at $41,000 for the digital 512 x 4 bits (red, green, blue, blink), a equipment 
(HRD, FD, TAB, CAM, LRD, HCUb, MOD) and file of 131K bytes; these are used for $33,000 for the remote 
site for HRD, FD, TAB, HCUa, temporary storage when the instructor SRU, MON, MOD. Operating costs include 
wishes to flip between alternate slides maintenance, paper costs, and telephone line charges (under 
$20/hr for two-line for the cities 2. Display file structure--ASCII file cited in the paper). Cost of 
specially developed composed of text, and one byte graphics software is ignored. operations and parameters, 
e.g., characters require one byte each, graphics For many educational institutions, these costs objects 
require one byte per command and would be prohibitive. As we examined alternatives four bytes per parameter 
point; rectangles for shrinking the configuration, the following require two parameter points; free hand 
issues were identified: lines require many parameter points. For example, Figures I and 2 are 3KB and 
4.6KB -The total costs might be cut 10% by directly respectively, including all the color and purchasing 
individual components, rather control bytes; this structure is used for than standardly configured units. 
transmission of slides to the remote site and for permanent storage The secondary LRD was not Justified. 
 - Operator's information could be adequately 3. Digitized image--up to 512 x 512 x I bit; displayed 
in a small window of the total 33K bytes; thi~ is used as one form instructor's display. for camera input 
of existing hard slides -The selection of 512 or 256 (nominal) 4. Compressed image--a run length encoded 
resolution is an open question that requires compression of a digitized image; further justification. 
The quality and compressions over 50 percent may be complexity of images that can be presented achieved 
for sparse images. with a low resolution device is very limited; yet, the cost of higher resolution Performance 
devices is correspondingly great. Also, the choice of a higher resolution device With only little experience 
so far, we find influences the other component costs. preparation of slides takes at least I/2 hour, 
which is over double the time for free-hand drawing -The digitizing camera might not be necessary of 
slides, but less than that required for drafting for some delivery cases. It allows existing of neat 
slides. Some timings for various diagrams and visuals to be viewed. In processing tasks are as follows: 
particular, it allows a "rush" factor whereby diagrams may be input rapidly. digitizing an existing 
slide ......... 3 seconds Without the camera, all slides would have to (driver written in Z80 assembler) 
be prepared using the composition capability of the system. compressing or expanding a ........... < 
10 seconds digitized picture (the prototype (est.) It has been noted to us, first, that lower - in 
BASIC took several seconds) cost hard copy devices can be selected and, second, that with advanced preparation 
by retrieval of slides from mini disk ---< 10 seconds the instructor and use of the US mail, the remote 
HCU might be eliminated. However, display of image file ................ 6 seconds the second action 
sacrifices the "immediateness" of service which students generation of image from ............. Z 2 
sec/KB seek. a display-file -The most crucial factor is the use of color. Using a simple error detecting 
protocol, the This determines the cost range of the whole effective transmission rate was cut in half 
to 2400 system. It has been argued to us that baud. At that rate, transmission times are about 4 adequate 
visual cues could be conveyed via sec/KB, i.e., 22 seconds for the slides in this B/W devices using encoding, 
such as dotted paper, and up to 30 seconds for compressed lines, thick lines, blinking, reverse video, 
digitized images. This delay is very tolerable etc. This argument ignores a psychological during a lecture 
because (I) slides can be factor which we have experienced. Color transmitted prior to the class period 
and (2) provides a stimulation--a degree of during discussion of one slide while commands are liveness--that 
may people have grown to transmitted (for example, for pointing to places on expect. There is much evidence 
for this, the slide), transmission of the next slide may be yet we lack an economic verification of the 
multiplexed. For free-hand diagrams entered during use of color. a class discussion, a small transmission 
delay is noticeable. Eventually, the question of cost Justification or cost effectiveness must be answered. 
For educational delivery, this type of system might not Costs, Alternatives, and Justification be "justified" 
on a cost basis. For example, in many institutions, remote courses are taught as an For our experimental 
system, using standardly assigned load such that instructors' time has zero available component subsystems, 
the instructor's incremental cost (they are being paid already). Thus, travel may be most economical. 
The potential advantages over travel are more effective use of instructor's time, freedom from vagaries 
of travel, and capability for reaching multiple remote sites. Conclusions The subject system for delivery 
of instruction is yet in experimental form; it has not been used for a regular teaching schedule. The 
software is being refined and extended. Still, several benefits are apparent. The visual stimulation 
of color images adds an intangible dimension of liveliness which focuses viewers' attention. Each presentation 
leaves a permanent record of legible slides. Also, the speed of interaction is good. There are difficulties. 
Administrators are reluctant to venture to use such new technologies. (While graphics and communications 
are not new for computer scientists, they are perceived so by many other educators.) So far, preparation 
of slides has been done only by people who were interested in the graphics systems. We do not know how 
others might be able to use the system. Also, we had definite need for help of our electrical technician 
with a few broken cables and failed IC's. Several questions remain to be answered through further experiments. 
The primary one is the effectiveness of the whole system--will instructors and recipients be able and 
willing to use such equipment? Next, the cost/capability decisions need further support, particularly 
selection of resolution, color, camera input, and hard copy options. Based upon previous trends, we expect 
the cost of such systems (and of the currently expensive options) to continue to drop. And, we expect 
delivery systems to serve an increasing need as organizations and individuals seek off-campus presentation 
of technical courses in fields where number of instructors is limited. References I. Association for 
Media-based Continuing Education for Engineers, Inc., %Georgia Institute of Technology, Atlanta, GA. 
 2. Pettie, J. and Grace, D., "The Stanford Instructional Television Network," IEEE Spectrum, Vol. 7, 
May 1970, pp. 73-80.  3. Southworth, G., "Slow Scan TV Teleconferenoing," (1978), and "Slow Scan TV 
Business Communications," (1979), Colorado Video, Inc., Boulder, CO.  4. Pinches and Liddell, "The Production 
of Programme Material for the Cyclops System," Proc. Conf. on Microprocessors in Automation and Communications, 
IEEE, No. 40, Sept. 1978.  5. Advent Corporation, 195 Albany St., Cambridge, MA, FTC Services, Inc., 
25 Broad St., New York, NY.  6. Baer, W., "Telecommunications Technology in the 1980's," Chap. 2, G. 
Robinson (Ed.), Communications for ~2_r_~g: Policy  Perspectives for the 1980's, Praeger, NY, 1978. 
 7. Cheng, R., "On-Line Large Screen Display System for Computer Instruction," ACM SIGCSE~ Vol. 8, #I, 
Feb. 1976, pp. 179-181.  8. Hiltz, S. R. and Turoff, M., "The Network Nation," Addison-Wesley Publishing 
Company, Inc., Reading, MA, 1978.  9. Hough, R. and Panko, R., "Teleconferencing Systems: A State-of-the-Art 
Survey and Preliminary Analysis," NTIS report number PB-268 455, 1977.  10. Johansen, R.; Vallee, J.; 
and Spangler, K., "Electronic Meetings," Addison-Wesley Publishing Company, Inc., Reading, MA, 1979. 
 11. Kumar, V. and Rogers, J., "Instructional Uses of the Olin Experimental Classroom," ACM $~GCSE, 
Vol. 8, #I, Feb. 1976, pp. 189-191.  12. Levine, D., "Computer-Controlled Display Demonstrations of 
Dynamic Concepts in Computer Science," ACM SIGCSE, Vol. 8, #I, Feb. 1976, pp. 192-199.  13. Martin-Vegue, 
C., "Technical and Economic Factors in University Instructional Television Systems," Proe. IEEE, Vol. 
59, #6, June 1971, pp. 946-953.  14. Wallentine, V. and Hankley, W., "A Computer Based Remote Education 
Delivery System," Technical Report, Department of Computer Science, Kansas State University, 1979.  
 Appendix I Picture Comoosition~. The tablet and camera commands are listed as control commands. Function 
key commands are: CURSOR ..... change visible/invisible COLOR ..... set color BACKGROUND . . . set background 
color FILL ...... causes solid objects to be filled with background color and external texture key 
BLINK ..... created objects blink ZOOM ...... zoom on area Editin~ Commands: F command . . . next object 
is filled B command . . . next object blinks X [command] . . object (or last object) is deleted D command 
. . . next object is dotted Kevboard Commands: ~_ize ..... set size for characters ~ize ...... set 
size factor for all lines Arrow ..... draw arrow defined by end points ~ect ...... define rectangle 
 ~ircle ..... define circle ~ubble ..... define bubble ~val ...... define oval ~ar ...... define small 
star L[*] ...... define line or continuous line LH ....... define horizontal line Control Commands 
 (a) Light-pen control commands CAMERA: OFF READY SCAN . . . initiate digitiz- ing camera POLL:.NUMBER 
=? ....... poll student response units QUESTION: OFF READY PERSON=? .... receive interrupt from student 
response unit TABLET: OFF POINTING LINE VECTOR FILL ERASE . . control of graphics tablet SCREEN: H 
L ........ select alternative screens: H=high resolution graphics L=text display and low resolution graphics 
 (b) Keyboard commands: The underlined letters of each command are required. Each slide is named as "COURSE.TOPIC.NUMBER[.VERSION] 
[PROTECTION]". Specific operands are not given. ~_~[AME .... define default root of names ~.ILE ..... 
store a created slide ~_~HANGE . . . rename y_IIELETE . . . delete ~[ABLE,X . . . enable/disenable 
remote stations ~UERY .... query status of remote stations MENU ..... define commands ~ISPLAY . . . display 
a slide ~VERLAY . . . overlay a slide ~EEK ..... peek at slide at operator's station ~ARDCOPY . . . generate 
a copy ~LEAR .... clear screens ~AVE ..... temporarily save a slide ~ESTORE . . . restore a saved slide 
LV ...... define vertical line V ...... define free form continuous object /~[ i .... define composite 
object i ~i .... end definition /I~AW i .... draw object i 9_~ ...... centered, horizontal text ...... 
horizontal text 9_K ...... centered, vertical text ...... vertical text Screen ~QntrolgQg~g/ig~.: HL 
...... highlight line with back- ground color HR ...... highlight rectangle ~_~ ...... scroll up, down, 
left, or right Pen or cursor controls automatically position the reference point or cursor on the screen. 
 RY \ CURRICULUM RESEARCH / \ ADVISING DIRECT!O ~ / FIGURE 1 FUNCTIONAL COMPONENTS OF AN EDUCATION DELIVERY 
SYSTEM \ STUDENTS / \ / \ / Z ~ PICTURES (DIGITAL AND VECTOR), ~ TEXT, AND ATTENTION I/0 DEVICES 
FOR STUDENT VISUAL ~IDS AND STUDENT RESPONSES REMOTE OR LOCAL I TELECOMMUNICATIONS @ l PATHS TO t DS 
SYSTEM FROM STUDENT SITE COMPUTER SYSTEM CONFERENCING ] PICTURE AND TEXT I/0 '-'~ FOR THE INSTRUCTOR'S 
VISUAL AID } REPARATION AND PRESENTATIONJ / x / \ z \ ! INSTRUCTOR \ FIGURE 2 Two ~IODE STRUCTURE 
FOR INSTRUCTION DELIVERY HIGH __J ADDITIONAL MONITORS I (HRD) 1 ~ STUDENT RESPONSE STUDENT I II I on, 
iTS ~SRU> SITE MODEM(blOD) SYNCH ~PROTOCOL ~ c~ MODEM (MOD/ IC~URE CONTROL [~] DATA COMPUTER I (CC) 
ASYNCH PROTOCOL ~ SMALL I IDISKS INSTRUCTOR SITE GRAPHICS COMPUTER I (CAM) "~'~.  I I "'" RESOLUTION 
RESOLUTION D I S PLAY D I S PLAY (HRD) (LRD) FIGURE 3 HARDWARE COMPONENTS FOR A DELIVERY SYSTEM CONFIGURATION 
 (AUDIO TELSCONFERENCING NOT SHOWN) 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1980</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>807485</article_id>
		<sort_key>154</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-1980</article_publication_date>
		<seq_no>26</seq_no>
		<title><![CDATA[Procedure models for generating three-dimensional terrain]]></title>
		<page_from>154</page_from>
		<page_to>162</page_to>
		<doi_number>10.1145/800250.807485</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=807485</url>
		<abstract>
			<par><![CDATA[<p>A method for generating arbitrary terrain models, including trees, bushes, mountains, and buildings, is described. Procedure models are used to combine fundamental data elements in the creation of unified objects comprising the terrain model. As an example, a procedure model to generate arbitrary trees of various species is implementation. These are the generation of the low level data elements, specification of input parameter requirements, and a brief explanation of the algorithmic structure. Terrain images rendered by this process are included, as are diagrams and illustrations explaining the procedure model organization. Comparisons with previous work are made.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Computer animation]]></kw>
			<kw><![CDATA[Computer graphics]]></kw>
			<kw><![CDATA[Data generation]]></kw>
			<kw><![CDATA[Flight simulation]]></kw>
			<kw><![CDATA[Interactive systems]]></kw>
			<kw><![CDATA[Procedure models]]></kw>
			<kw><![CDATA[Terrain model]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Animation</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.5.5</cat_node>
				<descriptor>Interactive systems</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003120.10003121.10003129</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interactive systems and tools</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP31035315</person_id>
				<author_profile_id><![CDATA[81100280460]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Robert]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Marshall]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Computer Graphics Research Group, The Ohio State University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P333492</person_id>
				<author_profile_id><![CDATA[81332535747]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Rodger]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wilson]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Computer Graphics Research Group, The Ohio State University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P334345</person_id>
				<author_profile_id><![CDATA[81100379189]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Wayne]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Carlson]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Computer Graphics Research Group, The Ohio State University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>807458</ref_obj_id>
				<ref_obj_pid>800249</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Csuri, C., Hackathorn, R., Parent, R., Carlson, W., Howard, M. Towards an Interactive High Visual Complexity Animation System, Proceedings Siggraph '79, (Aug. 1979), 289-299.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Newell, M.E., The Utilization of Procedure Models in Digital Image Synthesis. Computer Science, Univ. of Utah, UTEC-CSC-76-218, (1975).]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807383</ref_obj_id>
				<ref_obj_pid>800248</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Dungan, W., Stenser, A., Sutty, G., Texture Tile Considerations for Raster Graphics. Siggraph '78 (Aug. 1978), 130-134.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807436</ref_obj_id>
				<ref_obj_pid>800249</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Dungan, W., A Terrain and Cloud Computer Image Generation Model. Proceedings Siggraph '79, (Aug. 1979), 143-150.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>563875</ref_obj_id>
				<ref_obj_pid>563858</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Greenberg, D.P., An Interdisciplinary Laboratory for Graphics Research and Applications. Proceedings Siggraph '77, (July, 1977), 90-97.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Brooks, J., Murarka, R.S., Onuoha, D., An Extension of the Combinatorial Geometry Technique for Modeling Vegetation and Terrain Features. NTIS Report AD-782-883, (Aug. 1974).]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Parent, R., Chandrasekaran, B., Moulding Computer Clay. From Pattern Recognition and Artificial Intelligence. C.H. Chen, Ed., Academic Press, (1977), 86-107.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 PROCEDURE MODELS FOR GENERATING THREE-DIr~NSIONAL TERRAIN* Robert Marshall, Rodger Wilson and Wayne 
Carlson Computer Graphics Research Group The Ohio State University ABSTRACT A method for generating 
arbitrary terrain models, including trees, bushes, mountains, and buildings, is described. Procedure 
models are used to combine fundamental data elements in the crea- tion of unified objects comprising 
the terrain model. As an example, a procedure model to gener- ate arbitrary trees of various species 
is imple- mentation. These are the generation of the low level data elements, specification of input 
param- eter requirements, and a brief explanation of the algorithmic structure. Terrain images rendered 
by this process are included, as are diagrams and illustrations explaining the procedure model organ- 
ization. Comparisons with previous work are made. CR CATEGORIES: 3.12, 3.21, 3.56, 3.57, 3.65, 3.70, 
8.1, 8.2 KEY WORDS: procedure models, terrain model, data generation, flight simulation, computer graphics, 
interactive systems, computer animation INTRODUCTION Computer animation has developed to the stage 
where systems can handle the 3-D display of ob- jects or scenes involving several hundred thou- sand 
faces (i). There is a need for systems that can generate more complex data in a fashion and format that 
minimizes processing time while main- taining data integrity. Such is the case with terrain models. A 
typical 3-D image of a tree, for example, might consist of hundreds of thou- sands or even millions of 
faces, and a scene in- volving trees along with hills, mountains, and other terrain elements involves 
even more data. It seems that what is needed is not only a system capable of generating and displaying 
 This work is supported in part by National Science Foundation Grant No. MCS76-18659 and by U. S. Navy 
Contract No. N61339-80-C-0008. Permission to copy without fee all or part of this material is granted 
provided that the copies are not made or distributed for direct co~0ercial advantage, the ACM copyright 
notice and the title of the publication and its date appear, and notice is given that copying is by 
permission of the Association for Computing Machinery. To copy otherwise, or to republish,  requires 
a fee and/or specific permission. 01980 ACM 0-89791-021-4/80/0700-0154 $00.75 millions of faces, but 
one in which the user can interactively manipulate the data in a fashion not unlike a set designer, painter, 
or architect. The user needs to be able to look at a scene and de- cide to make a tree fuller by adding 
new branches, or move a corn field, or add a mountain. The massive quantity of data involved in creating 
a landscape make this type of interactive art direc- tion nearly impossible without the aid of a lan- 
 guage which permits the user to alter a few param- eters without changing the entire object or scene. 
The procedure model fits this need. Although the use of procedure models in computer graphics is not 
new, it has not been applied to solving the specific problem of generating 3-D terrain models of this 
complexity. By definition, a procedure is a precise step- by-step method for effecting a solution to 
a pro- blem. In the case of terrain models, it is a method by which data is organized so that storage, 
retrieval, and disDlay is simplified and made faster. With the help of a procedure model it is possible 
for the user to generate a tree without individually placing all of the thousands of leaves, or generate 
a mountain range without plac- ing each individual mountain. By changing a few parameters the user is 
able to display an entire- ly new tree or mountain, and this greatly enhances the terrain images which 
can be displayed. Digitizing real world objects is one way to accurately describe scenes or objects 
for use in a computer graphics environment. The problems which arise when using this method significantly 
inhibit the user. For example, digitization can involve much time and effort (5). Also, the data base 
must be carefully organized in order to facilitate the manipulation of objects in the scene. A second 
method of generation and display would be to interactively generate and store each individual terrain 
element for retrieval and dis- play. However, the effort a user must go through to create and position 
each individual object is far too prohibitive to make it a reasonable al- ternative in this case. As 
in digitization the storage of such a large quantity of data is also a difficult problem. Our research 
efforts in computer graphics have produced techniques that may have important implications for future 
computer image generation systems. We have developed procedure model algo- rithms for data generation 
that automate the creation of environmental features such as trees  154 and textured objects. It is 
now possible to gen- erate and display scenes with realistic looking features that demonstrate increased 
image com- plexity. Instead of images representing several thousand edges, it is now possible to display 
images involving millions of edges. Other groups have undertaken the general pro- blem of terrain display, 
using methods other than procedure models. Dungan, Stenser, and Sutty (3) have produced terrain pictures 
by using a texture tile technique. This two-dimensional technique is capable of assigning a specific 
texture approx- imation to a surface within an image. Although this capability is desirable in many instances, 
there are certain limitations with texture tiles that lead one to seek other techniques for ter- rain 
generation. The fact that only relatively few levels of detail are available limits the num- ber of applications 
suitable for this routine. In many cases a three-dimensional image is re- quired (single trees or a small 
group of trees) to render a terrain image so that it is possible to not only fly over the terrain, but 
also to move through it. Another method of simulating terrain uses source data from the Defense Mapping 
Agency (DMA). This again produces a two-dimensional image, but with the added enhancement of cloud or 
fog simu- lation (4). Using the DMAAC terrain file as the data source, an array of height values is accumu- 
lated and used as the basis for calculating a 2-D representation of terrain. Reflecting the organ- ization 
of the data base, the result resembles a topographic map. Once again, however, the image produced by 
this method is only suitable for very specific instances. The very nature of this method renders it appropriate 
for use in exer- cises in which the observer flies over the terrain. Even though a ground level image 
can be displayed, it is not possible to move through the terrain. Both the texture tiles and DMA techniques 
are better suited for high level fly- overs of terrain rather than ground terrain fly- through or the 
display of single terrain images. PROCEDURE MODELS  Procedure models are a method of creating 2-D and 
3-D object representations. The method is used to create instances of objects from the class of objects 
the procedure model can gener- ate. Procedure models compute the exact speci- fication of an object guided 
by the parameters that serve as input to the model. Procedure models are also capable of sending high-level 
information about the object generated (e.g., the region the object occupies or a measure of the complexity 
of the object) to other procedures such as a data base management system. Proce- dure models can produce 
enormous savings in the data base description of terrain, but at the expense of computation time. Instead 
of storing the representation of a forest, for example, a procedure model description can be used to 
gener- ate the complete representation as needed. Procedure models can be incorporated in a com- prehensive 
object generator language, where the language consists of several different procedure models which are 
called with their respective parameters. Procedure models may or may not be structured around mathematical 
models. Mathemati- cal models are usually approximations of physical properties, while procedure models 
only need to approximate the image of an object within a certain distance range from the observer. For 
example, a mathematical model of a tree trunk could represent every cell in a tree, while a procedure 
model need only to create the image of the visible bark. PREVIOUS WORK  Newell (2) used procedure models 
to generate data that is displayed with a priority list algo- rithm. His basic elements are polygons 
and patch- es. The high-level information returned by his procedure model is the enclosing convex polyhedron 
for the object generated. This information is then used to make the priority algorithm more efficient. 
His procedure models can include infor- mation on attributes of the generated objects that would be costly 
to calculate from the physical data structure, like volumes and surface areas. They can combine different 
representation schemes and different object primitives, like polygons, patches, and lines. View dependence 
can be built into the procedure model to control the level of detail. Also, any necessary clipping can 
be done by the model. In 1974, MAGI (6) produced a color picture of a deciduous tree, made up of several 
thousand leaves, and displayed at a 500x500 resolution. They used a combinatorial geometry technique 
that is similar to our procedure models. Their trees are made up of five parts: stems, primary branch- 
es, secondary branches, twigs, and leaves. The placement and repetition of the various elements are guided 
by statistical properties of different tree types, provided by tables generated from botanical studies 
of trees. The method is opti- mized for their ray-tracing algorithm, with the basic elements (twigs and 
leaves) represented as 2-D or 3-D grids with optical properties stored in tables. Each secondary branch 
on their tree is determined by two primary parameter arrays: length of each branch and the angle of the 
branch in relation to the primary branches. A procedure model was used at CGRG (I) to create images 
of a smoke cloud. A 3-D mathemati- cal approximation of a particulate cloud is gen- erated, and then 
a type of ray-tracing (with the observer at infinity) is used to create a 2-D array of the intensities 
in the image. The pa- rameters used include particle concentration, wind velocity, rate of emission, 
and height of source. SYSTEM SOFTWARE FOR PROCEDURE MODEL GENERATION The Computer Graphics Research 
Group has de- signed and implemented a computer animation system called ANTTS (Hackathorn) which employs 
many in- teractive techniques and presents a unified ap- proach to the graphical display of complex three- 
dimensional data. It is currently in operation on the group's VAX ii/780, using a frame buffer de- signed 
and constructed locally by Marc Howard. The system facilitates the generation, manipula- tion, and disnlay 
of highly detailed data with the aid of interactive devices and a video interface connected to a high 
resolution RGB monitor. The system enables the animator to create a variety of objects (including texture) 
and to specify the nec- essary transformations for an animationsequence. 155 A rim-length processing 
technique, combined with a brute force Z-buffer algorithm, has been designed and implemented tht can 
handle the intersection of several million faces, lines, and points. We are involved in an effort to 
integrate the procedure model technique for data generation into this soft- ware system for the following 
reasons: i. the system has the ability to handle complex data 2. the system can interface easily with 
various techniques used for data generation, includ- ing procedure models  3. the system has already 
been implemented and has been used to produce images of the de- sired complexity.  A PROCEDURE MODEL 
FOR TREES The procedure model for a tree uses many param- eters, some of which define the type of tree 
and others of which control its generation (Fig. i). Parameters which define the tree type are the leaf 
type, the branch structure, the color, and the lim- its on the size and shape of the overall tree. The 
other group of parameters which determine the spe- cific tree generated include the number of leaves 
in a tree, the actual size of the branches, the density of the leaves, and the height of the tree. The 
specific parameters are not independent. For example, the density of the leaves depends on the average 
size of the branches and the number of leaves. One need only specify two of the three parameters and 
the other will be implicit. If den- sity and branch size parameters are specified, the number of leaves 
is already determined. Also, an interval for the specific parameters can be speci- fied, with the value 
on the interval determined by a pseudo random number generator. This reduces the burden on the user of 
specifying each parameter value explicitly. Randomness is also used in combination with the specified 
parameters to produce unique trees of a given species. The approximate location of each branch and leaf 
is determined by the model using the input parameters. The final position and orientation of each individual 
leaf is selected by a random perturbation of the calculated values. The underlying structure, determined 
by the proce- dure model, insures that a realistic image is pro- duced, while the randomness gives the 
unique ap- pearance of the individual trees. One of our implemented procedure models for INPUT ~ PARAMETERS 
~ ~ HIGHLEVEL ~Qnr~m,,~ ~ INFORMATION PRIMITIVE ELEMENTS OR OTHER OBJECTDESCRIPTIONS //{ ~ ~ ' '~'~ ...... 
j # k_OBJECT TT.NSTANCE J INPUTS OUTPUTS trees (of a general type) uses the following param- eters 
: i. number of leaves 2. length of each branch  3. leaf element descrintion  4. color  5. position 
of tree  6. size of leaf element  7. distance between branches  8. distance between leaves  9. random 
number seed  Once these Darameters have been specified, the de- grees of variability of individual 
trees of this type is determined. On input of the parameters, the procedure model for a tree works as 
follows: the leaf elements are organized on the branch ac- cording to the number of leaves, the length 
of the branch, and the size of the leaf. These initial element positions are modified slightly with the 
use of a random number generator to provide a non- uniform orientation (with respect to the rotation 
of the element) and final position on the branch. After the number of branches specified has been created 
as above, they are positioned around the trunk element. (Plates 1 &#38; 2) Their positions and rotations 
are also modified in order to complete the three-dimensional tree model. Finally the tree is positioned 
in the terrain model according to its input parameter, and colors are selected from the color palette 
to be assigned to each prim- itive element according to its type and orienta- tion. The goal of the 
prototype technique used by MAGI was the realistic modeling of vegetation and terrain based on a combinatorial 
geometry tech- nique. Our goal is to use the concept of nroce- dure models to give the same kinds of 
realism while keeping computation times low enough to allow for efficient design and display of terrain 
models. A deciduous tree of about the same com- plexity at 500x500 resolution took approximately i0 minutes 
to create and display on our PDP ii/45. (While direct comparison of computation time could be misleading, 
it is interesting to note that the same kind of tree required approximately 3 hours on MAGI's IBM 360/65.) 
While the basic underlying structure, i.e., building a complex model from simple elements, is the same, 
the technique des- cribed here has certain advantages over a technique such as that used by MAGI. The 
model is created to satisfy a visual requirement, so that little mathematical or biological accuracy 
is necessary. MAGI bases their construction on a statistical model obtained from data from biological 
surveys. Thus, all position, rotation, and magnification information must be included in statistical 
tables to be input into the system. This information is inherent in the procedure models used here. Be- 
cause of the generality available with the use of procedure models, it is relatively easy to create a 
forest of similar or different vegetation types. Also, symmetric or asymmetric crown shades are done 
with equal ease, merely by setting limits on the parameter input. All models created by vari- ous procedures 
are processed by the same visible surface routines, with shading, color, and light source subprograms 
resident. Each model doesn't have to have its own unique processing t@chniques associated with it.  
156 ~e following section describes an extension to require any space for storage, and additional flex- 
the camouflage net suggested by MAGI, but realized ibility is obtained by being able to define these 
as another procedure model definition. element classes of the higher level procedure mod- els dynamically. 
 OTHER TERRAIN  The procedure model technique, as exemplified by the above tree model, takes basic elements 
are a trunk, major branches, and leaf clusters (or leaves and twigs). (Plate 3) Optional elements such 
as fruits, flowers, and seeds can be incorporated into the tree structure. Each procedure model selects 
elements from a given class assigned to it depend- ing on the type of spatial model desired. Thus, bushes, 
flowers, and other vegetation, as well as buildings and other man-made structures, can all be generated 
using different procedure models and ele- ment classes. ~e of the major advantages of a procedure model 
technique is that the class of primitive ele- ments assigned to it does not need to contain prev- iously 
defined elements. This means that a proce- dure model can take as its primitive elements the output of 
one or more other procedure models. This leads to a hierarchical organization that allows for a reasonably 
simple construction of a complete terrain model. For example, Figure 2 shows the organization necessary 
to create a model of a sam- ple terrain. The lowest level element classes (tree elements, building elements, 
and mountain elements) are prev- iously defined and stored on a mass storage device. The higher level 
element classes are comprised of output from procedure models. Thus, they need not  TREE LEAVES~ BRANCHES,I 
TRUNK, BUDS J ~"--- In Figure 2 procedure models PHI, PM2 and PM3 take basic elements that are created 
and stored on the disk to create a complex model. The output of these procedure models are used as the 
primitive elements of procedure models PM4, PH5 and PM6. Finally, PH7 combines these outputs into the 
fin- ished terrain model. Figure 3 demonstrates an example of a program in a sample language that creates 
a forest line. USER INTERFACE In the case of displaying a tree the individ- ual elements which make 
up a tree are created in a data generation system developed by Parent (7). Using a Vector General display 
the user draws leaves of the specific tree to be displayed. The same routine is used to make the trunk 
and branch- es of the tree. When creating the trunk and branches, points indicating the form of the object 
are placed and connected on the VG screen. By ro- tation around the Y axis this shape is made to be 3-D. 
The number of slices around the axis is con- trolled with the use of a dial. This solid of revolution 
capability enables the user to specify the number of faces in the trunk. In order to dis- play an accurate 
picture of a tree, the leaves or leaf clusters as well as the trunk and branches must be created ,to 
resemble the actual elements of the tree as closely as possible. For more  FOREST 1 MOUNTAIN MOUNTAIN 
RANGE TERRAIN MODEL PLAIN ELE. MOUNTAIN ELE. HILL ELE. PM 7 BUILDING BUILDINGS CURVED SURFACE~ FLAT 
SURFACES~ CUBES, SQUARES, I PM 3 E,TC I ] 157 realistic images other elements such as seeds, buds, 
and flowers can be created in data generation. FIGURE 3 FOR I = 1 to N DO: X = I00 LOOP: L = RANDOM 
(I000, 3000) H = RANDOM (S0, i00) S = RANDOM (0, i) CALL TREES (Type = POPLAR, Leaves = L, Height 
= H, Trunk = TRUNK1, Position = (X,IO0), Random = S, Extent = ELIST) X = X + i00 GO TO LOOP END The 
procedure model used can generate trees of various types. The variables L and H specify limits on the 
number of leaves and on the height of the tree, respectively. The variable S is a random number seed 
to the procedure mod- el. The random number seed controls the exact placement of the leaves. The position 
param- eter represents an X-Y mapping onto a previous- ly defined grid. The trunk parameter refers to 
a trunk named TRUNKI, which was previously defined. The extent is a returned value which is a description 
of the approximate extent of the tree (a list of enclosing rectangles). Any unspecified parameters can 
be given default values by the model. The result of this exam- ple is a line of slightly different sized 
pop- lar trees. The creation of terrain models also involves building plains, hills, and mountains. 
One of the more difficult problems confronting the user was the generation and display of a large expanse 
of ground terrain stretching to the boundaries of the object space. A solution to this problem has been 
obtained by the introduction of a data generation routine that displays a quadrilateral grid which can 
be warped in X, Y, or Z to create mountain peaks, or to a lesser degree, rolling hills. The resolution 
of the grid can be specified by using dials which control the number of vertical and hor- izontal slices. 
The greater the number of slices the more complex the image will be. Each inter- section of a vertical 
and a horizontal line can be warped with a cursor. The degree of warp and area affected by the warp is 
again controlled with dials. The routine is thus capable of making a surface with peaks and crevices 
or rolling hills. Once the data is converted into display format a procedure model can be defined that 
will position plains, mountains, and hills to build a high complexity terrain model consisting of millions 
of edges. With the display system's virtual intersection capability mountains can be positioned in such 
a manner that mountain ranges can be formed, or one large mountain can be formed from several smaller 
ones. Likewise, the ability to copy and display objects allows the procedure models, through rota- tion 
and translation, to create a scene consisting of seemingly different objects from one or two similar 
objects. Areas of texture, such as corn fields, can be specified by the user for use in building a scene 
for an animation sequence. By using a procedure model containing distribution in- formation about fields, 
an element designed and drawn in data generation can be distributed on the screen to create a field of 
corn or a forest at a great distance from the observer. CGRG has developed a system of designating colors 
consisting of a "palette" made up of dif- ferent colors, each with many levels. Each element used to 
make up an image on the monitor is assign- ed a number designating a particular color in the palette, 
the different levels of a particular hue, or a combination of various hues and intensities. As an example 
of the latter, consider the case of a tree with autumn foliage. The colors may range from red to brown 
to yellow with values in-between. These levels in each color are affected by a light source command which 
enables the user to place the "sun" or light source at any coordinates for the desired effect. The closer 
the light source is to an object, the more acute the contrast. As the light source is moved, or as the 
object is moved in relation to the light source, the value changes according to the angle from the object 
to the light source. Each leaf, then, can have a differ- ent intensity value. CONCLUDING REMARKS We 
feel that the implications for this kind of generation and display system are far reaching. One strength 
of the system is that it can be used for generating many different types of imagery. As the user desires, 
the pictures may be suitable for applications in architecture, urban planning, demographics, product 
design, visual communica- tions, etc. Once the images are created the sys- tem allows for the animation 
of the objects in a straight forward, easy to understand format so that a relatively inexperienced user 
can produce sophisticated animation with a minimum of effort. The value of this system is increased 
because of its accessibility to the inexperienced user. Likewise, the system itself can be enhanced be- 
cause of the knowledge an animator as an artist can bring to it. The computer is only as good as the 
user, and while the ability to produce high quality complex images is impressive, the anima- tion system 
still must be handled by a competent user who can observe and then simulate these ob- servations on a 
TV monitor. Results of the research performed during the implementation of these techniques are going 
to be used by us in further investigations related to flight simulation. Efficient algorithms, for generating, 
displaying, and manipulating terrain data are essential for realistic imagery to be provided in real 
time. It might be that a mar- riage of techniques such as those described here and descriptive data sources, 
like the Defense Mapping Agency data base, will provide some ini- tial steps toward the realization of 
this effi- ciency. REFERENCES I. Csuri, C., Hackathorn, R., Parent, R., Carlson, W., Howard, H. Towards 
an Interactiv~ High Visual Complexity Animation System, Proceedings Siggraph '79, (Aug. 1979), 289-299. 
 158 2. Newell, M.E., The Utilization of Procedure Models in Digital Image Synthesis. Computer Science, 
Univ. of Utah, UTEC-CSC-76-218, (1975).  3. Dungan, W., Stenser, A., Sutty, G., Texture Tile Considerations 
for Raster Graphics. Sig- graph '78 (Aug. 1978), 130-134.  4. Dungan, W., A Terrain and Cloud Computer 
Image Generation Model. Proceedings Siggraph '79, (Aug. 1979), 143-150.  5. Greenberg, D.P., An Interdisciplinary 
Labora- tory for Graphics Research and Applications. Proceedings Siggraph '77, (July, 1977), 90-97. 
 6. Brooks, J., Murarka, R.S., Onuoha, D., An Ex- tension of the Combinatorial Geometry Technique for 
Modeling Vegetation and Terrain Features. NTIS Report AD-782-883, (Aug. 1974).  7. Parent, R., Chandrasekaran, 
B., Moulding Com- puter Clay. From Pattern Recognition and Arti- ficial Intelligence. C.,H. Chen, Ed., 
Academic Press, (1977), 86-107.  1 59   
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1980</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>807486</article_id>
		<sort_key>163</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-1980</article_publication_date>
		<seq_no>27</seq_no>
		<title><![CDATA[Dynamic scan-converted images with a frame buffer display device]]></title>
		<page_from>163</page_from>
		<page_to>169</page_to>
		<doi_number>10.1145/800250.807486</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=807486</url>
		<abstract>
			<par><![CDATA[<p>A color interactive display system which produces images of three-dimensional polygons and labels on a frame buffer display device is being developed. The entire image is scan converted and written into the frame buffer whenever it is modified. Since an entire image cannot be written into the frame buffer faster than 4.6 frames per second for the particular device chosen, an illusion of continuous motion cannot be supported. However, a rate of 3 frames per second has been found sufficient to provide feedback to continuous user input.</p> <p>In order to achieve this frame rate for a reasonably complex picture, the display device has been microprogrammed to accept run length encoded data and text, and the instruction set of the computer has been extended by microprogramming special-purpose instructions which perform visible surface calculations. These microprograms currently can process a scene which consists of up to 170 polygons at 3 frames per second.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Frame buffers]]></kw>
			<kw><![CDATA[Interactive computer graphics]]></kw>
			<kw><![CDATA[Raster displays]]></kw>
			<kw><![CDATA[Run length encoding]]></kw>
			<kw><![CDATA[Visible surface algorithms]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.4.1</cat_node>
				<descriptor>Scanning</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.1</cat_node>
				<descriptor>Raster display devices</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor>Bitmap and framebuffer operations</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372.10010373</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering->Rasterization</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010583.10010588.10010591</concept_id>
				<concept_desc>CCS->Hardware->Communication hardware, interfaces and storage->Displays and imagers</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010506</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Document scanning</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P331006</person_id>
				<author_profile_id><![CDATA[81544465556]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[J.]]></first_name>
				<middle_name><![CDATA[H.]]></middle_name>
				<last_name><![CDATA[Jackson]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Bell Laboratories, Columbus, OH]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Bouknight W. J. An improved procedure for generation of half-tone computer graphics representations. University of Illinois, Coordinated Science Laboratory, Report R-432, September 1969.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807459</ref_obj_id>
				<ref_obj_pid>800249</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Kaplan, M., and Greenberg, D. P. Parallel processing techniques for hidden surface removal. SIGGRAPH '79, 300-307.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Myers, A, J. An efficient visible surface algorithm. Computer Graphics Research Group, Ohio State University, July 1975.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>362808</ref_obj_id>
				<ref_obj_pid>362759</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Newman, W. M. Display procedures. Comm. ACM 14, 10 (October 1971), 651-660.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Romney, G. W. Computer assisted assembly and rendering of solids. Dept. of Computer Science, University of Utah, TR 4-20, 1970.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>360802</ref_obj_id>
				<ref_obj_pid>360767</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Sutherland, I. E., and Hodgeman, G. W. Reentrant polygon clipping. Comm. ACM 17, 1 (January 1974), 32-42.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Watkins, G. S. A real-time visible surface algorithm. Computer Science Dept., University of Utah, UTECH-CSc-70-101, June 1970.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 DYNAMIC SCAN-CONVERTED IMAGES WITH A FRAME BUFFER DISPLAY DEVICE J. H. Jackson Bell Laboratories Columbus, 
OH 43213 ABSTRACT least 3 times per second to provide the feedback A color interactive display system 
which pro-duces images of three-dimensional polygons and labels on a frame buffer display device is being 
developed. The entire image is scan converted and written into the frame buffer whenever it is modified. 
Since an entire image cannot be written into the frame buffer faster than 4.6 frames per second for the 
particular device chosen, an illusion of continuous motion cannot be supported. However, a rate of 3 
frames per second has been found sufficient to provide feedback to continuous user input. In order to 
achieve this frame rate for a rea-sonably complex picture, the display device has been microprogrammed 
to accept run length encoded data and text, and the instruction set of the computer has been extended 
by microprogramming special-purpose instructions which perform visible surface calculations. These microprograms 
currently can process a scene which consists of up to 170 polygons at 3 frames per second. KEY WORDS 
AND PHRASES: interactive computer graphics, raster displays) visible surface algorithms) frame buffers, 
run length encoding CR CATEGORIES: 8.2 1. INTRODUCTION An interactive three-dimensional color graphics 
system is being developed for use in the Network Operations Center (NOC) operated by A. T. &#38; T. Long 
Lines. It is being added to an existing real-time computer sys-tem which collects and displays data used 
to analyze traffic patterns in the long distance telephone switch- ing network. NOC personnel interactively 
analyze this data and recommend controls to be applied to the switching network to adapt it to abnormal 
traffic con-ditions. The volume of data which NOC personnel must analyze is so large that thresholds 
often must be set to select subsets of the data which form recognizable traffic past.terns. The thresholds 
which are applied to graphic data are adjusted dynamically until a recogniz-able picture is obtained. 
Experiments in the labora= tory have shown that displays must be updated at necessary for this operation. 
In addition to the dynamic adjustment of threshold values) color coding is very useful for revealing 
traffic patterns in the data. The desire for color suggests that a raster display device, rather than 
a vector display device) be used in this application. A frame buffer display device) rather than a scan-converting 
display device, was chosen to display the output. Because visible surface calculations must be performed 
externally to this display device) and because a large volume of data must be written to the device per 
frame) such a device appears to be unsuitable for producing dynamic displays. This paper describes the 
techniques which are being applied in an attempt to overcome this apparent incompatibility. 2. SCENE 
CHARACTERISTICS The system is designed to produce both perspective and orthographic projections of three-dimensional 
scenes which consist of polygons and labels. A two- dimensional display is then simply an orthographic 
pro-jection of such a scene with all polygons and labels parallel to the screen. Because the polygons 
are filled areas) priorities must be assigned to them so that one of them can be chosen to be displayed 
in an area of the image where the projections of several polygons overlap. This representation of a two-dimensional 
scene effectively assigns the highest prior- ity to the polygon which is nearest the observer in any 
such area. 2.1 Polygons Each polygon is described by a sequence of coplanar vertices and is visible 
only from the side from which the first three vertices appear in a counterclockwise order. One side of 
each polygon is invisible in order to reduce the time required to generate the image of a polyhedron 
by eliminating most of the computation which is associated with its back faces. A polygon which is described 
by two vertices is interpreted to be a line segment, and a polygon which is described by only one vertex 
is interpreted to be a point. Both line segments and points are visible from all directions and are assigned 
a fixed thickness in the image, rather than in the scene, to avoid suppression of their images due to 
aliasing. Permission to copy without fee all or part of this material is the title of the publication 
and its date appear, and notice  granted provided that the copies are not made or distributed is given 
that copying is by permission of the Association for  for direct commercial advantage, the ACM copyright 
notice and Co~utlng Machinery. To copy otherwise, or to republish, &#38;#169;]980 ACM 0-8979]-02]-4/80/0700-0]63 
$00.75 163 requires a fee and/or specific permission. 2.2 Labels Each label is one line of text which 
is displayed hor-izontally at a fixed scale and is not hidden by polygons. With these restrictions, a 
simple character generator can display the label. The position of a label is a three-dimensional point 
which is transformed and projected in the same manner as a vertex of a polygon. The lower left hand corner 
o£ the first character in the label is positioned at the projection of this point. In this way, a point 
may be labeled such that the label will move with the point as vari-ous coordinate transformations are 
applied to it in successive frames. 2.3 Lighting model The identically colored adjacent faces of a polyhedron 
can be distinguished only if they are displayed as though they are illuminated from a concentrated light 
source. To allow for such faces, a single source of parallel light is modeled, and all polygons (other 
than lines and points) are assumed to be ideal diffusers. Shading calculations are then simple because 
the image of every polygon is uniformly colored. Further-more, the hue and saturation of the light reflected 
from each polygon depend only on the color code assigned to the polygon, and not on the color of the 
light source (as they would if specularly reflected light were modeled). The application programmer can 
specify both the direction o£ the light source and the percent of illumination from this source. (The 
remain-ing illumination is assumed to be from ambient light).  3. PROCESSES AND PROCESSORS As has been 
mentioned above, a major incentive for dynamic displays is the desire to provide feedback to the setting 
of threshold values which control which data are displayed. Since the data do not tend to be organized 
by area, this feedback is not limited to a local area of the screen. For this reason, the display system 
is designed to update the whole screen each time that an image is produced, rather than to update only 
the portions of the screen in which changes occur. This design is also easier to implement for a double 
buffered frame buffer, which was employed to prevent the user from confusing the display update sequence 
with the dynamics of the image. The four major processes which are performed to pro-duce an image are 
depicted in Figure I. These processes are: Ph Description of the picture (any one of several application 
programs), P2: Coordinate transformation, clipping, and subdivi-sion of polygons into trapezoids, P3: 
Scan conversion and visible surface calculation, and P4: Decoding of run length encoded data and charac-ter 
generation. Run length encoded data, rather than pixel data, are sent to the display device both tO avoid 
expanding these data as part of P3 (which would greatly increase its execution time) and to ensure that 
the computer's bus is not consumed by the transfer of these data. Only one process is needed to both 
decode these run length encoded data and to generate characters because the characters for each frame 
are generated after the images of polygons have been written into the frame buffer. This sequence prevents 
labels from being overwritten by polygon data. Ideally, each of these processes could be performed in 
parallel by separate processors. The processes then would form two pipelines: one involving PI, P2, P3, 
and P4 which produces images of polygons, and one involving Pl, P2, and P4 which adds the labels to the 
image. For economic reasons, however, Pl and P2 were both implemented on one processor, P3 was microprogrammed 
for the same machine, and P4 was microprogrammed for the display device. The major drawback to this type 
of implementation is that coor-dinate transformation must be performed sequentially with visible surface 
calculations. I I Processor P1 ] Macrocode Application Polygons I ~ Labels Processor Cll~ng, Macrocode 
Transformation, Subd v s on Trapezoids ~ TransformedLabels Processor Scan C:r3version, Microcode Visible 
Surface 1  Calculation I  I .unL.n Pdo ,. Character Generation ~ Pixels Figure 1. Organization of Processes 
 4. TRANSFORMATION AND CLIPPING All coordinate transformations, other than perspective projection, are 
applied to polygon vertices and label positions in an order specified by a sequence of calls to display 
procedures (4) in the application program. The resulting data are then clipped behind a plane which is 
parallel to the screen and just in front of the observer, projected, and finally clipped inside planes 
perpendicular to the screen which intersect the edges of a clipping window by a modified re-entrant polygon 
clipping algorithm (6). With this sequence, each clipping operation is simple because it is per-formed 
relative to a clipping plane which is perpendic- ular to an axis of the screen coordinate system. If 
any dipping operation were performed earlier in the sequence, it would be considerably more complicated 
because it would be performed relative a clipping plane with no special orientation. Then, unless a large 
portion of the data in the scene did not project within the dipping window (which is not expected), clipping 
operations would consume much more 164 processing time. 5. SUBDIVISION OF POLYGONS Perhaps the most 
obvious method of formatting data for P3 is to represent each transformed and clipped polygon as a color 
and a list of edges. This represen-tation has been used previously by Watkins (7), Bouk-night (I), and 
others. However, when polygons are represented in this way, the visible surface algorithm must maintain 
a somewhat complicated data structure which allows it to associate pairs of edges on each scan line. 
Furthermore, interpolation of depth values involves a division operation, which is difficult to microprogram. 
An alternative method is to subdivide each polygon into primitive polygons, each with a fixed number 
of sides. Romney (5) and Myers (3), for example, subdi-vided polygons into triangles. The reassociation 
pro-cess is then much simpler, as shown in Figure 2. The edges of the triangles are identified in this 
figure as the "left", "right"~ and "spare" edges. These edges may be identified easily by sorting them 
first in the Y (vertical) direction, and then in the X (horizontal) direction. Whenever either the left 
or right edge ter- minates on a scan line but the other does not, the spare edge is simply substituted 
for the edge which terminated. If both edges terminate on the same scan line, the entire triangle is 
removed from consideration.  Le~~ s Spar~eRight Right Left~ RightLeft pare Scan Line Order Right Spare 
Figure 2. Association of Triangle Edges The partial derivative of the Z (depth) coordinate with respect 
to the X coordinate (with Y constant) may be computed once for an entire triangle. This value may then 
be stored as part of the description of the trian-gle and used by the visible surface algorithm to inter-polate 
Z coordinates. The interpolation performed with this value involves only multiplication and addi-tion, 
and, consequently, is easier to microprogram than an interpolation which involves a division operation. 
5.I Trapezoid primitives Subdividing polygons into triangles, however, increases the complexity of scan 
line calculations. Figure 3a shows a concave polygon subdivided into triangles, whereas Figure 3b shows 
the same polygon subdivided into trapezoids with parallel edges parallel to the scan lines. (The triangle 
in Figure 3b is considered to be a trapezoid with a zero length parallel edge.) The com-plexity of the 
calculation of a scan line increases as the number of elements which intersect the scan line increases. 
Scan line A intersects 4 triangles in Figure 3a, whereas it intersects only two trapezoids in Figure 
3b. Similarly, scan line B intersects 6 triangles in Figure 3a, whereas it intersects only one trapezoid 
in Figure 3b. These examples illustrate that subdivision into triangles generally produces more intersections 
per scan line than does subdivision into trapezoids. In fact, subdivision into trapezoids produces the 
minimum number of intersections, which is equal to the number of intersections of the scan line with 
the original polygon. Line B a. Subdivision Into Triangles i LineB b. Subdivision Into Trapezoids Figure 
3. Methods for Subdividing Polygons Subdivision into trapezoids requires no more storage than does subdivision 
into triangles for polygons with 4 or more vertices. The maximum number of triangles which are formed 
from an n-vertex polygon is n-2, whereas the maximum number of trapezoids is n-h-l, where h is the number 
of horizontal edges. If one assumes that the number of horizontal edges in the image is insignificant, 
the maximum number of tra-pezoids per polygon is greater than the maximum number of triangles. However, 
each trapezoid can be represented by only its two nonhorizontal edges, whereas all three edges of each 
triangle must be stored. The storage required for a trapezoid is com-pared with that required for a triangle 
by Figure 4. (The pointers in the formats are needed to organize the trapezoids or triangles into the 
list structure described in Section 5.2 below.) From this figure, the ratio of storage for a trapezoid 
to that for a triangle is seen to be 2/3. Then, the storage required to sub-divide an n-vertex polygon 
into trapezoids is no greater than that required to subdivide it into trian-gles if n is at least 4. 
If n equals 3, however, subdi- vision into triangles requires less storage. In order to avoid complicating 
scan line calculations, each polygon is subdivided into trapezoids, rather than into triangles. The advantages 
of associating edges before visible surface calculations are performed and simplifying interpolation 
of depth coordinates are then gained without increasing scan line complexity. 165 Trapezoid Triangle 
Le"£1~Z_~.,.., It II Cok~r J Z~y 128"x(Left) 128*x(Right) 128"d~Jdy(Leff) 128"dx/dy(Right) 128"dx/dy(Left) 
--z(Left) 128"dx/dy(Right) 128"dx/dy(Spare) --dz/dy(Left) -- z(Left) --  --az/Ox -- z(Spare) -- --dz/dy(Leff) 
-- dz/dy(Spare) -- Oz/0x -- Figure t). Formats for Trapezoids and Triangles 5.2 Scene structure Although 
the entire image is scan converted to pro-duce each frame) the trapezoids and labels which represent 
parts of the scene which have not changed since the previous frame need not be recomputed. For this reason) 
the scene is organized into "seg-ments", which are similar to the "frames" in Newman's display procedures 
(4). However) each seg-ment for our raster display is implemented by linking together the trapezoids 
and labels which describe the segment) rather than as a display buffer. The data structure which represents 
all of the seg-ments which comprise the image is depicted in Figure 5. The data structure is accessed 
either by segment or by scan line. The two arrays shown are provided for this purpose. Each segment is 
represented by a list of labels) a list of trapezoids (linked by the third pointer in each trapezoid 
block)) and a pointer which associates a procedure which regenerates the segment with the segment, Each 
element of the scan line array is the head of a doubly linked list which con-tains all trapezoids whose 
uppermost edge has a Y coordinat~ either equal to that of the corresponding scan line or between that 
of the corresponding scan line and that of the next scan line. Whenever the procedure associated with 
the segment is called to regenerate it) the corresponding label and trapezoid lists are emptied. The 
forward and back-ward links in the list associated with each scan line facilitate this operation. Each 
label which is gen-erated by the procedure is then inserted at the begin-ning of the label list. Each 
trapezoid which is gen-erated by the procedure is inserted both at the begin-ning of the trapezoid list 
and at the beginning of the appropriate scan line list. In this way, only trapezoids and labels which 
belong to a segment which has changed since the previous frame are recomputed. SegmentArray Label Label 
Labels Trapezoids Procedure I Label Label Tra~ii~s~ Pr°cedure~ I S Trapezoid Trapezoid I I  Trapezoid 
Trapezoid Figure 5. Scene Structure 5.3 Mapping image coordinates to scan conversion coordinates The 
coordinate system with respect to which the image is defined is shown in Figure 6a. The origin of the 
coordinate system is at the center of the image in the plane of the image, and the image occupies an 
area 255 pixels wide by 255 pixels high. This image resolution) rather than a 256 x 256 resolution) was 
chosen to ensure that the length of a closed interval between coordinate values can be stored in an S-bit 
byte. The resolution of the display device9 however) is 512 pixels/line by 256 lines. A horizontal resolution 
of 512 pixels/line) rather than 256 pixels/line) was chosen for two reasons. First) a horizontal resolution 
of 256 pixels/line would permit only 36 characters to be displayed per line, whereas the chosen resolution 
per-mits 73 characters per line. Secondly) the additional horizontal resolution reduces aliasing along 
the edges of polygons which form angles greater than 45 degrees with the horizontal. This effect appears 
to reduce aliasing in the entire image) since human stereoscopic vision has conditioned us to be more 
sensitive to nearly vertical edges than to nearly horizontal edges. The vertical resolution was limited 
to 256 lines) rather than 512 lines) both to reduce the computation time for the image and to avoid the 
slight flicker which is present with the higher resolution. Figure 6b shows the coordinate system into 
which image coordinates are mapped by the system when trapezoids and labels are formed. This coordinate 
sys-tem was chosen to offset the image by half a pixel in the X and Y coordinates as shown in Figure 
6c. This offset causes truncation of coordinate values to integers to effectively round each point to 
the nearest pixel. 166 -5- Y = 0.5- Y = 127.5--T-- L Y = -t27.5--Y = 255.5 - I X = -127.5 X = 127.5 
X = 0,5 X = 510,5 -2~_~ Z< 2 ~ O<Z<2 3z a. Image Coordinates b. Scan Conversion Coordinates 0.5 Pixels 
i Screen Area -~1~i LL____ Image Given to IL ...... ;[__~ o,S:; inx ~1°: verter  o,P,x,is~ ~ ~tsP,x°is 
c. Offset of Image to Accomplish Rounding Figure 6. Mapping of Image Coordinates to Scan Conversion Coordinates 
 6. VISIBLE SURFACE ALGORITHM Visible surface calculations are performed by a microprogram which was 
written to add two instruc-tions to the processor 's instruction set. One of these instructions initializes 
the microprogram to generate a new image, whereas the other produces the next scan line in the image. 
The former instruction accepts a background color code as an operand and empties a list of active trapezoids 
(i. e., trapezoids which inter-sect the current scan line) which is stored in the RAM which contains 
the microcode. The latter instruction accepts the address of a buffer into which run length encoded data 
for the scan line is to be placed and a pointer from the scan line array as operands. It then produces 
the scan line by the following procedure: 1. Enter new trapezoids into the active list of tra-pezoids. 
 2. Adjust the left X, left Z and right X values of each active trapezoid to the proper values for this 
scan line9 and remove each trapezoid which has been completed from the active list. 3. Sort the active 
trapezoids by left X (bubble sor% since the order is likely to be the same as on the previous scan line). 
 4. Generate run length encoded data for the scan line.  The last of these steps examines the sorted 
active trapezoid list to subdivide the scan plane (i. e. the plane perpendicular to the image plane which 
contains the scan line) into "spans". The scan plane is subdi-vided parallel to the Z axis at each X 
coordinate which is the X coordinate of the left or right edge of an active trapezoid. Spans are formed 
from left to right so that each span may be processed as it is pro-duced to yield run length encoded 
data in the proper order. As each span is produced, it is classified as being one of the four types shown 
in Figure 7a by the following tests: 1. If the span intersects no trapezoids, it is classi-fied "empty". 
 2. Otherwise, if the span intersects only one tra-pezoid, it is classified "single". 3. Otherwise, 
if the same trapezoid is in front of all others which intersect the span at both the left and right edges 
of the span, the span is classified "multiple". 4. Otherwise, the span is classified "intersecting". 
 A trapezoid whose left edge intersects the right edge of a span is not considered to intersect the 
span. , , t i Empty Single Multiple Single Intersecting Single a. Example Scan Plane i , i i , Pi,e, 
b. Special Case of Intersecting Span Figure 7. Types of Span s Each of the first three types of spans 
produces only one color in the image. The empty span produces the background color, the single span produces 
the color of the trapezoid which intersects it, and the multiple span produces the color of the trapezoid 
found to be in front at both the left and right edges of the span. The intersecting span, however, may 
produce more than one color in the image. This type of span is processed by further subdividing it to 
form spans of the other three types. An intersecting span results from at least two polygons intersecting 
elsewhere than at their edges. Since this event is not expected to be common in our application, no attempt 
is made to process it effi-ciently. Instead, the processing is designed to capital-ize on microcode used 
elsewhere in the visible surface algorithm and to avoid a division operation. The intersecting span is 
subdivided by using a half interval method with the span classification tests listed above to find the 
span whose left edge is the left edge of the intersecting span and whose right edge is as far right as 
possible such that the new span is not an intersecting span. Mter the span formed in this manner has 
been processed, normal 167 -7- checkerboard pattern. The time required by P4 was derived from measurements 
of time required to pro-cess various run lengths and the geometry of the image, whereas the other times 
were measured directly. Although P3 executes faster than P% the two processes together require more time 
than P4 because they are not completely overlapped. o P3 and P4 3 Frames, sec. P4 0.3- P3 o E k- O.1-- 
 0.0 i i J i i i 100 200 300 400 500 Number of Trapezoids Figure ll. Two-dimensional Measurements Figure 
12 shows the time required for P3 and P4 to process scenes similar to that shown in Figure 10. These 
scenes were produced by varying the number of values of both independent variables from I to II. The 
time required by P4 was not measured because the geometry of the image is complicated. However, the other 
measurements indicate that P3 is responsible for most of the time consumed by P3 and pie. The time required 
for P3 to process each of these scenes increases rapidly with the number of trapezoids chiefly because 
the average depth of these scenes increases with the number of trapezoids. These measurements show that 
the time required by P3 is very sensitive to the average depth in the scene. This sensitivity might be 
reduced significantly by modifying the algorithm for P3 to utilize depth coherence between scan lines. 
8. CONCLUSION The system being developed uses a frame buffer display device. The particular device selected 
is able to rewrite its image memory at only #.6 frames/second. Since considerations external to the disptay 
device itself have caused us to not utilize the individual pixel capability of the frame buffer display, 
a better choice of display device might be one which decodes run length encoded data and generates charac- 
ters as part of its video refresh process. Such a display device could run considerably faster and might 
cost less because a much smaller memory than a 3.0 P3 and P4 2.0 P3 6 I-1.O 7 O.O 16o 2~o a60 4~o soo 
Number of Trapezoids Figure 12. Three-dimensional Mesurements frame buffer could store the image. In 
order to utilize a faster display device, images must be updated at a higher rate. By designing the system 
for two (smaller) processors, one which exe-cutes P1 and P2, and another which executes P3, coordinate 
transformation may be performed in paral-lel with visible surface calculations. Beyond this, an additional 
processor might be allocated to P1, and multiple processors might be allocated to P2 and P3. For example, 
each segment which is formed by P2 might be assigned to a separate processor, and por-tions of P3 for 
various parts of the image might be assigned to separate processors, as has been suggested by Kaplan 
and Greenberg (2). REFERENCES (1) Bouknight W. 3. An improved procedure for gen-eration of half-tone 
computer graphics represen-tations. University of Illinois, Coordinated Sci-ence Laboratory, Report R-432, 
September 1969. (2) Kaplan, M., and Greenberg, D. P. Parallel pro-cessing techniques for hidden surface 
removal. SIGGRAPH '79, 300-307. (3) Myers, A, J. An efficient visible surface algo-rithm. Computer Graphics 
Research Group, Ohio State University, 3uly 1975. (4) Newman, W. M. Display procedures. Comm. ACM 14, 
10 (October 1971), 651-660. (5) Romney 9 G. W. Computer assisted assembly and rendering of solids. Dept. 
of Computer Science, University of Utah, TR 4-20, 1970. (6) Sutherland, I. E., and Hodgeman, G. W. Re-entrant 
polygon clipping. Comm. ACM 17, 1 (3anuary 1974), 32-42. (7) Watkins, G. S. A real-time visible surface 
algo-rithm. Computer Science Dept., University of Utah, UTECH-CSc-70-101, 3une 1970.  169   
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1980</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>807487</article_id>
		<sort_key>170</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-1980</article_publication_date>
		<seq_no>28</seq_no>
		<title><![CDATA[Computer graphics in television (Panel Session)]]></title>
		<page_from>170</page_from>
		<doi_number>10.1145/800250.807487</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=807487</url>
		<abstract>
			<par><![CDATA[<p>Our society is increasingly relying on symbolic and imaginal communication to augment written and spoken language (advertising graphics, logos and corporate I.D.s, satellite weather maps, international traffic, dashboard symbols, etc.). Nowhere are graphics and sophisticated imagery being more widely and innovatively used than in television production. Greater availability of computer graphics and a strong trend in television technology towards digital electronics makes this an application of high synergy and potential.</p> <p>The purpose of this panel is to bring together several top system designers and leading graphic designers in television to exchange points of view about this exciting application area. Panelists will show and discuss recent on-air uses of electronic graphics and will exchange ideas about what is needed, possible, important, and useful for the future.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P333314</person_id>
				<author_profile_id><![CDATA[81100366482]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Richard]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Shoup]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Aurora Imaging Systems]]></affiliation>
				<role><![CDATA[Chairman]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P334104</person_id>
				<author_profile_id><![CDATA[81100060329]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Tom]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Klimek]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Computer Creations]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P331976</person_id>
				<author_profile_id><![CDATA[81332497813]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Larry]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Evans]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Ampex]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP31072654</person_id>
				<author_profile_id><![CDATA[81100576286]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Peter]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Black]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[XIPHIAS Group]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P330609</person_id>
				<author_profile_id><![CDATA[81100575853]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[H.]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bley]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[ABC Sports]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14209094</person_id>
				<author_profile_id><![CDATA[81332534878]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[D.]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Weise]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[KTVU Television, Oakland]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 COMPUTER GRAPHICS IN TELEVISION Panel Introduction Our society is increasingly relying on symbolic and 
imaginal communication to augment written and spoken language (advertising graphics, logos and corporate 
I.D.s, satellite weather maps, international traffic, dashboard symbols, etc.). Nowhere are graphics 
and sophisticated imagery being more widely and innovatively used than in television production. Greater 
availability of computer graphics and a strong trend in television technology towards digital electronics 
makes this an application of high synergy and potential. The purpose of this panel is to bring together 
several top system designers and leading graphic designers in television to exchange points of view about 
this exciting application area. Panelists will show and discuss recent on-air uses of electronic graphics 
and will exchange ideas about what is needed, possible, important, and useful for the future. Chairman: 
Richard Shoup, Aurora Imaging Systems Panelists: Tom Klimek, Computer Creations Larry Evans, Ampex Peter 
Black, XIPHIAS Group H. Bley, ABC Sports D. Weise, KTVU Television Oakland Permission to copy without 
fee all or part of this material is granted provided that the copies are not made or distributed for 
direct commercial advantage, the ACM copyright notice and the title of the publication and its date appear, 
and notice is given that copying is by permission of the Association for Computing Machinery. To copy 
otherwise, or to republish, requires a fee and/or specific permission. 01980 ACM 0-89791-021-4/80/0700-0170 
$00.75 170
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1980</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>807488</article_id>
		<sort_key>171</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-1980</article_publication_date>
		<seq_no>29</seq_no>
		<title><![CDATA[Optical printing in computer animation]]></title>
		<page_from>171</page_from>
		<page_to>177</page_to>
		<doi_number>10.1145/800250.807488</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=807488</url>
		<abstract>
			<par><![CDATA[<p>The optical printer can be considered as an optical analog computer, which can perform geometric transformations and simple arithmetic operations on pictures very efficiently. The principles of operation of the printer are explained, and many of its applications to computer animation are listed and discussed briefly.</p> <p>Two techniques are discussed in detail: the use of high contrast masks to suppress the bright spots where two lines of different colors cross, and the use of continuous tone masks and multiple exposures to create realistic transparency at low cost.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Computer animation]]></kw>
			<kw><![CDATA[Mask]]></kw>
			<kw><![CDATA[Matte]]></kw>
			<kw><![CDATA[Optical printer]]></kw>
			<kw><![CDATA[Transparency]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Animation</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Color, shading, shadowing, and texture</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010384</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Texturing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Performance</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP39044822</person_id>
				<author_profile_id><![CDATA[81100480335]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Nelson]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Max]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Lawrence Livermore Laboratory]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P331442</person_id>
				<author_profile_id><![CDATA[81100462506]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[John]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Blunden]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Lawrence Livermore Laboratory]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>807437</ref_obj_id>
				<ref_obj_pid>965103</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Appel, Arthur, Rohlf, F. James, and Stein, Arthur J., The haloed line effect for hidden line elimination. Computer Graphics Vol. 13, No. 2 (1979) pp. 151-157.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Fielding, Raymond. The techniques of special effects cinematography. Hastings House, New York (1968).]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807438</ref_obj_id>
				<ref_obj_pid>965103</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Kay, Douglas S. and Greenberg, Donald., Transparency for computer synthesized images. Computer Graphics Vol. 13, No. 2 (1979) pp. 158-164.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Maddalozzo, John. The simplification of posterization. Audio Visual Communications Vol. 14, No. 1 (1980) p. 40.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807439</ref_obj_id>
				<ref_obj_pid>965103</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Max, Nelson. ATOMLLL: ATOMS with shading and highlights. Computer Graphics Vol. 13, No. 2 (1979) pp. 165-173.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Max, Nelson. ATOMLLL: A three-d opaque molecule system (Lawrence Livermore Laboratory Version). UCRL 52645, Lawrence Livermore Laboratory (1979).]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Moscovitz, Howard S., OPAL A computer language for the control of optical printers. SMPTE Journal Vol. 89, NO. 3 (1980) pp. 181-187.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Salt, Brian G., Mathematics in aid of animation, in The technique of film animation, by Halas and Manvell, Hastings House, New York (1968) pp. 333-345.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807419</ref_obj_id>
				<ref_obj_pid>965103</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Whitted, Turner. An improved illumination model for shaded display, to appear in Communications of the ACM. Abstract in Computer Graphics, Vol. 13, No. 2 (1979) p. 14.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
    A high contrast negative A 3 was printed from 9. Whitted, Turner. An improved illumination model 
A2, and continuous tone negative transmittance masks B 3 and C 3 were printed from B 2 and C 2. The four 
printing passes described above are then: 1) print the background, with A 3 and B 3 as masks, (figure 
13), 2) add Ai, with B 3 as a mask, (figure 14), 3) add Bi, with C 3 as a mask, (figure 15), and 4) add 
C 1, with no mask, (figure 16). In pass 1, the two masks were bipacked in the main projector, while the 
background was printed from the aerial image projector. Figure 17, which does not distinguish the front 
surfaces, was printed by using passes 1) and 2) above, but replacing passes 3) and 4) by 3') add Bl, 
with no mask. The ATOMLLL system provides a CPU and IO efficient means of splitting the hidden-surface 
and shading computations between two computers. This splitting would be impossible if the shader required 
a depth ordering of the surfaces to produce pixel-by-pixel transparency. Thus the optical printer is 
essential for the efficiency of the above procedure. References 1. Appel, Arthur, Rohlf, F. James, and 
Stein, Arthur 3., The haloed line effect for hidden line elimination. Computer Graphics Vol. 13, No. 
2 (1979) pp. 151-157. 2. Fielding, Raymond. The techniques of special effects cinematography. Hastings 
House, New York (1968). 3. Kay, Douglas S. and Greenberg, Donald., Transparency for computer synthesized 
images. Computer Graphics Vol. 13, No. 2 (1979) pp. 4. Maddalozzo, John. The simplification of postepization. 
Audio Visual Communications Vol.  158-164. 14, No. 1 (1980) p. 40. 5. Max, Nelson. ATOMLLL: ATOMS with 
shading and highlights. Computer Graphics Vo|. 13, No. 2 (1979) pp. 165-173. 6. Max, Nelson. ATOMLLL: 
A three-d opaque molecule system (Lawrence Livermore Laboratory Version). UCRL 52645, Lawrence Livermore 
Laboratory (1979). 7. Moscovitz, Howard S., OPAL A computer language for the control of optical printers. 
SMPTE Journal Vol. 89, NO. 3 (]980) pp. 181-187. 8. Salt, Brian G., Mathematics in aid of animation, 
in The technique of film animation, by Halas and Manvell, Hastings House, New York (1968) pp.  333-36,5. 
 fop shaded display, to appear in Communications of the ACM. Abstract in Computer Graphics, Vol. 13, 
No. 2 (1979) p. 14. NOTICE This report was prepared as an account of work sponsored by the United States 
Government. Neither the United States nor the United States Department of Energy, nor any of their employees, 
nor any of their contractors, subcontractors, or their employees, makes any warranty, express or implied, 
or assumes any legal liability or responsibility for the accuracy, completeness or usefulness of any 
information, apparatus, product or process disclosed, or represents that its use would not infringe privately-owned 
rights. Reference to a company or product name does not imply approval or recommendation of the product 
by the University of California or the U.S. Department of Energy to the exclusion of others that may 
be suitable. 177
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1980</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>807489</article_id>
		<sort_key>178</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-1980</article_publication_date>
		<seq_no>30</seq_no>
		<title><![CDATA[Adaptation of scan and slit-scan techniques to computer animation]]></title>
		<page_from>178</page_from>
		<page_to>181</page_to>
		<doi_number>10.1145/800250.807489</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=807489</url>
		<abstract>
			<par><![CDATA[<p>The adaptation and generalization of scan and slit-scan animation stand techniques for use in computer generated animation is discussed. Scan and slit-scan techniques are based on moving artwork, camera, and, for slit-scan, a thin aperture while the camera shutter is open. These processes can be described as selectively sampling an environment over time and recording the result as a single image. Sequences of such images form the animated film. Rather than use mechanical means to accomplish this, it is possible to develop algorithms which mimic this processes but are based on sampling dynamic environment descriptions to generate computer produced images.</p> <p>The use of computer graphics allows these techniques to be generalized in ways difficult or impossible even with very elaborate animation stands. Use of multiple independent scanning apertures and three-dimensional environments are natural generalizations.</p> <p>The exact algorithms used depend on the characteristics of the graphics systems used. An approach based on using a real-time shaded graphics system and an approach using frame buffer systems are outlined. The first approach can also be applied to refresh vector graphics systems.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Animation]]></kw>
			<kw><![CDATA[Slit-scan animation]]></kw>
			<kw><![CDATA[Special effects generation]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.4.1</cat_node>
				<descriptor>Scanning</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Animation</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010506</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Document scanning</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP14111766</person_id>
				<author_profile_id><![CDATA[81100304539]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Frederic]]></first_name>
				<middle_name><![CDATA[I.]]></middle_name>
				<last_name><![CDATA[Parke]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Computer Engineering, Case Institute of Technology, Case Western Reserve University, Cleveland, Ohio]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[D. Trumbell, an article on the "2001: A Space Odyssey" star-gate sequence, American Cinematographer, October 1969.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[L. G. Sims, "Animation, Scan, and Slit Scan Photography," Proc. 25th Intl. Tech. Comm. Conf., Dallas, May 1978.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Z. Perisic, The Animation Stand, Focal Press, New York, 1976.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[F. Parke, "The Case Shaded Graphics System," Proc 25th, Intl. Tech. Comm. Conf., Dallas, May 1978. Also Tech. Rep., Computer Engineering, Case Institute of Technology.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 ADAPTATION OF SCAN AND SLIT-SCAN TECHNIQUES TO COMPUTER ANIMATION Frederic I. Parke Computer Engineering 
Case Institute of Technology Case Western Reserve University Cleveland, Ohio 44106 ABSTRACT The adaptation 
and generaliza- - {Ton-6r-scan and slit-scan animatioh stand techniques for use in computer generated 
animation is discussed. Scan and slit- scan techniques are based on moving art- work, camera, and, for 
slit-scan, a thin aperture while the camera shutter is open. These processes can be described as selec- 
tively sampling an environment over time and recording the result as a single im- age. Sequences of such 
images form the animated film. Rather than use mechanical means to accomplish this, it is possible to 
develop algorithms which mimic this processes but are based on sampling dynam- ic environment descriptions 
to generate computer produced images. The use of computer graphics allows these techniques to be generalized 
in ways difficult or impossible even with very elaborate animation stands. Use of multi- ple independent 
scanning apertures and three-dimensional environments are natural generalizations. The exact algorithms 
used depend on the characteristics of the graphics sys- tems used. An approach based on using a real-time 
shaded graphics system and an approach using frame buffer systems are outlined. The first approach can 
also be applied to refresh vector graphics sys- tems. CR Index -8.2, 3.41 Ke[ Words -Animation, Slit-Scan 
Anima- tion, Special Effects Generation Permission to copy without fee all or part of this material 
is granted provided that the copies are not made or distributed for direct co~.ercial advantage, the 
ACM copyright notice and the title of the publication and its date appear, and notice is given that copying 
is by permission of the Association for Computing Machinery. To copy otherwise, or to republish, requires 
a fee and/or specific permission. ~980 ACM 0-89791-021-4/80/0700-0178 $00,75 Introduction Scan and 
slit-scan animation tech- niques have been and continue to be used to produce visually interesting and 
often spectacular animated sequences [i]. We begin by discussing what the terms scan and slit-scan mean 
in the context of ani- mation produced using automated animation stand techniques. We then look at how 
these basic ideas may be adapted for use in computer generated animation. Scan Animation In scan animation, 
each frame is a time lapse photograph of a changing en-vironment [2]. The environment usually consists 
of some type of flat art (two-dimensional drawing, etc.) positioned on an animation stand. Changes in 
this en-vironment usually consist of moving the art work and/or moving the camera posi- tion. When the 
artwork or camera position are moved during exposure, this has the effect of leaving a trail or smear 
along the motion trajectory. Say we want a scene in which an ob- ject appears at point A, then moves 
to point B, leaving a trail from point A to B and finally the trail catches up to the object at point 
B. This is accomplished by positioning the object at position A and exposing the first frame of the 
scene. For the next frame, the object is again positioned at point A and then, with the shutter open, 
moved an increment toward point B. For the next frame the object is again positioned at point A and with 
the shutter open moved two increments toward point B. In subsequent frames, the object starts at point 
A and moves further toward point B until the frame is reached in which the object moves all the way from 
A to B. In each of these frames the object has the same starting point, A, but a dif- ferent ending position. 
Now, to make the trail catch up to the object at point B, the movement on each frame will have the same 
ending position, B, but a different beginning position between A and B. 178 This form of animation depends 
on having a mechanism to precisely control motion within each frame and from frame to frame. This 
is usually done using rather elaborate automated animation stands. Slit-Scan Animation In slit-scan 
animation, each frame is also a time lapse photgraph, but, here the environment is photographed through 
a nar- row slit or apertur~ which is moved during exposure [2,3]. With this approach, at any instant 
only a narrow region of the artwork is visible through the slit. If the slit is moved parallel to the 
 film plane, with the camera and artwork stationary, the slit will "paint" an exact image of the art 
work on the film. But, if either or both the camera or the art- work are also moving, the slit will 
 "paint" a distorted image on the film. If the artwork is moving in the same direction as the slit, 
but at a slower rate, the image will be stretched out. If the artwork and slit move in opposite directions, 
the image will be compressed. If the artwork moves perpendicular to the slit motion, the image will be 
slanted or skewed. Various combinations of camera, slit, and artwork motions produce a wide variety 
of visual effects. Psuedo-perspective can be.~dded by zooming the camera. Adaptation to Computer Animation 
 The essence of the techniques described above is to produce a sequence of images where each image 
Js the result of summing light energy from a carefully controlled dynamic environment. This can be 
thought of as summing the results of a continuous sampling process. When we move this idea over into 
the realm of digital computers and computer graphics we end up with algorithms which are in essence techniques 
for producing sequences of images where each image is the result of summing discrete samples of a carefully 
controlled dynamic environ- ment. We in effect substitute computer graphics technology for the mechanical 
and optical technology of the animation stand. In this domain, rather than manipu- lating physical 
parameters such as camera position, focal length, and artwork motion we are concerned with the manipulation 
of coordinate systems, clipping window param- eters, shading parameters, perspective transformations, 
etc. The following sections outline two methods for summing the computed environ- ment samples to form 
the desired images. The first of these uses film as the sum- ming or integrating mechanism, the second 
makes use of an image or frame buffer. ~!~ ~!l_~ ~{ ~h~ ~!~ Mechanism If one has access to a graphics 
sys- tem that can generate refresh images (shaded or vector) relatively rapidly and provided there is 
some provision for pho- tographing these images, then scan anima- tion can be reasonably produced by 
gen- erating for each frame of the animation a sequence of !images, each image being a sample of the 
changing environment. As these samples are produced for each frame, the camera shutter is left open. 
The film acts as the mechanism which sums the sam- ples into the desired image for that frame. The program 
controlling this process needs precise control over the display refresh mechanism. Each sample contribut- 
ing to the image is displayed a small integer number of refresh cycles. The camera lens aperture and 
the number of refresh cycles used determine the amount of light reaching the film for that sam- ple. 
 Slit-scan images are produced by introducing a scanning aperture or aper- tures. These apertures change 
position between samples, in effect painting the desired image on the film. There is usu- ally a fair 
amount of overlap between suc- cessive aperture positions. A single rectangular slit can be gen- erated 
by simply specifying appropriate clipping window parameters which change from sample to sample. Multiple 
slits can be produced by introducing an obscuring surface (po- lygon) , between the viewer (camera} and 
the environment, which has "holes" in it. By changing the positions of these holes between samples, the 
slits scan over the environment. Given a frame buffer system, the frame buffer memory may be used as 
the summing mechanism. For each sample con- tributing to the desired image, pixel values are computed 
and written to the frame buffer memory. In normal operation these values are simply written into the 
memory replacing previous contents. How- ever, for scan and slit-scan modes of operation, values written 
to the memory re added to previous memory contents. his summing operation creates the desired image. 
Since most frame buffer systems have only a few bits, typically 8, to represent each pixel value, care 
must be taken in choosing the intensity range of the com- 179 puted sample pixels. Since it is quite 
likely that a given pixel will be written into for successive samples in a single frame, the summed pixel 
value may saturate (reach the maximum pixel value). This saturation is not necessarily bad since it 
corresponds to the analogous saturation which occurs in film. The saturation for the film case is controlled 
by lens aper- ture and refresh cycles. Saturation for the frame buffer case is controlled by choosing 
the computed pixel intensity range. Samp] ing Since we have substituted computer graphic techniques, 
which are inherently discrete, for the continuous optica] and mechanical properties of the animation 
stand, we are bound to have discrete sam- pling artifacts in the images produced. Whether this is good 
or bad depends on the type of effect that is desired in the images generated. If one is interested in 
mimicing the smooth, fluid images produced by conventional scan and slit-scan tech- niques this will 
likely require computing a large number of closely spaced samples for each image. The number of samples 
required for a given effect requires ex- perimentation and depends on the composi- tion of the environment 
and the range of change within each frame. Generalizations The use of computer graphics rather than 
animation stand technology allows these techniques to be generalized in ways difficult or impossible 
with even very complex mechanical animation mechanisms. The constraints of two-dimensional artwork are 
immediately disposed of. The image environment then becomes a space of any or even mixed dimension containing 
collec- tions of objects limited only by the ima- gination. The objects may move in multi- ple independent 
coordinate systems or might perhaps be moving relative to each other in very subtle, complex relation- 
ships. In addition to complete control over object motions, there is complete control over viewer (camera) 
position, orientation, and field of view. For shad- ed images, one also has control over lighting, shading, 
texture, etc. The slit-scan concept easily general- izes to multiple slits or apertures moving in 
concert or independently. These aper- tures may be dynamic, changing in shape or size throughout a 
scan. Also, the reader, I am sure, will discover additional ways to generalize these techniques. Figure 
1 shows scan and slit scan images produced using the Case Shaded Graphics System [4] with film as the 
sum- minq mechanism. These images are all variations on the four interlocking squares which form the 
basis of this year's SIGGRAPH logo. The top row of scan imaqes were pro- duced by rotating while widening 
the field of view (zooming back). The left image has 50 samples, the center image has 100 samples, and 
the right image has 200 sam- ples. The second row of scan images were produced by moving the interlocked 
squares toward the top of the image while rotating and zooming back. The left image has 50 samples, the 
center image 100 samples, and the right image 200 samples. The third row of images were produced using 
the slit-scan technique. The left and center images are the result of moving a narrow horizontal aperture 
from the bot- tom to the top of the image while zooming back. The right image is similar except that 
the squares were rotated while scan- ning. Notice the introduction of nice curves into the rendering 
of objects de- fined as sets of polygons. The bottom row of images are scan images. The left and center 
ones were done by moving the squares from left to right and from bottom to top while zooming back. The 
bottom right image is the result of translation, rotation, and zoom- ing in (narrowing the fie]d of view). 
 References [i] D. Trumbell, an article on the "2001: A Space Odyssey" star-gate sequence, American 
Cinematographer, October 1969. [2] L. G. Sims, "Animation, Scan, and Slit Scan Photography," Proc. 
25th Intl. Tech. Comm. Conf., Dallas, May 1978. [3] Z. Perisic, The Animation Stand, Focal Press, New 
Yo~k~-I~7~? ........ [4] F. Parke, "The Case Shaded Graphics System," Proc 25th, Intl. Tech. Comm. Conf., 
Dallas, May 1978. Also Tech. Rap., Computer Engineering, Case Institute of Technology. 181
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1980</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>807490</article_id>
		<sort_key>182</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-1980</article_publication_date>
		<seq_no>31</seq_no>
		<title><![CDATA[Real time animation playback on a frame store display system]]></title>
		<page_from>182</page_from>
		<page_to>188</page_to>
		<doi_number>10.1145/800250.807490</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=807490</url>
		<abstract>
			<par><![CDATA[<p>A frame store display station capable of generating real time animation effects using filled polygonal shapes is described. The system consists of a host microcomputer, a high performance frame store display controller and a microprogrammed graphic command interpreter. Real time performance is attained using a combination of software, hardware and a systems approach. The display station can be used to interactively test and view simple animation sequences. Some proposals for further increasing performance are discussed.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Animation]]></kw>
			<kw><![CDATA[Computer graphics]]></kw>
			<kw><![CDATA[Frame store]]></kw>
			<kw><![CDATA[Graphic processor]]></kw>
			<kw><![CDATA[Polygon fill]]></kw>
			<kw><![CDATA[Raster scan]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Animation</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.1</cat_node>
				<descriptor>Raster display devices</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>J.7</cat_node>
				<descriptor>Real time</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010520.10010570</concept_id>
				<concept_desc>CCS->Computer systems organization->Real-time systems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010476</concept_id>
				<concept_desc>CCS->Applied computing->Computers in other domains</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010583.10010588.10010591</concept_id>
				<concept_desc>CCS->Hardware->Communication hardware, interfaces and storage->Displays and imagers</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372.10010373</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering->Rasterization</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Performance</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP14097486</person_id>
				<author_profile_id><![CDATA[81100255445]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Bryan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ackland]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Bell Laboratories, Holmdel, New Jersey]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39038846</person_id>
				<author_profile_id><![CDATA[81100349806]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Neil]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Weste]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Bell Laboratories, Holmdel, New Jersey]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[ROUGELOT, R.S. "The General Electric Computer Color TV Display", Pertinent Concepts in Computer Graphics, (M. Faiman and J. Nievergelt Editors), University of Illinois Press, 1969.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807375</ref_obj_id>
				<ref_obj_pid>800248</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[WEINBERG, R. "Computer Graphics in Support of Space Shuttle Simulation", Siggraph '78 Proceedings, Aug. 1978, pp. 82-86.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>563283</ref_obj_id>
				<ref_obj_pid>563274</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[MYERS, A. "A Digital Video Image Storage and Retrieval System", Computer Graphics, Vol. 10 No. 2, Summer 1976, pp. 45-50.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>563870</ref_obj_id>
				<ref_obj_pid>563858</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[HACKATHORN, R.J. "Anima II - A 3D Color Animation System", Siggraph '77 Proceedings, July 1977, pp. 54-64.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807418</ref_obj_id>
				<ref_obj_pid>800249</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[SHOUP, R. "Color Table Animation", Siggraph '79 Proceedings, Aug. 1979, pp. 8-13.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807424</ref_obj_id>
				<ref_obj_pid>800249</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[BAECKER, R. "Digital Video Display Systems and Dynamic Graphics", Siggraph '79 Proceedings, Aug. 1979, pp, 48-56.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[WESTE, N. and ACKLAND, B. "GUMBI - A Graphic User Microprogrammable Bit Slice Interpreter", IEEE Compcon Fall '79 Proceedings, Sept. 1979, pp. 232-237.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807380</ref_obj_id>
				<ref_obj_pid>800248</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[LIEBERMAN, H. "How to Color in a Coloring Book" Siggraph '78 Proceedings, Aug. 1978, pp. 111-116.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[PAVLIDIS, T. "Filling Algorithms for Raster Graphics", Dept. of Elec. Eng. and Computer Science, Princeton University. Report no. 238, January 1978.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 REAL TIME ANIMATION PLAYBACK ON A FRAME STORE DISPLAY SYSTEM Bryan Ackland Neil Weste Bell Laboratories 
Holmdel, New Jersey 07733 certainly beyond the I/O capabilities of most general ABSTRACT A frame store 
display station capable of generating real time animation effects using filled polygonal shapes is described. 
The system consists of a host microcom- puter, a high performance frame store display controller and 
a microprogrammed graphic command interpreter. Real time performance is attained using a combination 
of software, hardware and a systems approach. The display station can be used to interactively test and 
view simple animation sequences. Some proposals for further increasing performance are discussed. Key 
Words: Computer graphics, Raster scan, Frame store, Graphic processor, Polygon fill, Animation. CR Categories: 
3.24, 3.89, 6.22, 6.35, 8.2. 1. INTRODUCTION The raster scan CRT is unique within the field of graphic 
output devices, being the only viable display medium capable of generating moving, filled colored shapes. 
When coupled to a frame store, a refresh memory structure in which one or more bits are used to represent 
each picture element, it provides a relatively low cost, flexible graphics display device capable of 
rendering a wide range of static images. The naked frame store display does not, however, readily lend 
itself to dynamic picture generation. For moderate spa- tial and/or intensity resolutions, a large number 
of memory bits per frame are required. Typically, many of these bits must be changed to substantially 
modify a pic- ture from one frame to the next, and this necessarily limits the speed with which frame 
store images can be updated. Consider, for example, a frame store display system in which a general purpose 
computer attempts dynamic image generation into a 640 x 480 frame store. Assum-ing that 8 bits are used 
to represent each pixel (picture element), that 50% of all pixels need to be updated each frame and that 
smooth movement requires 15 new frames per second, this represents a random access data bandwidth of 
over 2 MBytes per sec. This is well beyond the bandwidth of most frame store displays and Permission 
to copy without fee all or part of this material is granted provided that the copies are not made or 
distributed for direct commercial advantage, the ACM copyright notice and ~980 ACM 0-8979]-02]-4/80/0700-0]82 
$00.75 purpose processors. Accordingly, it has not been possible, to date, to consider bona fide real 
time animation on a frame store based display system. Rather, CRT animation has required the use of special 
techniques such as: 1. Specialized raster conversion hardware[I],[2] -e.g. flight simulators 2. Offline 
picture generation/real-time playback[3],[4] -e.g. run-length decoding from disk 3. Color-map animation 
in which limited animation effects are produced in a frame store display by dynamically altering the 
color lookup table[5].  An extensive survey of the field is given in [6]. All these techniques represent 
some form of compromise, whether it be in terms of cost, real time interaction or generality. This paper 
describes a low cost frame store display system capable of generating real time animation effects based 
on 21/2 Dt colored polygonal shapes. Real time performance is attained through the use of 1) a carefully 
designed frame store controller, 2) the graphic processing capabilities of a microprogrammed graphic 
command interpreter and 3) a simple yet powerful polygon fill algorithm which readily lends itself to 
micro- code implementation. The paper also descibes how such a display station can be used to interactively 
test and view simple anima- tion sequences. The capabilities of the system are assessed and some proposals 
for further increasing per- formance are discussed. 2. SYSTEM OVERVIEW The complete hardware system, 
as shown in Figure 1, consists of a general purpose host computer, a frame store display and GUMBI (Graphic 
User Programmable Bit Slice Interpreter). The host, an LSI-11/23 with 256K bytes of memory supported 
by a 10 MByte disk, executes application programs, controls interactive dev- 2V2 D refers to an image 
space in which the third dimension (depth or Z) is represented as a finite number of prioritzed planes. 
Objects, whilst having positional depth, have no physi- cal depth and are constrained to lie within one 
Z plane. the title of the publication and its date appear, and notice is given that copying is by permission 
of the Association for Computing Machinery. To copy otherwise, or to republish, ]82 requires a fee 
and/or specific permission. ices and is responsible for data base management. Such tasks are computationally 
intensive (or in the case of interactive device control represent a very small over-head) and so are 
well suited to the capabilities of such a general purpose machine. HOST COMPUTER (LS1-tt/23) DA + DATA 
r----- ~.... ,.~ FRAME- " II I I ............... " I STORE ~ .11 II ~m-~c~u Du~ . . I DISPLAY X:: ..i, 
X::::LS.TEM I I I I DATA ILJL---- I----1 I SPECIALIZED I II PERIPHERALS II I J Figure 1. Overall System 
Configuration The control of a frame store display, however, is an I/O intensive task which represents 
a "bottleneck" in the display system and threatens to saturate the host. The efficient utilization of 
the processing power of a general purpose computer requires that a few "high-level" (or high information 
content) commands be used to transfer picture data. The data held in a frame store, however, consists 
of a large number of low information content pixels. Accordingly some kind of intermediate processor 
is needed to efficiently interface these two devices. This need led to the development of GUMBI[7] -a 
high speed, microprogrammed graphic processor. GUMBI functions as an interpreter, accept-ing high level 
primitives (e.g. draw filled polygon, translate/scale/rotate vector) from the host and decod-ing these 
into basic I/O sequences (e.g. write pixel, read pixel) which are in turn transmitted to the frame store 
memory via a high speed data bus. For special applica- tions, further processing power can be utilized 
by adding simple hardware peripherals (e.g. a shader) to the high speed bus. 3. DISPLAY The basic structure 
of the frame store controller is shown in Figure 2. At the heart of the display is a 512x512x8 frame 
memory constructed from 16K dynamic RAMs. Access to the frame memory is inter- leaved between refresh 
counters and a pair of x-y address counters. The refresh counters maintain video to the CRT screen and 
are programmable to allow hardware scrolling. The x-y address counters form an address interface between 
the interpreter and the frame store. These may be preloaded and subsequently incre- mented / decremented 
-a feature designed to speed up incremental (nearest neighbour) graphic algorithms. The data I/O controller 
allows data to be read or written in a variety of formats. For example, the frame memory may be accessed 
on either a 16-bit word (within one plane) basis or on a single pixel (across 8 planes) basis. Additional 
control and mask registers not shown in Figure 2, further modify read/write access to the frame store. 
For example, in word mode, a data mask register is used to selectively enable bits within the horizontal 
data word. In pixel mbde, a color mask register selects which planes are to be modified. P-BUS (FROM 
INTERPRETER) $ II OATA I X-Y ADDRESS I/O REGISTER/ CONTROLLER COUNTERS I'l ~.. yIDE-~ER~iON ~ADDR FRAME 
 MEMORY l REFRESH L TIMING COUNTERS l" I S "CGEN Figure 2. Frame Store Architecture The video conversion 
module is primarily responsi- ble for assembling the refreshed memory output into a standard video bit 
stream. Video post processing may be added at this stage, however, depending on the appli- cation. The 
polygon fill algorithm described in this paper uses processing at this point to complete the fill process. 
This is described in more detail in Section 5. The 8-bit video output from the frame store controller 
passes to the CRT via a 24 Kbit color map configured as 4x(256x24) maps to enable rapid switching between 
maps. This arrangement allows the processor to view the frame memory as two 512x512x4 or as four 512x512x2 
buffers without elaborate color map modifications. The various control, data and address latches all 
con- nect to a 16-bit bus termed the P-bus. A separate dev- ice address bus allows the interpreter to 
specify indivi- dual ports along this bus. In addition, a number of con- trol lines allow direct manipulation 
of certain devices 183 within the display, for example the x-y address counters. Cycle time of the frame 
store is 1.2 us. both for word and pixel mode operation. 4. INTERPRETER As mentioned previously, GUMBI 
functions as a hardware interpreter -accepting high level commands from the host and converting these 
into basic picture manipulation operations. It is designed around the AMD 2900 series of bit-slice microprocessor 
com-ponents and consists of a Processor Control Unit, Arith- metic Unit, Scratchpad RAM, Host Interface 
and I/O Device Interface. An overview is shown in Figure 3. HOST COMMANDS RETURN + DATA DATA / I , 8 
'~6 4 ON DJ,~_O ~] PROCE C CODES [ CONTuN  j._ T--- t6 I DATA BUS I BUS ~--~ ~ SCRATGHPAD I MANAGEMENT 
RAM I t t ADDRESS BUS I I 12 HIGH ED BUS Figure 3. GUMBI -Block Diagram The control unit of the interpreter 
is based on the 2910 microsequencer which addresses 4K words of 64- bit pipelined mieroprogram memory. 
A key point here is the direct control of the frame store x-y address regis- ter counters at a microcode 
level. In incremental algo- rithms, no I/O transfers are needed to change screen position. Display read 
and write commands are also under direct microcode control. Thus the screen posi-tion may be altered 
and a new memory write cycle ini- tiated within one machine cycle (200 ns.). The ALU is built around 
the 2903 4-bit slice device. A 16-bit word was chosen to match the word width of the host and frame store 
controller. The ALU data bus is connected to an output FIFO (for returning results to the host), pipelined 
I/O registers, 4K of fast scratchpad RAM, an up/down address counter and microcode buffer (to allow the 
use of immediate microcode constants). The simplicity and flexibility of a microprogrammed processor 
can be easily offset by problems associated with program development. Accordingly, a microassem- bler 
and simulator have been written to simplify the generation and debugging of new firmware. 5. POLYGON 
FILL When considering 2D or 2 1/2D animation, the abil- ity to represent filled colored shapes is an 
attribute which sets the frame store apart from most other graphic display systems. The generation of 
these filled shapes is, however, a time consuming task because of the large number of pixels involved. 
In order to efficiently use the processing hierarchy descibed in Sec- tion 2, it is necessary to find 
a contour (polygon) fill algorithm which is not only fast but is also readily mierocodable into a machine 
such as GUMBI. The classical approach to contour filling is to process a set of polygon edge lists in 
scanline order and from these derive a set of interior horizontal line segments (or spans) which are 
plotted to the screen. This is a relatively efficient algorithm in terms of I/O overhead since each pixel 
is visited only once. It is not readily microcodable, however, because of the large amount of list processing 
involved and the data storage required to hold these lists. In a frame store display system in which 
read/write access is available to the frame buffer, much of the processing (sorting) involved in contour 
filling can be eliminated by using the frame store as a working data memory. Accordingly, a number of 
algo- rithms which use the frame store in this manner were investigated. These included seed fill[8] 
and various classes of parity check routines [9]. The relative performance of these algorithms, in terms 
of execution speed, memory requirements, and ability to fill correctly was evaluated using an LSI-11 
microcomputer in conjunction with the frame store display. Test results showed seedfill and parity check 
rou-tines to be 6-8 times slower than the classical ordered edge-list approach. This is primarily due 
to the greater I/O overhead incurred by these routines. In addition, both require considerable code memory 
space in order to deal with the many "special cases" which threaten to foil the algorithm. Accordingly, 
none of the above were deemed suitable for this application. 5.1 Edge Fill As a result of these studies, 
a new algorithm, desig- nated edge fill, was developed. This is a parity check type algorithm which attempts 
to differentiate between interior and exterior pixels rather than trying to fill a line contour. As a 
starting point, the frame store co-ordinate system is modified so that integral co-ordinate values represent 
a line drawn between two pixels rather than the pixels themselves. When addressing the frame store, a 
convention is adopted whereby pixel (a,b) is that pixel immediately above and to the right of co-ordinate 
position (a,b) as shown in Figure 4. Edgefill is a technique whereby each edge is treated as a thin boun- 
dary between interior and exterior pixels. All pixels to 184 PIXEL (A,B) B+2 -- B+I -- B--\, CO-ORDINATE 
PAIR (A,B) B-l--A-1 A A+t A+2 Figure 4. Co-ordinate System one side of an edge are 'marked' by complementing 
a single bit within the pixel word. In the simplest case, this bit belongs to a special plane known as 
the working plane. When all edges have been considered in this manner, interior points of the working 
plane will have been complemented an odd number of times -exterior points an even number of times. The 
net result is a filled polygon within the working plane. A formal state- ment of the algorithm is as 
follows: for each edge do begin for each scanline intersecting the edge do begin Complement all pixels 
whose midpoint lies to the right of (has an x value greater than) the intersection point end end Figures 
5a-5d show the various stages in the filling of a simple triangle. The algorithm correctly fills any 
connected contour -polygonal or otherwise. The only processing involved in filling a shape in this manner 
is the generation of the outline contour. By processing each edge in an incremental fashion, integer 
arithmetic techniques (similar to those used in vector generation) can be used to calculate edge intersection 
values and hence sp_eed contour generation. Note also the algo-rithm requires no data storage. Simple 
integer calcula- tions combined with no global data storage define an algorithm well suited to microcode 
implementation. Once a polygon has been drawn into the working plane, it becomes a trivial exercise to 
copy this data into the appropriate viewing planes. Alternatively, the algo- rithm may be applied directly 
to the viewing planes as required. Note that this does not destroy previously drawn data as complementation 
is a reversible pro-cedure. It does, however, produce unusual overlap effects which must be taken into 
account when objects in the same plane overlap. 5.2 Edge Flag A disadvantage of edge fill as described 
above is that shapes having many edges crossing the same scanline (A) (a) (C) (D) Figure 5. Example of 
Edge Fill in Progress cause some pixels to be visited many times in the course of filling. Performance 
can be therefore improved by only complementing one pixel per edge/scan intersection and then using a 
scanline parity check to complete the fill. This algorithm, denoted edge flag is stated as follows: a. 
Outline contour for each edge do begin for each scanline intersecting the edge do begin Complement the 
left-most pixel whose midpoint lies to the right of (has an x value greater than) the intersection point 
 end end b. Fill for each scanline intersecting the polygon do begin inside: = 0 for x=0 (left) to x 
= xmax (right) do begin if scanline(x) is set then negate inside if inside then set scanline(x) in color 
planes then reset scanline(x) in color planes end end A more detailed implementation of the contour 
gen- eration part of the algorithm, showing the actual edge intersection calculations, is given in Appendix 
I. 185 Performance tests show this algorithm to be compar- able in speed to the ordered edge list (OEL) 
technique when both are executing on a general purpose host. This is encouraging in light of the fact 
that the algo-rithm is obviously simple enough to be translated into GUMBI microcode. Performance can 
be further improved, however, by noting that the parity fill portion is simple enough to be implemented 
in hardware by employing the video post processing circuit shown in Figure 6. This circuit is added to 
the video output of each color bit plane. Filling is accomplished by drawing the edge flag contour directly 
into the appropriate color planes and then setting the hardware fill line. Execution times measured using 
this direct hardware fill approach show improvements of up to 50:1 over the pixel based OEL technique. 
This is due to the fact that it is no longer necessary for the processor to address every pixel within 
a shape. The time taken to define each polygon with hardware assisted edge flag is proportional to the 
circumference of the shape -in contrast with other algo- rithms which depend on the area of the shape. 
FILL MODE"   VIDEO FRAME-STORE OUT COLOR VIDEOPLANE CLOCK H-SYNC Figure 6. Hardware Parity Fill 
There is yet another reason why edge flag is particu- larly suited to the animation problem. When displaying 
a sequence, the old frame must be erased before the new one can be drawn. Because edge flag is based 
on pixel complementation, a polygon may be erased by simply redrawing it -even if polygons inadvertantly 
overlap. This means that a simple sequence may be superimposed on a complex static image with no fear 
of destroying the background through erasure. 6. ANIMATION SYSTEM The generation of a 21/2 D animation 
sequence begins with a storyboard description of the action, from which an artist draws a number of "key 
frames" (as shown in Figure 7(a)). These key frames define control points -spatial and temporal discontinuities 
in the events to be portrayed. Between these key frames, picture content and motion are assumed to be 
relatively uniform. The next task is to interpolate between (sometimes known as "in betweening") these 
key frames to produce a com- plete animation sequence (Figure 7(b)). Priority levels are sometimes used 
to distinguish depth. If this is the case, visibility also needs to be determined. In the case of a real 
time video system, this can either be done in J~L KEY FRAME~ KEY FRAME 2" (a) Key Frames (b) Interpolated 
Sequence Figure 7. Example of Key Frame Interpolation hardware through video post processing (multi-planing), 
or alternatively software techniques such as polygon clipping can be employed. Ideally, a frame store 
based animation station should .be capable of performing key frame interpolation, visi- bility calculations 
and frame store image generation in real time - i.e. one new frame per 60ms.'~ Our first goal was to 
achieve real time image genera- tion since this is the process most closely tied to display architecture. 
The other tasks are computationally inten- .sive and more related to absolute processing power. Accordingly, 
a small software package was written to generate, store and replay simple polygonal sequences on the 
frame store display system. Operation is as fol-lows. An interactive graphic editor permits key frames 
to be entered via a data tablet. The artist specifies shapes in terms of polygon vertices and colors 
and this data is stored on disk in coded polygonal format. An interpolation routine is then invoked which 
linearly interpolates between key frames according to the opera- tors specifications. The entire sequence 
is once again stored on disk in polygonal format. Real time playback is performed by reading the animation 
sequence from disk and sending the appropriate polygon vertex data to GUMBI. The interpreter transforms 
this information into an edge flag description of the image followed by hardware fill as described in 
the previous Section. t Approximately 15 frames per second are required to create smooth motion effects 
 186  APPENDIX I Figure 8 shows three key frames from an abstract sequence drawn by D. Weimer. This 
shows the sort of image complexity which can be handled by the present system in real time. As complexity 
increases, the sys- tem fails "soft" and the sequence is merely replayed at a reduced rate. 8. CONCLUSIONS 
 Until now, computer generated real time animation has required the use of special purpose raster conversion 
hardware. This paper has described a general purpose frame store display station capable of generating 
useful polygonally based animation sequences. There are many ways in which this system can be further 
refined to improve performance and remove constraints. Planned additions to the system include: 1. Microcode 
linear interpolation within GUMBI. This means that sequences can be generated directly from a key frame 
description thereby reducing storage requirements and picture information bandwidth (Eq. 3). 2. The 
use of faster static RAMS within the frame memory to reduce frame store cycle time (Eq. 2). Once an extravagance, 
this is becoming a more realistic alternative as the density of static memories continues to rise. An 
increase in frame store plot- ting speed can be accomodated by using more than one GUMBI in a multiprocessing 
configuration. For example, 2 GUMBIs could simultaneously draw polygons into a dual ported memory. Alternatively, 
one interpreter could perform interpolation while the other deals with pixel generation. 3. Non-linear 
spatial interpolations (e.g. spline) and color interpolations can be used to achieve special effects. 
 4. The addition of simple processing power to the frame store controller would allow such operations 
as "complement pixel" to be completed within one frame store cycle period. This reduces frame store bandwidth 
overhead and relieves GUMBI of some tedious processing by extending the picture infor- mation hierarchy 
to one more level.  The animation station described in this paPer is now operational and was in fact 
used recently in the genera- tion of an animated title sequence for a TV documen- tary. The artist used 
the system to interactively test short sequences and verify their visual effect. A more detailed description 
of the edge flag contour generation routine as applied to a straight edge from (xl,yl) to (x2,y2) where 
x2 >xl and y2>yl is as fol-lows: X:~XI dely:= (y2-y 1 )'2 delx: = (x2-x 1)'2 rem: = (delx-dely)/2 for 
y=yl to y=y2-1 do begin while rem >_ 0 do begin x:=x+l rem: = rem-dely end complement(x,y) rem: = rem 
+ delx end  REFERENCES [1] ROUGELOT, R.S. "The General Electric Com- puter Color TV Display", Pertinent 
Concepts in Computer Graphics, (M. Faiman and J. Nievergelt Editors), University of Illinois Press, 1969. 
[2] WEINBERG, R. "Computer Graphics in Support of Space Shuttle Simulation", Siggraph "78 Proceedings, 
Aug. 1978, pp. 82-86. [3] MYERS, A. "A Digital Video Image Storage and Retrieval System", Computer Graphics, 
Vol. 10 No. 2, Summer 1976, pp. 45-50. [4] HACKATHORN, R.J. "Anima II -A 3D Color Animation System", 
Siggraph "77 Proceedings, July 1977, pp. 54-64. [5] SHOUP, R. "Color Table Animation", Siggraph "79 Proceedings, 
Aug. 1979, pp. 8-13. [6] BAECKER, R. "Digital Video Display Systems and Dynamic Graphics", Siggraph "79 
Proceedings, Aug. 1979, pp, 48-56. [7] WESTE, N. and ACKLAND, B. "GUMBI -A Graphic User Microprogrammable 
Bit Slice Inter- preter", IEEE Compcon Fall '79 Proceedings, Sept. 1979, pp. 232-237. [8] LIEBERMAN, 
H. "How to Color in a Coloring Book" Siggraph "78 Proceedings, Aug. 1978, pp. 111-116. [9] PAVLIDIS, 
T. "Filling Algorithms for Raster Graphics", Dept. of Elec. Eng. and Computer Sci- ence, Princeton University. 
Report no. 238, January 1978. 188  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1980</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>807491</article_id>
		<sort_key>189</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-1980</article_publication_date>
		<seq_no>32</seq_no>
		<title><![CDATA[Special problems in human movement simulation]]></title>
		<page_from>189</page_from>
		<page_to>197</page_to>
		<doi_number>10.1145/800250.807491</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=807491</url>
		<abstract>
			<par><![CDATA[<p>Three dimensional animation of human movement may be obtained by specifying movements in a goal-directed manner and constructing a sophisticated simulator to execute those movements. We briefly describe an architecture for such a simulator and then concentrate on five special problems which arise: scheduling movements which occur concurrently, computing motion of three-link chains, processing contacts, moving the center of gravity and maintaining balance, and adjusting limb twist for a standard orientation.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Computer animation]]></kw>
			<kw><![CDATA[Human movement]]></kw>
			<kw><![CDATA[Parallel processing]]></kw>
			<kw><![CDATA[Simulation]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.2.10</cat_node>
				<descriptor>Motion</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Animation</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>D.4.9</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10011007.10011006.10011050</concept_id>
				<concept_desc>CCS->Software and its engineering->Software notations and tools->Context specific languages</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352.10010380</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation->Motion processing</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352.10010238</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation->Motion capture</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010238</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->Motion capture</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Human Factors</gt>
			<gt>Performance</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP95031390</person_id>
				<author_profile_id><![CDATA[81452608047]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Norman]]></first_name>
				<middle_name><![CDATA[I.]]></middle_name>
				<last_name><![CDATA[Badler]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Computer and Information Science, The Moore School/D2, University of Pennsylvania, Philadelphia, PA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39076760</person_id>
				<author_profile_id><![CDATA[81332519188]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Joseph]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[O'Rourke]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Computer and Information Science, The Moore School/D2, University of Pennsylvania, Philadelphia, PA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P329253</person_id>
				<author_profile_id><![CDATA[81547785556]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Bruce]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kaufman]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Burroughs Corporation, Small Systems Group, P. 0. Box 235, Downingtown, PA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Badler, N.I., O'Rourke, J., Smoliar, S.W., and Weber, L. The simulation of human movement by computer. Technical Report, Dept. of Computer and Information Science, University of Pennsylvania, Philadelphia, PA, Dec. 1978.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Badler, N.I., O'Rourke, J., and Toltzis, H. A spherical human body model for visualizing movement. IEEE Proceedings 67, 10 (Oct. 1979), 1397-1403.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>356760</ref_obj_id>
				<ref_obj_pid>356757</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Badler, N.I. and Smoliar, S.W. Digital representations of human movement. Computing Surveys 11, 1 (March 1979), 19-38.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Baecker, R. Picture-driven animation. Proc. AFIPS 1969 Spring Jt. Comp. Conf. Vol. 34, AFIPS Press, Montvale, NJ, 273-288.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Cappozzo, A., Figura, F., Marchetti, M., and Pedotti, A. The interplay of muscular and external forces in human ambulation. J. Biomechanics 9 (1976), 35-43.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807414</ref_obj_id>
				<ref_obj_pid>965139</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Catmull, E. The problems of computer-assisted animation. Computer Graphics 12, 3 (Aug. 1978), 348-353.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Hutchinson, A. Labanotation, Theatre Arts Books, New York, 2nd Edition, 1970.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Kaufman, B. The simulation of human locomotion. MSE Thesis, Department of Computer and Information Science, University of Pennsylvania, Philadelphia, PA, June 1979.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[O'Rourke, J. Three-dimensional motion of a three link system. Technical Report, Dept. of Computer and Information Science, Univ. of Pennsylvania, Philadelphia, PA, June 1978.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Spegel, M. Programming of mechanism motion. Technical Report No. CRL-43, Division of Applied Science, New York University, New York, NY, Nov. 1975.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_obj_id>810121</ref_obj_id>
				<ref_obj_pid>800178</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Weber, L., Smoliar, S.W., and Badler, N.I. An architecture for the simulation of human movement. Proc. ACM National Conf., Dec. 1978, 737-745.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_obj_id>906863</ref_obj_id>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Wessler, B. Computer-assisted visual communication. Ph.D. Dissertation, Dept. of Computer Science, University of Utah, Salt Lake City, UT, August 1973.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 SPECIAL PROBLEMS IN HUMAN MOVEMENT SIMULATION Norman I. Badler * Joseph O'Rourke * Bruce Kaufman ** 
 Key words and phrases: Human movement, computer animation, simulation, parallel pro- cessing. Computing 
Reviews categories: 3.12, 3.49, 3.65, 8.2 ABSTRACT ments between the links at the joints are all three-dimensional 
rotations. Three dimensional animation of human Such an attitude, however, ignores the movement may 
be obtained by specifying fact that at least consciously we do not movements in a goal-directed manner 
and control our own bodies by imagining ap- constructing a sophisticated simulator to propriate joint 
angle rotations. Obser- execute those movements. We briefly des-vers of human movement have generally 
re- cribe an architecture for such a simula- cognized the importance of goal-directed tor and then concentrate 
on five special behavior in describing movements, even if problems which arise: scheduling move-they 
have not explicitly called it by ments which occur concurrently, computing that name [7]. motion of three-link 
chains, processing contacts, moving the center of gravity Badler and Smoliar [3] have recent- and maintaining 
balance, and adjusting ly reviewed some of the issues surround- limb twist for a standard orientation. 
ing the representation of human movement, discussing movement notation systems, computer animation techniques, 
and com- I. Introduction puter realizations of a human figure. In their survey, an architecture for 
simula- Human movement appears disarmingly ting human movement is proposed and simple to us since complex 
tasks, in-briefly examined. The complete architec- tricate motions, and skillful coordina-ture is described 
by Weber, Smoliar and tion are learned and executed with a Badler [11] and will only be summarized minimum 
of conscious thought. As com-here. Our purpose is to isolate and puter graphics researchers, it is se-describe 
solutions to five special pro- ductive to dismiss the process as being blems in human movement that are 
not rather simple: after all, the body stays typically considered or solved in com- connected by rigid 
links and the move-puter graphics languages or animation systems. * Computer and Information Science 
 These problems arise because we are The Moore School/D2 stimulating human movement from "high University 
of Pennsylvania level" three dimensional movement in- Philadelphia, PA 19104 structions rather than 
supplying artist- drawn two-dimensional frames [6]. ** Burroughs Corporation Small Systems Group 2. The 
Simulator Architecture P. 0. Box 235 Downingtown, PA 19335 Movement information is represented in five 
"movement primitives"" (I) directions, (2) rotations, (3) fac- ings, (4) shapes, and (5) contacts. 
A Permission to copy without fee all or part of this material is d~ction desc~bes achievement of a 
granted provided that the copies are not made or distributed position (point) in space by a body joint 
 for direct commercial advantage, the ACM copyright notice and the title of the publication and its date 
appear, and notice or movement of a joint in that direction. is given that copying is by permission of 
the Association for The former will result in different paths Computing Machinery. To copy otherwise, 
or to republish, to the final position depending on the requires a fee and/or specific permission. 
 starting location (for example, "move the wrist in front of the shoulder"), while ~980 ACM 0-89791-021-4/80/0700-0189 
$00.75 the latter will result in a path parallel to the given direction from the starting point (for 
example, "walk forward"). A rotation specifies a joint or whole body rotation, or twist of a body segment 
such as the lower arm. A rotation requires a fixed angular change about some axis. A facing, on the other 
hand, describes a di~ion in space to be achieved by the perpendicular to some body surface; the direction 
is achieved by a rota- tion of an appropriate (variable) amount. For example, different rotations are 
required for each person in a room to face the room's central point. A shape defines the path in space 
of a body~nt or, alternatively, the geometric configu- ration to be achieved by a series of body joints. 
In the former it resembles Baecker's p-curves [4], while in the latter it resembles key-frame interpolation 
(but the body segment lengths are not permitted to change). Finally, contacts describe a variety of relationships 
between one body part and another or between a body part and the environment. The relationships include 
actual contacts as well as re- lational concepts such as near, above, and support, and modifiers such 
as slide, grasp, and brush. All movement types include timing information, either as a duration ( (i) 
through (4) ) or as an achievement time (5). The duration implies achievement at the end of the time 
interval. Each type also indicates the body joint at the "fixed" end of the movement. (The joint responsible 
for initiating the movement is not indicated internally for reasons to be noted below). Directions in 
space, surfaces of the body, or locations in the room are given in any of a variety of rectangular coordinate 
systems, allowing very flexible specifications relative to the room, the whole body, any body seg- ment, 
or any body surface point. Most movement types also contain modifiers which refine the basic movement, 
direc- tion, path, or amount. Sequences of movements of these types are gathered together as "programs" 
(Figure I) for collection of "parallel processors" which simulate the action of each movement "instruction" 
on a data strueture representing a human body (Fig- ure 2) [2]. The simulator consists of one processor 
for each joint of the body which moves or positions that joint, a "progression processor" which is respon- 
sible for the center of gravity, and a monitor which maintains the body data base, schedules the other 
processors, and super- vises the achievement of contacts. The monitor is also responsible for passing 
a set of three-dimensional body joint posi- tions to a computer graphics body model at the conclusion 
of each simulation cycle [2]. The simulation is designed with goal- directed behavior capabilities, thus 
the TIME = 0 O ULL INSTRUCTION O DIP~.CTIONINSTRUCTION O DIRECTION INSTRUCTION O DIRFCTION INSTRUCTION 
O FACII,~ INST~dCTION ® ® ® ® INSTRU~FION INSTRUCTION O NULL INSTRUCTION O REVOLUTIO~II.~ IwnJLL O 
SHAPE INSTRUCTION ! vINSI~UCTION D!~SCP~PITON O SHAPE DESCR/gq~ION O : DURATION 01' INSq~CTION Figure 
1. Structure of a program for a joint processor. INST~.:UCFZO~ ~O~_ACT ~}TECTF~g INSTRUCTIONS SLFPPORT 
CO)2 Gf~! r~ J JOL~ PRO( 12?OIViI~JAL SI'P~A:.~ O1" DIRfXTfiON, REVOLUTION, FACiNg, ASa) SIL~PE INS'i~JCfIONS 
 Figure 2. The organization of the simulator. Processors are circles; programs are blocks; arrows show 
Communication paths. joint processor which receives an instruc- tion is not necessarily the joint which 
will "rotate" or transform in the graphi- cal sense: for example, instructions to move the whole arm 
are sent to the wrist with an indication that the shoulder is the fixed end. 3. Special Problems In 
the following sections, we shall examine some of the particular problems which arise in designing the 
simulator and describe our solutions: scheduling con- current movements, computing linkage mo- tion for 
multiply-connected movements, processing contacts, implementing loco- motor movements and maintaining 
balance, and orienting limbs by a standard orien- tation function. 3.1 Parallel movement' scheduler 
 Although the joint processors execute in parallel, some scheduling must be per- formed by the monitor 
to insure as much determinism as possible in the final move- ment. For example, the two arms might move 
quite independently, but if one wrist is executing a movement with fixed end at the center hip joint 
while the other wrist executes a movement from the center shoulder, movements of the former should precede 
those of the latter since its fixed end location (center shoulder) is being altered. More radical situations 
are possible, such as movement of the cen- ter shoulder from a fixed end at a foot, concurrent with a 
knee bend with fixed end at the hip. (For other examples see Hutchinson [7] ). The intersecting por- 
tions of the body tree (where joints are nodes and body segments are edges) af- fected by these movements 
give rise to scheduling conflicts which cannot be simply resolved by standard pre-order traversal from 
some universal root node. We define the scope of a movement instruction to be the tree consisting of 
the body joints lying along the path of joints from the fixed end specified by the instruction to the 
joint which re- ceives the instruction, and then beyond to the remainder of the tree of body connections 
rooted at the joint proces- sor (Figure 3). In addition, joints more distant from the fixed end may be 
involv- ed in the movement due to explicit "in- clusion" modifiers of a direction [Ii] or implicit inclusion 
by body constraints during certain directions or rotations. The furthermost affected joint (in the tree-connection 
sense) is called the augmented scope joint and the scope tree is extended to it from the specified fixed 
end. Figure 3 shows the augmented scope tree of an instruction with "center hip" as fixed end, "center 
shoulder" as joint processor, and inclusion of the left hip in the movement. All instructions gene- 
rate a unique directed augmented scope tree. NECK JOINT PI~3CESSOR i ) _/ LEFT L,E~T 'CENn~ /~" RISHT 
SHOULDER CLAVICLE S}DULDER CLAVICLE PTGHT ~ S~-~UL~ER  ¢ i LEFT DD %ROOT OF. % OF ~ AUC~3~ ~OOT OF 
SCOPE~ SCOPE TREE Figure 3. Example of an augmented Scope tree with jotnt processor at center shoulder, 
fixed end at center hip end augmented scope joint at left hip. The execution order of the active instructions 
during a simulation cycle is computed from the collection of corres- ponding augmented scope trees. Since 
there are no prohibitions against any particular arrangement of instructions, the scheduling algorithm 
employed by the monitor must allow for every possible situation. The basic algorithm follows: i. If 
two instructions have the same joint processor and the same augmented scope joint, the joint processor 
is itself res- ponsible for organizing the simultaneous execution of the two (or more) instructions. 
Otherwise the monitor performs the scheduling. 2. If one tree is contained in another, the instruction 
corres- ponding to the greater is exe- cuted first. (Note that this corresponds to the "usual"trans- 
formation nesting.)  3. If two trees are disjoint or share only a single common node (which must be 
their augmented scope joint) they represent in- structions which may be executed in parallel (or are 
at least independent of one another).  4. If two trees overlap in any other fashion then each joint 
proces- sor node is moved, if necessary, one body joint at a time toward  its respective augmented 
scope joint until the two augmented scope joints and the two (possibly moved) joint processor joints 
lie along a single path in the tree. The execution order is determined from the or- der of joints along 
this path: 4.1 Apply rule 1 or 2; if either is satisfied, use it. 4.2 Otherwise, the two trees must 
overlap in opposite directions (Figure 4). The one executed first is: 4.2.1 that which originally had 
a supporting joint as its joint processor; or 4.2.2 if none, then that with a supporting joint in its 
augmented scope tree; or 4.2.3 if none, that with a pas- sive contact point in its tree; or 4.2.4 if 
none, choose either instruction arbitrarily. Joint Processor (2) Augmented Scope Joint (1) teration. 
In any case~ the joint move- ments during step 4 are only performed to establish order and do not change 
the ac- tual (original) joint processor provided with the instruction. Applying these rules to every 
pair of instructions results in a partial ordering of all active instructions during a simu- lation cycle. 
A hypothetical (and rather unusual) set of instructions and the re- sulting ordering is shown in Figure 
5. Instructions ¢¢ Joint Processor 6u~EentedScope Joint l left ankle center hip 2 left foot left hip 
3 center shoulder center hip 4 center shoulder center hip 5 left elbow center shoulder 6 left hand center 
shoulder 7 left wrist upper pelvis 8 left wrist lower rib cage 9 upper pelvis center hip 10 right shoulder 
center shoulder 11 left elbow center hip 12 right e]bow center hip 13 left hand center hip 14 right hand 
lower rib cage 12 34 5678 91011121314 . > > > > > >>>>>> > 1 I . . > > > > > > > > > > > > 2 2 . . 
. > > > > > < > > > > > 3 .... > > > > < > > > > > 4 33 --o * -> < < < < < < < < 5 6 ...... < < < < < 
< < < 6 11 -- --><><<< > 7 I? ........ < > < < < > 8 .... .... > >'> > > 9 77 -  ..... < < < < lO 
14 .... ..... . >> > II 10 ....... .... > > 12 S ...... - . .... > 13 ............. . 14 6 Joint 
P (I)........ / k~/~kkk Augmented Scope Joint (2) Figure 4. One possible case of overlap of two augmented 
scope trees. The intent of step 4.2 is to execute first'any constrained movement: one which changes support 
(and will be executed as early in the simulation cycle as possibl~, one which depends on having a (new) 
posi- tion for a supporting joint, or one which is pursued during a contact and hence par- ticipates 
passively. If none of these cases occur, then the monitor is forced to choose one instruction arbitrarily 
which leads to an unavoidable non-deter- minism. Often, however, this may mean that the original movement 
specifications are indeed ambiguous and might require al- Figure 5. Partial ordering of active instructions. 
(a) instructions --given as Joint processor and augmented scope pairs. I~Ipartia] ordermatrix. final 
linearized order of execution for sequential simulation. 3.2 Three-Deminsional Motion of Linkage Systems 
The positional direction movement primitive instructs a particular joint to move from its current position 
to some new position in space. Depending on the particular joint being moved and on var- ious modifiers 
of the movement primitive, the move command may include other joints between it and the fixed end. For 
exam- ple, a move of the wrist normally includes the elbow, and may or may not include the shoulder. 
The positional goals of such intermediate joints are normally left un- specified by the direction primitive, 
with the understanding that their motions should be "natural" in some sense. If we temporarily ignore 
the orientation of the segments involved in the motion (see 3.5), the problem executing a directional 
in- struction is the problem of positioning a multi-link structure.  192 Our solution to this problem 
attempts to simultaneously satisfy four goals: (i) achieve the designated position for the end point 
of the last link;  (2) stay within the angular limita- tions of each joint in the link system;  (3) 
perform the movement "naturally" by equalizing the movements be- tween the joints;  (4) achieve (I), 
(2), and (3) above with a minimal amount of itera- tive searching.   Goals (I) and (2) are absolutely 
es- sential for a valid implementation; goal (3) provides a means for choosing among the many solutions 
which satisfy (i) and (2); and goal (4) is important for imple- mentation efficiency. 'We have developed 
procedures for satisfying these four goals for systems of I, 2, and 3 links. We will briefly describe 
our methods for both 2 and 3-D motions. Further details are con- tained in [9]. The motion of a 1-1ink 
system in either 2-D or 3-D is tr~l: simply align it along the vector towards the positional goal if 
this falls within the angular limits, and as close as possible other- wise. There are either 2, i, or 
~o posi- tionings of a 2-1ink system in 2-D which reach a particular end point, and these solutions can 
be computed analytically. The angular constraints can be satisfied by a check against the limits. Moving 
a 2-1ink system in 3-D can be accomplished by choosing a plane for the links which will result in minimal 
movement, and then positioning according to the 2-D proce- dure. A 3-1ink system is much more compli- 
cated (Figure 6). Although it is possible to derive the constraint equations for the system, their non-linearity 
necessitates a numerical search for a solution. For 2-D motion, the search can be mini- mized by computing 
analytically the valid range for one of the angles, and only searching through this one range. For any 
particular value in this range, the problem reduces to a 2-1ink system, and can be solved as previously 
described. Once the search is completed, a choice can be made among the various solutions according to 
a measure of "naturalness" [9.10]. We have chosen to balance the displacements of each joint relative 
to its neighbor closer to the fixed end. For 3-D motion, the main difficulty is choos- ing among the 
many solutions. Our imple- mentation projects the links onto an ap- propriate plane, solves in 2-D as 
above, and maps back to 3-D. The procedures described above are  effective for all cases except those 
which require twists for their solution, or for link systems whieh have complex angular limitations at 
the joints. In these cases, we have resorted to search- ing the space of potential solutions and checking 
for valid positionings. 3.3 Contacts Contact type instructions are de- livered to and processed by the 
monitor. The primary reasons for this arrangement are the monitor's unique global view of all active 
processors and the specifica- tion of contacts as events to be achieved at a certain time rather than 
£u~ing a g--~ven time interval. The monitor, there- fore, must find and direct appropriate joint processors 
to execute movements guaranteeing the contact. Another con- sequence is that a contact instruction has 
no intrinsic (or even unique) scope associated with it. Rather, its scope is determined after a joint 
processor is as- signed the task of achieving the contact. For the implementation of contacts we shall 
restrict the discussion here to contacts (or "nearness" conditions) be- tween two particular points on 
the body. The general case of achieving contacts between two segments or a point and a segment is fully 
described elsewhere [i]. The first step requires associating direction instructions (one for each active 
participant in the contact) with the specified contact. A direction in- struction chosen must overlap 
the contact time, must be the latest starter of any set of these and must contain the contact point in 
its augmented scope. If more than one exists which satisfies these criteria for one of the contact points, 
then that chosen has its joint processor closest to the contact point and its aug- mented scope joint 
furthest from it. If no existing direction instruction is available, then the monitor generates a direction 
instruction for the joint pro- cessor beyond the contact point and as- signs a default fixed end. (For 
example, if contact point is between the wrist and elbow, the joint processor selected will be the wrist, 
while the fixed end will be the default for the whole arm: the shoul- der.) Although we now have a direction 
in- struction for each active contact point, we cannot be certain that the desired re- lationship will 
be achieved solely by direction type movements. It is neces- sary to consider the surface orientation 
at the contact point. Since contact would require the two surface normals to be parallel but oppositely 
directed, fac- ing type instructions must be generated to achieve this alignment. Each facing instruction 
has as its goal direction the (changing) vector between its contact  point and that of the other segment, 
and has a duration based on the starting time of the concurrent direction instruction and end time at 
the contact instant. Thus the active contact points rotate "just enough" to match surface normals to 
oppo- site directions. Contacts are actually implemented by modifying the computed positions of the 
direction instructions associated with it. By the choice of the direction instruc- tions, the joints 
near the contact points should approach the general vicinity of the contact. The contacts affect this 
position during execution of the direction instruction by adding a displacement to- wards the other contact 
point. The ad- justment cannot be added indiscriminately, however, as it could perturb the normal path 
unacceptably (Figure 7). The solu- tion involves two quantities: the "dis- tance influence" causes greater 
displace- ments as the contact points get closer; while the "time influence" causes greater displacements 
as the contact time approa- ches. The result of applying these fac- tors to an example similar to Figure 
7 is shown in Figure 8. Once a contact is achieved, it may become irrelevant and the contact points 
are allowed to drift apart due to other movements. If the contact is to be main- tained, a direction 
instruction is generated to keep the contact points together until a conflicting movement instruction 
be- comes active. 3.4 Locomotion and balance The whole body is moved by the "pro- gression processor," 
whose primary duty is to maintain the body's center of gra- vity much as a joint processor moves its 
associated joint. This point of view has two important consequences: first, that locomotive movements 
are accomplished by moving the center of gravity and then executing other joint processor programs; and 
second, that the center of gravity is not a fixed point in the body and can be altered by motions not 
initiated by the progression processor. The movement instruction programs to the simulator are arranged 
so that all instructions which affect support are passed to the progression processor with timing and 
support joint indicators. The progression processor uses some of the in- formation to position (or move) 
the center of gravity (whose representation is analo- gous to a body joint), and subsequently passes 
the instructions on to their appropriate joint processor. Thus, for example, forward direction instruc- 
tions to the ankles (the normal supporting joints during walking movements) arrive first at the progression 
processor, are interpreted as movements of the center of gravity, and then are passed to the an- kles. 
By definition, instructions to the progression (center of gravity) processor have the highest priority 
and are there- fore executed before any other movements scheduled by the algorithm described above. 
We shall consider only the case of direction type movements now [8]. Ac- cording to the semantics of 
direction movements for a supporting joint (based on the conventions established in Labano- tation [7]), 
there is a "preparation phase" which occupies a fixed percentage of the preceding direction movement. 
During this time, the supporting joint is moved to the position associated with the beginning of the 
instruction. For exam- ple, the beginning of a step is consider- ed to be the instant of contact with 
the floor; thus the preparation phase inclu- des lifting and swinging the foot in the required direction. 
 Progression processor instructions are the only ones exhibiting such "con- text-sensitivity," and it 
affects the simulation in the following fashion. The preparation and achievement times of an instruction 
are translated into pairs of contact type instructions for the monitor. For our step example, one contact 
instruc- tion will be generated at the prepara- tion time to break the foot contact with the floor, and 
another generated at the achievement time to establish contact with the floor. Inbetween, the direction 
specified in the original instruction is used to move the ankle. The entire movement of the leg (for 
example) is completely determined if the path of the center of gravity is straight, if the initial and 
final knee angles are known, and if some sort of "propulsion" effect is assumed [10,12]. The first condition 
is met by the progression pro- cessor (although slight deviations up- and-down or side-to-side along 
a forward path, for example are allowed), the second by explicit defaults or instructions for knee bending, 
and the last by assuming that the angular velocity of the thigh is constant during virtually all of the 
pro- pulsion phase and also during the return phase. (For biomechanical evidence for this assumption, 
see [5]). The remaining degrees of freedom are constrained by the fixed segment len[ths, the joint stops, 
the linkage computation, and the geo- metry of the floor. The step length must be the same as the displacement 
of the center of gravity, so the supporting joints will appear in the proper relation- ship to the rest 
of the body (Figure 9). Once the center of gravity has been positioned during a simulation cycle, the 
other joint processors are permitted to execute their instructions. Unfortunate- ly, this may result 
in the displacement of the actual center of gravity. Once all ~olnt prooessors have finished during a 
simulation cycle, the progression proces- sor is again given control in order tore- establish the identity 
of the actual and desired positions of the center of gra- vity. In certain cases, no adjustment may be 
required, as when, for example, the body is deliberately placed in a state of imbalance so it will fall. 
 The adjustment proceeds by calcula- ting the actual center of gravity and checking whether or not it 
falls within the "support polygon" formed on the ground plane as the convex hull of all support points. 
If this is the case, then the actual center of gravity replaces the for- mer location known to the progression 
processor; otherwise an adjustment may be necessary. If body balance is not strictly required (for example, 
during running or during a fall), then a simple replacement is made as above; otherwise the body must 
be shifted to align the computed location with the desired loca- tion along the same vertical line. Rather 
than simply "rotate" the body to achieve this result, a more natural effect is ob- tained by rotating 
body segments above the center of gravity and counter-rotating those below it which touch the ground 
 (Figure i0) [8]. 3.5 Standard orientation In order to define the position of body segments, each is 
given as a vector and an orientation in a standardized eo- ordinate system associated with the seg- ment 
closer to the root (center hip) of the body tree. A "standard" or default orientation is defined at each 
possible position (within a sphere) of the limbs; all other orientations must be explicitly specified 
as a deviation from the default. If we erect a coordinate system at the north pole of the sphere (i.e., 
at 0=0 and %=0), and transport it to <0, %> by twisting it by 0 and then sliding it along a meridian 
by ~, then the coordinate system Will have some particular orien- tation dictated by the matrices used 
in the transformations. We can use this coordinate system as our base, and then specify the standard 
orientation as some furthe9 twist from this base orientation. In this paradigm, the standard orlenta- 
 tion oan be represented as a function from <0, %> to t, the further twist need- ed. For the arm and 
leg standard orien- tations (as culled from Labanotation [7]), this function is linear in both 0 and 
@, and only five pieces are required to de- fine it over the entire sphere surface. The actual equations 
for the left arm are as follows (all angles are expressed in degrees) : 0 ~ ~ ~,,tL0~ [-90,90] [0,90] 
t = 180 - O" (1-~190 [-90,g0] f9D,l~o]90,II~9 t -180' 1-o/~JO + ~-270)* 0/90) [go, 270] t : (i-180 
" 1+ 180-¢)/90) [90,180] 0,9G t " 2' 3-180 *@/90 - (180+o)*()-,tygo) [0,90] [180,270] t = (iBO-O)*(l-3¢/go)+sgn(Z25-e}*)6C*¢/90 
 The standard orientations vary continuous- ly over the entire surface of the sphere, except for a seam 
in the middle of the baok upper right octant, where the physical limitations of the joint dictate that 
the limb should change its direction of twist (Figure ii). The equations for the right arm and the 
left and right legs are very similar. When the limbs of the body are moved into some position, the 
orientation of the segments will in general depend on the path used to reach that position. So whenever 
the limbs are placed into a posi- tion by specifying the joint positions, the resulting orientation of 
the segments is computed and compared to the orienta- tion given by the standard orientation function, 
and the segments are then ad- justed to agree with the default orienta- tion. If an orientation (twist) 
different from the default is desired, then a fur- ther adjustment is made. 4. Summary The present 
simulator exists as a collection of PASCAL programs which are too large to fit simultaneously in the 
memory space of our UNIVAC 90/70 computer. The various components have been tested separately and have 
been used to generate most of the figures and examples given here. The only components of the simula- 
tor which have not been adequately imple- mented are the semantics of the individ- ual joint processors: 
the code which executes general programs composed of all types of movement instructions. We have not 
yet attempted to "fine tune" the pro- gression processor nor provide completely general contact processing. 
We expect, however, to continue evolving a function- ing simulation of human movement for in- terpretation 
of goal-directed movement commands and generation of realistic three- dimensional animations. 5. Acknowledgements 
 The authors wish to gratefully ac- knowledge support of NSF Grant MCS 76-19464 and O'Rourke's IBM Fellowship 
Grant. The authors are indebted to many people who have been involved in this project: Stephen Smoliar 
and Lynne Weber for the movement instruction architecture, and also Julie Luckraft, Dahlia Benaroya, 
and William Wood, Jr. 8, References [1] Badler, N.I., O'Rourke, J., Smoliar, S.W., and Weber, L. The 
simulation of human movement by computer. Tech-nical Report, Dept. of Computer and Tnformation Science, 
University of Pennsylvania, Philadelphia, PA, Dee. 1978. [2] Badler, N.I., O'Rourke, J., and [7] Hutchinson, 
A. Labanotation, Thea--Toltzis, H. A spherical human body tre Arts Books, New York, 2nd Edi- model for 
visualizing movement. tion, 1970. IEEE Proceedings 67, 10 (Oct. 1979), [8] Kaufman, B. The simulation 
of human 1397-1403. locomotion. HSE Thesis, Department  [3] Badler, N.I. and Smoliar, S.W. Dig-of 
Computer and Information Science, ital representations of human move-University of Pennsylvania, Phila- 
ment. Computing Surveys 11, 1 (March delphia, PA, June 1979. 1979), 19-38. [9] OtRourke, J. Three-dimensional 
mo- [4] Baecker, R. Picture-driven animation. tion of a three link system. Tech- Proc. AFIPS 1969 Spring 
Jt. Comp. nical Report, Dept. of Computer and Conf. Vol. 34, AFIPS Press, Montvale, Information Science, 
Univ. of Penn- NJ, 273-288. sylvania, Philadelphia, PA, June  [5] Cappozzo, A., Figura, F., Marchetti, 
1978. M., and Pedotti, A. The interplay [10] Spegel, M. Programming of mechanism of muscular and external 
forces in motion. Technical Report No. CRL-43, human ambulation. J. Biomechanics 9 Division of Applied 
Science, New (1976), 35-43. York University, New York, NY, Nov.  [6] Catmull, E. The problems of com-1975. 
puter-assisted animation. Computer [11] Weber, L., Smoliar, S.W., and Badler, Graphics 12, 3 (Aug. 1978), 
348-353. N.I. An architecture for the simula- tion of human movement. Proc. ACM National Conf., Dec. 
1978, 737-745.  [12] Wessler, B. Computer-assisted visual communication. Ph.D. Disser- tation, Dept. 
of Computer Science, University of Utah, Salt Lake City, UT, August 1973.  7 t-t (a) ground (b) ~tep 
/rr r fTT Y i'F'h ,,~ ..,"/If F I ~ I i I I \\\x ~llll I f T I 1 I I \\\xx .~7i'/////// ; ? Ix~\ II1.I,1:.1 
I ! I I I X X \\\\ I//"/I I I I i / : I \ \ \\\ //l/.l::.f ? .,[:'.I..,L 1: I I \ \kk I/I/I / I I I 
.t 1 \ \X\'.,~ /litiT;f :t-i, i :f.,.t: I It l\t III1 : I I I.'<7 1\.\\,.,,,,,, 81I I 11 L.t.:.l.x " 
.. ~:.\:",,-,>.,, nll ll I I-T 1,.\\>.._: tll.l. {:.~1 I ,1.1 71.:.!; I I I Ilk L\II\7~ '~ ~ ~- \'.,,-,._ 
//1t " i /l / t\.)l <t::::! I 1. I 1 1, I I lily \\\ \ \ \ \ \ '\ \ "." --'/I/ x\~\\X I I I 1.1 I I 
I 1I// \\\ \ \ \ \ \ \ \ ~. ~_~...,I// "\\\\~i. I 1 lt F:I [lie -i\\\ x '%..l 11 I I ~" Ylll:" \\\\\ 
\ \ \ \ ~ ~L~///i (It) '%\\\ X:'1 I T I I f I I fill Plgure9, Path of the center of gravity during changes 
in hvd (d, (b) ~"'<'~'''"-"---'-~--'2"~ steps (b), and Jumps (C). ] Line of desired center of gravity 
for balance :// :/.'~'~57 7 -7 " i 7" \'\¢-~, I////I 1' I \ \ \\x\\~\  \, 'l,. ~ i ~i I/t"// .1//I 
I .\ .\ ;\.'-.',-- ~ ~ .-\\~\ center of mass ",,, *'~ it ii ~ I ]//,//" ,,,  :: < ,, < va'.~ 5-> 7:~\~ 
 of counter- rotating segr~nts # (.[-..... :..:..:.:. ".....f: ..!/-1.:/1/I ' \\h\l~ ll!~il I ! '1,llll 
I in.! ".v., ~ \ ~1 ' !I h;ltlll s/H : .:~.~.K[7 ./. l: ~. \ " "\\~ i \'~p'lliil!!';U'l''l/l\///:''!.'';5.2A'l, 
~//i ~l I l I: I!,., ~5 1 7.: ~ . \ \ -. TM ' k(" ~: .... * ~ ~ .'.. ' //~ ~.\,v,\\k\ ~ ¢~'t. ~[':J,~ 
I J' 7 1 11 il:: ..... ,\\\ .N i ~r~-..Jotnt near actual center of gravity v . ¢ 7 1 v ~ ' // ...... 
,, ,~:<.',~4:D? ..... for counter-rotation fixed end ">"~ L I 1 .1 '~, \ \"~ .A ,...... 'v~E.~.>/ i.y 
I t \.\~/ center of mass of rotated segnents  // /.i l. I \.\'L) ,"':2,,-,: : "',,,7 computed center 
of mass of final %Y .Xt \~ ,I \ "..i--it,,/ position of rotated segments "'¢'/ / I I I ~ x ~ . . "~/17 
I I k % %~" Yigul~e II. St/JnJar'el opJGlt(itJon ruler.ion showing thunb dlrerltlotl Oil ]c[t h,.md: 
(~1) IPOlit, (h) h'gb ride, (c) btck, ~pport point in ~upport polygon arrJ (d) top vb,ws, llote the 
"seam" wt¢~rct th,~ orSer, iation is TIC,17 figure 10, Balance Is to-established by rotating body segments 
below ~II[ i rluous, tile center of gravity and counter-rotatlng tbe remaining body $mgnlents. The rotation 
(angle O) is cc~,puted to )lace the actual center of gravity just ~Ithin the support polygon.  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1980</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>807492</article_id>
		<sort_key>198</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-1980</article_publication_date>
		<seq_no>33</seq_no>
		<title><![CDATA[The integration of subjective and objective data in the animation of human movement]]></title>
		<page_from>198</page_from>
		<page_to>203</page_to>
		<doi_number>10.1145/800250.807492</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=807492</url>
		<abstract>
			<par><![CDATA[<p>Animation of human movement can be based either on analog inputs derived directly from actual movements or on symbolic inputs chosen to produce the desired movement. The former type of input can be quite accurate and objective but is a description of the required movement whereas the latter is often quite imprecise and subjective but provides an analysis of the required movements. Two existing systems for a computer based animation are being used to explore the problems involved in integrating such inputs. Specifically, animation driven by analog signals from electro-goniometers is integrated with animation derived from Labanotation commands; the results are illustrated with a short movie.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Animation]]></kw>
			<kw><![CDATA[Dance notation]]></kw>
			<kw><![CDATA[Labanotation]]></kw>
			<kw><![CDATA[Movement abnormalities]]></kw>
			<kw><![CDATA[Movement notation]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Animation</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.2.10</cat_node>
				<descriptor>Motion</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010352.10010238</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation->Motion capture</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010238</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->Motion capture</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352.10010380</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation->Motion processing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Human Factors</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP39053869</person_id>
				<author_profile_id><![CDATA[81100552470]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[T.]]></first_name>
				<middle_name><![CDATA[W.]]></middle_name>
				<last_name><![CDATA[Calvert]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Kinesiology and Computing Science Departments, Simon Fraser University, Burnaby, British Columbia, Canada]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39070150</person_id>
				<author_profile_id><![CDATA[81100422104]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[J.]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Chapman]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Kinesiology and Computing Science Departments, Simon Fraser University, Burnaby, British Columbia, Canada]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39077014</person_id>
				<author_profile_id><![CDATA[81339521799]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[A.]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Patla]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Kinesiology and Computing Science Departments, Simon Fraser University, Burnaby, British Columbia, Canada]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Birdwhistell, R.L., Kinesics and Context: Essays on Body Motion Communication, University of Pennsylvania Press (1970).]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Benesh R. and J., An Introduction to Benesh Dance Notation, A. and D. Black, London (1956).]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Eshkol, N. and Wachmann, R., Movement Notation, Weidenfeld and Nicholson, London (1958).]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Hutchinson, A., Labanotation, Theatre Arts Books, New York (1970).]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Eshkol, N., Melvin, P., Mitchel, J., Von Foerster, H., and Wachman, A., Notation of Movement, Report BCL 10.0, Department of Electrical Engineering, University of Illinois, Urbana, Illinois (1970).]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Archer, L.B., A Study of Computer Aided Choreography, Royal College of Art, London (1975).]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Wolofsky, Z., Computer Interpretation of Selected Labanotation Commands, M.Sc. Thesis, Kinesiology Department, Simon Fraser University, Burnaby, B.C., Canada (1974).]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Savage, G.J. and Officer, J.M., Choreo: An interactive computer model for choreography, Proc. 5th Man-Machine Communication Conference, Calgary, Alberta, (1977).]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_obj_id>810121</ref_obj_id>
				<ref_obj_pid>800178</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Weber, L., Smoliar, S.W., and Badler, N., An architecture for the simulation of human movement, ACM Conference, pp. 737-745 (1978).]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Smoliar, S.W. and Weber, L., Using the computer for the semantic representation of labanotation, in Lusignan, S. and North, J.S. (eds), Computing in the Humanities, University of Waterloo Press, Waterloo (1977).]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Barenholtz, J., Wolofsky, Z., Ganapathy, I., Calvert, T.W., and O'Hara, P. Computer interpretation of dance notation, in Lusignan, S., and North, J.S. (eds), Computing in the Humanities, pp. 235-240, University of Waterloo Press, Waterloo (1977).]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_obj_id>810120</ref_obj_id>
				<ref_obj_pid>800178</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Calvert, T.W. and Chapman, J., Notation of movement with computer assistance, Proc. 1978 ACM Conference, pp. 731-734, (1978).]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Brown, M., A Graphic Editor for Labanotation, MSE Thesis, University of Pennsylvania (1976).]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Cousins, S. J., Hannah, R.E., Foort, J., A clinically viable electrogoniometer, 2nd Annual Int. Conf. on Rehab. Eng., Atlanta, Georgia (1979).]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Hoff, F., Dance notation preserved at Motsuji, Dance Research Journal, Vol. 9, pp. 1-4 (1977).]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Strauss, G.B., Wing, C., and Yuen-wah, L., Translated excerpts of Chinese dance notation,# Dance Research Journal, Vol. 9, pp. 6-11, (1977).]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Grieve, D.W. and Cavanaugh, P.R., How EMG patterns and limb movements are related to speed of walking, pp. 9-15 in Human Locomotor Engineering, Institution of Mechanical Engineers, London (1971).]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[Benesh, R. and McGuinness, J., "Benesh movement notation and medicine", Psysiotherapy, 60, pp. 176-178 (1974).]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 THE INTEGRATION OF SUBJECTIVE AND OBJECTIVE DATA IN THE ANIMATION OF HUMAN MOVEMENT T.W. Calvert, Jo 
Chapman, and A. Patla Kinesiology and Computing Science Departments Simon Fraser University, Burnaby, 
British Columbia, Canada Abstract Animation of human movement can be based either on analog inputs 
derived directly from actual movements or on symbolic inputs chosen to produce the desired movement. 
The former type of input can be quite accurate and objective but is a description of the required movement 
whereas the latter is often quite imprecise and subjective bL~ provides an analysis of the required move- 
ments. Two existing systems for a computer based animation are being used to explore the problems involved 
in integrating such inputs. Specifical- ly, animation driven by analog signals from electro-goniometers 
is integrated with animation derived from Labanotation commands; the results are illustrated with a short 
movie. Keywords: Animation; movement notation; Labano- tation; dance notation; movement abnormalities; 
 C.R. Classification: 8-1, 8-2, 3"41, 3"34.  I. Introduction  When human movement is animated for 
cartoons or advertisements the goals are generally either to produce an illusion of natural movement 
or to produce unnatural but stylized movement which cormaunicates a message. In scientific, medical or 
artistic applications, on the other hand, human movement is animated to replicate actual movement as 
closely as possible. In these appli- cations there is often one particular aspect of the movement that 
is of special interest so that attention can be focussed on this aspect at the expense of others. Animation 
can be driven either by analog inputsderived from actual human movements or by symbolic inputs where 
a convenient symbolism has been developed for the particular application. Analog inputs can be quite 
precise and are de- rived from high speed cinematography, from elec- trogoniometers and from other instrumentation. 
A wide variety of symbolic input systems have been developed to analyse or describe movement patterns 
in such disparate fields as interperson- al communication [I), dance [2,3,4], industrial time and motion 
study [4] and clinical medicine. This work was supported by a grant from the B.C. Health Care Research 
Fund. The crucial difference is that while the analog inputs provide a description of human movement 
the symbolic input can provide an analysis. For example, high speed cinematography or electro- goniometers 
can provide an accurate description of how the angle of a joint such as the knee changes with time whereas 
the symbolic input gives a relatively imprecise indication of the changes in angle but indicates that 
the changes have been analysed to be (say) the flexion of the knee during the swing phase of walking. 
 The purpose of this paper is to examine an approach to the integration of analog and symbolic inputs 
and to describe some experiments imple- menting this approach. The need to integrate descriptive inputs 
which are relatively precise and objective with analytic inputs which are relatively imprecise but subjective 
arises in a number of problem areas. For example, in the clinical diagnosis of movement abnormalities 
precise instrumentation is used to describe the pattern of limb movements but at the same time the trained 
physician or physiotherapist can observe the pattern and make quite subtle judgements based on the cognitive 
analysis of visual data. In this application the accurate descriptive data is seldom adequately integrated 
with the subjective analytic data although the latter is often an important component of a diagnosis. 
Other applications include scientific studies in human biomechanics where the accurate descriptive analog 
data is usually utilized to the exclusion of subjective but analytic data derived from a symbolic input. 
At the other extreme, in describing dance, input is generally based exclusively on analytic but imprecise 
notation to the exclusion of accurate but non- analytic analog measurements. In our research on human 
biomechanics and in our clinical studies of movement abnormalities we use precise analog inputs derived 
from high speed cinematography and from electrogoniometers. The high speed cinematography provides a 
very precise description of how a human body moves with time but the analysis is tedious and time consuming; 
the analog data is certainly not available in real-time. In contrast, the electrical signals from a goniometer 
are somewhat less precise but are available in real time and are being used to drive a computer based 
anima- tion system. In our research on the computer assisted notation of dance we have developed a comprehen- 
sive interpreter which produces animated output the title of the publication and its date appear, and 
notice Permission to copy without fee all or part of this ~terial is granted provided that the copies 
are not made or distributed 0]980 ACM 0-8979]-02]-4/80/0700-0]98 $00.75 198 45 ~ 44 ~0 ~9 3gN I 
 C3~1 o I  1 Figure I. A partially completed Labanotation score. when a score written in a particular 
dance nota- tion, Labanotation, is provided as input. This paper focusses on the integration of these 
two quite different inputs to an animation system. The two input media will be described before their 
integration is discussed. 2. Computer Interpretation of Dance Notation Languages for the definition 
of human move- ment find application in dance, clinical medi- cine, industrial time and motion analysis 
and the development of animation systems. Perhaps the greatest interest has been in dance, where in spite 
of the development of a number of rival notation systems, there is still a need to bring the art to a 
satisfactory level of literacy. The difficulty with all movement notation systems is that they are inherently 
complex and difficult to master. This has led to a number of proposals for the implementation of computer 
interpretation systems [5,6,7,8,9]. The computer is seen as a tool which can aid in the composition and 
editing of the movement notation score [I0] and as an interpreter which can animate a score and thus 
assist in learning [8,9,11]. Notation systems have been developed in many cultures [15,16]. Some systems 
are particularly suited to certain stylized movements such as classical ballet (Benesh notation [2]) 
while others are more general [Eshkol-Wachman notation [3] and Labanotation [4]). One of the most general 
systems is Labanotation and at least three computer based schemes for the interpre- tation of this notation 
are under development (Weber, Smoliar and Badler [9] at the University of Pennsylvania, Savage and Officer 
[8] at the University of Waterloo and our own system at Simon Fraser University [7,11,12]). Labanotation 
is written on a vertical staff where the central columns represent the support of the left and right 
feet respectively. Moving out from the centre, successive col~nns represent the gestural movements of 
the left and right legs, the body, arms and hands respectively. The head is arbitrarily placed on the 
right. Symbols are placed in the columns of the staff to indi- cate the level and direction of a movement 
while its time duration is indicated by the vertical length of the symbol. A partially completed score 
is illustrated in Figure I. We have developed an interactive editor for the input of Labanotation [III 
and the score can be built up on a DEC GT40 graphics terminal. Currently we are implementing the more 
comprehensive input editor developed at the University of Pennsylvania by Brown and Smoliar [13]. The 
output of the interactive editor consists of an alphanumeric code which forms the input to an interpreter. 
The structure of the interpreter is summarized in Figure 2. Since this is a research tool, wherever possible 
the system has been made quite general. The topology of the body is defined with input data by specifying 
name, identification number, distal joint number, proximal joint number and length for each limb. This 
allows us to specify any con- nectivity for any number of limbs. In fact the body we define has 22 limbs 
with rather con- ventional topology (Figure N, but at least in principle we could define any body (e.g. 
that for a spider). It is also convenient to define six complex limbs (two arms, two legs, trunk and 
head) since these are regarded as the fundamental units of independent movement. Thus the complex limb 
"whole leg left", for example, comprises "upper leg left", "lower leg left" and "foot left". Generality 
is also maintained by allowing the user to define the names and meanings of the input commands. Thus 
although we normally define the Labanotation horizontal directions "front", "front left". "left side", 
"back left" and "back" to be 0 °, 45 O, 135 ° and 180 ° respectively these can be specified arbitrarily 
by the user. The body position is initialized to the default posi- tion assumed by Labanotation by defining 
the an- gular orientation in space and the rotation of each limb. In operation, the interpreter reads 
in a segment of instructions and translates the sym- bols to directions. The overlapping commands for 
the different limbs are entered into a data base which lists, for each limb, all orientation changes, 
the time at which they must commence and the time by which they must be completed. At this stage, the 
anatomic constraints imposed by joints are checked to ensure that the move- ments are realizable. Support 
changes must be handled separately, since they involve movement of the centre of gravity of the body 
and since they imply (rather thsn explicitly specify) angular changes for limbs. The database, which 
records the start and end times for all orientation and rotation changes for all limbs, is integrated 
with the topology of the body to calculate the cartesian co-ordinates of the 23 principal joints of the 
body. Interpolation is used to calculate these three-dimensional co-ordinates at the desired frame rate 
(typically 14/second). For display, two-dimensional projections are produced for any user specified viewpoint. 
 The interactive system for graphical entry and editing of Labanotation scores has been implemented on 
DEC GT40 graphics computer. The interpretation program is written in PASCAL and runs on a PDP-ll/34which 
drives the display on our Evans and Sutherland PS-]. We currently plan to transfer the system (which 
is written in PASCAL) to a PASCAL MICROENGINE driving a raster graphics display. The success of the 
animation can only be judged by viewing the results. Wolofsky [7] tested an earlier version of this system 
by showing a class of dance students two short movies; one was produced by the computer inter- pretation 
of Labanotation, and the other showed a stick figure derived from a movie taken of a dancer who was interpreting 
the same sequence of notation. The observers in this limited experiment judged both versions to be equally 
natural. We have now produced several more com- prehensive 16mmmovies illustrating the use of the system, 
and copies of these are available on loan from the authors.  3. Animation Driven by Analog Inputs 
The analog input is derived from an electro- Figure 2. DEFINE BODY TOPOLOGY DEFINE INSTRUCt IONS INITIALIZE 
BODY POSITION 3 ACCEPT SE(~IENT OF INPUT CODE INTERPRET C(~4ANDS (GESTURAL AND MOTIONAL) ENTER ANGLE 
(flANGES INTO LINB DATA BASE 1 A USE LIMB DATA BASE TO CCNPUTE CARTESIAN CO-ORDINATES OF EAGI JOINT 
FOR EAO-I FRAME 21 PRODUCE 2- DIMENS IONAL PRDJECTIONS FOR DISPLAY l Structure of the Labanotation 
Interpreter.  Table 1 *MACRO RLSTEPB 1 0.00 ULR 15.00 PL 1 0.00 LLR I0.00 BL 1 0.00 TOR 30.00 Ht 1 
10.00 LLR 20.00 PL 1 15,00 ULR 15.00 PL 1 30.00 ULR 20.00 PL 1 30,00 LLR 20.00 PL 1 30.00 TOR 20.00 FM 
0 40.00 ANR I0.00 *END *>~CRO RLSTEPE 1 00.00 ULR 17.00 PL 1 00.00 LLR 20.00 PL 1 00.00 TOR 20.00 FM 
0 00.00 ANR 20.00 1 17.00 ULR 18.00 PL 0 20.00 TOR 20.00 1 20.00 LLR 15.00 BL 1 20.00 TOR 30.00 FL 1 
35.00 ULR 15.00 PL 1 35.00 LLR 15.00 BL *END %~CRO LLSTEPB 1 0.00 ULL 15.00 PL 1 0.00 LLL I0.00 BL 1 
0.00 TOL 30.00 ~I 1 I0.00 LLL 20.00 PL 1 15.00 ULL 15.00 PL 1 30.00 ULL 20.00 PL 1 30.00 LLL 20.00 PL 
1 30.00 TOL 20.00 FM 0 40.00 ANL I0.00 *END *MACRO LLSTEPE 1 00.00 ULL 17.00 PL 1 00.00 LLL 20.00 PL 
1 00.00 TOL 20.00 FM 0 00.00 ANL 20.00 1 17.00 ULL 18.00 PL 0 20.00 TOL 20.00 1 20.00 LLL 15.00 BL 1 
20.00 TOL 30.00 FL 1 35.00 ULL 15.00 PL 1 35,00 LLL 15.00 BL *END *~CRO WALKRL *RLSTEPB 0.00.1.00.1.1.1 
*LLSTEPE 0. 00.1. 00.1.1.1 *RLSTEPE 1.00.1.00.1.1.1 *LLSTEPB 1.00.1.00.1.1.1 *END  FL BL FM PL FL PL 
PL PL BL FM PL BL FL BL FM PL FL PL BL D,I PL PL BL PL PL FM PL BL BL FM PL PL BM FM FL FL BL FH PL BM 
BM FM FL FL FM FM FL FM FM FM FM shading. Thus the right foot is successively moved to +9 ° (above the 
horizontal), 0 ° and -8 °. With a little practice an observer can quickly learn to adjust selected parameters 
such as the angle of the foot to indicate a dragging heel. Although specialized functions (such as the 
angle of the foot in walking) can be mastered fairly quickly, Labanotation and other notation systems 
have proven to be rather difficult to master completely and they have been very little used in clinical 
medicine (although recently it has been reported that Benesh notation is being successfully used in clinical 
applications in Britain [18]). We have developed a macro language to supplement the rather basic Labanotation 
co~mnands which are at a level comparable to computer assembly language. With this macro language we 
can tailor conmnan~s for each applica- tion and provide the physician with a refined tool [12]. The Macro 
WALKRL, for example, animates a continuing walk starting on the right foot; it must be preceded by the 
Macros WALKRLI and IVALKRLE which respectively handle the initiation and the end of a walk sequence. 
The WALKRL macro in turn calls the following macros: RLSTEPB: begin a right footed step LLSTEPE: end 
a left footed step RLSTEPE: end a right footed step LLSTEPB: begin a left footed step These macros are 
made up of elemental Labanotation commands as shown in Figure 6. The alphanumeric code for the macros 
is shown in Table I. The macros are called with the standard parame- ters: start-time, duration, and 
the number of repeats. It is rather simple to add any other parameter or parameters of interest and thus 
an observer could experiment, for example, with different limits on the movement of the left foot. 5. 
Discussion and Conclusions PL PL BL D.~ PL BL BL PL PL FM PL BL BL FM PL BM Fbl FL FFM The integration 
of analog and symbolic in- puts to the animation system was achieved by (a) driving an animated display 
with analog data from goniometers, (b) building a pattern of notation to produce an animated display 
which duplicates that produced by analog data, and (c) modifying the notation to reproduce those subtleties 
observed in the real movement pattern but not captured by the instrumentation. The disadvantage of this 
approach is that it involves producing a notated score of the complete movement pattern which is to be 
ani- mated, Although this process can usually be shortened by using stored patterns for a standard walk, 
there would be obvious advantages in only notating that part of the movement pattern which is of primary 
interest. The major difficulty in integrating an isolated fragment of notation is that it must somehow 
be synchronized with the analog data. This could be done by adding a timing channel to the instrumentation 
to provide a definite synchronization such as the heel-strike. Another approach would be to automatically 
produce a 15 lloff, F., Dance notation preserved at first approximation to the notation pattern from 
Motsuji, Dance Research Journal, Vol. 9, the measured angle data. This approach is non- pp. 1-4 (1977). 
trivial to implement but would be of real inter- est for its own sake as a means to simplify the 16 Strauss, 
G.B., Wing, C., and Yuen-wah, L., production of dance notation. Translated excerpts of Chinese dance 
nota- tion, Dance Research Journal, Vol. 9, pp. 6- 11, (1977), References 17 Grieve, D.W. and Cavanaugh, 
P.R., How EMG 1 Birdwhistell, R.L., Kinesics and Context: patterns and limb movements are related to 
Essays on Body Motion Conmnunication~Universi- speed of walking, pp. 9-15 in Human Loco- o~nhsy!iV~iSd'-~ed~2;-(1970)'.ty 
motor Engineering, Institution of Mec~cal -  Engineers, London (1971). 2 Benesh R. and J., An Introduction 
to Benesh Dance Notation, A. ~[-D.-Black, London (i956). 18 Benesh, R. and blcGuinness, J., Benesh move- 
ment notation and medicine", Psysiotherapy , 3 Eshkol, N. and Wachmann, R., Movement Nota- 6_2 , pp. 
176-178 (1974). tion, Weidenfeld and Nicholson, London (19~58). 4 Hutchinson, A., Labanotation, Theatre 
Arts Books, New York--. Eshkol, N., Melvin, P., Mitchel, J., Von Foerster, H., and Wachman, A., Notation 
of I i I i i ! ~ i I I I I ! I I I I I I ! I ! I l~.J ! I/'-.l..--'l-i-l, , i l ~ I i I I l i I Movement, 
Report BCL I0.0, Department of Electrical Engineering, University of Illi- nois, Urbana, Illinois (1970). 
 6 Archer, L.B., A Study of Computer Aided Choreography, Royal ColTege-ofArt, London (19 78) 7 Wolofsky, 
Z., Cg/_~uter Interpretation of Selected Labanotation C6nmnands, M.Z~d~T~esis, Kinesiology Department, 
Simon Fraser Univer- sity, Burnaby, B.C., Canada (1974). Figure S. Angle-angle plot for hip versus knee 
 Savage, G.J. and Officer, J.M., Choreo: An joint.  interactive computer model for d~oreography, Proco 
5th Man-Machine Communication Confer- ence, Calgary, A-Tb~-Ti~-~ 9 Weber, L., Smoliar, S.W., and Badler, 
N., An architecture for the simulation of human iO0 .... movement, ACM Conference, pp. 737-745 (1978). 
 i0 Smoliar, S.W. and Weber, L., Using the computer 8c for the semantic representation of labanota- 
 £ ' h  tion, in Lusignan, S. and North, J.S. (eds), 70 C~tetr~~ongprin the Humanities, University 
of ess~-~T~T977). Q~ Ii Barenholtz, J., Wolofsky, Zo, Ganapathy, I., Calvert, T.W., and O'Hara, P. Computer 
 - interpretation of dance notation, in Lusi~mn, 40 S., and North, J.S. (eds), Computin~in the &#38; 
Ht~nanities, pp. 235-240, Uni~ers~ b-f--.... ~-terloo Press, Waterloo (1977). E 12 Calvert, T.W. and 
Chapman, J., Notation of jo t movement with computer assistance, Proc. 1978 ACM Conference, pp. 731-734, 
(1978). o 13 Brown, M., A Graphic Editor for Labanotation, MSE Thesis, -]]fi~e'r~ty" ~~V~ii~'i~T6). 
14 Cousins, S.J., ttannah, R.E., Foort, J., A clinically viable electrogoniometer, 2nd Annual Int. Conf. 
on Rehab. En&#38;~, Atlanta, Figure 6. Simplified and detailed notation for Georgia (1979)~ walking. 
 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1980</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>807493</article_id>
		<sort_key>204</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-1980</article_publication_date>
		<seq_no>34</seq_no>
		<title><![CDATA[Graphic analysis and planning of electrical distribution systems]]></title>
		<page_from>204</page_from>
		<page_to>210</page_to>
		<doi_number>10.1145/800250.807493</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=807493</url>
		<abstract>
			<par><![CDATA[<p>Techniques for computer modeling of electrical distribution systems have been available to utility engineers for years. The formation of a distribution data base can be a huge task because of the enormous number of components in a utility's distribution system. The continuous changes that occur in a distribution system make data base maintenance difficult. Application of computer graphics to distribution engineering simplifies data base formation and maintenance and aids in interpreting the results of analysis. Graphic-aided distribution analysis allows the engineer to quickly see the results of alternate modes of system operations, thus speeding the iterative process of decision-making. Future applications for graphics techniques in utilities extend to distribution SCADA, work order prepration and protective device coordination.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>H.2.4</cat_node>
				<descriptor>Distributed databases</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>J.2</cat_node>
				<descriptor>Engineering</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.8</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010432.10010439</concept_id>
				<concept_desc>CCS->Applied computing->Physical sciences and engineering->Engineering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010507</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Graphics recognition and interpretation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002951.10002952.10003190.10003195</concept_id>
				<concept_desc>CCS->Information systems->Data management systems->Database management system engines->Parallel and distributed DBMSs</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Management</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P330594</person_id>
				<author_profile_id><![CDATA[81100161373]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Gwendolyn]]></first_name>
				<middle_name><![CDATA[L.]]></middle_name>
				<last_name><![CDATA[Fuehring]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Scott and Scott Consultants, Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Kurtz, Edwin B. and Shoemaker, Thomas M. (Ed) "The Lineman's and Cableman's Handbook", McGraw-Hill, New York, 1976, Chapter 2.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Scott, W. G. "Skim the Cream Off Distribution Costs." Electric Light and Power (March, 1975).]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 GRAPHIC ANALYSIS AND PLANNING OF ELECTRICAL DISTRIBUTION SYSTEMS Gwendolyn L. Fuehring Scott and Scott 
Consultants, Inc. SUMMARY Techniques for computer modeling of electrical distribution systems have 
been available to utility engineers for years. The formation of a distri- bution data base can be a huge 
task because of the enormous number of com- ponents in a utility's distribution system. The continuous 
changes that occur in a distribution system make data base maintenance difficult. Application of computer 
graphics to distribution engineering simplifies data base formation and maintenance and aids in interpreting 
the results of analysis. Graphic-aided distribu- tion analysis allows the engineer to quickly see the 
results of alternate modes of system operations, thus speed- ing the iterative process of decision- making. 
Future applications for graphics techniques in utilities extend to distribution SCADA, work order prep- 
aration and protective device coor- dination. THE ELECTRICAL DISTRIBUTION SYSTEM A complete electric 
power system con- sists of generating stations where mechanical power is converted to elec- trical power, 
transmission lines which carry bulk power to distribution sub- stations, and a distribution system which 
delivers electricity from the distribution substation to each of the utility's customers. A simple electric 
power system is depicted in Figure i. Permission to copy without fee all or part of this material is 
Eranted provided that the copies are not made or distributed for direct c0merclal advantage, the ACM 
copyright notice and the title of the publication and its date appear, and notice is given that copying 
is by permission of the Association for Computing Machinery. To copy otherwise, 0r to republish, requires 
a fee and/or specific permission. &#38;#169;1980 ACM 0-89791-021-4/80/0700-0204 $00.75 The distribution 
system is complex in terms of number of components and the frequency at which these components change. 
A distribution system con- tains thousands and thousands of com- ponents including transformers, poles, 
capacitors, protective devices and services. Every day changes occur --- new customers are connected, 
a Car knocks a pole over, new lines are built to accomodate industry or new subdivisions, a storm may 
take a line out, buildings are torn down and so on. Adding to this complexity is the large geographical 
area containing the com- ponents and the fact the~£hese com- ponents are uniquely combined in each small 
area of the system. The intri- cate and dynamic nature of distribu- tion systems requires the use of 
com- puters for modeling and analysis. Most of the information necessary for modeling a distribution 
system is shown on the utility's single line diagrams. Figure 2 shows part of a Eeeder single-line diagram 
for a utility in South Carolina. Distribu- tion feeders are the lines which extend from the substation 
to the various points of distribution to consumers. Each feeder has numerous branches and is normally 
operated radially, which means that the feeder has a tree con- figuration with no loops in its branches. 
 Following is the information required for modeling distribution feeders: Feeder Configuration  Primary 
Lines  Branches  Ends of Branches  Scott and Scott Consultants,Inc., P. O. Box 28549, West County 
Branch St. Louis, Missouri 63141 Line Physical and Electrical Charac- teristics  Length  Spacing 
 Type of Conductor  Number of Phases  Underground or Overhead  Resistance  Reactance  Capacity 
  Voltage  Equipment Locations and Ratings  Distribution Transformers  Autotransformers  Regulators 
  Boosters  Capacitors  Switches  Fuses  Reelosers  Sectionalizers  Loading Information 
 Number of Customers  Energy Requirements  Peak Demands  Special Loads  NON-GRAPHIC ANALYSIS 
OF DISTRIBUTION SYSTEMS Electrical distribution engineers must be able to identify trends, highlight 
new developments, and con- stantly update an overview of the entire distribution system. Economic operation 
of the system requires planning that corrects present operat- ing problems and provides for future load 
growth while avoiding costly overconstruction. Distribution system planning is an iterative process 
of combining the various elements, analyzing each combination and evaluating the results of analysis. 
For the distribution engineer, computerized distribution analysis is a tremendously powerful tool in 
comparison with rule-of-thumb techniques previously used. Distri- bution Primary Analysis (DPA) creates 
a digital computer model of a distri- bution system and can be used to model either actual or simulated 
conditions. Various types of analyses can be per- formed such as voltage regulation, loading, losses, 
fault current levels, and optimum placement of capacitors. With DPA the engineer synthesizes the problem, 
the computer analyzes the problem, and the engineer evaluates results of analysis. =~[~} ] ~ DISTRIBUTION 
 VOLTAGE TRANSMISSION LINE ~T~ PLANT HLGH ..... GENERATION.... K---TRANSMISSION--- --! DISTRIBUTION 
.... FIGURE 1 ---SIMPLE ELECTRIC SYSTEM 205 / \ i ~4ACSR FIGURE 2 ---DISTRIBUTION FEEDER DIVIDED 
INTO NODES AND SECTIONS DPA employs a central data base and DATA BASE FORMATION M~AINTENANCE uses 
modular programs to perform speci- fic analyses and for data base main- SWITCHING MANUAL METHODS tenance 
(Figure 3). The centralized ON-LINE EDITING OR DIGITIZING data base permanently stores the system 
 LIST FILES parameters in node, section, map feeder, UPDATE LOADING equipment, and conductor files. 
To simplify data handling, random num- bering is used for points (nodes) DATA BASE FILES on a circuit 
and line segments (sec- tions) between nodes (Figure 2). The FEEDER section file contains the line and 
 CONDUCTOR load data necessary to perform the SECTION various analyses. The map file con- NODE tains 
digital maps which indicate EQUIPMENT how the nodes and sections are con- DIGITAL }UPS nected together. 
If a portion of one COORDINATES circuit is switched to another cir- cuit or if new lines are constructed, 
a new digital map is generated to reflect the revised switching sequence A~;ALYSIS of the circuit. VOLTAGE, 
LOADING, LOSSES Basic data for the section file is 'FAULT LEVELS usually obtained from distribution 
 CAPACITOR LOCATION maps and customer-accounting or trans- former management files. In the manual extraction 
of data (Figure 4), nodes PLOTTING are marked on a circuit map and node and section numbers are assigned 
from SINGLE-LINE DIAGRAMS VOLTAGE, FAULT PROFILES a list of unused numbers. Section SINGLE-LINES WITH 
 length is measured with ruler or map ANALYSIS RESULTS measurer. Node and section data is FIGURE 3 
 then recorded on forms and keyed into data files through a computer ter- CENTRAL DATA BASE AND MODULAR 
PROGRAMS minal. To obtain load data and number of customers on each section, each consumer or each 
transformer is asso- ciated with the serving line section. An interface program collects trans- former 
and load data for each section and enters it to the section file. At this point, data extraction is 
com- plete and ready to be checked for errors and thecreation of digital maps. Manual formation of the 
DPA data base typically takes three days of a technician's time for each cir- cuit. The modular interactive 
programs of the distribution system model (Figure 3) enable the distribution engineer to simulate 
and analyze any operat- ing condition. A summary of the analyses at the computer terminal aids in 
spotting portions of a cir- cuit with low voltage, overloaded conductors or excessive fault levels. 
 Detailed output is available from the line printer (Figures 5 and 6). The time and cost involved in 
imple- menting and maintaining a distribu- tion system model can be almost imme- diately justified 
through the benefits. Investment can be minimized by identifying surplus capacity which can be used 
to accomodate load growth instead of building new facilities. Additional savings are realized by reducing 
losses and pinpointing over- load conditions prior to equipment failures. Typically, a utility can 
 reduce capital investment for distri- bution by 10% and reduce losses 25% in the first year after 
implementing DPA (Reference 2). A 25% reduction in losses would save about $280,000 the first year 
for a small utility (i0,000 consumers) purchasing I00,000,000 kwh a year at 3.5C/kwh and having 8% 
losses. MANUAL DATA EXTRACTION i. Mark node points on map. 2. Assign node and section numbers and 
mark on map.  3. Measure section length.  4. Record data on forms.  For section file: a) section 
number b) conductor c) spacing d) phasing e) equipment For node file: a) node number b) node location 
 For transformer or customer file: a) transformer or customer number b) section number 5. Key in data 
from forms.  6. Correct recording and keypunch errors.  DIGITIZING i. Mark node points on map. 2. 
Digitize node points and digitize section data from menu.  3. Mark node and section numbers on map (number 
assigned by digitizer program).  FIGURE 4 --- STEPS IN DPA DATA BASE FORMATION FAULT CURRENT LEVELS 
FEEDER 7 SUB-2-NW SUBSTATION VOLTAGE 12.47 KV LINE TO LINE FAULT DUTY BUS IMPEDANCES (OHMS) Ri = 1.678 
X1 = 1.324 RO = 0.858 XO = 0.568 .............. CUMULATIVE ............. NODE PREV MILES POSITIVE SEQ. 
ZERO SEO. PH-TO-OR PH-TO-PH 3-PH SECT FROM R X R X MIN~ MAX*~ (AMPS) (AMPS) WIRE SUB (OHMS) (OHMS) (AMPS) 
 BUS 4075 3368 I 336 AC 0.070 1.70 1.37 0.90 0.77 229 3891 2870 3299 2 336 AC 0.428 1.81 1.60 1.11 1.83 
228 3129 2595 2983 3 4 AC 0.745 2.63 1.86 2.02 2.85 222 2203 1946 2237 4 336 AC 0.862 1.94 1.88 1.37 
3.12 226 2497 2320 2666 5 336 AC 1.240 2.06 2.12 1.59 4.23 225 2114 2121 2438 6 336 AC 1.462 2.13 2.26 
1.72 4.89 224 1937 2018 2320 FIGURE 5 ---FAULT ANALYSIS 207 1980 SUMMER PEAK  FEEDER 7 SUB-2-NW VOLTAGE 
= 12.47 KV LINE TO LINE SECT END LGTH PHASE COND ..... LOAD THRU SECTION ..... VOLTAGE 120 BASE NODE 
K FT CONF SIZE COND KW KVAR AMPS CUSTSECT ACCU LEVEL % DROP DROP SUBSTATION TOTALS 2937. 1701. 126.0 
50 1 0.4 ABC 336 AC 27.7 2879. 1667. 462. 314. 0.I 0.1 125.9 51 2 1.9 ABC 336 AC 26.3 2735. 1579. 439. 
301. 0.5 0.6 125.4 52 3 1.7 B 4 AC 1.6 15. 9. 2. 4. 0.0 0.6 125.4 53 4 2.3 ABC 336 AC 25.0 2590. 1484. 
415. 282.0.6 1.1 124.9 54 5 2.0 ABC 336 AC 24.3 2516. 1429. 402. 275. 0.5 1.6 124.4 55 6 1.2 ABC 336 
AC 22.3 2302. 1294. 367. 248. 0.3 1.9 124.1 56 7 2.3 ABC 336 AC 20.3 2096. 1181. 334. 242. 0.5 2.3 123.7 
57 62 7.5 ABC 336 AC 1.9 195. 116. 31. 30. 0.1 2.5 123.5 FIGURE 6 ---VOLTAGE, LOADING, AND LOSS ANALYSIS 
 --LOSSES - KW KVAR 1.4 2.9 6.4 13.4 0.0 0.0 7.0 14.6 5.7 12.0 2.8 6.0 4.7 9.8 0.1 0.3 GRAPHIC 
ANALYSIS OF DISTRIBUTION SYSTEMS The problem solving power of dis- tribution primary analysis is ampli- 
 fied when graphic tools are used for data base establishment and interpre- tation of results. Studies 
that took months to accomplish with a calcula- tor and rule-of-thumb technology can be done in weeks 
using non-graphic distribution system analysis and can now be accomplished in days using graphic analysis. 
 The primary time saver in distribution system data base formation and manage- ment is the tablet digitizer 
with the data extraction software. Line and equipment information for a typical distribution feeder 
can be extracted in less than one day via digitizer as compared to three days by manual methods. A utility 
with i00 feeders can form a distribution data base in approximately four months of a technician's time 
as compared to a year or more by manual methods. The savings that result from modeling can be realized 
much sooner. The data is extracted from existing distribution maps using the digitizer stylus and 
a menu, which contains the possible conductors, spacing, phasing and equipment sizes for the utility 
(Figure 7). A portion of the menu is used for program control to indicate the type of point (substa- 
 tion, new or old node) being digitized from the map. The user marks node points on the map, tapes 
the map to the digitizer, types in map number and then digitizes nodes from the map, and line and 
equipment data from the menu. The automatic features of the digitizing software include calcula- tion 
of section !enqth, computation of node location code, assignment of node and section numbers and storage 
of node and substation co-ordinates for plotting later. The software stores transformer number or consumer 
number and section number for inter- facing with the customer/transformer file. Use of the digitizing 
system reduces the number of steps (Figure 4) in distribution primary data base establishment and eliminates 
recording and key punch errors. CONDUCTORS 3~ 2~ i~ SPACING TRANSFORMERS CAPACITORS FIGURE 7 ---EXAMPLE 
MEN[] Circuit diagrams generated on pen, electrostatic or CRT plotter are versatile tools in distribution 
sys- tem management. Single line diagrams plotted from digitizer co-ordinates and the DPA data base allow 
quick visual verification of digitized data. Data base and diagram updates can be easily accomplished 
by entering modi- fications through the digitizer and immediately generating a new plot. Circuit plots 
provide operating per- sonnel with current circuit repre- sentation and assures data consis- tency between 
data base files and sin- gle-line diagrams. Voltage and current profiles (Figure 8) and single-line 
diagrams plotted with voltage, loading and losses for each section (Figure 9) convey results of analysis 
much more clearly and rapidly than pages of computer printout. Graphic analysis plots allow engineers 
to quickly get the picture and get on with problem solv- ing. Analysis plotting saves engi- neering time, 
contributes to effec- tive decisions, and aids in commu- nicating ideas to non-engineers. In short, graphic-aided 
distribution system management is an "engineer- amplifier". 4000 / GG a. 3000 2500-- 2000 $6 7 a 1500 
o -- ~- 1000-- 27 '2 14 5 16 7 196T 5O0 I I 2 3 4 5 6 6s 7 MILES FROM SUBSTATION FIGURE 8 ---FAULT 
CURRENT PROFILE The next obvious step in graphic analysis of distribution systems is to make the analysis 
interactive on a CRT terminal. In an inter- active system, the engineer may make changes to a circuit 
plot using a light pen and then run an analysis program which displays results on the plot. The engineer 
may make addi- tional modifications such as switch- ing or changing conductors and rerun the analysis. 
The process of cir- cuit modification and graphic dis- play of analysis continues until the desired results 
are obtained. A per- manent copy of the final circuit plot and display of analysis is obtained by sending 
the screen image to a hard copy device. Future of Graphics and Analysis of Distribution Systems In 
the near future, real-time graphic analysis of distribution feeders will be available on SCADA (Super- 
visory Control and Data Acquisition). Present SCADA systems indicate to the dispatcher the voltage and 
loading levels at the substation but provide no information about conditions on a distribution circuit. 
Distribu- tion SCADA will display the voltage, loading and losses on a CRT color graphic terminal with 
colors and flashing lights emphasizing sections in trouble. Digitizer and plotter could also be teamed 
to aid in co-ordination of protective devices. The characteris- tic time-current curves of relays and 
fuses can be digitized and then plotted against each other for a check of protection levels. A time-consuming 
task that utility personnel face daily is preparation of construction work orders. The digitizer and 
menu concept could be employed in a system which would tabulate components needed for various pole assemblies, 
check warehouse inventory for necessary quantities, estimate material and labor cost for the construction, 
and revise ware- house inventory. References i. Kurtz, Edwin B. and Shoemaker, Thomas M. (Ed) "The 
Lineman's and Cableman's Handbook", Mc- Graw-Hill, New York, 1976, Chap- ter 2. 2. Scott, W. G. "Skim 
the Cream Off Distribution Costs." Electric Light and Power (March, 1975). 209 N / ! / /"/,/ / iz3~ 
 / / / ""/'///// l~ VDLTAG[ LEY£L &#38;W L~ THRbl)G~ 3ECTIO~ KW LOSSES IN 5[CTIDN DAlE /2q/80 PROGRESSIVE 
RNYT~N' XO. ELECTR]C FIGURE 9 --- SINGLE-LINE PLOT WITH RESULTS OF ANALYSIS  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1980</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>807494</article_id>
		<sort_key>211</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-1980</article_publication_date>
		<seq_no>35</seq_no>
		<title><![CDATA[B-spline surfaces for ship hull design]]></title>
		<page_from>211</page_from>
		<page_to>217</page_to>
		<doi_number>10.1145/800250.807494</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=807494</url>
		<abstract>
			<par><![CDATA[<p>The use of true sculptured surface descriptions for design applications has been proposed by numerous authors. The actual implementation and use of interactive sculptured surface description techniques for design and production has been limited. The use of such techniques for ship hull design has been even more limited. The present paper describes a preliminary implementation of such a system for the design of ship hulls and for the production of towing tank models using numerical control techniques. The present implementation is based on a Cartesian product B-spline surface description. Implementation is on an Evans and Sutherland Picture System supported by a PDP-11/45 minicomputer.</p> <p>The B-spline surface is manipulated by its associated polygonal net. Both surface and net are three-dimensional. Techniques both good and bad for 3-D picking of a polygon point when the net, its associated surface, and the 3-D picking cue independently exist and can be independently manipulated in three space are presented and discussed.</p> <p>The shape of a B-spline surface of fixed order is controlled by the location of the polygon net points, the number of multiple points at a particular net point, and the knot vector. Frequently multiple points imply multiple knot vectors. Practical techniques for controlling and shaping the surface with and without this assumption are discussed and the results illustrated.</p> <p>Experience attained by interactively fitting a single fourth order B-spline surface patch to the forebody half of an actual ship hull described by three dimensional digitized points is discussed and the results illustrated.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[B-spline]]></kw>
			<kw><![CDATA[CADCAM]]></kw>
			<kw><![CDATA[Computer graphics]]></kw>
			<kw><![CDATA[Numerical control]]></kw>
			<kw><![CDATA[Sculptured surface]]></kw>
			<kw><![CDATA[Ship hull]]></kw>
			<kw><![CDATA[Surface]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Splines</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Visible line/surface algorithms</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372.10010377</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering->Visibility</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003648.10003649.10003657.10003659</concept_id>
				<concept_desc>CCS->Mathematics of computing->Probability and statistics->Probabilistic representations->Nonparametric representations->Spline models</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP39030986</person_id>
				<author_profile_id><![CDATA[81100174598]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[David]]></first_name>
				<middle_name><![CDATA[F.]]></middle_name>
				<last_name><![CDATA[Rogers]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[United States Naval Academy, Annapolis, Maryland]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P333857</person_id>
				<author_profile_id><![CDATA[81100347503]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Steven]]></first_name>
				<middle_name><![CDATA[G.]]></middle_name>
				<last_name><![CDATA[Satterfield]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[United States Naval Academy, Annapolis, Maryland]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Izumida, K. and Matida, Y. Ship hull definition by surface techniques for production use, Proceedings of ICCAS '79 Computer Applications in the Automation of Shipyard Operations and Ship Design, North Holland, 95-104.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Munchmeyer, F.C., Schubert, C., Nowacki, H. Interactive design of fair hull surfaces using B-splines, Proceedings of ICCAS '79 Computer Applications in the Automation of Shipyard Operations and Ship Design, North Holland, 67-76.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>63448</ref_obj_id>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Rogers, D.F. and Adams, J.A., Mathematical Elements for Computer Graphics, McGraw-Hill, New York, 1976]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Rogers, D.F. B-spline curves and surfaces for ship hull design, Proceedings SNAME, SCAHD '77, First International Symposium on Computer Aided Hull Surface Definition, (September 1977), Annapolis, Maryland. 26-27]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>811712</ref_obj_id>
				<ref_obj_pid>800292</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Rogers, D.F., Rodriquez, F., Satterfield, S.G. Computer aided ship design and the numerically controlled production of towing tank models, Proceedings, 16th Design Automation Conference, (June 1979), 25-27. San Diego, California.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>563877</ref_obj_id>
				<ref_obj_pid>563858</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Satterfield, S.G., Rodriguez, F., Rogers, D.F. A simple approach to computer aided milling with interactive graphics, Proceedings of SIGGRAPH '77, Computer Graphics 11, 2 (1977), 107-111.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Stroobant, G. Soprindus, Bruxelles-Belgigue, private communication.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Yuille, I.M. The forward design system for computer aided ship design using a minicomputer, The Naval Architect, 120, (Nov 1978) 6, 323-341.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 B-SPLINE SURFACES FOR SHIP HULL DESIGN* David F. Rogers Steven G. Satterfield United States Naval Academy 
Annapolis, Maryland 21402 ABSTRACT The use of true sculptured surface descrip'- tions for design applicatiofis 
has been proposed by numerous authors. The actual implementation and use of interactive sculptured surface 
description techniques for design and production has been lim- ited. The use of such techniques for ship 
hull design has been even more limited. The present paper describes a preliminary implementation of such 
a system for the design of ship hulls and for the production of towing tank models using numeri- cal 
control techniques. The present implementa- tion is based on a Cartesian product B-spline sur-face description. 
Implementation is on an Evans and Sutherland Picture System supported by a PDP- 11/45 minicomputer. The 
B-spline surface is manipulated by its associated polygonal net. Both surface and net are three-dimensional. 
Techniques both good and bad for 3-D picking of a polygon point when the net, its associated surface, 
and the 3-D picking cue in-dependently exist:and can be independently mani- pulated in three space are 
presented and discussed. The shape of a B-spline surface of fixed order is controlled by the location 
of the polygon net points, the number of multiple points at a par- ticular net point, and the knot vector. 
Frequently multiple points imply multiple knot vectors. Prac-tical techniques for controlling and shaping 
the surface with and without this assumption are dis-cussed and the results illustrated. Experience attained 
by interactively fitting a single fourth order B-spline surface patch to the forebody half of an actual 
ship hull described by three dimensional digitized points is discussed and the results illustrated. * 
This work is particially supported by the Naval Ship Engineering Center and the U. S. Coast Guard. Permission 
to copy without fee all or part of this material is granted provided that the copies are not made or 
distributed for direct commercial advantage, the ACM copyright notice and the title of the publication 
and its date appear, and notice is given that copying is by permission of the Association for Computing 
Machinery. To copy otherwise, or to republish,  requires a fee and/or specific permission. ~980 ACM 
0-89791-021-4/80/0700-0211 $00.75 KEY WORDSAND PHRASES: Computer graphics, Sur-face, Sculptured surface, 
B-spline, Ship hull, CADCAM, Numerical Control. CR CATEGORIES: 3.20, 3.21, 3.26, 5.13, 8.2 INTRODUCTION 
Computer Aided Ship Hull Surface Design (CASHSD) has been investigated by a number of groups and several 
systems have been developed. Among these are Autokon, Viking, BRITSHIPS, FORAN, etc. In each case, a 
ships lines approach has been used, i.e. the surface of the ship is described by a net of lines. Usually 
the traditional three mutually orthogonal planes containing waterlines, station lines, buttock lines 
used by the naval architect have been chosen. Each set of lines is "faired" and then cross-faired until 
the surface is considered defined. The lines fairing techni-ques have been many and varied. Typically 
para- bolically blended curves, cubic splines, circular arc interpolated curves or rational polynomials 
have been used. Either interactive or batch type processing has been used. Implementation has typically 
been on a large scale computer. Recently a CASHSD/CADCAM system called CAMILL (~omputer Aided Milling) 
for the design of ship hulls and the production of towing tank models has been developed [5], [6]. CAMILL 
is implemen-ted on an Evans and Sutherland Picture System graphics display driven by a PDP 11/45 minicom- 
puter. It is highly interactive. Again, the ba- sic philosophy was to use a net of lines to define the 
surface. The designer is free to choose the type of fairing algorithm for each line from among parabolically 
blended, cubic spline, Bezier or B-spline curves up to order six. The system has been used to develop 
the fair hull shapes and pro- duce towing tank models for several ships. The most recent efforts involved 
two icebreakers for the U. S. Coast Guard [5]. Although CAMILL has been successful, it has become obvious 
that a full surface description technique is more desirable. THE NEED FOR SCULPTURED SURFACES True sculptured 
surfaces in engineering have traditionally been used in the field of aircraft, ship, and automobile design. 
They have also been extensively used in consumer product design. 211 Aircraft fusalages, wing-body fillets, 
cockpit can-opy fairings, and engine inlets require sculptured surface treatments. In ship design the 
bulbous bow, sonar domes, tunnel and conventional sterns as well as almost the entire underbody of a 
sailing yacht are sculptured surfaces. Because of styling considerations, automobile design has been 
a heavy investor in sculptured surface techniques. Con-sumer products as wide ranging as the shower sham- 
poo bottle and the case for the latest computer graphics device use sculptured surfaces. SURFACE DESCRIPTION 
TECHNIQUES The natural extension of the line or curve net surface description technique is the Coons 
sur- face patch [3]. The Coons patch, although more general than recognized, is normally implemented 
as a Cartesian product surface. In a Cartesian pro- duct surface the edge curves of a four sided sur-face 
patch are defined by algorithms similar to those used to define fair curves. Thus a Cartesian product 
Coons surface may be a parabolically blended surface, a cubic spline surface, a Bezier surface or a B-spline 
surface. As such, the sur-faces retain the advantages and disadvantages of the curves describing the 
edges of the patch. These advantages and disadvantages have been dis- cussed in [4] in the context of 
ship hull design. Reference [4] indicated that no one curve descrip-tion technique was a panacea. However, 
the most flexible of the curves for interactive ship design was found to be the B-spline basis curve. 
This is because the B-spline basis curve allows knuckles or sharp discontinuities within a contiguous 
curve description. It also provides significant data compression and convenient interactive curve mani- 
pulation handles. Thus, an extension to B-spline surfaces appeared to be of interest. POLYGONAL NET MANIPULATION 
The shape of a B-spline surface is controlled by a polygonal net of points. In the implementa-tion described 
here, the polygonal net which de- fines the B-spline surface, seen as a perspective or orthographic projection 
with intensity (gray) scaling, can be oriented in space by manipulating a set of control dials. The control 
dials provide rotation about the three coordinate axes, transla-tion along the three coordinate axes 
and overall scaling. An additional control dia} is used to adjust the sensitivity of the other dials. 
In interactively manipulating individual poly-gonal net points the basic problem is to acquire a defined 
polygon point in three space by manipula- ting various controls, i.e. to execute a three di- mentional 
hit test or pick. Several different techniques were implemented and tested. Three are described. The 
first implementation uses a three dimen- sional cursor in the form of a small cube. Figure la shows the 
CRT display with the cube and a poly-gon net. The polygonal net can be manipulated in three space independently 
of the cursor cube. The orientation of the polygonal net is shown by the 3-D axis system in the lower 
right hand corner. The cursor cube can be independently translated in the x,y,z directions by means of 
three control B-~IP!.INE ~,Ji~IE DISPI.AY  ,l ~o PERSPECT I V~ CURX -37 CURY B4 CURZ 7 XTRN 0 YTRH 
0 ZTRIql 0 8CRL X'R~T -180 YR~T gO ZROT 0 DIAL 3 "IT PE~CTIYE b J J jJ ,J \ Pf.RSPf~T 1 VE Figure 
l: Three dimensional pick scheme using cursor cube. 212 dials. The cursor cube is translated in the 
di- rection given by the 3-D axis system. The posi- tion of the cursor cube is continuously updated and 
displayed on the screen (Fig. la, CURX, CURY, CURZ). The objective was to place the cursor cube "over" 
one of the polygon net points to select that point for modification. Once the polygon point is selected, 
the three control dials are used to translate it to the required position. Figures Ib and Ic generated 
from the display, il-lustrate the fundamental problems. Figure Ib suggests that the cursor is over one 
of the points while Fig. Ic, in which the polygonal net has been reoriented, shows that this is not the 
case. While sufficient facilities are provided to accom-plish the required match, the process is slow, 
frustrating, and fatiguing. The basic difficulty is the necessity of manipulating two independent objects 
in three space with limited orientation cues. The technique is not recommended. The second implementation 
uses the fact that the nodes of the polygon net are numbered like the rows and columns of a matrix or 
the pixels of a raster scan display. A small keypad is generated in the lower left corner of the display. 
The tab- let pen and a screen cursor is used to indicate the number of a node by selecting digits from 
the key pad. [Fig. 2]. Once selected, the nodes are moved by using control dials to provide x,y,z translation. 
The relative location of the cursor is indicated as CURX, CURY, CURZ while the cursor is being moved. 
Again the entire net or surface can be manipulated in rotation and translation by control dials. The 
method is quite fast. How-ever, Fig. 2b shows a basic flaw in the scheme. In many orientations it is 
difficult to keep track of the numbering of the nodes. Thus, continuous reorientation of the polygonal 
net is necessary. Further, because the hardware character generator is used for speed, a phenomena similar 
to wrap a- round occurs when displaying the numbers. Figure 2c shows that adding the surface further 
excaber-ates the problem. The third implementation is a result of the realization that it, in fact, 
is not necessary to consider a three dimensional picking problem. To see this, note that the surface 
is controlled and shaped by the polygon net, which exists as an ordered set of points in the data base. 
What is seen on the display is either a perspective or orthographic projection of the polygonal net. 
The net is displayed by a simple drawing algorithm. Two dimensional hit testing can easily by per- formed 
by successively attempting to pass the projection of each polygon point through the 'hit' window. When 
a match is obtained, a 'hit' has occurred and the particular polygon point can be easily identified from 
the ordered list. The tablet is used to position the hit win-dow, which is made visible by means of a 
small tracking cross as shown in Fig. 3a. Once selec- ted, the position of the polygon net point is con-trolled 
by three dials. Figure 3b shows that se-lection in various orientations is quite easy. The control dials 
can be used to position the en-tire polygon net or surface to facilitate selec-tion of a particular point 
of interest (Fig.3c). 8-5PLINE 5UI~ACE OISPLAT C CIJRX 60 C.UII¥ ,-60 ~ 0 I~.II~M+(CT lYE O, * XTRN 0 
)(ROT -L79 YTRN YAOT 0 90 ZI"RH ZROT 0 0 SCRL 01111. 94 3 I--Jl-.I Figure 2: Three dimensional pick 
using numbered nodes. 213 A minimum amount of reorientation between picks is required. Figure 3c also 
illustrates that addi-tion of the surface does not overly complicate the operation. The scheme is fast 
and easy to use. Further, the overhead of supporting a three dimen-sional cursor is eliminated. User 
acceptance is good. B-SPLINE SURFACE ALGORITHM A Cartesian product parametic B-spline sur-face is given 
by m n Q(u,w) : ~ Bi+l,j+ 1Ni,k(U) Mj,~(w) i:O j=O where Ni,l(U) = II if x i ~ u < xi+ 1 ( 0 otherwise 
(u-xi)Ni,k_l(U) + (Xi+k-U)Ni+l,k-l(U) Ni,k(U) = Xi+k_l_xi Xi+k-Xi+ 1 Mj,l(W) = II if yj Sw < Yj+I 0 otherwise 
 (w-yj)Mi,~_l(W)+ (Yj+~-w)Mj+l,~_l(W) Mj,c(w) = yj+~_l_y i Yj+~-Yj+I and the xi,Y i, are the elements 
of a knot vector [3]. The shape controls for a B-spline surface are the order of the surface, the location 
of the poly- gon net points, whether multiple points occur on the polygon net, and whether multiple knots 
occur in the knot vectors. References [3]-[6] implement B-spline curves such that multiple vertices imply 
multiple knot values in the knot vector. This al- lows sharp corners or knuckles . The implementa- tion 
of the same concept for the surface is not as straightforward. For example, the calculation of the knot 
vector and the associated basis func-tion for the schematic polygon net configuration shown in Fig. 4a 
is straightforward. However, it is not as straightforward for the pplygon net con-figuration shown in 
Fig. 4b when multiple net points imply multiple or repeating knot values in the knot vector. In Fig. 
4b, the polygon net points (i,j) at (3,1), (4,1), and (5,1) and those at (3,2), (4,2), ~nd (5,2) are 
coincident or multiple ver-tices. If multiple vertices implied multiple in-ternal or repeating knot values 
in the knot vec-tor, a fourth order B-spline surface would yield a sharp discontinuity or knuckle along 
the line formed by these two sets of multiple vertices. However, the grid in Fig. 4b for net points (i,j) 
i=I..7 j>2 does not have any coincident or multiple vertices. This would imply no multiple internal or 
repeating knot values in the knot vector. Ex- amination of the basis function in the defining relationship 
for the B-spline surface implies that a single knot vector in either the u or w direc- tion is used to 
generate the basis function for 8-,~PLZNE f~JRFRCE DISPLRY PERSPECTIVE CURX 0 CURY 0 CUFIZ 0 XTRN 0 
YTRN 0 ZTRN 0 SCRL B8 J z~ llllIet m XROT 147 YROT 1~ ZROT -6 OIRL 3 Figure 3: Three dimensional picking 
in a two dimensional projection. the entire surface. Without independent control of the knot vector 
this would preclude the exis- tence of sharp discontinuities or knuckles within the surface patch itself. 
One possible scheme is to adjust the knot vector and thus the basis func-tion from net line to net line 
based on the char- acteristics of the polygon net. For the polygon net in Fig. 4b, this would imply that 
the knot vector in the u direction would have repeating 214 knot values for the Bi+l,j+ 1 corresponding 
to i<l and non-repeating values for i>l ,U' I ,U' I X I I I I T I I i l I l IIIII \ \ \\\\~ III// 
 I\ \\ \\~l/ll l I I \ \\ \\IV/I I I I i \till~ I X g Figure'4: B-spline polygon nets. 215 IMPLEMENTATION 
The algorithm is implemented in an interactive pilot program called BSSD (B-S_plineSurface De- scription). 
BSSD is modularly designed and imple- mented such that it can be substituted for the lines description 
techniques presently used in CAMILL. It is implemented in structured FORTRAN (RATFOR) on a PDP 11/45 
under the RT-II Version IIIB operating system. The interactive display system is an Evans &#38; Sutherland 
Picture System I. The program operates in two overlay segments in ap-proximately 42K bytes of memory 
with data arrays declared in virtual memory. Interactive shaping , ;" :.:~> j : ; ; ". ..... ", ".. 
 :! :. i ". "..". ';':~':"..; ;; . .. . ~ ~ ~ '.#." .~ ..... ".. .. ; . ; .; .'/...' Oo... ... ; ;...'~... 
"t'" " : e ..j°. :G')'~~, ... and orientation of the surface is controlled by menu picking using a tablet, 
by function switches and by control dials. The B-spline surface algorithm as implemented allows the net 
configuration shown in Figure 4b. However, multiple net points do not imply multiple CURX 0 CUR¥ 0 CURZ 
5 ~'I'HOG~API, t I C knots Thus, hard chines and knuckles within the patch are not possible. Figure 4c 
shows the planar surface generated by the polygon net in Fig. 4b. Figures 4d and 4e show the polygon 
net and assoc- XTRN XROT -2 ~ YTRN YROT -4 [60 ZTRN -[8 ZR~T -|33 $CAL OIRL 38 4 ]'~/-I/\ iated B-spline 
surface generated by the implemen- tation when the polygon net has no multiple verti- ces. In contrast, 
Figs. 4f and 4g show a polygon net with multiple vertices at (2,2) and (2,3), (2,5) and (2,6), (3,2) 
and (3,3), (3,5) and (3,6), and the associated surface. In Figs. 4f and 4g, multiple vertices do no__t 
imply multiple or re- peating knot values in the knot vectors Figure 4g shows that even without this 
capability, a 'cor- ner' with a very small radius can be generated. As implemented, the algorithm is 
limited to a fourth order B-spline surface, i. e. a bicubic sur- face. b TEST CASE BSSD is designed to 
test the feasibility of interactively, i. e., by eye, matching a B-spline surface to a known ship hull. 
The system can, of course, also be used for ab initio hull surface de- sign. In contrast to the technique 
implied by Munchmeyer et al [5], the objective is to use as few surface patches as feasible Using only 
a few surface patches significantly reduces the data storage requirements for the hull surface and provides 
a smoother (fairer) surface It also allows the designer to work on larger unit C '0 portions of the 
ship at one time, and thus, re- duces the time required to obtain a fair hull with the desired characteristics 
The forebody, i. e. the portion of the ship hull surface from the bow to midway to the stem, of an existing 
U. S. Navy ammunition ship (AE 23) was chosen as a test case. The ship has a sharp bow with a mild underwater 
bulb flaring back into a parallel mid-body and an essentially flat bottom [Fig. 5a]. Since the ship 
is symmetrical about the longitudinal plane, only half of the forebody need be considered. The ship hull 
sur- face does not have any knuckles or sharp corners in the area under consideration and is thus within 
 - the limits of the present B-spline surface imple- mentation d Figure 5: Test ship hull surface. 216 
 The body or stations lines* for the forebody of the ship were digitized from the existing lines drawing. 
Digitized points lie on the hull surface, and, thus constitute a three dimensional data base for the 
surface. These digitized points were dis-played on the Picture System I using BSSD. [Fig.5a] An impression 
of the extent of the shape of the surface can be obtained from Fig. 5a which shows a quarter section 
of the full ship. The full ship is 564 feet long, has a beam or width of 81 feet and a depth of 48 feet. 
A single, initially planar 5x7 B-spline poly- gonal net was created. This net was then interac- tively 
manipulated using the technique described above until the resulting B-spline surface matched the ship 
hull surface described by the digitized points. In manipulating the B-spline surface, an additional grid 
or net line was added in the area of the bow in order to more completely match the bulbous bow. Thus, 
the final B-spline polygonal net is 5x8. The digitized points and the assoc-iated polygon net are shown 
in Fig. 5b. The poly- gon net and the resulting fourth order B-spline surface are shown in Fig. 5c. This 
illustrates that the shape of the surface corresponds to that of the ship. Display or removal of the 
surface, polygon net, digitized points or any combination is controlled by function switches. Figure 
5d shows a section through the ship. The short straight lines are the B-spline surface. The points in 
the vicinity of the surface are the or- iginal digitized points and those to the right and outside are 
the polygon net points. The match be- tween the surface and the original digitized points is quite acceptable. 
These results were obtained in approximately l-l I/2 hours. More effort would yield an even better match. 
CONCLUSIONS AND RECOMMENDATIONS A B-spline surface patch generation and ma- nipulation scheme has been 
implemented and ap- plied to a test case using an existing ship. A single 5x8 surface patch has been 
acceptably fit to the entire forebody of the test ship using interactive techniques. The results are 
encoura- ging. Future work will concentrate on extending the algorithm to include sharp discontinuities, 
knuckles or hard lines within the patch and to in-creasing the speed of the algorithm. This techni- que 
should then allow even relatively complex sur- faces to be totally represented by from three to five 
surface patches. REFERENCES (1) Izumida, K. and Matida, Y. Ship hull defi- nition by surface techniques 
for production use, Proceedings of ICCAS '79 Computer Appli- cations in the Automation of Shipyard Opera- 
tions and Ship Design, Nc.rth Holland, 95-I04. (2) Munchmeyer, F.C., Schubert, C., Nowacki, H. Interactive 
design of fair hull surfaces using B-splines, Proceedings of ICCAS '79 Computer Applicatibns in the Automation 
of Shipyard Operations and Ship Design, North Holland, 67-76. (3) Rogers, D.F. and Adams, J.A., Mathematical 
Elements for Computer Graphics, McGraw-Hill, New York, 1976 (4) Rogers, D.F. B-spline curves and surfaces 
for ship hull design, Proceedings SNAME, SCAHD '77, First International Symposium on Computer Aided Hull 
Surface Definition, (September 1977), Annapolis, Maryland. 26-27 (5) Rogers, D.F., Rodriquez, F., Satterfield, 
S.G. Computer aided ship design and the numerically controlled production of towing tank models, Proceedings, 
16th Design Automation Confer- ence, (June 1979), 25-27. San Diego, Cali-fornia. (6) Satterfield, S.G., 
Rodriguez, F., Rogers, D.F. A simple approach to computer aided milling with interactive graphics, Proceedings 
of SIGGRAPH '77, Computer Graphics II, 2 (1977), I07-III. (7) Stroobant, G. Soprindus, Bruxelles-Belgigue, 
private communication. (8) Yuille, I.M. The forward design system for computer aided ship design using 
a minicom- puter, The Naval Architect, ]20, (Nov 1978) 6, 323-341.  *body or station lines are formed 
by cutting~planes through the ship hull surface perpendicular to the longitudinal axis. 217  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1980</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>807495</article_id>
		<sort_key>218</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-1980</article_publication_date>
		<seq_no>36</seq_no>
		<title><![CDATA[Interactive computer graphics at Ford Motor Company]]></title>
		<page_from>218</page_from>
		<page_to>224</page_to>
		<doi_number>10.1145/800250.807495</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=807495</url>
		<abstract>
			<par><![CDATA[<p>Over the past fourteen years the Ford Motor Company actively pursued the development and use of interactive computer graphics in the design and manufacture of automobiles. The success of Ford's pioneering efforts in the area of minicomputer graphics led to widespread acceptance and support for graphics within the corporation. This in turn led to a rapid expansion of computer graphics equipment. The advent of numerous Federal regulations in the areas of fuel economy, emissions control, and safety has generated additional Corporate interest in the use of computer graphics to help solve these difficult problems. Use of the Ford Computer Graphics System, as well as various commercially available turnkey computer graphics systems, has contributed substantially to cost reductions, improved productivity, and better documentation and control.</p> <p>This paper describes the on-going computer graphics effort at Ford Motor Company. A brief history of computer graphics is presented along with a detailed description of the Ford Computer Graphics System and the commercial systems. A few applications involving the various graphics systems are discussed. These include design activity at several design and manufacturing areas which are major users of graphics equipment.</p> <p>Next, four computer graphics-related projects presently receiving considerable attention are described. They include communications between graphics systems, a graphics information system and the use of graphics in both structural analysis and numerical control.</p> <p>Finally, some indication of where computer graphics is headed in the future is presented.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Automative design]]></kw>
			<kw><![CDATA[CAD/CAM]]></kw>
			<kw><![CDATA[Computer graphics]]></kw>
			<kw><![CDATA[Mechanical design]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.8</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.4</cat_node>
				<descriptor>Application packages</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.6</cat_node>
				<descriptor>Interaction techniques</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>J.7</cat_node>
				<descriptor>Industrial control</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010481.10010482.10010486</concept_id>
				<concept_desc>CCS->Applied computing->Operations research->Industry and manufacturing->Command and control</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10010391</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Graphics input devices</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010507</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Graphics recognition and interpretation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P85718</person_id>
				<author_profile_id><![CDATA[81100405005]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Frank]]></first_name>
				<middle_name><![CDATA[W.]]></middle_name>
				<last_name><![CDATA[Bliss]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Ford Motor Company, Dearborn, Michigan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 INTERACTIVE COMPUTER GRAPHICS AT FORD MOTOR COMPANY Frank W. Bliss FORD MOTOR COMPANY Dearborn, Michigan 
 ABSTRACT Over the past fourteen years the Ford Motor Com- pany actively pursued the development and 
use of interactive computer graphics in the design and manufacture of automobiles. The success of Ford's 
pioneering efforts in the area of minicomputer graphics led to widespread acceptance and support for 
graphics within the corporation. This in turn led to a rapid expansion of computer graphics equipment. 
The advent of numerous Federal regu- lations in the areas of fuel economy, emissions control, and safety 
has generated additional Corporate interest in the use of computer graphics to help solve these difficult 
problems. Use of the Ford Computer Graphics System, as well as various commercially available turnkey 
computer graphics systems, has contributed substantially to cost reductions, improved productivity, and 
better documentation and control. This paper describes the on-going computer graphics effort at Ford 
Motor Company. A brief history of computer graphics is presented along with a detailed description of 
the Ford Computer Graphics System and the commercial systems. A few applications involving the various 
graphics systems are discussed. These include design activity at several design and manufacturing areas 
which are major users of graphics equipment. Next, four computer graphics-related projects presently 
receiving considerable attention are described. They include communications between graphics systems, 
a graphics information system and the use of graphics in both structural analysis and numerical control. 
 Finally, some indication of where computer graphics is headed in the future is presented. KEY WORDS 
AND PHRASES: computer graphics, mechanical design, CAD/CAM, automotive design, Permission to copy without 
fee all or part of this material is granted provided that the copies are not made or distributed for 
direct commercial advantage, the ACM copyright notice and the title of the publication and its date appear, 
and notice is given that copying is by permission of the Association for Computing Machinery. To copy 
otherwise, or to republish, requires a fee and/or specific permission. 01980 ACM 0-89791-021-4/80/0700-218 
$00.75 auto industry, structural analysis CR CATEGORIES: 8.2, 8.1, 1.3, 3.26, 3.24 I. INTRODUCTION 
 Interactive computer graphics started in the early 60's with the work of Dr. Ivan Sutherland at MIT. 
Project Sketchpad demonstrated the feasibility of computer-aided design using graphics. This activ- ity 
was quickly picked up by the research depart- ments at General Motors and Lockheed Georgia. These early 
systems involved refresh displays connected to a large main frame computer. The cost was very high. 
By the middle 60's, however, the availability of low cost minicomputers and advances in display technology 
dramatically changed the entire picture for computer graphics. It was in this environment in 1966 that 
Ford Motor Company began development of an interactive computer graphics system. The goal was to develop 
a tool to assist the automo- bile body designer in specifying surfaces. After evaluating the alternatives, 
it was decided that a minicomputer would keep cost low and allow graphic stations to be completely independent 
from one another and unconstrained as to physical location. It was furthe~ decided that a refresh display 
with a light pen and dynamic rotation capability would provide an effective man-machine interface. II. 
THE FORD COMPUTER GRAPHICS SYSTEM The Ford Computer Graphics System is a stand-alone minicomputer based 
design station. The hardware is purchased from commercial sources and assembled by Ford. Changes in hardware 
configuration are made as new technology advances become available. The current hardware configuration 
consists of the following components: I. CDC 18-IOM minicomputer with 32K of memory. Word length is 
16 bits. 2. Lundy HyperGraf vector refresh display with a light pen. 3-Function buttons. 4. CDC disk 
drive with a total capacity of  4.4 M words.  5. CDC magnetic tape transport.  6. Teletype.  A recent 
trend in computer peripherals is toward intelligent terminals. The Lundy display is an excellent example 
of this. The Lundy HyperGraf contains two 16 bit microprocessors, a 16K display buffer, and two 16K refresh 
buffers. The display buffer contains three-dimensional part data. The dual refresh buffers contain 2-D 
display data. The first microprocessor refreshes the display from one of the refresh buffers. At the 
same time, the second microprocessor can transform the 3-D data into 2-D display data and place it in 
the other refresh buffer. This double buffering con- figuration allows dynamic rotation of the part in 
real time with only minimal intervention by the host computer. The command language for the Ford System 
is menu oriented. The operator uses the light pen to select from a list of items on the display screen, 
the operation that he wishes to perform next. (e.g., FRONT VIEW, TOP VIEW, ROTATE, DELETE.) The software 
for the Ford Computer Graphics System was completely designed, developed and implemented by engineers 
at Ford Motor Company. It is written primarily in Fortran. The software consists of the operating system, 
a library of utility rou- tines, and the applications programs. The CDC operating system was extensively 
modified by Ford to provide a more efficient real-time interactive environment. Tie utility routines 
form the heart of the interactive graphics system. They provide the software interface between the application 
programs and the display, the database, the operating system, and user input via light pen, function 
buttons and keyboard. These routines are called by the application programs. The application programs 
are a set of special purpose modules that perform various design func- tions. They are activated by the 
designer via a menu pick with the light pen. Writing of appli- cation programs is simplified by the fact 
that the programmer does not have to be concerned about interfaces. III. APPLICATIONS There are a number 
of application packages cur- rently supported on the Ford system. These in- clude the Product Design 
Graphics System (PDGS), Electrical Circuitry Graphics System, Plant Layout, Finite Element Modelling, 
and Numerical Control. The Product Design Graphics System (PDGS) is a general purpose three-dimensional 
design and drafting system which is designer/draftsman ori- ented. Human factors considerations have 
always been of paramont importance in software design decisions. Terminology and commands familiar 
to the designer are used throughout. Designers were heavily involved in all phases of the software 
 specification. Consequently, acceptance of this new technology was exceptionally good. Applica- tion 
program refinements were tailored to the specific requirements of the Ford Motor Company. The system 
allows the user to operate at the con- cept level, concentrating his efforts on the design task, interacting 
with the computer in a natural manner. PDGS has the capability of defining and manipula- ting very 
complicated automobile body surfaces. Point lines, generalized point-defined curves, and surface patches 
are available as entities to create surfaces. In addition, there are many functions available to the 
designer. Sections can be cut through any given surface and displayed. By selecting the boundaries and 
number of flow-lines, the designer can generate a proportioned surface over any area. Auxiliary and true 
view projections can be established, and the part can be dynamically rotated in three-dimensional space. 
This capabil- ity gives the designer a better idea of what the part looks like and also allows him to 
check clear- ance and interference on moving parts. Further- more, a detailing package has recently been 
added to round out the capabilities of the system. The new functions include dimensioning, notes, toler- 
ancing, user specified character heights, standard symbols, and various line types. The use of PDGS 
has led to reduced costs due to productivity improvements. Within a given period of time, a designer 
can examine many alternatives, resulting in a more optimal design that weighs less, costs less, and 
is more reliable than a manual design. The accuracy of the final design is greatly improved; errors resulting 
from the manual handling of data are eliminated. A dynamic door swing simulation can indicate an interference 
 that could result in expensive design changes if not caught in time. Area and volume calculation capability 
can determine the amount of paint re- quired to cover an automobile. The common data base simplifies 
tool and die design; it also im- proves the communication between various engineer- ing and manufacturing 
offices. The Electrical Circuitry Graphics System (ECGS) provides the capability to generate wiring 
assembly drawings. The designer initially builds up a library of standard electrical components, such 
as terminals, connectors, lamps, and wire types. These components can have a part number and product 
information stored as well as a graphic description of the part. This additional information allows the 
generation of parts lists and design informa- tion. The designer builds a wiring assembly drawing at 
 the graphics display by selecting components with the light pen and positioning them in the desired 
 location. He then interconnects them with wires. Hardcopy output can either be drawn by a plotter 
or put directly on microfilm (COM). Many advantages accrue from using ECGS. The design can be completed 
in much less time, and the integ- rity of final drawings is much better because the program checks for 
component mismatch and various other conditions. The system also provides data management information 
such as the number of con- nectors needed and the number of feet of wire required. This information combined 
with antici- pated production rates allows Ford to prevent any potential part shortages. IV. GRAPHICS 
SYST~ USERS PDGS is used throughout all phases of design and manufacturing within the Company. The 
concept of 219 a mathematical model of an automobile residing in a distributive data base is the link 
that ties the various graphics systems together. The initial specification for a new automobile ori- 
ginates in the Design Center. The concepts, sketches, and renderings created by Styling are eventually 
translated into a full size clay model. Once the design is approved, the basic specifica- tions are determined 
by electronically scanning the clay model. The three-dimensional coordinate data is then transferred 
to the Ford Computer Graphics System in Body ~gineering. The surface data is displayed on the screen 
and reviewed for overall completeness and accuracy. Individual lines are sweetened and then meshed with 
other lines to create a mathemati- cally continuous surface. This is the beginning of a three-dimensional 
mathematical model. Once the surface definition is complete, the component design activity takes the 
surface data in graphic form and designs the major components of the auto- mobile body (quarter panels, 
doors, roof, front end, decklid, etc.). This involves designing inner panels and adding flanges, reinforcements, 
braces, ribs and weld requirements. The component designer checks interference between moving parts and 
verifies that all components fit together pro- perly. When this process is complete, the graphic detailing 
package is used to generate the final release drawings for Manufacturing. Meanwhile, Manufacturing has 
received preliminary computer graphics data from Body Eagineering to begin the tool and die design process. 
Metal Stamping Division (MSD) uses computer graphics to perform die design. The designer uses the light 
pen to create the die surfaces and addenda, rotates the die to determine the die tip position, and checks 
for interference. As more up-to-date in- formation becomes available, it is incorporated into the die 
design. At the end of the process, a numerical control tape is punched to drive a machine to cut the 
die. Automotive Assembly Division (AAD) performs tool design with the assistance of computer graphics. 
The types of tools designed are fixtures and gages. Fixtures are devices for holding a part while some 
operation, such as welding, is per- formed. Gages are used to check for proper dimen- sions on a part. 
The designer starts with a pic- ture of the part that the fixture is to hold. He calls on a library of 
standard fixture components, modifying them when necessary, to build the fix- ture. He verifies on the 
screen that the fixture holds the part properly. He checks clearances. By rotating the fixture and part 
in three space, he observes all aspects of their common interface. The motion of any moving part is checked 
for inter- ference. In a similar manner, the gage designer verifies that the gage matches exactly the 
part it is to measure. In addition to the design work just described, all of these activities use computer 
graphics to per- form other functions. The Design Center is doing packaging studies. Body ~hgineering, 
MSD, and AAD are all generating electrical schematics and wiring diagrams using the graphics system. 
Body is also designing die models, which are wood models of the components that the dies are to produce. 
 ~D has developed its own Plant Layout program for the Ford graphics system. It relies on a library of 
standard symbols which are placed on the layout and positioned by the designer. Different proper- ties 
are assigned to different layers. Pipes, electrical, and machinery are all on different layers. This 
allows them to be viewed separately or superimposed on one another. The program can also show the motion 
of a conveyer belt loaded with parts to check for interference problems. And finally, as the scale of 
the plant layout is changed, the level of detail shown also changes. This feature applies to hard copy 
as well as the display. Ford Truck and Recreation Products is using their graphics system for wiring 
assemblies, electrical schematics, tandem rear axle geometry studies, and tire envelope design. Computer 
graphics accelerates the entire design process, and has the added advantage of providing a common three-dimensional 
data base. This makes communication between the various engineering and manufacturing offices much easier, 
quicker, and more precise. V. COMMERCIAL COMPUTER GRAPHICS SYST~S AT FORD In the late 1960's, two events 
occurred which had a profound effect on interactive computer graphics. The first was the advent of the 
low-cost minicom- puter. The second event was the development of inexpensive storage tube displays pioneered 
by Tektronix. These two products led to the develop- ment of relatively low-cost turnkey interactive 
computer graphics systems. These systems were marketed commercially by a number of vendors. The difference 
between the Ford Graphics System and the vendor systems reflect the effects of emphasizing different 
tasks. Ford software and data base are oriented toward design of sculptured three-dimensional surfaces, 
while the vendor soft- ware is oriented toward mechanical design and detailing of parts with conventional 
geometric shapes. The hardware differs primarily in the display. The Ford system uses a vector refresh 
display and the vendor systems use storage tube displays. Each has its strong and weak points. The 
primary advantage of a storage tube is the capability of displaying unlimited data without flicker. 
In addition, it is less expensive. The refresh dis- play, on the other hand, can demonstrate dynamic 
 motion in real time. The designer can rotate a part in three-dimensional space to get a feel for what 
it looks like. ~ Dynamic motion is also very useful for performing vibration studies and dyna- mic interference 
checking. The refresh display has the further advantage of "selective erase". Both types of display 
have good resolution, but a refresh display has better brightness and the capability of multiple intensity 
levels. Whether the user selects a refresh display or a storage tube generally depends on the proposed 
applications. In late 1975, Powertrain Engineering made a deci- sion to purchase an interactive computer 
graphics system. Powertrain designs engines, axles, and transmissions. Many of their designs involve 
large numbers of "geometric shape" parts. A major re- quirement was to be able to also perform detailing 
functions. As a result, they chose a vendor- supplied storage tube system. A short time later, Chassis 
Engineering selected a different vendor- supplied system. These two systems are made by Computervision 
and Gerber, respectively. They are functionally similar, although each offers some unique features. 
Computervision (CV) produces their own minicomputer called the CGP (Computervision Graphics Processor). 
 It is similar in performance and instruction set to the Data General Eclipse. The CGP drives a mag- 
netic tape, a storage module, and multiple design stations. Each design station is composed of a 19-inch 
Tektronix storage display, a small digiti- zer tablet with a pen, and an alphanumeric keyboard and display. 
Optional plotters and other hard copy devices are available. The system is task oriented; and each graphics 
station is considered a task. The CGP contains 8K of memory for the operating system and an additional 
24K for each task. Each task is totally independent of the other, however, there is no background operation 
 capability. Ford uses the mechanical design software; this is a modification of the Hanratty package 
that has been incorporated into the CV design philosophy. The man/machine interface of the CV system 
is a highly-structured command language which is very powerful and quite flexible. The language is key- 
board oriented, but a subset of it can be placed on a menu on the digitizer. The designer picks commands 
from the menu (or enters them via the keyboard) and locates graphic data on the display using the pen 
and tablet. The CV system is a little more difficult to learn than some graphics systems because of 
the complexity of the command language; however, once mastered, it is fast and efficient because of 
the power of the command language. A APT-like programming language is available with the graphics system, 
which is designer-oriented and is used to develop special- ized design applications. The Gerber Interactive 
Design System (IDS) uses a HP 21MXE minicomputer to drive a magnetic tape, a cartridge disk drive, and 
multiple design sta- tions. Each design station consists of a 19-inch Tektronix display with a graphics 
positioning lever, a one line alphanumeric display, a keyboard, and a set of function buttons. In addition, 
each design station contains an HP 21MXEminicomputer. Optional plotters and hard copy devices are avail- 
able. The IDS mechanical design software was also derived from the Hanratty package. The Gerber man/machine 
interface philosophy involves leading the operator through the design process by asking a series of questions. 
The designer indicates his response via the keyboard or function buttons. The positioning lever is used 
to enter or refer to graphics on the display. This system is simple to learn; and in a matter of hours, 
a designer can be doing productive work with the Gerber system. IDS is also task oriented; each task 
requires 32K of memory in the main computer, and each satellite computer con- tains a minimum of 8K of 
memory. IDS is a distributed processing system; the mini- computer at each design station controls the 
graphics display, thus relieving the host CPU of a very time-consuming function. The distributed processing 
concept has many advantages for computer graphics, from improving real-time response to simplifying 
the programming. Furthermore, there is always the potential for improving the system in the future with 
the implementation of additional memory and capability in the satellite computer. VI. USERS OF VENDOR 
SUPPLIED GRAPHICS SYSTEMS Engine, Axle, Transmission and Chassis Engineering are all using vendor supplied 
graphics systems. These systems were purchased from Computervision and Gerber to perform various design 
and detailing functions required by Powertrain and Chassis Engineering. All four of these groups must 
interface closely with one another. Each transfers part data to and from the others. For example, Chassis 
Engineering needs the outline of the transmission and drive- shaft to complete the chassis design. Engine 
 Engineering needs to know how much space is avail- able for the engine and the location of the engine 
mounting brackets on the chassis. Axle Engineer- ing must know where the axle fastens to the chassis. 
 Axle Engineering uses graphics for production axle and driveshaft detail drawings and program change 
 request drawings. Engine Engineering applications include packaging investigations within the engine 
compartment to study the effect of various component configura- tions. Special programs have been written 
to cal- culate the lengths of hoses, tubes, and belts routed throughout the engine compartment. Cataly- 
 tic converters and vacuum system schematics for engine emission control are designed on the graphics 
system. In addition, various studies are performed on the system including internal compo- nent valvetrain 
investigations and calculation of piston surface areas and combustion chamber volumes. Chassis Engineering 
is also working with catalytic converters. They exchange 3-D part data with Engine. Other graphics 
applications include rear suspension layouts, fuel tank volume calculations and finite element modeling. 
 Transmission Engineering is using graphics to perform engineering studies such as tolerance stacking 
and clearance investigations. Layouts, installation drawings, and cross-sectional draw- ings are done 
on the system along with gear tooth profiles. VII. PRINTED CIRCUIT BOARD DESIGN SYSTEM In response 
to both Federal regulations and con- sumer buying habits, the amount of electronics in 221 the automobile 
has increased dramatically in the past few years. Sophisticated electronic devices that include microprocessors 
have been developed to control engine emissions and fuel economy. As a result, the design and manufacture 
of printed circuit (PC) boards has become a major labor inten- sive effort at the Electrical and Electronics 
Division (EED) of Ford. The process of generating a printed circuit board from a logic diagram requires 
creation of a line study. The line study indicates where each compo- nent is placed on the PC board and 
contains the routing of connections between the components. The line study must be checked to verify 
that it exactly represents the logic diagram. It is then photographed to create the art work master which 
is used to etch the PC board. This entire process is very tedious and time consuming. Automation of 
these processes at EED has been ac- complished using a SCI CARDS System. This is a two station interactive 
graphics system that executes a printed circuit board design program. The hard- ware consists of s Prime 
400 minicomputer with 256K words of memory, a 80 Mbyte disk drive and a Vector General refresh display 
at each station. A photo plotter and pen plotter are the primary out- put devices. The program performs 
automated com- ponent placement, routing, and checking. The advantages of this system over most systems 
that perform these functions is that it is interactive. Algorithms that perform placement and routing 
have great difficulty completely routing a dense PC board. This results in manual completion of the board, 
which in many cases is very difficult if not impossible without using jumpers and feedthrus. The interactive 
system allows the combination of the best features of human and mechanical intel- ligence to be brought 
to bear on the design. The designer interacts with the system via the light pen and display. He can enter 
and exit the system at will without being required to restart the sys- tem from the beginning. This means 
that critical routing can be done in advance and then automatic routing can proceed. This system produces 
artwork masters, silk screens, and solder masks on the photo plotter. Circuit punch drawings and circuit 
assembly drawings are generated on the pen plotter. This system is being used for the majority of new 
printed circuits required for 1980 electronic modules, including very dense and complex circuits for 
electronic instrumentation and electronic engine control systems. Major benefits derived from this system 
include improvement in PC board design turnaround time, rapid response to design revisions, stable photo- 
plotted artwork masters, and computer generation of documentation. VIII. SPECIAL APPLICATIONS The next 
four sections outline graphics derivative projects in various stages of development and use. A. Data 
Communication As the number of graphics design systems increase within the Company (currently over 
100 Ford Computer Graphics Systems), the need for data com- munication becomes more important. The graphics 
data base becomes the design standard; the computer serves as the principal communication link between 
design groups, between design and manufacturing, and eventually between Ford and its suppliers. The 
first step is direct communication between similar systems via direct link or phone hook-up. This allows 
Body Engineering, for example, to transfer design data between individual design stations or to the manufacturing 
operation. The next step is telecommunication between the graphics systems and large host computers. 
This allows the graphics systems access to large analysis programs such as NASTRAN. There are a few standard 
commun- ication packages available: HASP, 2780, and UT200; all emulate a computer terminal and are supported 
on many graphics systems. The final step is communications via a distributed network which encompasses 
all graphics design stations, large host computers for engineering computations and other computerized 
control systems. A recently completed transatlantic data link connects Ford engineering centers in 
England and Germany with Ford's computing center in Dearborn. Ford European operations are using computer 
 graphics equipment to help design the new world cars. They are now able to exchange graphic data with 
U.S. operations who are also working on the design of the world cars. B. Graphics Data Base Management 
System The rapid expansion of computer graphics work stations and the attendent need to control the 
data generated has led to the conclusion that the central storage of design data is required. Each Ford 
computer graphics design station is an in- dependent, stand-alone computer system. Since thousands of 
drawings are made each year, a very extensive magnetic tape library results. Control of all these tapes 
poses a potentially serious problem. Transferring data between various en- gineering activities and information 
retrieval by management are additional problems. In response to these problems, Ford is developing 
a Graphics Information System. This system is a computer network that functions as a graphics data 
collector and a Management Information System. Every graphics design station in a particular area (e.g., 
Electrical Design) will be connected to a local data collector computer system. The data collectors 
for the various product engineering offices and manufacturing operations will in turn be part of a 
distributive computer network. The data collector is presently composed of a minicomputer and multiple 
disk drives. The stored data contains three years of design information. Every graphics station in that 
design area is connected via a high-speed communication link to the central computer. Currently, the 
primary func- tion is the storage and retrieval of graphical data files. However, it also has the capability 
to write part data on magnetic tape and to write plot and COM (microfilm) tapes. This facility eliminates 
the necessity of having a magnetic tape at each graphics design station. The common data base approach 
enables any user to readily access the latest version of any drawing. The next phase of this project 
involves connecting all the data collectors to form a computer network. This network will facilitate 
the transfer of re- leased drawings between the various product engi- neering offices; engineering and 
manufacturing; manufacturing and vendors. In addition, communi- cations to large host computers will 
be provided to support engineering analyses on graphics generated data. A very important part of this 
system is real-time management reporting. Since the information is already in the data base, it is only 
necessary to provide a means of easy access and the ability to format the data in various ways. The types 
of in- formation that a decision maker might need are: drawing status of specific parts, usage of a part, 
production quantity projections, and those parts affected by a change. This information inquiry and retrieval 
system will have a wide variety of op- tions that involve interpretation of data as well as retrieval. 
 C. Structural Analysis One of the fastest growing applications of computer graphics at Ford is in the 
structural analysis area. Recent Government mandated performance re- quirements have caused a major redirection 
in the design of automobiles. The necessity of meeting the Corporate Average Fuel Economy (CAFE) has 
led to a massive redesign of vehicle components and systems to reduce weight and material cost while 
maintaining structural integrity and vibration standards. The traditional methods of structural analysis 
are inadequate to handle these complex problems. In order to reduce costly design changes due to structural 
deficiencies, a faster, more ex- tensive and accurate method to determine structural integrity early 
in the design process was required. Finite Element Analysis (FEA) is the tool used to solve these problems. 
It is a technique wherein the distributed physical properties of a structure are modeled by a finite 
number of structural ele- ments which are interconnected at a number of grid points called nodes. Loads 
are applied to the grid points and displacements and stresses are then cal- culated. NASTRAN (NASA Structural 
Analysis Pro- gram) is one of the many finite element analysis software packages available. It provides 
accurate and reliable solutions to complex structural prob- lems, but requires a large computer to support 
it. The first step in using this system analysis tool is the creation of the geometry for a Finite 
 Element Model (FILl). In the past, creation of a finite element model was a tedious process taking 
 about 70 percent of the time required for the en- tire job. In many cases a prototype was built and 
 tested before the results of the finite element analysis were completed. Finite Element Modelling 
turned out to be a very natural extension of the capabilities of a computer graphics design system. The 
Finite Element Analysis ~st_em (FAST) interactive graphics program p~ovides Ford structural analysis 
engineers with a fast and efficient method of creating finite element models. Utilization of existing 
part geometry simplifies the process even more. The next step in the FEA process is mesh specifica- 
tion. FAST is capable of generating both quadran- gular and triangular mesh elements as well as iso- 
parametric and solid elements. The program con- tains automatic mesh generating capability and ex- tensive 
mesh editing and validating capability. The dynamic rotation feature is very useful for validating mesh 
data. After the physical characteristics (i.e., loads, constraints and properties) are specified, the 
data is formatted for input to the NASTRAN program and sent via phone lines to a large computer. The 
out- put of the NASTRAN analysis is reformatted for the graphics system. A modal analysis display allows 
visual evaluation of vibration data. The display demonstrates the dynamic deflection of each node in 
the part. A stress display shows the stress at the centroid of each mesh element. Utilization of a color 
display to represent various stress concen- trations with different colors is a more effective way of 
viewing the stress data. A Comtal color display is used for this purpose. In the future, many enhancements 
will be added to FAST including improved automatic mesh generating capability. The combination of 
computer graphics and finite element analysis provides a powerful tool with which the engineer can 
design components for lower cost, lower weight, better performance, and greater reliability. D. Numerical 
Control The use of computer graphics to generate numerical control (N/C) tapes to drive N/C machines 
has traditionally had a large economic payback. The N/C area was the first major success for CAD/CAM. 
Geometry created by designers was used by manufac- turing to punch an N/C tape to cut a part. This technique 
has been especially useful in the aero- space industry, where much of the early work involving N/C and 
graphics occurred. In the auto- mobile industry, N/C is used extensively in cutting dies and die models. 
 The process of generating a N/C tape involves three stages. First, the part or surface must be described; 
then the path of the cutter must be traced. Finally, the cutter path information must be converted into 
machine-control commands. Describing the part is the most difficult task. If the part was designed using 
a computer graphics system, then the part description already resides in the data base. The N/C parts 
programmer calls up the part on his graphics console; and by making use of graphics oriented N/C programs, 
completes the job. IX. THE FUTURE OF COMPUTER GRAPHICS AT FORD In order to control the large amount 
of graphic data, it will be necessary to integrate the graphic systems into a distributed processing, 
distributed 223 data base network. The graphic station will become a highly intelligent terminal connected 
to a local data base computer system which in turn will be connected to other local data base systems 
and large mainframe computers to form a hierarchical network. This network will also act as a Manage- 
ment Information System providing decision makers with up to the minute data on the status of design 
and manufacturing operations. Communication between different graphics systems requires a CorPorate 
standard data base. Efforts are presently underway on this task. Low cost raster displays are presently 
having a profound effect on the display industry as well as the computer graphics industry. Their importance 
in the future will continue to increase. The major drawback to raster technology is poor resolu- tion. 
Ford is using color raster displays for Finite Element Analysis output and electrical schematics. Computer 
graphics has come of age at Ford Motor Company. Graphics is expanding into every basic design office 
and manufacturing division of the company. Present graphics facilities are expand- ing dramatically and 
new applications are constant- ly being discovered. It is believed that this trend will continue well 
into the eighties and that computer graphics will play an ever increas- ing role in the design and manufacture 
of auto- mobiles. 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1980</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>807496</article_id>
		<sort_key>225</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-1980</article_publication_date>
		<seq_no>37</seq_no>
		<title><![CDATA[The user's view of CAD/CAM (Panel Session)]]></title>
		<page_from>225</page_from>
		<doi_number>10.1145/800250.807496</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=807496</url>
		<abstract>
			<par><![CDATA[<p>&#8220;The User's View of CAD/CAM&#8221; will share the experience gained in over 50 man-years of day-to-day use of CAD/CAM systems. Six industry segments are represented in this presentation. In addition, a wide range of systems costing under &dollar;50,000 to over &dollar;1,000,000 will be discussed.</p> <p>There is good news and bad news. All panelists admit to improved productivity and/or reduced design cycle time, but all is not free. You will hear about what it takes to operate and support CAD/CAM systems. Subjects include facilities planning, cost effectiveness, training, internal support requirements, and more. Here is your chance to hear practical and common sense comments about what CAD/CAM can and cannot do for you.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>J.6</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>H.1.2</cat_node>
				<descriptor>Human factors</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003120.10003121</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010432.10010439</concept_id>
				<concept_desc>CCS->Applied computing->Physical sciences and engineering->Engineering</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Human Factors</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP14045324</person_id>
				<author_profile_id><![CDATA[81437592266]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Ken]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Anderson]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Anderson Publications]]></affiliation>
				<role><![CDATA[Chairman]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P332555</person_id>
				<author_profile_id><![CDATA[81332531644]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Myrl]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Thompson]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[General Motors]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P329598</person_id>
				<author_profile_id><![CDATA[81547801556]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[D.]]></first_name>
				<middle_name><![CDATA[H. W.]]></middle_name>
				<last_name><![CDATA[Harris]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Boeing]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14185396</person_id>
				<author_profile_id><![CDATA[81332535439]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[R.]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wilson]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Intel]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P329108</person_id>
				<author_profile_id><![CDATA[81100490624]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[B.]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Crowley]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Structural Dynamics Research Corporation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14049462</person_id>
				<author_profile_id><![CDATA[81332491026]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[C.]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Boyd]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P332256</person_id>
				<author_profile_id><![CDATA[81100251533]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>7</seq_no>
				<first_name><![CDATA[M.]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wozny]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Rensselaer Polytechnic Institute]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 THE USER'S VIEW OF CAD/CAM Panel Introduction "The User's View of CAD/CAM" will share the experience 
gained in over 50 man-years of day-to-day use of CAD/CAM systems. Six industry segments are represented 
in this presentation. In addition, a wide range of systems costing under $50,000 to over $I,000,000 will 
be discussed. There is good news and bad news. All panelists admit to improved productivity and/or reduced 
design cycle time, but all is not free. You will hear about what it takes to operate and support CAD/CAM 
systems. Subjects include facilities planning, cost effectiveness, training, internal support requirements, 
and more. Here is your chance to hear practical and common sense comments about what CAD/CAM can and 
cannot do for you. Chairman: Ken Anderson, Anderson Publications Panelists: Myrl Thompson, General Motors 
D. H. W. Harris, Boeing R. Wilson, Intel B. Crowley, Structural Dynamics Research Corporation C. Boyd; 
Boyd, Broach, and Foster M. Wozny, Rensselaer Polytechnic Institute Permission to copy without fee all 
or part of this material is granted provided that the copies are not made or distributed for direct commercial 
advantage, the ACM copyright notice and the title of the publication and its date appear, and notice 
is given that copying is by permission of the Association for Computing Machinery. To copy otherwise, 
Or to republish, requires a fee and/or specific permission. 01980 ACM 0-89791-021-4/80/0700-225 $00.75 
225
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1980</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>807497</article_id>
		<sort_key>226</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-1980</article_publication_date>
		<seq_no>38</seq_no>
		<title><![CDATA[The workstation concept of GKS and the resulting conceptual differences to the GSPC core system]]></title>
		<page_from>226</page_from>
		<page_to>230</page_to>
		<doi_number>10.1145/800250.807497</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=807497</url>
		<abstract>
			<par><![CDATA[<p>GKS has evolved from a long process of national /1,2,3,4/ and international /5,6/ discussion on standardization of graphical systems. One of the basic principles of the Graphical Kernel System GKS is the concept of workstations that are used to address a display terminal with several input devices. This is a basic conceptual difference to the GSPC core system /6/, which will be discussed in this paper.</p> <p>A workstation represents a collection of graphical devices that are operated in a coordinated fashion by an operator at a given site. The whole workstation is treated in GKS as one logical unit.</p> <p>GKS features two dimensional output to and input from single or multiple workstations. Besides basic line drawing primitives raster graphics primitives are supported. The coordinates are transformed in a two-stage transformation process where the first stage can be set for each primitive and the second can be set for each workstation. Furthermore the setting of a workstation specific pen and text table allows control of the appearance of all primitives on the corresponding workstation.</p> <p>A segment facility provides means for structuring a picture in subparts. Segments may be created and deleted, the segment attributes may be dynamically modified, and the segments may be transformed. They can be displayed simultaneously or alternatively on different workstations. A device independent segment storage serves for inserting segments into other segments.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Decentralization]]></kw>
			<kw><![CDATA[Graphical kernel system]]></kw>
			<kw><![CDATA[Graphical system architecture]]></kw>
			<kw><![CDATA[Graphical terminal]]></kw>
			<kw><![CDATA[Intelligent terminal]]></kw>
			<kw><![CDATA[Protocol]]></kw>
			<kw><![CDATA[Workstation]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.6</cat_node>
				<descriptor>Standards</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>C.5.3</cat_node>
				<descriptor>Workstations</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010476.10011187</concept_id>
				<concept_desc>CCS->Applied computing->Computers in other domains->Personal computers and PC applications</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010919.10010177</concept_id>
				<concept_desc>CCS->Computing methodologies->Distributed computing methodologies->Distributed programming languages</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10011007.10011006.10011050.10011017</concept_id>
				<concept_desc>CCS->Software and its engineering->Software notations and tools->Context specific languages->Domain specific languages</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Standardization</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP31075266</person_id>
				<author_profile_id><![CDATA[81100063773]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[J.]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Encarnacao]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39071559</person_id>
				<author_profile_id><![CDATA[81100109845]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[G.]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Enderle]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39078597</person_id>
				<author_profile_id><![CDATA[81100508869]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[K.]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kansy]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P330376</person_id>
				<author_profile_id><![CDATA[81100607531]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[G.]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nees]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39082894</person_id>
				<author_profile_id><![CDATA[81100435852]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[E.]]></first_name>
				<middle_name><![CDATA[G.]]></middle_name>
				<last_name><![CDATA[Schlechtendahl]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14174632</person_id>
				<author_profile_id><![CDATA[81100499957]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[J.]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Weiss]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P332765</person_id>
				<author_profile_id><![CDATA[81100152998]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>7</seq_no>
				<first_name><![CDATA[P.]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wi&#223;kirchen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P334251</person_id>
				<author_profile_id><![CDATA[81100328186]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>8</seq_no>
				<first_name><![CDATA[W.]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Germany]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[J. Encarnacao, B. Fink, E. H&#246;rbst, R. Konkart, G. Nees, D.L. Parnas, E.G. Schlechtendahl A recommendation on Methodology in Computer Graphics; A position paper for the IFIP-Graphics Workshop, Seillac, France, 1976 (Seillac I), published as report KfK 2394, 1977, Karlsruhe]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>734701</ref_obj_id>
				<ref_obj_pid>647753</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[R. Eckert, F.J. Prester, E.G. Schlechtendahl and P. Wi{?}kirchen; Functional description of the Graphical Core System GKS as a step towards standardization; Informatik-Fachberichte 11, Springer 1977, pp. 163]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[R. Eckert, G. Enderle, K. Kansy and F.J. Prester; GKS'79 - Proposal of a Standard for a Graphical Kernel System; Proceedings of Eurographics'79, Bologna, October 1979, pp. 2]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Proposal of Standard DIN 0066252; Information Processing - Graphical Kernel System (GKS); Functional description, NI-5.9/26-79, Version 5.2; 5.11.1979]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[R.A. Guedj and H.A. Tucker (editors) Methodology in Computer Graphics (Seillac I) North-Holland, 1979]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[J. Weiss; Device driver interfaces for decentral device drivers; Proceedings of Eurographics'79, Bologna, October 1979, pp. 252]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_obj_id>988498</ref_obj_id>
				<ref_obj_pid>988497</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA["Status Report of the Graphic Standards Planning Committee"; Computer Graphics, Vol. 13, No. 3, August 1979]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 The workstation concept of GKS and the resulting conceptual differences to the GSPC core system J. 
Encarnacao, G. Enderle, K. Kansy, G. Nees, E.G. Schlechtendahl, J. Weiss and P. WiBkirchen W. Germany 
Abstract GKS has evolved from a long process of national /1,2,3,4/ and international /5,6/ discussion 
on standardization of graphical systems. One of the basic principles of the Graphical Kernel System GKS 
is the concept of workstations that are used to address a display terminal with several input devices. 
This is a basic conceptual difference to the GSPC core system /6/, which will be dis- cussed in this 
paper. A workstation represents a collection of graphi-cal devices that are operated in a coordinated 
fashion by an operator at a given site. The whole workstation is treated in GKS as one logical unit. 
GKS features two dimensional output to and input from single or multiple workstations. Besides basic 
line drawing primitives raster graphics pri- mitives are supported. The coordinates are trans-formed 
in a two-stage transformation process where the first stage can be set for each primi- tive and the second 
can be set for each worksta- tion. Furthermore the setting of a workstation specific pen and text table 
allows control of the appearance of all primitives on the corres-ponding workstation. A segment facility 
provides means for struc-turing a picture in subparts. Segments may be created and deleted, the segment 
attributes may be dynamically modified, and the segments may be transformed. They can be displayed si- 
multaneously or alternatively on different workstations. A device independent segment sto- rage serves 
for inserting segments into other segments. Permission to copy without fee all or part of this material 
is granted provided that the copies are not made or distributed for direct commercial advantage, the 
ACM copyright notice and the title of the publication and its date appear, and notice is given chat copying 
is by permission of the Association for Computing Machinery. To copy otherwise, or to republish, requires 
a fee and/or specific permission. 01980 ACM 0-89791-021-4/80/0700-226 $00.75 Content indicators (i) 
Key words: Graphical kernel system; Graphical system architecture; Graphical terminal; Worksta-tion; 
Intelligent terminal; Decentralization; Protocol (ii) Category numbers (CR): 4.1; 4.4; 8.2  Authors' 
affiliation: J. Encarnacao (TH Darmstadt); E.G. Schlechtendahl (KfK, Karlsruhe); K. Kansy and P. Wi~kirchen 
(GMD, St.Augustin); G. Nees (Siemens AG, UbE, Erlangen)  Mailing address: J. Encarnacao, TH Darmstadt 
FG Graphisch-lnteraktive Systeme Steubenplatz 12-111; D-6100 Darmstadt; FRG 1. The Workstation Concept 
GKS is based upon a concept of an abstract graphi-cal workstation with capabilities as described be- 
low. For every kind of workstation present in a given GKS implementation (e.g. refresh display, storage 
tube, plotter), an entry exists in a work- station description table. It describes the capa- bilities 
and characteristics of the workstation. GKS clearly distinguishes an abstract workstation from its physical 
realization, which we call a graphical terminal. The fully equipped (abstract) workstation has one addressable 
display surface permits the use of smaller display spaces than the maximum while guaranteeing that no 
display is generated outside the specified display space. An installation dependent ("paper" or "background") 
quality may be specified supports several linestyles, colours, text fonts, character sizes, etc. has 
one or more input devices for each class of input primitive permits request type, event type and sample 
input 226 In actual installations the graphical terminal may or may not be equipped with all of these 
capabili- ties. E.g., the input capabilities of a graphical terminal may range from no input to several 
input devices of every class. The application program may inquire via GKS which capabilities are available 
and adapt its behaviour accordingly. If capabili-ties are requested that a particular graphical ter-minal 
does not proviJe for, a standard error reac-tion is defined. Graphical terminals may provide more capabilities 
than those listed. These cannot be utilized by GKS. However, if the graphical terminal itself provides 
sufficient intelligence, the additional capabili- ties may well be utilized locally by the graphical 
terminal operator. As an example, if the graphical terminal as a storage tube and a plotter has out- 
put devices, the operator will likely use the storage tube for graphic modelling. When he is satisfied 
with the result he may switch to the plotter locally and request the picture to be re- drawn. Graphical 
terminals with more than one dis- play surfaces would have to be controlled by GKS only by defining a 
separate workstation for every display surface. The workstations are identified by the application program 
by use of a workstation identifier. When-ever the application program wants to use a work- station, it 
must first request the opening of this workstation by GKS. The opening actTon associates the workstation 
to the corresponding graphical terminal and gives the application access to all its capabilities except 
output of graphic primi- tives. For output the workstation must by explicit- ly activated. Output primitives 
are sent to all active workstations. Segment manipulation and input can be performed with any open workstation. 
Input devices at a particular workstation are identified by the triple: workstation identifier, input 
class, input device number. The latter selects one of the input devices of one class at a given workstation. 
E.g., a workstation may have a lightpen and a tablet for locator class input. For EVENT and SAMPLE type 
input the input device at a given work- station must be enabled. The display at a workstation ideally 
should al- ways reflect the actual state of the picture as defined by the application program. However, 
to use the capabilities of a workstation efficient- ly, it may be desirable to allow a workstation to 
defer the actions requested by the application program for some time. During this period the state of 
the display may be undefined. E.g., data sent to a plotter may be blocked to optimize data transfer. 
All functions (such as DELETE SEGMENT) which trigger an implicit regeneration of the whole picture on 
a workstation may be sup- pressed until a regeneration is required explicit- ly. To what degree a workstation 
suppresses the implicit picture regeneration is controlled by its deferral state.. The function SET DEFERRAL 
STATE allows the choice of that degree of deferring which takes into account the capabilities of the 
work- station and the requirement of the application program. The following deferral states can be specified: 
State 1: The visual effect of each function has to become visible as fast as possible. State 2: The visual 
effect of each function has to become visible before the next inter-action (i.e. no deferring if an input 
device is enabled; update before 'ENABLE input primitive' or 'REQUEST input primi- tive' is executed). 
State 3: The visual effect of all functions may be deferred. State 4: As state i , however, implicit 
regenera-tion is suppressed. State 5: As state 2, however, implicit regenera-tion is suppressed. State 
6: As state 3, however, implicit regenera-tion is suppressed. There is no state provided where the addition 
of graphical data must be deferred. Such a state would correspond to a complete buffering of all newly 
created graphic information. The application program should handle such situations using the segment 
storage facility and the visibility attri-bute. Deferred actions can be made visible at any time by appropriate 
resetting of the deferral state or by using the UPDATE-function. A workstation transformation is used 
to map the normalized device coordinate system onto the de- vice coordinate system that describes the 
display space of a workstation. The device coordinates are measured in meter. This workstation transformation 
can be set individually for every workstation, allowing the same graphical primitive on different display 
surfaces in different, application program controlled, scales. In this way it is possible, e.g. to use 
the full display space of an interac- tive workstation and to have simultaneously a drawing in correct 
scale on a plotter. The work- station transformation is set by specifying a workstation window in NDC 
and the workstation viewport size in DC space (see fig. 1). ~orkGtoT Lo~ Tr~ns i~ormo T ion / '[/~,/~ 
2 workGTTOT 1on worksT ~TT ,I on Independent picture dependent picture Fig. 1: Workstation transformation 
227 2. GKS workstation concept in comparison with the GSPC approach Graphical data will usually be displayed 
on diffe- rent viewing surfaces either sequentially or simul-taneously e.g. on an interactive display 
for edi-ting and finally on a plotter as a hardcopy. Both, the GKS and the GSPC core system support this 
possir bility. The appearance of graphic output primitives will vary signifidantly as graphical output 
devices offer different characteristics. In the GSPC core system the appearance of a primitive is associated 
with the primitive itself. In GKS the appearance of a primitive is defined by two stages (in a simi- 
lar way as the transformation). In the first stage a symbolic attribute is associated with the primi- 
tive, while in the second stage the symbolic attri- bute is mapped on the capabilities of the worksta- 
tion, thus determining the usual appearance on the graphical terminal. With GKS both stages are under 
the full control of the application program. Using this mechanism the application program can e.g. draw 
the same line primitive red on a plotter and dashed on a storage tube, without regenerating the line 
primitive. This means that in GKS conceptually in a first step a "workstation independent picture" is 
drawn on an "abstract viewing surface". In a second step for each active workstation this data is combined 
with workstation dependent data and sent to the physi- cal viewing surface associated with each of these 
workstations (see fig. 2). Appl ical/Ion Progrom I OKS I work=to:ion Independentpicture -- Workslotlon 
VorksTQt Ion drIvGr driv~r J workstation dependent p~c~ture CPophlc~l Fig. 2: Routing of a picture from 
GKS to workstation The set of workstation dependent features which can be set under full control of the 
application program separately for each workstation contains (see fig.3): - mapping from normalized device 
coordinates to device coordinates - setting of pen representation, i.e. correlating a pen number with 
a predefined (physical) pen or with linestyle, linewidth, colour - setting of text representation, i.e. 
correlating a text number with a predefined (hardware) character generator or with a text font, text 
quality, character size, and character spacing - choice of display space quality, i.e. paper quality 
for a plotter or a background colour for a raster display Vcr~clat ~'on ~y ,nc!ow/ ' I ~,'or~aTel !on 
v,~spor I ~1~p1~y ~p~ce qt:~l,ty (dev~c~ coord. , dav,cs di~p&#38;ndent ~-+ on ~,,._ ~, pun ¢,~d toxf 
~. I r:premontat=on~) 0 Sc~mont 1 I stor,~Ne I WOrk~TotlOR I ~ ' L Fig. 3: Workstation dependent features 
Full flexibility in setting the above workstation attributes is allowed only in GKS level 4, inclu- ding 
the dynamic setting of colour tables, the dy- namic setting of the mapping to device coordinates for 
scaling of a drawing. Thus GKS can support the advanced features of some high performance graphic devices 
that are available. For lower GKS levels the workstation attributes may be set only if no graphical data 
is present on the workstation, i.e. immediately after opening or clearing a workstation. In GKS level 
0 no setting of workstation attributes is possible. In both proposals a facility to control the deferral 
of picture changes is provided. Whereas the GSPC regards this control as a general feature, GKS has included 
this control into the workstation's faci- lities. The deferral state of GKS is a means to con- trol the 
behaviour of a specific workstation rather than the behaviour of the graphic system as a whole. Different 
workstations may contain different sets of segments depending on the activation of the workstations. 
GKS provide via the INSERT-function for a first step in the direction of a segment li- brary. Such a 
possibility is not included in the GSPC core system. Because transfers of segments from one workstation 
to another could be expensive, this possibility is provided only for segments which have been selected 
for transfer during generation via a segment storage workstation mechanism. This mechanism works similar 
to the static segment attribute IMAGE TRANSFORMATION TYPE of the GSPC core system. 228 3. The decentralization 
of the workstation As the workstation is under full control of GKS in a device independent fashion, the 
idea of decentra-lizing is almost obvious. Because of the rapid de- velopment in the field of microprocessing, 
decen-tralizing is also economical. The concept of a work- station description table offers the opportunity 
to deal with the resource problem in a similar way as it now provides a solution to the different func-tional 
capabilities of workstations. When the device drivers are decentralized, all in-formation which influences 
the device must be sent to the remote device driver in the form of a proto- col (see fig. 4). Appllomllon 
Progrom I GKS Work6 It o ~ ! o~r 1 V~ r Hoe I i HQBI porl ~f "-°i I .... ooo o., Wor~sI~flon I driver 
Proloooldr Ivor ] J Communl oll t Ion I Lno Protoco] [ dr~ver Remote L Iormln~l I workGtaT ion Cr~phlc~i 
lermlnQl Fig. 4: Protocol embedded in a workstation driver Within GKS the host part of the workstation 
driver and a protocol driver (for both input and output) would take the place of the actual device driver. 
It would of course be advantageous if a standard could be achieved with respect to this protocol. Possibly 
the Metafile protocol could be used as a basis. Protocol formats for management functions (e.g., ACTIVATE 
WORKSTATION, DEACTIVATE WORKSTA- TION) and input functions (REQUEST INPUT, AWAIT WORKSTATION EVENT and 
others would have to be ad- ded. The remote part of the device driver will be connected to the graphical 
terminal /7/. Which driver functions are realized in each part of the driver depends on the computational 
capabilities of the graphical terminal. Another important issue of decentralization is the location of 
the workstation state list. The work- station state list may either be associated with the remote device 
driver (it probably will be so, if the remote device has sufficient intelligence), or it may be associated 
with GKS, or a mixed solu- tion is conceivable. Which of these architectures is most suitable will depend 
not only on functio- nal requirements but to a great deal on the res-ponse time requirements. 4. Conclusions 
The workstation concept of GKS (based upon work- station description table, workstation state list and 
a set of management and inquiry functions) not only provides means for the optimal adaptation of application 
programs to hardware capabilities of graphical terminals it also lends itself to an ade- quate treatment 
of remote graphic operations. Instead of supporting the single graphical i/o de-vices (e.g. plotter, 
displays, light pen) as logi-cal i/o devices, GKS treats collections of such devices as a functional 
unit, the workstation. This concept suits better the present trend in the development of intelligent 
graphical systems: an operator handles a combination of graphical i/o devices as one operational unit. 
5. Acknowledgement The GKS concept has emerged from a long process of discussion and refinement within 
the DIN-working group NI-5.9 which, besides the authors, includes Baub~ck (Hamburg), Eckert (Darmstadt), 
Egloff (Berlin), Gnatz (MUnchen), Gonauser (MUnchen), Grauer (Karlsruhe), Grieger (Stuttgart), Konkart 
(Konstanz), Mittelstrass (Bonn), Nowacki (Berlin), Pasemann (Wolfsburg), Prester (Erlangen), Reumann 
(Hannover), Schuster (MUnchen), Roth (Lippstadt), Sch~nhut (Erlangen), Dobrowolski (Frankfurt), Borufka 
(Darmstadt), Kuhlmann (Darmstadt), Pfaff (Darmstadt), and others. GKS drived profit from the efforts 
of developing the GSPC core system; we are acknowledging the good interaction and communication between 
the two groups. 6. Bibliography /1/ J. Encarnacao, B. Fink, E. H~rbst, R. Konkart, G. Nees, D.L. Parnas, 
E.G. Schlechtendahl A recommendation on Methodology in Computer Graphics; A position paper for the IFIP-Graphics 
Workshop, Seillac, France, 1976 (Seillac I), published as report KfK 2394, 1977, Karlsruhe /2/ R. Eckert, 
F.J. Prester, E.G. Schlechtendahl and P. WiBkirchen; Functional description of the Graphical Core System 
GKS as a step towards standardization; Informatik-Fachberichte II, Springer 1977, pp. 163 /3/ R. Eckert, 
G. Enderle, K. Kansy and F.J. Prester; GKS'79 -Proposal of a Standard for a Graphical Kernel System; 
Proceedings of Eurographics'79, Bologna, October 1979, pp. 2 /4/ Proposal of Standard DIN 0066252; Information 
Processing -Graphical Kernel System (GKS); Functional description, NI-5.9/26-79, Version 5.2; 5.11.1979 
/5/ R.A. Guedj and H.A. Tucker (editors) Methodology in Computer Graphics (Seillac I) North-Holland, 
1979 229 /6/ J. Weiss; Device driver interfaces for de- central device drivers; Proceedings of Eurographics'79, 
Bologna, October 1979, pp. 252 /7/ "Status Report of the Graphic Standards Planning Committee"; Computer 
Graphics, Vol. 13, No. 3, August 1979
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1980</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>807498</article_id>
		<sort_key>231</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-1980</article_publication_date>
		<seq_no>39</seq_no>
		<title><![CDATA[BUMPS]]></title>
		<subtitle><![CDATA[A program for animating projections]]></subtitle>
		<page_from>231</page_from>
		<page_to>237</page_to>
		<doi_number>10.1145/800250.807498</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=807498</url>
		<abstract>
			<par><![CDATA[<p>BUMPS (Brown University Multiple Projection System) is a program that illustrates the implementation of viewing transformations using animation. The program uses the viewing model defined in the Core Graphics System. BUMPS employs interactive computer graphics to demonstrate how planar geometric projections are generated, what the effects of different projections and projection parameters are on the projected object, and how the viewing functions of the Core Graphics System work. After presenting background material on projections, the features of BUMPS are described, followed by a pictorial user scenario of BUMPS in action. The paper concludes with a discussion of the merits of user controlled animation for teaching and possible improvements to the program.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Animation]]></kw>
			<kw><![CDATA[Core graphics system]]></kw>
			<kw><![CDATA[Interactive computer graphics]]></kw>
			<kw><![CDATA[Planar geometric projections]]></kw>
			<kw><![CDATA[Viewing transformations]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.4.7</cat_node>
				<descriptor>Projections</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor>Viewing algorithms</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Animation</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061.10010068</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures->Random projections and metric embeddings</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P333406</person_id>
				<author_profile_id><![CDATA[81100503448]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Robert]]></first_name>
				<middle_name><![CDATA[F.]]></middle_name>
				<last_name><![CDATA[Gurwitz]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Department of Computer Science, Brown University, Box 1910, Providence, RI]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P333329</person_id>
				<author_profile_id><![CDATA[81332531851]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Richard]]></first_name>
				<middle_name><![CDATA[W.]]></middle_name>
				<last_name><![CDATA[Thorne]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Department of Computer Science, Brown University, Box 1910, Providence, RI]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P18590</person_id>
				<author_profile_id><![CDATA[81452592989]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Andries]]></first_name>
				<middle_name><![CDATA[van]]></middle_name>
				<last_name><![CDATA[Dam]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Department of Computer Science, Brown University, Box 1910, Providence, RI]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P330867</person_id>
				<author_profile_id><![CDATA[81100497708]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Ingrid]]></first_name>
				<middle_name><![CDATA[B.]]></middle_name>
				<last_name><![CDATA[Carlbom]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Computation Center, The Pennsylvania State University, University Park, PA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>356750</ref_obj_id>
				<ref_obj_pid>356744</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Carlbom, I., and Paciorek, J. "Planar Geometric Projections and Viewing Transformations," Computing Surveys 10, 4 (Dec. 1978), 465-502.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[GSPC. "Status Report of the Graphics Standards Planning Committee of ACM/SIGGRAPH," Computer Graphics 11, 3 (Fall, 1977).]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>988498</ref_obj_id>
				<ref_obj_pid>988497</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[GSPC. "Status Report of the Graphics Standards Planning Committee of ACM/SIGGRAPH," Computer Graphics 13, 3 (Aug., 1979).]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Gurwitz, R.F., Fleming, R.T., and van Dam, A. "MIDAS: A Microprocessor Display and Animation System," Brown University Department of Computer Science Technical Report CS-43, October, 1978.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Ostby, E. "Brown University Multiple Projections System (BUMPS) User's Guide," Brown University, Providence, RI, February, 1979.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Stocker, F.R., and Minsker, T. "Graphics Geometric Perception and Communications," Computers and Graphics 1 2/3 (Sep. 1975), 161-174.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 BUMPS: A Program for Animating Projections Robert F. Gurwitz Richard W. Thorne Andries van Dam Department 
of Computer Science Brown University Box 1910 Providence, RI 02912 Ingrid B. Carlbom Computation Center 
The Pennsylvania State University University Park, PA 16802 ABSTRACT BUMPS (Brown University Multiple 
Projection System) is a program that illustrates the implementation of viewing transformations using 
animation. The program uses the viewing model defined in the Core Graphics System. BUMPS employs interactive 
computer graphics to demonstrate how planar geometric projections are generated, what the effects of 
different projec- tions and projection parameters are on the projected object, and how the viewing functions 
of the Core Graphics System work. After presenting back- ground material on projections, the features 
of BUMPS are described, fol- lowed by a pictorial user scenario of BUMPS in action. The paper concludes 
with a discussion of the merits of user controlled animation for teaching and possible improvements to 
the program. Keywords: interactive computer graphics, planar geometric projections, ani- mation, Core 
Graphics System, viewing transformations C R Categories: 8.2, 3.51. i. Introduction They are useful 
for displaying the general appearance or clearly indicating the shape A central problem in computer graph-and 
certain measurements (height, width, ics is the projection of three-dimensional depth, angles) of an 
object. Depending on objects onto a two-dimensional viewing how the projection has been defined, dif- 
surface. To accomplish this, a class of ferent visual effects are obtained: vari- transformations known 
as planar @eometric ous properties of projected objects are projections is used. Planar geometric preserved 
or emphasized. For example, projections map points of an object onto a perspective projections are used 
to give plane using straight line projectors.* objects a realistic appearance, but do not accurately 
represent their dimensions. On * Throughout this paper we shall use the other hand, parallel projections 
the term projections to refer to attempt to preserve the dimensions of planar geometric projections. 
objects, but do not necessarily appear lifelike. Permission to copy without fee all or part of this 
~terial is granted provided that the copies are not made or distributed The visual effects of the resulting 
for direct commercial advantage, the ACMcopyright notice and projection depend on the shape of the the 
title of the publication and its date appear, and notice object being displayed, the type of pro- is 
given that copying is by permission of the Association for jection used, and several projection Computing 
Machinery. To copy otherwise, or to republish, requires a fee and/or specific permission. parameters 
(described in Section 2). It is often difficult to visualize the result of a projection of an object, 
given an 01980 ACM 0-39791-021-4/80/0700-231 $00.75 231 ~riori set of parameters: small changes zn 
parameter values may result in unex-pected changes to the projected object. Similarly, certain classes 
of projections behave in counterintuitive ways. An interactive program that dynamically illustrates the 
effects of different pro-jections and projection parameters can therefore be of great value. dimensional 
relationships, which are dif- ficult for most people to visualize. Another use of the program is the 
selection of the best views of projected objects for the production of illustrations. Choosing the most 
effective illustrations for teaching is difficult with conventional media, since different The Core 
Graphics System as defined by the Graphics Standards Planning Commit- tee of ACM/SIGGRAPH (GSPC) specifies 
a set of constructs that define a viewing transformation with which projections may be generated [GSPC, 
1977; GSPC, 1979]. The viewing transformation may be thought of as a "pipeline", wherein the object 
to be viewed is subjected to a sequence of six primitive operations which generate the desired view: 
translation, rotation, shearing, scaling, clipping, and (for per- spective projections) perspective 
divi- sion. In this paper, we shall describe a program called BUMPS (Brown University Multiple Projection 
System), that demon- strates how planar geometric projections are generated using the viewing operations 
of the Core Graphics System. BUMPS allows a user to interactively o specify an object  o select the 
type of projection to be used  o specify projection parameters.  Once these have been entered, it 
then ani- mates the effect of the six operations that comprise the viewing transformation. In addition 
to illustrating the decomposi- tion of projections, BUMPS also animates the clipping and window to viewport 
transformations which complete the viewing process. After the animation sequence, the user may interactively 
vary the view- ing parameters and see how they affect the resulting view of the object by re- animating 
the projection operations. BUMPS is a teaching tool that demon- strates how the projections are generated, 
the effects of varying the projection parameters, and how the Core Graphics System's viewing transformations 
work. It also exemplifies our philosophy of using user controlled animations for teaching purposes [Gurwitz, 
1978]. We believe that a user controlled animation (one where the user can select what will be animated, 
stop the animation, and vary its speed) can be more effective for complex, dynamic visualizations than 
a "canned" animation presented on videotape or film. For example, this approach allows the user to examine 
and more readily understand complex three- people prefer different views and require varying amounts 
of explanation. Once the "best" views are chosen, the cost of drafting quality illustrations is high. 
User controlled animations can solve these problems by offering illustra- tions which are customized 
(in terms of speed, level of detail, point of view, etc.) for the ind ividual. A previous example of 
the use of interactive computer graphics for instruc- tion about computer graphics was described by 
Stocker and Minsker [Stocker, 1975]. Their program was used to teach general three-dimensional geometry 
for computer graphics (effects of translation, rota- tion, and scaling on an object), but did not illustrate 
the decomposition of the steps of the viewing transformation. It was used successfully in a computer 
graph- ics course (for both graduates and under- graduates) and to assist research users. In all cases, 
it was found to reduce teaching time and assist the development of three-space geometric comprehension. 
 2. Planar Geometric Projections and the Core Viewing System Before we proceed with a description of 
BUMPS, some background on projections will be presented, along with definitions of the parameters used 
in the Core viewing operations. T A projection of an object is generated by passing straight lines called 
projectors through each point of the object, and finding the intersections of these projectors with a 
projection i~. The projectors emanate from a single point called the center of projection. When this 
point is at a finite distance from the object, a perspective projection is obtained. When it is at infinity, 
all the projectors are parallel and a parallel projection results. Both perspective and parallel projec- 
tions can be determined by the choice of the center of projection and the projec- tion plane. Another 
approach to determin- ing the projection is to fix the center of projection and projection plane, and 
posi- tion the object to obtain the desired view. While this method has been widely t For a detailed 
tutorial on projec- tions, see [Carlbom, 1978]. For more information on the Core Graphics Sys- tem viewing 
operations, see [GSPC, 1979]. 232 The object parameters are: o Translation  o Scaling  o Rotation 
  The viewing parameters are: o View reference point  o View plane normal  o View plane distance 
 o View up vector  o Window position and size  o Eyepoint (or direction of the pro- jector)  Each 
parameter is entered separately. As the user varies the parameter, the system displays its resulting 
effect on the object or its position. Once the desired value is obtained, the parameter is fixed by depressing 
a function key. Once the object and viewing parame- ters have been set, the implementation of the resulting 
viewing transformations can be animated. In animation mode, the transformations are slowly animated to 
show the user what is happening. The user can control the speed of the animation with a dial, and step 
through each part of the sequence with a function key. The action may be stopped and replayed at any 
time. The sequence of animation steps is o Translation The eyepoint is shifted to the origin.  o Rotation 
The view plane normal is rotated parallel to the Z- axis, and the orthographic projection of the view 
up vec- tor onto the viewplane is  rotated to be parallel to the Y-axis.  o Shearing For parallel projections, 
the projectors are made parallel to the Z-axis. For perspec- tive projections, the view pyramid is uprighted 
(the center of the window is posi- tioned on the Z-axis, while the window itself is kept per-  pendicular 
to the Z-axis).  o Scaling  The apex angle of the view pyramid is changed to a 90 ° angle (to facilitate 
clip- ping). o Clipping The object is clipped to fit the view volume, except that back clipping is not 
done. (This varies from the Core  specification which includes both front and back clipping.)  o Perspective 
division A perspective view of the object is presented (if a per- spective projection was speci- fied). 
  When the animation is finished, the resulting image is displayed in a separate viewport to show how 
it would be seen on the view plane. A detailed description of all the features of BUMPS can be found 
in [Ostby, 1978]. 4. A User Scenario To illustrate the operation of BUMPS, a typical user scenario 
is presented. For the purpose of clarity, the figures in this section were produced from plots gen- erated 
by BUMPS rather than actual photo- graphs of the display. Naturally, a sequence of static views cannot 
convey the effect of the dynamics of BUMPS in action. This section attempts to give the reader an idea 
of the capabilities of the pro- gram. To fully appreciate the power of the user controlled animation, 
the system must be experienced in person. The following figures illustrate the process of initially 
setting the object and projection parameters for a perspec- tive projection. First, the object (a house) 
is positioned (Figure 2), rotated and scaled. In Figure 2, note the use of dashed construction lines 
in positioning the house. Note also that the world coor- dinate system may be viewed from different angles 
at any point in the parameter set- ting or animation process. / Z Figure 2 234 Once the object is positioned, 
the viewing parameters can be set in a similar manner. In Figure 3, the view up vector is set. The labeled 
construction lines show the view up vector (VUV), view plane normal (VPN), and the projection of the 
view up vector onto the view plane (dashed line). The view reference point (VRP) is also shown. is at 
the origin of the coordinate system. EYE~ ~.~ VUV / / VPN / \ / VUV / \ / \\\ \\ / / I / I \\ 
g / %z )- / Figure 5 f Next, the pyramid is rotated so that view plane normal is parallel to the Z-axis 
and Figure 3 the projected view up vector is parallel to the Y-axis (Figure 6). Figure 4 shows the 
settings of all the parameters just prior to the start of the animation sequence. The view pyramid is 
shown in dashed lines emanating from the eyepoint. The pyramid's base is the window, which is the intersection 
of the semi-infinite pyramid with the view plane. VUV I ~ ~YE ~--_~ EYE VPN I /  -, ,-2"~\, L, I/ 
\ ] \ \ ~<--~._. \ { A'/ %--\ -",  ~\ Figure 6 I \\ ,- The view plane is then sheared until the window 
is centered about the Z-axis (Fig- Figure 4 ure 7). The sequence of steps in the anima- tion of the 
viewing transformation are shown in the next group of figures. In the translation step (Figure 5), the 
 pyramid is positioned so that the eyepoint 235 Clipping is then performed on all lines outside of the 
transformed pyramid. The clipping step is not shown, but in the animation each portion of the lines to 
be clipped is dimmed. Finally, perspective division is done, yielding the final view of the object (Figure 
9). I VUV VUV ~ : --:~ v~ ----- T I / I I I //~" I I I /! I I " IVRP I //\\\  / X l \\\ Figure 
7 ', The pyramid is scaled until the top of the pyramid has a 90 ° apex angle (Figure 8). Figure 9 
 VUV Figure 10a shows a side view of the final transformation. The final view of the object (from the 
face of the pyramid) is shown in Figure 10b. VUV  ~" IVRP I ~ ~ //\\\ /--- 71 /I / lOb.  /-- \ 
/ /I / " I \ I / / ~z \ / / I ,~ \\ /. <vRP I I k._ ¢ \ II ....... \' R - "-'k -'-I - i- \\ I~ I \ ~\1 
I \\ i~\ I 10a \ ,. I Figure 8 Figure i0 236 After the initial setting of parame- ters and animation, 
the user may decide to change some parameter and re-animate the entire viewing process. Parallel projec- 
tions proceed in a similar way, with the system illustrating the view paral- lelepiped. 5. Conclusions 
 The interactively controlled anima- tion approach used in BUMPS has obvious advantages over static 
forms of explana- tion, such as books with diagrams and accompanying textual descriptions, in illustrating 
the steps in the viewing transformation. This is because the view- ing process is inherently dynamic 
(i.e., it consists of a sequence of steps). The advantages of the BUMPS approach over other dynamic 
media, such as film or videotape are less obvious. The high degree of control available lets the user 
manipulate the parameters and control the animation at a pace suited to him or her. Animation steps can 
be slowed down or replayed as necessary until a good under- standing of the process is achieved. In addition, 
the choice of parameters, pro- jection types, and view of the entire operation, gives the user much more 
flexi- bility in selection of views than could be obtained with a set of predefined film or videotape 
sequences. As a teaching tool, BUMPS is designed to be used under the supervision of a knowledgeable 
teacher. Thus, it must be considered more an aid to visualization rather than a self-teaching tool. The 
presentation is much more effective if the student is already familiar with the ter- minology of projections 
and viewing transformations (such as presented in Sec- tion 2). However, the use of accompanying descriptive 
material on the screen, and the ease of interaction control also enables BUMPS to be effective for indivi- 
dual use. There are several areas for improve- ment of BUMPS. The system could be made more effective 
as a self-teaching tool with the addition of an alternative mode of operation in which a pre-defined 
sequence is presented. Then, after the user became familiar with the concepts involved, he or she could 
use the current capabilities to further experiment with the effects of different parameters and projections. 
The use of color and shading would offer marked improvement in the understandability of the display, 
espe- cially in delineating labels, illustra- tions of viewing parameters (such as the view plane normal, 
view up vector, etc.), and in the animation. Also, the use of sound (voice) instead of text for descrip- 
tive material could free the user from having too much to read during each anima- tion step and declutter 
the screen.  There are also several questions con- cerning the effectiveness of the presenta- tion effects 
that are used. Can the use of varying line styles for delineating construction lines and object lines 
be improved? What are the tradeoffs between the use of real-time dynamics (transla- tion, scaling, rotation, 
and clipping) available in vector displays, and the use of color and shaded surfaces offered by current 
raster displays (i.e., is smooth motion preferable to a more sophisticated pictorial representation)? 
These ques- tions present possible areas for future research.  In conclusion, we feel that user con- 
trolled animations, as exemplified by BUMPS, can be effective tutorial tools. With the rapid proliferation 
of relatively inexpensive smart graphics terminals, approach seems to have high payoff merits further 
development. the and Acknowledgements The authors wish to acknowledge the work of Eben Ostby, who originally 
imple- mented BUMPS, Michael Braca for the photo- graph of BUMPS, and Joseph Paciorek. References 
 [Carlbom, 1978] Carlbom, I., and Paciorek, J. "Planar Geometric Projections and Viewing Transformations," 
Computing Surveys 10, 4 (Dec. 1978), 465-502. [GSPC, 1977] GSPC. "Status Report of the Graphics Standards 
Planning Committee of ACM/SIGGRAPH," Computer Graphics l!, 3 (Fall, 1977). [GSPC, 1979] GSPC. "Status 
Report of the Graphics Standards Planning Committee of ACM/SIGGRAPH," Computer Graphics i__3, 3 (Aug., 
1979). [Gurwitz, 1978] Gurwitz, R.F., Fleming, R.T., and van Dam, A. "MIDAS: A Microprocessor Display 
and Animation System," Brown University Department of Computer Science Technical Report CS-43, October, 
1978. [Ostby, 1979] Ostby, E. "Brown University Multiple Projections System (BUMPS) User's Guide," Brown 
University, Pro- vidence, RI, February, 1979. [Stocker, 1975] Stocker, F.R., and Minsker, T. "Graphics 
Geometric Per- ception and Communications," Comput- ers and Graphics ! 2/3 (Sep. 1975), 16i-174.  237 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1980</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>807499</article_id>
		<sort_key>238</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-1980</article_publication_date>
		<seq_no>40</seq_no>
		<title><![CDATA[Natural and efficient viewing parameters]]></title>
		<page_from>238</page_from>
		<page_to>245</page_to>
		<doi_number>10.1145/800250.807499</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=807499</url>
		<abstract>
			<par><![CDATA[<p>The viewing scheme in the Core Graphics System is based on a unified approach to the specification of the viewing parameters for all types of planar geometric projections. As a result, the specification of some of the viewing parameters is inconsistent with traditional ways of specification and will often be found unnatural for use in certain application areas. Additionally, this choice of viewing parameters leads to inefficient implementations of viewing parameter modification, particularly in a high-performance graphics system which supports transformations in hardware or firmware.</p> <p>The natural ways to specify the different viewing parameters are discussed and efficiency considerations for viewing implementations are described. The Core System viewing scheme is evaluated in terms of its naturalness and efficiency of implementation. An alternate viewing scheme is proposed that provides viewing parameters that are more natural for many applications and that can he modified more efficiently.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Computer graphics]]></kw>
			<kw><![CDATA[Core graphics system]]></kw>
			<kw><![CDATA[Graphics package design]]></kw>
			<kw><![CDATA[Parallel projections]]></kw>
			<kw><![CDATA[Perspective projections]]></kw>
			<kw><![CDATA[Planar geometric projections]]></kw>
			<kw><![CDATA[Viewing transformations]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.3</cat_node>
				<descriptor>Viewing algorithms</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.4</cat_node>
				<descriptor>Graphics packages</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.7</cat_node>
				<descriptor>Projections</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061.10010068</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures->Random projections and metric embeddings</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P133036</person_id>
				<author_profile_id><![CDATA[81100509207]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[James]]></first_name>
				<middle_name><![CDATA[C.]]></middle_name>
				<last_name><![CDATA[Michener]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Intermetrics, Inc., Cambridge, Mass.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P330867</person_id>
				<author_profile_id><![CDATA[81100497708]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Ingrid]]></first_name>
				<middle_name><![CDATA[B.]]></middle_name>
				<last_name><![CDATA[Carlbom]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The Pennsylvania State University, University Park, Pa.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>356750</ref_obj_id>
				<ref_obj_pid>356744</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Carlbom, I. and J. Paciorek, "Planar Geometric Projections and Viewing Transformations," Computing Surveys 10, 4 (December 1978) pp. 465-502.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>909934</ref_obj_id>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Carlbom, I. B., System Architecture for High-Performance Vector Graphics, Ph.D. Thesis, Brown University, Providence, R.I. (1980).]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Graphics Standards Planning Committee, "General Methodology and Proposed Standard," Computer Graphics 11, 3 (Fall 1977) pp. II-1-II-117.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Graphics Standards Planning Committee, "General Methodology and the Proposed Core System (revised)," Computer Graphics 13, 3 (August 1979) pp. II-1-II-179.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Mallgren, W. R. and A. C. Shaw, "Graphical Transformations and Hierarchic Picture Structures," Computer Graphics and Image Processing 8 (1978) 237-258.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>356749</ref_obj_id>
				<ref_obj_pid>356744</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Michener, J. C. and J. D. Foley, "Some Major Issues in the Design of the Core Graphics System," Computing Surveys 10, 4 (December 1978) 445-463.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Newman, W. M., "Instance Rectangles and Picture Structures," Proc. Conference on Computer Graphics, Pattern Recognition, and Data Structures, University of California, Los Angeles (May, 1975) 297-301.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_obj_id>5532</ref_obj_id>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Newman, W. M. and R. F. Sproull, Principles of Interactive Computer Graphics, McGraw-Hill Book Company (1973).]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_obj_id>563898</ref_obj_id>
				<ref_obj_pid>965141</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Puk, R. F., "General Clipping on an Oblique Viewing Frustrum," Computer Graphics 11, 2 (Summer 1977) pp. 229-235.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807406</ref_obj_id>
				<ref_obj_pid>965139</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Warner, J. A., M. A. Polisher, and R. N. Kaplow, "DIGRAF&#8212;A FORTRAN Implementation of the Proposed GSPC Standard," Computer Graphics 12, 3 (August 1978) pp. 301-307.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Natural and Efficient Viewing Parameters* James C. Michener Intermetrics, Inc. Cambridge, Mass. Ingrid 
B. Carlbom The Pennsylvania State University University Park, Pa. ABSTRACT The viewing scheme in the 
Core Graphics System is based on a unified approach to the specification of the viewing parameters for 
all types of planar geometric projections. As a result, the specification of some of the viewing parameters 
is inconsistent with traditional ways of specification and will often be found unnatural for use in certain 
application areas. Additionally, this choice of viewing parameters leads to inefficient implementations 
of viewing parameter modification, particularly in a high-performance graphics system which supports 
transformations in hardware or firmware. The natural ways to specify the different viewing parameters 
are discussed and efficiency considerations for viewing implementations are described. The Core System 
viewing scheme is evaluated in terms of its naturalness and efficiency of implementation. An alternate 
viewing scheme is proposed that provides viewing parameters that are more natural for many applications 
and that can he modified more efficiently. Key Words and Phrases: computer graphics, Core Graphics System, 
viewing transformations, planar geometric projections, perspective projections, parallel projections, 
graphics package design. CR Categories: 8.2, 5.0 I. INTRODUCTION Viewing parameters are data and control 
information, specified by the application program, that determine the viewing transformation from 3D 
world coordinates to a viewport in normalized device coordinates. For example, the coordinate triplet 
defining a view plane normal is a viewing parameter. An example of control information, as distinguished 
from data values, is the information that determines whether a perspective or parallel projection is 
required. This paper represents views of the individual authors, and not those of any standards committee. 
 The paper has been submitted to ANSI ×3H3 for consider at ion. Permission to copy without fee all 
or part of this mmterial is granted provided that the copies are not made or distributed for direct 
commercial advantage, the ACM copyright notice and ~980 ACM 0-8979]-02]-4/80/0700-238 $00.75 In most 
graphics systems, viewing parameters are not used directly to transform and clip graphics primitives. 
Instead, internal, implementation- dependent values are derived from the parameters; these internal values 
are used in the transformation and clipping "pipeline." In this paper, computing these internal values 
will be called "setting up the viewing pipeline." The three principles guiding the design of viewing 
parameters in the results reported below are functional completeness, naturalness, and efficiency. By 
functional completeness we mean that all planar geometric projections are supported. Furthermore, a user 
should not be required to specify either any modelling transformation or a 4x4 viewing matrix to achieve 
a desired view. The viewing parameters should be sufficiently general to allow specification of a planar 
geometric projection either in terms of desired properties of the projected object or in terms of properties 
of the projection. Other methods of specification are generally agreed to lack naturalness. By naturalness 
we mean the ease with which an application programmer can understand, learn, and use viewing parameters. 
The viewing parameters should fit into a conceptual model that is easy to explain and remember. At the 
same time the specification of projections should conform to methods traditionally used in an application 
area. Efficiency considerations enter into the design of viewing parameters both in the efficiency of 
transforming and clipping individual graphics primitives and in the efficiency of setting up the viewing 
pipeline when a viewing parameter changes. Although the implementation of the viewing pipeline affects 
the efficiency of both of these aspects of processing, this paper will mainly focus on the latter. 
Many authors have elaborated on one of the three design principles discussed above, namely the desirability 
of a functionally complete viewing system that is independent of the modelling system [GSPC 77, Michener 
78, GSPC 79]. The other two design principles, naturalness and efficiency, are developed in this paper 
from ideas originating from work reported in [Carlbom 80]. The next section will further define what 
we mean by naturalness and efficiency. In the following section the Core System will be evaluated in 
terms of these design principles, and it will be shown that the Core the title of the publication and 
its date appear, and notice is given that copying is by permission of the Association for Computing Machinery. 
To copy otherwise, or to republish, requires a fee and/or specific permission. 238 System is inadequate 
in both of these respects for many applications. An alternate set of viewing parameters, which is more 
natural and efficient for many applications, is defined in the fourth section. 2. VIEWING PARAMETER 
DESIGN CRITERIA 2.1 NATURALNESS As discussed in [Carlbom 78], planar geometric projections are classified 
according to the method of projection. The two main classes are perspective and parallel. Parallel projections 
are further subdivided into orthographic projections, and plan and elevation oblique projections. Further 
subclassifications can be made, hut are not relevant to this paper. The different projection methods 
result in images having significantly different visual effects, and consequently, the different types 
of projections have different uses. A perspective projection gives a realistic representation of an object 
as it would be seen by the eye, whereas a parallel projection primarily attempts to represent metric 
properties of an object, e.g., distances and angles, at the expense of realism. Perspective projections 
are used in advertising, and for presentational drawings by architects, engineers, and industrial designers. 
Parallel projections are used in the same fields, but are primarily used for working drawings. Traditionally, 
different types of projections have been specified in different ways [Carlbom 78]. Perspective projections 
are specified in terms of the eyepoint and projection plane. Parallel projections are specified in terms 
of the direction of the projected axes and, for oblique projections, the foreshortening ratios along 
the axes, as well. In computer graphics applications, traditional uses of planar geometric projections 
have been extended. In some applications it is desirable to repeatedly alter the projection to simulate 
what a moving viewer "sees" (perspective projections) or to illustrate a three-dimensional shape (perspective 
or orthographic projections). These considerations do not apply to oblique parallel projections, because 
the projection plane should always be parallel to a principal face of the object being viewed. By natural 
viewing parameters we mean parameters that conform to traditional conventions for specification except 
where differences are needed to specify dynamic motion easily. The scheme proposed in Section 4, is 
a compromise between methods of specification in engineering drawing and in dynamic computer graphics. 
The projection plane for all projections is specified by its orientation and by its distance in a specified 
direction from a specified point. For perspective projection, that point is the eyepoint. In addition, 
for oblique projection, the receding lines (projections of lines perpendicular to the projection plane) 
are determined from an angle and a foreshortening ratio. The orientation of the window on the projection 
plane is specified by the direction that should appear "right" on the display surface. 2.2 EFFICIENCY 
 Efficiency considerations apply both to processing of individual primitives and to setting up of the 
viewing pipeline. Thus, efficiency in processing a primitive requires both an efficient clipping algorithm 
and an efficient coordinate transformation. Clipping can be performed in any of several coordinate 
systems involved in transforming modelling coordinates to device coordinates [Mallgren 78, Newman 73, 
Newman 73a, Puk 77]. By comparing different methods one sees that clipping operates most efficiently 
in a normalized clipping region. The normalized clipping regions that are used in this paper are illustrated 
in Figure I for perspective projection and Figure 2 for parallel projection. Yc (-I, I, I) W'n~,.,°w//l~ZC 
: Yc Front ~. ~ ~(I, I,I) 1 B~Z 'c c=l " "<al (I,-I,I) Figure I. Normalized clipping region for perspective 
projection. 0 < IXcl,IY~ < Zc; Front < Z c < I. Yc (-I, 1,0) ~........j ~(l,L, i ) ,,  ~~Xc (-I,-I,0} 
(I,-I,I) (I,-I,0) Figure 2. Normalized clipping region for parallel _ _ < I. projection. -I < Xc,Y 
c < 1 and 0 < Z c _  Efficiency in coordinate transformations is obtained by composing transformations 
so that as few transformations as possible are applied to each primitive. Therefore, ideally, clipping 
should be performed before any transformations are applied or after all have been applied. The former 
is undesirable for many applications since clipping to an arbitrarily oriented non-rectangular parallelepiped 
or pyramid must be supported. The latter is undesirable since all parts of the object must be transformed 
whether visible or not; in addition, objects behind the eye must be clipped before perspectlve division 
occurs [Newman 73a]. As a result, at least two transformations are required: one before and one after 
the perspective division. By chosing the normalized clipping regions discussed above, the second transformation 
is simplified without complicating the first. In summary, for many applications the most efficient 
 clipping/transformation algorithm is implemented by first transforming the Primitives to the normalized 
 clipping coordinate space, then clipping, and then mapping the primitives to device coordinates. (The 
 mapping to device coordinates is a simple non- uniform scale and translation; it is not discussed 
in subsequent sections.) The second area for efficiency considerations is setting up the viewing pipeline. 
In doing this, it is undesirable to recalculate a whole, composite transformation. This is particularly 
true for high- performance graphics systems with hardware transformation capabilities; here it is desirable 
to allow a sequence of transformations in the display program to be set up such that the composite is 
the desired transformation and such that changing one viewing parameter causes a change in only one transformation. 
The degree to which this can be accomplished is an efficiency criterion which will be called "separability 
of viewing parameters." In the next section, one implementation of the 3D Core System viewing transformations 
is presented. This implementation is evaluated in terms of its efficiency. Furthermore, the Core System 
viewing parameters are evaluated in terms of their capability for a programmer to express a projection 
in a natural way. A revised set of viewing parameters is proposed in the following section having greater 
naturalness and permitting greater efficiency. 3. THE CORE SYSTEM VIEWING PARAMETERS The Core System 
viewing parameters [GSPC 79] are listed in Table 3-I. An additional set of variables that are used throughout 
this section are defined in terms of the viewing parameters. These quantiti%s and some additional notation 
are listed in Table 3-2. In this paper, transformations are expressed as 4x4 homogeneous coordinate 
matrices, and a coordinate position is represented by either a three-or a four-element row vector. Vectors 
are denoted by an underscore (e.g., REF). Matrices will sometimes be represented in block form. Thus, 
the 4x4 identity matrix would be written: Parameter Meaning REF view reference point NORM view plane 
normal vd view distance fd front distance bd back distance UMIN,UMAX,VMIN,VMAX window edges WIDTH,HEIGHT,DEPTH 
NDC space XMIN,XMAX,YMIN, YMAX,ZMIN,ZMAX viewport P__~ projection direction or center of projection 
 UP view up direction PROJ type: parallel or perspective Table 3-1. Core System Viewing Parameters 
 Quantity Meaning NORM ~= unit normal IfNORMIf u_z-(uP.a)a ~= unit view vector  IIu.~_-(u_f.n)nfl 
u=nxv unit view right PR =PR.u pRU:~.v components of P_R in the pRv=~.n UVN-system n -- -- I 3x3 identity 
matrix Z:(0,0,0) the origin Table 3-2. Derived Quantities and Notation In what follows, perspective 
and parallel projections are handled separately. In each case, the transformation that maps the view 
volume in world coordinates to the appropriate normalized clipping region (Figures 1 and 2) is broken 
down into a sequence of simple matrices. Other, slightly different, sequences of matrices could be chosen. 
The sequences are chosen to achieve as much parameter separation as the Core System allows. As will be 
seen in later sections, a different viewing scheme gives better separation. 3.1 PERSPECTIVE PROJECTIONS 
 The sequence of mappings which transforms the view volume in world coordinates to the normalized clipping 
region is the following:  - translate the view reference point to the origin  - translate the center 
of projection to the origin  - change coordinates from XYZW to X Y Z W - scale so that the back of 
the v~ew c volumeC is in the Z =I plane 0 ~AXUVMIN 0 (3-7) - scaleCX and Y so that the back of the view 
volume 0 0 I is in the range UMIN - PR < X < UMAX -PR and VMIN -PR < Y < VMAX -P~-c -- u   ° il 0 
0 0  - shear so ~|~t ~he view voluVme center line moves to the Z axis - scale c so that the back of 
the view volume is The resulting transformation, the product of _ <1. bounded by -I < X c, Yc -- 
these seven matrices, will be denoted (Here X Y , and Z are the clipping coordinate (3-1)(3-2)(3-3)(3-4)(3-5)(3-6)(3-7). 
c c axes, aCrid correspond to the u, V, and _n world coordinate directions.) The seven corresponding 
3.2 PARALLEL PROJECTIONS matrices are:  The sequence of mappings in this case is the following: - 
translate the view reference point to the origin (i.e., (3-1)) -REF  - change coordinates from XYZW 
to X Y Z W (i.e., (3-3)) c c c - translate the window center to the origin - shear to align projection 
direction with Z - scale and shift so that -l<Xc,Yc~l_ c-- and O<ZC<I._ The matrices for the last 
three steps ale: ut v t n t it] (3-3) l  0 ~d_--IpRn o (3-4) 0 0 ~d_~Rn  ° il 0 0 0 vd-PR n 
0 0 0 0 vd-PRn 0 (3-5) 0 0 I 0 0 0  (3-6) [ o o  PRu_UMIN~UMAX PRy. VMIN+VMAX2 I 0 0  (3-8) -UMIN2-UMAX 
-VMIN-VMAX2 -vd (3-9) -PR u -PRv °°I  o ii PRon pR n 1 0 0  UMAX-UMIN 0 0 0 0 NAX~VMIN 0 I (3-10) 
o 0 bd- fd vd-fd 0 0 bd- fd  I. 1  and the total transformation is (3-1)(3-3)(3-8)(3-9)(3-10). 3.3 
ANALYSIS OF THE CORE SYSTEM VIEWING PARAMETERS  The design of the Core System viewing parameters is 
based upon similar geometrical models for both parallel and perspective projections. The advantage of 
this approach is that these models are simple for the application programmer to learn, by virtue of their 
similarity. This Core System model has three disadvantages regarding the naturalness of the viewing 
parameters. One disadvantage is that the geometric relationship between the window and the center of 
projection (i.e., the "eyepoint") depends on P R, NORM, UP, vd, and of course, the window edges. As a 
result, a change in NORM or UP (assuming P__R is non-zero) results in the window "sliding" on the view 
plane relative to the center (Figure 3). The geometric relationship between the window and the center 
of projection should be specified more directly, in a fashion that can capture the essence of the statement: 
"the eye is centered opposite a ten-inch window that is 22 inches away."  NORM VOLUME \ IIoV UWPN~p 
 ~d~~M~AX  UMIN~ }UMAX  Figure 3a. Orthographic view of view volume with NORM and P__Rparallel.  ]Alvd 
~VOLUM E Figure 3b. Orthographic view of view volume after a 12 degree change in NORM. The second disadvantage 
is that conventional specification of oblique parallel projections is in terms of desired properties 
of the resulting image, not in terms of projection direction. The third disadvantage is that in order 
to achieve an orthographic projection, the application program must keep the normal and the projection 
 direction parallel. This means that, when a sequence of orthographic views is desired, for each view 
in the sequence two parameters must be changed where one should suffice. The computations required to 
modify the viewing transformation when the parameters change can be judged by examining the matrices 
in Sections 3.1 and 3.2. For perspective projections, the components in four of the seven matrices must 
be modified when NORM or U__P changes. Similarly, a change in P_.R affects four of the seven matrices. 
The calculations are not as complicated for parallel projections; changes in NORM and U_P affect only 
two of the five matrices. In the viewing scheme presented in the following mection, the viewing transformation 
can be written as a product of matrices without these dependencies. 4. ALTERNATE VIEWING SCHEME This 
section proposes an alternate viewing scheme that is more efficient and that allows specification of 
viewing parameters in a more natural way than the Core System. The viewing parameters are described 
but no actual calling sequences are given; they are not necessary in order to develop the ideas in this 
paper. Subsequent subsections present matrix sequences for the alternate viewing transformations for 
comparison with the sequences in the preceding section. Unless stated otherwise, symbols have the same 
meaning as in Section 3. Table 4-I lists the alternate viewing parameters. The parameters that differ 
from those of the Core System are described below. Note that four types of projection are distinguished, 
as opposed to two in the Core System. The parameter VIEWCEN is an absolute world coordinate position 
that is the center of projection for perspective projection and is the same as REF for parallel projections. 
A world coordinate vector, PREF, specifies view right (as opposed to view up) for perspective, orthographic, 
and elevation oblique projections. This means that is calculated as PREF-(PREF.~)n u = and IIPREF-(PREF.n)~II 
 v=uxn. For plan oblique projections PREF specifies the "preferred direction" explained in Section 4.4. 
In all cases, the component of PREF that is perpendicular to NORM is the significant part of PREF. (This 
is directly analogous to the treatment of U__P_ in the Core System.) The viewing parameters F and G 
are used only for oblique projections. The reader may recall that in an oblique projection receding lines 
are the projections of lines perpendicular to the projection plane. The parameter F is the foreshortening 
ratio of receding lines and G denotes the angle in the projection plane of receding lines measured  
Parameter Meaning VIEWCEN center of projection NORM view plane normal vd view distance from VIEWCEN 
 fd front distance from VIEWCEN bd back distance from VIEWCEN WCu,WC v U and V coordinates of window 
center WSu,WS v U and V window half-sizes WIDTH,HEIGHT,DEPTH NDC space VCx,VCy,VC z viewport center 
 VSx,VSy,VS z viewport half-sizes PREF view right or preferred direction G angle of receding lines 
F foreshortening ratio PROJ type: perspective, orthographic, plan oblique, or elevation oblique Table 
4-1. Alternate Viewing Parameters  counterclockwise from the projection plane direction determined from 
PREF. Finally, in the alternate viewing scheme, the window and viewport are specified by their centers 
and half-sizes. This simplifies setting up the transformation to clipping coordinates and the transformation 
from clipping coordinates to device coordinates. The next four subsections deal with the matrices for 
the four types of projections. 4.1 PERSPECTIVE PROJECTIONS  The view volume for perspective projection 
is specified relative to the center of projection, VIEWCEN. The sequence of matrices for perspective 
projections can be derived from the sequence on Section 3.1 by substituting for PR, REF, and the window 
edges. (4-I) -VIEWCEN ~-o o o 1 -~-o.o (4-3) 1 0.~ 0 0 0 1 vd 0 (4-4) 0 1 0 0 0" 1 0 (4-5) C 
u -WC v 1   °°i] 0 0 (4-6)  l! vi°° i] 0 I 0 0  The product of these transformations can be 
written as (4-I)(4-2)(4-3)(4-4)(4-5)(4-6). 4.2 ORTHOGRAPHIC PROJECTIONS  Orthographic projections are 
parallel projections in which the direction of projection is perpendicular to the view plane. This means 
the projection direction need not be specified explicitly since it is parallel to NORM. Also, as it turns 
out, view distance is irrelevant for these projections. We derive the relevant transformation by substituting 
into the matrix product (3-I)(3-3)(3-8)(3-9)(3-10). Since P_R is parallel to ~, matrix (3-9) reduces 
to the identity. This leaves the following two matrices in addition to the translation matrix, (4-I), 
and the coordinate change matrix, (4-2): ] (4-2) lw I (4-7) C u -WC v -vd ! WSv 0 (4-8) 0 I bd- fd 
vd- fd  0 bd- fd The third element of the fourth row of the product of (4-7) and (4-8) simplifies; 
the result can be factored as: (4-9) -WC u -WC v -fd   !1__o WS v (4-10) I 0 bd- f-----d 0 0 
 ° !1 The composite transformation is (4-I)(4-3)(4-9)(4-10). 4.3 ELEVATION OBLIQUE PROJECTIONS  For 
an elevation oblique projection, the receding lines are determined by two viewing parameters. The foreshortening 
ratio is F, and the angle of the receding lines measured counterclockwise from the positive U-axis is 
denoted G. Such a projection would be achieved in the Core System by specifying PR=(FcosG,FsinG,-l). 
Substituting into the matrices of Section 3.2, one obtains the product (4-1)(4-2)(4-7)(4-11)(4-8), where 
(4-II) is defined below. (This matrix can, in turn, be written as a product of three matrices, one that 
is a function of only G and the other two that are functions of only F.) lO 0 I O (4-11) FcosG FsinG 
I 0 0 0 4.4 PLAN OBLIQUE PROJECTIONS  What distinguishes a plan oblique from an elevation oblique 
projection is that receding lines must be oriented downwards on the display surface. This complicates 
the calculation of u and v, in that n, PREF, and G are involved. Let s be the unit vector in the view 
plane determined from PREF: PREF-(PREF.~)n s= The u and v directions must be determined so that receding 
lines, at an angle C from s, point downwards (along -~). Let t = sxn; s and ~ form a coordinate system 
for the view plane. In that system, -~ = ~cos G + ~sin G. From this, ~ is easily found, and then u = 
nxv. Given these values for ~ and ~ and the usual values for the other viewing parameters, one sequence 
of matrices for plan oblique projections is the same as for elevation oblique projections except for 
the shear matrix. The shear matrix is 0 I -F 0 I (4-12) 0 0 An alternate sequence of matrices that 
separates the parameter G fxom the parameters NORM and PREF and that avoids explicit calculation of and 
X is found by factoring the ~, X, ~ matrix (i.e., (4-2)) into the following two matrices: s t t t n 
t ~t] (4-13) -sinG -cosO 0 0 cosG -sinG 0 (4-14) 0 0 I 0 0 0 Thus, the total transformation fol 
plan oblique projections from the view volume to the normalized clipping region can be written as the 
product (4-I)(4-13)(4-14)(4-7)(4-12)(4-8). 4.5 ANALYSIS OF THE ALTERNATE VIEWING PARAMETERS The alternate 
viewing system described in this section has abandoned the unified approach taken by the Core System. 
The different types of projections, orthographic, perspective, elevation oblique, and plan oblique, are 
treated as different cases. As a result, the viewing parameters are specified in a manner that is more 
natural for most applications and the implementation of the viewing pipeline can be made more efficient. 
 The view volume for a perspective projection is defined relative to the eyepoint, i.e., the shape and 
size of the view volume is defined independently Of its location and orientation. A change in NORM or 
view right now has the expected effect; it gives the effect of the viewer turning his head. IIPREF-(PREF.n)nll 
 The specification of parallel projections are  also more natural than in the Core System. Oblique projections 
are specified in terms of the foreshortening ratio of the receding lines and the angle of the receding 
lines. In the Core System viewing scheme the programmer must calculate the direction of the projectors 
from these properties of the projected object. Orthographic projections are also made simpler; no projection 
direction need be specified. The proposed viewing scheme is more efficient both in terms of setting 
up the viewing pipeline and in terms of modification of any of its parameters. The increased efficiency 
in setting up the viewing pipeline is evidenced by the simplified matrices. The increased efficiency 
in modification of a parameter is also evidenced by the matrices. Most of the matrices are independent 
of each other and almost every viewing parameter can be modified independently. 5. CONCLUSIONS  A general 
purpose graphics package should support all the planar geometric projections through its viewing capabilities. 
The design of the viewing parameters for such a graphics package can take several approaches. One approach 
is based on an all-encompassing model of viewing. This approach was taken in designing the Core System. 
 A second approach is based on an analysis of how different kinds of planar geometric projections are 
used. This approach results in increased naturalness of use and increased efficiency in several areas. 
 One particular efficiency area that has not previously received much attention is viewing support software 
for a high performance graphics device. It is shown above that by a careful choice of viewing parameters, 
the viewing transformation can be written as a product of relatively simple matrices with each viewing 
parameter affecting only one or two matrices. This means the viewing support software for modification 
can be concise and efficient. It simply alters the one or two affected transformations and depends on 
the graphics transformation hardware to recompute the composite transformation. Although the naturalness 
of use could be achieved by implementing the alternate viewing parameters on top of the Core System viewing 
scheme, the efficiency gains discussed in this paper could not be realized. In the scheme proposed in 
this paper, a fortunate conjunction of efficiency and naturalness of use has been accomplished by abandoning 
an all-encompassing model of viewing. REFERENCES  [Carlbom 78] Carlbom, I. and J. Paciorek, "Planar 
Geometric Projections and Viewing Transformations," Computing Surveys I0, 4 (December 1978) pp. 465-502. 
 [Carlbom 80] Carlbom, I. B., System Archltecture for High-Performance Vector Graphics, Ph.D. Thesis, 
Brown University, Providence, R.I. (1980). [GSPC 77] Graphics Standards Planning Committee, "General 
Methodology and Proposed Standard," Computer Graphics i i, 3 (Fall 1977) pp. II-I - 11-117.  [GSPC 
79] Graphics Standards Planning Committee, "General Methodology and the Proposed Core System (revised)," 
Computer Graphics 13, 3 (August 1979) pp. II-i -11-179. [Mallgren 78] Mallgren, W.R. and A.C. Shaw, 
"Graphical Transformations and Hierarchic Picture Structures," Computer Graphics and Image Processing 
8 (1978) 237-258. [Michener 78] Michener, J. C. and J. D. Foley, "Some Major Issues in the Design of 
the Core Graphics System," Computing Surveys I_~, 4 (December 1978) 445-463.  [Newman 73] Newman, W.M., 
"Instance Rectangles and Picture Structures," Proc. Conference on Computer Graphics, Pattern Recognition, 
an___d Data Structures, University of California, Los Angeles (May, 1975) 297-301. [Newman 73a] Newman, 
W. M. and R. F. Sproull, Principles of Interactive Computer Graphics, McGraw-frill Book Company (1973). 
 [Puk 77] Puk, R. F., "General Clipping on an Oblique Viewing Frustrum," Computer Graphics I I, 2 (Summer 
1977) pp. 229-235. [Warner 78] Warner, J. A., M. A. Polisher, and R. N. Kaplow, "DIGRAF--A FORTRAN Implementation 
of the Proposed GSPC Standard," Computer Graphics 12, 3 (August 1978) pp. 301-307.  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1980</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>807500</article_id>
		<sort_key>246</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-1980</article_publication_date>
		<seq_no>41</seq_no>
		<title><![CDATA[Graphics standards status report (Panel Session)]]></title>
		<page_from>246</page_from>
		<doi_number>10.1145/800250.807500</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=807500</url>
		<abstract>
			<par><![CDATA[<p>Two status reports will be presented. The first report describes the activities of Working Group 2 (Computer Graphics) of the International Standards Organization Subcommittee on Programming Languages (ISO TC97/SC5/WG2). Actions taken in recent WG2 meetings in October, 1979, and June, 1980, will be covered. Significant proposals being examined by WG2 include GKS, a German-designed 2D graphics package derived from the SIGGRAPH GSPC Core System proposals, and Videotex, a Canadian-designed protocol text communication that includes geometrically coded picture images.</p> <p>The second status report describes the recent activities of the American National Standards Institute (ANSI) Technical Committee X3H3, Computer Graphics. This includes the committee status, international activities, key technical issues, and future plans.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.6</cat_node>
				<descriptor>Standards</descriptor>
				<type>S</type>
			</primary_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10011007.10011006.10011050.10011017</concept_id>
				<concept_desc>CCS->Software and its engineering->Software notations and tools->Context specific languages->Domain specific languages</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010919.10010177</concept_id>
				<concept_desc>CCS->Computing methodologies->Distributed computing methodologies->Distributed programming languages</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10011007.10011006.10011050.10011017</concept_id>
				<concept_desc>CCS->Software and its engineering->Software notations and tools->Context specific languages->Domain specific languages</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010919.10010177</concept_id>
				<concept_desc>CCS->Computing methodologies->Distributed computing methodologies->Distributed programming languages</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Standardization</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P223671</person_id>
				<author_profile_id><![CDATA[81100626262]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Peter]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bono]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Naval Underwater Systems Center]]></affiliation>
				<role><![CDATA[Chairman]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P332770</person_id>
				<author_profile_id><![CDATA[81100450895]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[P.]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[tenHagen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39074653</person_id>
				<author_profile_id><![CDATA[81339494145]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Janet]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Chin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P332688</person_id>
				<author_profile_id><![CDATA[81347487511]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[P.]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bono]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP31075198</person_id>
				<author_profile_id><![CDATA[81100063773]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[J.]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Encarnacao]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[TH Darmstadt]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP31088966</person_id>
				<author_profile_id><![CDATA[81100509207]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[J.]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Michener]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Intermetrics, Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 GRAPHICS STANDARDS STATUS REPORT Panel Introduction Two status reports will be presented. The first 
report describes the activities of Working Group 2 (Computer Graphics) of the International Standards 
Organization Subcommittee on Programming Languages (ISO TC97/SC5/WG2). Actions taken in recent WG2 meetings 
in October, 1979, and June, 1980, will be covered. Significant proposals being examined by WG2 include 
GKS, a German-designed 2D graphics package derived from the SIGGRAPH GSPC Core System proposals, and 
Videotex, a Canadian-designed protocol text communication that includes geometrically coded picture images. 
The second status report describes the recent activities of the American National Standards Institute 
(ANSI) Technical Committee X3H3, Computer Graphics. This includes the committee status, international 
activities, key technical issues, and future plans. Following the two reports, the presenters will be 
joined in a panel by others active in the international standards effort. All panelists will be available 
to answer questions on either the technical or administrative aspects of the graphics standardization 
process. Comments from industry concerning ANSI X3H3 should be addressed to: Mr. Thomas Powers Digital 
Equipment Corporation 146 Main Street MLI-2/H26 Maynard, MA 01754 (617) 493-2704 Chairman: Peter Bono, 
Naval Underwater Systems center Panelists: P. tenHagen, Mathematical Center, Amsterdam (presenting ISO 
TC97/SC5/WG2) Janet Chin, Tymshare, Inc. (presenting Status of American National Standards Institute 
Committee X3H3) Other Panelists: P. Bono, Naval Underwater Systems Center J. Encarnacao, TH Darmstadt 
J. Michener, Intermetrics, Inc. Permission to copy without fee all or part of this material is granted 
provided that the copies are not made or distributed for direct commercial advantage, the A(~I copyright 
notice and the title of the publication and its date appear, and notice is given that copying is by permission 
of the Association for Computing Machinery. To copy otherwise, or to republish, requires a fee and/or 
specific permission. ~980 ACM 0-89791-021-4/80/0700-246 $00.75 246 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1980</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>807501</article_id>
		<sort_key>247</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-1980</article_publication_date>
		<seq_no>42</seq_no>
		<title><![CDATA[Computer-assisted chart making from the graphic designer's perspective]]></title>
		<page_from>247</page_from>
		<page_to>253</page_to>
		<doi_number>10.1145/800250.807501</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=807501</url>
		<abstract>
			<par><![CDATA[<p>It is important to improve the aesthetics and effectiveness of information graphics through greater awareness of graphic design as a visual communication discipline. Chart making is one subject area in which graphic design awareness can be enhanced. Basic design principles are reviewed which are relevant to creating better charts. Examples demonstrate improvements.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Charting]]></kw>
			<kw><![CDATA[Computer graphics]]></kw>
			<kw><![CDATA[Display design principles]]></kw>
			<kw><![CDATA[Effective data displays]]></kw>
			<kw><![CDATA[Graphic design]]></kw>
			<kw><![CDATA[Human factors]]></kw>
			<kw><![CDATA[Visual data encoding]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.3</cat_node>
				<descriptor>Line and curve generation</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>H.1.2</cat_node>
				<descriptor>Human factors</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.6</cat_node>
				<descriptor>Interaction techniques</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>E.2</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10010391</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Graphics input devices</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002951.10003152.10003161.10003162</concept_id>
				<concept_desc>CCS->Information systems->Information storage systems->Record storage systems->Record storage alternatives</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010399</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Parametric curve and surface models</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Human Factors</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP39049611</person_id>
				<author_profile_id><![CDATA[81100583778]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Aaron]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Marcus]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Department of Computer Science and Applied Mathematics, Lawrence Berkeley Laboratory, Berkeley, California]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Benson, Bill. "CHART: User's Guide." Lawrence Berkeley Laboratory, University of California, November 1976.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Bertin, Jacques. Semiologie Graphique. Gauthier-Villars/Mouton, Paris, 1973.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Blackburn, Bruce. Design Standards manuals. Federal Design Library, National Endowment for the Arts, Washington, D.C. 1977.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Eco, Umberto, Theory of Semiotics. Indiana University Press, 1976.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Foley, James D. and Victor L. Wallace. "The Art of Natural Graphic Man-Machine Conversations," IEEE, . 62: 4, (April 1974), 464-471]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA["Genigraphics", General Electric Company, Genigraphic Center 9, San Francisco, California, 1979.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Gerstner, Karl, Designing Programmes, Hastings House Publishers, New York, 1968.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Herdeg, Walter, ed. Graphis: Diagrams. Graphis Press, Zurich, 1975.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Hoffman, Armin, Graphic Design Manual, Hastings House Publishers, New York, 1965.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Hyder, Darrell. "Swiss Typography Today," Penrose Annual 63, edited by Herbert Spencer, Hastings House Publishers, New York, 1968.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Lockwood, Arthur. Diagrams. Watson-Guptill, New York, 1969.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Marcus, Aaron, "Modern Swiss Posters," The Art Museum, Princeton University, 1971.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Marcus, Aaron. "At The Edge of Meaning," Visible Language, 11: 2, 1977, 4-20.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Marcus, Aaron. "Routes, Loops, Transfers, and Dead-Ends," Print, 32: 2, 1978, 49-54.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_obj_id>574884</ref_obj_id>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Martin, James. Design of Man-Computer Dialogues. Prentice-Hall, Inc., Englewood Cliffs, NJ, 1973.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Meyers, Cecil H., Handbook of Basic Graphs: A Modern Approach. Dickenson Publishing Co., Inc., Belmont, California, 1970.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807429</ref_obj_id>
				<ref_obj_pid>965103</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Morse, Alan. "Some Principles for the Effective Display of Data." Computer graphics, 13: 2, 1979, 94-101.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[Muller-Brockman, Joseph. The Graphic Artist and His Design Problems, Hastings House Publishers, New York, 1968.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[Muller-Brockman, Joseph. A History of Visual Communication, Hastings House Publishers, New York, 1971.]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
			<ref>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA["New Ways to View World Problems," Perspectives, Vol. 1, No. 1, 1979, pp. 15-22.]]></ref_text>
				<ref_id>20</ref_id>
			</ref>
			<ref>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[Robinson, Arthur H., Randall Sale and Joel Morrison, Elements of Cartography, Fourth edition, New York, 1978.]]></ref_text>
				<ref_id>21</ref_id>
			</ref>
			<ref>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[Ruder, Emil. Typographie, Hastings House Publishers, New York, 1967.]]></ref_text>
				<ref_id>22</ref_id>
			</ref>
			<ref>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[Schmid, Calvin, Handbook of Graphic Presentation, First Edition, The Ronald Press Co., New York, 1954; second Edition, John Wiley and Sons, New York, 1979.]]></ref_text>
				<ref_id>23</ref_id>
			</ref>
			<ref>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[Spear, Mary Eleanor. Practical Charting Techniques. McGraw-Hill Book Co., New York, 1969.]]></ref_text>
				<ref_id>24</ref_id>
			</ref>
			<ref>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[Stevens, Carol, "Swiss Design Is Alive {etc.}," Print, Vol. 25, No. 1, January-February 1971, pp. 37-49.]]></ref_text>
				<ref_id>25</ref_id>
			</ref>
			<ref>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[Sventek, Virginia A. CHART: Workbook II. Socio-Economic-Environmental-Demographic Information System, Computer Science and Applied Mathematics Department, Publ. No., LBL-7277, Lawrence Berkeley Laboratory, University of California, March 1978.]]></ref_text>
				<ref_id>26</ref_id>
			</ref>
			<ref>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA["Tel-A-Graf", "Display", Integrated Software Systems Corp., San Diego, California, 1979.]]></ref_text>
				<ref_id>27</ref_id>
			</ref>
			<ref>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[Tschichold, Jan. Asymmetric Typography, translated by Ruari MacLean, Rheinhold, New York, 1967.]]></ref_text>
				<ref_id>28</ref_id>
			</ref>
			<ref>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[Vignelli, Massimo. Grids: Their Meaning and Use for Federal Designers. Federal Design Library. National Endowment for the Arts, Washington, D.C., 1977.]]></ref_text>
				<ref_id>29</ref_id>
			</ref>
			<ref>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[Von Grunigen, Swiss Poster Art, Verlag der Visualis, Zurich, 1968.]]></ref_text>
				<ref_id>30</ref_id>
			</ref>
			<ref>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA["Wertheimer, Max. Laws of Organization in Perceptual Forms," in Ellis Willis D., ed. A Source Book of Gestalt Psychology. Harcourt Brace, New York, 1939.]]></ref_text>
				<ref_id>31</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 COMPUTER-ASSISTED CHART MAKING FROM THE GRAPHIC DESIGNER'S PERSPECTIVE* Aaron Marcus Department of 
Computer Science and Applied Mathematics Lawrence Berkeley Laboratory Berkeley, California 94720 Abstract 
 It is important to improve the aesthetics and effectiveness of information graphics through greater 
awareness of graphic design as a visual communication discip- line. Chart making is one subject area 
in which graphic design awareness can be enhanced. Basic design principles are reviewed which are relevant 
to creating better charts. Examples demonstrate improvements. CR Categories: 3.19, 3.29, 3.39, 3.49, 
3.59, 6.69 3.79, 3.89, 3.9, 8.1, 8.2 Key Words and Phrases: charting, computer graphics, graphic design, 
effective data displays, display design principles, visual data encoding, human factors. Introduction 
 After two decades of development from the simplest typographic display equipment, computer graphics 
is entering a decade of visual sophistication. In the "Informa- tion Age" of the 1960's and 1970's, 
 society witnessed a proliferation of data gathering, processing, and distribution techniques. As the 
"Image of Information Age" evolves, researchers, policy makers, and the general public are realizing 
more clearly that human beings cannot effec- tively utilize large quantities of computer-processed 
information for making decisions, for becoming informed, or merely for perusing data. Technical achievements 
appear to surpass the actual quality of imagery in terms of conveying information through innovative 
forms and exhibiting sensitivity about aesthetic issues in communicating information graph- ically. 
There is clearly a need for con- verting vast amounts of numerically stored data into spatial (sometimes 
geographi- cal), temporal, and colorful forms so that significant patterns emerge from informa- tional 
graphics, particularly, charts, maps, and diagrams. *This work was supported by the Applied Mathematical 
Sciences Research Program of the Office of Energy Research, Department of Energy under contract W-7405-ENG-48. 
 Permission to copy without fee all or part of this material is granted provided that the copies are 
not made or distributed for direct commercial advantage, the ACM copyright notice and ~980 ACM 0-8979]-02]-4/80/0700-247 
$00.75 Many computer graphics advertisements and much professional literature exhibit charts, maps, 
or other diagrams. If one examines these carefully with an eye trained in visual communication, one notices 
oversights or errors in visual thinking which seem clearly unintended, such as color combinations that 
inhibit legibility, poor typographic hierarchies, or confused composition. The situation is often not 
the result of deficient equip- ment but simply the result of computer graphics displays created by persons 
with relatively little training in graphic design, i.e., ~n typography, color, compo- sition, non-alphanumeric 
symbolism, pho- tography, and reproduction processes. The ignorance of the computer graphics world about 
the world of graphic design will probably disappear in this decade. Computer graphics professionals will 
seek the principles of effective design of informational graphics. Unfortunately, there is far less research 
into legibil- ity, readability, and aesthetics of effec- tive charts, diagrams, and to some extent maps, 
than there is in terms of conven- tional linear texts for books, magazines, and newspapers. There will 
probably be increasing pressure to convey information through non-linear spatial arrays of sym- bols. 
[i] In this environment, a mutual interchange of expertise and interests between computer graphics and 
graphic design may generate more effective research into information graphics and eventually may yield 
higher quality displays, where quality signifies more than technical achievement. An intention of this 
paper is to begin to bridge the gap between computer graphics and graphic design by outlining some prin- 
ciples of effective graphic display of information, beyond the general ideas men- tioned in Morse [1979]. 
There is no defin- itive manual on informational graphics. There are some texts on the design of charts, 
diagrams, and maps [2] but these suffer from several limitations: they are often out of date in terms 
of media and stylistic qualities of imagery; they are sometimes too casual in presenting specif- ications 
for information display; and they ~he title of the publication ~d its date appear, and notice is given 
that copying is by permission of the Association for 247 Computing Machinery. To copy othe~Ise, or to 
republish, requires a fee and/or specific pe~isslon. are occasionally in disagreement with regard to 
specific recommendations [3]. One unusual book edited by Herdeg [1979] entitled Graphis: Diagrams offers 
a star- tling array of possibilities for chartmak- ing and diagramming at the highest level of artistic 
quality. However, it offers very limited information concerning the context from which each diagram 
emerged, how other similar diagrams might be designed, and how successful the diagrams were. Nevertheless, 
it demonstrates the levels of visual sophistication which computer-assisted informational graphics might 
achieve with the addition of the graphic design dimension. This present article emphasizes aspects 
of computer graphics displays that enhance the ability of human beings to recognize, comprehend, and 
remember information. The discussion also concerns factors that con- tribute to the user's enjoyment 
or interest in examining data. These are not insignificant aspects: the pattern recog- nition equipment 
for informational graph- ics is the human visual system, not an electro-mechanical device. The difference 
between these two is equivalent to a dis- tinction between readability (qualitative phenomena promoting 
alertness, receptive- ness, and interest) and legibility (quan- titative phenomena aiding discernment 
of discrete units or segments of informa- tion). From a graphic designer's point of view (the author's), 
the discussion in this article is meant to focus not on questions of style or taste but upon fun- damental 
problems of visual thinking. For the most part the comments are intended to enhance ideas about default 
displays of computer-assisted charts, one of the most common means of displaying data on a com- puter 
graphics system. Principles of Visual Communication Effective visual communication is based upon wise 
application of principles of visual organization and quantitative lim- its to perception. These principles 
are fundamental to any discussion of symbols in space, of their figure-field relation- ships, movement, 
implied depth, or over- all compositional scheme. In the early twentieth century, Gestalt psychologists 
isolated several principles of visual organization. During the inter- vening half-century these principals 
have been rephrased and re-emphasized, but they remain essential categories of great use- fulness. One 
researcher who formulated a careful list was Wertheimer [1939]. His set of principles include the following. 
Each is illustrated with simple visualiza- tions, for which correlates in charting can be found. One 
such example is given for the first entry. Proximity: objects seem to belong together because of their 
location in space. An example from charting would be the face that lines of closely spaced type in the 
same size appear to group together. The natural grouping unit is ab, not bc. ab cd ef gh .. .. .. .. 
 Similarity: objects seem to belong together because of their visual proper- ties such as shape, size, 
color, orienta- tion, and texture. The natural grouping unit is ab, not bc. a b c d e f g h X X . 
 X X Note that two principles, e.g., similarity and proximity, may be combined into pro- structural 
and contra-structural grouping. x x x x pro x x . . x x contra Common fate: Changes in an already 
esta- blished set of objects conform to grouping already evident. given pro ....... contra Objective 
set: within a spectrum of states of an object, condition, or group- ing, certain ones seem stronger or 
more objectively basic. For example, a nearly-right angle is seen as a right angle. Direction: objects 
are seen to belong together because of their unified direc- tion even in contrast to proximity. / / 
 . .'. is seen as ~ not as Closure: objects are usually seen as self-enclosed simple wholes. ~ is 
seen as ~ not as ~~ is seen as O not as C÷) Although the human visual system is an enormously sophisticated 
pattern recogni- tion device, it does have clear limits to its perception of visual phenomena. Of 
relevance in this discussion are various 248 changes in color, line weight, and other elements typical 
of chart-like information displays. These factors are discussed in Martin [1973, Chapter 21] and also 
by Foley and Wallace [1974 p. 464] whose com- ments follow: "The perception of structure among objects 
can be enhanced by using different line types (solid, dotted, dot-dash), widths, intensity levels, or 
geometric shapes. These all help make a display more meaningful .... [Other] references provide more 
specific information concerning the usefulness of various distingui3hing features, [and they are cited] 
in Table I. For each technique, the number of easily distinguished codes is listed. They are listed in 
approximately decreasing effec- tiveness, with color providing the most useful distinction. It is relatively 
expensive, however, and unacceptable for the color-blind [viewer]. The other tech- niques are often available 
inexpen- sively ...." Table I ........................................... Coding Method Maximum Number 
of Codes for Essentially Error-Free Recognition by Normal Individuals ........................................... 
 Color 6 Geometric Shapes I0 Line width 2 Line type 5 Intensities 2 ........................................... 
 Sources: J. Martin, Desig__n of Man- Computer Dialogues, Englewood Cliffs, N.J. Prentice-Hall, 1973; 
and J. Barmack and H. Sinaiko, "Human Factors Problems in Computer-Generated Graphic Displays," Inst. 
for Defense Analysis Study, DFSTI ASTIA Doc. AD636170, vol. 5-234, April 1966. ........................................... 
 The Graphic Design Perspective These principles of visual organization and limits of perception form 
a conceptual basis for the grid-oriented or so-called Swiss approach to graphic design. It is an approach 
eminently suited to informa- tion display in which many complex rela- tionships must be distinguished 
carefully and clearly. The Swiss approach to graphic design derives from the German Bauhaus and the 
Russian Constructivist artistic trends of the early twentienth century which emphasized functionalism, 
new technology, and rationalism. The Swiss approach to graphic communication as seen in posters, books, 
magazines, diagrams, etc., emerged in a clear form during the 1950s and early 1960s. Swiss graphics had 
a world-wide impact upon graphic design curricula. It also took hold in offices of major inter- national 
corporations which adopted the principles for the business world because they could be relatively clearly 
and pre- cisely formulated. This approach to graphic design could account for the myriad of visual forms 
which any large corporate entity or institution wanted to include in its visual identification. Presently 
the National Endowment for the Arts' Design Excellence Program is seeking to develop the use of the 
Swiss approach in governmental publications [Blackburn, 1977; Vignelli, 1977]. It is not without significance 
that Swiss graphic design is called programmatic design [Gerstner, 1968]. In theory a sig- nificant number 
of quantifiable attributes of the finished visual design can be traced to a list of needs with priorities. 
In effect syntactic conditions can be clearly traced back to semantic relation- ships, to use the terminology 
of visual semiotics, the science of signs and mean- ing [Eco, 1977]. The general visual characteristics 
of Swiss-oriented graphic design principles can be categorized in the following list [Marcus, 1971; 
adapted from Hyder, 1970]. While originally intended for a descrip- tion of printed poster designs of 
the 1960s, these observations are now appropriate to mention as principles which are helpful for the 
display of information through computer graphics sys- tems, whether printed on paper or appear- ing on 
a display screen. Sans Serif Type Styles: Following the Precedent of the Bauhaus and the Constructivist 
typographers, the Swiss designers rejected the traditional serif letters in favor of the more simplified 
 sans serif letterforms (Fig. I). In 1957 Swiss designers introduced two new sans serif typefaces, Helvetica 
and Univers (Fig. i) . Because of their fre- quent use in graphic design work influ- enced by the Swiss 
approach, Helvetica and Univers have become strongly associated with the Swiss typographic style. Both 
typefaces retain the 'machined' look and the uniform letter weight that the rougher versions of the 1930s 
introduced, but add a greater homogeneity and a new elegance to the curves used in their letterforms. 
These two faces remain standard typefaces for modern, clear typographic displays. Simplified Imagery: 
The standardization of format for Swiss posters prevented the poster designer from relying on the size 
 of the poster to gain impact. Instead, the designer had to work from 'effective- ness at a distance 
which belongs to the mural rather than [the easel] painting.' [Von Grunigen, 1968, p. T-45]. This con- 
 tributed greatly to an early universal 249 reduction in the amount of primary text presented on a 
poster as well as the use of one essential image for immediate recognition. The typographic emphasis 
of the Swiss poster asserted both the usefulness and the sufficiency of typography alone to attract 
the eye of the viewer and to com- municate, through the basic text of the poster, other abstract symbolic 
meaning. For such strict advocates of typographic design for posters as Ruder [1967], imagery was 
usually limited to typography. When illustration was included it was often geometrical in form. When 
photogra- phy was introduced it was usually treated in scale (e.g., greatly enlarged size) or in tonal 
emphasis (e.g., high contrast photographs) so as to immediately simplify the image. In conformity with 
the choice of modern typefaces, the imagery of Swiss design emphasized reduced complexity, flat surfaces, 
and images that were technically transformed, without traces of manual operations. 'Hand-drawn' images 
were generally excluded. Open Spaces: Within the simplified image of the Swiss poster, carefully used 
nega- tive spaces devoid of both text and illus- tration establish a geometric subdivision of the poster's 
field or provide emphasis for the visual elements within the poster. Given a fixed format, a limited 
set of typographic elements, and often no other imagery than type, the designer must rely essentially 
upon spatial composition to make the informative aspects of the poster clear and to provide the arena 
for provo- cative aesthetic relationships. Consistency of Design: No mixing of typefaces within a poster 
is one immediate result of a desire for simplicity. On the other hand, a variation within one type family 
(bold, medium, or light; condensed, regular, or expanded letterforms) can occur because of the uniform 
aesthetic features within a type family (Fig. I). The number of these changes (in size, boldness, or 
proportion of the type) is usually limited to two or three. The pro- portion of these changes is usually 
a sim- ple and dramatic factor. For example, for the proportional size of primary to secon- dary type, 
ratios of 2:1 or 3:1 are com- monly found. Strong reliance on a grid of implied lines that organizes 
and controls the position- ing of typographic and illustrative ele- ments also typifies the Swiss approach. 
The grid limits the horizontal and verti- cal intervals and establishes a series of harmonic visual relationships 
that make coherent the entire field of the poster. The grid is also related to the propor- tions of 
the entire visual field. In other words the visual composition con- forms to the available display field 
con- ditions. A rectangle of the proportions of one: square root of two (1:1.414) is the basic Swiss 
poster format and also the established norm for the European interna- tional paper size system. Within 
this rectangle lies the primary form of the generating square (Fig. I) which is often used as an immediate 
source of asymmetric division of the field. Primary visual elements can be made to express directly 
 or to imply this basic geometric relation- ship. For example, the location of a pri- mary lext line 
might align with a major grid line. Such alignments help to integrate the composition. In some cases, 
 such as in the work of Muller-Brockman, [1968] grids are created that can be used for a series of 
similar compositions. The grid provides many possibilities for the positioning of visual elements, 
allowing strongly differing variations to occur that are nevertheless clearly related visually. This 
approach is especially appropriate for chart making because typi- cally many nearly identical charts 
are produced by any one system. An Example of Chart Development with Graphic Design Assistance The 
examples of Figures 2, 3, and 4 demon- strate the effect of applying graphic design considerations such 
as those prin- ciples mentioned above on a typical graphic display (Tektronix and Varian hard copy devices). 
The examples are intended to demonstrate improvements under the most wide-spread conditions of-black-and-white, 
static images with simple lettering. The displays were all created by CHART, an interactive chart making 
program developed at Lawrence Berkeley Laboratory's Depart- ment of Computer Science and Applied Mathematics 
for the SEEDIS project (Socio-Economic-Environmental-Demographic Information System) [Benson, 1976]. 
The program is similar to other commercial and in-house systems now in use in some businesses, government 
agencies, and research laboratories. An attempt was made to create a superior, more effective image using 
standard equipment and simple graphic techniques which would become default options for CHART. This section 
discusses some of the changes that were made in the charts and the reasons for these chanqes. By presenting 
this comparison, it will be possible to demon- strate more clearly the meaning and vali- dity of the 
graphic design principles men- tioned earlier. An exemplary chart (Fig. 2) exhibits typi- cal errors 
of basic design principles such as those discussed earlier. These over- sights may be grouped into 
three general categories: syntax, semantics, and prag- matics. These categories of visual semi- otics 
will be discussed below in the order mentioned. The definitions as presented here are brief, convenient 
ones suited for 250 the context of chart making. For more extensive discussion, see, for example, Marcus 
[1978] and Eco [1976]. Visual syntax refers to qualities of the arrangements of letters, lines, and 
other symbols. One obvious characteristic of the chart information display shown in Figure 2 is that 
it has no visual limit, i.e., border. The total composition is vaguely organized around a central space 
of information, the chart itself. Another distinctive quality is that titles, out- lines, chart lines, 
and alphanumerical labels are all relatively equal in weight, i.e., no particular item stands out from 
 the others. In relation to the underlying grid of the chart space, the location of labels and the 
quality of lines is confus- ing. Note, for example, that horizontal and vertical lines are variously 
solid or dash-dot. The special quality of the 'PROJECTED NORMAL' chart line is not clear. Sometimes 
labels are centered on visible or implied lines and sometimes they are aligned along an edge. In the 
revised chart style of Figure 3, the outline of the total space is expli- citly stated so that all figure-field 
relationships are clear. The overall pro- portion of the chart has become decisive: 1:1.5. There is 
also an attempt to iso- late the central chart space itself by gray levels from the surrounding informa- 
tion support space. Greater organization of all typography is apparent. The titles and labels are more 
strongly related to the basic charting area by alignment along an edge rather than a more vague central 
axis. There is greater differentiation (i.e., visual hierarchy) in use of line. Lines of type vary 
in size, and chart lines vary in thickness. Even within the gray area, a line of differentiation dis- 
 tinguishes past fact from future predic- tion. The chart 'whiskers' have been removed from the central 
chart space in order to reduce the clutter within the primary information space. Wherever pos- sible, 
simplification and stronger organi- zation have been stressed by coherent application of the principles 
of visual organization mentioned above. Similar things are identified in similar ways, and positioned 
to make visual hierarchy clear. Visual semantics refers to the relation- ship between signs/symbols 
in the chart and the information to which the chart refers. One of the outstanding semantic features 
of the original chart of Figure 2 is the very high precision that is implied by the very thin chart 
lines. It is not necessarily the case that information is so precisely known. Another peculiar feature 
is that the chart lines are very precise, but the labels are not. This sug- gests that there is a discrepancy 
between the simple graphing of data and the intended reading of the overall trend or significance 
of the chart. In the suggested revisions of Figures 3 and 4, several alternatives are suggested. Instead 
of individual error flags for data points, an envelope of relative certainty is created in Figure 4 by 
simply changing the background gray levels. In Figure 3 the lines themselves are now more clearly distinguished 
from grid lines, and the thickness of lines is more clearly related to the precision suggested by the 
labels. In general, visual emphasis through larger type, heavier lines, and gray levels is given to the 
more important items of the chart. Even this simple semantic rela- tionship is sometimes inadvertently 
violated by many chart makers. Visual pragmatics refers to the technical conditions and characteristics 
of producing charts, and to the human factors related to their being read by human beings, not machines. 
The chart of Figure 2 was produced with clear limitations in variation of line weight and texture, typographic 
style (e.g., no lowercase letters and unvarying character widths) and gray levels. These limitations 
obtain in many commercial display systems, although it is to be expected that future equipment will be 
more sensitive to the value of graphic variables. The other kind of pragmatic factor is exemplified 
by the conditions of Figure 2 in which the 'PRO- JECTED NORMAL' line is very hard to dis- tinguish between 
grid lines and boundary outlines. Similarly, in terms of titling and labels, key words of the title 
are difficult to determine from the titling cluster. The changes which have been made in Figures 3 and 
4 are intended to demon- strate that with even limited resources it is possible to make significant improve- 
ments in the appearance of information. These changes result in making the chart more readable as well 
as legible. Varia- tions in typography through grouping, boldness, and size change have been used to 
make primary key words and phrases more identifiable. The chart 'invites' a viewer to examine information 
and is at the same time responsible to its content. One other pragmatic aspect may not be readily visible 
to a casual viewer examin- ing such displays: the problem of repro- ducing and of transfering the image 
of data for presentation as article illustra- tion, overhead projection image, slide lecture image or 
poster seminar illustra- tion. Such transformations are often done casually even though the images are 
not well designed for such charges. The scale of typography, the chart lines, and the general visual 
hierarchy exhibited in the original chart Figure 2 are not suited for most of the above uses except as 
a publi- cation image of approximately original size (about 15 cm x 15 cm) Any significant reduction 
in size of the image might require its typography and even its lines to be redrawn by hand by a trained 
techni- cal graphics person, thereby reducing the 251 cost-effectiveness of the computer- assisted 
chart making. Even conventional xerographic copying of the original image would very likely make labels, 
footnotes, even primary titles illegible. In some cases crucial information could easily be lost. 
 The revised images are intended to be more useful as slide and poster images. The border proportion 
has been selected to easily accommodate typical paper and film proportions. In addition, Figures 3 and 
4 are now designed to allow significant change in scale without harm: they can be enlarged to become 
primary visual images for an exhibit or can be reduced to become secondary, clearly articulated evidence 
for an article. At some future time, a skillful algorithm in a truly well-designed computer graphics 
system might be able to accept input con- cerning the eventual use of the image (e.g. as a xerographic 
print, a 4-color high-quality lithographic image, or as a lecture slide image) and automatically compensate 
for necessary changes of typesize, line weight, texture, and conversion from/to color/black and white, 
etc. Conclusion The above discussion has introduced some basic graphic design principles which are 
 relevant to the display of information graphics. The example shown presented a modest set of improvements 
for the crea- tion, distribution, and consumption of information through computer-assisted chart making. 
 The initial efforts hae focused specifi- cally on types of hard copy display equip- ment which have 
a limited graphic flexi- bility. Further developments need to be made to design better coordination 
of typography, line weight and texture, gray level, and symbol use among line charts, bar charts, 
pie charts, and maps. Wher- ever more sophisticated typography, gray values, color, and symbol presentation 
are functioning, even greater benefits can be expected from applying awareness of infor- mational 
graphic design principles to the technical capabilities of computer graph- ics display equipment. It 
is hoped that this presentation will encourage greater attention on the part of the computer graphics 
community to the potential con- tribution of graphic design to more effec- tive visual communication 
of information. Acknowledgements The author wishes to thank Mr. William Benson of the Computer Science 
and Applied Mathematics Department at Lawrence Berke- ley Laboratory for his assistance in re- programming 
CHART to create and display the examples shown with the text. Footnotes i. For a discussion of trends 
in typogra- phy and graphic design in the age of com- puter graphics, video and film see Marcus [1977]. 
 2. For texts specifically on the design of charts, diagrams, and maps, see Schmid  [1979], Spear [1978], 
Meyers [1970] and Robinson [1978].  3. Spear [1969,p.163] advocates bar  charts with spacing between 
columns of one-half the width of columns, while Meyers [1970, p.96] suggests that for a number of bars 
in a relatively small rec- tangle, a space of one-third is appropriate. References Benson, Bill. "CHART: 
User's Guide." Lawrence Berkeley Laboratory, University of California, November 1976. Bertin, Jacques. 
Semiologie Graphique. Gauthier-Villars/Mouton, Paris, 1973. Blackburn, Bruce. De_six Standards manu- 
 als. Federal Design Library, National Endowment for the Arts, Washington, D.C. 1977. Eco, Umberto, 
Theory of Semiotics. Indi- ana University Press, 1976. Foley, James D. and Victor L. Wallace. "The 
Art of Natural Graphic Man-Machine Conversations," IEEE, . 62: 4, (April 1974), 464-471 "Genigraphics", 
General Electric Company, Genigraphic Center 9, San Francisco, Cali- fornia, 1979. Gerstner, Karl, Designing 
Programmes, Hastings House Publishers, New York, 1968. Herdeg, Walter, ed. Graphis: Diagrams. Graphis 
Press, Zurich, 1975. Hoffman, Armin, Graphic Design Manual, Hastings House Publishers, New York, 1965. 
 Hyder, Darrell. "Swiss Typography Today," Penrose Annual 63, edited by Herbert Spencer, ~E-~ngs House 
Publishers, New York, 1968. Lockwood, Arthur. Diagrams. Watson- Guptill, New York, 1969. Marcus, Aaron, 
"Modern Swiss Posters," The Art Museum, Princeton University, 1971. Marcus, Aaron. "At The Edge of Meaning," 
visible Language, ii: 2, 1977, 4-20. Marcus, Aaron. "Routes, Loops, Transfers, and Dead-Ends," Print, 
32: 2, 1978, 49-54. 252 Martin, James. Design of Man-Computer Dialogues. Prentice-Hall, Inc., Englewood 
Cliffs, NJ, 1973. Meyers, Cecil H., Handbook of Basic Graphs: A Modern Approach. Dickenson Pub- fishing 
Co., Inc., Belmont, California, 1970. Morse, Alan. "Some Principles for the Effective Display of Data." 
Computer graphics, 13: 2, 1979, 94-101. Muller-Brockman, Joseph. The Graphic Artist and His Design Problems, 
Hast{ngs House Publishers, New York, 1968. Muller-Brockman, Joseph. A Histor[ of Visual Communication, 
Hastings House Pub- lishers, New York, 1971. "New Ways to View World Problems," Per- spectives, Vol. 
i, No. i, 1979, pp. 15-22. Robinson, Arthur H., Randall Sale and Joel Morrison, Elements of Cartography, 
Fourth edition, New York, 1978. Ruder, Emil. Typographie, Hastings House Publishers, New York, 1967. 
 Schmid, Calvin, Handbook of Graphic Presentation, First Edition, The Ronald Press Co., New York, 1954; 
second Edition, John Wiley and Sons, New York, 1979. Spear, Mary Eleanor. Practical Chartinq Techniques. 
McGraw-Hill Book Co., New York, 1969. Stevens, Carol, "Swiss Design Is Alive [etc.], "Print", Vol. 25, 
No. i, January- February 1971, pp. 37-49. Sventek, Virginia A. CHART: Workbook II. Socio-Economic-Environmental-Demographic 
Information System, Computer Science and Applied Mathematics Department, Publ. No., LBL-7277, Lawrence 
Berkeley Laboratory, University of California, March 1978. "Tel-A-Graf", "Display", Integrated Software 
Systems Corp., San Diego, Cali- fornia, 1979. Tschichold, Jan. Asymmetric Typography, translated by 
Ruari MacLean, Rheinhold, New York, 1967. Vignelli, Massimo. Grids: Their Meaning and Use for Federal 
Designers. Federal De----sig--n-Llb~-~ar~ N--atlona-~------If En--dowment for the Arts, Washington, 
D.C., 1977. Von Grunigen, Swiss Poster Art, Verlag der Visualis, Zurich, 1968. Wertheimer, Max. Laws 
of Organization in Perceptual Forms," in Ellis Willis D., ed. A Source Book of Gestalt Psychology. Har- 
court Brace, New York, 1939. Figure 1 ~?[R C~UBP?I~ IN S~r~KIS¢O lJv *~[* CUFIUL~I~,~T.LY US[ IN BILLIOMSOrGa~OhS 
.-................................................................... ,4e .... -.....,.... ......... 
, $// i. .................................... 21~:~:~.._<:~ ............... .............~6~[~-~ ........................... 
 ~IUi~ r i~I~ ..~z~ ~ ~--.~ ~t ~ T ................................................ Figure 2 HANAII 
[NTEGRATEO ENERGY A5BESSMENT HANAI[ DEPARTMENT OF PLANHINi] AND ECONOMIC DETELOPMENT LANRENEE BERKELEY 
LABORATORY PROJECTED ELECTRICITY StiLES, OAHU [NILLIONS OF KILOHATT-HOURB] 158@8 ~;HISTOR[EAL - PROJECTED 
I@@@@ 5@@@ 0 1965 197@ 1975 1988 1985 19B@ 1985 28@8 2Z05 rigu~-e 3 RA~JA[ I INTEORATEO ENEROY RSBEBBNENT 
HANAII OERARTNENT OF PLANNING QN9 [CONOM[C DEVELOPrqENT LRNRENCE BERKELEY LQBORATORY PROJECTED ELECTRICITY 
SALES, OAHU [NILL[ON5 OF KILORBTT HOUR5] 15@9@ E~ REGION OF , UNCERTAINTY i@@@@ 5@@@ @i 1965 t97@ 
1975 1980 1985 1998 1995 20@6 20@5 l Figure 4 253
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1980</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>807502</article_id>
		<sort_key>254</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-1980</article_publication_date>
		<seq_no>43</seq_no>
		<title><![CDATA[Perceptual color spaces for computer graphics]]></title>
		<page_from>254</page_from>
		<page_to>261</page_to>
		<doi_number>10.1145/800250.807502</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=807502</url>
		<abstract>
			<par><![CDATA[<p>Perceptually uniform color spaces can be a useful tool for solving computer graphics color selection problems. However, before they can be used effectively some basic principles of tristimulus colorimetry must be understood and the color reproduction device on which they are to be used must be properly adjusted. The Munsell Book of Color and the Optical Society of America (OSA) Uniform Color Scale are two uniform color spaces which provide a useful way of organizing the colors of a digitally controlled color television monitor. The perceptual uniformity of these color spaces can be used to select color scales to encode the variations of parameters such as temperature or stress.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Color]]></kw>
			<kw><![CDATA[Color science]]></kw>
			<kw><![CDATA[Color television]]></kw>
			<kw><![CDATA[Colorimetry]]></kw>
			<kw><![CDATA[Computer graphics]]></kw>
			<kw><![CDATA[False color]]></kw>
			<kw><![CDATA[Pseudo color]]></kw>
			<kw><![CDATA[Uniform color spaces]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.4.8</cat_node>
				<descriptor>Color</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.2.8</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10003809.10011254.10011258</concept_id>
				<concept_desc>CCS->Theory of computation->Design and analysis of algorithms->Algorithm design techniques->Dynamic programming</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010205</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Search methodologies</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010213</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Control methods</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010225.10010227</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision tasks->Scene understanding</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP39075903</person_id>
				<author_profile_id><![CDATA[81339516843]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Gary]]></first_name>
				<middle_name><![CDATA[W.]]></middle_name>
				<last_name><![CDATA[Meyer]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Program of Computer Graphics, Cornell University, Ithaca, New York]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P68459</person_id>
				<author_profile_id><![CDATA[81100196982]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Donald]]></first_name>
				<middle_name><![CDATA[P.]]></middle_name>
				<last_name><![CDATA[Greenberg]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Program of Computer Graphics, Cornell University, Ithaca, New York]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Bartelson, C.J. and Breneman, E.J., "Brightness Reproduction in the Photographic Process," Photogr. Sci. Engr. July-August, 1967.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Booth, J.M. and Schroeder, J.B., "Design Considerations for Digital Image Processing Systems," Computer, pp. 15,August 1977.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA["CIE Recommendations on Uniform Color Spaces, Colour-difference Equations, and Psychometric Colour Terms," Supplement No. 2 to Publication CIE No. 15, Colorimetry (E-1.3.1) 1971, Bureau Central de la CIE, Paris,1978.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[DeMarsh,L.E., "Optimum Telecine Transfer Charateristics," Journal of the SMPTE, October 1972.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Doucette, A.R., "Color Discrimination in Digital Displays," Society for Information Display (SID) Digest of Technical Papers, pp. 48, 1977.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Color Television Standards, Edited by D.G. Fink, New York: McGraw-Hill Book Co. 1955.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Hunt, R.W.G., The Reproduction of Color, Third Edition, New York: John Wiley and Sons, 1975.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807362</ref_obj_id>
				<ref_obj_pid>965139</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Joblove, G.H. and Greenberg, D.P., "Color Spaces for Computer Graphics," Computer Graphics, August 1978.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Judd, D.B. and Wyszecki,G., "Extension of the Munsell Renotation System to Very Dark Colors," Journal of the Optical Society of America, Vol. 46, pp. 281, April 1956.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Judd, D.B. and Wyszecki, G., Color in Business, Science and Industry, Third Edition, New York: John Wiley and Sons, 1975.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Limb, J.O., Rubinstein, C.B., and Thompson, J.E., "Digital Coding of Color Video Signals - A Review," IEEE Transactions on Communications, Vol. 25, pp. 1349, November 1977.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[MacAdam,D.L., "Maximum Visual Efficiency of Colored Materials," Journal of the Optical Society of America, Vol 25, pp. 361 November 1935.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[MacAdam, D.L., "Geodesic Chromaticity Diagram Based on Variances of Color Matching By 14 Normal Observers," Applied Optics, Vol. 10, pp. 1, January 1971.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[MacAdam, D.L., "Uniform Color Scales," Journal of the Optical Society of America, Vol. 64, pp. 1691, December 1974.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[MacAdam, D.L., "Colorimetric Data for Samples of OSA Color Scales," Journal of the Optical Society of America, Vol. 68, pp.121, January 1978]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[MacAdam, D.L., private communication.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Macbeth, a Division of Kollmorgen Corp., Munsell Color, The Munsell Book of Color, 2441 North Calvert Street, Baltimore, Maryland 21218.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[Meyer, G.W., Thesis, Program of Computer Graphics, Cornell University, Ithaca, N.Y.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807429</ref_obj_id>
				<ref_obj_pid>965103</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[Morse, A., "Some Principles for the Effective Display of Data," Computer Graphics, Vol. 13, pp. 94, August 1979.]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
			<ref>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[Munsell, A.H., A Color Notation, Baltimore: Munsell Color Company, Inc., 1946.]]></ref_text>
				<ref_id>20</ref_id>
			</ref>
			<ref>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[Neal,C.B., "Television Colorimetry for Receiver Engineers," IEEE Transactions on Broadcast Television and Receiver, August 1973.]]></ref_text>
				<ref_id>21</ref_id>
			</ref>
			<ref>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[Newhall, S.M., "Preliminary Report of the OSA Subcommittee on the Spacing of the Munsell Colors," Journal of the Optical Society of America, Vol. 30,pp. 617, 1940.]]></ref_text>
				<ref_id>22</ref_id>
			</ref>
			<ref>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[Newhall, S.M., Nickerson, D. and Judd,D.B., "Final Report of the OSA Subcommittee on the Spacing of the Munsell Colors," Journal of the Optical Society of America, Vol. 33, pp. 385, July 1943.]]></ref_text>
				<ref_id>23</ref_id>
			</ref>
			<ref>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[Novick, S.B., "Tone Reproduction from Colour Telcine Systems," British Kinematography Sound and Television, October 1969.]]></ref_text>
				<ref_id>24</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807361</ref_obj_id>
				<ref_obj_pid>965139</ref_obj_pid>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[Smith, A.R., "Color Gamut Transform Pairs," Computer Graphics, August 1978.]]></ref_text>
				<ref_id>25</ref_id>
			</ref>
			<ref>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[Wentworth,J.W., Color Television Engineering, New York: McGraw-Hill, 1955.]]></ref_text>
				<ref_id>26</ref_id>
			</ref>
			<ref>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[Wintringham, W.T., "Color Television and Colorimetry," Proceedings of the IRE, October 1951.]]></ref_text>
				<ref_id>27</ref_id>
			</ref>
			<ref>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[Wyszecki,G. and Stiles, W.S., Color Science, New York: John Wiley and Sons, Inc., 1967.]]></ref_text>
				<ref_id>28</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 PERCEPTUAL COLOR SPACES FOR COMPUTER GRAPHICS by Gary W. Meyer and Donald P. Greenberg Program of 
Computer Graphics Cornell University Ithaca, New York 14853 ABSTRACT Perceptually uniform color spaces 
can be a useful tool for solving computer graphics color selection prob- lems. However, before they can 
be used effectively some basic principles of tristimulus colorimetry must be understood and the color 
reproduction device on which they are to be used must be properly adjusted. The Munsell Book of Color 
and the Optical Society of America (OSA) Uniform Color Scale are two uniform color spaces which provide 
a useful way of organizing the colors of a digitally controlled color television moni- tor. The perceptual 
uniformity of these color spaces can be used to select color scales to encode the vari- ations of parameters 
such as temperature or stress. COMPUTING REVIEWS CLASSIFICATION: 3.1, 3.2, 3.41, 8.2 KEYWORDS: computer 
graphics, color, color science, colorimetry, uniform color spaces, pseudo color, false color, color television 
 i. INTRODUCTION A perceptually uniform color organization has been sought for years by color scientists 
attempt- ing to set tolerances on color reproduction tech- niques~ by psychologists probing the psychophysio- 
logy of vision, and by artists looking for new color harmonies. The idea is to define a color system 
in which an equal perceptual distance separates all of the colors. For example, the grayscale of the 
system should provide a smooth transition between black and white. Although such an ideal system has 
yet to be found, numerous proposals have been made for approximately uniform systems. Most of these proposed 
color organizations are described in terms of the color notation system standardized by the Commission 
Internationale de L'Eclairage (CIE). Since color television is based on the CIE color notation system, 
these uniform spaces can be di- rectly applied to work with digitally controlled color television monitors 
once the colorimetry and calibration of the monitors is understood. Color collections such as the Munsell 
Book of Color and the Optical Society of America (OSA) Uniform Color Scale can supplement the color organizations 
des- cribed in JOBL78 and SMIT78 as palettes from which Permission to copy without fee all or part of 
this material is gran=ed provided that the copies are not made or distributed for direct com~aerclal 
advantage, the A(~4 copyright notice and the title of the publication and its date appear, and notice 
 is given that copying is by permission of the Association for Computing Machinery. To copy otherwise, 
or to republish, requires a fee and/or specific permission. ~980 ACM 0-89791-021-4/80/0700-254 $00.75 
 to make color selections. In addition the percep- tual uniformity of these color spaces can be used 
in such problems as color encoding of information and image data compression. 2. COLOR SCIENCE AND TELEVISION 
COLORIMETRY Only the fundamental results of tri-chromatic color theory and television colorimetry will 
be presented here. For more detailed information, the reader is referred to HUNT75, JUDD75, and WYSZ67 
which are excellent general references on color science and color reproduction. Television color- imetry 
is discussed in NEAL73, WENT55, and WINT51. A basic assumption in tri-stimulus colorimetry is that the 
perception of color is primarily deter- mined by the spectral energy distribution of the electromagnetic 
energy entering the eye. Experi- ment has shown that the color sensation produced by a given spectral 
energy distribution can be quantified as a triplet of numbers computed from the expressions: f780 X 
: E(k)~(k)dk.7380 (1) fY : 780 E(%)y(1)d% .7380 z =f780 -J 380 E(X)E(1)dl where X, Y, and Z are referred 
to as the tristim- ulus values of the color, E(I) is the spectral en- ergy distribution of the color 
and ~(l), ~(l), and 254  ferred to as the "white point". Its coordinates in 1931 CIE XYZ space are 
usually given in terms of chromaticity coordinates and luminance. The moni- tor is adjusted so that this 
color has the coordi- nates in RGB space of the maximum RGB values. (In this paper the RGB values lie 
on the range 0.0 to 1.0). The mathematical relationship between the elements of the matrix in equation 
(3) and the above monitor properties can be found in several sources (MEYE80, NEAL73, WINT51) and will 
not be repeated here. Figure 2 shows the shape and posi- tion of a typical color television monitor gamut 
in 1931 CIE XYZ Space. In a typical color television monitor, the RGB values are controlled indirectly 
by specifying sig- nal voltages Rvolt , Gvolt , and Bvolt , normal- ized to the range 0.0 to 1.0 in 
this paper). The relation between, for example, R and Rvolt can be determined experimentally by varying 
Rvolt and using a digital photometer to measure the lumi- nance (Y) produced at the display. This is 
sim- ply a variable intensity light source of known chromaticity and the procedure generates points 
 which define the R axis in 1931 CIE XYZ space. When repeated for the G and B signals, the follow- 
ing relations are established: i/Y R Rvolt = R (4) i/Y G Gvolt : G i/Y B Bvolt = B where YR, YG, 
and YB are almost identical (often they are assumed identical and referred to collectively as the monitor 
"gamma") and lie on the range 2.5 to 3.0, depending on the moni- tor. The validity of the transformation 
given by equations (3) and (4) can be tested by reproducing colors of known chromaticity and relative 
luminance and comparing them to actual color samples. Figure 3 shows such an experiment. In making these 
com- parisons it is important that the color samples are illuminated with light of the same spectral 
energy distribution as the light used to illuminate them when their chromaticity coordinates were deter- 
mined. One cannot overemphasize the importance of ac- curately measuring and adjusting such monitor 
pro- perties as phosphor chromaticity, white point chro- maticity, white point luminance and gamma. If 
these parameters are not tightly controlled, accu- rate and consistent color reproduction is impos- sible 
(MEYE80). It must also be remembered that variables held constant during the experiments which led to 
the laws "of tristimulus colorimetry cannot be allowed to vary when these laws are applied. The level 
of ambient illumination has a significant effect on color perception and is something which can be quite 
different for an observer viewing an original scene and an observer viewing a reproduction of that scene 
on a monitor in a dimly lit room. Adjustments to straight colorimetric calculations to account for this 
have been suggested and are discussed elsewhere (BART67, DEMA72, MEYE80, NOVI69). 3. MUNSELL SYSTEM 
 In 1905 Albert H. Munsell published a book and a series of color charts which proposed a color notation 
system with three dimensions: Hue~ Value, and Chroma. His definitions for these terms were (MUNS46): 
 Hue -"It is that quality by which we distinguish one color family from anoth- er, as red from yellow, 
or green from blue or purple." Value -"It is that quality by which we distinguish a light color from 
a dark one." Chroma -"It is that quality of color by which we distinguish a strong color from a weak 
one; the degree of departure of a color sensation from that of white or gray; the intensity of a distinctive 
Hue; color intensity." Although Munsell used a sphere in his original publication to describe the geometry 
of his color coordinate system, it is more consistent with the organization of the Munsell Book of Color 
(MACB79) to think of the notation system in cylindrical coordinates with hue as the angle relative to 
the cylinder's central axis, value the vertical posi- tion, and chroma the radial position. In addition 
to labeling colors using hue, value, and chroma, Munsell also wanted a notation system which demonstrated 
his ideas about color balance. For example, colors which lie at opposite sides of the color solid are 
"balanced" about the central neutral gray. This example and others like it suggest that there is a psychological 
nature to his color system, i.e., steps along the hue, value, or chroma directions are perceptually equal. 
 In 1940 the CIE tristimulus values for the Munsell Book of Color were measured in terms of CIE illuminant 
C, and a new study of their perceptual spacing was undertaken. Forty observers made some 3,000,000 color 
judgements which resulted in a re- designation of the hue, value, and chroma specifi- cation for each 
color sample (NEWH40). Using this data, the relation between a sample's percent lumi- nous reflectance 
(Y) and value A was found to be (NEWH43): Y = 1.2219A -0.23111A 2 + 0.23951A 3 (5) - 0.021009A 4 + 
0.0008404A 5 The data for the samples was also plotted on chro- maticity diagrams of constant value 
(and constant luminous reflectance according to equation (5)). Smooth curves defining loci of constant 
hue and constant chroma were drawn on the charts based on the redesignated Munsell samples (NEWH43). 
The re- sults for value 5/ are shown in Figure 4. ~:ese loci were extrapolated to the i/ and 9/ value 
lev- els and to the boundary of the object color solid 256   8. ACKNOWLEDGEMENTS This work benefited 
from discussions with sev- eral people at the Cornell University Program of Computer Graphics. In particular, 
Christopher Odgers stressed the importance of having the proper equipment to adjust a color television 
monitor, Richard Gallagher wrote the program used to gener- ate Figures 14 and 15, Michael Schulman performed 
the structural analysis depicted in Figures 14 and 15, and Bruce Forbes helped draw Figure i0. Special 
thanks is extended to GTE Sylvania for help in measuring the colorimetric properties of our tele- vision 
monitors and to Rensselaer Polytechnic In- stitute for providing the data which defines the Munsell color 
system. The Cornell University Pro- gram of Computer Graphics is partially funded by the National Science 
Foundation.  9. REFERENCES BART67 Bartelson, C.J. and Breneman, E.J., "Brightness Reproduction in the 
Photo- graphic Process," Photogr. Sci. Engr. July-August, 1967. BOOT77 Booth, J.M. and Schroeder, J.B.,"Design 
Considerations for Digital Image Processing Systems," Computer, pp. 15,August 1977. CIE78 "CIE Recommendations 
on Uniform Color Spaces, Colour-difference Equations, and Psychometric Colour Terms," Supplement No. 
2 to Publication CIE No. 15,Colorimetry (E-i.3.1) 1971, Bureau Central de la CIE, Par~s,1978. DEMA72 
DeMarsh,L.E., "Optimum Telecine Transfer Charateristics," Journal of the SMPTE, October 1972. DOUC77 
Doucette, A.R., "Color Discrimination in Digital Displays," Society for Information Display (SID) Disest 
of Technical Papers, pp. 48, 1977. FINK55 Color Television Standards, Edited by D.G. Fink, New York: 
McGraw-Hill Book Co. 1955. HUNT75 Hunt, R.W.G., The Reproduction of Color, Third Edition, New York: 
John Wiley and Sons, 1975. JOBL78 Joblove, G.H. and Greenberg, D.P., "Color Spaces for Computer Graphics," 
Computer Graphics, August 1978. JUDD56 Judd, D.B. and Wyszecki,G., "Extension of the Munsell Renotation 
System to Very Dark Colors," Journal of the Optical Society of America, Vol. 46, pp. 281, April 1956. 
 JUDD75 Judd, D.B. and Wyszecki, G., Color in Busi- ness, Science and Industry, Third Edition, New York: 
John Wiley and Sons, 1975. LIMB77 Limb, J.O., Rubinstein, C.B., and Thompson, J.E., "Digital Coding 
of Color Video Sig- nals -A Review," IEEE Transactions on Communications, Vol. 25,pp. 1349, November 
1977. MACA35 MacAdam,D.L., "Maximum Visual Efficiency of Colored Materials," Journal of the Op- tical 
Society of America, Vol 25, pp. 361 November 1935. MACA71 MACA74 MACA78 MACA79 MACB79 MEYE80 MORS79 
 MUNS46 NEAL73 NEWH40 NEWH43 NOVI69 SMIT78 WENT55 WINT51 WYSZ67 MacAdam, D.L., "Geodesic Chromaticity 
Dia- gram Based on Variances of Color Matching By 14 Normal Observers," Applied Optics, Vol. I0, pp. 
i, January 1971. MacAdam, D.L., "Uniform Color Scales," Journal of the Optical Society of America, Vol. 
64, pp. 1691, December 1974. MacAdam, D.L.,"Colorimetric Data for Sam- ples of OSA Color Scales," Journal 
of the Optical Society of America, Vol. 68,pp.121, January 1978 MacAdam, D.L., private communication. 
 Macbeth, a Division of Kollmorgen Corp., Munsell Color, The Munsell Book of Color, 2441 North Calvert 
Street, Baltimore, Maryland 21218. Meyer, G.W., Thesis, Program of Computer Graphics, Cornell University, 
Ithaca, N.Y. Morse, A., "Some Principles for the Effec- tive Display of Data," Computer Graphics, 
Vol. 13, pp. 94, August 1979. Munsell, A.H., A Color Notation, Baltimore: Munsell Color Company, Inc., 
1946. Neai,C.B., "Television Colorimetry for Re- ceiver Engineers," IEEE Transactions on Broadcast 
Television and Receiver, August 1973. Newhall, S.M.,"Preliminary Report of the OSA Subcommittee on 
the Spacing of the Munsell Colors," Journal of the Optical Society of America, Vol. 30,pp. 617, 1940. 
 Newhall, S.M., Nickerson, D. and Judd,D.B., "Final Report of the OSA Subcommittee on the Spacing of 
the Munsell Colors,"Journal of the Optical Society of America, Vol. 33, pp. 385, July 1943. Novick, 
S.B., "Tone Reproduction from Col- our Telcine Systems," British Kinemato- graphy Sound and Television, 
October 1969. Smith, A.R., "Color Gamut Transform Pairs," Computer Graphics, August 1978. Wentworth,J.W., 
Color Television Engineer- ing, New York: McGraw-Hill, 1955. Wintringham, W.T., "Color Television and 
 Colorimetry," Proceedings of the IRE, October 1951. Wyszecki,G. and Stiles, W.S.,Color Science, New 
York: John Wiley and Sons, Inc., 1967. 261 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1980</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>807503</article_id>
		<sort_key>262</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-1980</article_publication_date>
		<seq_no>44</seq_no>
		<title><![CDATA[&#8220;Put-that-there&#8221;]]></title>
		<subtitle><![CDATA[Voice and gesture at the graphics interface]]></subtitle>
		<page_from>262</page_from>
		<page_to>270</page_to>
		<doi_number>10.1145/800250.807503</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=807503</url>
		<abstract>
			<par><![CDATA[<p>Recent technological advances in connected-speech recognition and position sensing in space have encouraged the notion that voice and gesture inputs at the graphics interface can converge to provide a concerted, natural user modality.</p> <p>The work described herein involves the user commanding simple shapes about a large-screen graphics display surface. Because voice can be augmented with simultaneous pointing, the free usage of pronouns becomes possible, with a corresponding gain in naturalness and economy of expression. Conversely, gesture aided by voice gains precision in its power to reference.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Gesture]]></kw>
			<kw><![CDATA[Graphics]]></kw>
			<kw><![CDATA[Graphics interface]]></kw>
			<kw><![CDATA[Man-machine interfaces]]></kw>
			<kw><![CDATA[Space sensing]]></kw>
			<kw><![CDATA[Spatial data management]]></kw>
			<kw><![CDATA[Speech input]]></kw>
			<kw><![CDATA[Voice input]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.2.7</cat_node>
				<descriptor>Speech recognition and synthesis</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.1</cat_node>
				<descriptor>Input devices</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010391</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Graphics input devices</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010179.10010183</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Natural language processing->Speech recognition</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Human Factors</gt>
			<gt>Management</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P241484</person_id>
				<author_profile_id><![CDATA[81100653961]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Richard]]></first_name>
				<middle_name><![CDATA[A.]]></middle_name>
				<last_name><![CDATA[Bolt]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Architecture Machine Group, Massachusetts Institute of Technology, Cambridge, Massachusetts]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Negroponte, N The Media Room. Report for ONR and DARPA. MIT, Architecture Machine Group, Cambridge, MA, December 1978.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Bolt, R.A. Spatial Data-Management. DARPA Report. MIT, Architecture Machine Group, Cambridge, MA, March 1979.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Reddy, D.R. Speech recognition by machine: a review. Proceeding of the IEEE, 64, 4 (April 1976), 501-531.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Robinson, A.L. More people are talking to computers as speech recognition enters the real world. (Research News) (First of two articles) Science, 203, (16 February 1979), 634-638.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Sondeheimer, N.K. Spatial reference and natural-language machine control. International Journal of Man-Machine Studies, 8, (1976), 329-336.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>889456</ref_obj_id>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Winston, P. Learning structural descriptions from examples. MIT Project MAC, TR-76, 1970.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Olson, D.R. Language and thought: Aspects of a cognitive theory of semantics. Psychological Review, 77, (1970), 257-273.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 "Put-That-There": Voice and Gesture at the Graphics Interface Richard A. Bolt Architecture Machine 
Group Massachusetts Institute of Technology Cambridge, Massachusetts 02139 ABSTRACT Recent technological 
advances in connected-speech recognition and position sensing in space have encouraged the notion that 
voice and gesture inputs at the graphics interface can converge to provide a concerted, natural user 
modality. The work described herein involves the user commanding simple shapes about a large-screen 
graphics display surface. Because voice can be augmented with simultaneous pointing, the free usage of 
pronouns becomes possible, with a corresponding gain in naturalness and economy of expression. Conversely, 
gesture aided by voice gains precision in its power to reference. Key Words: Voice input; speech input; 
gesture; space sensing; spatial data management; man-machine interfaces; graphics; graphics interface. 
 Category Numbers: 8.2, 6.9. The work reported herein has been supported by the Cybernetics Technology 
Division of the Defense Advanced Research Projects Agency, under Contract No. MDA-903-77-C-0037. Permission 
to copy without fee all or part of this material is the title of the publication and its date appear, 
and notice granted provided that the copies are not made or distributed is given that copying is by 
permission of the Association for for direct commercial advantage, the ACM copyright notice and Computing 
Machinery. To copy otherwise, or to republish, requires a fee and/or specific permission. G9980 ACM 
0-8979]-02]-4/80/0700-262 $00.75 INTRODUCTION Recently, the Architecture Machine Group at the Massachusetts 
Institute of Technology has been experimenting with the conjoint use of voice-input and gesture-recognition 
to command events on a large format raster-scan graphics display. Of central interest is how voice and 
gesture can be made to inter-orchestrate, actions in one modality amplifying, modifying, disambiguating, 
actions in the other. The approach involves the significant use of pronouns, effectively as "temporary 
variables" to reference items on the display. The interactions to be described are staged in the MIT 
Architecture Machine Group's "Media Room," a physical facility where the user's terminal is literally 
a room into which one steps, rather than a desk-top CRT before which one is perched. The Media Room 
Sketched in Figure i, is the size of a personal office: about sixteen feet long, eleven feet wide, and 
about eight feet from floor to ceiling. The floor is raised to accommodate cabling from an ensemble 
 of mini-computers which drives displays and devices resident in the Media Room. The walls, finished 
in dark brown pile fabric, house banks of loudspeakers on either side of a wall-sized, frosted-glass 
projection screen, and on either side and a bit to the rear of the user's chair. The user's chair is 
a vinyl-covered Eames-type chair, exactly as comes from the furniture store, except for two types of 
instumentation based in its arms. Either arm bears a small, one-inch high joystick, of the non- displacing 
variety, sensitive to pressure and direction. Nearby each joystick is a two-inch on edge, square-shaped 
touch sensitive pad. F I -:ii~, L_..~ "_-~ : = = .~ i r ("::1 r.i I i .... i! I i 1 I ."L-" ""-"', i 
i [Ii i i i i I !/ iii I I, I i I L l I I Figure 1 Sketch of Media Room The wall-sized screen, about 
eight feet to the user's front, is served by back-projection from a color TV light-valve projector situated 
in an adjoining room. Color TV monitors are situated on either side of the user's chair, each with 
its tube face overlain with a transparent, touch-sensitive pad. Apart from its role as an embodiment 
of the user terminal as an "informational surround [i]," the Media Room with its user chair has played 
a key role in our researches into a "Spatial Data-Management System, or SDMS [2]." The specific rationale 
for spatially indexing data derives from our everyday experience of retrieving items, say, from our 
desktop: the phone to the right and above the blotter; the appointment calendar in the lower right; 
the "in-box" nearby the ashtray at the lower left, and so forth. Retrieval is natural and automatic 
for these items, with even an apparently "messy" desk having a spatial logic well-known to its creator 
and user, the knowledge of where this and that item are located being encoded conjointly in mental and 
 motor models of the layout of the desktop. The user of SDMS retrieves information not by typing names, 
i.e., alphanumeric strings on a keyboard terminal, but instead uses joystick and, occasionally, touch 
controls to navigate about in a helicopter-like manner to where specific caches of information reside 
in a rich graphics world of color and sound. The world of information in SDMS, dubbed "Dataland," appears 
in its entirety upon one of the color TV monitors near-by the user chair. A small transparent rectangular 
overlay, a "you-are-here" marker, can be moved and positioned about Dataland by the user's managing of 
the chair's right-hand joystick (or by direct touch on the TV screen, if desired). That sub-portion of 
the Dataland surface indicated by the "you-are-here" rectangle is portrayed with increased detail on 
the large screen, effectively a magnifying window onto Dataland. The left-hand joystick on the user chair 
enables the user to zoom-in upon information to get a closer look at any of a number of multi- media 
data-types (e.g., maps, electronic "books," videodisc episodes), and perhaps to peruse them with the 
aid of an associated touch-sensitive "Key- map" which comes up on the Other TV monitor by the user chair. 
 The Media Room setting, in addition to its power to generate a convincing impression of interacting 
with an implicit, "virtual" world of data behind the frame of the physical interface, implies yet another 
realm or order of space rife with possibilities for interaction: the actual space of the Media Room 
itself. The sheer extent of the Media Room's physical interface creates a "real- space" environment. 
The user's focal situation amidst an ensemble of several screens of various sizes creates a set of geometrical 
relationships quite apart from any purely logical relationship between any one screen's content and 
that of any other. Properly orchestrated, the two spatial orders, virtual graphical space, and the 
user's immediate real space in the Media Room, can converge to become effectively one continuous interactive 
space. User awareness of this common space is implicit: the user points, gestures, references "up," 
"down," "...to the left of...," and so on, freely and naturally, precisely because the user is situated 
in a real space. Tapping this interactive potential is rooted in--two new technical offerings in the 
 areas of: I) connected speech recognition; 2) position sensing in space. SPEECH AND SPACE: THE TECHNOLOGIES 
 Two broad categories of currently commercially available speech recognizers may be distinguished: those 
which recognize discrete or isolated utterances, and those which recognize connected speech. With those 
speech recognition systems restricted to discrete utterances, parsing of the speech signal into word-by-word 
tokens, is not done. The human speaker must talk to the system in a "clipped" or word-by-word style. 
 The recognition of connected speech has been a classic challenge in the field of speech recognition 
generally [3]. The DP-100 Connected Speech Recognition System (CSRS) by NEC (Nippon Electric Company) 
America, Inc. is capable of a limited amount of recognition of connected speech [4]. No pause between 
words is necessary, and up to five words or "utterances" are permitted per spoken sentence. The recognition 
response time at the end of each sentence is about 300 milliseconds. Output is a display of the text 
of the utterance on an alphanumeric visual display, and/or a set of ASCII codes (numbers or letters) 
to be received by a processor interfaced with the NEC system. The device's vocabulary, held in the recognizer's 
active memory as a set of word reference patterns, is a maximum of 120 words. With an optional "discrete 
utterance" mode, the size of the active vocabulary in the system's memory may be larger, about 1000 words. 
Except for the digits "one" to "ten," which must be spoken twice by the user when "training" the machine, 
each word in training mode need be spoken only once. The standard system comes with a lightweight, head-mounted 
microphone. We look forward to eventual use of a "shotgun" microphone in the Media Room, remote from 
but aimed at the speaker. A space position and orientation sensing technology suitable for our intentions 
was found to be made by Polhemus Navigation Science, Inc., of Essex, Vermont. This system, called ROPAMS 
(Remote Object Position Attitude Measurement System) is based on measurements made of a nutating magnetic 
 field. Essentials of the system are as follows. Three coils are epoxied into a plastic cube, their 
mountings mutually orthogonal to correspond to x,y, and z spatial axes. Two such cubes are involved: 
one, about 1.5 inches on edge which acts as a transmitter, and another, 0.75 inches on edge, which functions 
as a sensor. The arrangement of coils in either cube essentially creates an antenna £hat is sensitive 
in all three orientations. The transmitter cube radiates a nutating dipole field pointed at the sensor 
 cube. When the pointing vector is correct, the field strength received will be constant. When it is 
not, there will be an error signal consisting of the nutation frequency. This error is used to generate 
the output pointing angles, and to re-aim the transmitter. The orientation in space of the sensor cube 
is determined by transforming the differential signals from the three individual orthogonal coils in 
the sensor cube. The sensor cube's distance from the transmitter cube is computed by the i/R 3 fall-off 
of signal strength from the radiating dipole, or by triangulation with an additional radiator. The sensor 
cube is very lightweight, and although it has a running out of it, it is not an especially troublesome 
item sensors can readily be wrist-mounted, worn as finger rings, mounted on the visor of baseball caps, 
or put on a sort of lieu of cuff and collar buttons or epaulets. small to "lab h cord andle. jacket" 
Such in COMMANDS Suppose the user seated before the Media Room's large screen, with a space-sensing 
cube attached to a watchband on his wrist, and that the system's microphone is ready and listening. Some 
commands from the system's current repertoireillustrativeof voice and pointing in concert are the following: 
 "Create . . ." In our demonstration system, the large screen is initially either clear, or bears some 
simple backdrop such as a map. Against this background, simple items are called into existence, moved 
about, replicated, their attributes altered, and then may be ordered to vanish. The items used are 
basic shapes: circles, squares, diamonds. They are non- representational in that the thing is the shape. 
Variable attributes are: color (red, yellow, orange, green, b-lue. .), and size (large, medium, small). 
 For example, the user points to some spot on the large screen. A small, white "x" cursor on the screen 
provides running visual feedback for pointing. The user then says: "Create a blue square there." The 
size of the square is not given explicitly in this example command; the default size, "medium," is used. 
A blue square appears on the spot where the user is pointing. There is no default color; some color from 
the pre- progranuned parent ensemble of color names must be given. The same is true for shape. Where 
the feed-back cursor is residing on the screen at the time the spoken "there" occurs becomes the spot 
where the to-be-created item is placed. The occurrence of the spoken "there" is thus functionally a 
"when"; that is, it serves as a "voice button" for the x,y cursor action of the pointing gesture. Accordingly, 
a considerable pause before the occurrence of the "there" is permissible, i.e.: "Create a blue square 
. . there." The complete utterance in effect is a "call" to a Create routine, which routine expects 
certain parameters to be supplied. Before the user recites "there," the routine is parameter hung. The 
awaited parameter is input, completing the conjunction of x,y pointing input from the wrist-borne space 
sensor with the utterance "there." Figure 2 shows the user having created a number of items on the screen 
 before him. "Move . ." The user can readily move items about the screen, and has available a variety 
of ways in which to express the complete "move" command. Consider the user command: "Move the blue 
triangle to the right of the green square." This example conunand relies on voice mode only. Should, 
for example, there exist only one triangle on the screen at the time the command is given, the adjective 
"blue" bears no information, and could be omitted; the same logic applies for the qualifier green in 
"green square." We note in passing that in the phrase " . the green square," the attribute "green" 
as voiced is treated simply as part of the name of the item as originally created. That is, the color 
name is used in a nominal sense, as in Moscow's "Red Square," where "Red" is functionally part of a 
proper name, not a signal that we should expect a city square to be painted all in red. Apropos of 
color, a more ambitious "interpretive" approach might be to map the utterance "green" to pixel values, 
the matching mediated through the classical CIE color space, partitioned into a number of referenceable 
regions. The partitioning of the CIE color space on the basis of an ensemble of color names could be 
programmer determined on an ad hoc basis, or the partitioning might involve a quite sophisticated calibration 
on the basis of having subject observers name or classify displayed colors. The essential point is that 
the mapping from attribute-name to item-attribute can be well defined, even though it may be as complex 
as one cares to attempt. In any event, the result of the above command is that the blue triangle upon 
 "hearing" its name, de-saturates as immediate feedback that it has been "addressed," disappears from 
its present site to re-appear centered in a spot to the right of the green square. The exact positioning 
"to the right" is programmer determined in our version; some reasonable placement is executed. The meaning, 
intent and interpretation of relational expressions in graphic space is a complex issue [5,6]; the  
important thing is that the item is now where the user has ordered it to be, and he can make minor modifications 
in position later. Now, in our example action, the user might equally well have said: "Move that to 
the right of the green square." In this option, the user employs the pronoun "that," simultaneously 
pointing to what is intended, the pointing act being a motor analogue to the speech string: ". . the 
blue triangle . " Notice that in this mode of giving the command, the user may not only omit the words 
"blue" and "triangle," he need not even know what the thing is, or what it is called. In our simple 
graphics world, what anything i_s, is-~n a subtle and interesting sense, where it is. "That" is thus 
defined as whatever is pointed out; effectively, it is "ostensively defined" [7]. For the namer, at 
least, the process is not unlike that of telling a small child what things "are": for example, pointing 
at a cat, and saying "cat" or "kitty." The meaning of the word is given by indicating what is the intended 
referent in the context of alternatives, namely, whatever else is in the scene. This process of "pronomialization" 
can readily be extended in our simple graphical example. The intended target spot to which the item 
is to be moved can be rendered as "Put that there" where there, now indicated by gesture, serves in 
lieu of the entire phrase ". . . to the right of the green square." The power of this function is even 
more general. The place description ". . . to the right of the green square" presupposes an item in the 
vicinity for reference: namely, the green square. There may be no plausible reference frame in terms 
of already extant items for a word description of where the moved item is to go. The intended spot, however, 
may readily be indicated by voice-and-pointing: there. In this function, as well as others, some variation 
in expression is understandably a valuable option; thus, a mini-thesaurus of common synonyms, such as 
"move," "put," "place," etc., is built into the vocabulary. "Copy . . ." as a command is simply a variant 
of the move action, except that the image of the item to be moved also remains in place at the original 
spot. "Make that . . ." The attributes of any item in this graphic mini-universe that the user has 
 called into existence by voice and gesture can be modified. Here, the attributes are those of color 
and size. For example, the utterance: "Make the blue triangle smaller" causes the referenced item 
to become reduced in size. The mode of reference in this instance is via voice alone, but the user could 
as well have said, pointing to the desired item: "Make that smaller . . ." The command: "Make that 
a large blue diamond" uttered while the user points at a small yellow circle causes the indicated transformation. 
 Extrapolations readily suggest themselves, e.g., the command line: "Make that (indicating some item) 
like that (indicating some other item). The second "that" is, functionally, a when to read the x,y coordinate 
of pointing. The item indicated when the second "that" is uttered becomes the "model" for change, and 
internally, the action is an expunging of the first referenced item, to be replaced in a "copy"-like 
fashion by the second referenced item. "Delete . . ." The "delete" command (synonyms: "erase; expunge; 
take out. ," etc.) allows the user to drop selected items from display. As before, the "operand" of 
the command can be: ". . . the large blue circle" or ". .that" (pointing to some item). Again, variations 
and extrapolations of the basic notion suggest themselves: global expunging, "clear" or "delete everything," 
in order to wipe the graphical slate clean; or "Detete everything to the left of this (drawing a line 
vertically down the face of the screen)." NAMING Consider a blue square that is present upon the screen. 
The user points to it, saying: "Call that . the calendar" with the intention of later somehow elaborating 
the blue square at that node into a graphical "appointment book." The initial portion of this utterance, 
"Call that . . .," when processed by the recognizer unit results in codes being sent over to the host 
system signalling that a "naming" command has been issued. The x,y coordinates of what item is singled 
out by pointing are noted by the host system. The host system then immediately directs the speech recognition 
unit to switch from "recognition mode" to "training mode" so that the recognizer will add the latter 
part of the utterance, " . . the calendar," as a new entry in its file of word reference patterns. Upon 
completion of this action, the recognizer is directed to go back into recognition mode, to be ready 
for the next verbal input. As the communications for switching the recognizer under host-system control 
 between recognition and training modes currently takes a finite amount of real-time, a brief pause 
(indicated in the command above by three dots) must occur in the spoken command line to accommodate 
the time taken for the mode shift. However, the user tends to pause at precisely that point in the 
command line anyway, waiting for momentary desaturation of the blue square. This quick desaturation 
of an addressed item was noted earlier in this paper as being the system's way of giving visual feedback 
that the user has indeed "contacted" the item. This spontaneous pause for feedback fortunately operates 
in this context to "mask" for the user the system's need for a pause in input. However, the obligation 
to pause represents to the system designer something of a break- down in the general convenience of 
continuous vs discrete speech input. An eventual strategy for relieving the necessity f--or a user pause 
in speech is the augmentation of the "intelligence" resident in a speech recognizer unit so that it 
to some extent interprets as well as recognizes. For example, upon the recognition of certain "key" 
words or phrases within the input utterance, the recognizer itself switches directly from recognition 
 to training mode so that sub-portions of the input utterance are handled appropriately. In the case 
of the "Call that . . ." or naming command, the action of the now "intelligent" recognizer would be 
in effect to truncate-off from the "front- end" of the original input speech signal that span of signal 
corresponding to successive recognized words of the command, the non-recognized residue of the speech 
line to be then assumed as the new name to be assimilated by the recognizer to its internal reference 
pattern lexicon. In order to maintain overall coordination with the host system, the recognizer would 
of course simultaneously transmit ASCII codes for recognized or learned words, together with any relevant 
"control" codes. While such a strategy may eliminate the need for a within-sentence speaker pause, 
the general problem of "coarticulation" remains: the phonemic properties of the speech signal for any 
word are influenced by what words are spoken with it, what particular words precede or follow the word 
in question (Cf. reference 3, p. 518). Thus, while not required to pause, the speaker yet must enunciate 
very clearly, particularly when about to utter the new name to be added. SUMMARY The foregoing rudimentary 
set of commands, concerning themselves with the simple management of a limited ensemble of non-representative 
objects, is intended to suggest the versatility and ease of use that can enter upon the management of 
graphic space with voice and gesture. More real-life examples of commanding about "things" in a more 
meaningful space come readily to mind: moving ships about a harbor map in planning a harbor facility; 
moving battalion formations about as overlays on a terrain map; facilities planning, where rooms and 
hallways as rectangles are tried out "here" and "there." The power of the described technique is that 
indications of what is to be done with these visible, out-there-on-view items can be expressed spontaneously 
and naturally in ways which are compatible with the spirit and nature of the display: one is pointing 
to them, addressing them in spoken words, not typed symbols. Further, the pronoun as verbal tag achieves 
in the graphical world the same high usefulness it has in ordinary discourse by being pronounced in the 
presence of a pointed to, visible graphic which functionally defines its meaning. ACKNOWLEDGEMENTS 
 The programming and systems expertise of Chris Schmandt and Eric Hulteen underlay the implementation 
and development of the concepts described in this paper. Their efforts are duly appreciated. REFERENCES 
 i. Negroponte, N The Media Room. Report for ONR and DARPA. MIT, Architecture Machine Group, Cambridge, 
MA, December 1978. 2. Bolt, R.A. Spatial Data-Management.. DARPA Report. MIT, Architecture Machine Group, 
Cambridge, MA, March 1979.  3. Reddy, D.R. Speech recognition by machine: a review. Proceeding of the 
IEEE, 64, 4 (April 1976), 501-531.  4. Rohinson, A.L. More people are talking to computers as speech 
recognition enters the real world. (Research News) (First of two articles) Science, 203, (16 February 
1979), 634-638.  5. Sondeheimer, N.K. Spatial reference and natural-language machine control. International 
Journal of Man-Machine Studies, 8, (1976), 329-336.  6. Winston, P. Learning structural descriptions 
from examples. MIT Project MAC, TR-76, 1970.  7. Olson, D.R. Language and thought: Aspects of a cognitive 
theory of semantics. Psychological Review, 77, (1970), 257-273.  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1980</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>807504</article_id>
		<sort_key>271</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-1980</article_publication_date>
		<seq_no>45</seq_no>
		<title><![CDATA[Prototyping and simulation tools for user/computer dialogue design]]></title>
		<page_from>271</page_from>
		<page_to>278</page_to>
		<doi_number>10.1145/800250.807504</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=807504</url>
		<abstract>
			<par><![CDATA[<p>The design and development of user interfaces to interactive computer systems is enhanced by permitting designers to easily express their design concepts in concrete, comprehensive, and comprehensible working models.</p> <p>A set of prototyping and simulation tools has been developed to be used as an integral part of the specification and design process. These include an interactive display building utility and a syntax-driven interactive dialogue controller. The display builder is used to develop initial conceptual snapshots of system display appearance at selected points in the user/system dialogue. The dialogue controller interprets a grammatical description of input tools and system logic, using predrawn and dynamically constructed displays to simulate the external appearance of the desired end system.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Computer graphics]]></kw>
			<kw><![CDATA[Human interaction]]></kw>
			<kw><![CDATA[Input tools]]></kw>
			<kw><![CDATA[Interactive techniques]]></kw>
			<kw><![CDATA[Programming languages]]></kw>
			<kw><![CDATA[System design]]></kw>
			<kw><![CDATA[User/computer dialogue]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>H.1.2</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>H.5.2</cat_node>
				<descriptor>Prototyping</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>D.2.1</cat_node>
				<descriptor>Tools</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003120.10003123.10010860.10011694</concept_id>
				<concept_desc>CCS->Human-centered computing->Interaction design->Interaction design process and methods->Interface design prototyping</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10011007.10011074.10011092</concept_id>
				<concept_desc>CCS->Software and its engineering->Software creation and management->Software development techniques</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Human Factors</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P332826</person_id>
				<author_profile_id><![CDATA[81100489447]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Paul]]></first_name>
				<middle_name><![CDATA[R.]]></middle_name>
				<last_name><![CDATA[Hanau]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Martin Marietta Aerospace, Denver Division, Denver, Colorado]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P329782</person_id>
				<author_profile_id><![CDATA[81100577728]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[David]]></first_name>
				<middle_name><![CDATA[R.]]></middle_name>
				<last_name><![CDATA[Lenorovitz]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Martin Marietta Aerospace, Denver Division, Denver, Colorado]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>807432</ref_obj_id>
				<ref_obj_pid>800249</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Anson,E. The semantics of graphical input. Proceedings of SIGGRAPH79, Computer Graphics 13,2 Aug 79.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[DEC Fortran/RT-11 Graphic Extensions Manual, Order No. DEC-11-LRTEA-B-D.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>1115922</ref_obj_id>
				<ref_obj_pid>1115918</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Dunlavey, M. The procedural approach to interactive design graphics. Computer Graphics 13,1 Mar 79.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Foley,J.D. &amp; Wallace,V.L. The art of natural graphic man-machine conversation. Proceedings of IEEE 1974, 62, 462-471.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>1024289</ref_obj_id>
				<ref_obj_pid>1024273</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Foley,J.D. Graphical output and input capabilities. In Treu,S.(ed.) User-Oriented Design of Interactive Graphics Systems (Proceedings of ACM/SIGGRAPH Workshop), Oct 1976.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[GPGS (General Purpose Graphics System) User's Tutorial. Graphics Group, University of NI jmegen, 1975.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Guthery,S. DDA: An interactive and extensible language for data display and analysis. Computer Graphics 10,1 1976.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_obj_id>804588</ref_obj_id>
				<ref_obj_pid>800139</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Hanau,P.R. &amp; Lenorovitz,D.R. A prototyping and simulation approach to interactive computer system design. in Proceedings of the 17th Design Automation Conference. ACM/SIGDA - in press.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Heindel,L.E. &amp; Roberto,J.T. LANG-PAK: An Interactive Language Design System. Elsevier Computer Science Library: Programming Language Series; 1. American Elsevier Publishing Company, Inc. New York 1975]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Lenorovitz,D.R. &amp; Ramsey, H.R. A dialogue simulation tool for use in the design of interactive computer systems. Proceedings of the 21st Annual Meeting of the Human Factors Society, Santa Monica, California: Human Factors Society, 1977, 95-99.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Moran,T.P. "How do users understand systems they use?". Panel presentation at SIGGRAPH '79.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Moran,T.P. Introduction to the Command Language Grammar. Xerox PARC Report SSL-78-3.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Ramsey,H.R. &amp; Atwood, M.E. Human Factors in Computer Systems: A Review of the Literature (Technical Report SAI-79-111-DEN). Englewood, Colorado: Science Applications Incorporated, Sept 1979.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Ramsey,H.R. A User&#176;s Guide to the Translator Writing System (TWS). Technical Report SAI-77-O67-DEN. Denver, Colorado: Science Applications Inc., 1977.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Reisner,P. Using a Formal Grammar in Human Factors Design of an Interactive Graphics System. IBM Research Report, San Jose, Ca., 1979.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Schwartz, B.K. The man-logic interaction in Information processing systems. Proceedings of the 16th Annual Meeting of the Human Factors Society, Santa Monica, California: Human Factors Society, 1972, 391-394.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Treu,S.(ed.) User-Oriented Design of Interactive Graphics Systems (Proceedings of ACM/SIGGRAPH Workshop), Oct 1976.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807367</ref_obj_id>
				<ref_obj_pid>965139</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[Van den Bos,J. Definition and use of higher level graphic input tools. Computer Graphics 12,3 1978.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 PROTOTYPING AND SIMULATION TOOLS FOR USER/COMPUTER DIALOGUE DESIGN Paul R. Hanau &#38; David R. Lenorovitz 
 Martin Marietta Aerospace Denver Division Denver, Colorado ABSTRACT The design and development of 
user interfaces to interactive computer systems is enhanced by permitting designers to easily express 
their design concepts in concrete, comprehensive, and comprehensible working models. A set of prototyping 
and simulation tools has been developed to be used as an integral part of the specification and design 
process. These include an interactive display building utiity and a syntax-driven interactive dialogue 
controller. The display builder is used to develop initial conceptual snapshots of system diplay appearance 
at selected points in the user/system dialogue. The dialogue controller interprets a grammatical description 
of input tools and system logic, using predrawn and dynamically constructed displays to simulate the 
 external appearance of the desired end system. keywords: computer graphics, human interaction, user/computer 
dialogue, input tools, interactive techniques, programming languages, system design CR categories: 
8.1, 8.2, 4.22, 4.12, 4.13 INTRODUCTION The number and complexity of interactive computer systems is 
continuing to expand across a wide range of application areas. In systems developed for military, aerospace, 
commercial, engineering, and manufacturing applications and in day-to-day personal activities (such as 
banking, calculators, and home energy management), more and more people are coming into Permission to 
copy without fee all or part of this material is granted provided that the copies are not made or distributed 
for direct commercial advantage, the ACM copyright notice and the title of the publication and its date 
appear, and notice is given that copying is by permission of the Association for Computing Machinery. 
To copy otherwise , or to republish, requires a fee and/or specific permission. Permission to copy without 
fee all or part of this material is granted provided that the copies are not made or distributed for 
direct co~ercial advantage, the ACM copyright notice and ~980 ACM 0-89791-021-4/80/0700-0271 $00.75 
271 direct "conversational" contact with computers. These systems are characterized by the ways in which 
their users interact with the computer to jointly accomplish system goals. Because of the interdependent 
relationship of the human and mechanized components of such systems, designers must be particularly sensitive 
to operational characteristics which affect the people using the system [16]. The design and development 
of interactive systems has typically suffered from the inability of designers to clearly visualize and 
easily express their designs as concrete, comprehensible models. The need to rely initially on pencil 
and paper descriptions of what are later to become dynamic interactive systems represents a major communications 
problem. Conventional specification media do not provide designers with an adequate means for conveying 
design alternatives to customers, intended users, or system implementors, making it difficult for any 
of them to fully appreciate and critically evaluate what is being proposed. This, in turn, reduces the 
likelihood that the designers will receive sufficient feedback with regard to the adequacy and appropriateness 
of the proposed design, or that implementors of a system will properly understand the intent of the designers. 
In either case, a poor implementation is likely to result. Once a system has been implemented, attempting 
to correct deficiencies via isolated patches or piecemeal design changes is an inefficient, costly, 
and technically risky process. Previous work in this area [10,13,17] indicates that the variability 
of the human component of interactive systems makes it difficult to design such systems by standard "cookbook" 
techniques. Empirical evaluation and iterative refinement are therefore essential components of the design 
process. Since time, resources, customer confidence and marketing factors often preclude "going back 
to the drawing board", there is understandably intense pressure to "get it right the first time"; design 
iteration must be confined to the design phase. AN APPROACH TO THE PROBLEM Our research efforts have 
centered around the creation of flexible, user-oriented design tools for interactive computer system 
development. The environment supported by these tools has the following characteristics: the title of 
the publication and its date appear, and notice is given that copying is by permission of the Association 
for Computing Machinery. To copy otherwise, or to republish, requires a fee and/or specific permission. 
 System design team members are able to construct simulations without heavy dependence on persons with 
specialized programming knowledge. This feature serves to lessen the time and difficulty in going from 
the initial conception of an idea by the designer to the realizatlon of that idea on an appropriate display 
medium. The simulation environment should be the primary medium for communicating design ideas. The 
specification language in which system designs are expressed (for documentary purposes) directly drives 
the simulation controller. This allows specifications to be verified for completeness and correctness 
by hands-on use. The design environment encourages a way of structuring applications which allows for 
easy exploration of alternative user conceptual models [11]. This implies a high degree of modularity 
in specification "code". The simulation mechanism allows the designer to investigate the appearance 
and behavioral characteristics of the system on a wide range of interactive devices. Experimentation 
with such alternatives provides a source of comparative data to support rational selection of deliverable 
operator station hardware. In order to provide a degree of achievable realism (compared to the actual 
system) limited only by the completeness and accuracy of the specifications provided by the designer, 
the simulation designer has access to all device capabilities which a production system using similar 
equipment would have. We refer to this collection'of tools as an Interactlve Dlalogue Synthesizer (IDS). 
The functional structure of the IDS Is depicted in Figure I. A more complete discussion of the design 
philosophy underlying the structure of the IDS can be found in [8]. The IDS consists of several related 
subsystems with the following purposes: * building "display" components for various kinds of devices 
 * specifying dialogue structure, input tools and system processing  exercising the resulting definitions 
to produce a working model of the intended system  Display Design in early stages of system conceptual 
design, detailed determination of application data bases and processing may proceed in parallel with 
or even follow the design of display and control elements. In this case, it Is often helpful to be able 
to directly construct static sample displays with an interactive display building utility rather than 
attempting to simultaneously specify both an application data base and the mapping functions by which 
application data is transformed into display images. Once accepted as adequate to support the user's 
decision making needs, ....... Figure I. Functional Structure of Interactive Dialogue Synthesizer (IDS). 
 these display "snapshots" provide a starting point for both data base requirements analysis and display 
mapping function design. Control oriented displays such as menus and messages can also be designed with 
this kind of utility. Our first version of the display builder utility (hereinafter referred to as BUILD) 
provided a modest set of capabilities on a small set of devices. With it, a designer could interactively 
draw, reposition, and copy entities such as connected line groups, circles, and text strings on a calligraphic 
refresh CRT (a DEC GT-42). Items could be tagged with properties such as blink, line texture and intensity, 
and lightpen sensitivity. Displays could be saved on disk, recalled, edited and combined as backgrounds 
and overlays. Although displays were saved in the device-dependent format of the main CRT device, they 
could be routed at scenario execution time to either of two other supported devices (a plasma panel or 
alphanumeric CRT) if display contents were limited to a subset of capabilities supported on that device. 
 The current display builder (called DRAW) is based on the GPGS device-independent graphics package [6] 
running under RT-11 on the GT-42. Its graphical primitives include lines (optionally constrained to be 
vertical or horizontal), text strings, and attributes such as line texture, intensity, and llghtpen sensitivity. 
DRAW also supports the use of user defined picture parts as master images with translation, rotation 
and scaling transformations applied to each instance independently. The device-independent picture segments 
and libraries supported by GPGS are used to save these images for later display on any of the devices 
supported by GPGS. Although the pictures so constructed are static in the sense that they are not tied 
to a modifiable data base, they can be combined and transformed at scenario execution time to provide 
many of the effects of parameterized graphic data structure. Figures 2,3,4 and 5 depict some of the 
kinds of displays which have been built with BUILD or DRAW. Figure 6 depicts a typical display building 
exercise using DRAW. The menu of available masters occupies the left and right margins, while the center 
area serves as the primary drawing area. The dotted box delimits the master being instanced. MS3 ~ ALONG 
.TRACK MS2 MSI Eli/3 ( f f--- Figure 2. BUILD -Produced Display (Naval C 3 Application). L]DCIR -OH 
C~:~LIBR~T]ON -~:IL]GNMENT RDY  I ALIGNMENT ((~F/~Y) uP C~L DmTP rUT~ I PITCH e.l. ~IM]'T E)P :HER 
[N~E~ RET~ ]N~T CUR- ;.TART STOP OPT SOR CONT P ..... T ~, Felt BI~CK 'Tee TnB ~ -~ FOR BPOK 2 DETECTOR 
~LIBRAT]ON ( OFF/ONARDY ) OFF L IDAR --OH DICTA Up Cf'I-omA I ru~ S~TVS R~NCE LIMI'TS -9~M LIMIT SET 
H I~ iNTER RErF~ INST2 RESCLUT]ON 2 ~ E~ CHECK s i N~BRV~L e.2 MINUTES CUR- 3 THRESHOLD ~TART STOP 
OPT SORD ISP~Y HOE S'-9 I~ I I CONT P~X3E E C I E FOR encK 0 \ , T~8 N BOOK .T "\ e .... J ~LT]'TUDE 
-K]LOPE'TEI-T3 VERIFY D~T~ -CONSLIL'T FL](3NT R-AN FOR NEXT ~E~ATION Figure 3. BUiLD-Produced Display 
 (Shuttle Payload Application). F IRE O ISPATC H E~'~LU~TE RESET F Mr~ -3 i Fi'~ -1 C~ i........ ~' 
g .......... ._.___ ....  RMS EI~CIR ; "'""",. NOrilNIJL PREDICTION ' "'"'''''''/"" IN ,S -- METI~S 
I I I I I I I I I 17ele I I I I 210 '1 laae 2U HOUR 'TIME RCIN~ ERROR VE~R~JS TIME,SW S/e~ Q SV1 M91 
pf~C~aE 2 HI=IS T(~O HCINY ~qD ~TA ]TEi~ .............. y '~° ,~, ,=o I I I i sv,~s, ~E> ~ ~ ........................................... 
~N~ EST PRED MES UA.. SV1/H~ ............................ BIRD NOT IN VIEW .......................................... 
 Sw/M~ ...................................................................... MS BEGIN  SVI/MS8 ........................... 
BiRD NOT IN VIEW ........................................... SV2/MSI .......................... SICK 
BIRD SICK BIRD SICK BIRD ......................... NOV PROCESSING FE£LTH gND S.TP'n.IS DI~LQY SYSTEM 
T]FI~ ~ :aS: 23 Figure 5. BUiLD-Produced Display (Satellite-Based Positioning Application). 1 Ft~-2 
Figure 6. DRAW Display-Building Screen Environment. V~ICE I V~ICE 2 v~Ic~ 3 v~ic~ Figure 4. BUILD-ProducedDisplay 
(Fire-Protection Planning Application). Dialogue Structure and Input Tool Design Dialogue structure 
refers to the sequence of actions which shapes the interactive interchange of information. A simple model 
of dialogues consists of a set of states, each corresponding to one or more displays on various devices. 
Transitions between states are triggered by user actions or system events. The dialogue control structure 
implemented in connection with the BUILD display builder consisted of a table driven sequencer which 
related current displays and specified keyboard, lightpen, or plasma display touch'panel inputs to new 
sets of displays to be routed to various devices, Dynamic updating of real-time displays was simulated 
by tying a sequence of pre-drawn frames to timer events. The way in which input devices were handled 
was statically defined by system code and thus could not be changed by non-programming users. Our more 
recent work has characterized the control of user/computer dialogues as a generalized translation process 
from an input stream to an output stream. Human input to a dialogue may include use of various interactive 
tools such as joysticks, lightpens, or even computer recognized voice commands. System responses may 
include alphanumeric and graphical CRT displays, control of various physical devices (such as a random 
access slide projector), and computer directed synthesized speech. The translation process is specified 
at several levels [5]: * syntactically --the structure of the input stream as a hierarchy of identifiable 
constructs  * semantically --the meaning associated with each construct which allows the simulator to 
correctly interpret and implement it  * lexically --the detailed input, processing, and output primitives 
upon which all dialogues ultimately rest  Given this characterization, we have applied the substantial 
body of knowledge relating to syntax-directed translation and the construction of translator writing 
systems [9,14] to syntax-directed interactive dialogue control [7,10,12,15]. Syntax-directed Dialogue 
Control Translator writing systems (often called compiler-compilers) provide an application-oriented 
programming environment for the production of language recognizers. In general, such systems accept a 
definition of the language to be recognized expressed in some-mete-notation. A commonly used notation 
which has proved useful for describing a wide range of artificial languages uses PRODUCTION RULES to 
specify the set of legal substitutions which may be employed in the course of expanding a starting symbol 
into that set of strings (or utterances, or actions,...) which belong to the language. A particular form 
of production rule notation called BNF (Backus-Naur Form) can be used to describe languages in which 
any given symbol is expanded in one of a fixed set of ways regardless of the symbols which appear around 
it. The resulting class of languages is called context-free. When we apply a grammatical approach to 
dialogue  control, the notion of expanding a symbol into a string of other symbols is replaced by the 
idea that an interactive dialogue is actually a hierarchy of action patterns at various logical levels. 
At a high level of abstraction one might, for example, characterize an interactive drawing dialogue 
somethin~ like begin; do operation until done; terminate  which in our BNF meta-notation appears as 
 <draw> = <initialize> (<operation>) (i 1000) <terminate>  The parenthesized construct represents repeated 
 application of the pattern <operation> as long as it successfully handles the user's actions. Here, 
a legal <draw> dialogue consists of at least I and at most 1000 <operation>s. An action sequence which 
does not satisfy <operation> causes the repetition to terminate, in this case leading to the next pattern, 
<terminate>. Next, we expand the definition of <initialize> to specify the details of the initialization 
procedure for a particular graphics system. In the example given in Figure 7, initialization involves 
allocating local storage for the dialogue main module, initializing the graphics package to be used, 
setting the terminal scroll area and display scale factors, and initializing input tools used by the 
dialogue. The range of <operation>s available in this sample scenario (connected lines, circles, and 
rectangles) is defined by the pattern <operation> = (<poly-line> ! <circle> ! <rectangle>)  where constructs 
separated by "!" are alternatives from which any one may be selected. Connected lines are begun by 
the command "p" and a <locator> input, and each line segment is specified by a command-locator pair. 
The construct <circle>, which allows the user to draw a circle on a CRT by typing the command "o" and 
then supplying the center point and a po!nt on the perimeter eppears as <circle> = "o" <locator> 'mark-center' 
<locator> 'compute-radius; drew-circle'  The "virtual tool" <locator> [1,4,18] is defined as either 
a lightpen hit on a system supported tracking cross, using the keyboard character "?" as a tip-switch, 
or a keyboard string of the form "=x,y;" giving digital coordinate values: <locator> = ("?" 'poll-lpen' 
! "=") <e-list>  Finally, an application datatype called <e-list> is defined as a sequence of real-valued 
expressions separated by a comma, terminated by a semi-colon: ~ <e-list> = <e> ("," <e>) (Z 100) ";" 
 (Here, we have assumed that the graphics terminal's lightpen support software also returns hit coordinates 
in this form.) picture drawing grammar v8.1 (dummy semantics) <draw> = <inltlalize> (<operation> I <quit> 
SUCCEED I <error>)(e 1888) <Inltlalize> = <Inlt-storage> <Init-scroller> < inlt-scale> <inlt-tools> <init-siorage> 
= "Inlt heap storage; allocate local variables-<Inii-scroller> = "set terminal screen scroll area" <Init-scole> 
= "set user drawing area window" <inlt-tools> = "InitiaIize llghtpen; place tracKing cross at screen 
center"  <operation> = (<poly-line> I <circle> I <rectangle>) <poly-line> = "p" <beqin-llne> (<line>l<horiz-line>l 
<uert-i Lne>)(8 188~) <begin-llne> <locator> "save (x,y); move to (x,y)" <line> = '@" <Iocator> "draw 
llne to new (x,g)" <horlz-llne> = m_. <locator> "draw horizontal l lne-segment to (new.x,old.y)" <uert-line> 
 "|" <Iocator> "draw vertical line-segment to (old.x,new.~j)" <circle> 4" <Iocator> "o" <Iocator> "draw 
CIRCLE given center, edge" <rectangle> = i r u <Iocator> <Iocator> $ "draw RECTANGLE given two opposite 
corner points" <quit> = "q" "free heap uariables" <error> -SC';") u;. "eat euer~jthing up to next ";'° 
 <locator> ('=" <Keyboard-loc> I m?. <IIghtpen-loc>) <Keyboard-loc> = "AWAIT-KEYBOARD" <e-llst> 41 ightpen-loo> 
= "GET-LOCATOR-DATA-2" <e-I ist> <e-list> = <e> ("," <e>)(8 18) R;H <e> = "e m Figure ?. Syntax-dlrected 
dialogue control (interactive drawing scenario) The definitional process is carried out to the point 
where all types are defined in terms of other defined types or built-in meta-language types. A Fortran-based 
interactive language design system called LANG-PAK [9] was selected as the basis of the prototype Dialogue 
Controller because of its transportability and availability in the public domain. LANG-PAK was originally 
intended to provide a supportive environment for the design, refinement, and implementation of application 
oriented problem solving and command languages. It consists of a package of subroutines including a table 
driven context-free language parse machine, various symbol table construction and accessing routines, 
and an interactive main program which allows the user to define end experimentally execute translator 
definitions. The syntax of language constructs Is defined in a BNF-like notation (see Table I) augmented 
with semantic actions defined by the user. Compilation of this notation into interpretive parse machine 
code is handled by a grammar itself implemented in LANG-PAK. Once a syntactic type has been defined, 
it may be invoked interactively and a test input string may be supplied. The resulting translation sequence 
is then displayed, and the process of parsing the input against the grammatical definition may be traced. 
 The principal modification required to utilize LANG-PAK as an interactive dialogue controller is to 
allow it to parse input which becomes available incrementally during the course of parslng. In normal 
translator operation, LANG-PAK assumes that the input is available in its entirety at the start of parsing 
and is independent of the course taking by the parser. An interactive dialogue input stream is created 
during parsing (at least partially) in response to the partially completed translation sequence (outputs 
such as displays, prompting messages, etc.). In the current Implementation, the input stream (characters 
received from the graphics terminal, whether solicited or not) is buffered and made available to the 
parser character by character. When the parser looks for an input symbol beyond the end of currently 
received input, the user is prompted to indicate that some sort of input is required in order to continue 
parsing. Table I. Meta-Language Elements m...m(.,.) Literal; a non-null alphanumeric sequence; integer 
attributes contained in parentheses are available to semantic machine. Integer; a non-null numeric sequence,, 
,optionally preceded by m÷= or -. terminated by any non-numeric character. Real number; a non-null numeric 
sequence optionally containing a decimal point, optionally preceded by ,+, or "-% terminated by a non-numeric 
character. St...) Break string~ the input string up to but not including the first occurrence of any 
member of the set of elements contained In the parentheses. FAIL Sgniactlo failure; a parse machine instruction 
which causes the current construct to terminate with failure. SUCCEED Syntactic success; a parse machine 
instruction which causes the current construct to terminate with success. (...) (i J) Repetition; the 
contruci succeeds if the pattern tn parentheses ts matched at least i times; the construct terminates 
(with success) after the Jth match. (...I...I.,.) Alternations the input is matched against each of the 
alternatives In sequence; if any succeeds, then the alternation succeeds, else it fails; can be combined 
with repetition. 4...> Syntactic type; a non-terminal pattern defined by the user; if definition consists 
solely of semantic actions, then this definition can be invoKed as a subroutine bw other semantic actions. 
J I I e Semantic action string; contents depends on semantic compiler and semantic machine In use; action 
i$ carried out ~Jhenevar It Is encountered in the course of selecting patterns to match against input. 
 The relationship between syntax parsing and embedded semantic processing is complicated by the fact 
that "backing up" the parser in the case of a parsing "dead end" cannot reasonably involve semantic actions 
which undo things already communicated to the user by the system. (While a compiler may reasonably "pop" 
symbols added to a symbol table on such a false path, an interactive system can hardly advise the user 
to "ignore all messages since..."). This is not to say that all the dialogues in which we are interested 
need be parsable without backup or are describable by finite state grammars (witness the parenthesized 
expressions of Figure 8); the restrictions placed on backup are solely of a s@mantic nature. The semantic 
actions associated with recognition of syntactic constructs are expressed in one of several notations 
suitable to the current level of detail of the design.  One useful semantics processor accepts arbitrary 
alphanumeric strings as semantic action specifications, stores them in their original form, and simply 
prints them out as their corresponding syntactic constructs are processed during scenario execution. 
This provides a method whereby a dialog can be specified at a very high level semantically, while syntactic 
(structural) checking of input sequences is carried out in full detail. The logical flow of the dialogue 
can thus be verified before time is spent working out the details of semantic implementation. The previous 
example and Figure 7 employ this kind of notation. A second semantic notation makes general-purpose 
computational capabilities available to the dialogue designer through a general purpose reverse Polish 
notation suggestive of a programmable hand calculator. Working storage is available in the form of a 
pushdown stack and a file of randomly addressable "registers". This notation was selected for initial 
use because of  Figure B. Syntax-directed drawing dialogue ulih executable semantics (RPH machine) 
~¢~picture drawing grammar v8.1 *** <draw> -<initialize> (<operation> I <quit> SUCCEED $ I <error>)(B 
I888) <initialize> = <inlt-storage> <IHIT-GSP> $ <inlt-soroller> <init-scole> <Init-tools> <Inlt-storage> 
= "AU$1HIT 2 RU$ALLOC" <Inlt-scroller> -"B SCROLLSIZE" <Inlt-scole> -"8 B 1.333 I" <SET-WINDOW> <Inli-tools> 
= "8.5 8.5 IHIT-LPEH" <operation> -(<polg-llne> I <circle> I <rectangle>) <pol~-llne> = "p <begln-line> 
$ (<llne>I<horlz-llne>t<vert-llne>)(8 1000) <begln-llne> = <locator> °save-xg MOVE-ABS-2" <line> = 'W 
 <locator> "line-segment" <horlz-llne> "-" <locator> "J prevY . line-segment" <vert-line> 1" <locator> 
$ ": ; prevX . : llne-segment" <line-segment> = "savelXg LIHE-ABS-2" <circle> = '+" <locator> 'o <Iooator> 
"CIRCLE" <rectangle> = "r" <locator> <locator> $ "2 R MOVE-ABS-2 sub2 : 8 LINE-REL-2 : B : $ LIHE-REL-2 
: CHS 8 LIHE-REL-2 CHS 8 : LINE-REL-2" <quit> = "q "2 AUSFREE" wolo~free heap storage <error> = S( ; 
=) ";" <looaior> = (m= <Ke~board-loc> I ? <llghtpen-loc>) <Ke~boord-loo> = "RLJAIT-KEYBORRD" <e-llst> 
<llghtpen-loo> -"GET-LOCRTOR-DRTA-2" <e-llst> <save-xg> "prevY := : prevX := :" <prevX> -"0 AUSRUTO" 
~ function returning addr <prevY> at AU$AUTO" ~OVX~ of Ith heap variable l ~ololcsubgrammor for llst 
of expressions <e-list> m <E> ('." <E>)(B 18) ':" subgrammor for arithmetic expressions with precedence 
of ~ over -I-- and parentheses. <E> = <P> ("+" <P> "+* I m_ <p> ,_,) (8 1888) <P> = <A> ('*" <A> "*" 
I /" <A> "/') (8 1880) <R> = (<PUSH~REAL>I<PUSH~IHTEGER>I ( <E> ")") semantic routines for 2-elernent 
vector ops <add2> = ": 4 ROLL + 3 ROLL + :" <chs2> = "--: --:" <sub2> = "chs2 add2" <:2> = "4 ROLL 4 
ROLL" Wololcsubgrommor for recognizing reals, integers <PUSH÷REAL> = (<REAL> I "; ;" FAIL) <PUSH+INTEGER> 
= (<INTEGER> I "; ;" FAIL) <REAL> = <SIGH><RL> <INTEGER> = <SIGN><IN> <RL> = "B." (<DIGIT><MPY~ADD>)(I 
188) ".= $ (<DIGIT><DIV~ADD>)(O taB) "*" <IN> = "B." (<DIGIT><MPY~RDD>)(I IBB) "*" <DIGIT>=("I'(I)I"2"(2)l 
3 (3)!"4"(4)I 5 (5)I $ 6,(6) 1.7,,(7)l'8 (8)1"9"(9)1"B"(B)) <SIGH> = "1." ("+" I - "--')(B 1) <MPY÷ADD> 
= "18. *" <PUSH÷DIGIT> "+" <DIV÷ADD> = <MPY÷ADD> ": 18. / :" <PUSH.DIGIT> = "3 T(H)" w,:~4* FGEGSP -- 
Core Standard" constructs :4ok* expressed in terms of FGE package <IHIT-GSP> = "IHIT-FGE" <SET-UINDOg> 
-<SCALE> <MDVE-ABS-2> = "save-cp APHT ~ <MOVE-REL-2> "cpX . cpY add2 MOVE-RBS-2" <LIHE-ABS-2> = "2 
"H cpX . cpY $ sub2 RVECT sove-cp ; ;" <LIHE-REL-2> "cpX . cpY add2 LINE-ABS-2" <RUAIT-KEYBOARD> = 
"RET" <GET-LOCATOR-DATA-2> = "POLL-LPTC" <save-cp> = "cpY := : cpX := :" <cpX> = "990" wolok reserved 
registers <cpY> = "991" wok.w~FGE--BASIC GRAPHIC COMMANDS OF FGE PACKAGE <IHIT-FGE> = "I I 16 @" <SCROLLSIZE> 
= "" I -24 * 4 3 27 ~" <SCALE> = "4 26 9" <e-list> "2 ; :" <IHIT-LPEH> = "2 34 Q" <APNT> = "B -99 4 
2 g" <RVECT> = "B 3 4 35 e" <POLL-LPTC> = "B 39 ~"  its compactness and ease of processing. See Table 
2 Table 2. Defintlion of RPN semantic machine and [3]. One of the instructions of this "programmable 
 pop: returns top element of stacK; delete from stack virtual semantic machine" provides access to the 
 popl. pop2. tosl .... : when operation involves more graphical input and output capabilities of our 
GT-42 than one stack operation, index indicates intelligent graphics terminal through its several graphics 
software packages [2,6]. Graphic function arguments are computed on the semantic machine stack and passed 
to the graphics package executing in the terminal. Operations may return values from the terminal to 
the host computer by creating a formatted string which is parsed by standard grammatical constructs. 
The semantics associated with these constructs places the the values on the semantic stack, where they 
are available for further processing. Figure 8 depicts the same scenario given in Figure 7, with semantics 
rewritten for the postfix semantic machine. The level of detail at which semantics is specified in this 
kind of system is highly variable, ranging from selection among a limited set of very high-level application-oriented 
actions, to more flexible, more general, but lower-level actions. Applied to interactive input tools 
and graphical displays, the alternatives range from acceptance of a limited number of predefined tool 
inputs and display of "canned" graphics (such as might be created using the interactive display building 
utility) to low-level specification of input and output constructs. An intermediate level is one at which 
"canned" display elements are dynamically manipulated and combined using lower level constructs. EXPERIENCES 
WITH THE IDS The IDS, in its various prototype versions, has been used in a number of diverse application 
areas. We have successfully applied these techniques to the design of user/computer interfaces for several 
complex command, control, and communications systems. In one such U.S. Navy application (Figure 2), the 
system being developed had multiple stations staffed at various times by quite different classes of users 
(with correspondingly varied information needs, levels of experience, etc.). Additionally, each station 
offered multimedia display and input devices (graphic CRT, alphanumeric CRT, large screen display, software 
 labeled dynamically changeable function button matrix, keyboard, and trackball). The user-computer 
interface for this system was therefore quite complex, and a large number of alternative design approaches 
had to be proposed, developed, communicated, and evaluated by a wide range of reviewers. The IDS was 
extensively used for all of these purposes and proved to be an invaluable asset. Specifically, it greatly 
enhanced the communication of proposed system designs among team members, project management, subcontractors, 
and customer representatives. It was instrumental in uncovering a number of subtle design flaws which 
otherwise would most likely have gone undetected until final implementation. And, although the early 
prototype version of the IDS used for this application did require a fair amount of user "handholding" 
by IDS development personnel, the bulk of simulation order in which data is used. i.e./ popl is first 
element pop'd, pop2 is second.. push: pushes one argument onto stack tos: returns top element of stack 
without deleting it R(N) push (REGISTER(pop)) IB 20 . <-> 10 R(2B) T(H) push (TRBNSLBTIOH÷ELEMEHT(pop)) 
POP ; discard = 10 20 10  SWAP : a=pop; b=pop; push(a); gush(b). 10 20 30 : <-> 10 30 28 DUP " push 
(tos)18 28 " <-> 10 28 28 DUPN "N duplicate top (pop) stack elements 28 30 48 2 'N <-> 20 30 40 30 48 
ROLL roll top (pop) elements of stock up(+) or down(-) one location 10 20 38 40 3 ÷ <-> 10 48 28 38 
10 28 30 40 -3 ÷ <-> 10 30 48 20 STOR := REGISTER(popI) :- tos2 CHS --push 20--tiP°P)<-> 18 10 IHT 
push (IHTEGERePART (pop)) FRAC push (FRACTIOHRL~PART (pop)) NOT I push (.NOT. (pop)) RBS \~ push (ABS 
(pop)) ADD + push (pop + pop) SUB -push (pop2 -popt) 18 6 -<-> 4 MPY * push (pop pop) DIV / push (pop2 
/ pop1) 18 4 / <-> 2.5 HOD // push (pop2 MODULO popt) EX1:~ ~* push (pop2 ~ pop1) AND &#38; push (pop 
.AND. pop) OR \ push (pop .OR. pop) CALL Q external application call: action~id = popl parameter count 
= pop2 parameters are next (pop2) pop's < push (pop2 .LT. popl) <= push (pop2 .LE. popl) push (pop2 
.EQ. popl) T. push (pop2 .HE. popl) >= push (pop2 .GE. pop[) > push (pop2 .GT. popl) [ if (pop .HE. 
B) then do stuff in brackets [...3 else sKip (balanced) brackets if (pop .HE, B)  then repeat stuff 
in brackets [...3 else exit this level of brackets DUMP tWpe (pop) stack elements on console RET return 
from semantic subroutine  convert to rectangular coordinates RECT radius theta RECT <-> x y POLAR 
convert to polar coordinates: x g POLAR <-> radius theta FAIL return to parser with local swntactlc 
failure GFAIL return to parser with global syntactic failure specification was done directly by designers 
wlth no software skills. Similar results were obtained when the same version of the IDS was applied 
to scenarios involving interactive control of a Space Shuttle experiment payload (Figure 3), and an interactive 
fire-fighting resource allocation trainer (Figure 4). Design concepts for a satellite control system 
are shown in Figure 5. SLIVIMARY An approach to the problem of designing, notating, and implementing 
interactive user/computer dialogues has been formulated. Tools which support this process have been implemented 
and used in real-world applications. The notion of designing any user/system interface totally without 
regard for other aspects of the sytem is unrealistic. Such a process could very easily result in an 
unimplementable design, due to unacceptable processing requirements or need for data which is actually 
not available to the system. However, design must begin somewhere, and beginning the design iteration 
process with the user does have advantages. It ensures the user/system interface the attention and visibility 
it should have, and it provides valuable inputs to other design activities, such as data base and algorithm 
design, by calling out required data and processes explicitly. A truly complete system evaluation simulator 
would integrate interactive dialogue simulations with performance simulations of other components such 
as data bases, processors, and communication links, to provide realistic estimates of crucial interaction 
parameters like system response time. As a first approximation, however, a capability to design user/system 
 interactions within the environmental constraints as best understood at the time is an effective technique. 
 Preliminary experiments with the dialogue design tools we have developed so far have yielded promising 
results. Expanding these prototypes into a production tool will Involve the Introduction of improved 
data-base building support, links to additional input and output devices, and an improved semantic notation. 
Extending these techniques to directly support the generation of production software systems will require 
a capability to compile dialogue descriptions into a language compatible with that being used for the 
 particular production system. REFERENCES [I] Anson,E. The semantics of graphical input. Proceedings 
of SIGGRAPH79, Computer Graphics 13,2 Aug 79. [2] DEC Fortran/RT-11 Graphic Extensions Manual, Order 
No. DEC-II-LRTEA-.B-D. [3] Dunravey, M. The procedural approach to interactive design graphics. Computer 
Graphics 13,1 Mar 79. [4] Foley,J.D. &#38; Wallace,V.L. The art of natural graphic man-machine conversation. 
Proceedings of IEEE 1974, 62, 462-471. [5] Foley,J.D. Graphical output and input capabilities. In Treu,S.(ed.) 
User-Oriented Design of Interactive Graphics Systems (Proceedings of ACM/SIGGRAPH Workshop), Oct 1976. 
 [6] GPGS (General Purpose Graphics System) User's Tutorial. Graphics Group, University of Nijmegen, 
1975. [7] Guthery,S. DDA: An interactive and extensible language for data display and analysis. Computer 
Graphics 10,1 1976. [8] Hanau,P.R. &#38; Lenorovitz,D.R. A prototyping and simulation approach to interactive 
computer system design, in Proceedings of the 17th Design Automation Conference. ACM/SIGDA -in press. 
 [9] Heindel,L.E. &#38; Roberto, J.T. LANG-PAK: An Interactive Language Design System. Elsevier Computer 
Science Library: Programming Language Series; I. American Elsevier Publishing Company, Inc. New York 
1975 [10] Lenorovitz,D.R. &#38; Ramsey,H.R. A dialogue simulation tool for use in the design of interactive 
computer systems. Proceedings of the 21st Annual Meeting of the Human Factors Society, Santa Monlca, 
California: Human Factors Society, 1977, 95-99. [11] Moran,T.P. "How do users understand systems they 
use?". Panel presentation at SIGGRAPH '79. [12] Moran,T.P. Introduction to the Command Language Grammar. 
Xerox PARC Report SSL-78-3. [13] Ramsey,H.R. &#38; Atwood,M.E. Human Factors in Computer Systems: A 
Review of the Literature (Technical Report SAI-79-111-DEN). Englewood, Colorado: Science Applications 
 Incorporated, Sept 1979. [14] Ramsey,H.R. A User°s Guide to the Translator Writing System (TWS). Technical 
Report SAI-77-O67-DEN. Denver, Colorado: Science Applications Inc., 1977. [15] Reisner,P. Using a Formal 
Grammar in Human Factors Design of an Interactive Graphics System. IBM Research Report, San Jose, Ca., 
1979. [16] Schwartz,B.K. The man-logic interaction in information processing systems. Proceedings of 
the 16th Annual Meeting of the Human Factors Society, Santa Monica, California: Human Factors Society, 
1972, 391-394. [17] Treu,S.(ed.) User-Oriented Design of Interactive Graphics Systems (Proceedings of 
ACM/SIGGRAPH Workshop), Oct 1976. [18] Van den Bos,J. Definition and use of higher level graphic input 
tools. Computer Graphics 12,3 1978.   
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1980</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>807505</article_id>
		<sort_key>279</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-1980</article_publication_date>
		<seq_no>46</seq_no>
		<title><![CDATA[3-D transformations of images in scanline order]]></title>
		<page_from>279</page_from>
		<page_to>285</page_to>
		<doi_number>10.1145/800250.807505</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=807505</url>
		<abstract>
			<par><![CDATA[<p>Currerntly texture mapping onto projections of 3-D surfaces is time consuming and subject to considerable aliasing errors. Usually the procedure is to perform some inverse mapping from the area of the pixel onto the surface texture. It is difficult to do this correctly. There is an alternate approach where the texture surface is transformed as a 2-D image until it conforms to a projection of a polygon placed arbitrarily in 3-space. The great advantage of this approach is that the 2-D transformation can be decomposed into two simple transforms, one in horizontal and the other in vertical scanline order. horizontal scanline order, Sophisticated light calculation is also time consuming and difficult to calculate correctly on projected polygons. Instead of calculating the lighting based on the position of the polygon, lights, and eye, the lights and eye can be transformed to a corresponding position for a unit square which we can consider to be a canonical polygon. After this canonical polygon is correctly textured and shaded it can be easily conformed to the projection of the 3-D surface.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[2-pass algorithm]]></kw>
			<kw><![CDATA[Bottleneck]]></kw>
			<kw><![CDATA[Foldover]]></kw>
			<kw><![CDATA[Scanline algorithm]]></kw>
			<kw><![CDATA[Spatial transforms]]></kw>
			<kw><![CDATA[Stream processor]]></kw>
			<kw><![CDATA[Texture mapping]]></kw>
			<kw><![CDATA[Warping]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Hierarchy and geometric transformations</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.4.1</cat_node>
				<descriptor>Scanning</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010506</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Document scanning</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010240.10010244</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision representations->Hierarchical representations</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P330051</person_id>
				<author_profile_id><![CDATA[81100160637]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Ed]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Catmull]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Lucasfilm Ltd., P.O.Box 7, San Anselmo, CA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P15765</person_id>
				<author_profile_id><![CDATA[81100078209]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Alvy]]></first_name>
				<middle_name><![CDATA[Ray]]></middle_name>
				<last_name><![CDATA[Smith]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Lucasfilm Ltd., P.O.Box 7, San Anselmo, CA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>507101</ref_obj_id>
				<ref_obj_pid>800248</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[James F. Blinn, "Simulation of Wrinkled Surfaces". SIGGRAPH Proceedings, August 1978, 286-292.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Edwin Catmull, "Computer Display of Curved Surfaces", Proc. IEEE Conference on Computer Graphics, Pattern Recognition, and Data Structures, Los Angeles, May 1975.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Steven A. Coons, "Transformations and Matrices", Course Notes No. 6, University of Michigan, Nov. 26, 1969.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[A. Robin Forrest, "Coordinates, Transformations, and Visualization Techniques", University of Cambridge, Computer Laboratory CAD Document 45, June 1969.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 3-D TRANSFORMATIONS OF IMAGES IN SCANLINE ORDER Ed Catmull and AIw Ray Smith Lucas film Ltd. P.O.Box 
7 San Anselmo, CA 94960 ABSTRACT -Currer/tly texture mapping onto projec-tions of3-D §urzaces is time 
consuming arK:l sub3ect to consiaergDle a&#38;iasing errors. Usually ~e pro-cesure is to perrorm sane 
inverse mapping from the area or tne pixel onto the surface texture. It is airricult to do tnis correctly. 
There is an alter- nate approach where the %exture surface is transrormea as a 2-D image until it conforms 
to a projectior! or a polygon placed arbitrarily in 3-space. Tne great auvantage of this approacn is 
rmat the 2-D transrormation can be deccmDosed into twp simple . tragsform§, one in horizon%al @nd the 
other in vertical scanllne oraer, horizontal scan-line order. Sophisticated light calculatio 9 is also 
time consumlnq and difficult to calculate co.rrectly c 9 projec£ed polygon.s. Instead of calq/- ±ating 
rme li~htlng Dase(1 on tne Dosition of the polygon, lights, afld eye, the ligh,_s andeye can be nransrorm.~ 
to a corresponqing position for a unit square wnicn we can ccnsiaer to be a canonical Do- J_ygon. After 
th,is canonical polygon is qorrec~!y t~xturea ana snaaeu it can De easily conrormed to the projection 
of the 3-D surface. KEY WQ~S AND PHRASES: texture mapping, scanline a.lgoritnm, spatial transforms, 
z-pass algorithm, stream processor, warping, bottlenecK, fold6ver CR CATEGORY: 8.2 INTRODUCf ION even 
though the data is sent through " the processor in s canllne oraer oniy;~ .wnile this concept has been 
Known ~or some time [J,4j, we snow nere ~nat the technique can be generalized to perspective ~rojectiogs. 
Further generalizations incluae..qug- grlc ann Dlvarlate curve(~ surraces. Tne aD111ty to transzorm a 
wnole raster image very quickly lets us consiaer aoing snaaing calculatlons on a unit square wnere tne 
calculations may be more amenable to stream processing and than transrorming the results. b~ren we 
say "scanline order",_ we use a . slightly oaaer meaning, tDan no[real. Usually ~nis means that the oraer 
or rme pixels is rrcm l elt to right across a scanline and that the scanlines come in top to bottom order. 
We broaden the definition to include vertical scanline oraer. In addition, tne scanlines may al~o occur 
in bottom to top or right to left oraer. ThiS gives us trivially a 90 degree rotate and flopping a picture 
over in one pass rnrough the picture. EXAMPLE: SIMPLE ROTATION For illustration, we present the simple 
case of ro- tation. We would like to rotate an entire image in the framebuffer. The rotation matrix is: 
 cS] ~eeXture m@ppin@ is an imensely powerful idea now ing ex~ip~tea in computer ' graphics. It was 
rirst velopeg by. one or rme aurmors [z~ and extended by Blinn [i] wno proaucec some startling pictures. 
In ~nis pape r we present a new apprcach to texture mapping , that is potentially mucn raster than previ- 
ous tecnniques ana nas fewer problems. The two chief difficulties have been aliasing f~a the time it 
takes to do the transformation o ,i cture or/to the projection of so.me patch. Us.ually ne proceaure 
~s rt~ 2 perform some inverse mapping or @ ~ixel onto a surrace texture (Fig. i): It is airricult to 
do this correctly because tne inverse mapping does not happen in scanline oraer ann also because we must 
integrate under the whole inverse image in oraer to prevent sampling errors. We present in this paper 
all approach that des the [napplng ~n s canline oraer ootn in scanning rme tex- ture map arn in producing 
the projected image. Frocess~ng pixels in scanline oraer allows us to specify hardware that may work 
at video rates. . We empnaslze, nowever, that the approacn is valuable for software as well as hardware 
Lmplementations. One of the key concepts we use is that of a "stream processor". Plxels enter the stream 
processor at via eo rate., are. mouiriea or merg~ea in sane way with anotner ~ncom~ng^§tre~m or pixels 
an~ then sent to the oqtput (Fig. z;. ThiS. concept has. been i~ple- mentec Dy several manurac~.urers 
for image process- i ng. a generaliz@tion or tne concept would be to allow the rrameDurrers to feed the 
streams in ei- ther horizontal or vertical scanline order. We will show here that the class of transformations 
that can be ar~lied to streams is much broader than Rreviqusl~ believed. For exgmple , an image in a 
rrame Duffer may De rotated by some arbitrary angle Permission to copy without fee all or part of this 
~terlal is granted provided that the copies are not made or distrlbu~ed for direct commercial advantage, 
the ACM copyright notice and 0]980 ACM 0-8979]-02]-4/80/0700-0279 $00.75 279 where x and y refer to 
cpordinates in the original picture and x', y' are tne new coordinates (c=cos, s=sin) . We want to 
transforna every pixel in the original picture. If we nold y constant ana move along x tnen we are transrorming 
the data in scanline order out the results are not coming out in scanline ord- er. Not only is tnis inconvenient, 
it is also dif- ricult to prevent aliasing errors. There is an alternate method for transforming all 
of the. pixels, and that is to evaluate only Ehe x' in a first pass and then the y' in a second-pass. 
 So again hold y constant, but just evaluate x': [x' y] = [cx-sy y]. We now have a picture that has 
been skewed and scaled in the x direction, but ever Z pixel has its original y value. Se_e Fig..4, where 
~i~.~a is the original picture and ~ig.~o is after ~ne norizontal scanline computation. Next we can 
transform the intermediate picture by holding x' constant and calculatinq y. Unfor- tunately, the equation 
y' = sx~.y can't b~ used be- cause tne x value for that vertlcal scanline is not the right one for the 
equation. So let us invert x' to get the correct x. We need x in terms of x'. Recall x' = cx-sy, so 
 x = x'/c + sy/c. Plug this into y' = sx + cy to get: y' = (sx' + y)/c. the title of the publication 
and its date appear, and notice is given that copying is by permission of the Association for Computing 
Machinery. To copy otherwise, or to republish, requires a fee and/or specific permission. Now we transform 
the y value of the pixels in the intermediate picture in vertical scanline order to get the final picture, 
Fig.4c. The first pass went in horizontal sca[~line order on input and output. The secon~ was vertical 
in both. SO in two passes the entire picture was rotated. Before we generalize, two points should be 
noted. (i) A 90 degree rotate would cause ~e intermedi- ate picture to collapse t 9 a line. It would 
 be be~:ter to read the scan!ines horizontally from the source buffer an~ write them verti- callv to 
effect that rotate. It follows that B0 degree rotate should be performed by first rotating 90 ^degrees 
@s noted then by -iD aegrees using the z-pass algorithm. (2) The rate at which pixels areread from the 
in- put buffer is generally aifferent than the rate at which they are sent to the output buffer. If we're 
not careful we could qet sampling problems. However, since all of £he ~ixels pass through th.e processor, 
it is not airricultto filter ar~ integrate the incoming values to get an output value. Next we generalize 
to: [x' y'] = [X (x,y) Y(x,y) ]. This generalization/ will include perspective. Whatever the translormation 
is. we shall show that we cai/ do the x transforms firs£__followed by the y transforms, but in order 
to do the y transforms we must be able to find the inverse of x'.. This may be very difficult to ~o ana 
x" may even nave multi- ple values. So we present first a more ~ormal way of talking about the mer~on 
before a~x~ressing some of the difficulties. THE 2-PASS T~CHNIQUE Wb~ are interested in mapping the 
2zD regi~ bounded a unit ~quare int/) a3-D surrace wnicn is pro- jected back into 2-D £or final viewing. 
Since tne unit. square ~ny wnicn we mea Dtne enclos~es points also~ may n 9 r e~presentea Dy pozr/t samples 
in a az- gi~l £rameDurre.r, an~ since a rrameDurrer is typi- cally arrange~ zn rows an~ ~olumns. we ar 
9 in- terested in row-or~ere~ or colt~nn-ordered implemen- tations of these mappings. The technique 
we now present is a means of decomposing a 2-D mapping into a succession of two I-D maDpings, or scan 
line-ordered mappings, wnere a scab line may be eitner horizontal (a row) or vertical ~a coltmm). Th 
9 technique is quite general as we shall show subsequent 19. This figure illustrates the 2-pass technique: 
f(u) &#38;,;v) [ g (v) r uv  (x',y') want to ~ap the set of points u,v) :0,<u<l,0<v<l) zn the unit 
@quare into the set x'.~y') T Where lfhe desired mapping is given as an arDzrxary pair of functions 
x'=l(u,v)  y'=r (u,v) . We wish to replace this pair of functions with the pair  x =flvU I  whe.re 
it is understcxxl that f(u) is applied to all poln~ in the unit ~/uare befgre g~v) is applied to any 
o .them. we call the a~licatzon of ~the n- pass {rot horizontal) and %he application of g the v-pass 
(for vertical). In general, there will be a different f(u) for each value of v, so f might be thought 
of as a function of (u,v). Weprefer however to rmink of v as a parameter which select§ a particular f(u~ 
to be ap- plied .to all u on scan±ine.y..:lb, esi0nas.lze wnen. v zs Dezng nela constant J.lKe tnzs, 
we will use tne notation 99. Thus 99 is an index into a table or horizontal n~a/~oings..Similarly,, there 
will in gen- eral be a airrerent g ~v) for eacn vertical scanline x' (where the Drim~ indicates that 
the h-pass has alreadv occurred}. We will use the notation ~' to indicca%e a given vertical scanline 
just prior to the v-pass. n this section, we will always have the v-pass llow the n-pass. ThiS is just 
a convenience. e gecomposition into the other oraer .proceess similarly, ana we will nave occasion to 
choose one order over the other in a later section. An algorithm for the decomposition of l,r into f,g 
 is the following: (i) f(u)=l(u,99) is the function f for scanline 99.  (2) Solve the equation l~u,v)-~'=0 
for u to obtain  u=h(v) for scanline x'.  (3) ~Iv)=r(h(v),v) is the function g for scanline  x- 
~ simply take f(u) as defined in .(i~ and show that ) in (3) is consistent, wirt~ it. '±ne n-pass es 
tDe set or points l[urv) l into tne set x',v)~. We desire a functlon which may De ap- plied at this 
time to scanline ~'. But being given x' is equivalent to being given the equation ~':I (u,v). If this 
eguation can be rearranged to have the form u=h(v), £hen r (h(v),v) is a function of v only and is the 
desired g. us solving the 9quation l[u,v)-~'=0 for u is the y to the t.~h~ique, we snall snow some cases 
ere tnis is s'~le, put in genera± it is not.. An iterative solution sucn asproviaes Dy ~ewton- Raphson 
iteration could be used-but is expensive.We shall treat these problems in the following sec- tions. It 
should be noted that we haveplaced no restric- tions on functions l,r. So the 2-pass technique can 
be applied to a larqe class of ~ picture t ransform@%ipns and distor£ions, only a few exam- Dies of 
wnicn will be presented here In particu- lar, we henceforth restrict our attention t~ ratios of ix~lyncmials. 
 We shall illustrate the 2-pass technique by apply- ing it, it/ detail, to the case or a rectangle un- 
 aergoing ar~ine transformations followed by a per- spective transrormatiqn .~ projection into 2- space. 
inen, in less aetaii~ we will . treat Dil-- inear and b iquasratic patcnes unaer tge same. type of 
transrormatlon. 'inis snoula serve to inazcaue how the method can be extended to higher degree surfaces. 
THE SIMPLE RECTANGLE Consider the (trivial) parametric representation of a rectangle given by x(urv)=u 
L y(u!v)=v, z~u,v)=0, w(u,v)=l. The class or transformations we aDDIV are exactly those which can be 
represented by a-~x~ matrix mul%iplyir!g a. 3-space vector represen%ed in nemogensous coor~lnates as 
inalcatea below: [x y z w] a~ Then projection into 2-space is accqmplished by di- viding through the 
homogeneous coordinate w-: [x' y' z'] = [x"/w" y"/w" z"/w"]. p lacin~ x, y&#38;.z, and w with their 
parametric orms ana expanding the equations above gives x' --(au+bv+d)/(mu+nv+p) = l(u,v)  y' = (eu+fv+h) 
/ (mu+nv+p) = r (u,v).  We are interested in the 2-D projection only so we shall ignore z' from here 
on. The functions l,r in this ca§e represent an ordi- nary linear transformation or. rzg.e unit ..~..are, 
.rpl- lowed by a perspective .pro3ectzon.. ~otice tnat ~.ey are both ratiqgal l%n.ear pgiynomlal§ - i:e¢, 
a linear polynomial dlvlflea Dy a linear polynomial. Applying the 2-pass algorithm to the functions 
l,r gives: (I) The h-pass function for scanline 99 is f(u) = (Au+B) / (Cu+D) where A=a, B=b~+d, C=m, 
D=n99+p. (2) u=h (v) is obtained by solving ' = (au+bv+d) / (mu+nv+p) for u: u = Ev+F where E=(b-n~')/(m~:'-a) 
and F=(d-p~')/(n~'-a). (3) Thus g (v) = (e (Ev+F)+fv+h) / (m (Ev+F)+nv+p) = (Gv+H) / (Iv+J) is the 
v-pass function for scanline ~', where G=f+eE, H=h+eF,I=n+mE, J=p+mF. Fig.5 shows the results of applying 
this f,g ~pair. Fig.5a is the original rectangular texture. Fig..5b is its a~pearance after the n-pass, 
and ~ig.~c is the resul% of the v-pass. Following are several points about this computa- tion: (i) 
The s_~led image (Fig.5a) was recqnstr~tt~t with a first-order ril~er [~ne so-ca±J.es ~ar~ e window) 
then resangpled with a zergth-prder" filter (the Fourier windSw). This is only mlnlma± use or sampling 
th~eory. Apiece of hargware . or software ror proguc~lon qk]allty wer. K woula cert@in±y employ more 
sopnisticated r i±ter ing. uur figures lOOK surprislng±y nice . aespite use or tne ±ow-order fil%ers 
mentioned above. ~'rne edges are not an- tialiased, however. ) (2) Clipping is natt!ral. The f function 
generates final value or x'. If this value sh6uld fall  outside the limits of the output buffer then 
it does so with no loss. The g function, which operates only on the scanlines ougput by f, will never 
nees values clipped in the n-pass. (3) The 2-pass technique does notavo!d the ordi- nary problems of 
perspective p ro3ectlor/s:_ For ex- ~mio!e, the tr~gsrormation can DIow upzr tne deL~- inator or eisner 
r or g goes ~o zero. '~nzs corresponds to the usual problem or wrap@rgund through infinity ana requires 
the norma± solution of cllpping before transformation. (4) There is a problem introduced ~ the 2-pass 
technique not encountered before. This iS wnat we call the "bottleneck problem". We shall discuss this 
in greater detail and offer a solution to it in the nex% section, then return to the examples. BOTTLENECK 
 3. rotate 90 degrees and transform x first  4. rotate 90 degrees and transform y first In each case 
the area is easily found by integrat- ing the area between x' (0,y) and x' (l,y] where x' = (ax+by+c) 
/ (dx+ey+f) and y varies from 0 to i. This gives  area = K*in (l+e/(d+f)) - k*in (l+e/f) where K-- 
((ce-bf) + (ae-bd))/ee~ and k-- (cd-bf)/ee. We use the method that gives the maximtm~ intermediate area. 
THE BILINEAR PATCH The preceding class of tran§form@tigns of the rec- tangle does not generate al~ quaari±atera±s 
- e.g., nonplanar quaarizateral§. ~ince in general we can- not guarantee tnat a.ll quaarllac~rals are 
planar, we generalize to the pzlinear patc. The general bilinear patch (Fig. 3a) has a parametric representation 
 x(u,v) = a~k I~ [u l]raoo al0 a  where aO0= (x3-x2) - (xl-xO) r aOl=xl-x0, al0=x2- xO, all=x0. There 
are simllar repr.e.sentations for. y(u,v), z~u,v) , and w(u,yl, wnere p13, c13, ,ana al 3 corres~x)na 
respective±y to tne al 3 for x[u,v). As in the preceding example we tran§form a. bilinear ±patch with 
a 4x4 matrix multiply ro±lowed.Dy a. pro- 3ection into 2-sfpace. Hence we snall again ignore z' (but 
see discussion of foldover ~elow). The transformation may be represented by the following matrix equation: 
 [x" y" z". w"] = [xy zw] a T la01 b01 c01 d011 ~ m = lal0 bl0 cl0 dl01  ~ll bn cn dnJ ~ 1  After 
the homogeneous divide x' = (Auv+Bu+Cv+D) / (Muv+Nu+Ov+P) = 1 (u,v) y' = (Euv+Fu+Gv+H) / (Muv+Nu4Ov+P) 
= r (u,v). The 2-pass algorithm gives: (i) f(u)=(A'u+B')/~C'u+D') for scanline 99, where A'=A99+B, 
B'=C~+D,C'=Mq+N, D'=O~+P.  (2) For vertical scanline ~', it can be shown that  g (v) = (A"w+B"v4C") 
/ (D"vv+E"v+F") ~,~re ,A"= ,~' +GG~,, ,B"=~, '+~,'+~ '+HG~,, C, "~, '+HH' , D =ME +OG . E =MF +NE +OH 
+PG,, ~F, =NF +PH with E'=C-O~', F'=D-P~', G'=M~'-A, H =NR-B. With the perspective transformation we 
have a p rob T lem anal.ogous to that of the ~u gegree rotate, ~nat is, it is ix2ssible to have an 
intermesiate picture coll@pse. In the case or rotation the so±union was simple: rotate the texture 
90 degrees and .char/ge the tranrormation by that amount. The solution ror the per~tiye case is the 
sane. however it is more airricu±t to tell from the £ransformation ma- trix when a problem will (x~zur. 
 We base our criteria on the area of the image in the intermediate picture. There are four possible 
 ways to generate an intermediate picture: I. transform x first 2. transform y first Fig.6 shows a 
plarLa ~ b ilinear ~atch .represent.ing the texture in ~ig.~a twistea about its center point. For this 
par£icular example, the h-pass is %he identity £unction f(u)=u and hence is not shown. Fig.7 shows the 
h-pa3ss and v-pass .f°r a nonp!anar patch transror.ma%ion..Note tne rolaover. Fig.Da is tne source texture 
again. All of the considerations discussed for the simple rectangle apply here also. In addition we 
have new problems in%~oauc.~ due to the higher complexity or the surface. A bilinear patc.h may be n 
onplanar,.so from some views, it may be dounle v aluea. '±nat is, a line from tne viewpoint throuqn tne 
surraoe may intersect the surface ~wice. In £erms of the scan- line functions, g(v) can map scanline 
R' back over We call this problem "foldover". It occurs at a silhouette' edge of. the. ~projected surface. 
The solutlon Is to con~mlte z- for v=0 and for v=l. The ~a~rPOZnt of ~.anl.ine ~' which maps into the 
z' artnest from the is transformed first, so that later points overwrite..points that would be ob- scu.rea 
an y~;ay. ~gr antzaiz@sing purposes, the lo- catzon or tne rola.over point must be remembered and an 
apprgpr~ate. Fezg,ht go~puted for combining the p~xel~nere wztn a DacKgrouna. THE BIQHADRATIC PATCH 
 ~j cgfr.es~x)nding to the,point of foldover occurs. rom tnls p9%nt or/ all. u~ls are stored zn only 
the other .Jtocat~on rrmneDurrer ana tne corresponaing intensztles zn the other intensitv franebuffer. 
Toe f lnal image is a combination of-the two inten- szty rr~buffers. In general, we believe this to De 
a airricult hidden surface problem and do not treat it further here. e simplification produced by the 
addition of the ree extra rramebu]~fers reduces the gi (v) to  The highest order patch we shall discuss 
here is the Di~Uadratic pa%ch (Fig.3b). It is particularly interesting bec@use surf.ace patches on 
quadric sur- races . [e.g.~ ellips~ias; may De represented as bi- guaarat.lc p@tcnes. ".in9 parametric 
equation or x for a Dlquaaratlc patcn nas form x(u,v) = [uu u i] [a00 a01 a02] vT] [a20 a21 a22J lal0 
all al 21 U and similarly for y(u,v), z(u,v), and w(u,v). [x" y" z" w"] = [uuvv uuv uu uvv uv u vv v 
i] [A0 B0 C0 DO] IAI B1 Cl    /A B2 C2 8 B8 C8 t can be shown, in a manner analogous to that used 
or the. bilinear .patch, that f~u) is a ratio of quaaratzc polynomials ana g Iv; is a ratio of 4th-~/egr~. 
polyno~t~azs. Act^ually there are two v-~ss runctlons -say gi (v~,. i=u or± - one c orres~x~nHing to 
each or two solutlons or a ~uaaratic equation encountered in the aerivation. THe ract tha£ there ~e e 
tw 9 v-pass functions requiresan explanationo. now turn to ~nis ana other consiaerations wnicn have been 
added because of the introduction of i higher degree surfaces. irst, we present a technique for reducing 
.gi (v) rom a ratlo of 4th.~egree rorms to a rational qug~ ratic polynpmial like r[u). Prestm~ablv 
we ooula i~lement the gi (vl a§. they stand. ~owever~ be- sines being .gomputat~ona~ly nasty~ tney are 
dzffi- cult to . znterpret ana nen c~ nzqe many ~itfalls. ~9r.exa~ple, tne rolaover problem aiscussea 
in the pl±znear case could (x:cur Three tlmes in a scanline with attendant an tialiasin~ problems. We 
prefer to Introduce a metn~x~ wnicn__ at the cosT: of more m~ory, greatly reduces foe oormplexity of 
the gi~v~. It q@n also De applied to the szmple rec- tangle.and bilinear patch. Its utility comes how- 
ever zn extenaing rnge metnoas of ~his paper to nzsner ~g,ree - e.g., to ~ne transrormation o~ bi- CUbic 
patcnes in perspective - which we reserve for a future paper. g(v) --(B0'vv+Bl'v+B2')/(D0'vv+Dl'v+D2') 
where,, for example I B0'=B0ujuj+B3uj+B6 with u 9 ~- z~ optazned, by table, lOOKUp. Tne.problem of whzcn 
gz Iv; .is to.De, uses/ ls replaced w%th the problem of qomputlr/g in tq~ frame Duffers ana solvlng 
the hid- aen surrace prOOzem impliea. Notice that g(v) may cause foldover in both inten- sity framebuff@rs~ 
5ut in any one franebuffer there z s or/zy a single roldover per vertical scanline in- st eaa o.r tne 
triple ro.ldoger implied by the origi- nal gi Iv; ana no auxiliary rr~neDuffers. Wehave claimed that 
the use of addit!on.al memory makes unnecessarv tne aetermination or tne uj c the znverses of x' unaer 
f(u). To make this strzctly true, we must make the following observations. A typicalway to ~mplement 
the function f(u) is to .step algng x' in equal increments (e.g., one pixel inc.rements; ana compute 
the inverse image u. The nelgnDornooa or u is tnen used to compute the in- tensity at location x'. Of. 
cour.se, this defeats the whole purpose of ayQldln 9 znverses,..asstmdng tney can De cGmputea at all. 
We pro~x)se -straign- t aneaa- m@pping, for impl.~.enting f (u) to avoid znverses altogetner. Tne iaea 
nere is to step along u . in equal increments, ccmDuting x'=f(u} after each increment. Let x~ be a value 
o~ x' for whzch we wish to know its znverse image Let ui be values of u at the equal increment pQin~s 
used as sample.s .9,r u...'I Den wnen x~f(ui) is ~ess than x 3, ana x~z÷l) =r{u~l+l]) ls greater tnan 
x3, we eirmer (i) iterate on the interval [ui,u(i+l)] to obtain  the desired inverse image u3, or 
 (2) approximate uj by uj=ui+a(u(i+l)-ui) where  a=xj'-xi '. The figures used to illustrate this Paper 
were gen- erated using tne approximation (2) ~ve. Filter- ing ana sampling require integration of the 
inten- si£y. function of u. This integration requires pgtation at e acn ui, so the cost, if ~tgy, of 
 s traightaneaa implementation is a small additzon to tnat already required. SIMPLIFICATI(INS Much of 
the heavy machinerv in the examples above becomes unnecessary in ~he following ~wo special cases: 
The notion is that during the h-pass we have al- ready cpmputed the u's which we need in the v-pass. 
It is^the reco~putation of these u's .(step (2~ in r99e z-pgs.s" algorithm) whicn makes ~ne v-pass more 
airricult tnan the n-p@ss. ~ .propose a high- precision rrameDurrer [e.g., ID Dzts per pixel) to hold 
the u's as they are compt~ted during the h- pass. Thus, if uj n~aps int.o ~' under f(u~, then at location 
x~' in one framebuffer we store the inten- sity computed fr~ the neighborhood of ujin the source picture 
ana in allot,her r ramebuffer ~%he ~ne w itn nlgner precision; ~e value uj itselr. Then auring t~e.v-pa~ss 
o[~ s canline x3% we merely lookup in tne extra rrameourrer the value or uj mapped b~ ~e h-pass into 
the current pixel, say ~x~,Y].z. on tne vertical scanline. It wzll be th~ u 3 at (x~,y) zn the extra 
framebuffer. A difficulty which arises is that the h-pass func- tion f(u~.can cause a foldover on horizontal 
scan- Iznes: T~zs means that the intensitv computed at i.~, t~on x~ is a function of one o~ two ~ifferent 
u 3 s. 5~ur .sol L/tion isto have t~ auxiliary loca- tlon rrameDurrers ana one a(x~itional intensity 
fr~buffer. 13urin9 the h-pass a scanline is eom- pu ea in an oraer wnere the deepest points are gen- 
erated first, .as discussed in ~he bilinear case. As each u3 zs determined, it is ~/ritten into only 
one or the locatlon rrameDurrers and the .corresponding intgnsity is written into one of the ~ntens~ty 
rramecurrers only. Thzs occurs until the rspective: It is easy to see that the divi-at each output pixel 
is unnecessary in this gas@ -i.e., .~e scanline mappings are polyncmials znsteaa or ratzos or polynomzals. 
 Planar patch: If the patch is known to be 2-D .~dless . of the order of its bounding curve s~ tne re 
car/ De no roldover problem with the rigia-oogy transrormatiqns considered here. (Lines can completely 
re vers.e ~/~rection how@ver (Fig.6) .) hence no.extra rrameDurrers are needed. The prob- lem slmpllfles 
su.bstantlally, becoming a 2-D "warp" or a rectangular texture. f~r example, a plarkgr b iquadratic 
patch under af- ine..projection only, (no perspectlve) has scanline nctzons or form f [u; ~a.uu+bulFc 
and g (v)=dw+ev+f ana can De accomplzsnea zn only one rramebuffer. SHADING a t le who.have .~lement@d 
hidden surface programs sop~.istlcatea I zghtlng mfx~els nave aiscoverea tna ~ne time spent ror rne lighting 
calculation is mucn greate,r rman the time spent solving the hidden §urrace proolem. We p,ropo§e here 
that it may be raster to perform ~ne light calculations on a square canonzcal polygon ana then to transform 
the resui ts. Typically, normals for a polygon are determined and thin interpolated across segments. 
The normal at eacn.pixel zs dotted witn vectors to the lights and eye zn some function to find the 
shading. While framebuffers have been used to store intensi- ties and depth values, they can also be 
used tp store normal values. 'Ine normal values c~n De Kept in a buffer at arbitrary resolution. Tne 
stream processor can then interpolate or approximate those normals to get normals at. a nigner resolutlor/, 
nor- malize them, aot them witn otner streams, or nor- reals, and use the dot pronucts in intenszty 
calcu- latons. Tne approach is: i. Transform eye and lights relative to canonical polygon. 2. The 
can,onical ~x)lygon normal framebuffer is filled witn normals at some resolution (say 4 by 4). 3. Generate 
@ high .resolution array of normals us- ing..cupiq.splires (we used bvsplines) first in the vertical 
cllrsotlonw tnen tne Norlzontal. 4. Normalize the normals.  5. The stream of normals is dotted with 
a stream  of light vectors and/or eye vectors to i~plement the lighting function. 6. The results are 
transformed into position into the final frame buffer yielding the shaded polygon. 7. If we are also 
doing texture mapping, then the intensity of each pixel in the texture is usea as the color in the lighting 
function and the results The approximation of normals with cubic curves can be dS~e in a streamprocessor 
Dy using airrerence equations. Each overlapp!nq set or tour values can De used to .generate a air terence 
equation.witn.a matrix multiply. LrDen the air rerence..equatlon ~ iS USed to generate all or tne values, 
untzl normalz- aticn, x, y, and z may be treated alike and in-epenuent ly. CONCLUSIONS We have presented 
what we believe to be a powerful new way of looking at 3DD surface rendering in conP; puter grapnics. 
It is uaseg on tne ola ~tiq9. or transforming to a canoniqal r Ormw w.nere tne air ri- cult work may 
be perrormea witn relatiy.e ease c tne.n transforming back. The success or tnzs r/otzon in 3-D surface, 
grapnic§ aePenas o.n the ease or realiT zation or tne transformations to ana from canonical form. We 
have shown that a stream processor and l~e 2Tpass~ d~ition technique give a techr/o- ically feasible 
realization of the notion for ern computer grapnics. There is much work to be done to fully el(plore 
thi§ approach. This .paper begins the exxRloratio.r / or tnls territory ang points out several or the 
airri- culties peculzar to it. ~NOWLEZX~V~TS Although we do not know the details of their work, we 
are aware that Larry Evans and Steve Gabriel have been pursuing an apparently similar line ,or research 
anu wish to acKncwlecge them nere. MiKe Shantz and his colleagues at De Anz 9 Systems Inc. have also 
done !naependent work on separable transformations.. "iney nave implemented secono- order polynomial 
coorainate transformations in hardware. ~.e _pho.tographs for this pa~r were prepared ~__Da- vzd DiFrancesco 
at the Jet Propulszon LaD [dFl~). The source picture in all cases was generated by Turner Whit%ed of 
Bell Labs for SIGGRAPH '79 and is used with his permission and that of the CACM. Cqmputinq facilities 
at JPL were generou§ly provid- eg Dy Jim Blinn and Bob Holzman. Facilities were also provided by Tom 
Ferrin and Bob_ Langriage or tne university of California at San Franczsco. [2] Edwin Catmull,,, "Computer 
I)zsplay" of Curved Surfaces , Prcx:. IEEE Conference on Computer Graphics, Pattern Recognition, ana 
Data Struc- tures, Los Angeles, May 197D. [3] Steven A. Coons, "Transformations and Ma- trices", Course 
Notes No. 6, University of Michigan, Nov. 26, 1969. [4] A. Robin Forrest, "Coordinates, Transforma- 
 tions, and Visualization Techniques", Univer- sity of Cambridge~.~mputer Laboratory CAD Do- ct~ent 
45, June l~b~. ~SCanline pixel inverse image of pixel inverse perspective projection exture to 
be mapped Fig.l. Texture mapping. video out Stream Processor T REFER~CES [i] James F. Blinn, "Simulation 
of Wrinkled Sur- Fig.2. Stream processor. faces". SIGGRAPH Proceedings, August 1978, 286-292.   
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1980</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>807506</article_id>
		<sort_key>286</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-1980</article_publication_date>
		<seq_no>47</seq_no>
		<title><![CDATA[Continuous anti-aliased rotation and zoom of raster images]]></title>
		<page_from>286</page_from>
		<page_to>293</page_to>
		<doi_number>10.1145/800250.807506</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=807506</url>
		<abstract>
			<par><![CDATA[<p>Raster graphics images are difficult to smoothly rotate and zoom because of geometric digitization error. A new algorithm is presented for continuous rotation and zoom, free from the disturbing aliasing artifacts introduced by traditional methods. Applications include smooth animation. No matrix multiplication of pixel coordinates is executed. Instead row and column parallel operations which resemble local digital filters are used. This suggests real time implementation with simple hardware. Anti-aliasing is inherent in the algorithm which operates solely on pixel data, not the underlying geometric structures whose images the pixels may depict. Zoom magnification is achieved without replicating pixels and is easily attained for any rational scale factor including but not restricted to the integer values which most existing commercial raster graphics systems use.</p> <p>The algorithm is based on a digitized code for lines on rasters, generalized to an interpolation scheme capable of executing all linear geometric transformations. Samples of images which have been rotated and zoomed by a software implementation of the algorithm are presented.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Aliasing]]></kw>
			<kw><![CDATA[Image processing]]></kw>
			<kw><![CDATA[Large scale integration]]></kw>
			<kw><![CDATA[Linear interpolation]]></kw>
			<kw><![CDATA[Parallel computation]]></kw>
			<kw><![CDATA[Raster graphics]]></kw>
			<kw><![CDATA[Real-time graphics]]></kw>
			<kw><![CDATA[Rothstein code]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.3</cat_node>
				<descriptor>Display algorithms</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.1</cat_node>
				<descriptor>Raster display devices</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.1</cat_node>
				<descriptor>Imaging geometry</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010235</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->Epipolar geometry</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372.10010373</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering->Rasterization</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010583.10010588.10010591</concept_id>
				<concept_desc>CCS->Hardware->Communication hardware, interfaces and storage->Displays and imagers</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P329387</person_id>
				<author_profile_id><![CDATA[81332534996]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Carl]]></first_name>
				<middle_name><![CDATA[F. R.]]></middle_name>
				<last_name><![CDATA[Weiman]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Simulation and Control Systems, General Electric, Daytona Beach, Florida]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>5532</ref_obj_id>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Newman, W.M. and Sproull, R.F. Principles of Interactive Computer Graphics, 2nd Edition 1979, McGraw-Hill, New York.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Vector General Corporation, 3400 System Product Specification Sheets, 1978.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Bunker, W.M. "Computer generation of images, the multi-purpose tool", Proceedings of the Society of Photo-Optical Instrumentation Engineering Program on Simulators, Anaheim, California (March 17-18, 1975).]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Sutherland, I. and Mead, C. "Microelectronics and computer science", Scientific American, Vol. 237, Sept. 1977, pp. 210-229.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Cohen, D. and Demetrescu, S. "A VLSI approach to computer image generation" Information Sciences Institute, Univ. of Southern Calif. (Presented at Interservice/Industry Training Equipment Conference, Orlando, Fla., November 1979.)]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Rothstein, J. and Weiman, C.F.R. "Parallel and sequential specification of a context sensitive language for straight lines on grids", Computer Graphics and Image Processing, Vol. 5, 1979, pp. 106-124.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Weiman, C.F.R. and Rothstein, J. Pattern Recognition by Retina-Like Devices, Computer and Information Science Dept., Ohio State University, OSU-CISRC-TR-72-8 (AD 214 665/2) 1972.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_obj_id>225496</ref_obj_id>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Castleman, K.R. Digital Image Processing, Prentice-Hall, Englewood Cliffs, New Jersey, 1979.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807359</ref_obj_id>
				<ref_obj_pid>800248</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Crow, F.C. "The use of gray scale for improved raster display of vectors and characters", SIGGRAPH '78 Proceedings, pp. 1-5.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Andrews, H.F. "Digital image processing", IEEE Spectrum, April 1979, pp. 38-49.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_obj_id>578095</ref_obj_id>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Rosenfeld, A. and Kak, A. Digital Picture Processing, Academic Press, 1976.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Freeman, H. "On the encoding of arbitrary geometric configurations", IRE Trans. Electron Computers, EC-10, 1961, pp. 260-268.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 CONTINUOUS ANTI-ALIASED ROTATION AND ZOOM OF RASTER IMAGES Carl F. R. Weiman Simulation and Control 
Systems General Electric Daytona Beach, Florida Abstract: Raster graphics images are difficult to smoothly 
rotate and zoom because of geometric digi- tization error. A new algorithm is presented for continuous 
rotation and zoom, free from the dis-turbing aliasing artifacts introduced by tradi- tional methods. 
Applications include smooth ani- mation. No matrix multiplication of pixel coordi-nates is executed. 
Instead row and column parallel operations which resemble local digital filters are used. This suggests 
real time implementation with simple hardware. Anti-aliasing is inherent in the algorithm which operates 
solely on pixel data, not the underlying geometric structures whose images the pixels may depict. Zoom 
magnification is achieved without replicating pixels and is easily attained for any rational scale factor 
in-cluding but not restricted to the integer values which most existing commercial raster graphics systems 
use. The algorithm is based on a digitized code for lines on rasters, generalized to an interpolation 
scheme capable of executing all linear geometric transformations. Samples of images which have been rotated 
and zoomed by a software implementation of the algorithm are presented. Keywords: Real-time graphics, 
raster graphics, parallel computation, image processing, linear interpolation, aliasing, large scale 
integration, Rothstein code. CR categories: 5.13, 5.25, 6.1, 6.22, 6.39, 8.2 I. Introduction Real-time 
computer graphics systems push the limits of hardware capability because of the large and complicated 
data structures required to represent realistic images, the complexity of the computa- tions required 
to manipulate and render them, and the speeds necessary to provide the illusion of smooth motion by creating 
new images every Permission to copy without fee all or part of this mterial is granted provided that 
the copies are not made or distributed for direct coz~erclal advantaEe , the ACM copyright notice and 
the title of the publication and its date appear, and notice is 81yen that copyinE is by permission of 
the Association for Computln 8 Machinery. To copy otherwise, or to republish, requires a fee and/or specific 
permission. 01980 ACM 0-89791-021-4/80/0700-0286 $00.75 thirtieth of a second. Basic data structures 
represent a framework of Euclidean coordinates of line segment endpoints and descriptions of polygon 
visual attributes such as color or shading (I). State of the art hardware systems for manipulat-ing and 
depicting wire-frame data are capable of handling images containing 22,000 edges (2) in real time by 
using special hardware for matrix multiplying end-point coordinates. Hardware sys- tems for manipulating 
and depicting full color shaded images in real time have a somewhat smaller capacity and cost one or 
two orders of magnitude more. This high price limits their cost-effective-ness to such application as 
flight simulators (3). Some of the major computational bottlenecks of computer graphics are the result 
of traditional Von Neumann architectures whose drawbacks include a completely sequential CPU attached 
to a separate memory whose contents are addressable one item at a time. In a landmark paper Sutherland 
(4) has identified new principles of computer architecture which exploit the unique characteristics of 
large scale integration (LSI). These new principles account for the very high cost of transferring and 
rearranging data in contrast to the low cost of replicating processing components and data. The relation 
between these costs is the opposite of that assumed in Von Neumann designs in which wires are cheap and 
computation expensive. One recent computer graphics architecture based on the new principles is that 
of Cohen and Demetrescu (5). Their design replicates identical computing units several thousand fold, 
each with its own local memory. The result is simplicity in construction and thousand fold speedup due 
to parallelism. The topic of this paper is an algorithm for per-forming smooth linear transformations 
(scaling, rotation, shearing) by a computational scheme which exploits the principles set forth by Suther- 
land. Among the characteristics that set it apart from traditional methods for achieving these image 
transformations are that it is highly parallel and is based on local filtering operations on small fixed 
numbers of nearest neighbors. Transforma-tion algorithms are based on a digitized linear interpolation 
scheme which has its roots in a binary code for straight lines on grids. This code was devised and studied 
by Rothstein (6) and is briefly described below. 2. Rothstein's Code for Digitized Straight Lines Rothstein's 
code is a binary sequence, each of whose digits correspond to the nearest neighbor configuration of a 
grid cell crossed by a straight line; 0 corresponds to a cell whose neighbors on opposite sides are crossed 
by the line and l to one whose neighbors are crossed on adjacent sides (see figure 2.1). In the latter 
case the next cell is ignored, avoiding redundancy and yielding one code digit per grid column (grid 
row for slopes whose absolute values exceed unity). For a line 1 jf ,J m J o I,, ~ 0 0 1 The nth code 
digit is l for arg(f)<2~(p/q)n and 0 otherwise. The resulting encoding for straight lines differs from 
Freeman's chain coding (13) by a slope dependent phase shiftinequation (1). 3. Digitized Raster Transformations 
 The algorithms described below are based on a gen- eralization of Rothstein's code as follows. Rather 
than using the code to scan-convert lines into pixel positions, it is used as a kind of rescanning recipe 
to distribute pixels of a source image into those of a destination image. The code specifies (4,P),-(5,2)~""" 
, _-~,,f m s o ,o o s o -w 0 1 FiSure2;,~ Rothste:tn~s Code for Slope 2/5. of slope p/q where 0 < 
p < q are integers with no common factors, the code ~as period q with p l's per period. The digit sequence 
can be simply gen- erated without solving the equation of the straight line at intersections with grid 
parallels by view- ing the line between (0,0) and (q,p) as divided into pq equal segments and noticing 
that a digit occurs once for each interval of p such segments (i.e., the distance between two successive 
grid verticals). That digit is l if the interval in question also happens to contain the termination 
of an interval of q such segments (i.e., the line crosses a grid horizontal); otherwise the digit is 
O. This can be expressed in hardware (figure 2.2) by synchronizing to the same clock, two cyclic binary 
shift registers of lengths p and q respec- tively, detecting end-around shifts of a single bit in each 
to determine code digits. A faster method using more complex hardware consists of suc-cessively adding 
p to a modulo q counter and de- tecting values less than p to generate code l's. The doubly periodic 
nature of this generation pro-cess can be succinctly expressed by the complex valued function which gives 
the nt__hroots of unity as f(p,q,n) = e 2~i (p/q)n (1) digitized linear transformations, not the images 
of lines on rasters. The pictures being transformed may be completely arbitrary; their contents have 
no effect whatever on the operation of the algorithms. Details follow. 3.1 Axis Scaling The geometry 
of figure 2.1 shows that the codes has the most homogeneous possible distribution of p l's among q digits. 
This suggests scaling the x-axis of a gray-scale picture by a ratio of q to p by distributing the p columns 
of the original picture among q of the transformed picture using the same homogeneous distribution. This 
is an attempt at carrying out the affine transformation on pixel coordinates (2) (x,Y) (q/OP i)= (x" 
q/P,Y) subject to the restriction that grid resolution is fixed. Shrinking the picture along the x-axis 
(re- place q/p with p/q above) similarly corresponds to selecting p columns from q of the original picture 
according to the same homogeneous distribution. 287 By i ! Signal for --3-S-q-O(mod q) 4 Digit Trisser 
a) Synchronized Cyclic ghlft Registers b) Modulo q Counter Figure, 2.2 Code Generatin S Hardware Figure 
3.1 schematically represents this technique with the corresponding code written near appropri- ate columns. 
Cell contents represent pixel values (gray scale) with blanks representing zeroes. Un-fortunately, in 
the case of expansion, empty "seams" are introduced and in contraction, columns are de- leted. This visually 
unaesthetic effect could be ameliorated somewhat by spatial smoothing. That solution is undesirable not 
only because informa- tion is lost but also because the gap geometry may "moirE" with picture features 
and therefore depends strongly on the relative positions of the grid and picture. The latter violates 
intuitions about pic- ture information invariance under translation. Looking again at figure 2.2 note 
that changing the relative phases of the shift registers shifts the resulting code digits cyclically 
but does not change the average density of code l's nor the homogeneity of their distribution. Thus, 
starting the code at any position other than when both reg- isters are at the zero position yields a 
column se- lection rule equally as good in terms of homogene- ity. Therefore, averaging the gray-levels 
result- ing from all cyclic permutations of the column selection code averages the gap positions over 
all columns so that there are no discontinuities other than the cell boundaries themselves. No parts 
of the picture are arbitrarily altered because all cells are represented. Figure 3.2 illustrates this 
averaging process for a ratio of 4/3. Though the averaging approach just described satis- fies informational 
intuitions, it must be proven geometrically correct. That is, the resulting grey-scale )icture must be 
the same as would have a) Orisinal Picture (0 1 1 1) 0 n [ I2! ' ~ ! 12 I I [ x2 12 8 8 8 I 8 12 8 
8 12 ,4 12 12 4 12!12 , 4 12 12 i 4 iI 4~ (0 1 I 1)0 I 1 b) Shrunk Vers£on c) Screeched Version FIRure 
3.1 DiRttized Stretchin S and Shrinkin S PI I I i 4 1 &#38; 1 8 12 1 0 12 L2' 8 L2 )2 i 1 1 l 12 12 1' 
2 8 8 12 1' 3 412 1 1 0 8 12 8 11 12~ 12 12 j- 3 2 1 2 5 6 9 F 6 ! 6 6!8 9 3 6 6 6 4 1212 Fi~ure3.2 Picture 
Stretchin R by Averaging 12~ 12 P4 8 8 12 4 1212 4 1 01 11 01 resulted from optically scaling the original 
picture and then redigitizing. The proof requires some re- sults from the geometry of numbers beyond 
the scope of this paper but covered in detail elsewhere (6,7). The outline of the proof is as follows. 
Stretching a picture in the continuous (non-digitized) case by the factor q/p can be viewed as a projection 
of p consecutive columns of the original picture onto q consecutive columns of the transformed picture; 
the code for p/q is a description of where column bound- aries fall in the image. Each of the p columns 
spread into several of the q columns; the contribu- tion of each of the former to each of the latter 
is proportional to the relative area of the image of the former which occupies each of the latter. Now 
consider figure 2.1 as a cross-section of the columns in the obvious sense. Relative area in the preceding 
sentence becomes relative length under this interpretation. These lengths could be mea- sured by stepping 
along the q-cells I/p units at a time counting steps and observing when the image of a p-cell boundary 
is crossed. Since step lengths are equal, each unit distance is equivalent to a count of p. If this stepping 
proceeds from each of the p-cell boundary images, q steps are both nec- essary and sufficient to count 
the lengths. But this yields the same result as translating a line of slope p/q vertically by one grid 
cell and noting the number of times a l appears in each column. Since l's change position only when the 
line crosses lattice points, and between such lattice points the code must be identical to the original, 
the result of translation must be a sequence of cyclic shifts in the code. That this sequence con- sists 
precisely of all possible shifts follows from the fact that the smallest parallelograms with lat- tice 
points as vertices have unit area. Such parallelograms with base Vp2 + q2 (i.e., line of slope p/q connecting 
lattice points) must therefore have altitude l~p2 + q2 or I/qth of the distance between the line y = 
(p/q)x and y = (p/q)x + I. Thus q cyclic shifts result and all must be distinct because no lattice point 
is less than one unit above any other. The proof for horizontal shrinking is virtually identical; the 
figure used in both cases is the same. 3.2 Shearing Transformations Combinations of horizontal and vertical 
stretching map rectangles into rectangles leaving the orienta- tions of edges unchanged. Shearing transformations, 
characterized by matrices of the form 1 (vertical) and (horizontal) (3) 0 1 map rectangles into parallelograms, 
leaving the orientation of one set of parallel edges unchanged. They are of interest here not only because 
they can be easily carried out using an averaging method similar to that just described for axis scaling 
but also because appropriate combinations of shearing and scaling y~eld the entire group of affine (Non-perspective, 
linear) transformations. Using the same reasoning as for scaling, the code for p/q can be considered 
a rule for shearing the grid upward by sliding a column and those to its right upward one unit whenever 
a code digit 1 ap- pears under a column. Just as in scaling, the jagged steps are removed by averaging 
over all pos-sible cyclic shifts in the code (see figure 3.3). e2 i2! 12 12i 12 :21 8 '12 8 8 F I 8 12 
12 ' " I 12 4 :4 I 6 4 ,L+ ,t. 011 10"- -1 1~.,0 1 'v 1 L lo 11 ~ 1 0 1 1 1"  V,   ]/t2 f ~" 12 11" 
2:8 3 9 i12 / 29,i / z ../J f 3 j J For horizontal shearing, the word "column" should be replaced by 
"row", and "under" by "next to". In examining the proof for geometric correctness using the same geometry 
of numbers argument as used for scaling, it can be shown that averaging cor-responds to sliding the columns 
upward by p/q grid units progressively. Figure 3.4 illustrates the computation of averaging shifts described 
below. 3.3 Algorithms and Architecture The shifting and averaging computations can be re-arranged so 
that each column of a resulting image is regarded as having contributions from several columns of an 
original picture. The weight of each contribution is simply the fraction of the time the code digit I, 
corresponding to a column in the ori- ginal, spends in the column of the image when the code is cyclically 
permuted. Figure 3.4 illus-strates an architecture embodying the rearranged compuation. e 3 1' 4 12 12 
12 8 8 8 12 8 ! 12 4 12 I 12 r4 14 J J J Figure 3.3 Shearlnm Xlxorlt:ha DIGITAL FILTER DETAIL TRANSFORM 
TYPE, (Weic iting Coefficients) RATIO U m Pixel Value)Wei ghti ng Coefficients MAU (Modulo Ari thme 
tic Unit) (Input Pixel Values) F DIGITAL i /SOURCE FILTER --' / DESTINATION / VIDEO ^AA / VIDEO / 
REFRESH ~ / REFRESH / MEMO.RY / MEMORY Source Address Shifting Destination Address Shifting~ Figure 3.4 
System Architecture Inputs denoting stretch, shear, or shrink ratios rows of refresh memory is achieved 
by assigning feed the MAU which generates timing signals and one filter to each row. Given the 7 MHz 
computa- weighting coefficients in synchrony with the video tion rate, a 512 x 512 image can be rotated 
in 0.25 refresh scan. The MAU outputs feed the digital ms, about two orders of magnitude faster than 
video filter (expanded view in upper right of figure 3.4) refresh. The simplicity of the filter and the 
which accesses a window of three consecutive pixels, stereotyped mode of operation suggests LSI imple- 
combines their values, and sends the result to the mentation of a multi-filter chip. The high speed destination 
video refresh memory. Such a system of operation may be traded off against larger re- can just keep up 
with 30 frame per second video if fresh memory, sequential pixel bit processing, and the computation 
rate is 7 MHz in the digital fil-less replication of filters. ter and ~U. The architecture described 
differs from state of Image rotation by 8 ° can be achieved by combining the art image processing systems 
primarily in the two shearing and two zooming operations. Using function of the MAU in calculating coefficients 
and matrix notation to denote the sequence of trans- shifting addresses. Conventional image filtering, 
formations (but not the hardware computation), de- roving, and pixel replication can be achieved by compose 
rotation into the four row-column indepen- by-passing the MAU. The algorithms described here dent operations 
below: are not scan-line algorithms in the usual sense because there is no searching, sorting, or raster 
rendering of inherently 3-D data. The transforma- tions are purely pixel driven and two dimensional. 
\-sin 0 cos -sinecose 1 4) The simplest extension to perspective transforma- tion requires generalizing 
the control of the MAU by feeding it scan-line dependent ratios and ad- dressing the filters by scan 
line. By introducing such scan-line control, perspective transformations could be applied to patterns 
which represent pro- Although rotation may now be accomplished within jected images of 3-D polygons. 
the computation scheme of figure 3.4, it is four times too slow to be executed at video rates. How-4. 
Images and Aliasing ever, since computations are row-column independent, the digital filter may be replicated 
(dotted boxes) The pictures below were generated by software sim- without causing memory conflicts and 
without in-ulation of the algorithms discussed above. The creasing control complexity beyond fanning 
out the algorithms yield very smooth (anti-aliased) image weighting coefficients emerging from the single 
MAU. transitions when applied to the successive incre- A maximum speedup factor equal to the number of 
mental transformations found in animation because REFERENCES I. Newman, W.M. and Sproull, R.F. Principles 
of Interactive Computer Graphics, 2nd Edition 1979, McGraw-Hill, New York. 2. Vector General Corporation, 
3400 System Product Specification Sheets, 1978. 3. Bunker, W.M. "Computer generation of images, the 
multi-purpose tool", Proceedings of the Society of Photo-Optical Instrumentation Engi-neering Program 
on Simulators, Anaheim, Califor- nia (March 17-18, 1975). 4. Sutherland, I. and Mead, C. "Microelectronics 
and computer science", Scientific American, Vol. 237, Sept. 1977, pp. 210-229. 5. Cohen, D. and Demetrescu, 
S. "A VLSI approach to computer image generation" Information Sciences Institute, Univ. of Southern Calif. 
(Presented at Interservice/Industry Training Equipment Conference, Orlando, Fla., November 1979.) 6. 
Rothstein, J. and Weiman, C.F.R. "Parallel and sequential specification of a context sensitive language 
for straight lines on grids", Computer Graphics and Image Processing, Vol. 5, 1979, pp. 106-124.  7. 
Weiman, C.F.R. and Rothstein, J. Pattern Recognition by Retina-Like Devices, Computer and Information 
Science Dept., Ohio State University, OSU-ClSRC-TR-72-8 (AD 214 665/2) 1972. 8. Castleman, K.R. Digital 
Image Processing, Prentice-Hall, Englewood Cliffs, New Jersey, 1979.  9. Crow, F.C. "The use of gray 
scale for im-proved raster display of vectors and charac- ters", SIGGRAPH '78 Proceedings, pp. I-5. 
 lO. Andrews, H.F. "Digital image processing", IEEE Spectrum, April 1979, pp. 38-49. II. Rosenfeld, A. 
and Kak, A. Digital Picture Processing, Academic Press, 1976. 12. Freeman, H. "On the encoding of arbritrary 
geometric configurations", IRE Trans. Electron Computers, EC-IO, 1961, pp. 260-268.  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1980</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>807507</article_id>
		<sort_key>294</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-1980</article_publication_date>
		<seq_no>48</seq_no>
		<title><![CDATA[Synthetic texturing using digital filters]]></title>
		<page_from>294</page_from>
		<page_to>301</page_to>
		<doi_number>10.1145/800250.807507</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=807507</url>
		<abstract>
			<par><![CDATA[<p>Aliasing artifacts are eliminated from computer generated images of textured polygons by equivalently filtering both the texture and the edges of the polygons. Different filters can be easily compared because the weighting functions that define the shape of the filters are pre-computed and stored in lookup tables. A polygon subdivision algorithm removes the hidden surfaces so that the polygons are rendered sequentially to minimize accessing the texture definition files. An implementation of the texture rendering procedure is described.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Anti-aliasing]]></kw>
			<kw><![CDATA[Computer graphics]]></kw>
			<kw><![CDATA[Digital filtering]]></kw>
			<kw><![CDATA[Hidden surface removal]]></kw>
			<kw><![CDATA[Sampling]]></kw>
			<kw><![CDATA[Texturing]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Color, shading, shadowing, and texture</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.4.3</cat_node>
				<descriptor>Filtering</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010224</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010384</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Texturing</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P330112</person_id>
				<author_profile_id><![CDATA[81100223210]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Eliot]]></first_name>
				<middle_name><![CDATA[A.]]></middle_name>
				<last_name><![CDATA[Feibush]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Program of Computer Graphics, Cornell University, Ithaca, New York]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP15037068</person_id>
				<author_profile_id><![CDATA[81100593780]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Marc]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Levoy]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Program of Computer Graphics, Cornell University, Ithaca, New York]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35024317</person_id>
				<author_profile_id><![CDATA[81100111623]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Robert]]></first_name>
				<middle_name><![CDATA[L.]]></middle_name>
				<last_name><![CDATA[Cook]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Program of Computer Graphics, Cornell University, Ithaca, New York]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>908845</ref_obj_id>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Blinn, James, "Computer Display of Curved Surfaces", Dissertation, University of Utah, 1978]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>360353</ref_obj_id>
				<ref_obj_pid>360349</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Blinn, James, and Newell, Martin, "Texture and Reflection in Computer Generated Images", Communications of the ACM, Vol. 19, No. 10, Oct., 1976]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>907242</ref_obj_id>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Catmull, Edwin, "A Subdivision Algorithm for Computer Display of Curved Surfaces", Dissertation, University of Utah, 1974]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807360</ref_obj_id>
				<ref_obj_pid>800248</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Catmull, Edwin, "A Hidden-Surface Algorithm with Anti-Aliasing", Computer Graphics, Vol. 12, No. 3, Aug., 1978 (Siggraph '78)]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>907952</ref_obj_id>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Crow, Franklin, "The Aliasing Problem in Computer Synthesized Shaded Images", Dissertation, University of Utah, 1976]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807359</ref_obj_id>
				<ref_obj_pid>800248</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Crow, Franklin, "The Use of Grayscale for Improved Raster Display of Vectors and Characters", Computer Graphics, Vol. 12, No. 3, Aug., 1978 (Siggraph '78)]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Feibush, Eliot, "Texture Rendering for Architectural Design", Computer Aided Design, Vol. 12, No. 2, Mar., 1980]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Weiler, Kevin, "Hidden Surface Removal Using Polygon Area Sorting", Masters thesis, Cornell University, 1978]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_obj_id>358882</ref_obj_id>
				<ref_obj_pid>358876</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Whitted, Turner, "An Improved Illumination Model for Shaded Display", Preliminary papers to be published in Communications of the ACM, Aug., 1979]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 SYNTHETIC TEXTURING USING DIGITAL FILTERS by Eliot A. Feibush, Marc Levoy, Robert L. Cook Program 
of Computer Graphics Cornell University Ithaca, New York 14853 ABSTRACT Aliasing artifacts are eliminated 
from computer generated images of textured polygons by equivalently filtering both the texture and the 
edges of the polygons. Different filters can be easily compared because the weighting functions that 
define the shape of the filters are pre-computed and stored in lookup tables. A polygon subdivision algorithm 
removes the hidden surfaces so that the polygons are rendered sequentially to minimize accessing the 
texture definition files. An implementation of the texture rendering procedure is described. COMPUTING 
REVIEWS CATEGORY: 8.2 KEYWORDS: Computer Graphics, Anti-Aliasing, Sampling, Digital Filtering, Texturing, 
Hidden Surface Removal i. INTRODUCTION Sampling converts a function into a sequence of discrete values 
so the function can be reproduced at a finite resolution. If the sampling rate is insufficient for the 
function, then the discrete values will contain aliasing artifacts. The most common aliasing artifacts 
in computer generated images are jagged edges and Moire patterns. Animated sequences can also suffer 
from temporal aliasing artifacts such as strobing and false motion (e.g., wagon wheels that appear to 
spin backwards). There are two solutions to the aliasing problem in computer graphics: increasing the 
sampling rate and filtering the original function. Increasing the sampling rate means computing and displaying 
the image at a higher resolution. Filtering the original function means blurring the image before sampling. 
The two approaches are not mutually exclusive. Catmull and Crow point out that if the only goal is to 
eliminate aliasing, then filtering the original function is better than increasing the sampling rate 
(4,5). Furthermore, it is often impossible to increase the sampling rate without more costly display 
technology. Permission to copy without fee a~ or part of this material is granted provided that the 
copies are not made or distributed for direct commercial advantage, the ACM copyright notice and the 
title of the publication and its date appear, and notice is given that copying is by permission of the 
Association for Computing Machinery. To copy otherwise, or to republish, requires a fee and/or specific 
per~tssion. 01980 ACM 0-89791-021-4/80/0700-0294 $00.75 Filtering was introduced to the computer graphics 
literature by Catmull in (3) and was studied comprehensively by Crow in (5). Since then, several researchers 
have made filtering an integral part of their synthetic imaging systems. These systems can be classified 
by the type of data they display: i. Continuous functions (parametric data): 1. Crow (5) (parabolas) 
 2. Objects (polygonal or patch data): i. Catmull (3) (patches) 2. Crow (5) (polygons)  3. Crow (6) 
(vectors)  4. Catmull (4) (polygons)  5. Whitted (9) (polygons)  3. Textures (pixel data): 1. Catmull 
(3) (on patches)  2. Blinn and Newell (2) (on patches)  3. Crow (6) (characters)  4. Blinn (1) (on 
patches)  Most of the above implementations that filter only edges use an unweighted filter (i.e., 
a filter with a weighting function that is constant throughout the convolution mask). This is far better 
than no filter at all, but not as good as a weighted filter. Unweighted filters have been used to avoid 
the computational expense of weighting functions. Researchers who have used weighted filters for the 
texture did not use the same filter for the edges of the surfaces. The texture and the edges of each 
surface should be filtered separately and equivalently to produce correct renderings. The texture should 
be filtered first to remove excessively high  frequencies that could cause aliasing in the form of Moir~ 
patterns. Then the edges of surfaces should be filtered to eliminate the excessively high frequencies 
that could cause aliasing in the form of "jaggies." Of the implementations listed above that include 
texturing, only Catmull's (3) applies equivalent filters to both the texture and the edges. He uses unweighted 
filters to display environments of textured patches. 2. IMPLEMENTATION This paper describes the implementation 
of two filtering processes used for displaying textured polygons. Both the texture filter and the edge 
filter are based on a polygon subdivision hidden surface algorithm, and both procedures use pre-computed 
lookup tables to define any desired filter shape. 2.1 DATA REPRESENTATION Objects in this texture rendering 
system are defined by planar polygons. These polygons may be concave, may contain holes, and may be coplanar 
with other polygons in the environment to create detail faces within larger faces. Each polygon is assigned 
a texture which completely covers its surface. A texture is a two-dimensional array of texture definition 
points. The color of each point is represented by either one intensity value, producing a gray scale 
texture, or three intensity values, producing a full color texture. The construction of the database, 
including the creation and assignment of textures to polygons, is handled by an interactive geometric 
modeling package which is described by Feibush (7). The textures can be generated by any of several 
methods, including optical scanning or synthetic airbrushing. In practice, several techniques are usually 
combined for each texture. The word "pixel" (picture element) must be clearly defined. A pixel is often 
thought of as a rectangular block whose width is equal to the distance between centers of adjacent blocks. 
In this paper, however, a pixel is defined as an infinitesimal point having an intensity value. 2.2 
COORDINATE SYSTEMS Three coordinate systems are used: i. Texture definition space. 2. Object definition 
space. 3. Image display space. The first coordinate system is a two-dimensional space for defining textures. 
The textures are created and stored on the X -Y plane shown in Figure la. The second coordinate system 
is a three-dimensional space used for defining the polygons. When a texture is assigned to a polygon, 
a matrix is constructed that transforms the polygon from its location in object space to the X -Y plane 
in texture space, as shown in Figure lb. The third coordinate system is a  COORDINATE SYSTEMS Y, FIGURE 
1 ill'if . ° . . . . . . ° . ° °" ....... ° °   F . ° . ° . ° . t_ la. Polygon and two rows 
of pixels in texture definition space. Vo J~ ./ A lb. Polygon and two rows of pixels x° 1  Z O in 
object space. /[  # ~ Ic. Polygon and two rows of pixels Z i in image space. three-dimensional space 
used for displaying the object. A single matrix is used to transform the polygons from object space to 
image space and create the perspective distortion, as shown in Figure 16. Also shown in the figure are 
two rows of display pixels which are drawn as points in accordance with the above definition of the term 
"pixel." These display pixels are initially defined in image space and can be transformed to object space 
(care must be taken in reversing the perspective distortion), and then to texture definition space, as 
shown in the figure. 2.3 HIDDEN SURFACE ALGORITHM  Most researchers use a scanline hidden surface 
algorithm to determine the contribution of each polygon in the object to the display pixels. In a scanline 
algorithm, all the polygons contributing to the color of a pixel are processed simultaneously. The color 
of each pixel can therefore be computed in a single pass. An alternative solution presented here is to 
use a polygon subdivision hidden surface algorithm to compute the visible portions of all the polygons 
before computing the color of the display pixels. The color of each pixel is built up piecemeal from 
the visible portions of each contributing polygon. The polygon hidden surface algorithm developed by 
Weiler (8) has been implemented. Separating the hidden surface removal from the filtering process has 
a significant advantage over approaches that do both tasks simultaneously. Rendering a textured polygon 
involves accessing its texture definition file. A scanline algorithm requires simultaneous access to 
the texture files of all the polygons that are visible on each scanline. The storage problems this entails 
can not be taken lightly even in a virtual memory machine, particularly if the textures are high resolution 
color images. The polygon subdivision hidden surface algorithm produces a list of visible polygons defined 
at machine precision so that the polygons can be rendered sequentially. Hence only one texture file has 
to be accessible at a given time, and each texture file is processed completely before another one is 
needed. 2.4 TEXTURE FILTERING  Whenever a polygon is displayed in perspective and is not parallel to 
the picture plane, the amount of blurring required to avoid aliasing varies across the polygon and is 
different in the horizontal and vertical directions. The method described in this paper produces sufficient 
blurring at each display pixel by selecting specific texture definition points that correspond to the 
pixel and then filtering the points to determine the color of the pixel. A description of the procedure 
follows: i. For a given view of the object, use the polygon subdivision hidden surface algorithm to 
make a list of the portions of the polygons that are visible in image space. The visible portions are 
called display polygons.  2. Working with one display polygon at a time, make a list of all the pixels 
that contribute to the display of the polygon. A convolution mask, whose shape is determined by the weighting 
function of the filter, is centered at each display pixel. Each pixel has a bounding rectangle, which 
is the smallest rectangle that completely bounds the pixel's convolution mask. The bounding rectangles 
may overlap depending on the size and shape of the convolution masks. List every display pixel whose 
bounding rectangle is completely or partially within the polygon, as shown in Figure 2a. Also save a 
list of the intersections of each bounding rectangle with the polygon. 3. For each display pixel, transform 
its bounding rectangle from image space to object space and then to texture definition space. The rectangle 
can be transformed to texture definition space because its vertices have three-dimensional coordinates 
coplanar with the display polygon. The rectangle in image space transforms to a quadrilateral in texture 
space, as shown in Figure 2b. The texture definition points within this quadrilateral contribute to the 
color of the display pixel. To simplify the selection of these points, a rectangle is constructed around 
the bounding quadrilateral. This rectangle includes some texture definition points that do not contribute 
to the color of the pixel, but these extra points will be eliminated from the filtering in step 6.  
4. Transform the parent polygon of the current display polygon from object space to texture definition 
space. Clip the rectangle around the convolution mask quadrilateral against the parent polygon. The texture 
definition points within this area will be filtered, as in Figure 2c.  5. Transform each texture point 
that will be filtered to object space and then to image space, as shown in Figure 2d.  6. Eliminate 
the extra points selected in step 3 by clipping the transformed texture points against the bounding rectangle 
of the convolution mask in image space, as shown in Figure 2e.  7. Filter the selected texture points 
by computing the weighted average of their color values. Points near the center of the convolution mask 
are weighted more heavily than those near the edge. The cone shown in Figure 2f represents one possible 
weighting function. The weighting function is computed at a number of locations and the values are stored 
in a two-dimensional lookup table. The location of each transformed texture point within the convolution 
mask is used as an index to the lookup table. The color values of all the texture definition points are 
multiplied by their respective values in the lookup table and sun, ned together in a weighted average. 
When the transformed texture points do not coincide precisely with the discrete locations at which the 
weighting function is calculated, the nearest value is used.  This completes the texture filtering. 
The edges of the polygons are filtered next to complete the rendering procedure. FIGURE 2 TEXTURE FILTERING 
I~ ~iii~,~t I'/" :., , , i: ::::::~iii ::~i~ !~ i~:'   " --- Xi I.I. I. I-li~iiiil. I Z i >"t  2a. 
Select the pixels that contribute to 2b. Transform the bounding rectangle to the display of the polygon. 
For texture space and select texture clarity, the bounding rectangles definition points. shown do not 
overlap. Figures 2b-2f illustrate the texture calculation for each selected pixel. / t  / ¥ ~Xt / 
Z i 2c. Select the points inside the 2d. Transform the points to image space. transformed parent polyg~ 
 polyRon.  Y I i  Z I 2e. Select the points inside the 2f. Compute the weighted average of the bounding 
rectangle. selected points. 2.5 EDGE FILTERING  The intensity of a pixel whose convolution mask is 
completely within one display polygon is determined just by the texture filter. The intensity of a pixel 
near an edge of a polygon is only partly determined by the texture filter because its convolution mask 
covers more than one display polygon. The intensity of a pixel computed by the texture filter for one 
polygon is weighted by the percentage of the total intensity of the pixel that is contributed by that 
polygon. The total intensity of a display pixel is built up sequentially as each polygon is rendered 
by accumulating the partial intensities in a frame buffer. The contribution of a polygon to a pixel 
is determined by filtering its edges with the same weighting function that was used for the texture filter. 
But unlike the texture, which is defined by discrete points, the edges of the polygon are defined by 
a continuous function. Edge filtering is therefore an analytic problem. The cone above the convolution 
mask shown in Figure 3a represents one possible weighting function for the filter. The value of the weighting 
function at any point in the convolution mask is the distance from the point to the surface above it. 
The contribution of a polygon to the pixel is the percentage of the volume of the entire cone that is 
above the polygon, as shown by the shaded volume in Figure 3a. The calculation of this volume is described 
below. i. Clip the display polygon against the bounding rectangle of the convolution mask, as shown 
in Figure 3b. The points of intersection of each polygon edge with the bounding rectangle are already 
known from step 2 above. The clipped polygon may be concave and may contain holes. 2. For each vertex 
of the clipped polygon, construct a triangle with the following sides (as shown in Figure 3c): i. BASE 
is the line segment between the current vertex and the next vertex (going clockwise around the polygon). 
 2. SIDE1 is the line segment between the current vertex and the pixel.  3. SIDE2 is the line segment 
between the next vertex and the pixel.  3. Calculate the volume above the polygon from the volumes 
above all the triangles constructed in step 2, as shown in Figure 3c. The volume above a single triangle 
is added to the total if the cross product of SIDE1 and SIDE2 is negative; it is subtracted from the 
total if the cross product is positive.  4. The task of finding the volume above an arbitrary polygon 
has now been simplified to finding the volume above a series of triangles, each having one vertex at 
the pixel. The problem can be simplified  further. For each triangle, the perpendicular from the pixel 
to BASE (or to its extension) forms two right triangles. The volume above the original triangle is the 
sum of the volumes above the two right triangles if the perpendicular lies within the triangle, as shown 
in Figure 3d; it is the difference of the volumes if the perpendicular lies outside of the triangle, 
as shown in Figure 3e. 5. The problem has now been simplified to finding the volume above a group of 
right triangles. The base and height of each triangle are used as indices to a lookup table that contains 
the volume above this triangle for the given weighting function. Care must be taken in computing the 
lookup table so that areas inside the bounding rectangle but outside the convolution mask have no volume 
above them. Only the shaded area of Figure 3f has volume above it. Each filter shape needs only one 
lookup table, regardless of the filter's absolute size. The filter size can be changed by scaling the 
indices to the lookup table. The organization of the lookup table assumes that the filter function is 
circularly synmnetric. For a filter that is not circularly symmetric, one more parameter describing the 
location of the right triangles (such as a polar sweep angle) is required. A four parameter lookup table 
would give the volume above the original triangle without constructing the two right triangles. The X 
and Y positions of the two vertices of BASE of the original triangle would be used as the indices to 
the four parameter lookup table. This further simplifies the filtering computation but requires significantly 
more table storage. 3. EXAMPLES  A polygon textured with alternating red and white vertical stripes 
has been rendered by the system described in this paper. Due to the rotation and perspective transformations, 
the number of texture definition points that were filtered for each display pixel varied considerably. 
The images were computed at a resolution of 512 x 512 and displayed on a 24-bit color frame buffer. 
The five images of the polygon demonstrate the effectiveness of different filters, as shown in Figures 
4a-e. Figure 4a shows the polygon in texture definition space. In Figure 4b this polygon is displayed 
in image space with no filtering. In Figure 4c it is displayed using an unweighted filter with a square 
convolution mask whose sides are equal to the distance between adjacent display pixels. In Figure 4d 
the polygon is displayed using a filter with a Gaussian weighting function that has a standard deviation 
equal to the distance between adjacent display pixels. The convolution mask is a circle whose radius 
is equal to twice the standard deviation of EDGE FILTERING FIGURE 3 Yi Y / 3b. Clip the polygon to 
the bounding  Z i 3a. The intensity computed by the texture rectangle of the pixel's convolution  filter 
is weighted by the ratio of the shaded volume to the total volume mask. of the cone. \ \ /  3c. For 
each vertex of the clipped polygon, construct the triangle formed by the vertex, the next vertex (going 
clockwise), and the pixel. From the volumes above these triangles, calculate the volume above the clipped 
polygon as shown in Figures 3d-f. 3d. For each triangle, construct the perpendicular from the pixel 
to BASE. If the perpendicular is inside the triangle, the volume above the triangle is the sum of the 
volumes the two right triangles formed by the perpendicular. then above f 3f. Find the volume above each 
right triangle by using its height (h) and base (b) as indices to a lookup table. The value stored in 
the lookup table includes only the volume above the shaded portion of the triangle. 3e. If the perpendicular 
is outside the triangle, then the volume above the triangle is the difference of the volumes above the 
two right triangles formed by the perpendicular. J  the Gaussian. Displaying the polygon with no 
filtering is completely unsatisfactory due to the jaggedness of not only the edges of the polygon but 
also the stripes in the texture. Using an unweighted filter is better and nearly satisfactory along the 
edges, but Moir~ is still evident in the center of  the polygon. The weighted filter, however, produces 
an excellent image. In the hardware magnification shown in Figure 4e, the polygon is inclined slightly 
more than in Figures 4b-d to enhance the visibility of the filtering. Notice that the filtering along 
the left edge of the polygon is equivalent to the filtering along the stripes of the texture. The final 
image, Figure 5 shows the front facade of an imaginary house that has been rendered by the system described 
in this paper. It demonstrates an application of the system to a complex database composed of many polygons 
and textures. The textures were extracted from optically scanned photographs of real objects. The background 
was created by assigning an optically scanned photograph of a real site to the rearmost polygon in the 
environment. 4. LIMITATIONS  It is possible to obtain views where textures are magnified beyond their 
original resolution (i.e., zooming into a texture). During the texture filtering process, the area of 
the texture definition that corresponds to a display pixel will contain only a few texture definition 
points. To avoid reproducing these texture definition points as large square areas, the color values 
of the closest texture definition points are bilinearly interpolated. Bilinear interpolation of the 
texture definition points is also necessary when the edges of two polygons are very close to each other, 
but do not actually touch. The hidden surface algorithm will detect the narrow slot between the polygons, 
so texture definition points of the polygon seen through the slot should be selected for filtering. If 
no texture definition points from the background polygon fall within the slot, then the nearby texture 
definition points are bilinearly interpolated. More blurring is required to avoid aliasing if there 
are high frequency components in the texture definition. Aliasing that is not noticeable in a static 
image may become visible if the image is part of an animated sequence, so that additional blurring is 
needed. 5. CONCLUSIONS  Two filtering processes, one for the textures and one for the edges, are necessary 
for displaying textured polygons without introducing aliasing artifacts. A weighted filter, such as the 
Gaussian used in the examples, produces more realistic images than an unweighted filter or no filter 
at all. A polygon subdivision hidden surface algorithm is superior to a scanline hidden surface algorithm 
for displaying textured polygons. By making a list of all the visible portions of the polygons before 
computing the color of the display pixels, the polygons can be filtered sequentially to minimize accessing 
each of the texture definition files. Complex filters no longer have to be considered prohibitively 
expensive. If the filter's weighting function is stored in a lookup table instead of being computed at 
each pixel, an image can be computed in the same amount of time regardless of the complexity of the filter. 
The filter can be changed just by using a different lookup table. ACKNOWLEDGEMENTS  This research has 
been performed at the Program of Computer Graphics at Cornell University and was funded in part by the 
National Science Foundation. The authors thank Theodore Crane and Stuart Sechrest for implementing the 
polygon subdivision hidden surface algorithm. REFERENCES  i. Blinn, James, "Computer Display of Curved 
 Surfaces", Dissertation, University of Utah, 1978  2. Blinn, James, and Newell, Martin, "Texture and 
Reflection in Computer Generated Images", Communications of the ACM, Vol. 19, No. i0, Oct., 1976  Catmull, 
Edwin, "A Subdivision Algorithm for Computer Display of Curved Surfaces", Dissertation, University of 
Utah, 1974  3.  4. Catmull, Edwin, "A Hidden-Surface Algorithm with Anti-Aliasing", Computer Graphics, 
Vol. 12, No. 3, Aug., 1978 (Siggraph '78)  5. Crow, Franklin, "The Aliasing Problem in Computer Synthesized 
Shaded Images", Dissertation, University of Utah, 1976  6. Crow,' Franklin, "The Use of Grayscale for 
Improved Raster Display of Vectors and Characters", Computer Graphics, Vol. 12, No. 3, Aug., 1978 (Siggraph 
'78)  7. Feibush, Eliot, "Texture Rendering for Architectural Design", Computer Aided Design, Vol. 12, 
No. 2, Mar., 1980  8. Weiler, Kevin, "Hidden Surface Removal Using  Polygon Area Sorting", Masters 
thesis, Cornell University, 1978  9. Whitted, Turner, "An Improved Illumination Model for Shaded Display", 
Preliminary papers to be published in Communications of the ACM,  Aug., 1979    
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1980</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>807508</article_id>
		<sort_key>302</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-1980</article_publication_date>
		<seq_no>49</seq_no>
		<title><![CDATA[The display of characters using gray level sample arrays]]></title>
		<page_from>302</page_from>
		<page_to>307</page_to>
		<doi_number>10.1145/800250.807508</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=807508</url>
		<abstract>
			<par><![CDATA[<p>Character fonts on raster scanned display devices are usually represented by arrays of bits that are displayed as a matrix of black and white dots. This paper reviews a filtering and sampling method as applied to characters for building multiple bit per pixel arrays. These arrays can be used as alternative character representations for use on devices with gray scale capability. Discussed in this paper are both the filtering algorithms that are used to generate gray scale fonts and some consequences of using gray levels for the representation of fonts including:</p> <p>1. The apparent resolution of the display is increased when using gray scale fonts allowing smaller fonts to be used with higher apparent positional accuracy and readability. This is especially important when using low resolution displays.</p> <p>2. Fonts of any size and orientation can be generated automatically from suitable high precision representations. This automatic generation removes the tedious process of &#8220;bit tuning&#8221; fonts for a given display.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Anti-aliasing]]></kw>
			<kw><![CDATA[Character representation]]></kw>
			<kw><![CDATA[Computer graphics]]></kw>
			<kw><![CDATA[Convolution]]></kw>
			<kw><![CDATA[Fonts]]></kw>
			<kw><![CDATA[Gray-scale]]></kw>
			<kw><![CDATA[Raster displays]]></kw>
			<kw><![CDATA[Shading]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.4.3</cat_node>
				<descriptor>Grayscale manipulation</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.1</cat_node>
				<descriptor>Raster display devices</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.7</cat_node>
				<descriptor>Texture</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010240.10010243</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision representations->Appearance and texture representations</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372.10010373</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering->Rasterization</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010583.10010588.10010591</concept_id>
				<concept_desc>CCS->Hardware->Communication hardware, interfaces and storage->Displays and imagers</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P146471</person_id>
				<author_profile_id><![CDATA[81100406185]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[John]]></first_name>
				<middle_name><![CDATA[E.]]></middle_name>
				<last_name><![CDATA[Warnock]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Xerox Palo Alto Research Center, Palo Alto, California]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>807417</ref_obj_id>
				<ref_obj_pid>965103</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Catmull, Edwin. A Tutorial on Compensation Tables, Quarterly Report of SiGGRAP-ACM, Vol. 13, 2, August 1979. pp. 1-7.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>359869</ref_obj_id>
				<ref_obj_pid>359863</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Crow, Frank C. The Aliasing Problem in Computer Generated Shaded Images, Communications of the ACM, Vol. 20, 11, November 1977 pp. 799-805.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>22881</ref_obj_id>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Gonzalez, Rafael C. and Wintz, Paul. Digital Image Processing, Addison-Wesley Publishing Company, Inc., London, 1977.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Knuth, Donald E. METAFONT, A System for Character Shaping, Stanford Artificial Intelligence Laboratory, Report No. STAN-CS-79-000, 1979.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Pearson, D. E. Transmission and Display of Pictorial Information, John Wiley &amp; Sons, New York, 1975.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Seitz, Charles et al. Digital Video Display System with a Plurality of Grey-Scale Levels. United States Patent 4,158,200.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Webster, R .J . A Generalized Hamming Window, IEEE Transactions on Acoustics, Speech, and Signal Processing, Vol. 26, 2, April 1978, p. 176.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 The Display of Characters Using Gray Level Sample Arrays John E. Wamock Xerox Palo Alto Research Center 
 Palo Alto, California Abstract Character fonts on raster scanned display devices are usually represented 
by arrays of bits that are displayed as a matrix of black and white dots. This paper reviews a filtering 
and sampling method as applied to characters for building multiple bit per pixel arrays. These arrays 
can be used as alternative character representations for use on devices with gray scale capability. Discussed 
in this paper are both the filtering algorithms that are used to generate gray scale fonts and some consequences 
of using gray levels for the representation of fonts including: 1. The apparent resolution of the display 
is increased when using gray scale fonts allowing smaller fonts to be used with higher apparent positional 
accuracy and readability. This is especially important when using low resolution displays. 2. Fonts 
of any size and orientation can be generated automatically from suitable high precision representations. 
This automatic generation removes the tedious process of "bit tuning" fonts for a given display.  Key 
Words and Phrases: computer graphics, fonts, gray-scale, raster displays, shading, anti-aliasing, convolution, 
character representation CR Categories." 8.2 Introduction The motivation behind the work presented in 
this paper comes from research into the problems of building an interactive display system that allows 
users to design high quality page layouts much like those found in advertising copy. To achieve this 
goal, the problem of displaying a wide variety of different character fonts on an interactive display 
must be solved. It is important for this application that the characters on the interactive display reflect 
the Permission to copy without fee all or part of this material is granted provided that the copies are 
not made or distributed for direct commercial advantage, the ACM copyright notice add the title of the 
publication and its date appear, and notice is given that copying is by permission of the Association 
for Computing Machinery. To copy otherwise, or to republish, requires a fee and/or specific permission. 
 1111980 ACM 0-89791-021-4/80/0700-0302 $00.75 shapes and styles that are to be in the final printed 
copy. The aesthetic judgments demanded to lay out a high quality page impose these requirements on such 
a system. In most other text display applications where raster displays are used, each character in a 
font is represented as a matrix of binary values, representing white/black dots on the display. Such 
applications only require that the character font be legible. For this reason most text display systems 
provide only one character font, and this font is simple and tuned to the characteristics of the display. 
However, when applications arise that relate to the printing industry, then the standards regarding the 
printed word change dramatically. It is common, for instance, for a single advertisement to contain a 
dozen fonts having different sizes, weights, and styles. A technical article may contain as many as thirty 
different variations of fonts. In these applications the appearance of the characters, their layout, 
and their spacing are all important. Systems that generate the bit matrix representations of character 
fonts automatically from higher level representations fall into three classes. analytic --Knuth's Metafont 
[4] system falls into this class. It allows for the inclusion of information about the strokes that make 
up each character so that variations on a font may be generated from a common source by applying suitable 
transformations to each stroke. high resolution bitmaps --Character masters are scanned at high precision, 
and these images are cut off at a threshold to obtain bit matrices at the desired resolution. parametric 
curved outlines --The outline around each character defined by a parametric function is scan converted 
to produce high precision bit matrices. All of the above approaches work quite well when high precision 
character matrices are required. But none of the above strategies are satisfactory at low resok~tions. 
The low resolution character matrices produced by the above schemes amount to images that are undersampled 
and therefore are not good representations of the information in the character. Because of the lack of 
an automatic scheme, the bit matrices for low resolution fonts usually are constructed, very tediously, 
by hand. This manual process involves an artist sitting at a console, turning individual pixels, on or 
off in order to assess the aesthetic appearance of the resulting character on the display. If wide ranges 
of font sizes and rotations are needed then this latter approach is impractical. This paper discusses 
a methodology for building character sets for use with low resolution raster displays that have gray-scale 
capabilities. There are three important goals: The characters generated must have the same general appearance 
as the masters from which they are made even though fonts are generated for a low resolution device. 
The characters must be free of sampling artifacts. They must not have unnatural holes or dark spots, 
nor should t_hey be unnecessarily blurry. It should be possible to make the characters in a line of text 
look properly spaced. Positioning errors due to inadequate display resolution should be avoidable. The 
basic strategy described here is an old technique that is straightforward, automatic, and achieves the 
above goals. The technique proceeds as follows: characters represented by high precision black and white 
bit matrices, obtained by one of the above schemes, are filtered, using a low-pass filter. The resulting 
low frequency image is sampled and displayed. This filtering and resampling process is very common in 
the image processing field, and is used extensively to scale down images. The application of building 
grey encoded characters with this, and other techniques has been previously accomplished by Seitz [6] 
and Crow [21. This paper reviews the general application of this method to automatic character font generation, 
and gives some additional results on the efI?cts of subpixcl positioning of characters that are new. 
The images included in this paper give an indication of how well this scheme works for various filter 
variations. Also a number of the side benefits of the scheme arc discussed. Input Character Masters The 
character masters used in this work are 100x100 bit arrays. They are obtained by scan converting curved 
outline representations of each character. The particular process used to build these high precision 
master characters is not critical to the techniques presented in this paper. Figure 1 shows an example 
of a 100xl00 bit matrix of the character "&#38;". &#38;  Figure 1 Gray Values The mapping function 
between computed gray values and the intensities actually shown on the display must be care fitly controlled 
in order to remove non-linearities introduced the display hardware. This problem has been addressed by 
CatmuU [1], and the techniques suggestcd in his paper should be followed. For the purposes of the illustrations 
in this paper, gray scale wedges are provided in each image in order to calibrate the gray values used. 
Filtering Process The basic strategy in building gray encoded fonts is to take a high resolution, black 
and white image (a high precision bitmap in this case), to filter the image, and then to resample to 
produce the low resolution character. The way in which this is achieved in practice is quite simple and 
intuitive. Consider figure 2 which shows the high precision character "a". The computation that is performed 
simulates an idealized sampling camera that is pointed at the character. As this camera scans the image, 
it looks at ovel%pping sampling areas of the image. These sampling areas correspond to the pixels that 
are shown on the display. The value that each pixel receives is a function of the black and white subareas 
within the sampling area. In particular, if only white is within the sampling area, then a white pixel 
is produced. If only black is within the sampling area, then a black pixel is produced. If a combination 
of black and white are in the sampling area, then a shade of gray is produced. ) ) >  !~s! "~.. ~ points 
of T Px Figure 2 One kind of area averaging scheme that can be used for producing the gray values in 
this simulation consists of taking the ratio of the black subarea to the total area and using this ratio 
to determine the gray value. Another approach is to weight each point of the area as some fimction of 
the distance of the point from the center of the sample area. With this latter weighting function, black 
subareas of the master character near the center of the pixel may contribute more to the blackness of 
the pixel than do black areas near the edge of the pixel. Three families of weighting functions (filters) 
are used in this work. First generalized 2-dimensional Hamming filters [7] are used. The Hamming filters 
used are of the form: fix,y) = SCose(~r sqrt(x2+y2)/W) for sqrt(x2+y 2) < W/2 (1) f(x,y) = 0 for sqrt(x2+y2)) 
> W/2 Another family of filters used in this work are of the form: fix, y) = S (Cos((~r x)/W)Cos((~r 
y)/I40) e for -W/2 < x,y < W/2 (2) fix,y) = 0 for x,y > W/2 or x,y < - W/2 The third family of filters 
used are bi-linear and are of the form: fix, y) = S g(x)g(.v) where g(t)=-(4/(W2))t+2/W for t >= 0 and 
(3) g(l)=(4/(W2))t+2/W for t < 0 g(t) = 0 for t < -W/2 or t > W/2 In these formulas W, e, and S are 
constant parameters that determine which member of the family is used. W--the width of the filter; --controls 
the relative weighting of the filter near the origin. (Large values of e weight values near the origin 
more heavily. Note that e --0 provides for the simple averaging case in the first two families.); S 
--chosen so that the sum of all the elements in an array of filter values is a desired maximum intensity 
L In this work, because of display controller limitations, only 16 intensity values are used ranging 
from 0 to 15). The shapes of examples of the filters in the three families are illustrated in figure 
3. 1 2 3 Figure 3 All the resulting character fonts are scaled down versions of the master characters. 
It is assumed that the master characters' resolution is high relative to the resolution of the fonts 
that are produced. To make a character of a given size, the following steps are taken: 1. Given that 
the high precision character is represented as an nxn matrix, and given that we wish to make a character 
of approximately mxm pixels where m << n, we let d = n/m. This spacing, d, is the increment used between 
pixels relative to the master character. 2. Next, chose a filter width W appropriate to the sampling 
increment. In this case, filter widths between .5d and 2.5d give an adequate range of results for the 
purposes of this work. In the camera simulation metaphor, this width corresponds to the diameter of the 
sampling area of a single pixel. It should be pointed out that with these families of filter functions, 
the wider filters are lower pass filters and tend to blur the characters. The narrower filters tend to 
undersample, leaving unwanted gaps in the characters. 3. The next step is to choose e (if using one 
of the first two filter families.) The value of e determines the shape Of the filter function and therefore 
the relative weights assigned to the filter elements. Lower values of e make lower pass filters, and 
therefore tend to blur the characters. It should be noted that the values of e interact with W in that 
large values of e tend to cancel the effect of making W large. '/'he values of e used in this work are 
between 0 and 4. This range of values brackets the usable range of results. 4. In this step the weighting 
array (filter), a WxW array, F, is constructed which is centered on the origin, and which has values 
at each array position that correspond to the value of the filter ~nction at that position. 5. Now to 
compute the weighted average (gray value) for a given x,y position relative to the character matrix C, 
F is centered at x,y, and convolved with C to yield rxy. i.e.,  rxy = sum(F( W/2- i, W/2-j)C(x-i,y-j)) 
for Max(-W/2,-x) < i < Min(n-x, W/2) and Max(-W/2,-y) < j < Min(n-y, Here C(Q) = 0 or 1 for all id. The 
last step is to select a sample grid, T, where the points of the grid are d units apart and where the 
starting phase of the grid is picked to be Pie Py (-d < pxpy < d) (see fig. 2). The sample grid T covers 
C so that pixels that could have sampling areas that intersect C are included in T. For this reason T 
may be larger than mxm. Finally, the gray array, R, is computed on the grid points of T. The gray array, 
R, is the gray scale representation of the character.   Bibliography 1. Catmull, Edwin. A Tutorial 
on Compensation Tables, Quarterly Report of SiGGRAP-ACM, Vol. 13, 2, August 1979. pp. 1-7. 2. Crow, 
Frank C. The Aliasing Problem in Computer Generated Shaded Images, Communications of the ACM, Vol. 20, 
11, November 1977 pp. 799-805. 3. Gonzatez, Rafael C. and Wintz, Paul. Digital hnage Processing, Addison-Wesley 
Publishing Company, lnc., London, 1977. 4. Knuth, Donald E. METAFONT, A System for Character Shaping, 
Stanford Artificial Intelligence Laboratory, Report No. STAN-CS- 79-O00, 1979. 5. Pearson, D. E. Transmission 
and Display of Pictorial Information, John Wiley &#38; Sons, New York, 1975. 6. Seitz, Charles et al. 
Digital Video Display System with a Plurality of Grey-Scale Levels. United States Patent 4,158,200. 
7. Webster, R.J. A Generalized Hamming Window, IEEE Transactions on Acoustics, Speech, and Signal Processing 
Vol. 26, 2, April 1978, p. 176.   
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1980</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>807509</article_id>
		<sort_key>308</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-1980</article_publication_date>
		<seq_no>50</seq_no>
		<title><![CDATA[Human vision, anti-aliasing, and the cheap 4000 line display]]></title>
		<page_from>308</page_from>
		<page_to>313</page_to>
		<doi_number>10.1145/800250.807509</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=807509</url>
		<abstract>
			<par><![CDATA[<p>Despite its other advantages, one of the major objections to raster graphics has been the poor image quality and aliasing effects caused by discrete sampling. These effects include &#8220;jaggies&#8221; or stair-stepping, crawling, line breakup, and scintillation. Several solutions have been proposed in the literature, however, most suffer severe drawbacks and are only partially successful at eliminating aliasing effects. One solution, area anti-aliasing, is not only effective, it produces results comparable to higher resolution systems. Using widely available data on human visual response, it is shown how this technique actually increases the perceived resolution of a display beyond the hardware resolution by factors of up to 16X. The requirements of such a system are discussed, as well as some of the problems encountered.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Anti-aliasing]]></kw>
			<kw><![CDATA[Discrete sampling]]></kw>
			<kw><![CDATA[Grey scale]]></kw>
			<kw><![CDATA[Halftoning]]></kw>
			<kw><![CDATA[Human vision]]></kw>
			<kw><![CDATA[Raster graphics]]></kw>
			<kw><![CDATA[Resolution]]></kw>
			<kw><![CDATA[Visual perception]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.2.3</cat_node>
				<descriptor>Resolution</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.1</cat_node>
				<descriptor>Raster display devices</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.1</cat_node>
				<descriptor>Quantization</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010583.10010588.10010591</concept_id>
				<concept_desc>CCS->Hardware->Communication hardware, interfaces and storage->Displays and imagers</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372.10010373</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering->Rasterization</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010187</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Knowledge representation and reasoning</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P334430</person_id>
				<author_profile_id><![CDATA[81100594890]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[William]]></first_name>
				<middle_name><![CDATA[J]]></middle_name>
				<last_name><![CDATA[Leler]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Texas Instruments, Box 1443 M/S 617, Houston, Texas]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>807454</ref_obj_id>
				<ref_obj_pid>800249</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Barros, J. &amp; Fuchs, H., Generating smooth 2-D monocolor line drawings on video displays. Proc. SIGGRAPH '79 pp. 260-269]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Biberman, L., Perception of Displayed Information. Plenum Press, New York, 1973, pp. 3-4]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Blackwell, H. R., Contrast thresholds of the human eye. J. Opt. Soc. of Am. 36-11 (Nov. 1946) pp. 624-643]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Campbell, F. &amp; Green, D., Optical and retinal factors affecting visual resolution. J. Physiol. 181 (1965) pp. 576-593]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Campbell, F., The human eye as an optical filter. Proc. IEEE 56-6 (June 1968) pp. 1009-1014]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807360</ref_obj_id>
				<ref_obj_pid>800248</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Catmull, E., A hidden-surface algorithm with anti-aliasing. Proc. SIGGRAPH '78 pp. 6-11]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807417</ref_obj_id>
				<ref_obj_pid>800249</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Catmull, E., A tutorial on compensation tables. Proc. SIGGRAPH '79 pp. 1-7]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Cornsweet, T., Visual Perception. Academic Press, New York, 1970, p. 341]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_obj_id>359869</ref_obj_id>
				<ref_obj_pid>359863</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Crow, F. C., The aliasing problem in computer-generated shaded images. Comm. ACM 20-11 (Nov. 1977) pp. 779-805]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807359</ref_obj_id>
				<ref_obj_pid>800248</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Crow, F., The use of greyscale for improved raster display of vectors and characters. Proc. SIGGRAPH '78 pp. 1-5]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Davidson, M., Perturbation approach to spatial brightness interaction in human vision. J. Opt. Soc. of Am. 58-9 (Sept 1968) pp. 1300-1308]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Erickson, R. &amp; Hemingway, J., Visibility of raster lines in a television display. J. Opt. Soc. of Am. 60-5 (May 1970) pp. 700-701]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_obj_id>356627</ref_obj_id>
				<ref_obj_pid>356625</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Freeman, H., Computer processing of line-drawing images. Computing Surveys 6-1 (March 1974) pp. 57-97]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Schade, O., Electro-optical characteristics of television systems. RCA Review 9-6 (June 1948) pp. 5-37]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Szabo, N., Digital image anomalies: static and dynamic. SPIE v. 162 Visual Simulation &amp; Image Realism (1978) pp. 11-15]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Thompson, F., Television line structure suppression. J. SMPTE 66-10 (Oct. 1957) pp. 602-606]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Human Vision, Anti-aliasing, and the Cheap 4000 Line Display William J Leler Texas Instruments Box 
1443 M/S 617 Houston, Texas 77001 ABSTRACT Despite its other advantages, one of the major objections 
to raster graphics has been the poor image quality and aliasing effects caused by discrete sampling. 
These effects include "jaggies" or stair-stepping, crawling, line breakup, and scintillation. Several 
solutions have been proposed in the literature, however, most suffer severe drawbacks and are only partially 
successful at eliminating aliaslng effects. One solution, area anti-aliasing, is not only effective, 
it produces results comparable to higher resolution systems. Using widely available data on human visual 
response, it is shown how this technique actually increases the perceived resolution of a display beyond 
the hardware resolution by factors of up to 16X. The requirements of such a system are discussed, as 
well as some of the problems encountered. Keywords and phrases: anti-aliaslng, discrete sampling, grey 
scale, halftoning, human vision, raster graphics, resolution, visual perception. CR Categories: 3.79, 
6.35, 8.2 Introduction Raster scan computer graphics systems, formerly too expensive except for such 
applications as flight simulators, are now becoming practical in other areas due to plummeting memory 
and processor costs. These systems offer increased realism, color, shading and substantial information 
display capabilities. The advantages, however, have been hampered by low resolution and poor image quality 
 due to ,the sampling process. These sampling effects, commonly known as the jaggies, are caused by taking 
discrete samples over objects which have infinitely sharp edges. According to sampling theory, this will 
result in the appearance of artifacts called aliasing, which is the transformation of frequencies higher 
than one half the sample rate into lower ones. This topic is well covered by Crow (9). Aliasing Effects 
 The visible effects of aliasing fit into four general categories (15). Stair-stepping is most noticable 
on the edge of a surface which is close to, but not quite horizontal or vertical (figure one). The effect 
is more pronounced in dynamic displays due to the eye being attracted to changing edge detail, but can 
be seen in static images as well. DESIRED DISPLAYED Permission to copy wlthout fee all. or part of 
thie material is granted provided the= the copies are not mxde or dlstrlbu=ed for dlrec~ coumerclal advantage, 
the AQ( copyrlgbt notice and the title of the publication and its date appear, and notice is given that 
copyln$ is by permission of the Association for Computing Machinery. To copy otharwlse, or to republlsh, 
 requires a fee and/or specific per~selon. &#38;#169;1980 ACM 0-89791-021-4/80/0700-0308 $00.75 Figure 
one: "The Jaggies" Crawling, unlike stalr-stepping, can only be seen in dynamic displays. It causes 
a slowly moving object to appear to change shape due to the fact that the position of an edge can only 
change in pixel increments. Since some edges change position at different times than others the object 
appears to crawl like an inchworm rather than move smoothly. Line breakup is seen on objects, usually 
lines, whose width is small compared to a picture element (pixel). For example a line one plxel wide 
at a 30 degree angle will miss the center of approximately one out of every three pixels and will appear 
as a dotted line. This effect is also seen on curved lines and causes moire patterns on closely spaced 
lines. The worst case is when an object falls in between the pixels, and just disappears. This case is 
called scintillation when it occurs on dynamic displays. Scintillation is the effect of small moving 
objects appearing and disappearing in a dynamic display. This is aggravated by the fact that when a very 
small object does hit the sample point it is suddenly represented by an entire pixel. This can be imagined 
by considering a large building with many small windows. When the building moves in relation to the screen 
the windows will shimmer on and off in a very annoying manner. Potential Solutions The cost effective 
removal of aliasing effects would eliminate one of the last barriers to the acceptance of raster graphics 
displays. Consequently there has been a great deal of work done in this area recently and several solutions 
proposed. The simplest, and cheapest, solution is to blur the image in some way, possibly by defocusing 
the display device or by applying contour smoothing to the computed image (13). These techniques try 
to mask the problem by spreading it over a larger area and are only partially successful, and also have 
the bad side effect of severely restricting information display capabilities. Intentional blurring of 
images, however, is effective in combating temporal aliasing of images, a topic not covered in this paper. 
 The popular solution is to assume that more resolution is required and that increasing the number of 
samples until the spot size is made smaller than the human eye can resolve will alleviate the problem. 
The resolution requirements of such a display will be determined by the resolving power of the eye according 
to the Rayleigh criterion such that, in order to be resolved, the angle between two objects must be greater 
than arcsin (1.22 A / d) For a pupil diameter (d) of 0.3 centimeters and a wavelength (~) of 0.000055 
cm, this angle is 0.77 minutes of arc (14). For a 17 inch diagonal measure monitor viewed at 25 inches 
this would require at least 1788 optical lines. Since an optical line requires a dark line and a light 
llne on a display (figure two) this would mean a OPTICAL SCAN LINES LINES 1 I 2 2 3 4 3 5 6 4 I 1 ~ 
7 8 Figure two: Each optical line requires two scan lines on a monitor. resolution of 3577 TV lines 
(2). This kind of resolution is not attainable on a CRT and requires a film recorder or large plotter, 
which can only be used in non-interactive graphics systems. Even with this much resolution some aliasing 
effects, especially scintillation, will still be visible unless measures are taken to limit the bandwidth 
of the input data to less than half the sample rate. These measures now have to be computed on many times 
more data than before due to the increased number of samples. Most other aspects of picture generation 
are made more time consuming too, since the computation time of most algorithms is related to the square 
of the resolution. Another solution is to prefilter the image to remove all frequencies greater than 
one half the sampling frequency. This is done by convolving the image with the sin(x)/x function and 
does result in an elimination of aliasing, but is computationally very expensive and causes a visible 
loss of resolution. A simpler filter function is to make the intensity of each pixel the average, by 
area, of all the objects within the boundary of the pixel. This method was proposed by Crow (9), (10) 
with algorithms for its implementation given by Catmull (6) and extended by Barros &#38; Fuchs (I). This 
solution works by representing varying sized objects by varying the intensity of the pixel, and is relatively 
easy to implement. Surprisingly, results obtained with this technique are much better than could be expected 
from the elimination of aliasing effects alone. As we shall see there is actually a noticeable gain in 
resolution. This gain in resolution is also found in a process known as halftoning which is used in 
magazines and other publications (including SIGGRAEH) to reproduce continuous tone images. Halftoning 
is the opposite of the above scheme in that instead of converting size into varying intensities, it converts 
intensities into different sized dots. This results in an image which can be printed with only black 
ink while appearing to possess grey scale. In figure three notice how the blowup seems to have less detail 
than the full image. This is because the dots in the full image are small enough that they have become 
point sources. In a point source the eye does not distinguish between size and brightness even though 
the dots are easily resolved, even without looking closer. This means that the resolution at which the 
dots become point sources is considerably less than the resolving power of the eye. For high quality 
halftoning this Figure three: Halftoned image and blowup. resolution is between 85 and 133 dots per 
inch. 100 dots per inch at 12 inches from the eyes corresponds to 480 TV lines on a 17 inch monitor 25 
 inches away. Experimental data supporting this pheomenon can be found in studies of contrast thresholds 
by Blackwell (3). Figure four is a plot of visual angle versus contrast required for stimulus detection 
at various background brightnesses. These data represent over 90,000 observations. The lower portion 
of each curve shows the linear region where the stimulus is a point source. The angle subtended by the 
stimulus at the point at which the curve becomes nonlinear is called the critical visual angle for that 
brightness. At 20 foot-lamberts, the average brightness of a CRT, this value is approximately 0.5, which 
corresponds to 440 TV lines on a 17 inch monitor at 25 inches. This result agrees well with the 480 line 
resolution suggested by halftoning. Apparent Resolution Since in a point source brightness and size 
are interchangeable, subpixel details can be represented using varying grey scale. This means that the 
effective resolution of a display generated using the area anti-aliasing scheme will be greater than 
the pixel resolution. To see why this is so consider a large object with an edge covering half a pixel. 
The intensity calculated for that pixel will be 50% on (or grey). If this scheme works then the anti-aliased 
image should appear the same to the viewer as a higher resolution image with no anti-aliasing. Figure 
five is a graph of the spatial frequency response of the eye/braln (8), (11), (4), (5). Figure six shows 
the original stimulus (or how the image would appear on an infinite resolution system), and the same 
stimulus after anti-aliasing has been applied. For simplicity only one dimension is considered. In figures 
seven and eight the two stimuli are modified by the response curve of the eye/brain using Fourier analysis 
techniques. The result in figure nine shows that the anti-aliased stimulus appears essentially the same 
to the brain as the infinite resolution stimulus, and therefore that subpixel displacements of an edge 
can be represented using grey scale. Moreover, changing the brightness of the intermediate pixel will 
be interpreted as a corresponding displacement of the edge. Figure five: Spatial frequency response 
of the eye/braln, from Cornsweet (10), Davidson (11), Campbell &#38; Green (12), and Campbell ( 13). 
 Horizontal scale: 0 to 50 cycles per degree Vertical scale: 0 to 100% relative response Figure four: 
Visual angle (stimulus size) versus contrast required for detection at various ambient light ~ -"] 
I.. .. . ..... levels. From Blackwell (3). / LOG LIMINAL CONTRAST Horizontal scale: 0 to 16 pixels (one 
dimension) Vertical scale: 0 to 100% relative brightness Figure six: Visual stimulus -infinite resolution 
and anti-aliased one dimensional views of an object covering a pixel halfway. Horizontal scale: 0 to 
50 cycles per degree Vertical scale: 0 to 325 relative amplitude Figure seven: Frequency spectrum of 
stimuli. Horizontal scale: 0 to 50 cycles per degree Vertical scale: 0 to 35 relative amplitude Horizontal 
scale: 0 to 16 pixels Vertical scale: 20 to 80% relative perceived brightness. Figure nine: Infinite 
resolution and anti-aliased stimuli as they appear to the brain. Notice mach banding effects. Number 
of Grey Levels Required The effective resolution of a display is therefore determined not only by the 
number of pixels but by the number of differentiable grey levels for each pixel. Just as ~he resolution 
due to the number of pixels is bounded by the resolving power of the eye, the resolution due to grey 
levels is bounded by the number of grey levels reproducible by the display device and distinguishable 
by the eye. The number of grey levels required is difficult to determine exactly since it depends on 
room lighting, phospor characteristics of the monitor, and spot size among other things. Typically, however, 
for a 17 inch monitor with a 512 by 512 display viewed at 25 inches a single pixel subtends an angle 
of 2.7 minutes of arc. Using Blackwell's data again (figure four), the threshold contrast at this spot 
size with an average luminance of 20 foot-lamberts is approximately 4%. As for the monitor, the contrast 
ratio of any light generating display device is highly dependent upon the room lighting. Typical phosphors 
are capable of contrasts of greater than 700 to one, and a monitor viewed in a completely dark room would 
have almost this much contrast, however, under medium room lighting of 50 foot-candles this value is 
reduced to around 25 to one. With a contrast ratio of 25 to one and a contrast sensitivity of 4%, 83 
grey levels are required. Notice that the eye has greater contrast sensitivity to larger areas, which 
means that more grey levels may be required to eliminate mach banding in the entire image, but here we 
are only concerned with the contrast sensitivity at the pixel level. The situation is even more complicated 
by the fact that these levels are on an exponential scale. If using a linear scale, which is more common, 
628 levels would be required. Empirical data, however, Figure eight: Response spectrum-frequency indicates 
that eight bits of grey level data (256 spectrum of stimuli multiplied by human visual levels) is sufficient 
using a linear grey scale, with the loss of smooth shading at low pixel response. brightnesses only 
barely visible. With 256 brightness levels the smallest corresponding change in area is 1/256 of a 
pixel which would be a square object 1/16 of a pixel on a side, therefore eight bits of pixel intensity 
data will enhance the resolution sixteen times, or four bits in each dimension. For a 512 line system 
this means an effective resolution of 8192 lines, or more than twice the resolving power of the eye. 
 Note that this "effective" resolution may not be "real" resolution but it is perceptually equivalent. 
 Specifications The resulting interactive system would have a pixel resolution of 512 by 512 with eight 
bits of data per pixel. A full color system would require three times as many bits per pixel. The video 
bandwidth (~f) required is given by the formula Z~f : 0.85 H V / T where H and V are the horizontal 
and vertical resolutions, respectively, and T is the frame time (14). For a frame rate of 30 (interlaced) 
the bandwidth would be 6.7 MHz. A non-interlaced system would require twice this bandwidth. The effective 
resolution of this system is 8192 by 8192, but since the human eye is limited to around 3600 lines, less 
than 4096 of this is usable resolution. The monitor should measure 17 inches diagonally and be viewed 
from a distance of 25 inches, but other display sizes and viewing distances may be used as long as the 
viewing angle remains roughly the same. For different viewing angles the requirements will be different. 
 The low cost of such a system can be demonstrated by comparing it to the cost of an interactive system 
with a hardware resolution of 4096 by 4096. No dynamic display device with this much resolution is available 
commercially at any price. Even 1000 line CRTs are expensive and critical to adjust, and offer less than 
one fourth the resolution of the proposed system. If such a display device could be found, however, it 
would have to be driven by a graphics device capable of generating a pixel in less than 1.5 nanoseconds. 
If this were a frame buffer it would require 64 times the memory and 64 times the memory speed of a 512 
by 512 frame buffer. To put this in perspective, a 4096 by 4096 full color frame buffer built out of 
16K memory would require 24,576 memory chips alone. Problems There are a few problems associated with 
this technique which need to be discussed. Acuity. There is, of course, a loss of acuity due to the 
averaging of subpixel information. For example a scene containing many small objects spaced less than 
a pixel apart will appear as a grey field. Actually it would appear that way to the eye anyway (halftone 
dots do not appear as separate dots but as uniform fields), so this is not really so much of a problem. 
Additionally, in an interactive system the user can be allowed to zoom in on the image if more detail 
is required. Nonlinearity. There are a number of nonlinearities between the time the image is computed 
and viewed. This includes the CRT phosphors and electronics, digital to analog converters and anything 
else that has happened to the image before you see it. If the image has been photographed or reproduced 
in a magazine there are additional nonlinearities in the photographic and graphic reproduction processes. 
In order for grey scale techniques to work these nonlinearities must either be eliminated or compensated 
for. For example a line one pixel wide may either pass exactly over one pixel or may half cover two 
pixels or anything in between. Unless two pixels at half intensity have exactly the same total intensity 
as one pixel at full brightness the line will appear to be striped or "barber-poled". This effect can 
look almost as bad as the original aliasing. Techniques for compensating for these nonlinearities are 
presented by Catmull (7). Rastering. Unless a monitor is carefully adjusted there will be dark bands 
between the scanned lines of an image which are easily visible (12). Thompson (16) presents data which 
shows that viewers prefer to sit at distances from a television set at which this line structure just 
disappears. This distance is smaller for a system with more lines, therefore a 1024 line image will look 
better than a 512 line image when viewed on such a monitor. Increasing the hardware resolution is an 
expensive way to compensate for a poor or badly adjusted monitor, and only reduces the problem without 
solving it. The solution offered by Thompson is a simple technique called spot-wobble. When the line 
structure was suppressed, he found that viewers would move closer to a television monitor, increasing 
the viewing angle from eight degrees to around thirteen degrees. This compares favorably with the popular 
viewing angle for movies of 17 degrees, however, these results were also influenced by the 4 MHz bandwidth 
of the television signal used. Increasing the bandwidth to the suggested 6.7 MHz and compensating for 
display nonlinearities should increase the viewing angle even more. A 17 inch monitor at 25 inches has 
a viewing angle of 18.8 degrees, greater than the prefered viewing angle for movies: Another technique 
for reducing line structure is to employ multiple interlace. Interestingly, line structure tends to be 
slightly less apparent on color monitors since each pixel is represented by three dots instead of one. 
 Summary Under certain conditions, images produced on a display system with 512 lines of resolution 
using anti-aliasing techniques will be perceived to have as much resolution as those produced on 4000 
to 8000 line resolution systems. Luckily, these conditions include those found in most interactive graphics 
setups. This kind of resolution would be otherwise unattainable due to the limited resolutions available 
in interactive display devices, mainly CRTs. Indirect benifits include more accurate tonal rendition 
due to the elimination of display nonlinearities and a more pleasing image to the viewer. Naturally, 
this technique is not suitable when the images produced will be subject to close scrutiny, such as for 
hardcopy, but in this case the extra time required to compute a higher resolution image is not as critical 
as in an interactive system. With these few exceptions, anti-aliasing techniques can have tremendous 
benefits. Acknowledgements I am grateful to Danette Wilson for help with the photography and proofreading, 
to Roberta Maxwell for research assistance, David Freeman for criticism, and Dave Dyche for the difficult 
task of being indistinguishable from myself. References I. Barros, J. &#38; Fuchs, H., Generating smooth 
2-D monocolor line drawings on video displays. Proe. SIGGRAPH '79 pp. 260-269 2. Biberman, L., Perception 
of Displayed Information. Plenum Press, New York, 1973, pp. 3-4  3. Blackwell, H. R., Contrast thresholds 
of the human eye. J. Opt. Soc. of Am. 36-11 (Nov. 1946) pp. 624-643  4. Campbell, F. &#38; Green, D., 
Optical and retinal factors affecting visual resolution. J. Physiol. 181 (1965) pp. 576-593  5. Campbell, 
F., The human eye as an optical filter. Proc. IEEE 56-6 (June 1968) pp. 1009-1014  6. Catmull, E., A 
hidden-surface algorithm with anti-aliasing. Proc. SIGGRAPH '78 pp. 6-11  7. Catmull, E., A tutorial 
on compensation tables. Proe. SIGGRAPH '79 pp. I-7  8. Cornsweet, T., Visual Perception. Academic Press, 
New York, 1970, p. 341  9. Crow, F. C., The aliasing problem in computer-generated shaded images. Comm. 
ACM 20-11 (Nov. 1977) pp. 779-805  10. Crow, F., The use of greyscale for improved raster display of 
vectors and characters. Proe. SIGGRAPH '78 pp. I-5  11. Davidson, M., Perturbation approach to spatial 
brightness interaction in human vision. J. Opt. Soc. of Am. 58-9 (Sept 1968) pp. 1300-1308  12. Erickson, 
R. &#38; Hemingway, J., Visibility of raster lines in a television display. J. Opt. Soc. of Am. 60-5 
(May 1970) pp. 700-701  13. Freeman, H., Computer processing of line-drawing images. Computing Surveys 
6-I (March 1974) pp. 57-97  14. Schade, O., Electro-optical characteristics of  television systems. 
RCA Review 9-6 (June 1948) pp. 5-37 15. Szabo, N., Digital image anomalies: static and dynamic. SPIE 
v. 162 Visual Simulation &#38; Image Realism (1978) pp. 11-15  16. Thompson, F., Television line structure 
suppression. J. SMPTE 66-10 (Oct. 1957) pp. 602-606  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1980</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>807510</article_id>
		<sort_key>314</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-1980</article_publication_date>
		<seq_no>51</seq_no>
		<title><![CDATA[Techniques for interactive raster graphics]]></title>
		<page_from>314</page_from>
		<page_to>320</page_to>
		<doi_number>10.1145/800250.807510</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=807510</url>
		<abstract>
			<par><![CDATA[<p>The visual quality of raster images makes them attractive for applications such as business graphics and document illustration. Such applications are most fully served using interactive systems to describe curves, areas and text which can be rendered at high resolution for the final copy. However, to present such imagery in an interactive environment for moderate cost is difficult. Techniques are presented that provide solutions to the problems of scan conversion, screen update, and hit testing for a class of interactive systems called <italic>illustrators.</italic> The design rests on the use of software display file encoding techniques. These ideas have been used in the implementation of several illustration programs on a personal minicomputer.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Chain encoding]]></kw>
			<kw><![CDATA[Computer graphics]]></kw>
			<kw><![CDATA[Display encoding]]></kw>
			<kw><![CDATA[Illustration systems]]></kw>
			<kw><![CDATA[Interactive graphics]]></kw>
			<kw><![CDATA[Run-length encoding]]></kw>
			<kw><![CDATA[Scan conversion]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.1</cat_node>
				<descriptor>Raster display devices</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.6</cat_node>
				<descriptor>Interaction techniques</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10010391</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Graphics input devices</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372.10010373</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering->Rasterization</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010583.10010588.10010591</concept_id>
				<concept_desc>CCS->Hardware->Communication hardware, interfaces and storage->Displays and imagers</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P332796</person_id>
				<author_profile_id><![CDATA[81100375394]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Patrick]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Baudelaire]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[TECSI, 29, rue des Pyramides, 75001 PARIS, FRANCE]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP31095950</person_id>
				<author_profile_id><![CDATA[81100388123]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Maureen]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Stone]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Xerox Palo Alto Research Center, Palo Alto, California]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Baudelaire, P., Flegal, R.M., and Sproull, R.F., Spline curve techniques, Xerox Palo Alto Research Center Internal Publication, (December 1977).]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Erdahl, A.C., Displaying computer-generated half-tone pictures in real time, University of Utah Computer Science Technical Report, Salt Lake City, (1969), 4-14.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>356627</ref_obj_id>
				<ref_obj_pid>356625</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Freeman, H., Computer processing of line-drawing images, ACM Computer Surveys, Vol. 6, No. 1, (March 1974), 57-97.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Laws, B., and Newman, W.M., A gray-scale graphics processor, using run-length coding, Proceedings of the IEEE Conference on Computer Graphics, Pattern Recognition, and Data Structures, (May 1975).]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>5532</ref_obj_id>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Newman, W.H. and Sproull, R.F., Principles of Interactive Computer Graphics, 2nd edition, McGraw Hill, New York, 1979.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Pitteway, M.L.V., A simple data compression technique for graphics displays or incremental plotters, Symposium on Computer Processing in Communications, (April 1969).]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807418</ref_obj_id>
				<ref_obj_pid>965103</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Shoup, R., Color table animation, Computer Graphics, Vol. 13, No. 2, (August 1979), 8-13.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Shoup, R., Some experiments in television graphics and animation using a digital image memory, Presented at the 13th Television Conference of the SMPTE and published in Digital Video, Vol. II, SMPTE, Scarsdale, New York, 1979, 88-98.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Smith, A.R., Paint, Technical memo #7, Computer Graphics Lab, NYIT, Old Westbury, New York, 11668, (July 1978).]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807456</ref_obj_id>
				<ref_obj_pid>965103</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Smith, A.R., Tintfill, Computer Graphics, Vol. 13, No. 2, (August 1979), 276-283.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Sproull, R.F. and Newman, W.M., The design of gray-scale graphics software, Proceedings of the IEEE Conference on Computer Graphics, Pattern Recognition and Data Structures, (May 1975).]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807428</ref_obj_id>
				<ref_obj_pid>965103</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Sproull, R.F., Raster graphics for interactive programming environments, Computer Graphics, Vol. 13, No. 2, (August 1979), 83-93.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Thacker, C.P., McCreight E.M., Lampson, B.W., Sproull, R.F., Boggs, D.R., Alto: a personal computer, Xerox Palo Alto Research Center Internal Publication, (July 1979).]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Techniques for Interactive Raster Graphics Patrick Baudelaire* &#38; Maureen Stone Xerox Palo Alto Research 
Center Palo Alto, California ABSTRACT The visual quality of raster images makes them attractive for 
applications such as business graphics and document illustration. Such applications are most fully served 
using interactive systems to describe curves, areas and text which can be rendered at high resolution 
for the final copy. However, to present such imagery in an interactive environment for moderate cost 
is difficult. Techniques are presented that provide solutions to the problems of scan conversion, screen 
update, and hit testing for a class of interactive sytems called illustrators. The design rests on the 
use of software display file encoding techniques. These ideas have been used in the implementation of 
several illustration programs on a personal minicomputer. Key Words and Phrases: computer graphics, interactive 
graphics, display encoding, chain encoding, run-length encoding, scan conversion, illustration systems. 
CR Categories: 8.2 INTRODUCTION Pictures are a part of a large number of applications. A certain class 
of pictures can be referred to as illustrations. That is, the point of the picture is to illustrate a- 
principle as part of an article .or presentation. Good illustrations are visually interesting, but are 
judged more on their content than on their artistic merit. With respect to picture complexity, illustrations 
are simple in that there are a moderate number of shapes, clearly bounded by curves and lines. The image 
is essentially two dimensional, and color is often restricted to flat filled areas; that is, uniform 
colors and simple textures. Illustrations are important in business and publishing environments today. 
The advantages to be gained from using a computer to generate these images are similar to the ones for 
word processing: images are easily modifiable, pictures can be filed and copies easily generated, subimages 
can be libraried to facilitate the composition of a series of illustrations. However, to be acceptable 
outside of the experimental environment, the image quality of illustrations must be high. Curves and 
lines must be smooth, text *Present Address: TECSI, 29, rue des Pyramides, 75001 PARIS, FRANCE Permission 
to copy without fee all or part of this material is granted provided that the copies are not ~ade or 
dlstributed for direct com~ercial advantage, the ACM copyright notice and the title of the publication 
and Its date appeart and notice is given that copying is by permission of the Association for Couputlng 
Machinery. To copy otherwise, or to republlsh, requires a fee and/or specific permission. must be represented 
in a variety of fonts, objects must be accurately positioned. While each illustration may contain a limited 
number of colors, a wide range must be available. Given the type of imagery desired, a raster display 
will give much better representation of the picture than a line oriented display. The constraint of high 
image quality, even after scaling and repositioning operations, means that the picture must be stored 
using a high precision representation. Furthermore, if the data base has sufficient precision, the set 
of affine transformations can be used as tools for generating images. The process of designing an image 
is the mapping of some visualization onto a medium. Therefore, that any effective tool should provide 
visual feedback as the image was formed. Furthermore, the image should be built up in a two-dimensional 
manner, that is, by pointing to positions on a page, which indicates an interactive system. In this paper, 
such a system will be called an 'illustrator. Given a high precision data base, such as endpoints for 
line% coefficients for splines, plus information about the resolution of the display, there are many 
standard techniques for displaying straight lines, curves and areas (5). This process is known as scan 
conversion. There are a number of problems in using these techniques in an illustrator, specifically: 
All straight lines and curves must be represented with a finite width. In an illustration, using lines 
of different widths is part of the visual effect. Therefore, displaying these shapes involves more than 
just digitizing the curve. Areas are discribed by their outline. Therefore, it is necessary to compute 
which points are inside the outline and color them in accordingly. While there are many techniques which 
address this problem (10, 11) in an interactive system the speed at which this can be done is an important 
factor as the display is constantly changing as the user works. In an interactive system, the display 
is constantly changing. For speed considerations, it is desirable to do this incrementally, updating 
only the areas that are affected by the change. Incremental refresh can cause problems displaying the 
correct overlap order for objects. In an interactive system it is necessary for the user to select objects 
to be manipulated. A natural way to do that is to point to the object. Therefore, given a point on the 
display, it is necessary to determine which object has been selected. This is called hit testing. &#38;#169;1980 
ACM 0-89791-021-4/80/0700-0314 $00.75 Hit testing is the inverse of the display problem, and is even 
more speed critical than refresh. This paper will discribe methods for solving these problems that achieve 
a reasonable compromise in simplicity of implementation and response time. The solution is based on using 
display encoding techniques which provide a representation of the image that is compact, structured, 
and simple to manipulate. These techniques have been implemented in systems which have been used to successfully 
create illustrations. Examples are included at the end of the paper. DATA REPRESENTATIONS AND SYSTEM 
DESIGN Graphics systems can be categorized by the type of data used to represent the picture. One type 
of interactive graphics systems uses raster dots or arrays of intensity samples as the unique representation 
of the image. These systems use a painting model to manipulate the dots directly. These systems are easy 
to use, and have been shown to produce effective imagery (8, 9). However, the picture is unstructured, 
which limits the types of manipulations that can be performed without special hardware. The resolution 
is tied strongly to the display resolution, which can limit image quality. In general, to be effective 
visually, the images from such systems require large amounts of data, either in resolution or in bits 
per pixel. We have chosen a more structured approach. Given a geometric representation, one can consider 
that geometric elements (lines, curves, areas) are all "made of filled contours. This paradigm is quite 
useful for applications such as the production of rasters for high-resolution printing of graphics and 
text (1). However, this model is difficult to implement in an interactive application because using the 
geometric data for display and hit testing can be expensive in terms of computation time. An interactive 
display file can be used to provide a definition of the picture that is structured, yet can be manipulated 
fast enough for an interactive system. It is natural for several representations to coexist in the design 
of a graphics system, usually according to levels of hierarchy. At the top level may exist a representation 
that embodies some specific knowledge or meaning relevant to a particular application: architectural 
drawings, blueprints for mechanical parts, electrical circuit diagrams, etc. Here we will consider that 
the top level representation that we chose aims only at giving a unambiguous and complete description 
of high-quality business graphics illustrations. To implement these ideas, the following systems approach 
was used: The workbench is a moderate resolution raster display plus pointing device attached to a 16 
bit mini-computer. The final output device is a high resolution raster device such as a film recorder, 
phototypesetter, or raster printer. Shapes are defined by their geometry: trajectories and contours; 
plus style informations such as line width, colors, and textures. Trajectories can be specified by one 
of several mathematical schemes such as splines or other knot-based approximations, circles or other 
conical equations. Text is unformatted, and described in terms of position and string information plus 
style parameters such as font, color, and orientation. The user builds display objects by pointing at 
fitting points and indicating fitting methods such as straight lines or curves. All numbers are represented 
as floating point values to provide sufficient precision. The top level description is converted to an 
interactive display file, where the interactive processing, refresh and hit testing, will take place. 
The display file is used to generate a bitmap, that is a one bit per point rectangular array of Output 
Device (high resolution) I  Picture Definition (geometric representation) Interactive [ Display File 
[ input-, (encoded representation)r-~pointing devicy Display (raster representation)  Figure 1: Diagram 
Showing Levels of Representation samples (13), which is used as the refresh buffer for the display. 
This structure is summarized in figure 1. THE INTERACTIVE DISPLAY FILE The display file representation 
is used for refresh and hit testing. In designing such a representation, the first consideration is to 
what precision the objects should be encoded. Clearly, display resolution is sufficient for refresh. 
Since the user cannot specify a position by pointing which is more precise than the display resolution, 
it is also sufficient for hit testing. Both hit testing and incremental refresh involve scanning through 
the display file to determine what objects are in a specified area. Therefore, partitioning the display 
file to facilitate culling will increase performance. It is important that the display file be reasonably 
compact, yet not so difficult to generate or decode that it negates the advantages in terms of speed. 
It is also convenient to be able to encode all objects in a uniform manner. Given these considerations, 
we have applied and modified some well known encoding techniques, chain encoding of trajectories (3, 
6) and run-length encoding of areas (2, 4). These techniques are essentially compressed representations 
of the bitmap. Chain encoding is based on the assumption that edges are continuous. Therefore, the basic 
representation is the differences between adjacent pixels. Run-length encoding is based on the assumption 
that fiat areas contain most of the information in the outlines. Therefore, the basic representation 
is the position of the edge on each scan line. While it would be possible to run-length encode all objects, 
the increased structure in the chain encoding is appealing for lines. The particular encoding schemes 
chosen permit the segmentation of each object into pieces that are independent and bounded in display 
size. It follows from this that the time for display of one ,piece is bounded too. This makes it possible 
to run the screen refresh as a background process. All shapes can be described by one of the two types 
of encoding. Thus, pointing detection can be done by a single algorithm, independent of the mathematical 
definition of the elements (lines, circles, conics, splines, etc.). CHAIN ENCODING Chain encoding is 
a differential encoding scheme which records the screen coordinate increments between successive raster 
points on a trajectory. That is, from one trajectory point to the next, raster coordinates may differ 
only by -1, 0 or 1. Thus, a point may have eight possible successors, so each point new position could 
be represented in four bits. But, since common curve trajectories are monotonic for reasonably long intervals, 
one can take advantage of the continuity in direction to further reduce the number of bits per point. 
A number of schemes are possible for encoding coordinate increments. Two interesting and practical ones 
are described below. The first scheme uses two-bits "to represent the coordinate increments (figure 2). 
The set of eight possible curve directions is divided into four quadrants. For each direction quadrant, 
the three possible coordinate increments are assigned code values 1 to 3. Code value 0 is used to indicate 
a change of quadrant, with the following two bits specifying the new quadrant. Therefore, the trajectory 
encoding is a stream of two-bit codes, starting with the quadrant number (0 to 3), followed by increment 
codes (1 to 3), terminated by a 0. The second scheme requires two streams (figure 3). The set of eight 
possible curve directions is divided into eight octants. Within each octant there are two possible directions. 
Therefore, it is possible to indicate each step within an octant with one bit. One stream contains the 
one-bit increment codes for a given direction octant, and the second stream contains the octant numbers 
and the number of steps in each octant. Besides being somewhat more efficient, it is possible to get 
a general idea of the behavior of a curve segment by examining the dir&#38;tion octants alone. An example 
of where this feature can be used is for defining edges for the scan converter. Using the chain encoding 
in the scan conversion process will be described further below. As mentioned above, in order to gain 
efficiency in the screen updating and pointing detection algorithm, the encoding is 2 1 3 4  Quadrants 
I 1 3 2  4,,1kl Z 5/6 Octants />2 --- 111111 "--IIIIII °°° ..~ r bits / iI r I...  octant length 
Figure 3: Chain Encoding, One Bit/Direction fragmented into independent pieces of similar length that 
we call chunks (figure 4). The bounding box for each chunk can be stored to facilitate culling on refresh 
and hit testing. The display file contains the following information for each chunk: Screen coordinates 
of the starting point S Stream(s) for the chain encoding The bounding frame: H and W. It is interesting 
to note that if the chunk size is such that each segment contains at most N trajectory points, all these 
points are enclosed in a square of size 2N centered at the starting point of the segment. So even if 
the bounding box is not explicitly stored with the chunk, a bounding region can be computed. quadrant 
quadrant W Figure 2: Chain Encoding, Two Bits/Direction Figure 4: Curve Divided into Encoded Chunks 
RUN-LENGTH ENCODING Run-length encoding defines an area in terms of a ~ta~ng scan line (Y) value plus 
a list of pairs of raster (X) values. In practice, it is more efficient to make the second X value relative 
to the first, so each run is defined as a starting X (SX) and a delta X (DX) value. The list of runs 
can be broken into chunks, such that each chunk defines a maximum of N runs. Therefore, each chunk has 
a bounding frame. It is desirable to make the starting X values relative to this frame, so that the chunk 
can be relocated simply by translating the frame boundries. (figure 5) The encoding described above works 
only for convex areas, specifically, it assumes one continuous run of rasters per scan line within the 
area. For concave shapes, there are two options: break the area into convex pieces, or introduce a flag 
into the list of runs that defines the number of runs per scan line. We have implemented the second option, 
choosing a negative starting X as a flag, signaling that the delta X value is the new number of runs 
per scan line. Assuming the run-length encoding describes areas at display resolutions, the starting 
and delta X values need be no larger than the maximum display coordinate. In practice, most runs can 
be described in fewer bits. Therefore, it is worthwhile to consider using a variable field for X. While 
maximum compression would be obtained by using a technique such as Huffinan coding for field size, for 
simplicity we have chosen to implement two fixed fields (8 and 16 bits). The display file contains for 
each chunk: Frame boundry: upper left point S in screen coordinates (includes the starting Y value), 
plus frame size H and W. Field length: either bytes or words. List of run values, defined as SX relative 
to S, and DX relative to SX. SCAN CONVERSION The geometry to rasters conversion scan conversion process 
can be decomposed in two operations: Converting the geometry into the display file representation, and 
converting the display file to rasters. The conversion from geometry to display file only need be performed 
when an object is created or the shape is changed. Because this is a relatively infrequent operation, 
standard techniques for digitizing curves and filling areas provide acceptable speed. The specific algorithm 
used for areas is given at the end of this section. Objects are displayed in back to front order to give 
the correct overlap information. All display, refresh, repositioning, and hit testing operations can 
be implemented by manipulating the display file. Display The display operation is defined as the conversion 
from the display file representation to the rasters which are stored in the bitrnap. The final action 
of writing raster bits into the bitmap is implemented by an efficient and versatile firmware function 
called RasterOp. This function copies and modifies bit patterns from an arbitrary rectangular area in 
picture memory to another rectangular area. RasterOp is described in more detail in (13) and (5). For 
areas, the run-length encoding can be displayed directly using RasterOp to display each run as a one 
line high area. To draw curves we use the paradigm of painting with a "brush" moving along their trajectories. 
The chain encoding provides a digitized representation of the trajectory. A round brush will approximate 
a line of uniform width equal to the brush diameter. In the simplest implementation of this model, RasterOp 
can be used to paint an image of the brush at each new raster position along the trajectory. However, 
because the successive brush images overlap, many of the pixels along the trajectory are written several 
times. The model can better be implemented by breaking down the brush into a set of horizontal sections 
and accumulating the rasters filled as the brush moves. Once a scan line is complete, that is, the brush 
has moved far enough that it no longer touches the scan line, the entire horizontal section can be displayed 
using RasterOp. This implementation is more efficient than the traveling brush because the affected rasters 
are only written once. Furthermore, the form of RasterOp used to display lines has been reduced to the 
same case used for areas. This uniformity makes applying colors or halftones to objects straightforward. 
Refresh In an interactive graphics system, the displayed image is constantly changing as the user adds, 
transforms, and deletes objects. In a system which contains only lines and curves, it is possible to 
write all new objects as they are created or repositioned, and to simply leave the small areas that are 
erased out of overlapping objects (from a transform or delete operation) unrefreshed. The user can then 
replot the entire picture when the image degrades too much. For systems which include filled areas, this 
approach is inadequate because the amount of the picture obliterated by erase operations overlapping 
other objects is too extensive. It is therefore necessary to find some way to quickly and accurately 
refresh subareas of the entire picture. We will call this process incremental refresh. The screen update 
process should be as fast as possible, yet must leave the screen in the correct state as to the shape 
of the objects and their overlap order. It follows that the design considerations for an incremental 
refresh algorithm are: The definition of an area to refresh should contain a minimum number of rasters 
that need to be regenerated. Objects within the refresh area must be replotted quickly and with no rippling 
effects. Figure 5: Run-Length Encoding, Showing Chunks If the incremental refresh is going to run as 
a background process, the time necessary to refresh an area must be bounded. In general, the affected 
area has such a complex boundary that .determining exactly which rasters fell inside the outline would 
be too time consuming a process. It is therefore necessary to use some approximation to the area. The 
simplest approximation is a rectangle. While for certain operations, such as erasing a diagonal line, 
the minimum rectangle that describes the area affected covers far more of the picture than actually need 
be redisplayed, rectangles are much easier to manipulate than other shapes such as trapezoids. Once a 
refresh area has been defined, the objects which are affected must be found. This is the same process 
as hit testing, except that each object must be compared to the boundaries of the refresh area instead 
of a small area around a point. If the display system contains an efficient clipper with variable clipping 
boundries, the update problem can be solved simply by setting the clipping region to the boundries of 
the affected area and refreshing the entire screen. In the type of system we are describing here, since 
the rasters are generated from the display file, the partitioning of the display file into bounded chunks 
provides a method for fast culling for this type of refresh. If the clipper is not used, the problem 
of rippling effects Can arise because the object or chunk definition will in general generate rasters 
outside of the refresh area, which can affect obje¢ts not currently in the list of objects to be refreshed. 
For example, in figure 6a, object A is shown as overlapping object B. Part of object B needs to he refreshed 
(figure 6b). In refreshing object B, care must be taken that the correct overlap order is maintained 
between A and B. If overlap order is determined simply by back to front refresh of the display, just 
replotting all of B will result in B appearing to be on top of A (figure 6c) unless figure A is also 
refreshed. It is common for the user of an interactive graphics system to operate on several objects 
in a picture at one time leaving several areas needing to be refreshed. These objects may or may not 
overlap. One approach is to simply accumulate a maximum area as each object is operated on. However, 
if the objects are disjoint, it can give a very bad estimate of the affected area. Another approach is 
to treat each object independently. However, if the objects overlap, intersecting refresh areas will 
be redisplayed several times, which, while leaving the display in a correct state, is distracting to 
the user as well as time consuming. A third approach is to accumulate a refresh area for each object, 
and then process these areas to eliminate overlap cases. In a system where RasterOp is the limiting factor 
for display operations, the third approach is far superior to the other two. Area Scan Conversion Polygon 
scan conversion is described in detail in chapter 16 of (5), and much of the terminology in this section 
will be taken from that source. Here we would like to outline an approach that has been shown to be adequate 
for line and curve bounded areas, including concave areas, areas with holes, and areas with twists. 
The problem is to display solid areas which have overlap order but no depth. That is, areas do not intersect 
in the Z direction. The areas are bounded by combinations of spline curves and lines, and are not strictly 
convex. The pictures displayed are of moderate complexity, probably around 20-25 areas. a) Objects A 
and B b) Rectangular region erased c) Replotting all of B gives incorrect overlap  Figure 6: Rippling 
Efforts of Refresh Overlap order is resolved using the painters algorithm. That is, objects are displayed 
in overlap order, back to front such that front objects simply "paint over" objects that are behind them. 
The outline of each area is chain encoded. Each chunk is constrained to be monotonic. Scan conversion 
occures at display resolution, using the chain encoding as the edge definition. Each area is taken separately, 
and the outline is broken into a list of "edges" which are monotonic in the scan direction. This is determined 
by examining the chunks of chain encoding. These edges can be sorted on Y, then the intersections for 
each scan line are determined by running along the encoding. The X values are sorted, and taken pairwise 
to define the filled interior of the object. This is essentially the Y-X algorithm described for polygons 
in (5). Care must be taken when defining the edges that endpoints for edges are properly defined. The 
chain encoding is one continuous stream with one definition for each raster point. However, it is essential 
for the Y-X algorithm to have an even number of edges for each scan line. Therefore, the endpoint of 
edges that fall at maxima/minima of the object must be doubled. Furthermore, points which are not at 
a maximum/minimum in the scan direction must not be defined on two edges. This can be achieved by making 
edge boundries only at maxima/minima in the scan direction (figure 7). It is important to note that there 
may be horizontal sections inside an edge (figure 8). Therefore, it is necessary to return a range of 
X values for each edge intersection at each scan line. The left or right value is taken depending on 
whether the edge is a left or fight edge. This determination takes place after the X values are sorted. 
The main advantages to this approach are speed and Consistency. All curves are converted to rasters using 
some standard algorithm. Figure 7: Edge, Definition   imi -,,-- ) Figure 8: Horizontal Sections Inside 
Edge Once this is done, any analysis of the curve, such as for monotonicity or for intersections, is 
done using the chain encoding. Furthermore, if the area is outlined, the outline and the edge of the 
filled interior are guaranteed to match since they come from the same digitizing algorithm. HIT TESTING 
By the term "hit testing" we mean given some target point, usually the display coordinates of a cursor, 
which is the selected object? In hardware augmented systems, hit testing is done by the clipping an d 
display system. The clipping boundry is set to a small window around the hit point, and the entire picture 
is refreshed. An object falling inside the window is returned as a possible hit. in a raster system, 
redisplaying the object list is too slow, and redisplaying the rasters provides no structural information. 
However, the segmented encoding provides both structural information and a way to quickly determine which 
objects are candidates. The flame information on the encoding chunks provides a method for quickly culling 
out those segments which do not fall near the target point. The remaining chunks can be decoded using 
the same routines which display the encoding, except that the resulting points are compared to the target 
point instead of being displayed. Once the objects which lie near the target point are identified, some 
algorithm must be applied to select one of the objects. Some considerations are: absolute distance from 
the point, overlap order of the objects, prefered objects, etc. CONCLUSIONS Using these design principles, 
it is possible to make an interactive system which uses a raster display for design, yet has a geometric 
data structure which can be used to generate quality output on a high resolution raster device such as 
a film-recorder, a photo-typesetter, or a laser printer. Two systems using these principles have been 
designed and implemented on the Alto personal computer. One provides only lines and curves, the other 
also provides the capabilty for filled areas. Typical imagery is shown in figures 9 and 10 and in the 
illustrations in this paper. ACKNOWLEDGEMENTS Several people have contributed significantly to the design 
and implementation of the systems this paper is based on. The authors would like to thank Rick Tiberi, 
Geoff Brown, and John Dawson for their help and ideas. XEROX Telecommunications Guide &#38; Directory 
 WESTERN EDITION Figure 9: Telecommunications Guide-and Directory by Patrick Baudelaire BIBLIOGRAPHY 
Baudelaire, P., Flegal, R.M., and Sproull, R.F., Spline curve techniques, Xerox Palo Alto Research Center 
Internal Publication, (December 1977). Erdahl, A.C., Displaying computer-generated half-tone pictures 
in real time, University of Utah Computer Science Technical Report, Salt Lake City, (1969), 4-14. 3. 
Freeman, H., Computer processing of line-drawing images, ACM Computer.Surveys, Vol. 6, No. 1, (March 
1974), 57-97. 4. Laws, B., and Newman, W.M., A gray-scale graphics processor, using run-length coding, 
Proceedings of the IEEE Conference on Computer Graphics Pattern Recognition, and Data Structures, (May 
1975).   
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1980</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>807511</article_id>
		<sort_key>321</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-1980</article_publication_date>
		<seq_no>52</seq_no>
		<title><![CDATA[Filling regions in binary raster images]]></title>
		<subtitle><![CDATA[A graph-theoretic approach]]></subtitle>
		<page_from>321</page_from>
		<page_to>327</page_to>
		<doi_number>10.1145/800250.807511</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=807511</url>
		<abstract>
			<par><![CDATA[<p>Filling regions in raster images is the term given to the problem of extracting a connected region (that contains some preselected <underline>seed</underline> pixel) and filling it with some <underline>color</underline>. A connected region is formally defined as the collection of all pixels that are in the transitive closure of a pixel-connectivity operator that is applied to the seed. For example, the 4-pixel-connectivity operator selects all pixels that are (spatially) 4-connected to the pixel operand, and have the same color. This problem can be solved relatively easily if the fill color is distinguishable so that every pixel, once colored (i.e. flagged OLD) will not be considered again. In binary images, the fill &#8220;color&#8221; is usually a binary pattern and therefore, a &#8220;colored&#8221; pixel may still have its previous (e.g., black or white) color. This fact makes the problem non-trivial.</p> <p>In this paper, a region to be filled is represented as a connected directed a-cyclic planar graph in which nodes are <underline>regular regions</underline> (defined below) that are easy to handle. An arc connects a regular region to its neighbor below which shares a common horizontal sub-boundary. Based on this abstract representation of a region, a formulation of the filling problem is developed as a variant of graph traversing. The difficulties imposed by filling with a binary pattern, and the avoidance of an explicit description of the graph are explored and a solution is presented. This solution utilizes the frame buffer (that is used to display the image) for improved efficiency of a graph traversal algorithm.</p> <p>This method turns out to be similar to [Lieberman-78], that is shown here to be incorrect. The complexity of the new algorithm is 0(N*L+ N*Log N), compared to 0(N*L*Log N) there, where N is the number of nodes in the graph and L is the average composite degree of each node. A proof of correctness for the new algorithm is given too.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Binary raster graphics]]></kw>
			<kw><![CDATA[Computer graphics]]></kw>
			<kw><![CDATA[Filling]]></kw>
			<kw><![CDATA[Graph theory]]></kw>
			<kw><![CDATA[Region representation]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.1</cat_node>
				<descriptor>Raster display devices</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>G.2.2</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003624.10003633</concept_id>
				<concept_desc>CCS->Mathematics of computing->Discrete mathematics->Graph theory</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372.10010373</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering->Rasterization</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010583.10010588.10010591</concept_id>
				<concept_desc>CCS->Hardware->Communication hardware, interfaces and storage->Displays and imagers</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP39033906</person_id>
				<author_profile_id><![CDATA[81543805056]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Uri]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Shani]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Computer Science Department, University of Rochester, Rochester, NY]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>578775</ref_obj_id>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Aho, A.V. et al. The Design and Analysis of Computer Algorithms, Addison Wesley, (1976).]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>1096893</ref_obj_id>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Berge, C. Graphs and Hyper Graphs, North Holland/American Alsevier, (1976).]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>359618</ref_obj_id>
				<ref_obj_pid>359605</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Guttag, S. Abstract data types and the development of data structures, CACM, 20(6), (June 1977), 396-404.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Kay, A.L. Microelectronics and the personal computer, Scientific American 273(3), (September 1977), 231-244.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>260999</ref_obj_id>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Knuth, D.E. The Art of Computer Programming, vol 1, section 2.2.1.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807380</ref_obj_id>
				<ref_obj_pid>800248</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Lieberman, H. How to color in a coloring book, Proceedings SIGGRAPH'78, (August 1978), 111-116.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807456</ref_obj_id>
				<ref_obj_pid>800249</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Smith, A.R. Tint fill, Proc. SIGGRAPH'79, (August 1979), 276-283.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Filling Regions in Binary Raster Images: A Graph-Theoretic Approach. Uri Shani Computer Science Department 
University of Rochester Rochester NY 14627 KEY WORDS AND PHRASE: computer graphics, filling, region 
representation, binary raster graphics, graph theory. CR CATEGORIES: 8.2, 5.25, 5.32. ABSTRACT Filling 
regions i D raster images is the term given to the problem of extracting a connected region (that contains 
some preselected seed pixel) and filling it with some color. A connected region is formally defined as 
the collection of all pixels that are in the transitive closure of a pixel-connectivity operator that 
is applied to the seed. For example, the 4-pixel-connectivity operator selects all pixels that are (spatially) 
4-connected to the pixel operand, and have the same color. This problem can be solved relatively easily 
if the fill color is distinguishable so that every pixel, once colored (i.e. flagged OLD) will not be 
considered again. In binary images, the fill "color" is usually a binary pattern and therefore, a "colored" 
pixel may still have its previous (e.g., black or white) color. This fact makes the problem non-trivial. 
 In this paper, a region to be filled is represented as a connected directed a-cyclic planar graph in 
which nodes are regular regions (defined below) that are easy to handle. An arc connects a regular region 
to its neighbor below which shares a common horizontal sub-boundary. Based on this abstract representation 
of a region, a formulation of the filling problem is developed as a variant of graph traversing. The 
difficulties imposed by filling with a binary pattern, and the avoidance of an explicit description of 
the graph are explored and a solution is presented. This solution utilizes the frame buffer (that is 
used to display the image) for improved efficiency of a graph traversal algorithm. This method turns 
out to be similar to [Lieberman-78], that is shown here %o be incorrect. The complexity of the new algorithm 
is O(N*L+ N'Log N), compared to 0(N'L'Log N) there, where N is the number of nodes in the graph and L 
is the average composite degree of each node. A proof of correctness for the new algorithm is given too. 
 Permission to copy without fee all or part of chls ~Cerlal is granted provided that the copies are not 
made or distributed for direct commercial advantage, the A(~ copyrlght notice and 0]980 ACM 0-8979]-02]-4/80/0700-032] 
$00.75 1.0 INTRODUCTION Raster graphics devices are of ever growing popularity among computer users. 
Their usefulness is obvious for computer image analysis, computer animation and cartoons, ~i~ure and 
drawin~ editing, drafting, and so on. ~uch devices are found as satellites o~ the main-frame computer, 
or even as super-smart advanced CRT terminals and mini-computers (li~e the ALTO that is used for the 
implementation of the ideas in this paper [Say-W]). In these devices the image is stored in a frame-buffer. 
The frame-buffer is a fast memory that is "mapped" to a C~ display many times (&#38;O) a second, in such 
a way that every pixel in the buffer is translated into light intensity on a corresponding spot on the 
display. In computer-aided design, drawin~ and figures editing, or drafting, a high resolution display, 
even on the account of the number of bits per pixel, is desired. Figures and images are mainly line drawings, 
and closed re~ions are painted (filled) with patterns or textures rather than by a continuous tone shading. 
In most cases the frame buffer is accessible bidirectionally %o the controlin~ computer, and may be used 
as the working space for programs that ~ill re~ions in displayed images. Havin~ a single copy o¢ the 
region, difficulties in fillin~ i% arise when the region is sli~ht]~ non-trivial, i.e. it has "islands" 
so that the filling program may ~o around such an island forever. If t~e ~illin~ paint differs from the 
original color o{ the region, then it may be naturally used to mark painted pixels as 'o18' (i.e. not 
tO be considered a~ain). This case is solved by "simple fill" [Smitb-70]. When the frame buffer is binary 
(but of a high resolution), fill "color" is a binary pattern which may leave some of the painted pixels 
with their old color, and thus undistinguished from not yet ~ainted pixels in the re~ion. An example 
of such a binary pattern is a checker-board pattern: the title of the publication ~d its date ~pasr, 
and notice is given that copying is by pertaisslon of the ~s~latlon for Computing ~chlne~. To copy othe~ise, 
or to rapubllsh, 32] requires a fee and/or specific permission. p.color := if (p.x + p.y) is ODD For 
example, let the connectivity then white operator be 4-connectivity defined as: else black ; Where: 
p.color, p.x and p.y are the color, x and y coordinates of the pixel ~, respectively. This pattern is 
a function of the absolute location of the pixel. Another possibility is to make it relative to the seed-pixel, 
location, or any other method one may think of. The main point in this paper is not the generation of 
a fill pattern, but the correct fill of the complete region, knowing that the fill "color" is unreliable 
as a marker for already painted pixels.  2.0 REGION REPRESENTATION Two of the many possibilities to 
represent a bounded region in a raster image are: I) a set of pixels (elements of the image), each represented 
by a pair of coordinates (x,y) as in fig 1.a; and 2) a set of scan segments, each represented as a triple 
(y,xl,x 2) that means: all pixels with coordinates (x',y) such that x1<x'<x 2 are in the scan segment 
 (fig 1.b). The region, in this context, is a white area, outlined with black boundary, as it will be, 
without loss of generality, the convention all through this paper. (×i, Yl) ! 2 a. ~-((Xl,Yl),(x2,~),... 
} b. R= ((~l,Xl,Xi),...)  Fig I." representations of a region in a raster image that has two islands 
inside: a) R is a set of pairs (xi,Yi) , each is a pixel in the image (a grid square in the figure); 
and b) R is a set of triples (Yi,Xli,y2i) , each is a scan segment (a strip in the drawn strips pattern). 
 A constructive way to represent such a region is by defining a connectivity operator, and choosing a 
"seed" pixel in the region. Then the region is defined to be the transitive closure of the connectivity 
operator when applied to the seed pixel. C4(P)= { all pixels q_ s.t. I ((,q.x-p.xl + lq.y -~.vl) = 
I) and (Value(p) = Value(q) I. In other words, p and q must have a common edge and the sam~ valu~ (=color). 
 The transitive closure of C 4 is defined recursively as: Closure(C4)(p)= { all pixels q s.t. (q in 
C4(p) F or (exist r s.t. (q in-- C4(r)) and (~ in Closure(C4)(p)) ) }. In other words, ~ is in the 
transitive closure if there is a "chain" of pixels connected to each other so that one is connected to 
~ and one is connected to p. The connectivity operator can he extended to scan segments or any other 
bigger element (that consists of more than a single pixel): ExtendedC4(P)= { all scan segments Q s.t. 
( exist (q in 0) and (p Yn P) s.t. in--C4 (P)) }. Abstractly, a region R can be represented as a planar 
connected directed graph G [Berge] in which nodes are region elements and arcs go between connected nodes. 
Nodes may be pixels (fig 1.a), or scan segments (fig 1.b), or more complex regular regions as depicted 
in fie 2.a. The graph representing that region is shown in fig 2.b. In the followinR text the terms 
node and regular-region will he used interchangeably. A regular reRi6n is such that ~t is easy to fill 
by a systemstic scanning of its elements, and since a regular region does not have islands ana turns, 
this should be sufficient. In the general case, a regular region may be ~s small as a single scan segment. 
Therefore, i% is more convenient to define it as such. A scan segment is easy to extract by scanning 
twice: scan leftward I to locate x , and scan rightward to ~ocate x 2, for a given y. Once (y,xl,x 
2) are known, the scan segment may be safely filled.  A directed graph G is a pair (E,V) where E is 
the set of arcs, and V is the set of nodes. An arc is an oraered pair of nodes (e.g., (nl,n2)). For every 
 directed graph G, there is an undirected graph G'=(E',V') such that V'=V and E'=E but that the pairs 
in E' are unordered. A cycle in G is a list of nodes (nl,n2, ..,nk) such that nk=n I and (nl,n2) , 
(n2,n3), ..,(nk_1,n k) are in E. A cycle in the directed graph G implies the same cycle in its undirected 
version G'. It is not true in the other way, however, since a cycle in G' does not imply that the corresponding 
arcs in G all occur in the same direction to make a directed cycle. ~, Regi on e,s a set of b. gorrespor, 
din 9 9r0.ph rep. of r e 9,ul ar regi o~$, the regi on. Fig 2: A region and its graph represe-~tation: 
a) A non-trivial region is made up of regular sub regions. The horizontal lines are sub-boundaries between 
pixels of one regular region and its neighbor's; and b) The corresponding graph representation. In this 
graph, V= ~I ,2,3,4,5,6,7,8,91, and E= {(7,5), (5,4), (5,2), (2,1), (3,1), (8,3), (9,8), (8,7), (9,6), 
(6,5)t. Note that nodes 7,5,6,9,8 make a cycle only in the undirected version of the graph. That cycle 
corresponds to the top island. Theorem: if G is the graph representation of a region R, then it is a-cyclic. 
 Proof: Lets define n.y to be the coordinate of the scan segment that correspond to node n. Lets assume 
that --there is a cycle (nl,n2,...,nk). Each arc in this cycle, (ni,ni+1) implies that n.Yi>n.Yi+ I 
in virtue of the relation between G and R. Therefore, n.Y1>n.Y2 > ..>n.Yk=n.y I --> contradiction. 
Q.E.D. Lemma: If (nl,n2,...,nk) is a cycle in G', then there are two indices 1<i,j<k-1 such that (ni,ni+1),(nj+1,nj) 
 are in E. That is: on stepping from node n I alone the cycle and back to n I (=nk), some arcs will 
be followed head-to-tail, and some tai]-to-head.  This lemma is intuitive, since a cycle corresponds 
to an island, ann the spatial constraints in the re~ion implies that the nodes in the cycle must he located 
around this island.  3.0 REGION EXTRACTION Extracting region R is equivalent to traversing the correspondin~ 
graph representation G. If G is given, then it can be traversed by finding one of its spanning trees. 
A depth-first search 4or such a tree is given recursively by algorithm 5.2 in [Aho]. (Though it is written 
for G' -the undirected version o4 G) Algorithm A below is a non-recursive version of the depth-first 
search for the raph's spanning tree. It uses a stackf Knuth] for which ~Empty, Top, Pop, Push, and Empty-Stack 
are primitive operations and definitions. Alg A." Let S be a stack, the initial node. A.I S:= Push(FmptlF-~tack, 
~);  A.2 WHILE not Fmpty(~) DO  A.2.1 q := ~op(g) ;" A.2.2 g := Pop(S) ; A.2.3 FOR all r that are 
~onnected with q DO A.2.Z.I IF r is 'new' THEN --S := Push(S, r) ; A.2.4 mark ~ 'old' ; A.~ STOP ; 
 mhe algorithm will run in time O(max(~,N)) where: N= IVI (= H nodes in the ~raph G'). M= IEI (= 
# arcs in the ~raph G').  A very important part of the algorithm is the ability to mark nodes as 'new' 
or 'ola'. The reason is to prevent the algorithm from wonderin~ infinitely on the graph due to a cycle. 
Initially all nodes are assumed to be 'new'. ~ 14 there is no explicit representation of the graph, this 
marking may be very expensive. Each node is distin~uishabIe by its description as a regular-region so 
that the marking is implementable by having a set that will contain 811 'old' nodes. In this way, statement 
A.2.~.I will be o4 complexity O(N) since i% will invoIve a linear search of that set. This does not 
have to be so hard, however. The fact that Al~ A Finds a spanning tree of the ~raph means that the nodes 
that are recorded on the stack are the 'leaves' of the tree discovered so far. These leaves are the periphery 
of the already explored region and, therefore, are the only candidates to be found 'old' by the test 
in statement A.2.3.1. In this case, this test can be replaced by: "r is not on the stack S". It seems 
that this marking is the crucial point in the whole problem, and an efficient way must be found to deal 
with it. If the complexity of this statement is 0(K), then the complexity of the whole algorithm will 
be O(max(M,N)*K). As suggested above (also in [Lieberman-78]): K= Average stack size = O(Log N). In 
the case of a non-binary image, the fill color can be used to mark 'old' nodes (regular-regions) making 
the complexity of A.2.3.1 a constant and the complexity of the whole algorithm 0(Max(M,N)) (as in "simple 
fill" [Smith-79]). Note: Some definitions and conventions that will be used in the sequel: I) A "node" 
is a component of the graph representation that corresponds to a "regular region" in the image. Filling 
or painting a node means to fill or paint the corresponding regular region. 2) Node ~ in Alg A is the 
pivot node. Every node in the graph will eventually become a pivot (the graph is connected). 3) Arcs 
are obvious in the graph representation and correspond to sub-boundaries (between regular regions) in 
the image. This is in contrast to the boundar~ of the region itself that is a set of pixels that are 
colored differently than the region (by virtue of which they bound it). Arcs do not correspond to any 
pixel, but in the traversal process, they will be represented in the image as the set of pixels on one-side 
of the corresponding sub-boundary. 4.0 LIEBERMAN'S SOLUTION A solution by [Lieberman-78] suggests the 
search of the stack for 'old' nodes. His solution uses two stacks so that only one of them is searched 
for 'old' nodes, while the nodes on the other are used as in statements A.2.1 -A.2.2. The directed version 
of the graph representation serves as the basis for the search by distinguishing at each node two kinds 
of arcs: incoming and outgoing. Connected neighbors with the current node (the pivot) are divided into 
2 grouDs: I. Those a__~which the connecting arc points; and 2. Those from which the connecting arc 
points. Depending on how the pivot node was approached (i.e. the current search direction), each of 
those groups of nodes is pushed into a different stack. In other words, arcs are first followed in one 
direction (e.g., head-to-tail), and when all those are exhausted, the opposite direction is pursued, 
and so on alternately until all arcs are exhausted. Fig 3 illustrates the search on the graph of fig 
2.b. The bold arrows are enumerated alphbetically in the order that they are pursued, ant show the order 
in which nodes are pivoted. Node v contains the seed pixel. Fig 3: Lieberman's method for graph trave~al. 
Nodes are pivoted in the following order and search directions: In sweep I, direction is head-to-tail: 
7,8,9(via a); and in sweep 2, direction is tail-to-head: 5,2,1(via b), 4(via c), 3(via d), 6(via e). 
 Lieberman's solution would have worked perfectly well if for every node, a record of its description, 
and all links incident upon it, is created. Now, whenever an 'old' node is rediscovered, one link is 
removed from the record. When the last link is removed, the none is discarded from becoming a Divot. 
I~ no such detailed reoora is kept, and an old node is discarded on the sole evidence o~ its being rediscovered, 
some arcs may be "forgotten" and the region will not be filled entirely. This is the case with Lieberman's 
solution as depicted in fig 4. I~4~I iIis:. il:.i :.:. !I:. :.I Fig 4_/_ Lieberman's solution will 
not fill the whole region. Explanation : Beginning with node I, node ~ is put on the stack to be filled 
later in the uDward  direction. Later, nodes 2,4, and 6 are painted (arrow a). When node 7 is the pivot, 
node 5 is placed on the stack as a node to be filled later in the downward direction. When node 3 becomes 
the pivot, node 5 is rediscovered and discarded (arrow b). As a result, node 5 will not become a pivot 
and node 8 will never be filled. 5.0 IMPROVED SOLUTION Definition of "Blocking": An operation in which 
a solid black (the same color as the region boundary) horizontal line is drawn in the image. It will 
block, or cut, the connection between two connected regular regions. Its effect on the graph representation 
is of "pruning" by cutting off a link. Blocking is not done at every pivoting node, but only at cycle-sensitive 
situations. Based on the lemma in section 2, such situations may occur in pivot nodes which were discovered 
via an arc at a given direction (e.g., tail-to-head), and from which the search can be continued via 
an arc of an opposite direction (i.e., head-to-tail). If no arcs in the opposite direction are found, 
then none of the nodes thus found are potential junctions for cycles. To ensure that, the graph is traversed 
in "sweeps" (similar to Liebermans solution). In each sweep, arcs are pursued only in one direction: 
unward (which corresponds to head-to-tail~, or downward (which corresponds to tail-to-head). Whenever 
an arc that points in the opposite direction (vs. the current one) is discovered, it is blocked as 
a potential lead to a cycle. By the theorem in section 2, the directed graph is a-cyclic, and since 
in every sweep a directed a-cyclic graph is searched depth-first, there is no danger of completing 
a cycle. The search order is kept through a single data structure called queue-stack (cf. dequeue in 
[Knuth]). The following are the defined operations on it: I. Stack := Empty-Stack ; // create an empty 
stack. I. Stack := PushOnTop(Stack,element) ; // add element to the top. 2. Stack := PushOnBottom(Stack,element); 
// add element to the bottom.  3. Stack := Pop(Stack) ; // remove the top.  4. element := Top(Stack) 
; // retrieve the top.  5. boolean := Empty(Stack) ; // test for emptyness.  Where Stack is the queue-stack, 
and element is a cell in it. The following are axioms that precisely define it: I. Top(Push0nTop(S, 
p)) = p ;   ~. Pop(PushOnTop(S, p) Top(PushOnBottom(S,)p~)S= ; if Empty(S) then p else Top(S) ; 4. 
Pop(PushOnBottom(S, p)) =  if Empty(S) then New else PushOnBottom(Pop(~),p) ;  5. Empty(Empty-Stack) 
= true ;  6. Pop(Empty-Stack) = error ;  7. Top(Empty-Stack) = error ;  (See also [Gutta~-7v].) 
Informally, all elements that are pushed on top will be retrieved in a last-in-first-out discipline (=stack), 
elements that are pushed on bottom will be retrieved in a 9irst-in-first-out discipline (=queue), while 
elements that are pushed on top are always retrieved before those pushed on bottom. This data structure 
is essentially a unified way to implement the double stack used by Lieberman. The blocking operation 
makes obsolete the test of discovered nodes for being 'new' or 'old' in statement A.2.Z.I. All discovered 
nodes are 'new' because otherwise the arc leading (in the current search direction) to an 'old' node 
is blocked. An improved algorithm is: Alg B: DEFINE : 9: a queue-stack, dir: a direction variable, 
p,q,r: nodes (p is the initial noae), o : an arc ; B.1 Block(p) ; B.2 S:= Push0nBottom(EmDtv-~tack, 
D) ; B.3 S:= Push0nTop(S, p) ; B.4 WHILE not Empty(S) DO B.~.1 q := mop(S! ; 3.4.2 S := Pop(S) ; 
 B.4.3 dir := DirectionOf(q) ; B.4.4 if q is blocked then B.4.4.1- PaintAro(q) ;  B.4.4.2 q := Node0f(q, 
dir) ; B.4.4.3 PaintNode(q) ;  B 4.5 ~OR all r connected to q Tn direction dir DO  B.4.5.1 PaintNode(r) 
; B.4.5.2 S := PushOnTop(S, r) ;  B.4.6 FOR all r connected to q Tn direction -dir DO  B.4.6.1 Block(r) 
; B.4.6.2 S := Push0n9ottom(S, r] ;  B.4.7 I~0PR all blocked arcs o leading to q D0  B.4.7.1 Remove(o, 
~) B.4.7.2 PaintArc(o) ;   B.5 STOP ; Where : PaintNode(q) means to paint a node. Remove(o,S) means 
to remove an element anywhere off the stack  Block(o) means to block an arc by painting it black. 
 PaintArc(q) means to paint a blocked arc. Node0f(q, dir) means to locate the node that the arc q 
is leading to via direction dir. The way in which nodes are discovered and links are followed in the 
image is shown in fig 6 in the implementation section (section 7) below. It should be noted here, however, 
that links and nodes are mixed in use by Alg B. Nodes that are discovered in the current search direction 
are pushed-on-top as nodes, after they are filled. All arcs of the newly discovered node that point in 
the opposite direction are blocked and pushed-on-bottom. An arc is described by the set of pixels that 
were painted black. They are some subset of the pixels in the newly discovered node that are adjacent 
to the sub-boundary that corresponds to that arc. Therefore, in statement 9.4.4 q is an arc, and by painting 
it, a portion of an already painted node is filled. Then, it is followed to a node.  6.0 PROOF OF CORRECTNESS 
To prove that this approach is correct is to prove that Alg A is correct. This fundamental graph traversal 
algorithm is proven correct in [Aho]. What remains to be shown is that: I) all arcs that are incident 
upon a pivot node are discovered (shown in section 7 below); and 2) No effort is made to search in a 
painted place so that (I) will be feasible. (2) is supported by the blocking operation that is made 
in order to avoid cycles: Fig 5.a depict the general situation of a blocked arc. ¢ I b Fig 5." The general 
case of blocking.  6. I Explanation (fie 5): Node p is the pivot and arc (p,a) points arc (p,a) will 
not be searched, since it is disguised as a boundary (blocked). After the node a is filled, the stack 
is searched for a --potential blocked arc leading to node a, and arc (p,a) is discovered, painted 
and discarded. If eventually, arc (p,a) appears on top o~ the stack, it will be painted and followed 
 %o node i, which will be used as a pivot. Note that there is no danger in having more than one blocked 
arc to a single node (a possible situation in a very "bushy" graph). If more than one arc in the current 
direction is discovered, they are pushed to the top of the stack in order {rom ]_eft to right. This strict 
pivoting order means that in the situation depicted in fig 5.b, arc (b,p) will be the leftmost one, and 
all other arcs that are incident upon node b are not on the stack yet. Even if any ~f the arcs (m,b)...(n,b) 
is discovered before arc (b,p) is pursued, they will be deferred by being pushed %o the bottom of the 
stack. As a result, deferred arcs that point in the current direction, and are pushed on top, need not 
be blocked. An 'old' node may he rediscovered only if in the current "sweep", a previous pivot node accessed 
it via a reverse pointing arc, and pushed it to the bottom.  6.2 Complexity If L is the average composit 
degree of each node (both the incoming and outgoin~ arcs), L= (M*2/N), then statements N.4.~ and 9.4.6 
together are executed N*L times. Statement 9.4.7 is executed N times, while the stack search takes O(K) 
time. As a result, the total complexity of the algorithm is: 0(N*L + N* Z).  7.0 IMPLEMENTATION:  
The mechanism by which a node is pivoted, and arcs are blocked and stored on the stack is depicted in 
FiE 6 for the pivot node (yl ,xF,xL, UP). ×I ×2 ×3 ×4 ×5 x6 ~_-:: :_-:j -~ ...... F-~ ×F XL Fig 6: 
Implementation of the pivotin~ operation. in the opposite direction of the current The dashed line 
is the sequence in one. Therefore this arc is blocked. Node  which pixels are scaned. As ~ result o~ 
may be rediscovered through one of the this scan, the followin~ operations will arcs (i,a)...(j,a), 
and/or (a,k)...(a,l),  be done: in the case that a is in a cycle. If node is discovered through those 
arcs, the 2ush0nTop(y2, xl, x3, UP); PushOnTop(y2~ x4, x6, UP); Push0nBottom~y2, xl, x2, DOWN, BLOCKED); 
PushOnBottom(y2, x5, x5, DOWN, BLOCMED); Push0nBottom(y2, x6, x6, DOWN, BLOCKED); Fig 7 shows these 
operations schematically both on the region and its graph representation.   I~---I -blocked arcs ( 
o.lnted blook). ~-g~inted nodes ~obe pursued Ioter. i~-p~linted ,'eglon. -bound~ry pixel _<. ~. Im~9e 
 &#38;#169; @ 0 b. Graph Fig 7: Schematic of the operations when pivoting a node. Note that the calls 
to PushOnTop and PushOnBottom above are a compact version for both creating an element and pushing it 
to the stack. The blocked elements are not real nodes but the collection of pixels that share an edge 
with the arc and are painted black.  7.1 Finale The fact is that in most cases regions are not so complex 
(not having a very "bushy" graph representation) and Lieberman's solution will work just fine. In this 
case, a simplified version of the new algorithm will do very well also, and at a much reduced cost than 
that of Lieberman, by use of the "blocking" operation. In such restricted types of regions, the search 
for blocked arcs to the pivot node in statement B.4.7 are avoided. If an arc leading to a node was blocked, 
that node will have no other arcs in the same direction as the blocked one. When such a node is rediscovered 
there will be no way to continue from it in the current "sweep" direction. These few occasions will trigger 
a search for a potential blocked arc and remove it from the stack. In a simple region, these occasions 
will be much fewer than the total number of nodes in the region, each of which cause a search of the 
stack in Lieberman's solution.  S~4MARY: A correct (proven) algorithm ~or filling regions in a raster 
binary image is presented. It uses the image itself by creating pseudo boundaries in the region to block 
potential branches that mat lead to a cycle (loop around an island in the region). A presentation of 
the region as an planar directed a-cFclic graph provides an abstract view of the problem and lightens 
the difficulties associated with it. With the help of this abstract representation, a bug in a previous 
solution [Lieberman-78] was discovered. The new solution is based on a variant of a graph traversal algorithm. 
The complexity of this algorithm is explored and shown to be smaller than the one used by the old solution. 
 ACKNOWLEDGMENT R : The ideas in this power were developed and implemented on the XEROX ALTO mini-computer, 
which has an attractive high-resolution binarff CRT display. Eugene Ball sparked mff interest in this 
problem and wrote some o~ the code for the low level routines. Thanks are due %o an anonymous referee 
for his useful suggestions and to Rose Peer for proofreading this manuscript. REFERENCES [AH0] Aho, 
A.V. et al. The Design and Analysis of Computer Algorithms, Addison Wesley, (1976). [Berge] Berge, C. 
Graphs and Hv~er Graphs, North Holland/American Alsevier, (197~. [Guttag-77] Guttag, S. Abstract data 
types and the development o~ data structures, CACM, 20(6), (June IO~7), ~9G-404. [May-77] Kay, A.L. 
Microelectronics and the personal computer, ~cienti~ic American 27Z(Z), (September IQ77), 2ZI-244. [Znuth] 
Knuth, D.E. The Art of Computer Programming, vol I, section 2.2.1. [Lieberman-78] Lieberman, H. How 
to color in a coloring book, Proceedings SIGGRAPE'78, (August Iq78], 111-116. [Smith-7q] Smith, A.R. 
Tint fill, Proc. SIGGRAPH'79, (August lqTq], 276-28Z.  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1980</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
</content>
</proceeding>
