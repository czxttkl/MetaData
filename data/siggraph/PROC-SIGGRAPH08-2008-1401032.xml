<?xml version="1.0" encoding="ISO-8859-1"?>
<!DOCTYPE proceeding SYSTEM "proceeding.dtd">
<proceeding ver="6.0" ts="04/10/2010">
<conference_rec>
	<conference_date>
		<start_date>08/11/2008</start_date>
		<end_date>08/15/2008</end_date>
	</conference_date>
	<conference_loc>
		<city><![CDATA[Los Angeles]]></city>
		<state>California</state>
		<country></country>
	</conference_loc>
	<conference_url></conference_url>
</conference_rec>
<series_rec>
	<series_name>
		<series_id>SERIES382</series_id>
		<series_title><![CDATA[International Conference on Computer Graphics and Interactive Techniques]]></series_title>
		<series_vol></series_vol>
	</series_name>
</series_rec>
<proceeding_rec>
	<proc_id>1401032</proc_id>
	<acronym>SIGGRAPH '08</acronym>
	<proc_desc>ACM SIGGRAPH 2008 talks</proc_desc>
	<conference_number>2008</conference_number>
	<proc_class>conference</proc_class>
	<proc_title></proc_title>
	<proc_subtitle></proc_subtitle>
	<proc_volume_no></proc_volume_no>
	<isbn13>978-1-60558-343-3</isbn13>
	<issn></issn>
	<eissn></eissn>
	<copyright_year>2008</copyright_year>
	<publication_date>08-11-2008</publication_date>
	<pages>79</pages>
	<plus_pages></plus_pages>
	<price><![CDATA[]]></price>
	<other_source></other_source>
	<publisher>
		<publisher_id>PUB27</publisher_id>
		<publisher_code>ACMNY</publisher_code>
		<publisher_name>ACM</publisher_name>
		<publisher_address>2 Penn Plaza, Suite 701</publisher_address>
		<publisher_city>New York</publisher_city>
		<publisher_state>NY</publisher_state>
		<publisher_country>USA</publisher_country>
		<publisher_zip_code>10121-0701</publisher_zip_code>
		<publisher_contact>Bernard Rous</publisher_contact>
		<publisher_phone>212 869-7440</publisher_phone>
		<publisher_isbn_prefix></publisher_isbn_prefix>
		<publisher_url>www.acm.org/publications</publisher_url>
	</publisher>
	<sponsor_rec>
		<sponsor>
			<sponsor_id>SP932</sponsor_id>
			<sponsor_name>ACM Special Interest Group on Computer Graphics and Interactive Techniques</sponsor_name>
			<sponsor_abbr>SIGGRAPH</sponsor_abbr>
		</sponsor>
	</sponsor_rec>
	<categories>
		<primary_category>
			<cat_node/>
			<descriptor/>
			<type/>
		</primary_category>
	</categories>
</proceeding_rec>
<content>
	<section>
		<section_id>1401033</section_id>
		<sort_key>10</sort_key>
		<section_seq_no>1</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Smile for the camera]]></section_title>
		<section_page_from>1</section_page_from>
	<article_rec>
		<article_id>1401034</article_id>
		<sort_key>20</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>1</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Extracting higher-level information from facial mocap]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1401032.1401034</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401034</url>
		<categories>
			<primary_category>
				<cat_node>I.4.6</cat_node>
				<descriptor>Edge and feature detection</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.5.2</cat_node>
				<descriptor>Feature evaluation and selection</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.7</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.4.8</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010225.10010227</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision tasks->Scene understanding</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010257.10010321.10010336</concept_id>
				<concept_desc>CCS->Computing methodologies->Machine learning->Machine learning algorithms->Feature selection</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245.10010246</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems->Interest point and salient region detections</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Measurement</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1098357</person_id>
				<author_profile_id><![CDATA[81409595942]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[J.]]></first_name>
				<middle_name><![CDATA[P.]]></middle_name>
				<last_name><![CDATA[Lewis]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Weta Digital]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098358</person_id>
				<author_profile_id><![CDATA[81100085512]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Ken]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Anjyo]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[OLM Digital]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[T. Hastie, R. Tibshirani, and J. Friedman. <i>The Elements of Statistical Learning</i>. Springer Verlag, New York, NY, 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[E. Vatikiotis-Bateson, I. Eigsti, S. Yano, and K. Munhall. Eye movement of perceivers during audio visual speech perception. <i>Perception &amp; Psychophysics</i>, 60(6):926--940, 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 ExtractingHigher-LevelInformation fromFacial Mocap J.P.Lewis* KenAnjyo* WetaDigital OLMDigital Figure 
1: a), b) automatic weight regions for skinning; c), d) importance of facial regions for emotion (c) 
and speech (d) discrimination. Motioncapturedata isvaluable information,but ina low-levelform that makes 
subsequent editing operations dif.cult. In cases where extensivemanipulation isneeded amanuallyanimatedperformance 
maybe be preferable, because the manually constructed represen­tation has artist-friendly higher level 
controls. Algorithmic ap­proaches can potentially extract higher-level and semantic repre­sentationsfrommotioncapture,but 
much work remains tobedone in thisarea.In this talkweshowexamplesof severaltypesofhigher level information 
that canbeextracted using anaf.nitymeasureon the mocapdata. 1 Af.nity Measures Ideally,wewould like todiscover 
temporallycorrelated movement regardless of the direction of that movement. Standard linear cor­relation 
isarguablyapoormeasure in thisrespect.Forexample, in a walking motion, the movement of one leg is more 
related to the movement of the other leg than to head motion, but linear correla­tionwouldnotdiscover 
thisbecause the leg movement isnegatively correlated. Similarly, in making an oo sound in speech, the 
mo­tion of the upper and lower lips is anti-correlated, while the inward motion of the lip corners is 
perpendicular to the up-down motion and thushaszerocorrelation. We employtwo measures. Firstconsidering 
only linear approaches, we seek a direction-invariant af.nity measure. This can be for­mulated as maximizing 
the expected movement correlation of two mean-removedpoints a, b by .ndingthe rotation R thatbest aligns 
their motion, X 1 TT arg max a Rbt + tr L(RR - I) R N t whereL isaLagrangemultipliermatrixenforcing orthogonalityof 
R.Takingthederivativeand setting tozero,wehave X 1 T RL = - atbt N where therighthand side isgiven(note 
that it isanouterproduct, i.e.a cross-covariance matrix).Thesingularvaluedecomposition RL = UDV T =(UV 
T )(V DV T ) gives the solution, where R = UV T isthedesired rotation taking bt intobest alignment with 
at. The direction-invariant linear measure handles the perpendicularly and anti-correlated examples mentioned 
previously. While we do not expectthemotionof variouspointstohaveaperfect linearrela­tionship, often 
the linear component of the motion is dominant. In thesecases linearmodelispreferredduetolearning and 
regression theory considerationsof model simplicityand lowervariance[1]. * e-mail: zilla@computer.org, 
anjyo@olm.co.jp In other cases, however, a linear model is unlikely to be suf.cient. To cover these cases 
we switch to an af.nity measure based on the mutual information I(X, Y )= H(X)+ H(Y ) - H(X, Y ) be­tween 
two variables X, Y , where H() is the marginal or joint en­tropy. This measure re.ects any functional 
dependence between quantities, i.e. the mutual information between X and Y = f (X) is high regardless 
of the nature of f (). Intuitively, mutual informa­tion is the information thatavariableYprovidesaboutX(orvice 
versa), measured as the expected number of bits that can be saved in specifying X when Y isknown.Although 
motioncapturedata is not random, themutualinformationconcept canneverthelessbeap­plied simply by treating 
normalized histograms of the motion cap­ture variables as if they are probability densities, and then 
directly applying themutual informationde.nition. 2 Applications Fig. 1 a), b) show the linear af.nity 
measure applied to the task of determining weight map regions for cluster based skinning as im­plementedin 
systems suchasMaya. The motion capture is obtained from a model-based video tracking system. The set 
of vertices that aresigni.cantlycorrelated withan indicated vertex(red) arehigh­lighted inblue.Notethat 
ina), thedirection-invariantmeasurehas discovered symmetric weight maps on both sides of the mouth, al­though 
the horizontal movement on one side of the mouth is typi­cally anti-correlated with that of the other 
side. In b) the eyebrow movement was not completely symmetric. Fig. 1 c), d) show the versatility the 
nonlinear af.nity measure. HereI(X, Y ) is appliedbetweenpoint motion(X)on aface across images and subjective 
ratings (Y ) ofdifferences inemotional affect (c)and speechdiscrimination(d) across these images.For 
theemo­tional affect task, the cheeks are unimportant, the mouth region is slightly important,but thebrowandinnereyeregionsaredominant 
suggestingthat motioncapturemarkersshouldbeplaced in these areas,andfurther thataCGfacemodel shouldcarefullyreproduce 
movementintheseareas.Inthespeech task,themouthisquiteim­portant, as expected, but the inner eyes are 
again more signi.cant than one mightexpect.Thisresultmaybeconsistentwithpsycho­logical research showingthatpeople 
look at theeyesmore than the mouthwhen listening tospeech[2]. This work was partially supported by the 
Japan Science and Tech-nologyAgency,CRESTproject. References [1] T. Hastie, R. Tibshirani, and J. Friedman. 
The Elements of Statistical Learning. SpringerVerlag,NewYork,NY,2001. [2] E. Vatikiotis-Bateson, I. Eigsti, 
S. Yano, and K. Munhall. Eye movement of perceivers during audio visual speech perception. Perception 
&#38; Psychophysics, 60(6):926 940,1998. Copyright is held by the author / owner(s). SIGGRAPH 2008, Los 
Angeles, California, August 11 15, 2008. ISBN 978-1-60558-466-9/08/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1401035</article_id>
		<sort_key>30</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>2</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Estimating multi-layer scattering in faces using direct-indirect separation]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1401032.1401035</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401035</url>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>F.2.2</cat_node>
				<descriptor>Geometrical problems and computations</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Curve, surface, solid, and object representations</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Geometric algorithms, languages, and systems</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.8</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010507</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Graphics recognition and interpretation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061.10010063</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures->Computational geometry</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010399</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Parametric curve and surface models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010401</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Volumetric models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1098359</person_id>
				<author_profile_id><![CDATA[81385599077]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Abhijeet]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ghosh]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[USC Institute for Creative Technologies]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098360</person_id>
				<author_profile_id><![CDATA[81100086933]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Paul]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Debevec]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[USC Institute for Creative Technologies]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1073308</ref_obj_id>
				<ref_obj_pid>1073204</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Donner, C., and Jensen, H. W. 2005. Light diffusion in multi-layered translucent materials. <i>ACM Transactions on Graphics 24</i>, 3 (Aug.), 1032--1039.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1141977</ref_obj_id>
				<ref_obj_pid>1141911</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Nayar, S. K., Krishnan, G., Grossberg, M. D., and Raskar, R. 2006. Fast separation of direct and global components of a scene using high frequency illumination. <i>ACM Transactions on Graphics 25</i>, 3 (July), 935--944.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Estimating Multi-layer Scattering in Faces using Direct-Indirect Separation Abhijeet Ghosh and Paul 
Debevec USC Institute for Creative Technologies *  (a) Shallow scattering (b) Deep scattering (c) Re.ectance 
R1 (d) Transmittance T1 (e) Re.ectance R2 Figure 1: Direct-indirect separation of the observed diffuse 
re.ectance of a face ((a) &#38; (b)), and .tting of a two-layer scattering model of diffuse skin re.ectance. 
The estimated re.ectance and transmittance of the two layers are depicted under a novel lighting condition 
in (c)-(e). 1 Introduction Accurate simulation of the diffuse scattering of light in skin is im­portant 
for achieving the charateristic softness in skin appear­ance and subtle effects such as light bleeding 
at shadow bound­aries. While the dipole diffusion model has been widely used in computer graphics to 
ef.ciently simulate these effects for translu­cent materials, it tends to over-smooth the details near 
the skin sur­face resulting in an unnatural waxy appearance for faces. Don­ner&#38;Jensen [2005] recently 
introduced a multi-layer subsurface scattering model for rendering human skin more realistically. They 
relate the multi-layer model to the various epidermal and dermal layers of skin and provide scattering 
parameters for the layers from tissue optics literature. While providing more convincing results for 
human skin than the dipole model, the greater complexity of the multi-layer model also makes it more 
challenging to .t the scatter­ing parameters from measured data. Unlike the dipole model, it is unclear 
how to .t the various parameters of the multi-layer model from a typically observed scattering pro.le 
from a live subject. We seek to address this problem in this work. Our approach is to em­ploy the recently 
introduced direct-indirect separation technique of Nayar et al. [2006] in order to decompose the diffuse 
scattering of light in skin into a shallow and a deep scattering component re­spectively. Given the separated 
components and an additionally observed scattering pro.le, we then estimate parameters of a sim­pli.ed 
two-layer scattering model by employing the Kubelka-Munk theory to the total diffusely re.ected radiance. 
 2 Method Let the total diffuse re.ectance of skin at any surface point be Rd, and the separated direct 
and indirect components be Rdirect and Rindirect respectively. Then Rd = Rdirect + Rindirect. According 
to the Kubelka-Munk theory of layered diffuse re­.ectance, if R1 is the diffuse re.ectance and T1 is 
the diffuse trans­mittance of layer 1 and R2 is the diffuse re.ectance of layer 2, then the combined 
total diffuse re.ectance of the two layers R12 is given as: T1 · R2 · T1 R12 = R1 + . (1) 1 - R2 · R1 
*e-mail: {ghosh, debevec}@ict.usc.edu Assuming a two-layer scattering model we have Rd = R12, which T1·R2·T1 
leads to Rdirect = R1, and Rindirect = . We also make 1-R2·R1 the observation that the shallow scattering 
component exhibits sig­ni.cantly less scattering compared to the deep scattering compo­nent (Figure 1 
(a)-(b)). Given an observed scattering pro.le, we .t the lower 2/3rds of the pro.le (far enough from 
the peak to correspond only to deep scattering) to the dipole model assuming Rindirect as the albedo. 
Such a .t is fairly accurate for the lower end of the scattering pro.le where scattering in the second 
layer dominates, but underestimates the true pro.le close to the peak where the contribution of the top 
layer is signi.cant. We attribute this residual close to the peak of the pro.le to be the re.ectance 
pro.le of layer 1 and .t a multipole model to it assuming R1 as the albedo. We then employ the estimated 
parameters of the multipole .t for scattering in layer 1 in order to compute diffuse transmittance T1 
due to layer 1, and feed this back into Equation 1 in order to es­timate the true diffuse re.ectance 
R2 of layer 2. Finally, we re.ne our estimate of the scattering parameters of layer 2 by employing R2 
as the albedo for the dipole .t to deep scattering.  3 Results Our acquisition setup involves an LCD 
projector cross-polarized with respect to a digital still camera in order to project appropriate patterns 
for direct-indirect separation, as well for observing diffu­sion pro.les over a face (the acquisition 
details are described in a current submission to SIGGRAPH 2008). Figure 1 presents render­ings of an 
acquired face with the estimated scattering parameters of the shallow and deep scattering layers. We 
employ the multipole model for layer 1 with an assumed thickness of 0.5mm, while ap­proximating deep 
scattering with the dipole model assuming layer 2 to be in.nitely thick.  References DONNER, C., AND 
JENSEN, H. W. 2005. Light diffusion in multi­layered translucent materials. ACM Transactions on Graphics 
24, 3 (Aug.), 1032 1039. NAYAR, S. K., KRISHNAN, G., GROSSBERG, M. D., AND RASKAR, R. 2006. Fast separation 
of direct and global compo­nents of a scene using high frequency illumination. ACM Trans­actions on Graphics 
25, 3 (July), 935 944. Copyright is held by the author / owner(s). SIGGRAPH 2008, Los Angeles, California, 
August 11 15, 2008. ISBN 978-1-60558-466-9/08/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1401036</article_id>
		<sort_key>40</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>3</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[A high-resolution geometry capture system for facial performance]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1401032.1401036</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401036</url>
		<categories>
			<primary_category>
				<cat_node>I.4.1</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.2.10</cat_node>
				<descriptor>Intensity, color, photometry, and thresholding</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.10</cat_node>
				<descriptor>Multidimensional</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.2.10</cat_node>
				<descriptor>3D/stereo scene analysis</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010240.10010241</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision representations->Image representations</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010240</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision representations</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010225</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision tasks</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1098361</person_id>
				<author_profile_id><![CDATA[81100447998]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Wan-Chun]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ma]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Southern California]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098362</person_id>
				<author_profile_id><![CDATA[81100556708]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Andrew]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Jones]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Southern California]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098363</person_id>
				<author_profile_id><![CDATA[81100179328]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Tim]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hawkins]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Southern California]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098364</person_id>
				<author_profile_id><![CDATA[81455605851]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Jen-Yuan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Chiang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Southern California]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098365</person_id>
				<author_profile_id><![CDATA[81100086933]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Paul]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Debevec]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Southern California]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>2383873</ref_obj_id>
				<ref_obj_pid>2383847</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Ma, W.-C., Hawkins, T., Peers, P., Chabert, C.-F., Weiss, M., and Debevec, P. 2007. Rapid acquisition of specular and diffuse normal maps from polarized spherical gradient illumination. In <i>EGSR 2007</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1073258</ref_obj_id>
				<ref_obj_pid>1073204</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Wenger, A., Gardner, A., Tchou, C., Unger, J., Hawkins, T., and Debevec, P. 2005. Performance relighting and reflectance transformation with time-multiplexed illumination. <i>ACM Trans. Graph. 24</i>, 3, 756--764.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 A High-Resolution Geometry Capture System for Facial Performance Wan-Chun Ma Andrew Jones Tim Hawkins 
Jen-Yuan Chiang Paul Debevec Universityof Southern California, Institute for CreativeTechnologies  X-gradient 
Y-gradient Z-gradient Full-on Stripe #1 Stripe #2 Stripe #3 White Albedo Normal Displacement Geometry 
Figure 1: Top, images taken under spherical gradient illumination. Middle, Asubset of the structured 
light pattern images of three different frequencies plus a full white pattern. The other three stripe 
patternshave the same frequenciesbut witha90degree phase shift. Bottom, Results for each scan: albedo 
texture, world-space normal map, displacement map, and a high-resolution mesh. Introduction We developed 
a high-resolution, real-time facial performance capture system based on a spherical gradient photo­metric 
stereo technique[Maetal.2007]and multi-view stereo.We use four spherical gradient illumination patterns 
to estimate nor­mal maps of subjects. Astructured-light-assisted two-view stereo system is employed to 
acquire 3D positions of the subject. The cap­tured stereo geometry is then enhanced using the gradient 
normals. This allows details such as dynamic wrinkles and .ne-scale stretch­ing and compression of skin 
pores to be captured in real-time. Capture System Our real-time 3D capture system uses a com­bination 
of structured light and photometric stereo to obtain high­resolution face scans. The grayscale structured 
light patterns are outputbyahigh-speed MULE (Multi Use Light Engine) DLP video projector fromFakespace 
Lab, running at 288 frames per second. To capture .ne detailsof human skin,a stereo pairofVision Re­search 
Phantom high-speed digital cameras synchronized to the projector and a similar spherical gradient illumination 
device as in [Ma et al. 2007] to capture 24 full measurements per second, each comprising 12 images. 
We use six sinusoidal structured light patterns at varying scales andafull-on projector pattern. After 
each structured light sequence we generate four gradient illumination patterns and an additional diffuse 
tracking pattern with a spherical lighting apparatus. Be­cause our gradient illumination patterns are 
captured at different points in time, we correct for subject motion as in [Wenger et al. 2005] using 
an optical .ow algorithm . We compute this .ow be-  Figure 2: The acquired high-resolution geometry 
(top) and shadings by using the nor­mal map and albedo texture with a simple Phong BRDF shader, from 
three different viewpoints. tween the the .rst gradient pattern and the tracking pattern, and then use 
this .ow to warp the four gradient-lit images to the same point in time. This allows for accurate calculation 
of surface normals using ratios of the gradient-lit images. Geometry Processing A dynamic programming 
algorithm is used to .nd camera-to-camera correspondences from the ratios of the sinusoidal structured 
light patterns to the full-on pattern. The stereo geometry is created by triangulating camera rays based 
on the correspondences. We compute photometric surface normals from the spherical gradient patterns, 
and then use the photometric normalsto correctthe stereo geometrybyminimizingthedifference between the 
geometry normals and correctedphotometric normals. This step not only adds .ne-scale details to the stereo 
geometrybut also corrects the low frequency geometry noise that is dif.cult to get ridofby using mesh 
smoothing algorithms. Results The two cameras capture data at a resolution of 2400 × 1800 (Bayer pattern). 
With a internal RAM storage of 12GB, the maximum recording time is around5seconds. The result of each 
scan contains ahigh resolution mesh that usually consistsof1M triangles, a smoothed medium resolution 
mesh, a color texture, a world-space normal map, and a displacement map represents the difference between 
the high resolution mesh and the smoothed mesh. References MA, W.-C., HAWKINS, T., PEERS, P., CHABERT, 
C.-F., WEISS, M., AND DE-BEVEC, P. 2007. Rapid acquisition of specular and diffuse normal maps from polarized 
spherical gradient illumination. In EGSR 2007. WENGER, A., GARDNER, A., TCHOU, C., UNGER, J., HAWKINS, 
T., AND DE-BEVEC,P. 2005. Performance relighting and re.ectance transformation with time­multiplexed 
illumination. ACM Trans. Graph. 24, 3, 756 764. Copyright is held by the author / owner(s). SIGGRAPH 
2008, Los Angeles, California, August 11 15, 2008. ISBN 978-1-60558-466-9/08/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1401037</article_id>
		<sort_key>50</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>4</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[Dynamic bluescreens]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1401032.1401037</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401037</url>
		<abstract>
			<par><![CDATA[<p>We synchronize cameras and analog lighting with high speed projectors. Radiometric compensation allows displaying flexible blue screens on arbitrary real world surfaces. A fast temporal multiplexing of coded projection and flash illumination enables professional chroma keying and camera tracking for non-studio film sets.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[compensation]]></kw>
			<kw><![CDATA[keying]]></kw>
			<kw><![CDATA[projector-camera systems]]></kw>
			<kw><![CDATA[radiometric]]></kw>
			<kw><![CDATA[temporal multiplexing]]></kw>
			<kw><![CDATA[tracking]]></kw>
			<kw><![CDATA[video composition]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.4.1</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.2.10</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010225.10010227</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision tasks->Scene understanding</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1098366</person_id>
				<author_profile_id><![CDATA[81100631719]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Anselm]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Grundh&#246;fer]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Bauhaus-University Weimar]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098367</person_id>
				<author_profile_id><![CDATA[81100622976]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Oliver]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bimber]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Bauhaus-University Weimar]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1042236</ref_obj_id>
				<ref_obj_pid>1042191</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Bimber, O., Emmerling, A., and Klemmer, T. 2005. Embedded Entertainment with Smart Projectors. <i>IEEE Computer 38</i>, 1, 56--63.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Dynamic Bluescreens Anselm Grundh¨, Oliver Bimber ofer* Bauhaus-University Weimar  Figure 1: Odd frames 
record projected images that neutralize the appearance of a real background surface and display a keying 
color instead (a). Even frames record the fully illuminated scene without projection (b). Repeating this 
at HD scanning speed (59.94Hz) and registering both sub-frames during post-processing supports scene 
reconstruction and camera tracking (c), as well as keying and professional video composition at approximately 
30 Hz within real (non-studio) environments (d). Abstract We synchronize cameras and analog lighting 
with high speed pro­jectors. Radiometric compensation allows displaying .exible blue screens on arbitrary 
real world surfaces. A fast temporal multiplex­ing of coded projection and .ash illumination enables 
professional chroma keying and camera tracking for non-studio .lm sets. Keywords: Projector-camera systems, 
keying, tracking, radiomet­ric compensation, temporal multiplexing, video composition. 1 Introduction 
and Motivation Blue screens and chroma keying technology are essential for digital video composition. 
Professional studios apply tracking technology to record the camera path for perspective augmentations 
of the origi­nal video footage. Although this technology is well established, it does not offer a great 
deal of .exibility. For shootings at non-studio sets, physical blue screens might have to be installed, 
or parts have to be recorded in a studio separately. We present a simple and .exible way of projecting 
corrected keying colors onto arbitrary diffuse sur­faces using synchronized projectors and radiometric 
compensation. Thereby, the re.ectance of the underlying real surface is neutralized. A temporal multiplexing 
between projection and .ash illumination allows capturing the fully lit scene, while still being able 
to key the foreground objects. In addition, we embed spatial codes into the pro­jected key image to enable 
the tracking of the camera. Furthermore, the reconstruction of the scene geometry is implicitly supported. 
 2 Technical Approach A high de.nition 3CCD camera is synchronized with off-the-shelf DLP projectors 
and a custom-built Osram Ostar (5600K) LED light­ing system. The projectors are automatically calibrated 
to an ar­bitrary (preferably Lambertian) background surface. The camera is used to scan the sufrace geometry 
as well its re.ectance of.ine using structured light projection. This data can be applied for special 
video composition effects, such as occlusions and shadow casts (cf. .gure 1d), but is mainly used for 
real-time per-pixel displacement mapping and radiometric compensation on the GPU during runtime [Bimber 
et al. 2005]. These image correction techniques widely *anselm.grundhoefer@medien.uni-weimar.de bimber@uni-weimar.de 
neutralize the geometric and radiometric structure of the real back­ground surface and lead to appearance 
of a uniform keying color. Every odd frame is recorded with analog illumination turned off, and the background 
color being replaced with the keying color (cf. .gure 1a). Every even frame records the fully illuminated 
scene with the projection being turned off (cf. .gure 1b). As a .rst post­processing step, corresponding 
odd and even frame sequences are time warped by a factor of two to interpolate the intermediate frames. 
The radiometric compensation ensures a uniform appearance of the keying color throughout the spatially 
varying background surface and consequently enables an ef.cient separation of foreground and background. 
Synthetic feature points integrated into the keying color (cf. .gure 1b) allow the reconstruction of 
the camera path (cf. .gure 1c). Tracking and keying together .nally support perspective video composition 
effects within real (non-studio) environments (cf. .gure 1d). 3 Results and Outlook Our current prototype 
system records 720p frame at the HD scan­ning speed of 59.94Hz. The uncompressed frames are captured 
via HD-SDI and recorded to disk. Specular highlights on transparent objects are inconsistent per se, 
since the positions of projectors and analog light sources in odd and even frames don t match. However, 
they can be recovered and superimposed in the .nal composition frame by identifying them in each corresponding 
even (illumination) frame. The subsequent post-processing steps are fully compati­ble with common video 
composition pipelines. We use Autodesk MayaTM2008 Unlimited for 3D rendering, its LiveTMmodule for match-moving 
based on the synthetic feature points, as well as Adobe After Effets TMTime WarpTMfunction for motion 
interpola­tion, and DVMatte ProTMfor keying. Our method supports keying that is invariant to foreground 
colors. Currently we are not only investigating the integration of enhanced patterns into the keying 
frames that might allow creating environment mattes for refractive objects, but also developing techniques 
that select optimize keying colors depending on the background.  References BIMBER, O., EMMERLING, A., 
AND KLEMMER, T. 2005. Embed­ded Entertainment with Smart Projectors. IEEE Computer 38, 1, 56 63. Copyright 
is held by the author / owner(s). SIGGRAPH 2008, Los Angeles, California, August 11 15, 2008. ISBN 978-1-60558-466-9/08/0008 
  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	</section>
	<section>
		<section_id>1401038</section_id>
		<sort_key>60</sort_key>
		<section_seq_no>2</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Particle man]]></section_title>
		<section_page_from>2</section_page_from>
	<article_rec>
		<article_id>1401039</article_id>
		<sort_key>70</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>5</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Tackling computer generated clouds in <i>'Madagascar: The Crate Escape'</i>]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1401032.1401039</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401039</url>
		<abstract>
			<par><![CDATA[<p>Computer generated clouds are an important component in the animated movie 'Madagascar: The Crate Escape'. Our particle based cloud system is used throughout the animated feature in collaboration with our Layout, Lighting and Matte painting departments. It was clear from the beginning that several objectives had to be defined - the techniques used had to be relatively fast and able to create realistic and stylistic clouds, but, also and mainly, be art-directable. Another key component was to create a flexible and memory efficient pipeline designed to accommodate hundreds of production shots.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>H.5.1</cat_node>
				<descriptor>Animations</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>H.5.1</cat_node>
				<descriptor>Artificial, augmented, and virtual realities</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.8</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010392</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Mixed / augmented reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341.10010349.10011310</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Simulation types and techniques->Simulation by animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010507</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Graphics recognition and interpretation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010866</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Virtual reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010866</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Virtual reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1098368</person_id>
				<author_profile_id><![CDATA[81322497424]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Laurent]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kermel]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[PDI/DreamWorks]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098369</person_id>
				<author_profile_id><![CDATA[81421594593]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Fangwei]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lee]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[PDI/DreamWorks]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098370</person_id>
				<author_profile_id><![CDATA[81421602118]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Joon]]></first_name>
				<middle_name><![CDATA[Taik]]></middle_name>
				<last_name><![CDATA[Song]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[PDI/DreamWorks]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098371</person_id>
				<author_profile_id><![CDATA[81100505245]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Scott]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Peterson]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[PDI/DreamWorks]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1401040</article_id>
		<sort_key>80</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>6</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Art directing particle flows with custom vector fields]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1401032.1401040</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401040</url>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Curve, surface, solid, and object representations</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.6.1</cat_node>
				<descriptor>Systems theory</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.8</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.6.3</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010507</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Graphics recognition and interpretation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010399</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Parametric curve and surface models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341.10010346.10010347</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Simulation theory->Systems theory</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010401</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Volumetric models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1098372</person_id>
				<author_profile_id><![CDATA[81421601833]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Gordon]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Chapman]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Rhythm and Hues Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098373</person_id>
				<author_profile_id><![CDATA[81100445884]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Jerry]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tessendorf]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Rhythm and Hues Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098374</person_id>
				<author_profile_id><![CDATA[81541020056]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Michael]]></first_name>
				<middle_name><![CDATA[A.]]></middle_name>
				<last_name><![CDATA[Kowalski]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Rhythm and Hues Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1276435</ref_obj_id>
				<ref_obj_pid>1275808</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Bridson, R., Hourihan, J., and Nordenstam, M. 2007. Curl-noise for procedural fluid flow. <i>ACM SIGGRAPH</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Art directing particle .ows with custom vector .elds Gordon Chapman Jerry Tessendorf Michael A. Kowalski 
Rhythm and Hues Studios * Introduction Complex physically based .uid simulations can take hours or even 
days to process, yet the degree of user control over them is never as detailed as directors and supervisors 
would like. By creating and combining user de.ned vector .elds, we gain a level of control over .uid 
simulations that allows us to directly ad­dress these particular needs without having to worry about 
breaking the simulation and allowing faster turnarounds to give us more iter­ations of each shot. FELT 
FELT is Rhythm and Hues award winning Field Expression Lan­guage Toolkit, a .exible standalone application 
that allows the artist to manipulate both scalar and vector .elds as well as handling in­teraction with 
traditional models. An interface has been added to Side Effects Houdini by means of a SOP that allows 
the artist to import, create, manipulate and export .elds within the program s procedural work.ow. The 
resulting vector.eld is then used to advect particles either by directly controlling their velocities 
or by applying an additive force. These particles will ultimately be used to populate the voxel buffer 
used in the .nal render, which is itself a FELT scalar density .eld. In Production In the .rst example, 
from The Incredible Hulk, Hulk is bursting his way through the front wall of a large glass walkway .lled 
with teargas. Initially we started with a conventional grid based .uid simulation (i), to direct the 
overall motion of the particles that will ultimately be used within the voxel render. Once the vector 
.eld is generated, a Houdini POP allows us to advect particles within that domain. Speci.c art direction 
dictated that the impact force by Hulk as he smashes the front of the walkway apart needed to be more 
forceful. Rather than increasing the magnitude of the force used within the simulation and hoping it 
didn t lead to instability later on, it was a simple matter to sculpt a force using implicit surfaces 
and then convert this into an vector .eld (ii). Curl-noise based turbulence was introduced as another 
vector.eld (iii). Lastly, a simple upwards .ow.eld was created and then modulated by a spatial scalar 
noise .eld to introduce some apparent randomness (iv). The resultant advection .eld is generated as a 
simple summation of these basis .elds (v). Most of these .ow .elds are required to act only in very speci.c 
regions of the particle simulation de.ned by the scene geometry; getting .ne control over the domain 
of POP style particle forces can be problematic, but with FELT we are able to specify regions using signed 
distance functions built from modeled geometry to mask or modulate the intensity of our forces. It is 
apparent in (iii) and (iv) that the volume representing the interior of the walkway (ghosted) has been 
masked off from the .ow.eld. * e-mail: gchapman, jerryt, mak@rhythm.com In the second example, Hulk 
leaps from the shattered walkway through the previously generated gas cloud. This presented it s own 
challenges, such as the use of slow motion for the jump itself. The use of slow motion makes a physically 
based simulation awk­ward to implement, however, with FELT, we can control the mag­nitude of the composite 
vector.eld and so directly manipulate the velocities of the advected particles. This allows us to directly 
con­trol the particle velocities to match the shot s timing. Additionally, a reasonable approximation 
to incompressible .uid .ow can be ob­tained by stamping the velocities of the rapidly moving objects 
into the .ow.eld and then returning the divergent free part of the .ow using the fourier transform. 
Conclusion The biggest advantages to building .elds in this manner are that it allows rapid turnarounds 
of particle simulations with a greater de­gree of control than conventional particle forces allow. Whilst 
these user directed .ows somewhat break the physically based nature of the simulation, we can still maintain 
some measure of incompress­ibility. The result is a solution that appears pleasing to the eye. References 
BRIDSON, R., HOURIHAN, J., AND NORDENSTAM, M. 2007. Curl-noise for procedural .uid .ow. ACM SIGGRAPH. 
Copyright is held by the author / owner(s). SIGGRAPH 2008, Los Angeles, California, August 11 15, 2008. 
ISBN 978-1-60558-466-9/08/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1401041</article_id>
		<sort_key>90</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>7</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Snow avalanche effects for Mummy 3]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1401032.1401041</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401041</url>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>H.5.1</cat_node>
				<descriptor>Animations</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.6.1</cat_node>
				<descriptor>Systems theory</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.8</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010507</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Graphics recognition and interpretation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341.10010349.10011310</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Simulation types and techniques->Simulation by animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341.10010346.10010347</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Simulation theory->Systems theory</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1098375</person_id>
				<author_profile_id><![CDATA[81421601140]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Tae-Yong]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kim]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Rhythm and Hues Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098376</person_id>
				<author_profile_id><![CDATA[81320489071]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Lucio]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Flores]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Rhythm and Hues Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1401042</article_id>
		<sort_key>100</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>8</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[Golden Compass daemon deaths]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1401032.1401042</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401042</url>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.4.10</cat_node>
				<descriptor>Volumetric</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.6.1</cat_node>
				<descriptor>Systems theory</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.8</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010507</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Graphics recognition and interpretation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061.10010063</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures->Computational geometry</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341.10010346.10010347</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Simulation theory->Systems theory</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010240.10010242</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision representations->Shape representations</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1098377</person_id>
				<author_profile_id><![CDATA[81100257057]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Scott]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Townsend]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Rhythm and Hues Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098378</person_id>
				<author_profile_id><![CDATA[81421596380]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Eric]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Horton]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Rhythm and Hues Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098379</person_id>
				<author_profile_id><![CDATA[81100490071]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Sanjit]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Patel]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Rhythm and Hues Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098380</person_id>
				<author_profile_id><![CDATA[81100445884]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Jerry]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tessendorf]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Rhythm and Hues Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Golden Compass Daemon Deaths ScottTownsend Eric Horton SanjitPatel JerryTessendorf Rhythm and Hues Studios* 
 Visualizing .uid motion around CG and live characters. 1 Introduction The daemon death effect for 
The Golden Compass called for an elaborate daemon decomposition when a person met their demise. The effect 
had to consist of natural and directable .ow of layers ofgalactic matter which tied together the dust 
concept, central to the plot. The solution we developed advected layers of particles through .uid simulations 
driven by the deterioration of the host creature. The .uid simulations were fully 3D Computational Fluid 
Dynamics,usingQUICKadvectionto minimally dissipatevortices, multigrid mass conservation, and Iterated 
Orthogonal Projections to handle the boundary conditions with the bodies of the host and dae­mon. The 
particles seeded several different species of volumetric densityand color layers, which were generatedintheFeltvolumet­ric 
system, then rendered in a proprietary volume renderer. 2 Fluid and Particle Simulations Gas simulation 
at Rhythm and Hues is based on the Ahab .uid simulator. Artists work directly with Ahab through a graphical 
in­terface inHoudini. By using the minimally viscousQUICK ad­vection scheme, vortices in the gas .ow 
persist for long periods of time without dissipating or slowing down, giving rise to more vortex stretching 
and turbulence than can be computed otherwise. With this level of quality coming from advection, it is 
unneces­sary to use vorticity injection algorithms to keep the gas stirred. Mass conservationis enforced 
usingavery simple andfast multi­grid scheme, which is made possible by the Iterated Orthogonal Projection 
method for enforcing boundary conditions in the multi­grid solution. Several particle systems were emitted 
from the surface and interior of the daemon, each corresponding to a species of smoky, wispy, or sparklygalactic 
material. The particles were advected using the .uid simulationvelocity .eld, then used as the parent 
particles from which volumetric density was created. *e-mail: scotty, ehorton, patel, jerryt@rhythm.com 
Final death with .owing dust.  3 Volume Framework The framework for generating and manipulatingvolumetric 
models and simulationsatRhythmandHuesisacustom scripting language called Felt. In this particular situation, 
the advected particle sys­tems were converted to volumetric density and color with several different 
noise-based stamping procedures that use the particle as a seed position with attributes that drive the 
noise and volume .lling procedures. In this conversion the volumetric data is held in a grid structure 
that is aware of volume occupation rates and smartly con­serves memory when possible. Felt then interprets 
the volume data for the volume renderer at rendering time. Felt also provided artistic control of model 
animation in the daemon death process. The decaying daemon vanished over a short interval via levelset 
eroding. Ahab also generated model erosion as part of the .uid forcing. Copyright is held by the author 
/ owner(s). SIGGRAPH 2008, Los Angeles, California, August 11 15, 2008. ISBN 978-1-60558-466-9/08/0008 
 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	</section>
	<section>
		<section_id>1401043</section_id>
		<sort_key>110</sort_key>
		<section_seq_no>3</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Green scenes]]></section_title>
		<section_page_from>3</section_page_from>
	<article_rec>
		<article_id>1401044</article_id>
		<sort_key>120</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>9</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[It's not easy being green]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1401032.1401044</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401044</url>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>H.5.1</cat_node>
				<descriptor>Animations</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>H.5.1</cat_node>
				<descriptor>Artificial, augmented, and virtual realities</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.8</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010392</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Mixed / augmented reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341.10010349.10011310</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Simulation types and techniques->Simulation by animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010507</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Graphics recognition and interpretation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010866</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Virtual reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010866</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Virtual reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1098381</person_id>
				<author_profile_id><![CDATA[81100029745]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Hans]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Rijpkema]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Rhythm and Hues Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098382</person_id>
				<author_profile_id><![CDATA[81421593139]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Gregory]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Steele]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Rhythm and Hues Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098383</person_id>
				<author_profile_id><![CDATA[81421596522]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Matt]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Derksen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Rhythm and Hues Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 It's not easy being green Hans Rijpkema* Gregory Steele Matt Derksen Rhythm and Hues Studios 1. 
Introduction A tight integration between rigging, animation and lighting is one of the essential components 
in making the Hulk and Abomination characters come to life for the movie "The Incredible Hulk". When 
the characters are animated the rigs allow for automatic generation and manual control of many lighting 
properties.  2. Design Interactive design sessions with the director were used to home in to the final 
look of the Hulk and Abomination. Using reference materials from body builders which showed all their 
muscles and veins flexing and pumping, the Hulk became a very fit and trim character with zero percent 
body fat. A special feature of Abomination are the bones that are protruding through the skin. 3. Muscles 
The body deformations of the characters are almost entirely based upon a muscle system which allows for 
automated and manually controlled muscle flexing. The animator controls the muscle firing and fake dynamics 
methods are used to simulate skin and muscle jiggle. The facial deformations are also fully muscle based 
and are crucial in creating the effect of skin sliding over the distinctive jaw (mandible) and cheek 
(zygomatic) bones. Motion capture was used to verify that the rigs are capable of the full range of motion 
(from subtle to extreme) of the two actors playing the creatures human counterpart. The facial rigs can 
both be driven by motion capture and keyframe animation using the same controls. 4. Animated lighting 
properties The rig produces animated lighting properties in the form of point attributes on the skin 
geometry. These are, amongst others, based upon skin compression and stretching. A robust subsurface 
technique that allows for mapping of many of its parameters, like * hans@rhythm.com gsteele@rhythm.com 
mderksen@rhythm.com scatter distance, refraction index, and tissue color. then uses these point attributes 
to drive for example the blending of color maps. This can be used, for example, to simulate the whitening 
of the knuckles on a clenched fist, the accumulation of blood and the appearance of subsurface veins 
when an arm is flexed. The displacement maps are separated into a number of layers to independently and 
locally control muscle striation, vein detail and skin wrinkles. These again are driven by the point 
attributes and by key framed curves created by either character animators or technical animators. On 
top of that, a pulsing vein system to simulate blood pumping through the body can be controlled by the 
lighter, using animated 3D mattes and noise filters provided by the rigger. Another good example of how 
rigging and lighting are interconnected is the method used to create the effect of skin sliding over 
fine detailed bone, muscle and tissue structure underneath it. This is done by generating animated displacement 
maps from independently moving skin geometries at render time.  5. Conclusion The tight integration 
between stages of the character pipeline which are traditionally much more independent is crucial in 
order to create a smooth work flow for complex characters with a great amount of skin and muscle detail. 
 Copyright is held by the author / owner(s). SIGGRAPH 2008, Los Angeles, California, August 11 15, 2008. 
ISBN 978-1-60558-466-9/08/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1401045</article_id>
		<sort_key>130</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>10</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Shaping, simulating and rendering the grasses of <i>Madagascar: Escape 2 Africa</i>]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1401032.1401045</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401045</url>
		<abstract>
			<par><![CDATA[<p>Covering large savannas of Africa with grass created unique challenges for set-dressing, character interaction, and rendering for DreamWorks Animation's "Madagascar: Escape 2 Africa". In many sequences, grass is our most important set dressing element. We enabled layout artists to set-dress grass by placing large geometric shapes. We wrote tools that enable our surfacing department to fill these shapes with stylized grass. We expanded our fur collisionavoidance software, <i>smoosh</i>, to enable thousands of characters to interact with grass. To keep computation cost manageable, each colliding strand is assigned a simple collision response animation rather than using a dynamic strand simulation. We sped up turnaround time and deployment using arbitrary fur geometric regions, geometry simplification, significant parallelization, and "click button" simplicity. To improve grass anti-aliasing in our renderer, we apply transparency. We made improvements to our deferred shading renderer in order to accommodate large volumes of transparent geometry.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>F.2.2</cat_node>
				<descriptor>Geometrical problems and computations</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Curve, surface, solid, and object representations</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Geometric algorithms, languages, and systems</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.6.1</cat_node>
				<descriptor>Systems theory</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341.10010346.10010347</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Simulation theory->Systems theory</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061.10010063</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures->Computational geometry</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010399</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Parametric curve and surface models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010401</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Volumetric models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1098384</person_id>
				<author_profile_id><![CDATA[81421592438]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Robyn]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Rindge]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[PDI/DreamWorks]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098385</person_id>
				<author_profile_id><![CDATA[81100418327]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Feng]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Xie]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[PDI/DreamWorks]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<article_type art_type="regular_article" />
	</article_rec>
	</section>
	<section>
		<section_id>1401046</section_id>
		<sort_key>140</sort_key>
		<section_seq_no>4</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Machines & monsters: Iron Man & cloverfield]]></section_title>
		<section_page_from>4</section_page_from>
	<article_rec>
		<article_id>1401047</article_id>
		<sort_key>150</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>11</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Cloverfield]]></title>
		<subtitle><![CDATA[how to destroy a city in 12 easy steps]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1401032.1401047</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401047</url>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.6.1</cat_node>
				<descriptor>Systems theory</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.6.5</cat_node>
				<descriptor>Modeling methodologies</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.8</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010341.10010342.10010343</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Model development and analysis->Modeling methodologies</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010507</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Graphics recognition and interpretation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341.10010346.10010347</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Simulation theory->Systems theory</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1098386</person_id>
				<author_profile_id><![CDATA[81421596995]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Eric]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Leven]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tippett Studio]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098387</person_id>
				<author_profile_id><![CDATA[81421600094]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Devin]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Breese]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tippett Studio]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098388</person_id>
				<author_profile_id><![CDATA[81421600743]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Chris]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Morley]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tippett Studio]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 CLOVERFIELD: How to Destroy a City in 12 Easy Steps Eric Leven Devin Breese Chris Morley Visual Effects 
SupervisorMatchmove SupervisorCompositing Supervisor Tippett Studio Tippett Studio Tippett Studio  Figure 
1: Various stages of The Monster 1 Introduction Plunk a large CG monster into a metropolitan city, mix 
in alow budget with high entertainment value, and watch it froma single, shaky point of view for 84 minutes 
to createCloverfield. Bad Robot approached us and asked if it waspossible to destroy a digital NYC. Here's 
our recipe. 2 Exposition The complexity of the visual effects work in Cloverfield lay inthe challenge 
of addressing the cinéma-vérité, single pointof view, handheld camera style of the film while seamlesslyintegrating 
the CG character the perpetrator of mayhem central to the narrative. As the camera swoops and jigglesin 
the style of a non-professional videographer, we catchglimpses of the monster stomping and crashing through 
thestreets of New York. To create this 'no cuts' style of filmmaking, multiple liveaction plates were 
digitally 'stitched' together to simulate asingle take. Prior to starting animation, matchmovechallenges, 
made more difficult due to plates not lining up,had to be overcome. The seamless takes were constructed 
employing various tricks from editorial: soft cuts, threeframe dissolves; and from visual effects: a 
basic morph, apartial 3D reconstruction of NYC, and sometimes all of theabove. Eric Leven, Tippett Studio 
s VFX Supervisor, declared that the matchmove artists were the unsung heroes of Cloverfield. Matchmovers 
used in-house tools for much of the work, but largely relied on the brute force of hand andtheir excellent 
eyeballing abilities to create the backgroundplates. The monster was designed to have a translucent, 
albinoquality as if it had very little exposure to sunlight. However, the monster attacks at night and 
tromps through parts of the city with varying amounts of light. Throughout the film the monster s color 
and textures had to be constantly tweakedto match the change in environments and visibility. As more 
of the creature is revealed, the audience sees the body,muscles and face become more defined with the 
use of muscle simulations and subsurface scattering. Even the veins and pulsating clots just beneath 
the surface of the monster s translucent skin are visible. In the middle of postproduction the filmmakers 
decided tochange a few key shots in the movie, providing additionalchallenges for the crew. Instead of 
keeping the creaturestationary while being attacked by fighter jets, the monsterwould now tromp through 
ten city blocks before the cameravirtually tilts up to reveal a B2 bomber swooping in anddropping its 
payload on a carpet-bombed Manhattan.Before rising again through CG explosions and billowingclouds the 
monster falls down taking with it chunks ofskyscrapers and bits of New York. At the beginning of the 
project the filmmakers wanted theaudience to see very little of the creature but as postproduction progressed 
they changed their minds. Not satisfied with a fleeting glimpse of the monster's jaw as heattacked the 
cameraman, the monster would now stand before the audience in all its glory, staring down into avirtual 
camera for a fully CG shot that lasted over a minute.Finally, the audience got what they came to see, 
the monster. The breathing sacks, facial detail, and internalmusculature are all monstrously visible, 
right up until thecamera and the cameraman are eaten. If one looks closely,Hud's legs are visibly stuck 
between the monster's teeth, asthe camera falls back to the ground. 3 Conclusion The team of artists 
at Tippett Studio accomplished all of thison a shoestring budget with a fast turnaround schedule.After 
all, Cloverfield was a low budget monster movie. Copyright is held by the author / owner(s). SIGGRAPH 
2008, Los Angeles, California, August 11 15, 2008. ISBN 978-1-60558-466-9/08/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1401048</article_id>
		<sort_key>160</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>12</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Digital costuming and virtual backgrounds on Iron Man]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1401032.1401048</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401048</url>
		<abstract>
			<par><![CDATA[<p>Overview, On-Set Experiences and Virtual Backgrounds: <b>Ben Snow</b></p> <p>- Preproduction Preparation</p> <p>- Stories From the Set</p> <p>- On-set Environment Capture For Lighting and Virtual Backgrounds</p> <p>- Using Tiled Backgrounds to Enhance or Replace Practical Camera Moves</p> <p>- Creating Backgrounds From Scratch: the Suit Up Sequence</p> <p>Sequence Development and Animation: <b>Hal Hickel</b></p> <p>- Developing the Story and Action Sequences</p> <p>- Limitations of the Practical Suits</p> <p>- Full and Partial Suit Replacements</p> <p>- Mocap vs. ILM's iMocap vs. Hand Animation</p> <p>- Animating Realistic Flight Dynamics</p> <p>- Rigging a Rigid Suit</p> <p>- Smoke Trail Path Visualization During Animation</p> <p>Technical Issues, Lighting and Rendering: <b>Doug Smythe</b></p> <p>- Complexity of the CG Suit, Internal Pieces and "Cool Mech"</p> <p>- Lighting and Materials</p> <p>- Smoke Trail Renders</p> <p>- 3D Renders vs. 2D Comp Treatments</p> <p>- Summary</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>J.5</cat_node>
				<descriptor>Fine arts</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>H.5.1</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.8</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010469.10010471</concept_id>
				<concept_desc>CCS->Applied computing->Arts and humanities->Performing arts</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010469.10010470</concept_id>
				<concept_desc>CCS->Applied computing->Arts and humanities->Fine arts</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002951.10003227.10003251</concept_id>
				<concept_desc>CCS->Information systems->Information systems applications->Multimedia information systems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010507</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Graphics recognition and interpretation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1098389</person_id>
				<author_profile_id><![CDATA[81421592281]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Ben]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Snow]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Industrial Light & Magic]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098390</person_id>
				<author_profile_id><![CDATA[81421594268]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Hal]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hickel]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Industrial Light & Magic]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098391</person_id>
				<author_profile_id><![CDATA[81421597508]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Doug]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Smythe]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Industrial Light & Magic]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Digital Costuming and Virtual Backgrounds on Iron Man Ben Snow Hal Hickel Doug Smythe Visual Effects 
Supervisor Animation Director Digital Production Supervisor Industrial Light &#38; Magic Industrial Light 
&#38; Magic Industrial Light &#38; Magic  1 Introduction Iron Man presented ILM with a unique opportunity 
to leverage the latest computer graphics technology in bringing Marvel's latest big-screen superhero 
to life. ILM's team built CG versions of Iron Man's Mark 2 and Mark 3 suits that looked as real as the 
practical suits but without physical mobility limitations. This, along with the inherent tweakability 
of look and motion that CG provides resulted in the use of ILM's CG model almost every time Iron Man 
appears on screen in the silver or red and gold suit. ILM also created Iron Man's nemesis, Iron Monger, 
digital fighter planes, cars, industrial assembly robots, and several partially-or fully-digital environments 
to allow complete freedom of camera movement beyond what was filmed on location. The presenters will 
discuss some of the technical and creative issues encountered during the shooting and production of Iron 
Man, including on-set environment capture, the use of on-set or off-set motion capture vs. traditional 
hand animation, the complexities of the various CG models, lighting and rendering technology, and other 
effects.  2 Abstract Overview, On-Set Experiences and Virtual Backgrounds: Ben Snow .Preproduction 
Preparation .Stories From the Set .On-set Environment Capture For Lighting and Virtual Backgrounds .Using 
Tiled Backgrounds to Enhance or Replace Practical Camera Moves .Creating Backgrounds From Scratch: the 
Suit Up Sequence Sequence Development and Animation: Hal Hickel .Developing the Story and Action Sequences 
.Limitations of the Practical Suits .Full and Partial Suit Replacements .Mocap vs. ILM s iMocap vs. Hand 
Animation .Animating Realistic Flight Dynamics .Rigging a Rigid Suit .Smoke Trail Path Visualization 
During Animation  Copyright is held by the author / owner(s). SIGGRAPH 2008, Los Angeles, California, 
August 11 15, 2008. ISBN 978-1-60558-466-9/08/0008 Technical Issues, Lighting and Rendering: Doug Smythe 
.Complexity of the CG Suit, Internal Pieces and Cool Mech .Lighting and Materials .Smoke Trail Renders 
.3D Renders vs. 2D Comp Treatments .Summary  3 Presenters In 1994, Ben Snow left his native Australia 
to join ILM, where his first project was to help create the three­dimensional computer graphics image 
of the Enterprise B for Star Trek: Generations. In addition to his feature credits, Snow has been instrumental 
in the research and development required for the groundbreaking images seen in Twister, Deep Impact and 
Pearl Harbor. Snow has twice been honored with Academy Award nominations for best achievement in visual 
effects for his work on Pearl Harbor and Star Wars: Episode II Attack of the Clones. Hal Hickel joined 
Industrial Light &#38; Magic in 1996 as an animator for The Lost World: Jurassic Park. In 2000, Hickel 
was moved up to animation supervisor, on Steven Spielberg s A.I. Artificial Intelligence. Hickel has 
since supervised the animation work on Star Wars: Episode II Attack of the Clones, Lawrence Kasdan s 
Dreamcatcher and the Pirates of the Caribbean trilogy. In 2007, his work on Pirates of the Caribbean: 
Dead Man s Chest was recognized with both an Academy Award® and a BAFTA for Best Achievement in Visual 
Effects. Doug Smythe joined the Computer Graphics Department of ILM in June of 1987 after receiving his 
B.S. degree in Electrical Engineering and Computer Sciences from the University of California at Berkeley. 
He received an Oscar for Best Visual Effects for his work on Death Becomes Her, and three Scientific 
and Technical Achievement awards from the Academy of Motion Picture Arts and Sciences in the areas of 
Morphing, Digital Image Retouching and Digital Compositing. Doug has worked on 29 feature film projects, 
including Willow, Indiana Jones and the Last Crusade, Back to the Future Parts 2 and 3, Terminator 2, 
Star Trek: Generations, Star Wars Episode 1, The Perfect Storm, Star Wars Episode 3, Pirates 2 and 3 
and Iron Man. 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	</section>
	<section>
		<section_id>1401049</section_id>
		<sort_key>170</sort_key>
		<section_seq_no>5</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Teaching with graphics]]></section_title>
		<section_page_from>5</section_page_from>
	<article_rec>
		<article_id>1401050</article_id>
		<sort_key>180</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>13</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Addressing student and industry needs through experiential learning courses to better prepare the student for real-world work experience]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1401032.1401050</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401050</url>
		<categories>
			<primary_category>
				<cat_node>K.3.1</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>J.1</cat_node>
				<descriptor>Education</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>K.3.2</cat_node>
				<descriptor>Curriculum</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>H.5.1</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.8</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010406</concept_id>
				<concept_desc>CCS->Applied computing->Enterprise computing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003456.10003457.10003527.10003530</concept_id>
				<concept_desc>CCS->Social and professional topics->Professional topics->Computing education->Model curricula</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002951.10003227.10003251</concept_id>
				<concept_desc>CCS->Information systems->Information systems applications->Multimedia information systems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010507</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Graphics recognition and interpretation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010489</concept_id>
				<concept_desc>CCS->Applied computing->Education</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Management</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1098392</person_id>
				<author_profile_id><![CDATA[81100540972]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Jana]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Whittington]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Purdue University Calumet]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098393</person_id>
				<author_profile_id><![CDATA[81320492908]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Kim]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nankivell]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Purdue University Calumet]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Addressing Student and Industry Needs through Experiential Learning Courses To Better Prepare the Student 
for Real-World Work Experience. Jana Whittington, Associate Professor of Computer Graphics Technology 
Kim Nankivell, Assistant Professor of Computer Graphics Technology Purdue University Calumet whitting@calumet.purdue.edu 
nankivel@calumet.purdue.edu 219-989-2354 Students in a Graphics degree program need a variety of real-world 
portfolio projects and experience before graduation. One way to incorporate real world experiences is 
through experiential learning (EL) curriculum. Employers want a minimum of three years experience and 
relevant portfolio work. Real world experience, to employers, often includes not only the technical skills 
but also the soft skills of teamwork, appreciation for diversity, and communication. Traditional classroom 
learning, as well as traditional classroom simulations of real-world portfolio projects, may not always 
fully prepare the learner for the CG work environment. Conversely, immersing a student in the real-world 
CG work environment may not always provide the needed pedagogical structure that will fulfill EL requirements 
or standard course and program requirements. The presentation will include a quick overview and questions 
for discussion. A student who has experienced experiential education courses will be available to answer 
questions about the EL experience. Copyright is held by the author / owner(s). SIGGRAPH 2008, Los Angeles, 
California, August 11 15, 2008. ISBN 978-1-60558-466-9/08/0008 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1401051</article_id>
		<sort_key>190</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>14</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Building Planet Diggum]]></title>
		<subtitle><![CDATA[a case study of multi-discipline, multi-touch gaming collaboration]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1401032.1401051</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401051</url>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>K.8.0</cat_node>
				<descriptor>Games</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>H.5.1</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.8</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.6.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10002951.10003227.10003251.10003258</concept_id>
				<concept_desc>CCS->Information systems->Information systems applications->Multimedia information systems->Massively multiplayer online games</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10011007.10010940.10010941.10010969.10010970</concept_id>
				<concept_desc>CCS->Software and its engineering->Software organization and properties->Contextual software domains->Virtual worlds software->Interactive games</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010476.10011187.10011190</concept_id>
				<concept_desc>CCS->Applied computing->Computers in other domains->Personal computers and PC applications->Computer games</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002951.10003227.10003251</concept_id>
				<concept_desc>CCS->Information systems->Information systems applications->Multimedia information systems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010507</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Graphics recognition and interpretation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1098394</person_id>
				<author_profile_id><![CDATA[81100433862]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Paul]]></first_name>
				<middle_name><![CDATA[J.]]></middle_name>
				<last_name><![CDATA[Diefenbach]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Drexel University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Building Planet Diggum: A Case Study of Multi­discipline, Multi­Touch Gaming Collaboration Paul J. 
Diefenbach RePlay Lab and Digital Media Program Antoinette Westphal College of Media Arts and Design, 
Drexel University pjdief@drexel.edu 1 Introduction The emergence of digital games in academia is well 
documented in trade publications and the mass­media. While academic video game development has garnered 
widespread publicity due to the size of the industry, the growth of the serious game movement, and the 
potential impact on faltering Computer Science enrollment, many academic programs fail to address the 
expansive and interdisciplinary nature of modern game development because they live in one department. 
At Drexel, we are trying to address these issues by leveraging the strengths of the entire university 
as well as by uniting previously separate capstone projects. At Drexel, most majors require a senior 
project as a capstone project for graduation. While traditionally these have been inside each major, 
several recent multi­discipline collaborations have been very successful. Gaming has offered several 
opportunities for such collaborations as well as for extending the work beyond its original development. 
This paper focuses on one such effort: Planet Diggum. Planet Diggum is a god­game in the tradition of 
the DS game, Nintendogs, in which multiple users can interact with individual Diggum creatures and their 
world via natural finger and hand gestures. The advance of gesture computing and the new availability 
of multi­point touch screens permit a novel mechanism and universal vocabulary for direct interaction 
with a virtual world, but also provide challenges for an academic environment. This project is a true 
interdisciplinary project involving at least 5 departments in two of Drexel s colleges, and three prior 
multi­discipline efforts; further, it shows how collaboration between Computer Science and other programs 
can expand the use of technologies beyond their original intent. 2 Capstone Case Study Planet Diggum 
is an investigative gaming application being developed by Computer Science and Drexel Digital Media (DIGM) 
graduate students as part of their gaming curriculum and utilizes a multi­point FTIR display hardware 
and software being built by Computer Science (CS) and Electrical and Computer Engineering (ECE) undergraduate 
students. This project is coordinated by Drexel s RePlay Lab, which is a multi­departmental lab researching 
play and gaming technology. Gross body gestures, such as those employed by the Wii, were abandoned in 
favor of a more intimate gaming experience to enable proximity­based collaborative play. This physicality 
is possible using multi­touch displays, and the accessibility of this technology was also a motivating 
force.  One example of the cross­discipline coordination required is the translation of multiple hardware­detected 
touch blobs into a single recognizable gesture. Our approach is to segment the problem by defining a 
glossary of finger and hand gestures which define time/space events. Finger gestures are used to define 
initiating and terminating events of a hand gesture; hand gestures map to corresponding actions. These 
translations had to take into account hardware limitations, server architecture, and game API requirements: 
expertise which bridged several disciplines. 3 Conclusion and Future Work Multi­point touch screens 
and gesture tracking offer new opportunities to create highly intuitive and user­friendly systems for 
even the novice user. This project is attempting to create a testbed kiosk where multiple users can interact 
without need of specialized training. The Planet Diggum project leverages low­cost hardware, open source 
software libraries, and standards­based 3D protocols to create a modular, extensible framework for rapid 
prototyping and experimentation, and current and future work centers around extensions of this platform 
for inclusion of learned arm gestures through Hidden Markov Model (HMM) software originally developed 
for the Wii, integration of mini­games to demonstrate the extensibility of the platform, and re­purposing 
this kiosk technology for middle­school education. Copyright is held by the author / owner(s). SIGGRAPH 
2008, Los Angeles, California, August 11 15, 2008. ISBN 978-1-60558-466-9/08/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1401052</article_id>
		<sort_key>200</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>15</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Integrate experiential learning to simulate a website design project process]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1401032.1401052</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401052</url>
		<abstract>
			<par><![CDATA[<p>The current literature suggests that experiential learning is a necessary component of formal instruction in higher education. Experiential learning as a formal part of college and university curricula extends across the range of subject areas and disciplines. The purpose of this project is to detail research how to blend experiential learning principles with project management into a real case of website design practice for the new age of electronic learning. Based on the conceptualization in experiential learning and Internet technology development, a teaching and learning project flows in the practice of web design development is designed to facilitate students in this process. A cohort group of students in Web Design class join this experimental scenario to implement the assigned project of creating a new website design. Within the terms of the learning process, students simulate three basic steps of learning models: Experience, Reflect, and Apply. A total of six learning objectives were initiated, evaluated, and interpreted: creativity, skill sets, productivity, team work, deadlines, and client skills.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>K.3.2</cat_node>
				<descriptor>Computer science education</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>K.3.2</cat_node>
				<descriptor>Information systems education</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>K.6.1</cat_node>
				<descriptor>Systems analysis and design</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>K.6.1</cat_node>
				<descriptor>Systems development</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>H.3.4</cat_node>
				<descriptor>World Wide Web (WWW)</descriptor>
				<type>P</type>
			</other_category>
			<other_category>
				<cat_node>I.3.8</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003456.10003457.10003527.10003531.10003532</concept_id>
				<concept_desc>CCS->Social and professional topics->Professional topics->Computing education->Computing education programs->Information systems education</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003456.10003457.10003490.10003491.10003496</concept_id>
				<concept_desc>CCS->Social and professional topics->Professional topics->Management of computing and information systems->Project and people management->Systems development</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003456.10003457.10003490.10003491.10003495</concept_id>
				<concept_desc>CCS->Social and professional topics->Professional topics->Management of computing and information systems->Project and people management->Systems analysis and design</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002951.10003317</concept_id>
				<concept_desc>CCS->Information systems->Information retrieval</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010507</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Graphics recognition and interpretation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002951.10003152</concept_id>
				<concept_desc>CCS->Information systems->Information storage systems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003456.10003457.10003527.10003531.10003533</concept_id>
				<concept_desc>CCS->Social and professional topics->Professional topics->Computing education->Computing education programs->Computer science education</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Management</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1098395</person_id>
				<author_profile_id><![CDATA[81335488888]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Mei-Fen]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Chen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Robert Morris College]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Cantor, J. A. (1995). Experiential learning in higher education: Linking classroom and Community. ASHE-ERIC Higher Education Report Series 95--7, (Volume 24--7).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Dewey, J. (1938). Experience and education. New York: Collier Books.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Simm, D. (2005). Experiential learning: assessing process and product. Planet No. 15. December 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Stevens, P. W., &amp; Richards, A. (1992). Changing schools through experiential education. ERIC Clearinghouse on Rural Education and Small Schools Charleston WV]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Sterling, M. (2007). Service-learning and interior design: A case study. Journal of Experiential Education.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1088873</ref_obj_id>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[McNurlin, B. C., &amp; Ralph H. Jr. (2006). Information systems management in Practice. 7&#60;sup&#62;th&#60;/sup&#62; edition: Prentice-Hall.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Turk, W. (2008). Project management top 20. Defense AT&L: January-February 2008.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Integrate Experiential Learning to Simulate a Website Design Project Process Mei-Fen Chen, Ph.D. Robert 
Morris College mchen@robertmorris.edu Abstract The current literature suggests that experiential learning 
is a necessary component of formal instruction in higher education. Experiential learning as a formal 
part of college and university curricula extends across the range of subject areas and disciplines. The 
purpose of this project is to detail research how to blend experiential learning principles with project 
management into a real case of website design practice for the new age of electronic learning. Based 
on the conceptualization in experiential learning and Internet technology development, a teaching and 
learning project flows in the practice of web design development is designed to facilitate students in 
this process. A cohort group of students in Web Design class join this experimental scenario to implement 
the assigned project of creating a new website design. Within the terms of the learning process, students 
simulate three basic steps of learning models: Experience, Reflect, and Apply. A total of six learning 
objectives were initiated, evaluated, and interpreted: creativity, skill sets, productivity, team work, 
deadlines, and client skills. 1 Introduction David Simm (2005) stated reflective observation, abstract 
conceptualization, and active experimentation are used to derive concrete experience. John Dewey (1938) 
was an early promoter of the idea of learning through direct experience, by action and reflection. This 
type of learning differs from much traditional education in that teachers first immerse students in action 
and then ask them to reflect on the experience (Stevens &#38; Richards, 1992). The use of David Kolb 
s experiential learning models to examine the process of creating knowledge acquisition through the transformation 
of experience has been adopted in an increasing number of areas (Lai, 2007). In light of approaching 
theory into practice in a real world situation, corporate learning professionals have begun to achieve 
a better understanding of the relationship between education, work, and technology. The new type of learning 
model is taking hold in the business world by blending technology and experience-based learning together 
(Adkins. 2004). Art and design faculty are no strangers to experiential learning. One cannot learn the 
complexities of the design discipline without extensive design studio projects. Where experiential learning 
is well integrated, students demonstrate a greater understanding of the complexity of real-world problems 
than those without field experiences (Sterling, 2007). 2 Design Methodologies and Approach The case 
study approach is utilized to proceed through a university service-learning project. Following the project, 
students keep tracking qualitative journals based on their weekly learning and execution experiences; 
a number of semi-structured interviews are conducted with students and faculty in order to get an insight 
into their perceptions and experiences of the learning exercise. Students experience phases of different 
stages during the process including design plan, project management, site architecture design, interface 
mock up layout design, file management, and the completion of the whole site design. 3 Summary This 
result of study benefits the academic community with an understanding of the theory to practice between 
education, work, and technology. The finding also brings positive impact for program design and development 
and operation in web learning community. It provides rich dialogues about the students experiences which 
lead to enhance their collegiate experience, and assists them in building a foundation for life leading 
toward real world practice. Research on experiential learning on website design development and management 
is minimal. With this approach the process is viewed as important as the product , this study hopes to 
stimulate further work in this area.  Reference Cantor, J.A. (1995). Experiential learning in higher 
education: Linking classroom and Community. ASHE-ERIC Higher Education Report Series 95-7, (Volume 24-7). 
Dewey, J. (1938). Experience and education. New York: Collier Books. Simm, D. (2005). Experiential learning: 
assessing process and product. Planet No.15. December 2005. Stevens, P.W., &#38; Richards, A. (1992). 
Changing schools through experiential education. ERIC Clearinghouse on Rural Education and Small Schools 
Charleston WV Sterling, M. (2007). Service-learning and interior design: A case study. Journal of Experiential 
Education. McNurlin, B. C., &#38; Ralph H. Jr. (2006). Information systems management in Practice. 7th 
edition: Prentice-Hall. Turk, W. (2008). Project management top 20. Defense AT&#38;L: January-February 
2008. Copyright is held by the author / owner(s). SIGGRAPH 2008, Los Angeles, California, August 11 15, 
2008. ISBN 978-1-60558-466-9/08/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1401053</article_id>
		<sort_key>210</sort_key>
		<display_label>Article No.</display_label>
		<pages>2</pages>
		<display_no>16</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[Teaching computer graphics in secondary education]]></title>
		<subtitle><![CDATA[building a model program for computer graphics background: adding to the partnership academies model]]></subtitle>
		<page_from>1</page_from>
		<page_to>2</page_to>
		<doi_number>10.1145/1401032.1401053</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401053</url>
		<categories>
			<primary_category>
				<cat_node>K.3.2</cat_node>
				<descriptor>Computer science education</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>K.3.2</cat_node>
				<descriptor>Curriculum</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.8</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010507</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Graphics recognition and interpretation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003456.10003457.10003527.10003530</concept_id>
				<concept_desc>CCS->Social and professional topics->Professional topics->Computing education->Model curricula</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003456.10003457.10003527.10003531.10003533</concept_id>
				<concept_desc>CCS->Social and professional topics->Professional topics->Computing education->Computing education programs->Computer science education</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Management</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1098396</person_id>
				<author_profile_id><![CDATA[81421599792]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Linda]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Neuhaus]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Monache High School]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[ACM SIGGRAPH Education Committee. http://www.siggraph.org/education/.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Jung, T., (Oct. 2006) State <i>Schools Chief Jack O'Connell Awards $22.4 Million in Grants For California Partnership Academies</i>. Retrieved May28, 2008 from http://www.cde.ca.gov/nr/ne/yr06/yr06rel128.asp]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Kennedy, M., 28 Principles of Animation, February, 2008; http://www.animationarena.com/principles-of-animation-3.html]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Meirs, D., Acting and Animation, January, 2008; http://www.animationarena.com/principles-of-animation-3.html]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1179328</ref_obj_id>
				<ref_obj_pid>1179295</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Orr, G. and Alley, T. 2007. A Knowledge Base for Emerging Discipline of Computer Graphics http://ogems.inesc.pt/ModuleInfo.aspx?id=4 http://education.siggraph.org/resources/knowledge-base/FrontPage/report]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Teaching Computer Graphics in Secondary Education: Building a Model Program for Computer Graphics Background: 
Adding to the Partnership Academies Model Mrs. Linda Neuhaus* Monache High School  1 Introduction 
 I am currently in my third year as a high school art teacher at Monache High School, Porterville, California. 
My program, which is in its second year at the school, uses Autodesk Maya 8.5. This program is under 
the umbrella of the State of California s Partnership Academies. The Partnership Academy Model is a three 
year program for grades 10-12 and is structured as a school-within-a school. This model originated with 
the Philadelphia Academies, 1960 and was started in California in the 1980 s. The program is part of 
the high school reform movement and includes integrating academic and career technical education. (http://www.cde.ca.gov/ci/gs/hs/cpagen.asp) 
 Business partnerships are established between the academy supervisor and students and each grade is 
given a community project based on the partnership agreement. Academies have been evaluated and shown 
to have a positive impact on raising pass rates on high school exit exams, higher rates in English and 
Math than the state average. The Academy model has effectively increased student academic achievement 
and enrollment in post-secondary education. This is amazing considering half the students enrolled in 
these academies are identified as at-risk . (Jung, T., 2006). 2 Exposition Because the emphasis of 
the Academies includes forward thinking concerning post-secondary education, I looked at the core areas 
addressed by ACM SIGGRAPHS Curriculum Working Groups knowledgebase (ACM SIGGRAPH). These core areas also 
fit well in addressing the National Standards for Visual Arts Education for the secondary school level. 
The areas covered are CGF fundamentals students begin an in-depth study of the discipline by introduction 
to fundamental vocabulary, history and tools (Orr et al. 2007) used in CG programs, the film industry, 
and animation past and present. It is my hope that by using a CG program to address the art standard 
Connections, Relationships, Applications ( California Art Standard 5.0) this program would tie into the 
Physical sciences; understanding the real to create virtual worlds, and the use of visual literacy to 
convey a story beginning with visual writing or storyboard. (Orr et al. 2007). I also hope it will give 
marginal students who have checked out , not by lack of ability or talent, a reconnect with high school 
because they can learn vocational skills to be used immediately after graduation from high school or 
in higher education. Initial First Year of Program and Goals: During the first year of our computer 
graphics class, we worked in Maya 7.0; we had 22 licenses available and 22 students applied. Out of those 
22 students half the students had previous training in Photoshop for photography as an art course, yearbook, 
web page design or AutoCAD (for their engineering/physics or drafting classes). Few students had any 
drawing background. The first 4 weeks of the year we spent learning basic drawing and composition, story 
boarding characters, and designing environments. Students were introduced to the UI elements of Maya 
7.0 which can be overwhelming to beginners, and then were assigned projects in Getting Started in Maya. 
Through trial and error, I found students worked best in pairs helping each other read through extensive 
material and new vocabulary. Quizzes were given after four weeks of introducing students to the UI hoping 
they would also understand why an action or tool was used. The best results occurred when students collaborated. 
Students who caught on quickly gave helpful information to the other confused students. After finishing 
Getting Started in Maya, first year students finished their second semester working on Maya 6 Foundation 
projects of Jack in the Box and building NURBS and Polygon space ships. These three projects were well 
written and thorough in showing production pipeline in creating animations with NURBS and Polygons. 
Second Year Developments in the Program: This year 2007-2008 word spread among students and more informed 
students in Photoshop art, and web page design registered for the class. We also picked up several students 
from the Cisco academy classes. All these classes are offered in the Academy Partnership Program. Students 
from the Cisco program were familiar with gaming and expressed interest in MEL programming skills. Many 
of these students improvised their projects from Getting Started in Maya and as a result all were finished 
with this book by end of the first semester. Their semester final was a portfolio of their first semester 
work. This second semester I asked students do the Jack in the Box project and the NURBS /Polygon Spaceship 
projects from Maya 6 Foundation during the third quarter. The Jack project gives the students experience 
in basic and secondary animation, character sets and sculpting. The spaceship projects show the differences 
in polygonal and NURBS surface modeling as well and the use of dynamics and the painting program in Autodesk 
Maya. From this foundation, I required a final project of a short animation. Students were paired in 
teams. They began by storyboarding an environment and two characters. Principles 1 are introduced through 
online readings on Acting and Animation 2. They were also introduced to the various camera shots used 
in film to help them decide camera angles, shots and batch rendering. 3 Conclusion Our Board of Education 
adopted the 3D graphics curriculum for Fall, 2007. Our class also was approved to become a two-hour Regional 
Occupational Program class for our county in the future. I hope this still occurs despite state spending 
cuts. I will be selecting an advisory committee to further help define our future needs; equipment recommendations, 
business partnerships, visiting artists, UC accreditation, and adoption of our course for community college 
credit or certificates. We hope to work with local community colleges that offer computer graphic courses 
like ours so that students will earn advance placement credit. __________________________________ *email 
l.neuhaus@comcast.net References ACM SIGGRAPH EDUCATION COMMITTEE. http://www. siggraph.org/education/. 
JUNG, T., (OCT. 2006) STATE Schools Chief Jack O'Connell Awards $22.4 Million in Grants For California 
Partnership Academies. Retrieved May28, 2008 from http://www.cde.ca.gov/nr/ne/yr06/yr06rel128.asp KENNEDY, 
M., 28 Principles of Animation, February, 2008; http://www.animationarena.com/principles-of-animation-3.html 
 MEIRS, D., Acting and Animation, January, 2008; http://www.animationarena.com/principles-of-animation-3.html 
 ORR, G. and ALLEY, T. 2007. A Knowledge Base for Emerging Discipline of Computer Graphics http://cgems.inesc.pt/ModuleInfo.aspx?id=4 
 http://education.siggraph.org/resources/knowledge-base/FrontPage/report 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	</section>
	<section>
		<section_id>1401054</section_id>
		<sort_key>220</sort_key>
		<section_seq_no>6</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Dancing with computers & technology]]></section_title>
		<section_page_from>6</section_page_from>
	<article_rec>
		<article_id>1401055</article_id>
		<sort_key>230</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>17</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Echo locations]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1401032.1401055</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401055</url>
		<abstract>
			<par><![CDATA[<p>The echo locations project is a series of site-specific installations utilizing motion sensing to invite observers to slow down, give the site their attention, and be still long enough for ghostly images to form of how people have moved through the site in the past.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.4.8</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.2.10</cat_node>
				<descriptor>Motion</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.6</cat_node>
				<descriptor>Edge and feature detection</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.5.2</cat_node>
				<descriptor>Feature evaluation and selection</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245.10010246</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems->Interest point and salient region detections</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352.10010380</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation->Motion processing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010257.10010321.10010336</concept_id>
				<concept_desc>CCS->Computing methodologies->Machine learning->Machine learning algorithms->Feature selection</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010238</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->Motion capture</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352.10010238</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation->Motion capture</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010225.10010227</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision tasks->Scene understanding</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1098397</person_id>
				<author_profile_id><![CDATA[81421595508]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Kirk]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Woolford]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Lancaster University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098398</person_id>
				<author_profile_id><![CDATA[81337489518]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Carlos]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Guedes]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Escola Superior de Artes Aplicades (ESART-IPCB)]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1291322</ref_obj_id>
				<ref_obj_pid>1291233</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Woolford, K., "Will.o.w1sp -- Installation Overview", Proceedings of the 15&#60;sup&#62;th&#60;/sup&#62; International Conference on Multimedia, ACM MM07, 2007, pp 379--380]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1291392</ref_obj_id>
				<ref_obj_pid>1291233</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Woolford, K., Guedes, C, "Particulate Matters: Generating Particle Flows from Human Movement", <i>Proceedings of the 15&#60;sup&#62;th&#60;/sup&#62; International Conference on Multimedia</i>, ACM MM, 07, 2007, pp 691--696.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Woolford, K., "Will.0.w1sp ('willo-wisp'), Performance Research, 11(4), 2006, pp. 30--38. http://www.bhaptic.net http://www.mac.com/carlosguedes/]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Echo Locations Kirk Woolford Carlos Guedes Lancaster University Escola Superior de Artes Aplicades 
(ESART-IPCB) ABSTRACT The echo locations project is a series of site-specific installations utilizing 
motion sensing to invite observers to slow down, give the site their attention, and be still long enough 
for ghostly images to form of how people have moved through the site in the past. MOVING IMAGES The 
Echo Locations projects build on the motion capture, particle systems, and slow interaction techniques 
developed for Will.0.w1sp. However, whereas the Will.0.w1sp characters moved through motion sequences 
captured in a studio, Echo Locations makes a stronger link to specific locations by capturing motion 
on site. The characters recreated by the particle software then become similar to ghosts, repeating movements 
which once occurred in the site. The software driving the particle systems creates chains of movement 
sequences and randomly drops in a sequence from either the original Will.0.w1sp performances, or from 
an earlier Echo Location. These chance movement sequences form a link between the location, the history 
of the location, and the history of the project. Just as with the original Will.0.w1sp, installations, 
if a visitor chases after the ghosts or Echos , they flee from that particular location and either re­emerge 
in another location on the site, or scatter into seemingly random forms. Only when visitors to the site 
are still and quiet will they reform and return to their movements. The intention of the piece is to 
use interaction to make visitors reflect on their personal impact on an environment as they move through 
a location and to hint at its history. The movement choreography and styling of the echo characters is 
intended to hold visitor s attention long enough for them to become aware of the environment, and the 
locations of individual screens are chosen as much for visual impact, as for their ability to communicate 
the former, and current, life of the location.   MOVING AUDIO The installations uses sound in an attempt 
to awaken curiosity and invite visitors to various locations on the site. The audio environment mixes 
samples recorded onsite together with simple melodies to create feelings of past inhabitants whether 
they be 6th C quiet and contemplative as in the case of the ruins of a Catholic Church overlooking Morecambe 
Bay, or very loud as in the case of the Storey Institute when the building was owned by the Mechanic 
s guilde. The installations also use Will.0.w1sp s granular synthesis code to generate audio from the 
movement data. If visitors to the site are cam and still, this data is played out very melodically, but 
if visitors move around or make sound of their own, the sound from the particle flows becomes very sharp, 
aggressive scratches and hisses. Just as the motion of the particle dancers evoke the site s past history, 
so does the audio environment. REFERENCES Woolford, K., Will.o.w1sp Installation Overview , Proceedings 
of the 15th International Conference on Multimedia, ACM MM07, 2007, pp 379-380 Woolford, K., Guedes, 
C, Particulate Matters: Generating Particle Flows from Human Movement , Proceedings of the 15th International 
Conference on Multimedia, ACM MM,07, 2007, pp 691-696. Woolford, K., Will.0.w1sp ( willo-wisp ), Performance 
Research, 11(4), 2006, pp.30-38. http://www.bhaptic.net http://www.mac.com/carlosguedes/ . Copyright 
is held by the author / owner(s). SIGGRAPH 2008, Los Angeles, California, August 11 15, 2008. ISBN 978-1-60558-466-9/08/0008 
 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1401056</article_id>
		<sort_key>240</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>18</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[re(PER)curso]]></title>
		<subtitle><![CDATA[an interactive mixed reality chronicle]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1401032.1401056</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401056</url>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.6</cat_node>
				<descriptor>Interaction techniques</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>H.5.1</cat_node>
				<descriptor>Artificial, augmented, and virtual realities</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.8</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10010391</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Graphics input devices</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010866</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Virtual reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010507</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Graphics recognition and interpretation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010866</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Virtual reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010392</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Mixed / augmented reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1098399</person_id>
				<author_profile_id><![CDATA[81421599742]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Anna]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Mura]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[SPECS, IUA, UPF, Barcelona, Spain]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098401</person_id>
				<author_profile_id><![CDATA[81421592714]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Behdad]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Rezazadeh]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[SPECS, IUA, UPF, Barcelona, Spain]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098402</person_id>
				<author_profile_id><![CDATA[81421601960]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Armin]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Duff]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[SPECS, IUA, UPF, Barcelona, Spain]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098403</person_id>
				<author_profile_id><![CDATA[81100217326]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Jonatas]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Manzolli]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Campinas, Campinas, Brasil]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098404</person_id>
				<author_profile_id><![CDATA[81310503273]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Sylvain]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Le Groux]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[SPECS, IUA, UPF, Barcelona, Spain]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098405</person_id>
				<author_profile_id><![CDATA[81421592668]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Zenon]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Mathews]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[SPECS, IUA, UPF, Barcelona, Spain]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098406</person_id>
				<author_profile_id><![CDATA[81316487917]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>7</seq_no>
				<first_name><![CDATA[Ulysses]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bernardet]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[SPECS, IUA, UPF, Barcelona, Spain]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098407</person_id>
				<author_profile_id><![CDATA[81421597848]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>8</seq_no>
				<first_name><![CDATA[Sytse]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wierenga]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[SPECS, IUA, UPF, Barcelona, Spain]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098408</person_id>
				<author_profile_id><![CDATA[81421597482]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>9</seq_no>
				<first_name><![CDATA[Sergi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bermudez]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[SPECS, IUA, UPF, Barcelona, Spain]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098400</person_id>
				<author_profile_id><![CDATA[81100464259]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>10</seq_no>
				<first_name><![CDATA[Paul]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Verschure]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[SPECS, IUA, UPF, Barcelona, Spain and ICREA Barcelona, Spain]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1112856</ref_obj_id>
				<ref_obj_pid>1112848</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Manzolli, J and Verschure P. 2005. Roboser: a Real-World Composition System. <i>Computer Music Journal</i>. 29: 55--74.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Eng, K., et al. 2003. Ada: An artificial creature for the Swiss Expo. 02. <i>IEEE International Conference on Robotics and Automation (ICRA 2003)</i>, pp. 4154--4159 Taipei, Taiwan]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 re(PER)curso: an interactive mixed reality chronicle Anna Mura 1 * Jonatas Manzolli 2 Ulysses Bernardet 
1 Behdad Rezazadeh 1 Sylvain Le Groux 1 Sytse Wierenga 1 Armin Duff 1 Zenon Mathews 1 Sergi Bermudez 
Paul Verschure 1, 3 1 SPECS, IUA, UPF 2 NICS, University of Campinas 3 ICREA Barcelona, Spain Barcelona, 
Spain Campinas, Brasil 1 Introduction re(PER)curso presents an interactive mixed reality narrative where 
two human performers a percussionist and a dancer -and a number of real-time synthetic actors including 
sonification, virtual cameras and an anthropomorphic avatar, explore the confluence of the physical and 
the virtual dimensions underlying existence and experience (Figure 1). The synthetic components of re(PER)curso 
are realized with computer generated graphics, automated moving light and stage control, video art, a 
synthetic music composition system called RoBoser [Manzolli and Verschure 2005], and an avatar embedded 
in a 3D graphic environment. The integration of all elements is realized through the multi-modal mixed 
reality system the eXperience Induction Machine (XIM) that is based on an earlier large scale public 
exhibition called Ada [Eng 2003]. XIM is controlled through a neuromorphic system that defines all the 
rules of interaction and performance dynamics and as a result the complete performance is synthesized 
in real-time and evolves without human intervention beyond that of the two human actors on the stage. 
re(PER)curso is an experiment in interactive narrative and explores the potential of virtual reality 
and augmented feedback technologies as tools for artistic expression. It expresses a general research 
strategy where the limits of advanced technologies are explored through their application in art. re(PER)curso 
is operated as an autonomous interactive installation that is augmented by 2 human performers. It is 
supported by a number of input devices that track and analyze the ongoing performance through cameras 
and microphones; controllers such as the synthetic composition engine RoBoser and output systems that 
include the large-scale real-time computer graphics, moving virtual and real cameras, and moving lights. 
Stage information obtained by the tracking systems is also projected onto the virtual world where it 
modulates the avatar s behavior allowing it to adjust body position, posture and gaze to the physical 
world and to adjust properties of the virtual cameras.  2 Performance re(PER)curso is performed in an 
augmented mixed reality environment, where the physical and the virtual are not overlapping, instead 
they are distinct and continuous. The border between the two environments is the projection screen that 
acts like a dynamic all seeing bi-directional eye (Figure 1).  Figure 1: in re(PER)curso the physical 
space and the virtual space are continuous and tightly coupled supporting real-time interaction. The 
narrative concept of re(PER)curso is captured in its name: Percurso: pathway, course; Recurso: source, 
point of view, re­PER-curso: re-assigned course. The chronicle of re(PER)curso evolves around four performative 
elements: Sopros (wind), Pedras (stones), Folha (leaves), Peles (skin) that are metaphors for our memory 
of the past, our interpreted present, anticipated future and their confluence. Throughout the performance, 
these elements are interpreted and re-interpreted by the percussionist, the dancer, RoBoser and the avatar. 
The live performances of re(PER)curso at the museum for contemporary art (MACBA) in Barcelona (June, 
2007) and during the ArtFutura festival (Barcelona, October, 2007) have shown that interactive autonomous 
mixed reality performances are a viable technological and artistic paradigm. 1. MANZOLLI, J and VERSCHURE 
P. 2005. Roboser: a Real-World Composition System. Computer Music Journal. 29: 55-74. 2. ENG, K., et 
al. 2003. Ada: An artificial creature for the Swiss Expo.02. IEEE International Conference on Robotics 
and Automation (ICRA 2003), pp. 4154-4159 Taipei, Taiwan  Supported by PRESENCCIA (FP6 IST FET 27731) 
and Phonos. Dancer : Afrika Martinez, Percussionist: Carme Garrigo Corresponding author: amura@iua.upf.edu 
Copyright is held by the author / owner(s). SIGGRAPH 2008, Los Angeles, California, August 11 15, 2008. 
ISBN 978-1-60558-466-9/08/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1401057</article_id>
		<sort_key>250</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>19</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Choreographisms]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1401032.1401057</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401057</url>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>H.5.1</cat_node>
				<descriptor>Animations</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>H.5.1</cat_node>
				<descriptor>Artificial, augmented, and virtual realities</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>J.5</cat_node>
				<descriptor>Performing arts (e.g., dance, music)</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.8</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341.10010349.10011310</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Simulation types and techniques->Simulation by animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010469.10010471</concept_id>
				<concept_desc>CCS->Applied computing->Arts and humanities->Performing arts</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010507</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Graphics recognition and interpretation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010392</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Mixed / augmented reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010866</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Virtual reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010866</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Virtual reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1098409</person_id>
				<author_profile_id><![CDATA[81421594398]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Alice]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bodanzky]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[ESDI]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098410</person_id>
				<author_profile_id><![CDATA[81421594666]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Julio]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lucio]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[IMPA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098411</person_id>
				<author_profile_id><![CDATA[81442602956]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Ilana]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Paterman]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[IMPA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098412</person_id>
				<author_profile_id><![CDATA[81421600242]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Analivia]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Cordeiro]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[IMPA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098413</person_id>
				<author_profile_id><![CDATA[81421600335]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Silvia]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Steinberg]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[ESDI]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098414</person_id>
				<author_profile_id><![CDATA[81100202267]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Luiz]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Velho]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[IMPA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Cordeiro, A., and Velho, L. 2007. <i>Unsquare Dance</i>. http://www.visgraf.impa.br/unsquare-dance.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Choreographisms Alice Bodanzky* Julio Lucio Ilana Paterman Analivia Cordeiro§ Silvia Steinberg¶ Luiz 
Velhol ESDI IMPA IMPA ESDI IMPA  (a) (b) (c) (d) Figure 1: Stages of a choreographism: (a) Markers on 
dancer; (b) Capturing body trajectory; (c) Graphics interpretation of Motion Path; and (d) Visual elements 
integrated in dance performance . 1 The Setting ... Choreographisms is a multidisciplinary project that 
aims to com­bine techniques from Computer Graphics, Vision and Animationwith applications in Contemporary 
Dance, Stage Design and Art. Inrecentyears,theuseofmultimediaresources, suchasliveprojec­tion and dynamic 
lighting, has been increasingly exploited in musicshows, dance presentations and art performances. However, 
up tonow, these powerful visual sources have been applied mostly as anadditional element to the set background, 
thus transforming it intoa visually dynamic stage. Our goal in this project is to push forward the state-of-the-art 
bycalling into the scene real-time interaction and vision techniques.In this way, by integrating motion 
capture with procedural graph­ics and live displays, we expand the creative possibilities enablinggraphicalelementstobeused,notas 
amerepassiveset element, butprimarily as an active one a virtual agent capable of interactingwith other 
actors in the scene under the guidance of the director­choreographer. 2 X-Motion: The System The above 
context frames the proposed project. For that, we de­veloped the X Motion system which enables the design 
of a virtualdynamic stage that is directly linked to the dance. The system usesmovements of dancers as 
an input for the generation of graphismsthat are projected on the set in real-time. In this manner, the 
chore­ographer directs not only the dancers movements, but also graphi­cal elements that areprojected 
into theset, whichbecomesdynamicand interactive. Thus, these graphisms act as virtual dancers, es­tablishing 
an interaction and dialog with real dancers mediated bythe choreographer. They can be modi.ed by interactions 
with thedancers, instigating changes to the dancers movements as well. *e-mail:alicebodanzky@gmail.com 
e-mail:juliolucio@gmail.com e-mail:ilanapaterman@gmail.com §e-mail:analivia300@gmail.com ¶e-mail:s.steinberg@terra.com.br 
Ie-mail:lvelho@impa.br  Authoring is based on graphical interpretation of movement, thatestablishes 
an association of motion-paths with visual forms. Thesystem captures the motion of markers on the dancers 
body to gen­erate trajectories (see Figures 1-a and 1-b). It computes position,velocity, curvature, and 
tangents of these motion-paths that are rep­resented as curves parametrized in time. In this way, trajectories 
oftracked points become the system s central mathematical abstrac­tion. They embody the essence of spatial-temporal 
aspects of themovement. Expressive visuals result from applying syntactic andsemantic graphical rules 
to motion paths translating them into ac­tive shapes (see Figures 1-c and 1-d). In sum, the system correlates 
not necessarily in the literal sense basic spatial structures to the elementary components of dance. 
The system architecture is centered on a .nite state machine where a state is de.ned by a set of formal 
attributes that determines theappearance (brush type, granularity, interpolation) and the behavior(persistence, 
synchronicity, projection) of the generated graphisms.Real-time interaction sets the machine in motion, 
scheduling pro­cedural evolutions: i.e., changes through time, and possibilities ofalterations in states 
with transitions between them. When de.ning a state, the choreographer is in fact programmingthe way 
the dancer can interact with the graphisms. The dancer,on the other hand, also controls the system through 
state changes.Triggering of a new state may be linked to variables such as motionpath parameters, clock 
time, procedural animation objects (motors,oscillators, etc.), or even randomness. 3 The Experiment: 
Unsquare Dance An experiment to evaluate the system s potential was conductedwith a renowned choreographer 
and dancer Analivia Cordeiro. Sheis a pioneer on dance-technology in Brazil and also a specialist onthe 
Laban method. During the experiment, she made use of thesystem to develop an artistic work called Unsquare 
Dance. Some of the results of her performance can be seen in Figure 1, or on thewebsite we created for 
the project [Cordeiro and Velho 2007]. References CORDEIRO,A., AND VELHO, L. 2007. Unsquare Dance. http://www.visgraf.impa.br/unsquare-dance. 
Copyright is held by the author / owner(s). SIGGRAPH 2008, Los Angeles, California, August 11 15, 2008. 
ISBN 978-1-60558-466-9/08/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	</section>
	<section>
		<section_id>1401058</section_id>
		<sort_key>260</sort_key>
		<section_seq_no>7</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[To trace or not to trace]]></section_title>
		<section_page_from>7</section_page_from>
	<article_rec>
		<article_id>1401059</article_id>
		<sort_key>270</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>20</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Compact, fast and robust grids for ray tracing]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1401032.1401059</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401059</url>
		<abstract>
			<par><![CDATA[<p>Ray tracing is becoming more and more the method of choice for both offline global illumination simulations as well as interactive visualizations. Because intersecting a ray with all objects in a scene is usually very expensive, almost all ray tracers rely on acceleration structures, trading preprocessing time and memory for faster ray-object intersections.</p> <p>The uniform grid was one of the first proposed acceleration structures. Over time, several other acceleration structures, such as bounding volume hierarchies and kd-trees, have been introduced. For static scenes, kd-trees are by many considered the best acceleration structure. Uniform grids usually perform worse than kd-trees, mainly because they are not adaptive. For dynamic scenes however, there is no consensus. The acceleration structure has to be rebuilt every frame, and rather than minimizing render time, the time to image, the sum of the build time and the render time, has to be minimized. Building a grid can be done in linear time, while other popular acceleration structures require super linear time. For dynamic scenes, a shorter build time can compensate for a longer render time. Therefore, a grid can result in a shorter time to image than other acceleration structures that are usually considered superior.</p> <p>Algorithms are typically CPU-bound or memory-bound. The execution time of an algorithm that is CPU-bound mainly depends on the speed of the CPU, while the execution time of an algorithm that is memory-bound mainly depends on the access speed of the memory. Memory-bound algorithms can be made significantly faster just by reducing the memory footprint of the data they work on. Building a grid is memory-bound, while rendering is CPU-bound. Therefore, reducing the memory footprint of a grid can results in shorter build times.</p> <p>Uniform grids were used in one of the first systems for interactive ray tracing. Recent work on grids for ray tracing concentrated on fast traversal, parallelizing the build process, and choosing the grid size. In this work, we present two efficient methods for representing and building a grid.</p> <p>The <i>compact grid</i> method consist of a static data structure for representing a grid with minimal memory requirements, more specifically exactly one index per grid cell and exactly one index per object reference, and an algorithm for building that data structure in linear time, that does not require additional memory.</p> <p>The <i>hashed grid</i> method consists of a static data structure for representing a grid that reduces memory requirements even further, by using perfect hashing based on row displacement compression, and a fast algorithm for building that data structure.</p> <p>Figure 1 shows several results. For example, the time to image and memory usage for the 1.89 GB <i>David</i> model is respectively 10.21 s and 379.94 MB using the hashed grid method. We show that the compact grid method and the hashed grid methods are more efficient in both time and space than traditional methods based on linked lists and dynamic arrays. We also investigate parallell grid building and rendering, we compare with other acceleration structures using the recent <i>bwfirt</i> benchmark, and we present a more robust grid traversal algorithm. We show that, for applications where time to image or memory usage is important, such as interactive ray tracing and rendering large models, the grid acceleration structure is an attractive alternative.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Raytracing</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.2</cat_node>
				<descriptor>Distributed/network graphics</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Radiosity</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372.10010376</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering->Reflectance modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010520.10010521.10010542.10011714</concept_id>
				<concept_desc>CCS->Computer systems organization->Architectures->Other architectures->Special purpose systems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372.10010374</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering->Ray tracing</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1098415</person_id>
				<author_profile_id><![CDATA[81300379401]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Ares]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lagae]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Katholieke Universiteit Leuven]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098416</person_id>
				<author_profile_id><![CDATA[81100172909]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Philip]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Dutr&#233;]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Katholieke Universiteit Leuven]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>2383490</ref_obj_id>
				<ref_obj_pid>2383465</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Lagae, A., and Dutr&#233;, P. 2008. Compact, fast and robust grids for ray tracing. <i>Computer Graphics Forum (Proceedings of the 19th Eurographics Symposium on Rendering) 27</i>, 8.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Compact, Fast and Robust Grids for Ray Tracing *  AresLagae&#38;PhilipDutr´e Department ofComputerScience 
KatholiekeUniversiteitLeuven  Figure 1: Build time,render time, time to image,and memory usage for thecompactgridmethod 
andhashedgridmethodpresented in this work, for various scenes. Images were rendered using simple shading, 
at a resolution of 1024×1024. Timings were obtained on a 3 GHz IntelXeonX5365CPU. Abstract Ray tracing 
is becoming more and more the method of choice for both of.ine global illumination simulations as well 
as interactive visualizations. Because intersecting a ray with all objects in a scene is usually very 
expensive, almost all ray tracers rely on acceleration structures, trading preprocessing time and memory 
for faster ray­object intersections. The uniform grid was one of the .rst proposed acceleration struc­tures. 
Over time, several other acceleration structures, such as bounding volume hierarchies and kd-trees, have 
been introduced. Forstaticscenes,kd-treesareby many considered thebest acceler­ationstructure.Uniformgridsusuallyperformworse 
thankd-trees, mainly because they are not adaptive. For dynamic scenes how­ever, there is no consensus. 
The acceleration structure has to be rebuilt every frame, and rather than minimizing render time, the 
time to image, the sum of the build time and the render time, has to be minimized. Building a grid can 
be done in linear time, while otherpopularaccelerationstructuresrequiresuper linear time. For dynamic 
scenes, a shorter build time can compensate for a longer render time. Therefore, a grid can result in 
a shorter time to im­age than other acceleration structures that are usually considered superior. Algorithms 
are typically CPU-bound or memory-bound. The exe­cution time of an algorithm that is CPU-bound mainly 
depends on thespeed oftheCPU,whiletheexecution timeof analgorithmthat ismemory-bound mainlydependson 
theaccessspeed of themem­ory. Memory-bound algorithms can be made signi.cantly faster just by reducing 
the memory footprint of the data they work on. Building a grid is memory-bound, while rendering is CPU-bound. 
Therefore, reducing the memory footprint of a grid can results in shorterbuild times. * e-mail: {ares.lagae,philip.dutre}@cs.kuleuven.be 
Uniform grids were used in one of the .rst systems for interactive ray tracing. Recent work on grids 
for ray tracing concentrated on fast traversal,parallelizingthebuildprocess,and choosingthegrid size.In 
thiswork,wepresent twoef.cient methodsforrepresenting andbuilding agrid. The compact grid method consist 
of a static data structure for rep­resenting agrid with minimal memory requirements, more speci.­callyexactlyoneindexpergridcelland 
exactlyone indexperobject reference,and analgorithmforbuildingthatdatastructurein linear time, thatdoesnot 
requireadditional memory. The hashed grid method consists of a static data structure for rep­resenting 
agrid that reducesmemory requirementsevenfurther,by usingperfecthashingbased on rowdisplacement compression, 
and afast algorithmforbuilding thatdatastructure. Figure1 showsseveral results.Forexample,the timetoimageand 
memory usagefor the1.89GB David model isrespectively10.21 s and 379.94 MB using the hashed grid method. 
We show that the compact grid method and the hashed grid methods are more ef­.cient in both time and 
space than traditional methods based on linked lists and dynamic arrays. We also investigate parallell 
grid building and rendering, we compare with other acceleration struc­tures using the recent bw.rt benchmark, 
and wepresent a more ro­bustgrid traversal algorithm.Weshowthat,forapplicationswhere time to image or 
memory usage is important, such as interactive ray tracing and rendering large models, the grid acceleration 
structure is an attractive alternative. References LAGAE, A., AND DUTRE´, P. 2008. Compact,fast and 
robustgrids for ray tracing. ComputerGraphicsForum(Proceedingsof the 19thEurographicsSymposium onRendering)27,8. 
Copyright is held by the author / owner(s). SIGGRAPH 2008, Los Angeles, California, August 11 15, 2008. 
ISBN 978-1-60558-466-9/08/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1401060</article_id>
		<sort_key>280</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>21</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[An interactive system for realistic rendering of large-scale terrains]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1401032.1401060</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401060</url>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>F.2.2</cat_node>
				<descriptor>Geometrical problems and computations</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.6</cat_node>
				<descriptor>Interaction techniques</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.8</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010507</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Graphics recognition and interpretation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10010391</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Graphics input devices</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061.10010063</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures->Computational geometry</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061.10010063</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures->Computational geometry</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1098417</person_id>
				<author_profile_id><![CDATA[81100329647]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Yoshinori]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Dobashi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Hokkaido University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098418</person_id>
				<author_profile_id><![CDATA[81100602092]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Tsuyoshi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yamamoto]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Hokkaido University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098419</person_id>
				<author_profile_id><![CDATA[81421597781]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Tomoyuki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nishtia]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>566612</ref_obj_id>
				<ref_obj_pid>566654</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Sloan, P.-P., Kautz, J., and Snyder, J. 2002. Precomputed radiance transfer for real-time rendering in dynamic, low-frequency lighting environments. ACM Trans. on Graphics, 21, 3, 527--536.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Max, N. L. 1988, Horizon mapping: shadows for bump-mapped surfaces, <i>The Visual Computer, 4</i>, 2, 109--117.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>569060</ref_obj_id>
				<ref_obj_pid>569046</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Dobashi, Y., Yamamoto, T., and Nishita, T. 2002. Interactive rendering of atmospheric effects using graphics hardware. <i>In Proceedings of Graphics Hardware 2002</i>, 99--108.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 An Interactive System for Realistic Rendering of Large-scale Terrains Yoshinori Dobashi* Tsuyoshi Yamamoto* 
Tomoyuki Nishtia** 1Hokkaido University 2The University of Tokyo 1 Introduction This paper presents 
a method to improve the realism of the synthetic images of the large-scale terrains. We assume that the 
terrains are represented by a height field. In order to create realistic images, both the direct sunlight 
and skylight have to be taken into account. It is also desirable to allow the user to change the sunlight 
direction at run time. These purposes could be achieved by using precomputation-based approaches such 
as the one developed by Sloan et al [2002]. However, the storage and computational costs for the precomputated 
data are generally expensive in these approaches. This becomes a problem for the large-scale terrains. 
To address the problem, we develop a new method that is suitable for the terrains. Our method employs 
a method called horizon mapping [Max 1988]. Although this method can compute the shadows cast by the 
direct sunlight efficiently, the skylight is not taken into account. Our new method can efficiently calculate 
the intensities of the terrains illuminated by skylight by exploiting the properties of the horizon map. 
 2 Efficient Intensity Calculation due to Skylight The horizon map fh(P, .k) stores an elevation angle 
fh to the horizon in a sample direction of azimuth angle .k at each point P in a height field (see Fig. 
1). Max [1988] used the horizon map to determine whether the point is visible from the light source. 
However, as shown in Fig. 1, the horizon map also provides us the information about the visible range 
of the sky. The part of the sky is visible for the range of elevation angles [fh, p/2]. Since the horizon 
map is generated for a set of discrete sampling directions .k, the skylight illumination at P is calculated 
by: m Isky (P) =.P .F(.k -.sun ,fh (P,.k ),.n -.sun ,fn ,fsun ) , (1) k=1 where m is the number of the 
sampling directions of the azimuth angles for the horizon map, .P is the diffuse reflectance at P, (.n, 
fn) is the direction of the normal vector at P, and (.sun, fsun) is the sunlight direction. We call F 
a skylight illumination function that is given by: .. .+ p/2 2 ** *** F(.,f,.n ,fn ,fsun ) = .. Lsky 
(.,f,fsun )cosacosfdfd., (2) .. .- f 2 where Lsky is the intensity distribution of the sky, a is the 
angle between the directions (.*, f*) and (.n, fn). .. is the sampling interval of the azimuth angles 
for the horizon map. F indicates the illumination due to the shaded region of the sky shown in Fig. 2. 
Our method precomputes the skylight illumination function F. To reduce the size of the precomputed data, 
we make use of the following properties. First, the elevation angle fn of the normal vector is always 
greater than zero for any points in the height field. Second, we assume Lsky is zero when f< 0. Therefore, 
it is sufficient to evaluate F for 0 < f, fn < p/2. Furthermore, since the intensity distribution of 
the sky is symmetrical with respect to the sunlight direction, F is precomputed by assuming .sun = 0. 
In calculating Eq. (1), the sunlight direction is correctly handled by *e-mail: {doba, yamamoto}@nis-ei.eng.hokudai.ac.jp 
**e-mail: nis@is.s.u-tokyo.ac.jp y x .. Figure 2: Definition of skylight Figure1: Horizon mapping. illumination 
function F. referring to the relative direction from .sun. Since F is independent of the geometry of 
the height field, it is sufficient to carry out the precomputation only once. 3 Results and Conclusion 
Fig. 3 shows an example image generated by our system. We used the height field published by NASA1. The 
size of the height field is 6000x4800. To increase the rendering speed, a hierarchical data structure 
with seven levels was constructed. The horizon map was computed for the third level only and was linearly 
interpolated for the use of the height fields in other levels. The size of the horizon map was only 14 
MB. According to the previous paper [Max 1988], we set the number of sampling directions m = 8. The size 
of the precomputed data for F was 96 [MB]. We used a desktop PC with Pentium Core 2 Quad Extreme Q6800 
and an nVidia GeForce 8800 Ultra. However, the intensity calculations of the terrains were carried out 
on the CPU in our current implementation. The atmospheric scattering effects are calculated by using 
the method proposed by Dobashi et al [2002]. The frame rate of the rendering ranged from 1 to 5 frames 
per second. We plan to implement the method on the GPU for further acceleration.  Figure3: An example 
image generated by our method. References SLOAN, P.-P., KAUTZ, J., AND SNYDER, J. 2002. Precomputed 
radiance transfer for real-time rendering in dynamic, low-frequency lighting environments. ACM Trans. 
on Graphics, 21, 3, 527-536. MAX, N. L. 1988, Horizon mapping: shadows for bump-mapped surfaces, The 
Visual Computer, 4, 2, 109-117. DOBASHI, Y., YAMAMOTO, T., AND NISHITA, T. 2002. Interactive rendering 
of atmospheric effects using graphics hardware. In Proceedings of Graphics Hardware 2002, 99-108. 1 http://www2.jpl.nasa.gov/srtm/ 
Copyright is held by the author / owner(s). SIGGRAPH 2008, Los Angeles, California, August 11 15, 2008. 
ISBN 978-1-60558-466-9/08/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1401061</article_id>
		<sort_key>290</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>22</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Image-space horizon-based ambient occlusion]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1401032.1401061</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401061</url>
		<categories>
			<primary_category>
				<cat_node>I.4.1</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.2.10</cat_node>
				<descriptor>Intensity, color, photometry, and thresholding</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010225</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision tasks</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1098420</person_id>
				<author_profile_id><![CDATA[81319488052]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Louis]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bavoil]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[NVIDIA Corporation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098421</person_id>
				<author_profile_id><![CDATA[81100048438]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Miguel]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sainz]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[NVIDIA Corporation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098422</person_id>
				<author_profile_id><![CDATA[81421595945]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Rouslan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Dimitrov]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[NVIDIA Corporation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>23954</ref_obj_id>
				<ref_obj_pid>23944</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Max, N. L. 1986. Horizon mapping: Shadows for bump-mapped surfaces. In <i>Proceedings of Computer Graphics Tokyo '86 on Advanced Computer Graphics</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1281671</ref_obj_id>
				<ref_obj_pid>1281500</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Mittring, M. 2007. Finding next gen: Cryengine 2. In <i>SIGGRAPH '07: ACM SIGGRAPH 2007 courses</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1230113</ref_obj_id>
				<ref_obj_pid>1230100</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Shanmugam, P., and Arikan, O. 2007. Hardware accelerated ambient occlusion techniques on gpus. In <i>I3D '07: Proceedings of the 2007 symposium on Interactive 3D graphics and games</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Image-Space Horizon-Based Ambient Occlusion Louis Bavoil Miguel Sainz Rouslan Dimitrov NVIDIA Corporation 
 Figure 1: Ambient occlusion without any shading. (left) 67 MPixels/s with our horizon-based algorithm. 
(right) 15 MPixels/s with ray marching and4rays per direction. Images rendered with Nd = 8andNs = 8on 
GeForce 8800 GTX Ultra. 1 Introduction Ambient occlusion is a lighting model that approximates the amount 
of light reaching a point on a diffuse surface based on its directly visible occluders. It gives perceptual 
clues of curva­ture and spatial proximity. Like [Mittring 2007] and [Shanmugam and Arikan 2007], we propose 
a real-time ambient occlusion com­putation as a postprocessing pass mainly based on a depth image from 
the eye s point of view. This approach requires no scene­dependent precomputations and is applicable 
to dynamic scenes. Our proposed method does not have the overocclusion issue from [Shanmugam and Arikan 
2007] and samples inside the radius of in.uence, unlike [Mittring 2007]. We use the following form of 
the ambient occlusion illuminationA atagiven surface point P 1 A = 1- V (.V)W (.V)d. (1) 2pO where V 
is the visibility function over the normal-oriented unit hemisphere O, which returns 1 if a ray starting 
from P in direc­tion V. intersects an occluder and 0 otherwise, and W is a linear attenuation function. 
 2 Horizon-Based Ambient Occlusion We use a spherical coordinate system with the zenith axis aligned 
in the view direction V , azimuth angle . and elevation angle a (see Figure 2a). Similarly to horizon 
mapping [Max 1986], we split the unit sphere by a horizon line de.ned by the signed horizon angle h(. 
) (see Figure 2b). Assuming that the neighborhood of P is a continuous height.eld, rays that would normally 
be traced below the horizon are known to intersect an occluder so the intersection test for these rays 
can be omitted. Under the continuous height.eld assumption, Equation1canbe rewritten as: p h(.) 1 A = 
1- W (.V)cos(a)dad. (2)2p.=-pa=t(.) We use the linear attenuation functionW (. )= max(0,1- r(.)/R) where 
r(. ) is the distance between P and the horizon point in di­rection .Vand R is the radius of in.uence. 
In this case, p 1 A = 1- (sin(h(. )) - sin(t(. )))W (. )d. (3) 2p.=-p 3 Image-Space Integration Our 
algorithm takes as input per-pixel linear depths and eye-space normals. For every pixel, we compute its 
eye-space position P and we integrate Equation3usinga Monte Carlo approachbysampling (a) (b) (c) Figure 
2: (a) The azimuthal angle . around the view vector V . The tangent angle t(. ) is the signed elevation 
angle of the surface tan­gent vector. (b) The horizon angle h(. ) is the maximum elevation a >= t(.) 
for which .Vis occluded for all a < h(. ) (c) Example ofsample locationsfor4directionsandastepsizeof2texels. 
directions in image space and stepping on the height.eld stored in the depth image. We pick Nd directions 
. in image space around the currentpixel which correspondto directions aroundtheZaxisin eye space(V on 
Figure 2a).For each angle ., we compute the hori­zon angle h(. ) by sampling the depth image along a 
line segment in image space. The eye-space radius of in.uence R is projected on the image plane and is 
subdivided into Ns steps of equal lengths. For .nding the horizon angle in direction. , we start by computing 
the tangent angle t(. ) and intersect a view ray with the tangent plane de.ned by P and the surface normal 
Vn.We then stepin the depth image in direction . and compute D = Si - P, where Si is the reconstructed 
eye-space position of a given sample Si. Based on the elevation angles a(Si)= atan(-D.z/||D.xy||), the 
horizon angle h(.) is max(t(. ),a(Si),i = 1..Ns) where Ns is the number of steps per direction. We ignore 
samples for which ||Si - P|| > R. Because D.z mustbetheexactdepth associatedwiththeoffset D.xy, we make 
sure to always sample at texel centers. To do so, we snap the texture coordinates of the samples along 
each direction to the nearest texel centers (see Figure 2c). To trade banding artifacts for noise, we 
randomly jitter the step size per pixel and randomly rotate the Nd uniform directions per pixel. Although 
using a single depth layer typically produces plausible results(see Figure1),it couldbeextendedbyusing 
multiple layers such as the front and backfaces. References MAX,N.L. 1986. Horizon mapping: Shadows 
forbump-mapped surfaces. In Pro­ceedings of Computer Graphics Tokyo 86 on Advanced Computer Graphics. 
MITTRING, M. 2007. Finding next gen: Cryengine 2. In SIGGRAPH 07: ACM SIGGRAPH 2007 courses. SHANMUGAM,P., 
AND ARIKAN,O. 2007. Hardware accelerated ambient occlusion techniques on gpus. In I3D 07: Proceedings 
of the 2007 symposium on Interactive 3D graphics and games. Copyright is held by the author / owner(s). 
SIGGRAPH 2008, Los Angeles, California, August 11 15, 2008. ISBN 978-1-60558-466-9/08/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1401062</article_id>
		<sort_key>300</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>23</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[Pismo]]></title>
		<subtitle><![CDATA[parallax-interpolated shadow map occlusion]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1401032.1401062</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401062</url>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>H.5.1</cat_node>
				<descriptor>Animations</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.8</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341.10010349.10011310</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Simulation types and techniques->Simulation by animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010507</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Graphics recognition and interpretation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061.10010063</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures->Computational geometry</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1098423</person_id>
				<author_profile_id><![CDATA[81100390217]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Ivan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Neulander]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Rhythm & Hues Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>281030</ref_obj_id>
				<ref_obj_pid>280811</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Chen, S. E., and Williams, L. 1998. View interpolation for image synthesis. 381--390.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>566616</ref_obj_id>
				<ref_obj_pid>566570</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Stamminger, M., and Drettakis, G. 2002. Perspective shadow maps. In <i>SIGGRAPH '02</i>, ACM, 557--562.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Pismo: Parallax-Interpolated Shadow Map Occlusion Ivan Neulander Rhythm&#38;Hues Studios Four discrete 
shadows from the corners of an area light, rendered at 1600x1200 using 4 depth maps with PCF antialiasing: 
65 sec on a 2133MHz Athlon. Soft shadow rendered using the same 4 depth maps with Pismo: 323 sec, 128 
shadow rays/pixel Ground truth soft shadow rendered using a ray tracer: 1732 sec, 128 shadow rays/pixel 
Introduction and Motivation Soft shadows castby area lights areessentialto realistic image synthe­sis. 
They are commonly rendered using a ray tracer to perform multi­ple occlusion tests between a given shading 
point and a set of sample positions on the light. While this method is reliably accurate, it is often 
expensive in terms of time and memory. A potentially less expensive approach is to approximate the soft 
shadow with a discrete set of hard shadows, each evaluated using a depth map. An example of this is shown 
in the top image above. In practice this method requires an excessively high number of depth maps to 
produce an adequately smooth penumbra. Percentage-closer .ltering(PCF)canbeusedto smooththe penumbraby 
uniformlyblur­ring each hard shadow, but this also blurs detail that should remain crisp, such as contact 
shadows. Pismo is a technique developed at Rhythm &#38; Hues for rendering .lm-quality soft shadows, 
as shown in the middle image above, us­ing a sparse set of depth maps created from points on an area 
light. We can test anyshadow ray quickly, using only3depth map lookups plus a low amortized overhead. 
Moreover, Pismo is designed to take full advantage of all available depth maps, smoothly converging to 
an accurate solution as their number is increased. Representation We represent the area light as a triangle 
mesh, with a depth map ren­dered from eachvertex. The depth maps are rendered using perspective cameras, 
all .xated on a common point that is central to the shadow­casting geometry. Our depth maps are augmented 
slightly from their traditional de.­nition: theystore multiple ordered depth values per pixel, one for 
each distinct surface covering the pixel, with some proximity-based prun­ing. The multiple depths are 
needed to determine the distance from a shadowed point to its nearest occluder, which is useful for short-range 
shadowing. More traditional, single-channel depth maps (contaning maximum depths, however) can also be 
used with Pismo, but the re­sulting soft shadows are accurate only outside the casting geometry s convex 
hull, which precludes good self-shadowing. Copyright is held by the author / owner(s). SIGGRAPH 2008, 
Los Angeles, California, August 11 15, 2008. ISBN 978-1-60558-466-9/08/0008  Implementation Overview 
For any shading point P, we compute fractional area light occlusion by sampling multiple shadow rays. 
Each shadow ray Ri extends from P to a unique point Li on one of the area light triangles. The vertices 
of this triangle contain the3 corner depth maps we will interpolate to estimate the occlusion of Ri.We 
use the barycentric coordinates of Li with respect to its triangle as linear interpolation weights. 
We begin by estimating the distance alongRi from P to the nearest likely occluder.For this we usea barycentric 
blendof nearest occluder depths for P from the3corner depth maps. Although this computation requires 
depth mapevaluations, theyoccur only once per shading point per depth map and are subsequently cached, 
so their cost is amortized over all shadow rays. The depth maps used at this stage havebeen bled out 
to .ll empty pixels with nearest nonempty neighbors a simple, one-time preprocess. Next, we displace 
Li toward P (along -Ri)by the above occlusion distance to yield a putative occluder point Oi. Finally, 
we estimate the occlusion along Ri by barycentrically blendingthe occlusion of P from each corner depth 
map. But instead of transforming P into each depth map s screen space, we transform Oi. This alternate 
projection amountstoa parallax shiftof the putative occluding surface from each depth map s original 
vantage point to that of Li. Summary and Conclusions Pismo samples each shadow ray Ri by making two 
estimations of oc­cluder depth along it. The .rst is a rough approximation based on queryingthe3 vertexdepth 
mapsat P (a calculation that is amortized over manyshadow rays). The second is a re.ned approximation 
that uses the .rst to account for the parallax shift between each depth map and Li. The accuracy of both 
estimations, and therefore, the quality of Pismo s shadows, depends on the density of depth maps per 
solid an­gle of area light and on the occluding geometry s surface complexity. Because both estimations 
are barycentrically blended, shadowrays hit­ting an area light triangle s vertices are always evaluated 
correctly and other shadow rays are evaluated with increasing accuracy as the size of the triangle diminishes. 
Unlike methods that simulate soft shadows through texture-space PCF, ours is immune to artifacts from 
distorted depth maps created via [Stamminger and Drettakis 2002]. Asomewhat similar approach to ours 
is the image morphing method presented in [Chen andWilliams 1998], which canbuild manyhigh­quality interpolated 
depth maps from a sparse initial set. However, hundreds of such interpolated maps may need to be created, 
stored, and sampledin orderto capturea smooth shadow.Ahybrid approach that uses morphing to moderately 
increase the number of depth maps available to Pismo may leverage the best features of both techniques. 
 References CHEN,S.E., AND WILLIAMS,L. 1998.Viewinterpolation for image synthesis. 381 390. STAMMINGER,M., 
AND DRETTAKIS,G. 2002. Perspective shadow maps. In SIGGRAPH 02,ACM, 557 562.  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	</section>
	<section>
		<section_id>1401063</section_id>
		<sort_key>310</sort_key>
		<section_seq_no>8</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Many things]]></section_title>
		<section_page_from>8</section_page_from>
	<article_rec>
		<article_id>1401064</article_id>
		<sort_key>320</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>24</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Shading the many]]></title>
		<subtitle><![CDATA[solutions for shading crowd characters on <i>WALL&#183;E</i>]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1401032.1401064</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401064</url>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>H.5.1</cat_node>
				<descriptor>Animations</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>H.5.1</cat_node>
				<descriptor>Artificial, augmented, and virtual realities</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341.10010349.10011310</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Simulation types and techniques->Simulation by animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061.10010063</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures->Computational geometry</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010392</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Mixed / augmented reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010866</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Virtual reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010866</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Virtual reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1098424</person_id>
				<author_profile_id><![CDATA[81421601750]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Maxwell]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Planck]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Pixar Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098425</person_id>
				<author_profile_id><![CDATA[81100233914]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Stephan]]></first_name>
				<middle_name><![CDATA[Vladimir]]></middle_name>
				<last_name><![CDATA[Bugaj]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Pixar Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Shading The Many: Solutions For Shading Crowd Characters on WALL E Maxwell Planck e-mail: mplanck@pixar.com 
Stephan Vladimir Bugaj e-mail: sbugaj@pixar.com Pixar Animation Studios Pixar Animation Studios The challenge 
with shading crowds is creating a population that is believably complex and detailed in as little time 
as possible. Dur­ing the making of Pixar s WALL E, we were tasked with .lling a city-sized spaceship 
with robotic and human characters. Art di­rection called for visual uniformity, showing conformism had 
set in amongst characters marooned in space for 400 years, but with enough visual difference to create 
a believably rich population. On top of the visual requirements, we needed to shade the 200+ char­acters 
quickly, ensure render ef.ciency, and work with a reduced set of our shading tools. To do this, we developed 
a .exible global shading palette, an automatic approach for driving shading features based on curvature 
and gprim proximity, and a process for baking customized surface details into a more ef.cient data set. 
 Figure 1: Image from Pixar s WALL E illustrating the variety of background robot characters. c @Disney 
/ Pixar. All rights reserved. 1 Designing A Global Palette In WALL E, the Axiom cruise liner is staffed 
with hundreds of ser­vice robots divided into utility, economy and luxury classes. Each bot s various 
metal, plastic, rubber, and glass components have .n­ish qualities representing their social class. At 
Pixar, each shad­ing artist usually develops their own SLIM network, providing un­restricted creative 
freedom. In order to minimize complexity and enforce consistency, we created a global palette of surfaces 
which our artists could reuse, achieving a custom look by adjusting preset controls. For example, our 
metal template provides variable con­trols for how the metal is scuffed, rusted, chipped, bronzed, oxi­dized, 
faded, greased, dirtied, and painted. Our goal was to provide enough .exibility so that all classes of 
a material, plastics for ex­ample, could be created using a single shader. As shading artists developed 
instances of the materials we liked, we augmented the global palette with these. Eventually, shading 
a robot was as easy as picking from our off-the-shelf materials and tweaking a few vari­ables. 2 Using 
Signals To Drive Details Painting detail on all of our characters also proved prohibitively expensive. 
Most of the 200+ characters were built from over 100 unique primitives, many of which were referenced 
from dozens of external .les. We devised a way to automatically generate two types of presence maps based 
on geometric features of the bots. The .rst, Figure 2: (a) Blurred Curvature (b) Proximity Occlusion 
(c) Using both to drive surface details c @Disney / Pixar. All rights reserved. blurred curvature, was 
created by baking the absolute value of the surface s second derivative into a pointcloud. Processing 
the point­cloud into a uniform voxel space and blurring using a .xed kernel allowed us to blur a signal 
which we normally can t in our scan­line renderer, Renderman. Blurred curvature (Fig. 2a) is a great 
signal for driving features such as scratching, rust, and chipping on metal, saturation on plastics, 
and thickness in glass and plastic clearcoat. The second, proximity occlusion, was generated using Renderman 
s ray traced occlusion with the bot in a default pose, and then baked into a UV texture. Proximity occlusion 
(Fig. 2b) is a great signal for representing features like wear between mov­ing parts, less fading in 
plastic, and the presence of grease and dirt. Combined with varying weights of the dot product of the 
surface normal against an upward facing vector, as well as chaotic fractals, these two presence maps 
would give us a believably rich set of de­tails (Fig. 2c) unique to the geometry within 1 hour of beginning 
shading. 3 Baking Painted Features and Graphics Even with automatic feature discovery, there were still 
cases where we needed to paint custom presence maps and graphics. Pixar s es­tablished work.ow uses projection 
paint, per vertex paint, and UV manipulation in order to de.ne feature location and apply graphics. However, 
our high-end paint tools add complexity to our characters which made rendering time and memory requirements 
impractical. We designed a way to allow shading artists to use our familiar tools while developing the 
look, but then to bake the result into a sim­pli.ed representation. Once a look was approved, the artist 
would render in a baking mode, which would write out all of the painted signals into a set of pointclouds. 
These pointclouds would then be parsed into a set of UV map textures. We were thus able to avoid the 
render-time expense of our paint tools and related shader code, but still achieve a suf.ciently re.ned 
.nal look. 4 Lessons Learned At .rst, we were concerned that these approaches to shading crowds would 
limit our shading artist s creative control and the level of detail complexity. We discovered that by 
maintaining less code and keeping the work.ow simple, consistent, and ef.cient this gave us enough time 
to address iteration, hit the production goals, and in the end populate the Axiom. Copyright is held 
by the author / owner(s). SIGGRAPH 2008, Los Angeles, California, August 11 15, 2008. ISBN 978-1-60558-466-9/08/0008 
  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1401065</article_id>
		<sort_key>330</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>25</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Beyond procedurally modeled foliage in <i>Madagascar: Escape 2 Africa</i>]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1401032.1401065</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401065</url>
		<abstract>
			<par><![CDATA[<p>Developing vegetation to capture a very distinct and stylized art direction is a great challenge in computer cinematography. Completely procedural approaches constrain artists to model foliage by connecting nodes, modifying numbers, or writing code. On the other hand, manually placing thousands of branches is completely infeasible. Attempts at hybrid techniques have been inadequate, as they are still too biased towards procedural systems.</p> <p><i>Bonsai</i>, our tree modeling tool for <i>Madagascar</i> 2, yields dramatic improvement in stylistic control over previous methods. Modelers use familiar modeling tools to create and connect small branch parts to form more complex networks, which can be connected or dynamically grown into a large hierarchical system of branches. More importantly, this allows the user to put shape language into every branch. <i>Bonsai</i> facilitates duplicating and manipulating thousands of branch surfaces to help artists create trees efficiently.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>F.2.2</cat_node>
				<descriptor>Computations on discrete structures</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>F.2.2</cat_node>
				<descriptor>Geometrical problems and computations</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003752.10010061.10010063</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures->Computational geometry</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061.10010063</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures->Computational geometry</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1098426</person_id>
				<author_profile_id><![CDATA[81421595413]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Jeffrey]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Budsberg]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[DreamWorks Animation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098427</person_id>
				<author_profile_id><![CDATA[81100505245]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Scott]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Peterson]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[PDI/DreamWorks]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1401066</article_id>
		<sort_key>340</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>26</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Brain springs]]></title>
		<subtitle><![CDATA[fast physics for large crowds on <i>WALL&#183;E</i>]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1401032.1401066</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401066</url>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>F.2.2</cat_node>
				<descriptor>Geometrical problems and computations</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.6.1</cat_node>
				<descriptor>Systems theory</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.8</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341.10010346.10010347</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Simulation theory->Systems theory</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010507</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Graphics recognition and interpretation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061.10010063</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures->Computational geometry</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061.10010063</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures->Computational geometry</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1098428</person_id>
				<author_profile_id><![CDATA[81365593228]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Paul]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kanyuk]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Pixar Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098429</person_id>
				<author_profile_id><![CDATA[81540814656]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Chris]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lawrence]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Pixar Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1401067</article_id>
		<sort_key>350</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>27</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[A.I. cars for speed racer]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1401032.1401067</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401067</url>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>H.5.1</cat_node>
				<descriptor>Animations</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.6.1</cat_node>
				<descriptor>Systems theory</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.8</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010507</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Graphics recognition and interpretation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341.10010349.10011310</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Simulation types and techniques->Simulation by animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341.10010346.10010347</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Simulation theory->Systems theory</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1098430</person_id>
				<author_profile_id><![CDATA[81365596930]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Brad]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Herman]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Dreamworks SKG]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098431</person_id>
				<author_profile_id><![CDATA[81421599093]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[John]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gibson]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Digital Domain]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098432</person_id>
				<author_profile_id><![CDATA[81421595710]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Erik]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gamache]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Digital Domain]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 A.I. Cars for Speed Racer Brad Herman* John Gibson Erik Gamache Dreamworks SKG Digital Domain Digital 
Domain 1 Introduction The feature .lm Speed Racer presented many interesting chal­lenges for visual effects 
production. One which was how to ani­mate forty race cars at speeds above three hundred miles an hourfor 
several hundred shots on tracks that defy physics and most localzoning laws. Many ideas were proposed 
to solve this large prob­lem. The .rst attempts were done with video game and drivingsimulation engines. 
The drawbacks of those systems lead to build­ing a fuzzy logic based race car in Massive that could procedurallyanimate 
itself in the animation style approved for the show. 2 Testing and Design Our initial approach was to 
use rFactor, a high end commercial carsimulation engine. This same engine was used to control the motionbase 
on-set used for the cockpits. It performed very well at that task, allowing the actors to drive the motion 
base with realistic carmotion. A Maya pipeline was built to export the tracks to rFactorand we started 
driving. The highly accurate physics engine of thesimulator that worked so well on set was its undoing 
for productionwork. It became apparent that neither our driving skills nor the A.I.built into the engine 
was capable of dealing with the speeds andtypes of tracks featured in this .lm. These issues extended 
to otherdedicated car engines we looked at, a more .exible solution wasneeded. Figure 1: Cars falling 
off the Racetrack in rFactor. Massive, the award wining simulation software, is best known for A.I. crowds 
in feature .lms and commercials. It s less known for generating procedural animation, but the tool set 
is very well suitedto it. We designed a Massive agent around the concept of art di­rected car driving. 
It s default behavior was to drive like a regularcar and obey physics. * e-mail: brad.herman@dreamworks.com 
e-mail: jmgibson@d2.com e-mail: egamache@d2.com Figure 2: Information painted onto the track for the 
cars to read. A layer of rules on top of that instructed the car how and went tobend and break those 
rules. The race tracks of Speed Racer fea­ture several inverted loops, which caused our rFactor cars 
to fall offthe track while our Massive cars knew to stick to the track. The Massive cars could read data 
placed on the track and used that tohelp in their decision process. Information such as where to breakand 
accelerate, passing zones, tightness and direction of upcomingturns. Each car could also react to the 
cars around it. Variables for aggressiveness, passing, drifting, acceleration, breaking, and speedallowed 
each car to have the appearance of distinct driver person­alties and car capabilities. We often had very 
aggressive cars thatwould run others off the track causing them to crash. When crash­ing, the car would 
activate Massive s dynamics system and allowit to take over from the A.I. output. The cars were built 
with very.nite controls to allow us to tune the driving for shots. It was veryimportant that this animation 
blend in with that of our animationteam. It also allowed for artistic changes based on feedback fromreviews. 
 3 Conclusion We were able to match the stylistic driving look achieved by ouranimation team. Massive 
is able to drive a full grid of forty cars atnear realtime speed. The Massive cars were used to visualize 
somelarger driving shots in the .lm. The extremely high ef.ciency ofour animation and previz teams ended 
up negating the need to usethe simulated cars in production. As with most Massive agents the.nished A.I. 
is reusable, and may end up in another project. Copyright is held by the author / owner(s). SIGGRAPH 
2008, Los Angeles, California, August 11 15, 2008. ISBN 978-1-60558-466-9/08/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	</section>
	<section>
		<section_id>1401068</section_id>
		<sort_key>360</sort_key>
		<section_seq_no>9</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Measurement & textures]]></section_title>
		<section_page_from>9</section_page_from>
	<article_rec>
		<article_id>1401069</article_id>
		<sort_key>370</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>28</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Coded aperture projection]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1401032.1401069</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401069</url>
		<abstract>
			<par><![CDATA[<p>We integrate coded apertures into off-the-shelf projectors to increase their focal depth. The regional defocus of the projection on the surface is measured automatically. The projected images are then deconvolved with locally scaled aperture codes. This leads to significantly better results than deconvolving with Gaussians in cases where regular spherical apertures are used.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[coded apertures]]></kw>
			<kw><![CDATA[deconvolution]]></kw>
			<kw><![CDATA[defocus compensation]]></kw>
			<kw><![CDATA[inverse filtering]]></kw>
			<kw><![CDATA[projector-camera systems]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.4.1</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1098433</person_id>
				<author_profile_id><![CDATA[81421597701]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Max]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Grosse]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Bauhaus-University Weimar]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098434</person_id>
				<author_profile_id><![CDATA[81100622976]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Oliver]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bimber]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Bauhaus-University Weimar]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1137518</ref_obj_id>
				<ref_obj_pid>1137246</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Bimber, O., and Emmerling, A. 2006. Multifocal Projection: A Multiprojector Technique for Increasing Focal Depth. <i>IEEE Transactions on Visualization and Computer Graphics (TVCG) 12</i>, 4, 658--667.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1153682</ref_obj_id>
				<ref_obj_pid>1153171</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Brown, M. S., Song, P., and Cham, T.-J. 2006. Image PreConditioning for Out-of-Focus Projector Blur. In <i>Proc. of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</i>, vol. II, 1956--1963.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Coded Aperture Projection Max Grosse* , Oliver Bimber Bauhaus-University Weimar Figure 1: The power 
spectra of a Gaussian PSF (a) and of the PSF of a coded aperture (b): Fourier magnitudes that are too 
low are clipped (black), which causes ringing artifacts. Image projected in focus (c), and with the same 
optical defocus (approx. 2m distance to focal plane) in three different ways: with spherical aperture 
 untreated (b) and deconvolved with Gaussian PSF (e), with coded aperture and deconvolved with PSF of 
aperture code (f). The sub-images in c-f are photographs of the apertures and their captured PSFs. Abstract 
We integrate coded apertures into off-the-shelf projectors to increase their focal depth. The regional 
defocus of the projection on the surface is measured automatically. The projected images are then deconvolved 
with locally scaled aperture codes. This leads to sig­ni.cantly better results than deconvolving with 
Gaussians in cases where regular spherical apertures are used. Keywords: Projector-camera systems, defocus 
compensation, coded apertures, inverse .ltering, deconvolution. 1 Introduction and Motivation Coded aperture 
imaging has been presented recently in the context of computational photography. In contrast to conventional 
apertures, coded apertures enable for instance re-focussing captured images after they have been taken. 
Video projectors apply simple spherical apertures and suffer from a relatively short focal depth. This 
is problematic when images are projected onto non-planar surfaces, such as in case of spherical or cylindrical 
projection displays. They might be regionally defocussed if the aperture of the projector is too large. 
Using smaller apertures for increasing the focal depth, how­ever, decreases the light throughput. Several 
approaches exist that increase projected focal depth digitally by convolving the projected images with 
the inverse blur function (e.g., [Brown et al. 2006]). The Gaussian point-spread function (PSF) of regular 
spherical apertures, however, sets clear limitations in terms of recovering .ne image details. We show 
that deconvolution with coded apertures leads to the reconstruction of signi.cantly more image details. 
 2 Technical Approach In computer vision, optical defocus is often described as convolution with a .lter 
kernel that corresponds to an image of the aperture being used by the imaging device. The degree of defocus 
correlates to the scale of the kernel. Convolving an image with the inverse aperture kernel will digitally 
sharpen the image and consequently compensate optical defocus. This is referred to as deconvolution or 
inverse .ltering. In frequency domain, the reciprocal of the .lter kernel is its inverse, and deconvolution 
reduces to a division. Low magnitudes in the Fourier transform of the aperture image, *e-mail:max.grosse@medien.uni-weimar.de 
bimber@uni-weimar.de however, lead to intensity values in spatial domain that exceed the displayable 
range. Therefore, the corresponding frequencies are not considered, which then results in visible ringing 
artifacts in the .nal projection. This is the main limitation of previous approaches, since in frequency 
domain the Gaussian PSF of spherical apertures does contain a large fraction of low Fourier magnitudes 
(cf. .gure 1a). Applying only small kernel scales will reduce the number of low Fourier magnitudes (and 
consequently the ringing artifacts) but will also lead only to minor focus improvements (cf. .gure 1d). 
To overcome this problem, we apply a coded aperture whose Fourier transform has initially less low magnitudes 
(cf. .gure 1b). Consequently, more frequencies are retained and more image details are reconstructed 
(cf. .gure 1d). 3 Implementation and Future Work We apply the momentum preserving principle used in 
[Bimber and Emmerling 2006] for measuring the local defocus of the projector on a non-trivial surface. 
This leads to relative defocus values for each projector pixel, which are normalized to a discrete number 
of (15 in our case) aperture scales, whereby the smallest defocus pixel is assumed to be optically focussed 
and correlates to scale 0. The original image as well as the 15 scaled aperture images are Fourier transformed. 
Deconvolution is carried out for all scales in frequency domain and the results are transformed back 
to spatial domain. A .nal compensation image is composed on a per-pixel basis from all 15 results depending 
on each pixel s measured de­focus. All operations (Fourier transformations, deconvolution and composition) 
are carried out on the GPU and require less than 3 sec­onds. Currently we are working on a faster implementation 
to reach real-time compensation rates, and on the investigation of optimized code patterns.  References 
BIMBER, O., AND EMMERLING, A. 2006. Multifocal Projection: A Multiprojector Technique for Increasing 
Focal Depth. IEEE Transactions on Visualization and Computer Graphics (TVCG) 12, 4, 658 667. BROWN, M. 
S., SONG, P., AND CHAM, T.-J. 2006. Image Pre-Conditioning for Out-of-Focus Projector Blur. In Proc. 
of IEEE Conference on Computer Vision and Pattern Recognition (CVPR), vol. II, 1956 1963. Copyright is 
held by the author / owner(s). SIGGRAPH 2008, Los Angeles, California, August 11 15, 2008. ISBN 978-1-60558-466-9/08/0008 
  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1401070</article_id>
		<sort_key>380</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>29</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Evaluation of tone mapping for multi-band high dynamic range images]]></title>
		<subtitle><![CDATA[(0258)]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1401032.1401070</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401070</url>
		<categories>
			<primary_category>
				<cat_node>I.4.1</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.4.10</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010240</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision representations</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010240.10010241</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision representations->Image representations</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1098435</person_id>
				<author_profile_id><![CDATA[81320491476]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Junko]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kishimoto]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tokyo Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098436</person_id>
				<author_profile_id><![CDATA[81100449795]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Masahiro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yamaguchi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tokyo Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098437</person_id>
				<author_profile_id><![CDATA[81100151272]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Nagaaki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ohyama]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tokyo Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Yamaguchi. M, 2006 "High-fidelity video and still-image communication based on spectral information: Natural Vision system and its applications" Proc. of SPIE-IS&T Electronic Imaging, SPIE Vol. 6062, 60620G,]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1186169</ref_obj_id>
				<ref_obj_pid>1186155</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Kishimoto. J, Yamaguchi. M, Haneishi. H, and Ohyama. N, 2004 "IRODORI - A Color-rich Palette Based on Natural Vision Technology," ACM SIGGRAPH 2004 Emerging Technology]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>258884</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Debevec, P., and Malik, J. 1997. "Recovering high dynamic range radiance maps from photographs." SIGGRAPH 1997, Annual Conference Series, ACM, 369--378.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Johnson, G., and Fairchild, M. 2003. "Rendering hdr images." In Proceedings of IS &amp; T/SID 11th Color Imaging Conference, 36.41.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Evaluation of Tone Mapping for Multi-band High Dynamic Range Images (0258) Junko Kishimoto Masahiro 
Yamaguchi Nagaaki Ohyama Tokyo Institute of Technology 1 Introduction The Natural Vision [1] project 
studies the video and still imaging using color reproduction based on spectral information. Our goal 
is to reproduce highly-natural images with true color, gloss and texture by using the visual telecommunication 
systems. We have researched and developed multi-band cameras (6 and 16 bands still image camera and 6 
bands HDTV camera) and multi­primary display(6 primary display) [2], to break through the limitations 
of conventional RGB systems. As recent important image processing, a number of successful makings of 
high dynamic range (HDR) images[3] and tone mapping operators have been proposed to visualize HDR scenes. 
Then we attempted to make HDR image of multi-band using existing method and reproduce them using existing 
tone mapping operator on low dynamic range devices. To make an effect and problems clear, we are conducting 
a subjective and quantitative evaluation experiment by using images that applied some existing tone mapping 
operators. Multi-band HDR images and conventional HDR images are compared with their corresponding real 
scene by human eyes. 2 Multi-band capture The spectrum estimate accuracy from the camera signal improves 
by increasing the number of bands of cameras. We think that the highly precise estimate of spectrum is 
synonymous with having caught an accurate color. As a result of evaluating the color reproducibility 
of the 6-band still camera using Macbeth color checker, the CIELAB color difference average is .E*ab 
= 1.7. In the case of the conventional RGB camera(NIKON D200) with spectral characterization, average 
is .E*ab = 3.6. For reference, in case of using conventional color management system ICC profile(sRGB), 
average is .E*ab = 5.6. The Macbeth color checker was measured by a spectrometer (TOPCON SR-3) under 
daylight illuminant (almost D65). Especially it is effective to take objects and scenes including structure 
color and the high chroma color with a multi-band camera. Since spectrum of those colors has a tendency 
towards a narrow wavelength, a conventional RGB (3band) camera cannot capture them precisely. In addition, 
object having structural color tends to create images that have high dynamic range. For making HDR image 
of multi-spectrum, we used 6-band camera. We took image which included double Macbeth color checkers 
(Figure 1_a) with 6-band camera while changing the exposure time(4, 1, 1/4, 1/15, 1/60, 1/160, 1/500, 
1/1000s) and we generated radiance map from them[3]. Using a method of Wiener estimation, we estimated 
the spectrum from a pixel value of 6band radiance map image. They are wave patterns almost same as a 
spectrum measured by a spectrometer. The color difference average of Macbeth color chart is .E*ab = 2.2(bright 
regions) and .E*ab = 0.34(dark regions). It is possible to estimate accurate spectrum from dark and bright 
regions of multi-band HDR image.  3 Tone Mapping Operation To represent high dynamic range images on 
low dynamic range devices, a number of successful tone mapping operators have been presented. We need 
to reproduce actual color based on spectrum information. In order to develop or investigate tone mapping 
techniques that satisfy our aims, we conduct a subjective and objective experiment of multi-band HDR 
images by using existing methods. We chose linear mapping that we usually use to reproduce actual color, 
logarithmic linear mapping and iCAM model [4] based on color appearance model. 6band HDR images and conventional 
HDR images which displayed on wide gumat RGB display (AdobeRGB) were compared with their corresponding 
real scene. Figure 1 shows the acquired images for our evaluation experiment. We applied order from the 
one with a high evaluation. We show a result of subjective evaluation in Table 1. "Overall realistic" 
is a result that compared the impression of overall image with the real scene. "Bright regions" was evaluated 
paying attention to the bright Macbeth. The iCAM image was reproduced well overall. But the color difference 
average of iCAM image which made from 6band HDR is ..E*ab = 23, and normal(conventional) HDR is E*ab 
= 25. Color reproduction precision becomes about the same. Though it depends on the purpose, a good reproduction 
result seems to be provided by modifying existing methods. Figure 1 a)Macbeth color checker b)Morpho 
butterfly and hologram paper Dynamic range 48,371:1 Dynamic range 16,287:1 6band linear(bright regions) 
6band linear(dark regions) 6band log linear Normal log linear 6band iCAM Normal iCAM Overall realistic 
5 6 3 4 1 2 Bright regions 1 6 4 5 2 2 Dark regions 6 3 4 4 1 1 Table 1 Evaluation of reproduction images 
 4 Conclusion We have shown that it is possible to estimate accurate spectrum sufficiently from multi-band 
HDR images. And we attempted evaluation of existed tone mapping operation. Our future work is to develop 
tone mapping techniques focusing on accurate color reproduction. Additionally, we will develop an appropriate 
method to display multi-band HDR images on multi-primary displays. References [1] Yamaguchi. M, 2006 
High-fidelity video and still-image communication based on spectral information: Natural Vision system 
and its applications Proc. of SPIE-IS&#38;T Electronic Imaging, SPIE Vol. 6062, 60620G, [2] Kishimoto. 
J, Yamaguchi. M, Haneishi. H, and Ohyama. N, 2004 "IRODORI -A Color-rich Palette Based on Natural Vision 
Technology," ACM SIGGRAPH 2004 Emerging Technology [3] Debevec, P., and Malik, J. 1997. "Recovering high 
dynamic range radiance maps from photographs." SIGGRAPH 1997, Annual Conference Series, ACM, 369 378. 
[4] JOHNSON, G., AND FAIRCHILD, M. 2003. "Rendering hdr images." In Proceedings of IS &#38; T/SID 11th 
Color Imaging Conference, 36.41. Copyright is held by the author / owner(s). SIGGRAPH 2008, Los Angeles, 
California, August 11 15, 2008. ISBN 978-1-60558-466-9/08/0008   
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1401071</article_id>
		<sort_key>390</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>30</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Lace curtain]]></title>
		<subtitle><![CDATA[measurement of BTDF and rendering of woven cloth - production of a catalog of curtain animations]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1401032.1401071</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401071</url>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>H.5.1</cat_node>
				<descriptor>Animations</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>H.5.1</cat_node>
				<descriptor>Artificial, augmented, and virtual realities</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.6.1</cat_node>
				<descriptor>Systems theory</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.8</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341.10010349.10011310</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Simulation types and techniques->Simulation by animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341.10010346.10010347</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Simulation theory->Systems theory</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010507</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Graphics recognition and interpretation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010392</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Mixed / augmented reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010866</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Virtual reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010866</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Virtual reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1098438</person_id>
				<author_profile_id><![CDATA[81421598784]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Hitoshi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Uno]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Kwansei Gakuin University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098439</person_id>
				<author_profile_id><![CDATA[81421599981]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Yoshiki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Mizushima]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Kwansei Gakuin University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098440</person_id>
				<author_profile_id><![CDATA[81100522455]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Noriko]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nagata]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Kwansei Gakuin University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098441</person_id>
				<author_profile_id><![CDATA[81319500817]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Yoshiyuki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sakaguchi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Digital Fashion Ltd.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>882430</ref_obj_id>
				<ref_obj_pid>882404</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Neeharika Adabala, Nadia Magnenat-Thalmann, Guangzheng Fei. 2003. Visualization of woven cloth. <i>EGRW '03: Proceedings of the 14th Eurographics workshop on Rendering</i>, 178--185.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Lace curtain: Measurement of BTDF and rendering of woven cloth Production of a catalog of curtain animations 
Hitoshi Uno Yoshiki Mizushima Kwansei Gakuin University Kwansei Gakuin University  1 Introduction The 
need for rendering woven fabrics arises frequently in com­puter graphics[Neeharika Adabala, Nadia Magnenat-Thalmann, 
Guangzheng Fei 2003]. Woven fabrics have a speci.c appearance, luster, and transparency. A BRDF model 
is well known as the ba­sic technology employed for expressing the appearance of a woven fabric. In order 
to represent the transparency of a woven fabric, a BTDF (bidirectional transmittance distribution function) 
model is required in addition to the BRDF model. In this paper, we propose two rendering methods for 
woven fabrics, particularly transparent fabrics such as lace, based on a BTDF model. 2 Measurement of 
woven cloth We measured the BTDF of the two woven fabrics by using a BRDF instrument (OGM-3, DFL), which 
consists of a .xed digital cam­era, a movable light source (metal halide), and a movable sample plate. 
2400 points per cloth were measured by repositioning the lamp and the plate. As shown in Figure 1, we 
made the following observations: (1) a woven fabric has the property of bidirectional transmittance and 
scattering and (2) transmitted light consists of two components diffusional and directional transmission. 
 3 BTDF model and parameters estimation In order to express various types of woven fabrics, it is essential 
to use a standardized BTDF model that can compress the measure­ment data. We propose a standardized model 
consistings of two components diffusional and directional transmission that are de­.ned by the Henyey-Greenstein 
function. A method for the automatic estimation of parameters is required  (a) Material A (b) Material 
B Figure 1: Comparison of two woven fabrics *e-mail:nagata@kwansei.ac.jp Figure 4: Rendering of images 
with the modeled BTDF algorithms are implemented as shader plug-ins for Maya. In both the images, we 
can observe that the transmission factor differs ac­cording to the position of the curtains, and the 
resulting shadows feature uneven shading. These results demonstrate that the mod­eled BTDF is as effective 
in depicting the transmission property as the measured BTDF. 5 Real-time rendering of the BTDF A real-time 
rendering algorithm of this BTDF model was imple­mented by using a combination of OpenGL and Nvidia s 
Cg. The algorithm was programmed using texture sampling by means of an LUT, which was constructed as 
a two-dimensional bitmap image translated from the four-dimensional BTDF data (Figure 5). The transmitted 
and re.ected light obtained from the LUT, the back­ground light de.ned by cube mapping, and a texture 
element were added during rendering. 6 Conclusions We have proposed a BTDF model and two algorithms 
for both the of.ine and real-time rendering of woven fabrics. Our goal is to generate a catalog of curtain 
animations that can express various types of woven fabrics under arbitrary light conditions. References 
NEEHARIKA ADABALA, NADIA MAGNENAT-THALMANN, GUANGZHENG FEI. 2003. Visualization of woven cloth. EGRW 
03: Proceedings of the 14th Eurographics workshop on Rendering, 178 185. Copyright is held by the author 
/ owner(s). SIGGRAPH 2008, Los Angeles, California, August 11 15, 2008. ISBN 978-1-60558-466-9/08/0008 
  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1401072</article_id>
		<sort_key>400</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>31</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[Polynomial wavelet trees for bidirectional texture functions]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1401032.1401072</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401072</url>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>F.2.2</cat_node>
				<descriptor>Computations on discrete structures</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>F.2.2</cat_node>
				<descriptor>Geometrical problems and computations</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>G.1.2</cat_node>
				<descriptor>Spline and piecewise polynomial approximation</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>G.1.2</cat_node>
				<descriptor>Wavelets and fractals</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>G.1.10</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010432.10010442</concept_id>
				<concept_desc>CCS->Applied computing->Physical sciences and engineering->Mathematics and statistics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061.10010063</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures->Computational geometry</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003714.10003736.10003737</concept_id>
				<concept_desc>CCS->Mathematics of computing->Mathematical analysis->Functional analysis->Approximation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003714.10003715.10003720</concept_id>
				<concept_desc>CCS->Mathematics of computing->Mathematical analysis->Numerical analysis->Computations on polynomials</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003714.10003715</concept_id>
				<concept_desc>CCS->Mathematics of computing->Mathematical analysis->Numerical analysis</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1098442</person_id>
				<author_profile_id><![CDATA[81421601254]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Jerome]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Baril]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[LaBRI-INRIA, Bordeaux, France and Orange Labs Rennes, France]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098443</person_id>
				<author_profile_id><![CDATA[81309487978]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Tamy]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Boubekeur]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[TU Berlin, Germany]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098444</person_id>
				<author_profile_id><![CDATA[81100563296]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Patrick]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gioia]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Orange Labs Rennes, France]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098445</person_id>
				<author_profile_id><![CDATA[81100297901]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Christophe]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Schlick]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[LaBRI-INRIA, Bordeaux, France]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1020443</ref_obj_id>
				<ref_obj_pid>1018427</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Filip, J., and Haindl, M. 2004. Non-linear reflectance model for bidirectional texture function synthesis. In <i>ICPR '04</i>, IEEE Computer Society, vol. 1, 80--83.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>285310</ref_obj_id>
				<ref_obj_pid>285305</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Heidrich, W., and Seidel, H.-P. 1998. View-independent environment maps. In <i>HWWS '98</i>, ACM, New York, NY, USA, 39--ff.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383320</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Malzbender, T., Gelb, D., and Wolters, H. 2001. Polynomial texture maps. In <i>SIGGRAPH '01</i>, ACM, New York, NY, USA, 519--528.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Meseth, J., M&#252;ller, G., and Klein, R. 2004. Reflectance field based real-time, high-quality rendering of BTF. <i>Computers &amp; Graphics 28</i>, 1, 105--112.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Polynomial Wavelet Trees for Bidirectional Texture Functions Jerome Baril*,1,3 Tamy Boubekeur ,2 Patrick 
Gioia ,3 Christophe Schlick§,1 LaBRI-INRIA, Bordeaux, France1 TU Berlin, Germany2 Orange Labs Rennes, 
France3 0.8 Figure 1: (a) The general pipeline to generate a Polynomial Wavelet Tree from a BTF (b) 
error of reconstruction PWT versus PTM 1 Introduction Bidirectional Texture Functions (BTF) currently 
offer the highest quality representation for the appearance of complex real-world materials (see Figure 
1). However, BTF acquisition leads to a huge amount of data and thus cannot be used as is. In a pixelwise 
approx­imation, working with light varying material for each eye direction is more convenient since BTF 
include strong parallax effects arised from high depth varying materials. The light transition is smooth 
and can be .tted with approximation functions but the multiplicity of physical effects captured during 
acquisition leads to errors in ap­proximated data [Malzbender et al. 2001; Meseth et al. 2004; Filip 
and Haindl 2004]. Usually, a .ner .tting is found with a multi-scale approximation since data are decomposed 
into varying importance sets to drive locally the complexity of approximation functions. In this paper, 
we present a new kind of factorization for compressing BTF, called the Polynomial Wavelet Tree (PWT). 
The key idea is to separate directional and spatial variations by projecting the spatial BTF domain onto 
a wavelet domain and to approximate the result­ing light-dependent wavelet coef.cients with varying degree 
poly­nomial functions that spans the lighting directions. Our method im­proves upon previous polynomial 
approximations and is designed for high quality materials rendering using GPU. 2 Polynomial Wavelet 
Tree The BTF is a six dimensional function of (x,y). R2 the spatial co­ordinates, (.v ,fv ). S2 the viewpoint 
directions and (.l ,fl ). S2 the light directions. Our method consists in decomposing the BTF to an adaptive 
structure generated with three steps (see Figure 1). The .rst one is a projection of the BTF onto a wavelet 
basis. Then, wavelet coef.cients are factorized using a polynomial approxima­tion. Finally, the PWT is 
rendered with an ef.cient pre-computed GPU algorithm. Wavelets for BTF Multi-resolution wavelet analysis 
decompose a function into a low resolution part and a set of coef.cients that capture details from higher 
resolutions. We found enough to ap­ply the wavelet expansion in spatial dimensions only to use the good 
properties of a wavelet decomposition: energy compaction and decorrelation. We use Wavelet Packets to 
decompose images into small subbands and then, to compute statistical informations about the contribution 
of a subband in the original data. Note that while a geometric wavelet decomposition can be applied in 
direc­tion space subject to data resampling, this can lead to inaccurate approximation. Adaptive Data 
.tting Fitting wavelet coef.cients instead of data themselves is more ef.cient because low frequency 
transitions are *baril@labri.fr boubek@cs.tu-berlin.de patrick.gioia@orange-ftgroup.com §schlick@labri.fr 
 smooth and higher frequency coef.cients can be quantized with less importance. We choose to use varying 
degree bivariate polyno­mials along the light directions, (.l ,fl ), to approximate the data and, in 
this way, make the light direction space continuous. First, light space is projected onto a better suited 
space for data .tting (.l ,fl ) .. .(.l,fl ), where .(.l,fl) . R2 is a paraboloid pa­rameterization of 
the light hemisphere [Heidrich and Seidel 1998]. The data .tting process is computed on each node of 
the decompo­sition tree (i.e. per pixel) for a .xed point of view along the param­eterized light directions 
using Singular Value Decomposition. The polynomial degree is chosen per subband according to its level 
of importance found during statistical analysis. The polynomial co­ef.cients in the wavelet decomposition 
tree describe the PWT.A simpli.cation of the PWT is done in a post-process by excluding minor contribution 
branches of the tree (i.e. smallest importance subbands) according to a quality criterion, a .xed amount 
of en­ergy, or a size criterion. Rendering The inverse Wavelet Transform is partially pre­computed on 
polynomial coef.cients and resulting textures are stored in a mipmap texture. Note that, in this way, 
our GPU data structure is independent of the wavelet basis and thus makes it pos­sible to use high degree 
wavelet basis. Moreover, hardware texture .ltering can be used for mini.cation (embedded in our represen­tation) 
and magni.cation. The synthesis of a pixel is done in one pass: n texture access (n< 3, degree of bivariate 
polynomial) per pixel and few arithmetic operations for .xed view and light directions. Each viewpoint 
is stored using this method. Interpo­lation between viewpoints is done using a paraboloid map to store 
barycentric weights of intermediate view directions. 3 Results The quality of our approximation outperforms 
existing solutions (see Figure 1 (b)) and, thanks to adaptive PWT simpli.cation, the size of the data 
is less than .fty percent that of PTM. The synthesis remains ef.cient with a partial reconstruction of 
the .rst levels of the PWT and avoids aliasing with a better low resolution approxi­mation than classical 
mipmap generation. Last, PWT is not limited to BTF and can be applied on any spatially varying material 
rep­resentation. Note that polynomial approximation is not robust on specular materials. However, our 
method is independent of any ap­proximation scheme and, therefore, better approximation functions could 
be used. References FILIP, J., AND HAINDL, M. 2004. Non-linear re.ectance model for bidirectional texture 
function synthesis. In ICPR 04, IEEE Computer Society, vol. 1, 80 83. HEIDRICH, W., AND SEIDEL, H.-P. 
1998. View-independent environment maps. In HWWS 98, ACM, New York, NY, USA, 39 ff. MALZBENDER, T., GELB, 
D., AND WOLTERS, H. 2001. Polynomial texture maps. In SIGGRAPH 01, ACM, New York, NY, USA, 519 528. MESETH, 
J., M ¨ ULLER, G., AND KLEIN, R. 2004. Re.ectance .eld based real-time, high-quality rendering of BTF. 
Computers &#38; Graphics 28, 1, 105 112. Copyright is held by the author / owner(s). SIGGRAPH 2008, Los 
Angeles, California, August 11 15, 2008. ISBN 978-1-60558-466-9/08/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	</section>
	<section>
		<section_id>1401073</section_id>
		<sort_key>410</sort_key>
		<section_seq_no>10</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Geometry]]></section_title>
		<section_page_from>10</section_page_from>
	<article_rec>
		<article_id>1401074</article_id>
		<sort_key>420</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>32</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Self-organizing primitives for shape composition based on chemotaxis and genetic programming]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1401032.1401074</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401074</url>
		<categories>
			<primary_category>
				<cat_node>J.3</cat_node>
				<descriptor>Biology and genetics</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>F.2.2</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.5.2</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010257</concept_id>
				<concept_desc>CCS->Computing methodologies->Machine learning</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10003809</concept_id>
				<concept_desc>CCS->Theory of computation->Design and analysis of algorithms</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010444.10010935</concept_id>
				<concept_desc>CCS->Applied computing->Life and medical sciences->Genetics</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010444.10010087</concept_id>
				<concept_desc>CCS->Applied computing->Life and medical sciences->Computational biology</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010444.10010095</concept_id>
				<concept_desc>CCS->Applied computing->Life and medical sciences->Systems biology</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1098446</person_id>
				<author_profile_id><![CDATA[81100023224]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[David]]></first_name>
				<middle_name><![CDATA[E.]]></middle_name>
				<last_name><![CDATA[Breen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Drexel University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098447</person_id>
				<author_profile_id><![CDATA[81363591674]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Linge]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bai]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Drexel University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098448</person_id>
				<author_profile_id><![CDATA[81363603672]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Manolya]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Eyiyurekli]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Drexel University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Self-Organizing Primitives for Shape Composition Based on Chemotaxis and Genetic Programming  Figure 
1: Morphogenic Primitives self-organizing into a gear-like shape. In living things, cells aggregate and 
grow to create complicated structures. This process, called morphogenesis, is one of the fun­damental 
components involved in the development of all complex organisms. One of the essential processes involved 
in morphogen­esis is chemotaxis. Chemotaxis is the phenomenon where cells interact with other cells by 
emitting a chemical that diffuses into the surrounding environment. Neighboring cells detect the overall 
chemical concentration at their surfaces and respond to the chemi­cal stimulus by moving either towards 
or away from the source. The motions induced by chemotaxis may then produce patterns or sort­ings of 
cells, or even large-scale structures, e.g. cavities or vessels. These phenomena have motivated us to 
look to developmental bi­ology for concepts that lead to a more organic, living-cell-inspired approach 
to shape composition. In this approach, we discover the local interaction rules that di­rect self-organizing 
primitives, that we call morphogenic primitives (MPs), to aggregate into a particular user-de.ned shape. 
Macro­scopic shapes are formed automatically by the aggregation of these simple primitives responding 
only to local information. A collec­tion of MPs are at .rst randomly placed in the modeling environ­ment. 
The primitives emit a .eld, and then respond to the cumu­lative .eld by moving along its gradient. A 
macroscopic, user­de.ned shape then emerges from the combined actions of the in­dividual primitives. 
This capability should be useful in a number of applications, e.g. control of robotic swarms, motion 
speci.ca­tion of animated crowds, generative model creation, and even the computer-aided design of real 
cell aggregates. Several principles were followed when developing morphogenic primitives. 1) MPs are 
autonomous agents . Each MP is an in­dependent entity that senses the environment, responds to it, and 
then modi.es the environment and its internal state. There is no master designer directing the actions/motions 
of the MPs. 2) Ac­tions are based on local information. Each primitive emits a .nite chemical .eld that 
can be sensed only by other primitives within a certain range. The only information received by an MP 
is gath­ered at its surface, namely the concentration of the cumulative .eld and contact with immediate 
neighboring MPs. 3) MPs respond to information with prescribed behaviors. The general actions per­formed 
by each MP are the same, but the speci.cs of the individual actions are based on information received 
from the environment. 4) MPs have no representation of the .nal, macroscopic shape to be produced. MPs 
do not use information about the .nal shape to de­termine what actions to take. Their actions are pre-determined 
by the ultimate shape to produce, but MPs do not carry or access infor­mation about the shape. The MP 
s .nal global position relative to *e-mail: lb353@drexel.edu, me52@drexel.edu, david@cs.drexel.edu the 
shape is not known ahead of time. 5) The shape emerges from the aggregation of the local interactions 
and behaviors between ran­domly placed MPs. Rather than follow a plan to produce the shape, MPs sense, 
change and respond to the cumulative .eld concentra­tion. This simple behavior, when combined with somewhat 
com­plex chemical .elds, will direct the MPs to take individual actions based on local information that 
will ultimately aggregate to pro­duce a user-de.ned, macroscopic shape. The main challenge here is to 
determine which local chemical .elds will direct the MPs to ultimately come together into the desired 
shape. While MPs fundamental interaction is based on chemotaxis, we do not limit their behaviors/properties 
to be physically realistic or completely consistent with biology. Instead, developmental biol­ogy provides 
a motivating starting point for MPs. As a way to cus­tomize chemotaxis-driven cells for shape composition, 
we alter the chemical concentration .elds around individual cells. Instead of the chemical concentration 
dropping off as a function of distance (the physically accurate description), we de.ne the concentration 
.eld with a mathematical function of distance, angle and time. Since it is extremely dif.cult to determine 
which particular local .eld function will direct MPs to form a speci.c macroscopic shape, we employ genetic 
programming, a type of evolutionary computing, to produce the mathematical expressions that explicitly 
specify the .eld function. A .tness measure, based on the shape that emerges from the chemical-.eld-driven 
aggregation, determines which ex­pressions will be passed along to later generations. The genetic process 
stops once an expression in the population provides the desired shape, or after a certain number of expression 
generations have been produced and evaluated. Once the local .eld function has been identi.ed for a speci.c 
shape, MPs may be randomly placed in the computing environment, with each MP surrounded by the GP-produced 
concentration .eld. A simulation is performed where the cumulative .eld is computed, and each MP moves 
along the .eld s gradient, until the MP popu­lation reaches an equilibrium, which is the desired shape. 
We have utilized this new approach to de.ne morphogenic primi­tives that aggregate to form a number of 
user-de.ned shapes, e.g. an ellipse, a diamond, a boomerang, and an hourglass. In the pro­cess of evolving 
the .eld functions for user-de.ned shapes, a few pleasant surprises materialized. Most of them included 
repeated patterns, e.g. stripes, spots and sine waves, but most interest­ingly a gear shape emerged from 
the evolutionary process. See Figure 1. More detailed descriptions of this work can be found at http://www.cs.drexel.edu/~david/Papers/SMI08 
LB.pdf and http://www.cs.drexel.edu/~david/Papers/GECCO08 LB.pdf. Copyright is held by the author / owner(s). 
SIGGRAPH 2008, Los Angeles, California, August 11 15, 2008. ISBN 978-1-60558-466-9/08/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1401075</article_id>
		<sort_key>430</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>33</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Eccentric radial basis functions and the applications]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1401032.1401075</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401075</url>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.6</cat_node>
				<descriptor>Graphics data structures and data types</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.6.5</cat_node>
				<descriptor>Modeling methodologies</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.6.4</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010341.10010342</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Model development and analysis</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061.10010063</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures->Computational geometry</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341.10010342.10010343</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Model development and analysis->Modeling methodologies</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341.10010342.10010344</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Model development and analysis->Model verification and validation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010394</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Graphics file formats</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Performance</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1098449</person_id>
				<author_profile_id><![CDATA[81421600624]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Yoshihiro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kanamori]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098450</person_id>
				<author_profile_id><![CDATA[81421600218]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Eiji]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Takaoki]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[META Corporation, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098451</person_id>
				<author_profile_id><![CDATA[81100539710]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Tomoyuki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nishita]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>549676</ref_obj_id>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Bloomenthal, J., and Wyvill, B., Eds. 1997. <i>Introduction to Implicit Surfaces</i>. Morgan Kaufmann Publishers Inc., San Francisco, CA, USA.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Eccentric Radial Basis Functions and the Applications Yoshihiro Kanamori* Eiji Takaoki Tomoyuki Nishita 
The University of Tokyo META Corporation, Japan The University of Tokyo 1 Introduction The use of radial 
basis functions (RBFs) has a long history in com­puter graphics. Typical applications of RBFs are continuous 
in­terpolation/approximation of discrete data, and .eld functions of metaball-like implicit surfaces. 
In this paper we introduce eccen­tric radial basis functions (ERBFs) in order to enhance the power of 
expression of RBFs. An ERBF is an RBF equipped with a point called eccentric center , which is used to 
compute the distance for the RBF and introduce eccentricity into the distribution of the func­tion values; 
the gradient of the function becomes gentle at the far side of the eccentric center while steep at the 
near side of it. The ex­tension is simple yet powerful, and allows us to .t data with fewer basis functions 
than RBFs, especially when .tting or modeling data consisting of both gentle and steep gradations. We 
demonstrate the effectiveness in applications such as .tting of scattered data and modeling with ERBF-based 
metaballs.  2 Eccentric Radial Basis Functions (ERBFs) The concept of ERBFs was originally invented 
by Eiji Takaoki early in 1985 and used in his art work in 1992 (which is found in [Bloo­menthal and Wyvill 
1997]) for enhancing .eld functions of meta­balls to represent steeply curved surfaces. He referred to 
such meta­balls as eccentric metaballs, however, the detail of the enhancement has never been published. 
Here we reveal it for the .rst time as a general form, an ERBF. Figure 1: Calculations of r for (a) 
an RBF and (b) an ERBF. An RBF f(r) is a function of the distance r from the center c to a point x in 
d-dimension (Fig. 1a), and when a cutoff radius R is used, f(r)=0 in the range of r>R. Well-known examples 
of RBFs are a Gaussian a exp(-ßr2) (where a and ß are constants), and a thin-plate spline r 2 log |r|. 
While both an RBF and an ERBF are functions of the distance r, they differ in how to compute the distance. 
For an ERBF (Fig. 1b), r is measured not from the center c, but from the eccentric center e, and scaled 
as follows: v ie-xi(e-c, x-e)+ D r = R = R, (1) ie-x'iR2 -ie-ci2 2 `2´ D = (e-c, x-e)2 + ix-eiR2 -ie-ci, 
(2) where x' is the intersection point between a d-dimensional sphere with radius R and a half line originating 
at e and passing through *e-mail: pierrot@nis-lab.is.s.u-tokyo.ac.jp e-mail: etak@metaco.co.jp e-mail: 
nis@nis-lab.is.s.u-tokyo.ac.jp x. Intuitively, by regarding Fig. 1 as the top views of upside-down cones, 
the distance r for an RBF is considered as the height of a cone whose apex is located at c. By contrast, 
for an ERBF, r is considered as the height when the apex is translated to e. 3 Results and Conclusion 
 Figure 2: Approximating a log plot of the Mie phase function (green) using eccentric Gaussians (red). 
The .tted curve is blue. Fig. 2 shows a result of approximating complex data using eccen­tric Gaussians. 
As an example, we chose a log plot of the Mie phase function, which is essential to the computation of 
scattering of lights in computer graphics. We can approximate the data us­ing 10 eccentric Gaussians 
with 4.6% relative RMS error, while in our experiments we need 17 ordinary Gaussians with 8.0% relative 
RMS error.  Figure 3: Rendering results of Takaoki s torso model using eccen­tric metaballs. Fig. 3 
shows rendering results of Takaoki s torso model consisting of only 24 eccentric metaballs. Note that 
the breast and the hip exhibit steeply curved surfaces that are hard to represent with ordi­nary metaballs. 
We conclude that these results have demonstrated the power of expression of ERBFs. References BLOOMENTHAL, 
J., AND WYVILL, B., Eds. 1997. Introduction to Implicit Surfaces. Morgan Kaufmann Publishers Inc., San 
Francisco, CA, USA. Copyright is held by the author / owner(s). SIGGRAPH 2008, Los Angeles, California, 
August 11 15, 2008. ISBN 978-1-60558-466-9/08/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1401076</article_id>
		<sort_key>440</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>34</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Shading with apparent relief]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1401032.1401076</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401076</url>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>F.2.2</cat_node>
				<descriptor>Geometrical problems and computations</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Curve, surface, solid, and object representations</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Geometric algorithms, languages, and systems</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.8</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010507</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Graphics recognition and interpretation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061.10010063</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures->Computational geometry</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010399</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Parametric curve and surface models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010401</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Volumetric models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1098452</person_id>
				<author_profile_id><![CDATA[81421593758]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Romain]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Vergne]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[INRIA Bordeaux University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098453</person_id>
				<author_profile_id><![CDATA[81314487621]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Pascal]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Barla]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[INRIA Bordeaux University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098454</person_id>
				<author_profile_id><![CDATA[81100325428]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Xavier]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Granier]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[INRIA Bordeaux University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098455</person_id>
				<author_profile_id><![CDATA[81100297901]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Christophe]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Schlick]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[INRIA Bordeaux University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1124749</ref_obj_id>
				<ref_obj_pid>1124728</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Barla, P., Thollot, J., and Markosian, L. 2006. X-toon: An extended toon shader. <i>In NPAR</i>, ACM.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>145386</ref_obj_id>
				<ref_obj_pid>145375</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Koenderink, J. J., and van Doorn, A. J. 1992. Surface shape and curvature scales. <i>Image Vision Comput.</i> 10, 8, 557--565.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1142015</ref_obj_id>
				<ref_obj_pid>1141911</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Rusinkiewicz, S., Burns, M., and DeCarlo, D. 2006. Exaggerated shading for depicting shape and detail. <i>ACM Trans. Graph (Proc. SIGGRAPH) 25</i>, 3 (July).]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Shading with Apparent Relief RomainVergne * PascalBarla * XavierGranier * ChristopheSchlick *  (a) 
(b) (c) (d) Figure 1: (a) Shape descriptor domain. we show the color code used to visualizeourshapedescriptor:planesareinwhite, 
caps(inyellow) and cups(inpink) arelocated around the symmetry axis, and saddles(inblue) separateconvex(ingreen) 
fromconcaveregions(inred). Theaxisofthe parameter space correspondtoprincipalcurvaturesk1 andk2.(b)Apparent 
Relief.(c)planarregionsare .ltered outtoreveal theentirefeaturesof the object.(d) concavities aredrawninwhite,convexitiesinblack 
andplanar­and saddle-like regions in gray. The respective ARMs are shown in the top left cornerof the 
.gures(0.4Mtri/244fps). 1 Introduction Shape depiction is an important dimension of image creation. For 
example, techniques are used to remove ambiguities in scienti.c illustrations, or to create more legible 
representations in paintings and drawings. Previous approaches focus on a set body of tech­niques: line-based 
rendering.Suchtechniquesgeneratesome shape cues to depict the object characteristics that correspond 
to discon­tinuities of shape features. Many artists rather depict an object s shapethrough shading.Theyhave 
touseotherkindsof shapecues that correspond to continuous variations of shape features to con­vey more 
subtle information about shape andintegrate it seamlessly into conventional lighting. Instead of detecting 
sharp discontinu­ities as in line-based rendering, we thus seek a set of continuous cuesthathave tobede.nedforeachpixel 
of animage.Tothisend, we introduce an intermediate representation that we call Apparent Relief to assist 
the user. It is a view-dependent shape descriptor, from which continuous shape cues are easily extracted, 
and which gives rise to stylized shading-based shape depictions. By construc­tion,itisfreeof temporal 
coherenceartifactsand naturallyleads to automatic Levels-of-Detail(LOD) effects. Ourapproach issimple 
to implement,runs inreal-timeonmoderngraphicshardware,and allowsauserselectionoffeatures. We illustrate 
itspotential using several shading styles. 2 Apparent Relief descriptor Oursolution toshading-basedshapedepiction 
is tomanipulatecon­tinuouscues through ashapedescriptor inspiredfrom[Koenderink and van Doorn 1992]: 
for each pixel, we extract a vector S, which corresponds toapositionintheshapedescriptor(Figure1(a)).The 
direction D gives information about surface convexity and is com­puted for each vertex in object-space 
using the vector K =(k1, k2) composed of principal curvatures: D = K/||K||. When ||K||= 0 * e-mail: {vergne|barla|granier|schlick}@labri.fr 
(a)0.3Mtri/250fps (b)0.15Mtri/250fps (c)1Mtri/40fps Figure 2: Cartoon shading, counter shading, and exaggerated 
shading. (i.e., the center of the shape descriptor domain, corresponding to planar regions on the surface), 
we set D =(0, 0). The length L of the vector S, which represents the curvedness information, is di­rectly 
computed in image space to provide view-dependency and automatic LODs. It is computed as the amount of 
variation of nor­mals inanextendedpixel neighborhood usingGaussianderivatives of normal images transformed 
tocameraspace. OurApparentRe­lief Descriptor consists in convexity and curvedness information: S = D 
* L. Figure1(b) shows an example where S is computed for eachpixel,anddisplayed using thecolorcodegiven 
in(a). S caneasilybeused asa texturecoordinate tolookup arelief value r .[0, 1] ina texturecalledApparentReliefMap(ARM) 
which can bemodi.edinreal-timebytheuser.Bydirectlyassigningthisvalue to thepixel,weobtainasimple cel-shading 
style(Figures1(c) and (d)).Theusercaneasilydrawintothe texturetoselectand enhance the desired shape cues. 
As shown in Figure 2(a), conventional cartoon-shading may easilybe modi.ed usingX-Toon[Barla et al. 2006] 
tocreatemorecomplexstyles.Inourcase,thedetailcoordi­nate corresponds to the relief value. Counter-shading 
is a drastic shading style which uses onlyblack and white colors. We use relief values obtained from 
the ARM as contrast values (Figure 2(b)). Countershadingisthusde.nedby a linearinterpolationbetweena 
diffuse and inverted diffuse intensity: I =(1-r)n · l + r(1-n · l), where n and l de.ne the current point 
s normal and light unit vec­tors. I is then thresholded at 0.5. Figure 2(c) shows how the ARM is combined 
with exaggerated shading [Rusinkiewicz etal. 2006] to control which cues maybe enhanced(here convex regions). 
 3 Discussion and future work Our new shape descriptor called Apparent Relief, based on a com­bination 
of object-space and image-space measurements, provides a simple and .exible approach to select continuous 
and view­dependentshape cues with automaticLODfunctionalities. Apaint­ing interface allows the user to 
directly draw into the ARM to choose which cues are selected and which ones are .ltered with a direct 
feed-back. The user can also use the inverse approach and enhance some cues by simply clicking on a pixel 
of the rendered image. The ARM is then automatically updated with the corre­sponding vector S and a chosen 
color. We would like to improve the interface to provide a faster and a more .exible control for the 
user.Wealsoplan todeal withdynamicscenesonwhichprincipal curvaturesneed toberecomputed at eachframe. 
 References BARLA, P., THOLLOT, J., AND MARKOSIAN, L. 2006. X-toon: An ex­tended toon shader. In NPAR,ACM. 
KOENDERINK, J. J., AND VAN DOORN, A. J. 1992. Surface shape and curvature scales. ImageVisionComput.10,8,557 
565. RUSINKIEWICZ, S., BURNS, M., AND DECARLO, D. 2006. Exagger­ ated shading for depicting shape and 
detail. ACM Trans. Graph(Proc. SIGGRAPH)25,3(July). Copyright is held by the author / owner(s). SIGGRAPH 
2008, Los Angeles, California, August 11 15, 2008. ISBN 978-1-60558-466-9/08/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1401077</article_id>
		<sort_key>450</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>35</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[Transferring surface data across geometric models in a digital production environment]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1401032.1401077</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401077</url>
		<categories>
			<primary_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Curve, surface, solid, and object representations</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>F.2.2</cat_node>
				<descriptor>Computations on discrete structures</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>F.2.2</cat_node>
				<descriptor>Geometrical problems and computations</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Geometric algorithms, languages, and systems</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061.10010063</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures->Computational geometry</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010401</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Volumetric models</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010399</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Parametric curve and surface models</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1098456</person_id>
				<author_profile_id><![CDATA[81538931856]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Kiran]]></first_name>
				<middle_name><![CDATA[S]]></middle_name>
				<last_name><![CDATA[Bhat]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Industrial Light + Magic]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098457</person_id>
				<author_profile_id><![CDATA[81421597456]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Cary]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Phillips]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Industrial Light + Magic]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<article_type art_type="regular_article" />
	</article_rec>
	</section>
	<section>
		<section_id>1401078</section_id>
		<sort_key>460</sort_key>
		<section_seq_no>11</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Let's get physical]]></section_title>
		<section_page_from>11</section_page_from>
	<article_rec>
		<article_id>1401079</article_id>
		<sort_key>470</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>36</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Laughing out loud]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1401032.1401079</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401079</url>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Physically based modeling</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.6.1</cat_node>
				<descriptor>Systems theory</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.8</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010507</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Graphics recognition and interpretation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003741.10003742.10003745</concept_id>
				<concept_desc>CCS->Mathematics of computing->Continuous mathematics->Topology->Geometric topology</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341.10010346.10010347</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Simulation theory->Systems theory</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352.10010379</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation->Physical simulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1098458</person_id>
				<author_profile_id><![CDATA[81100090629]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Paul]]></first_name>
				<middle_name><![CDATA[C.]]></middle_name>
				<last_name><![CDATA[DiLorenzo]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of California, Riverside]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098459</person_id>
				<author_profile_id><![CDATA[81100319485]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Victor]]></first_name>
				<middle_name><![CDATA[B.]]></middle_name>
				<last_name><![CDATA[Zordan]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of California, Riverside]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098460</person_id>
				<author_profile_id><![CDATA[81421595523]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Benjamin]]></first_name>
				<middle_name><![CDATA[L.]]></middle_name>
				<last_name><![CDATA[Sanders]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of California, Riverside]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Filippelli, M., Pellegrino, R., Iandelli, I., Misuri, G., Rodarte, J., Duranti, R., Brusasco, V., and Scano, G. 2001. Respiratory dynamics during laughter. <i>Journal of Applied Physiology 90</i>, 4, 1441--1446.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Luschei, E., Ramig, L., Finnegan, E., Baker, K., and Smith, M. 2006. Patterns of Laryngeal Electromyography and the Activity of the Respiratory System During Spontaneous Laughter. <i>Journal of Neurophysiology 96</i>, 1, 442.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1141970</ref_obj_id>
				<ref_obj_pid>1141911</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Park, S. I., and Hodgins, J. K. 2006. Capturing and animating skin deformation in human motion. <i>ACM Transactions on Graphics 25</i>, 3 (July), 881--889.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1028528</ref_obj_id>
				<ref_obj_pid>1028523</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Zordan, V., Celly, B., Chiu, B., and DiLorenzo, P. 2004. Breathe easy: model and control of simulated respiration for animation. <i>ACM SIGGRAPH/Eurographics symposium on Computer animation</i>, 29--37.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Laughing Out Loud PaulC.DiLorenzo * VictorB.Zordan BenjaminL.Sanders University ofCalifornia,Riverside 
University ofCalifornia,Riverside University ofCalifornia,Riverside DreamWorksAnimationSKG We present 
a novel technique to generate and control laughter us­ing aphysicallybased,anatomically inspired torsosimulation. 
We use this system to synthesize motion of the trunk and to create sec­ondary effects that propagate 
to other parts of the body, e.g. the arms and the head. A hybrid set of rigid and .exible components 
comprise the model of the torso: spine, ribs, diaphragm, and ab­domen as well as neck and shoulders. 
We employ hierarchical, Hill-type muscles to actuate laughter movement. The result is a rich, controlled 
motion derived from a simple, intuitive set of con­trolparameterswhich wecanuse todemonstratearangeof 
laugh­ing animations. Further, by utilizing biomedical research in laughter, we offer a second, innovativemethodforcontrollingthedescribed 
systemau­tomatically starting from a source audio track of laughing. Our keyinsight,which isdescribed 
in thesupplemental material, is that lung pressure is a direct re.ection of the strength and phase of 
the laughter which can be assessed from the time varying amplitude of an audio signal. From this observation, 
we can determine the pressure required for a desired recording of laughter. By adding a pressure-based 
model of the lungs to our torso simulation, we formulateanoptimizationproblemwhichdeterminesactivations 
to producethedesiredpressurepro.les.Weusethis technique togen­erate animation that syncs well with a 
given audio track. Notably, thisapproach isaligned withcommonpracticesused inproduction animation where 
movement is often made to match a pre-recorded soundtrack. Our approach differs in important ways from 
pure data-driven techniques, such as the impressive skin capture examples shown in[Park andHodgins2006] 
which attempt topreciselyrecreatea given example motion. Foremost, we argue that our approach of­fers 
a strong advantage in control over the motion. It is dif.cult to modify asurfacemodel of agiggle tobemoreuproarious,oreven 
to be somewhat longer, without a reasonable model of laughter. In addition, with our added emphasis placed 
on controlling laughter with an audio clip, we support the common practice of separating themotiongeneration(orcapture) 
from thesound recording. The matching of aprerecorded animation with aprerecorded soundtrack is a dizzyingly 
complex proposition. In addition, our anatomically inspired, physically-based torso is superior to a 
procedural model, such as one which uses a weighted blending technique. During laughter,thereisrichinterplaybetweenthesubsystemsofthe 
torso (i.e. abdominalcavity, ribcage, clavicles, and spine) which changes over the course of a single 
bout of laughter and across the range of possible laughingbehaviors(a littlegiggle toadeep-bellyuproar.) 
The degree of effects depends on the intensity of the contraction and thepressure inthegutaswellascontractionoftheothercom­ponents. 
Propagation is intensi.ed if the contraction speedexceeds the damping effects of any link in the chain. 
Further, our method could be made to react to external forces, so the torso would react inaphysicallyplausiblemanner 
todisturbances. Aprocedural ap­proach wouldnotbeasuitablemethodfor this typeof interaction. The primary 
contributions of this paper are: 1) we introduce an anatomically motivated model for the generation of 
laughter mo­tion; 2) we propose a straightforward method for controlling the activationofthemodel using 
simple, intuitiveinputsignals; and3) weprovideanalternativeapproachforcontrollingthesystemauto­matically 
using an audio soundtrack of laughing. * e-mail: pdiloren@cs.ucr.edu e-mail: vbz@cs.ucr.edu e-mail: bsanders@cs.ucr.edu 
 Figure 1: Sample output from our laughter simulation and a plot of an input audio and the simulated 
pressure for a corresponding laugh animation found with our optimization. Experimental Results. Examples 
ofour work are includedin the accompanying video. We include manually controlled laughter: a giggle, 
an average laugh, and adeepbelly laugh.We includeaset of optimized,audio-driven laugh animations. To 
test the strength and range of our system, we ran the audio-driven laughter optimization over many different 
typesof laughter,somenatural,somecomputergenerated.Weob­served that more natural laughs produced better 
results than com­puter generated laughs. In addition, we compare our technique to apuredata-driven animation(motion 
capture) and aprocedural an­imation. Finally, we generated a set of manually controlled mo­tions including 
breathing, coughing, and sneezing to demonstrate the range of possible motions that can be generated 
using the pro­posed system.Forskinrendering,weshowaproof of concept that wasdeveloped inDreamworksAnimation 
sproductionpipelineon how our torso simulation can be integrated into a production set­ting. References 
FILIPPELLI, M., PELLEGRINO, R., IANDELLI, I., MISURI, G., RODARTE, J., DURANTI, R., BRUSASCO, V., AND 
SCANO, G. 2001.Respiratorydynamicsduringlaughter. Journal of Applied Physiology 90,4,1441 1446. LUSCHEI, 
E., RAMIG, L., FINNEGAN, E., BAKER, K., AND SMITH, M. 2006. Patterns of Laryngeal Electromyography and 
theActivityof theRespiratorySystemDuringSpontaneous Laughter. Journal of Neurophysiology 96,1,442. PARK, 
S. I., AND HODGINS, J. K. 2006. Capturing and animat­ing skin deformation in human motion. ACM Transactions 
on Graphics 25,3(July),881 889. ZORDAN, V., CELLY, B., CHIU, B., AND DILORENZO, P. 2004. Breatheeasy: 
model and control of simulated respirationforan­ imation. ACM SIGGRAPH/Eurographics symposium on Com­ 
puter animation,29 37. Copyright is held by the author / owner(s). SIGGRAPH 2008, Los Angeles, California, 
August 11 15, 2008. ISBN 978-1-60558-466-9/08/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1401080</article_id>
		<sort_key>480</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>37</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Real time hair simulation and rendering on the GPU]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1401032.1401080</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401080</url>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>F.2.2</cat_node>
				<descriptor>Geometrical problems and computations</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Curve, surface, solid, and object representations</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Geometric algorithms, languages, and systems</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.1</cat_node>
				<descriptor>Graphics processors</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.8</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010507</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Graphics recognition and interpretation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061.10010063</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures->Computational geometry</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010399</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Parametric curve and surface models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010401</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Volumetric models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010389</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Graphics processors</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1098461</person_id>
				<author_profile_id><![CDATA[81309513319]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Sarah]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tariq]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[NVIDIA Corporation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098462</person_id>
				<author_profile_id><![CDATA[81319488052]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Louis]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bavoil]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[NVIDIA Corporation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[M. M&#252;ller, B. Heidelberger, M. Hennix, J. Ratdiff. <i>Position Based Dynamics</i>. Proceedings of Virtual Reality Interactions and Physical Simulations, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1089521</ref_obj_id>
				<ref_obj_pid>1089508</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Bertails, F., Menier, C., and Cani, M.-P. 2005. <i>A Practical Self-Shadowing Algorithm for Interactive Hair Animation</i>. In proceedings of Graphics Interface 2005, 71--78.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1142012</ref_obj_id>
				<ref_obj_pid>1179352</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Bertails, F., Audoly, B., Cani, M.-P., Querleux, B., Leroy, F., and Leveque, J. L. <i>Super-Helices for Predicting the Dynamics of Natural Hair</i>. In Proceedings of ACM Transactions on Graphics (Proceedings of the SIGGRAPH conference, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Real Time Hair Simulation and Rendering on the GPU Sarah Tariq Louis Bavoil NVIDIA Corporation NVIDIA 
Corporation  Figure 1: Hair Simulated and Rendered entirely on the GPU. 15 FPS on NVIDIA GeForce 8800 
GTX. 1280 x 1024, 8X MSAA. 166 simulated strands, 10220 rendered strands, 1.6 million triangles. 1. Introduction 
Simulating and rendering realistic hair with tens of thousands of strands is something that until recently 
was not possible in real time. We present a method for simulating and rendering realistic hair in real 
time using the power and programmability of modern GPUs (Graphics Processing Units). Our method utilizes 
new features of graphics hardware (like Stream Output, Geometry Shader and Texture Buffers) that make 
it possible for all simulation and rendering to be processed on the GPU in an intuitive manner, with 
no need for CPU intervention or read back. In addition, we propose fast new algorithms for inter-hair 
collision, and collision detection and resolution of interpolated hair. 2. Simulation We simulate the 
hair based on a particle constraint method [Muller et al. 2006], which is extremely parallelizable and 
well suited to be implemented on the GPU. To simulate the guide hair, we create one long Vertex Buffer 
(VB) of positions of all the guide hairs, inserting them back to back. Other attributes that we need 
for simulation (e.g.: rest lengths distance constraints) are stored on additional buffers and bound to 
the input as needed and read in the Vertex Shader (VS), where all the update computations are performed. 
To simulate the movement of the hair we render this VB to another VB using the vertex shader and the 
Stream Output pipeline stage. Adding collision forces to hair vertices is a completely parallel process. 
However, all simulation updates are not parallel. For example, distance constraints work on pairs of 
vertices. To fully utilize the GPU we need to update as many constraints in parallel as possible. Two 
constraints can be updated in parallel only if they are independent, i.e. they don t share a vertex. 
Hence, to resolve distance constraints we use a two pass approach, sectioning the Vertex Buffer into 
independent subsets (using two Index Buffers) which are updated one after the other. Since the shader 
that has to enforce these constraints needs to work on a pair of vertices we use the Geometry Shader 
(GS). 3. Inter-hair Collisions One of the most important parts of hair simulation is the simulation 
of inter-hair collisions. We are particularly interested in the volume preserving nature of inter-hair 
collisions. We use an approach similar to [Bertails et al. 2005]; we create a voxelized representation 
of all the interpolated hair and then apply repulsive forces to hair vertices in high density areas. 
However, unlike their approach our forces are more globally coherent. We formulate our forces to point 
in the direction of the negative gradient of the blurred density. These forces aim to push hair where 
we would intuitively want; out of areas of high density and towards areas of low density. In addition 
we also voxelize collision obstacles into this density grid; this prevents our inter-hair forces from 
pushing hair into solid objects. 4. Tessellation, Interpolation and Rendering While we simulate hair 
in the order of a few hundred strands with only a few control points to achieve a realistic look we create 
a lot more hair strands with more segments for rendering. We use a combination of two methods to generate 
the additional hair: clump based interpolation and barycentric interpolation. The clump based method 
creates additional hairs following a single guide. Barycentric interpolation adds new hairs within a 
scalp triangle by interpolating from the hairs rooted at the triangle s vertices. To render the interpolated 
hair we render a set of dummy lines, and use the VS to read and interpolate the appropriate simulated 
guide vertices. The GS is used to expand the interpolated lines into camera facing triangle strips, and 
then finally we render the hair with shading, shadows and Alpha to Coverage. 5. Avoiding Collisions 
in Interpolated Hair One of the artifacts with interpolating new hair from multiple guide hair occurs 
when the interpolated hairs go through the collision obstacles (for example because the guide hair ended 
up on different sides of an obstacle). To avoid this we detect when any interpolated strand would penetrate 
an obstacle, and switch the interpolation mode of such strands to single-strand interpolation. It is 
not sufficient to adjust only those vertices in the interpolated hair strands that actually undergo penetration; 
we must alter the positions of all vertices beneath them as well. To identify hair vertices below other 
object­penetrating vertices we do a pre-pass, rendering all the interpolated hair vertices to a texture, 
such that all the vertices in one interpolated strand are rendered to the same pixel. For each vertex 
we output its offset index from the root if that vertex collides with an object or a large constant otherwise. 
We use minimum blending in this pass such that the result is a texture that encodes for each interpolated 
hair strand the first vertex that intersects an object, if any. In the final pass we use this texture 
to decide the interpolation mode to use on each vertex. 6. References M. Müller, B. Heidelberger, M. 
Hennix, J. Ratcliff. Position Based Dynamics. Proceedings of Virtual Reality Interactions and Physical 
Simulations,2006. Bertails, F., Menier, C., and Cani, M.-P. 2005. A Practical Self-Shadowing Algorithm 
for Interactive Hair Animation. In proceedings of Graphics Interface 2005, 71-78. Bertails, F., Audoly, 
B., Cani, M.-P., Querleux, B., Leroy, F., and Leveque, J.L. Super-Helices for Predicting the Dynamics 
of Natural Hair. In Proceedings of ACM Transactions on Graphics (Proceedings of the SIGGRAPH conference, 
2006. Copyright is held by the author / owner(s). SIGGRAPH 2008, Los Angeles, California, August 11 15, 
2008. ISBN 978-1-60558-466-9/08/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1401081</article_id>
		<sort_key>490</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>38</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Massive particles]]></title>
		<subtitle><![CDATA[particle-based simulations on multiple GPUs]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1401032.1401081</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401081</url>
		<abstract>
			<par><![CDATA[<p>There is no study using multiple GPUs for particle-based simulation to the best of our knowledge although several researchers have been using a GPU. In this study, a particle-based simulation is parallelized on multiple GPUs. There are several challenges to accomplish it. For example, the simulation should not have serial computations that can be a bottleneck of the simulation. Particle methods cannot assign a fixed computation data to each GPU but the data has to be reassigned each iteration because particles not having any fixed connectivity can move freely. It is difficult for a particle-based simulation to scale the performance to the number of GPUs because the overhead of the parallelization can be high. We overcame these hurdles by employing not a server-client computation model but a peer-to-peer model among GPUs. Each GPU dynamically manages their own computation data without a server. A sliced-grid was used to lower the traffic among GPUs. As a result, the simulation speed scales well to the number of GPUs and the method brings it possible to simulate millions of particles in real-time. Of course, the proposed method is effective not only for a simulation on GPUs but also one on CPUs. The contribution of this study also includes a sorting technique utilizing the coherency between the time steps which was also introduced to increase the performance on a GPU.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.1</cat_node>
				<descriptor>Graphics processors</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.6.1</cat_node>
				<descriptor>Systems theory</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.8</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010341.10010346.10010347</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Simulation theory->Systems theory</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010389</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Graphics processors</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010507</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Graphics recognition and interpretation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1098463</person_id>
				<author_profile_id><![CDATA[81319492976]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Takahiro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Harada]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Havok]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098464</person_id>
				<author_profile_id><![CDATA[81421596118]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Issei]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Masaie]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[PrometechSoftware]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098465</person_id>
				<author_profile_id><![CDATA[81329489943]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Seiichi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Koshizuka]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098466</person_id>
				<author_profile_id><![CDATA[81100605028]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Yoichiro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kawaguchi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1321271</ref_obj_id>
				<ref_obj_pid>1321261</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Harada, T., Koshizuka, S., and Kawaguchi, Y. 2007. Sliced-data structure for particle based simulations on gpus. In <i>Proc. of GRAPHITE</i>, 55--62.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[NVIDIA. Compute unified device architecture. http://www.nvidia.com/object/cuda_home.html.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1401082</article_id>
		<sort_key>500</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>39</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[Smash it!]]></title>
		<subtitle><![CDATA[simulating car crash in the Incredible Hulk]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1401032.1401082</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401082</url>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>H.5.1</cat_node>
				<descriptor>Artificial, augmented, and virtual realities</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.6.1</cat_node>
				<descriptor>Systems theory</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.6.7</cat_node>
				<descriptor>Environments</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.8</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.6.3</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010341.10010346.10010347</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Simulation theory->Systems theory</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010866</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Virtual reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341.10010366.10010367</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Simulation support systems->Simulation environments</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010866</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Virtual reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010507</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Graphics recognition and interpretation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010392</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Mixed / augmented reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1098467</person_id>
				<author_profile_id><![CDATA[81421601141]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Tae-Yong]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kim]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Rhythm and Hues Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098468</person_id>
				<author_profile_id><![CDATA[81100288854]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[David]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Horsley]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Rhythm and Hues Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>846284</ref_obj_id>
				<ref_obj_pid>846276</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Grinspun, E., Hirani, A., Desbrun, M., and Shroder, P. 2003. Discrete shells. <i>ACM SIGGRAPH / Eurographics Symposium on Computer Animation</i>, 62--67.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Smash It! -simulating car crash in the Incredible Hulk Tae-Yong Kim* David Horsley Rhythm and Hues Studios 
Rhythm and Hues Studios.  For the recent production of The Incredible Hulk, Rhythm and Hues Studios 
has developed a set of tools to simulate crushing and crum­pling of metalic objects (such as car smashed 
by a hero character). Metal Deformation To simulate crumpling of metals, we revised our in-house cloth 
sim­ulator in many ways. In reality, stiffness of metal is orders of mag­nitude higher than normal fabric. 
This presented challenges -cloth would stretch unrealistically and , more seriously, bending models would 
break simulation as we cranked up the stiffness of bending. In the limit, the cloth object had to move 
almost like a rigid object, We derived the bending energy on dihedral angles between a pair of triangles 
as in prior works [Grinspun et al. 2003]. In such mod­els, force jacobians (as used for implicit integration) 
are not always positive de.nite, and this accounted for most instability in the cloth solver when the 
bending stiffness had to be high (in fact, orders magnitude higher than the usual cloth materials). We 
came up with a novel linearization technique that would trans­form these force Jacobians such that jacobians 
matrices stay always positive de.nite regardless of the material stiffness. The resulting simulator was 
very stable -in fact, we did not have any stability issue with even .xed time stepping scheme of as few 
as 4 steps per frame. To solve the equations of motion, we used the standard conjugate gradient method 
for lower stiffness and would turn to the ef.cient PARDISO solver for higher stiffness. Plasticity Once 
we could simulate very stiff material, the next step was to add a notion of shape memory, so that metals 
could preserve de­formed shape after each crushing event. We added several control on plasticity, updating 
rest condition on both length and bending forces. When and how these conditions were updated depended 
on such parameters as amount of collision impact, impact prop­agation distance, weakening factor of the 
material upon collision (plastic softening), and hardening factor of the material after colli­sion (plastic 
hardening). Such paramters could be painted over the cloth surface, yielding different behavor on different 
parts (e.g. a door would crush upon collision while a tire would bounce back after initial deformation). 
 Deformation Transfer The simulator was relatively ef.cient (it could handle the whole simulation for 
50000 triangles under a couple of hours). However, *e-mail: tae@rhythm.com e-mail:dfh@rhythm.com actual 
rendered geometry had over a million triangles, and it was clearly not practical to run simulation on 
such complex geometry. We started by a proximity based point trasfer -for each point of the rendered 
geometry, we .nd the closest triangle on the cloth geome­try, and store local coordinate of the point 
w.r.t the triangle. As the cloth gets deformed, we use the stored coordinate to update posi­tion of the 
rendered geometry. When a point is bounded to only one triangle, this tends to generate artifacts for 
points that were far away from the cloth geometry and/or around highly deformed region. As our cloth 
geometry was constantly wrinkled due to crushing, this artifact would distort the rendered geometry too 
much. On the other hand, using global transfer technique such as Lattice (FFD) would smooth out too much 
of detail from the underlying simulation. In the end, we developed a tool that seamlessly trans­fers 
deformation that would sit somewhere in-between above two techniques. Instead of binding each point to 
a single triangle, we scanned a region of cloth geometry so that each point would be at­tached to a set 
of triangles. The amount of deformation transfer was then weighted by triangle normals and distance to 
the triangle. We called this technique multi-proxy, and it served our purpose well. Rigid body simulation 
When some parts of the car had to be detached (such as doors) due to collision, we turned to rigid body 
simulation that was run in Hou­dini. From a single cloth simulation, each breakable piece would be separtely 
deformed using multi-proxy until breaking event would happen. Once detached, each piece was converted 
to a rigid body initialized with deformed shape at the moment, and then simulated with rigid body simulator 
for the remainder of the shot. When two metal objects had to collide (such as hummer getting hit against 
a pole), we simulated heavier object .rst and then locked it as a rigid body, and ran a second pass simulation 
of another object while using the .rst one as collision object. Such simulation lay­ering turned out 
to be more ef.cient both in compuation and user time than a single simulation on combined objects. Most 
of the car crash had to be accompanied with other destrutive events, such as breaking window and falling 
pieces. Breaking win­dow was handled similarly as other rigid objects, except in this case, each fragment 
of window was handled separately. References GRINSPUN, E., HIRANI, A., DESBRUN, M., AND SHRODER, P. 
2003. Discrete shells. ACM SIGGRAPH / Eurographics Sympo­sium on Computer Animation, 62 67. Copyright 
is held by the author / owner(s). SIGGRAPH 2008, Los Angeles, California, August 11 15, 2008. ISBN 978-1-60558-466-9/08/0008 
  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	</section>
	<section>
		<section_id>1401083</section_id>
		<sort_key>510</sort_key>
		<section_seq_no>12</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Ride, watch, and learn]]></section_title>
		<section_page_from>12</section_page_from>
	<article_rec>
		<article_id>1401084</article_id>
		<sort_key>520</sort_key>
		<display_label>Article No.</display_label>
		<pages>2</pages>
		<display_no>40</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Artistic expression using Second Life in the classroom]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1401032.1401084</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401084</url>
		<categories>
			<primary_category>
				<cat_node>K.3.1</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>H.5.1</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.8</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010507</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Graphics recognition and interpretation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002951.10003227.10003251</concept_id>
				<concept_desc>CCS->Information systems->Information systems applications->Multimedia information systems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010489</concept_id>
				<concept_desc>CCS->Applied computing->Education</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Management</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1098469</person_id>
				<author_profile_id><![CDATA[81319498084]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Bonnie]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Mitchell]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Bowling Green State University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098470</person_id>
				<author_profile_id><![CDATA[81100501759]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Dena]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Eber]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Bowling Green State University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098471</person_id>
				<author_profile_id><![CDATA[81421599794]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Anthony]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Fontana]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Bowling Green State University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[http://slurl.com/secondlife/bowling%20green%20state/140/140/140/]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[http://ww/www.slcn.tv/]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Artistic Expression using Second Life in the Classroom Bonnie Mitchell Dena Eber § Anthony Fontana 
* Bowling Green State University Bowling Green State University Bowling Green State University 1 Introduction 
The persistent online virtual world of Second Life is transforming education in unique and innovative 
ways. Universities and educational institutions have taken advantage of this flexible platform by purchasing 
'islands' in the virtual world where social interaction and creative endeavors can be the focus of project­based 
learning. Bowling Green State University s virtual campus provides an environment for art educator s 
to explore new means of artistic expression and critique. The arts area of the Bowling Green State Island 
includes three visual art galleries, a media theatre, an interactive music and nature walk, and a performance 
center. Based on the experiences of three professors at Bowling Green State University, this panel will 
focus on educational prospects in the arts in the virtual world of Second Life. Dr. Dena Eber will present 
on student artistic reactions to Second Life experiences and a public virtual art critique event that 
followed. Anthony Fontana will focus on the use of machinima to create documentary and expressive narrative 
works in an educational context. Bonnie Mitchell will explore a new form of scripted immersive art that 
is surfacing because of the programming and modeling capabilities of Second Life. Bowling Green State 
University Virtual Campus 2 Reactions and Artistic Expression After creating virtual identities through 
avatars and experiencing other galleries and events in Second Life, advanced imaging students made artworks 
that were based on a reaction to their experience. After critiquing the works, the students then posted 
them in a virtual gallery on the BGSU island in Second Life. A public critique was held in-world with 
educators and art enthusiasts from remote locations attending. The students were questioned and received 
feedback from the Second Life attendees. When the event was over, the students were asked to analyze 
their experience within the broader context of exhibition, educational, and expressive opportunities 
already available. One overarching objective was to have students examine Second Life as a possibility 
for display and dissemination of work as well as other options such as mobile technologies and traditional 
galleries. 3 Scripting Art Experiences The programming language embedded in Second Life provides educators 
and students the ability to construct unique interactive art environments. Through the development of 
object specific or genetic-like algorithms, environments can be created that respond to avatars and change 
over time. These responsive environments are often fully immersive and avatars can travel through the 
experience and trigger audio and visual events. Although interactive electronic art environments are 
not uncommon in the digital art world, this unique form of art experience is beginning to emerge in the 
virtual world as an exciting new direction. As a digital art educator, the ability to create virtual 
art experiences offers opportunities for students to combine their programming, art, and usability skills. 
 4 Machinima The Second Life platform provides a unique opportunity for documentary and expressive narrative 
works in the form of machinima, or computer-generated animation that uses the virtual world engine to 
render real-time three-dimensional graphics. Machinima has been used extensively to teach Second Life 
residents how to use software, maneuver the virtual world, and build and script objects. Machinima can 
also act as a means to document the interactive happenings of the virtual world. The Second Life Cable 
Network (SLCN) also features a number of interview and talk-show style machinima programs. Machinima 
has also been a catalyst for independent filmmakers and burgeoning animators. Second Life machinimatographers 
are able to create their own sets, costumes, and animations and film the narrative without the time and 
material costs of 3D animation. As an expressive medium, machinima can be used both as an educational 
tool and a means of creating time-based works of art. 5 Conclusion The persistent online virtual world 
of Second Life is transforming education in unique and innovative ways. Bowling Green State University 
s virtual campus provides an environment for art educator s to explore new means of artistic expression 
and critique. Using images, real-time interaction, and animation to illustrate the pedagogical possibilities 
of Second Life, the presenters discuss student response, faculty engagement, and technical possibilities. 
They also analyze the benefits and pitfalls of educational applications in Second Life and conclude with 
suggestions. 6 References bonniem@bgsu.edu § deber@bgsu.edu * anthonymfontana@gmail.com http://slurl.com/secondlife/bowling%20green%20state/140/140/140/ 
http://www.slcn.tv/ Copyright is held by the author / owner(s). SIGGRAPH 2008, Los Angeles, California, 
August 11 15, 2008. ISBN 978-1-60558-466-9/08/0008   
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1401085</article_id>
		<sort_key>530</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>41</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Bikeware]]></title>
		<subtitle><![CDATA[have a match with networked bicycle in urban space]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1401032.1401085</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401085</url>
		<categories>
			<primary_category>
				<cat_node>H.5.1</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.8</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010507</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Graphics recognition and interpretation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002951.10003227.10003251</concept_id>
				<concept_desc>CCS->Information systems->Information systems applications->Multimedia information systems</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Management</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1098472</person_id>
				<author_profile_id><![CDATA[81421595008]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Shunpei]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yasuda]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098473</person_id>
				<author_profile_id><![CDATA[81421601327]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Fumitaka]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ozaki]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098474</person_id>
				<author_profile_id><![CDATA[81421597731]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Hiroshi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sakasai]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098475</person_id>
				<author_profile_id><![CDATA[81421593968]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Shino]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Morita]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098476</person_id>
				<author_profile_id><![CDATA[81319498464]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Naohito]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Okude]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Esbj&#246;rnsson, M, Juhlin, O &amp; &#214;&#214;stergren, M. 2003. <i>Motorbikers using Hocman - Field Trials on Mobile Interaction</i>. In Proceedings of Mobile HCI'2003. Udine, Italy. Springer. Berlin Heidelberg New York. pp 32--44.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Bikeware: Have a Match with Networked Bicycle in Urban Space.  Shunpei Yasuda* Fumitaka Ozaki* Hiroshi 
Sakasai* Shino Morita* Naohito Okude* Keio University Keio University Keio University Keio University 
Keio University  1. Introduction  Bikeware is a social media that provides user with urban pleasure. 
It is a networked bicycle which user can enjoy a race match with other members in urban spaces. User 
doesn t have to get together in the same time, by uploading the data during the race. User are able to 
experience matches with competitors who participated in past races in real time. In Bikeware, the strategy 
of a match and knowledge of the local area stands superior to simple physical strength. Only the starting 
and finishing points are assigned in the race. Users can seek their own route that leads them to the 
destination faster than others.  Fig 1: Have pleasure from networked bicycle 2. New pleasure in the 
city  In Japan, bicycle is both a casual and common tool for transportation in cities. Many people 
use bicycle for commute or shopping. Bikeware enhances these social experiences in urban spaces from 
two aspects: relationship and contextualization. First, Bikeware empowers a virtual communication in 
urban spaces. Web technology have developed virtual communications such as e-mail and SNS. Bikeware can 
expand it to physical and urban world, since Bikeware enables networked matches in physical space. People 
will be able to communicate with many people on the courses by their activity and presence. Secondly, 
with Bikeware, any road in the city can become a race course and hold another context. Local train station 
may become a start point or an end point; uphill roads may act as obstacles to lose time. Spaces to which 
we ordinarily don t pay attention will become a meaningful place. Any town and any space could become 
a place filled with pleasure. Bikeware is an alternative way to design urban environment. Currently, 
architectural and polical means are the major measures for such activities. Experiences from Bikeware 
are totally novel from the experience provided before. 3. Prototype  The prototype of Bikeware has 
GPS, 3D acceleration sensor, speed sensor and network. Bikeware downloads all of the match data from 
the internet by connecting to the network. When user reaches the starting point, Bikeware alerts the 
user and indicates the location of the finishing point, number of entrants and lap time. User can choose 
to participate in the match or not and the race will --- *e-mail: {shunp,t05244fo,t05442hs,t05907sm,okude}@sfc.keio.ac.jp 
start. During the race, time for the race and distance to the end point are calculated. Users can see 
their ranking after each race and have matches with other users who joined past races as if they are 
competing in real time. When the user arrives at the finishing point, the race record is saved and the 
final ranking will be indicated. If the device is connected to the network again, the data of the match 
is uploaded to the internet.  Fig 2: Architecture of the Bikeware 4. Related Works  Motor bike was 
the focus for the project called Hockman [Esbjörnsson and Östergren, 2003]. It achieves a new value on 
urban pleasure by passing other friend on the street. Bikeware also uses interaction towards the city 
and bring a merit. In Bikeware, place can get an additional context and the relationship is created from 
that new context. Also communication is not the only pleasure in Bikeware. User will find the pleasure 
through the city; exploring the route and finding a match. 5. Future works / Conclusion  The goal for 
Bikeware is to design a new form of social relationship and urban lifestyle on the existing urban infrastructure 
with interactive technologies. We are prepareing a web page for Bikeware. Through the web page, user 
will be able to select starting points and finishing points and create new matches. It will also enable 
users to check where races are being held, as well as the records of past races. With both device and 
web, Bikeware enhances the value of the activities in the urban spaces. The prototype which we made has 
an acceleration sensor and is capable of setting local rules. This application is not only designed for 
entertainment purposes, as it can provide a variety of experiences. Bikeware will be a novel tool to 
design and enhance the value and experience in the urban society. Acknowledgments This project is granted 
by CREST, JST. References ESBJÖRNSSON, M, JUHLIN, O &#38; ÖSTERGREN, M. 2003. Motorbikers using Hocman 
- Field Trials on Mobile Interaction. In Proceedings of Mobile HCI'2003. Udine, Italy. Springer. Berlin 
Heidelberg New York. pp 32-44.. 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1401086</article_id>
		<sort_key>540</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>42</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Bird watching]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1401032.1401086</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401086</url>
		<categories>
			<primary_category>
				<cat_node>H.5.1</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002951.10003227.10003251</concept_id>
				<concept_desc>CCS->Information systems->Information systems applications->Multimedia information systems</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Management</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1098477</person_id>
				<author_profile_id><![CDATA[81421600082]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Kathy]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Marmor]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Vermont]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Bird Watching Kathy Marmor University of Vermont  Bird Watching is an interactive audio and video installation 
created specifically to comment on the invisible presence of space satellites. I focus on satellites 
to highlight the politics of remote sensing, amateurism, and identity. Bird Watching asks in a personal 
and intimate way if satellites, as tools of globalization, are transforming our conceptions of identity. 
My playful cardboard birds challenge the perception of satellites as remote objective eyes and my distinctive 
low-tech approach makes remote sensing satellites accessible and their surveillance palpable. The DIY 
creativity of amateur inventors inspired the unusual combination of cardboard and sensors. Bird Watching 
consists of six folded cardboard boxes that are suspended from the ceiling. Two boxes open and close 
each time a sensor is triggered. The other four boxes are embedded with proximity sensors and audio speakers. 
Each box s sensors, speakers, and wiring are clearly apparent and available. The audio speaker in one 
box is connected to a VHF/UHF radio scanner that listens for transmissions being sent to ground stations 
from low earth orbiting satellites. The radio receiver sits on a small worktable and is connected to 
a satellite antenna. Next to the radio is a computer monitor that displays the internet satellite tracking 
application Predict. Predict visually represents the satellite s orbit over the installation space and 
uses the computer s voice to announce the satellite s arrival when it is within radio range. The apparatus 
that my satellites use for remote sensing are simple proximity sensors. These sensors trigger a sound 
to play through the satellites speakers and the sound s duration and pitch is controlled by the participant 
s proximity to the sensor. The sound s pitch gets higher the closer one is to a sensor. Only when one 
maintains a specific distance from the sensor does the sound become comprehensible. I use sounds, such 
as a woman crying or children laughing, to remind us of the specificity of satellite data. The obvious 
correlation between sound and physical presence fosters an awareness of the ongoing interchange between 
people and technology. e-mail: kmarmor@uvm.edu e-mail: j6@umbc.edu. By emphasizing this exchange as well 
as initiating anticipation for the next orbiting satellite, I encourage participants to be satellite 
watchers . This notion of watching and being part of an active exchange inverts the usual power dynamic 
in surveillance by suggesting that the observed (or the watched) is an active accomplice who always has 
agency. My installation further accentuates our complicated relationship with technology by using the 
proximity sensors to track the participant s route in the installation as well. I worked with Jonathan 
Decker, a computer science student from the University of Maryland, Baltimore County to create a visualization 
that maps participants paths through the installation. This map is analogous to the satellite tracking 
on the computer monitor and these two adjacent tracking systems articulate the dialectic between the 
local and the global. The local and global are also linked through the viewer s participation in the 
installation. Moving one s body to activate the sound is an immediately local experience. However, over 
time participants realize that their interactions with the installation are being displayed in real time 
as an orbit. Thus a local action has global implications. Bird Watching creates an environment that emphasizes 
what I refer to as the ecology of perception. I define the ecology of perception as a reciprocal relationship 
between self and environment. It is my belief that our experience of our environment informs our perceptivity 
of self. Bird Watching suggests that space satellites provide a unique perspective of the earth and its 
peoples. This new perspective has come to define us but my installation questions how we identify ourselves 
as global citizens with new responsibilities.  Copyright is held by the author / owner(s). SIGGRAPH 
2008, Los Angeles, California, August 11 15, 2008. ISBN 978-1-60558-466-9/08/0008 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	</section>
	<section>
		<section_id>1401087</section_id>
		<sort_key>550</sort_key>
		<section_seq_no>13</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[The future of art]]></section_title>
		<section_page_from>13</section_page_from>
	<article_rec>
		<article_id>1401088</article_id>
		<sort_key>560</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>43</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Newtoon]]></title>
		<subtitle><![CDATA[learning science socially through cell phone game creation]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1401032.1401088</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401088</url>
		<abstract>
			<par><![CDATA[<p>Newtoon is a prototype mobile phone and webbased activity enabling young people to create and play mobile phone games that embed physics learning. Game creation is in the hands of young people, creating java-based microgames via a webinterface that can then be downloaded and played on their own phones. Our user-trials in two schools provided insights into the opportunities of learning using mobile phones and games.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[cell phone games]]></kw>
			<kw><![CDATA[education]]></kw>
			<kw><![CDATA[learning]]></kw>
			<kw><![CDATA[physics]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>K.3.1</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>J.2</cat_node>
				<descriptor>Physics</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>K.8.0</cat_node>
				<descriptor>Games</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>H.5.1</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.8</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010432.10010441</concept_id>
				<concept_desc>CCS->Applied computing->Physical sciences and engineering->Physics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10011007.10010940.10010941.10010969.10010970</concept_id>
				<concept_desc>CCS->Software and its engineering->Software organization and properties->Contextual software domains->Virtual worlds software->Interactive games</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010476.10011187.10011190</concept_id>
				<concept_desc>CCS->Applied computing->Computers in other domains->Personal computers and PC applications->Computer games</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002951.10003227.10003251.10003258</concept_id>
				<concept_desc>CCS->Information systems->Information systems applications->Multimedia information systems->Massively multiplayer online games</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002951.10003227.10003251</concept_id>
				<concept_desc>CCS->Information systems->Information systems applications->Multimedia information systems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010507</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Graphics recognition and interpretation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010489</concept_id>
				<concept_desc>CCS->Applied computing->Education</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Management</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1098478</person_id>
				<author_profile_id><![CDATA[81421600950]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Clara]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lemon]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Futurelab, Bristol, UK]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Newtoon: learning science socially through cell phone game creation Clara Lemon Futurelab, 1 Canons 
Road, Bristol, BS1 5UH, UK clara.lemon@futurelab.org.uk  Abstract Newtoon is a prototype mobile phone 
and web­based activity enabling young people to create and play mobile phone games that embed physics 
learning. Game creation is in the hands of young people, creating java-based microgames via a web­interface 
that can then be downloaded and played on their own phones. Our user-trials in two schools provided insights 
into the opportunities of learning using mobile phones and games. Keywords: Cell phone games, learning, 
physics, education. 1 Background Futurelab and Soda Creative have created a mobile phone and web application 
that enables young people to create microgames via a web interface on a PC, in a 2D world consisting 
of balls and springs. The games can be previewed and edited on the PC and various physics principles 
regulating the movement of objects can be manipulated via the interface. Each created game lasts only 
a few seconds, during which players need to figure out how to play the game. A number of these microgames 
can be aggregated into gamestacks , shared with other players and played on a mobile phone. We have trialed 
our prototype in two schools and have some interesting early findings.  2 Mobile phones and learning 
Key findings from our trials in school include: Newtoon had a striking effect on learner engagement. 
Students of all abilities and genders embraced the experience with enthusiasm.  Newtoon not only helped 
learners understand scientific concepts, but also helped them experience the scientific process and design 
process and develop their digital literacy.  Students took their learning home and engaged their families 
in the activity as well.   3 Next steps The prototype is currently being developed further so that 
schools can try Newtoon themselves. 4 Origination of the idea The idea was submitted to Futurelab s 
Call for Ideas programme from Soda Creative in London and they collaborated on the project. 5 Further 
information For further information, please go to: http://www.futurelab.org.uk/projects/newtoon Copyright 
is held by the author / owner(s). SIGGRAPH 2008, Los Angeles, California, August 11 15, 2008. ISBN 978-1-60558-466-9/08/0008 
  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1401089</article_id>
		<sort_key>570</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>44</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Digital reconstruction and 4D presentation through time]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1401032.1401089</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401089</url>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.4.5</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245.10010254</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems->Reconstruction</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061.10010063</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures->Computational geometry</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1098479</person_id>
				<author_profile_id><![CDATA[81452617507]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Sabry]]></first_name>
				<middle_name><![CDATA[F.]]></middle_name>
				<last_name><![CDATA[El-Hakim]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[National Research Council Canada]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098480</person_id>
				<author_profile_id><![CDATA[81100038665]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Jean-Fran&#231;ois]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lapointe]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[National Research Council Canada]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098481</person_id>
				<author_profile_id><![CDATA[81421600602]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Emily]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Whiting]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MIT]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Camuffo, D., and Sturaro, G. 2003. Sixty-cm submersion of Venice discovered thanks to Canaletto's paintings. <i>Climate Change 58</i>, 333--343.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Demotride Viewer. http://www.demotride.net.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[El-Hakim, S. F., 2001. A flexible approach to 3D reconstruction from single images. ACM SIGGRAPH'01 Technical Sketches.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Sideris, A., and Roussou, M. 2002. Making a new world out of an old one: In search of a common language for archaeological immersive VR representation. <i>VSMM</i>, 31--42.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Digital Reconstruction and 4D Presentation through Time Sabry F. El-Hakim* Jean-Franc¸ois Lapointe 
 Emily Whiting Visual Information Technology Visual Information Technology Department of Architecture 
National Research Council Canada National Research Council Canada MIT  Figure 1: The 4D interactive 
presentation, with time slider, in the Demotride viewer. 1 Introduction Virtual time travel transforming 
the existing remains of a heritage site to its original condition has value for education and cultural 
understanding. However, digitally reconstructing objects which no longer exist is a challenge. Interaction 
and navigation within virtual 4D worlds (adding time to 3D worlds) is also problematic due to imprecise 
understanding of the time dimension. In this project we developed an approach to 3D modeling of sites 
that have undergone changes over the years. The method creates independent models from different types 
of data, such as frescoes, paintings, drawings, old photos, historic documents, and digitized remains. 
The models are assembled and integrated for a 4D interactive presentation. Several research issues have 
been addressed: (1) Modeling from frescoes and drawings with incorrect perspective, (2) modeling from 
paintings and old photos including .ne geometric details from shading (3) coloring models from old photos 
and drawings to match existing elements, (4) creation of models by seamless and accurate integration 
of data obtained from independent sources, and (5) the creation of intuitive interactive presentations 
that link the models with other multimedia components and information related to the history of the site. 
We will describe contributions to these issues, including our own advanced model viewer [Dem ], and apply 
them to modeling heritage sites such as Venice which appeared in paintings by Canaletto, Bernardo Bellotto, 
and Francesco Guardi, and many 19th century photos. Canalettos paintings have been used to measure the 
subsidence of Venice [Camuffo and Sturaro 2003]. 2 Approach Techniques to create models from frescoes, 
paintings, and old photos have been developed. For old frescoes or paintings with incorrect perspective, 
the method requires that part of the object or site still exists, for example, its foundation or archaeological 
remains. We use measurements from those remains to resample the source material, create an image with 
correct perspective, and use this to reconstruct the site. For Renaissance paintings, which have correct 
perspective, and old photos, we use our .exible approach for 3D construction from single images [El-Hakim 
2001]. * e-mail:Sabry.El-Hakim@nrc-cnrc.gc.ca e-mail:Jean-Francois.Lapointe@nrc-cnrc.gc.ca e-mail:ewhiting@csail.mit.edu 
The approach applies several constraints: coordinate constraints, surface constraints, and topological 
constraints in two steps: a calibration step and a reconstruction step. The solution is based on Photogrammetric 
principles. It is challenging to develop effective tools for visualizing virtual sites in 4 dimensions, 
for use by experts and non-experts. There is no standard approach for interacting with the time dimension, 
especially with models of sites that have changed over time. Many approaches have been tried, however 
the outcome is usually only a pre-rendered movie, which bypasses the interactivity problems. We developed 
the Demotride viewer, which works with content based on VRML and X3D. Speci.cally, Demotride s support 
of time sensors allows animation of time and interaction with the 4D models. One feature that distinguishes 
Demotride from other viewers is its ability to provide what we call controlled inter­action, where the 
user is bound to respect the visualization and interaction parameters speci.ed by the content developer. 
For example, Demotride supports unique features that permit control of the walkthrough parameters (orientation 
speed and interaction technique) when navigating in the virtual scene. The approach has been applied 
to Venice sites, such as Piazza San Marco and the Arsenale (.gure 1), and other cities in Italy (Florence, 
Padua, Trento) and Canada. Since we are still re.ning the procedure, future work includes comprehensive 
evaluation of the interactive presentation alternatives, at various con.gurations, by several groups 
of users. Both experienced and non-experienced users will be drawn upon. Further work involves designing 
and implementing an immersive system. References CAMUFFO, D., AND STURARO, G. 2003. Sixty-cm submersion 
of Venice discovered thanks to Canaletto s paintings. Climate Change 58, 333 343. Demotride Viewer. http://www.demotride.net. 
EL-HAKIM, S. F., 2001. A .exible approach to 3D reconstruction from single images. ACM SIGGRAPH 01 Technical 
Sketches. SIDERIS, A., AND ROUSSOU, M. 2002. Making a new world out of an old one: In search of a common 
language for archaeological immersive VR representation. VSMM, 31 42. Copyright is held by the author 
/ owner(s). SIGGRAPH 2008, Los Angeles, California, August 11 15, 2008. ISBN 978-1-60558-466-9/08/0008 
  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1401090</article_id>
		<sort_key>580</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>45</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Creating ceramic art using rapid prototyping]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1401032.1401090</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401090</url>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>J.6</cat_node>
				<descriptor>Computer-aided design (CAD)</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>J.5</cat_node>
				<descriptor>Fine arts</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.8</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010469.10010472.10010440</concept_id>
				<concept_desc>CCS->Applied computing->Arts and humanities->Architecture (buildings)->Computer-aided design</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010507</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Graphics recognition and interpretation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010432.10010439.10010440</concept_id>
				<concept_desc>CCS->Applied computing->Physical sciences and engineering->Engineering->Computer-aided design</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010469.10010470</concept_id>
				<concept_desc>CCS->Applied computing->Arts and humanities->Fine arts</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010469.10010471</concept_id>
				<concept_desc>CCS->Applied computing->Arts and humanities->Performing arts</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1098482</person_id>
				<author_profile_id><![CDATA[81421594057]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[John]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Balistreri]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Bowling Green State University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098483</person_id>
				<author_profile_id><![CDATA[81331491142]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Sebastien]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Dion]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Bowling Green State University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Creating Ceramic Art Using Rapid Prototyping John Balistreri* Bowling Green State University Sebastien 
Dion Bowling Green State University  Figure 1: Traditional tea bowl and rapid prototyped replica, wood 
fire. 1 Introduction Our research team is currently creating ceramic art objects on a Zcorp rapid prototype 
machine, both in the form of sculpture and pottery. These pieces are then finished through normal ceramic 
studio methods (i.e. kiln firing). We believe this new method of ceramic fabrication is non only making 
important contributions to the Ceramic and Digital Art communities but will offer great opportunities 
in the fields of scientific visualization, architecture and engineering. This research seeks to addresses 
the theme of SIGGRAPH 2008 and embodies a number of categories including Slow Art, Future History and 
Crossing boundaries. 2 Significance and Applications The results of this research have taken 3d printing 
beyond prototyping to achieve an original and permanent state. This technology has the potential to revolutionize 
the tile and brick industry as well as design and ceramic art fields. For example it is possible to create 
large-scale ceramic surfaces through modules such as tile or brick of infinite topography or relief without 
using mold or carving techniques. Designs created with the use of traditional and digital modeling methods 
can be acquired, manipulated and then converted into rapid ceramic manufacturing projects. A wall of 
unlimited dimension composed of individual tiles could be generated from a drawing or from an original 
relief model, which once scanned can be 3D printed. If one of the tiles that compose the larger image 
fails it would simply be reprinted. This could only be done before by carving every tile by hand. 3 
Significance as Art The conceptual, aesthetic and historical implications of this work has profound meaning 
as it directly ties one of the most ancient mediums of expression, Ceramic Art (28,000 years old) to 
the most current advancements within Digital Art. Most of our knowledge of previous civilizations comes 
from the study of their ceramic objects The interface of these mediums evokes issues of permanence, history 
and human expression in interesting ways. As an example Teabowls are highly studied by contemporary ceramic 
artists and have a revered status as objects originating from Zen Buddhism and the tea ceremony in Japan. 
They are valued for how they express mans connectivity to nature and have influenced abstract expressionism 
as well as many of the leading ceramic artists of this century. This is why, as a ceramic artist I have 
chosen to use the iconic ceramic object of the teabowl as subject matter in this research. By making 
a teabowl following the traditional method and then by 3D scanning it and rendering from the obtained 
data an analogue version of it, we are questioning deeply the notion of the originality and of the aura 
of the object but also demonstrating among other things that this new digital ceramic tool can be use 
to create significant objects of art that will outlive most other digital creations. In the case of the 
teabowls represented in figure 1 , both were fired to completion in a traditional wood fire kiln at temperature 
above 2381 degrees Fahrenheit. 4. Conclusion Our research has shown that there are many direct applications 
that could result from using 3D RP technology to form ceramic objects. We believe, for instance, that 
the use of 3D modeling can allow an artist to create objects without the limitations of traditional forming 
processes. Ceramic artists in the past have been largely limited to what can be constructed while the 
force of gravity is acting on the wet object. Our process avoids some of the constraints of gravity because 
the object is supported on all surfaces by the dry medium. The ability to create a virtual object in 
a gravity-neutralized environment is not only a very thought-provoking concept for ceramicists but could 
also present structural solutions for building ceramic objects in industrial applications. This property 
has potential impacts that have yet to be fully grasped by engineers and manufacturers. *balistr@bgsu.edu 
sebastien.dion@gmail.com  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1401091</article_id>
		<sort_key>590</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>46</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[SKORPIONS]]></title>
		<subtitle><![CDATA[kinetic electronic garments]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1401032.1401091</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401091</url>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.6.1</cat_node>
				<descriptor>Systems theory</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.8</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010341.10010346.10010347</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Simulation theory->Systems theory</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010507</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Graphics recognition and interpretation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1098484</person_id>
				<author_profile_id><![CDATA[81309488703]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Joanna]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Berzowska]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Concordia University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098485</person_id>
				<author_profile_id><![CDATA[81421594847]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Di]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Mainstone]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Di Mainstone Design]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 SKORPIONS: kinetic electronic garments Joanna Berzowska Di Mainstone XS Labs, Concordia University Di 
Mainstone Design joey@xslabs.net dimainstone@hotmail.com 1. Introduction SKORPIONS are a set of kinetic 
electronic garments that use the shape-memory alloy (SMA) Nitinol to move and change on the body in slow, 
organic motions. The dresses are not interactive: their programming does not respond to sensor data. 
They behave like kinetic sculptures that display characteristics such as control, anticipation, and unpredictability. 
They have personalities and anthropomorphic qualities. They can be imagined as parasites that inhabit 
the skin of the host. The garments shift and modulate personal and social space by imposing physical 
constraints on the body. They alter behavior, by hiding or revealing hidden layers, inviting others inside 
the protective shells of fabric, by erecting breathable walls, or tearing themselves open to divulge 
hidden secrets. They reference the history of garments as instruments of pain and desire. 2. Technical 
details SKORPIONS integrate electronic fabrics, custom electronics, Nitinol, mechanical actuators such 
as magnets, and traditional textile construction techniques. The cut of the pattern, the seams, and other 
construction details are an important component of designing and engineering specific behaviors. Nitinol 
is a shape memory alloy that has the ability to indefinitely remember its geometry. The range of applications 
has been increasing in recent years, especially in medicine. During its relaxed (or martensite) state, 
Nitinol is malleable and resistant; it can be integrated into a soft fabric substrate without causing 
any disruption in its natural movement and flexibility. Once heated to its shape memory (or austenite) 
state, a Nitinol wire becomes stiff and returns to its original shape with enough force to lift its own 
weight several times. The process happens around 500°C, so it is impossible to shape the Nitinol after 
integration into the fabric. This limits design and fabrication possibilities. To develop the kinetic 
mechanisms utilized in the SKORPIONS, it was necessary to develop a variety of custom shaped Nitinol 
Copyright is held by the author / owner(s). SIGGRAPH 2008, Los Angeles, California, August 11 15, 2008. 
ISBN 978-1-60558-466-9/08/0008 wires and explore techniques like knitting, beading, and hand stitching. 
Nitinol wire that has been previous shape-set can also be woven into a textile in its martensite state. 
Textiles are traditionally flexible surfaces that can softly envelop the human body. To successfully 
merge textiles with electronics, we use conductive yarns, inks, and fabrics to allow the construction 
of soft electronic circuits. The traditional textile materials and construction methods such as weaving 
and embroidery allow us to merge functionality and aesthetics in our design process. For instance, we 
connect our custom control electronics to the Nitinol coils with decorative stitching made with conductive 
thread. The use of snaps is another technique that allows a durable and seamless connection between the 
flexible threads and the rigid printed circuit board (PCB). The male snaps are soldered to the PCB, while 
the female are sewn into the dress and connected to the conductive thread with conductive epoxy. A small 
inside pocket holds the electronics and insulates them from skin and moisture. This modular assembly 
allows for the board to be completely removed when necessary. Nitinol is not only used as the scaffold 
for the desired shape change, but it can also, through resistive heating, produce the heat necessary 
for actuation. Since the Nitinol wire needs to undergo a temperature change of approximately 40°C to 
be activated, it requires a considerable amount of power. These requirements are met by two small rechargeable 
lithium polymer cells that can power the dresses for a couple of hours, depending on the timing and frequency 
of activation. 3. Acknowledgements SKORPIONS is an XS Labs project, funded by The Canada Council for 
the Arts, the Hexagram Institute, and the Social Sciences and Humanities Research Council of Canada. 
Members of the team include Marguerite Bromley, Marcelo Coelho, David Gauthier, Francis Raymond, and 
Valerie Boxer. http://www.xslabs.net/skorpions/  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	</section>
	<section>
		<section_id>1401092</section_id>
		<sort_key>600</sort_key>
		<section_seq_no>14</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Lions + Whos + Hulks, oh my!]]></section_title>
		<section_page_from>14</section_page_from>
	<article_rec>
		<article_id>1401093</article_id>
		<sort_key>610</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>47</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[A new approach to procedural character rigs]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1401032.1401093</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401093</url>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>F.2.2</cat_node>
				<descriptor>Geometrical problems and computations</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.6.5</cat_node>
				<descriptor>Modeling methodologies</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341.10010342.10010343</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Model development and analysis->Modeling methodologies</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061.10010063</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures->Computational geometry</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061.10010063</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures->Computational geometry</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1098486</person_id>
				<author_profile_id><![CDATA[81421595613]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Erik]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Malvarez]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Blue Sky Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098487</person_id>
				<author_profile_id><![CDATA[81421593479]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Scotty]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sharp]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Blue Sky Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098488</person_id>
				<author_profile_id><![CDATA[81421602109]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Stephen]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Unterfranz]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Blue Sky Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 A New Approach to Procedural Character Rigs Erik Malvarez Senior Character TD Scotty Sharp Senior Character 
TD Stephen Unterfranz Supervising Character TD Blue Sky Studios Blue Sky Studios Blue Sky Studios 1 
Introduction For 'Horton hears a Who' the character technical directors at Blue Sky Studios developed 
a new approach to procedural character rigs. Many rigging systems reduce or eliminate redundant work 
by importing pre existing components into a character scene or using scripts to generate components or 
entire rigs; our system goes several steps further. Essentially an object oriented approach, it allows 
the artist to assemble the template for a character rig independently of the character itself. The template 
exists as a network of nodes that defines each component's attributes and functionality, identifies argument 
joints and surfaces, and represents the components' relationships to each other and to their arguments. 
These highly modular networks offer consistency, robustness, and flexibility. 2 Evolution of the Blue 
Sky Character Rig On Blue Sky's second film Robots, the rigs were relatively simple, providing the animators 
with 236 keyable objects and 1,759 keyable attributes on the lead character Rodney. For Ice Age: The 
Meltdown, several common rig components were identified and automated, allowing for faster creation of 
more complex rigs; however, these were prone to varying from character to character as they were refined, 
and difficult to customize or update once installed. For Horton Hears a Who, senior members of the department 
identified early the huge demands of aesthetic performance, as well as a much larger character count. 
This meant that the rig components themselves would need to provide more functionality (the Mayor of 
Whoville has 804 keyable objects and 7,042 keyable attributes!) as well as facilitate efficient and frequent 
updates, allowing the artists to concentrate on appealing deformation. One key aspect of the new system 
would be to separate the aesthetics of the deforming model from the objects, attributes, and underlying 
structure the animators would use to create the performance, allowing parallel development of the deformation 
skeleton and the control rig. Artists would be able to refine shaping of the character independently 
from the network of components that would ultimately drive that deformation. 3 The Concept and Structure 
The system is designed to provide character TDs the ability to set up the control rig of a character 
efficiently and consistently. The control rig is made up of a variety of rig components (rc's) ­previously 
prototyped MEL scripts that construct all the groups, constraints, ik handles, etc. required to deliver 
specific animation control over a region of a character. A character rig begins as a Maya skeleton and 
a referenced model file containing surfaces deformed by a skin cluster and corrective blend shapes. The 
system requires that the deformation skeleton follow strict hierarchical, labeling, and naming conventions. 
Rig components are represented in three forms in the system. First, as the basic command line form of 
the rc MEL script, requiring input arguments (typically, a list of joints in the deformation skeleton 
and a set of optimization and behavior options); this will install a fully functional rig component in 
the current scene. This component is said to exist in the land of the living . Second, as a script node 
(rcSN) that contains argument attributes to provide the rc script for installation; the script node may 
exist in the scene without the component itself if it has been created, but not installed. This node 
is said to exist in the land of the dead . Third, as the component user interface (rcUI), which is parented 
to the rc manager; the rc manager is the top level interface for managing all components currently created 
in a scene (installed or uninstalled). Additional rcSNs were created to identify and connect argument 
transforms or attributes, as well as package groups of nodes to organize the scene file. Combined in 
networks called character components, the rcSNs provide object oriented assembly of a character rig. 
In a rig's dormant state, all the components and their relation to each other may be represented by as 
few as 500 nodes; these modular networks contain all the information required to call the most current 
rc scripts, creating extremely complex character rigs in about three minutes. The system foundation is 
a library of generic, common usage procedures (including commands to populate sets, modify attributes, 
assemble hierarchies, create and remove script nodes, install and uninstall components, and build user 
interfaces), updates to which can be propagated throughout a character component the next time it is 
installed. 4 Advantages of the New System The most immediately apparent advantage of procedural rigging 
is consistency. Shared libraries and a singular interface allow each technical director to avoid redundant 
or divergent work. In preproduction, an established criteria for continuity ensures that a component 
is not being developed in isolation, but rather is following a clear set of guidelines. During production, 
assembled networks of rig components can be shared across characters, reducing set up schedules and allowing 
for transfer of animation from one character to another of similar anatomical structure. The next and 
perhaps most compelling advantage of procedural rigging is robustness. As animation preferences alter 
with the requirements of the picture, additions to a particular rig component can be rapidly disseminated. 
This also creates an environment in which improvements to rig functionality are cumulative across pictures; 
for example, one component will become more customizable as features are added without sacrificing any 
of its original installation options. Combinations of components are also possible, meaning complexity 
can be added or removed in layers. The final and most exciting advantage is flexibility. The procedural 
rigging system provides insulation from proportional design alterations. A procedural network can be 
uninstalled, the skeleton modified to match the new form, and reinstalled at no cost. This also allows 
for automated builds on multiple characters; one character may be the generator for many similar variations. 
Finally, with the correct preparations made during the character evaluation process, shot-by-shot customization 
by the animator is now possible. A rig component can be added or removed depending on the demands of 
a particular performance. Copyright is held by the author / owner(s). SIGGRAPH 2008, Los Angeles, California, 
August 11 15, 2008. ISBN 978-1-60558-466-9/08/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1401094</article_id>
		<sort_key>620</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>48</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Art-directable dynamic-hair shells in <i>Madagascar: Escape 2 Africa</i>]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1401032.1401094</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401094</url>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>H.5.1</cat_node>
				<descriptor>Animations</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.6.1</cat_node>
				<descriptor>Systems theory</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.8</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341.10010349.10011310</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Simulation types and techniques->Simulation by animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341.10010346.10010347</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Simulation theory->Systems theory</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010507</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Graphics recognition and interpretation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061.10010063</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures->Computational geometry</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1098489</person_id>
				<author_profile_id><![CDATA[81100473781]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Stephan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Osterburg]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[PDI/DreamWorks]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098490</person_id>
				<author_profile_id><![CDATA[81548023765]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Nathaniel]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Dirksen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[PDI/DreamWorks]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098491</person_id>
				<author_profile_id><![CDATA[81548023769]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Rob]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Vogt]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[PDI/DreamWorks]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Scapel, N., and Boylan, T., 2005. The wig system. ACM SIGGRAPH Sketch.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1401095</article_id>
		<sort_key>630</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>49</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Merging bipedal and quadrupedal functionality into one rig for <i>Madagascar: Escape 2 Africa</i>]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1401032.1401095</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401095</url>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.6.1</cat_node>
				<descriptor>Systems theory</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.8</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.6.3</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010341</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010507</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Graphics recognition and interpretation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341.10010346.10010347</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Simulation theory->Systems theory</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1098492</person_id>
				<author_profile_id><![CDATA[81421597916]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Rex]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Grignon]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[PDI/DreamWorks]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098493</person_id>
				<author_profile_id><![CDATA[81421592400]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Milana]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Huang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[PDI/DreamWorks]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098494</person_id>
				<author_profile_id><![CDATA[81548023769]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Rob]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Vogt]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[PDI/DreamWorks]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1401096</article_id>
		<sort_key>640</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>50</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[Don't make me angry]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1401032.1401096</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401096</url>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>H.5.1</cat_node>
				<descriptor>Animations</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>H.5.1</cat_node>
				<descriptor>Artificial, augmented, and virtual realities</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.8</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003752.10010061.10010063</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures->Computational geometry</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341.10010349.10011310</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Simulation types and techniques->Simulation by animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010507</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Graphics recognition and interpretation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010392</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Mixed / augmented reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010866</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Virtual reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010866</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Virtual reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1098495</person_id>
				<author_profile_id><![CDATA[81100029745]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Hans]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Rijpkema]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Rhythm and Hues Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098496</person_id>
				<author_profile_id><![CDATA[81421596522]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Matt]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Derksen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Rhythm and Hues Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098497</person_id>
				<author_profile_id><![CDATA[81421595066]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Dante]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Quintana]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Rhythm and Hues Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Don't make me angry Hans Rijpkema* Matt Derksen Dante Quintana Rhythm and Hues Studios 1. Introduction 
When in the 70 s television series Dr. Banner changed into the Hulk, the transformation was shown through 
a quick montage of ripping clothes and close ups. The movie The Incredible Hulk however contains a sequence 
that shows the entire transformation process slowly and without any gaps in the series of events that 
changes the human sized Bruce Banner into the monster sized Incredible Hulk. This presentation discusses 
the methods used to non-uniformly transition between two different characters with different topologies, 
proportions and sizes. 2. Rigging The animator is presented with a single transformation rig which drives 
a full set of Bruce Banner deformations and a full set of Hulk deformations. The rig is essentially a 
rig for Banner which allows the bones to be stretched to Hulk proportions, but independently of muscle 
mass. The translation of the bones drive muscle deformations and a number of blend shapes to generate 
an intermediate deforming skin. The final skin then slides and relaxes over the deforming skin. In order 
to incorporate Hulk deformations a full Hulk rig with its original proportions is retargeted on the fly 
to the Banner animation in the background. All muscle firings, tendon motion, etc. are calculated for 
this hero Hulk rig. An inverse retarget then creates a non uniformly scaled down Hulk at the current 
Banner size and proportions. The differences between the full size and scaled down Hulk are analyzed 
and then applied to the Banner skin deformations. 3. Animation There are four main sets of attributes 
that drive the transformation: bone size, muscle growth, muscle striation and Hulk progression. The animator 
can control the timing of when and how each individual part of the body transforms. Theoretically a 6 
feet Bruce Banner can first transform to a Hulk shape at the same size and then be scaled up to the final 
9 feet size or it can transform first to a super sized 9 feet Banner and then morph into the final Hulk 
shape. Every body part can be animated to take any path in between these two extremes. Interactive feedback, 
while still being able to see the final deformation result so that the animator can adapt to the changing 
sizes of the individual limbs and body parts, is essential. 4. Lighting Look development was done on 
different stages of both the Bruce Banner character and the Hulk character. For each look a multi layered 
set of textures and displacement maps was created. They include maps for relaxed, flexed and emaciated 
Banner as well as relaxed and flexed Hulk. The Banner look development had to match the actor very closely 
since the sequence starts out with a live action Banner which then slowly changes into a full CG version. 
Based on the animation, the rig then generates a large number of animated lighting properties in the 
form of point attributes on the skin geometry. These attributes can be used to locally select between 
mixes of these different looks as well as control color shifts and large and small vein growth. The rendering 
generates multiple mattes and image layers which then can be manipulated further and more precise in 
the 2D compositing stage.  5. Conclusion A versatile rig presented animators with a relatively straightforward 
yet powerful set of controls to manage a complex nonlinear transformation sequence while generating many 
animated lighting properties at the same time.  * hans@rhythm.com mderksen@rhythm.com dante@rhythm.com 
Copyright is held by the author / owner(s). SIGGRAPH 2008, Los Angeles, California, August 11 15, 2008. 
ISBN 978-1-60558-466-9/08/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	</section>
	<section>
		<section_id>1401097</section_id>
		<sort_key>650</sort_key>
		<section_seq_no>15</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Science in 3D]]></section_title>
		<section_page_from>15</section_page_from>
	<article_rec>
		<article_id>1401098</article_id>
		<sort_key>660</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>51</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Visualizing ultra-scale data]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1401032.1401098</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401098</url>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.2</cat_node>
				<descriptor>Distributed/network graphics</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.1</cat_node>
				<descriptor>Parallel processing</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010520.10010521.10010542.10011714</concept_id>
				<concept_desc>CCS->Computer systems organization->Architectures->Other architectures->Special purpose systems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010169</concept_id>
				<concept_desc>CCS->Computing methodologies->Parallel computing methodologies</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1098498</person_id>
				<author_profile_id><![CDATA[81452595774]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Kwan-Liu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ma]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of California at Davis]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Visualizing Ultra-Scale Data Kwan-Liu Ma University of California at Davis  From left to right, visualization 
of cosmology, turbulent combustion, and supernova simulations at ultra scale. Parallel supercomputers 
enable scientists to model complex physical phenomena and chemical processes in efforts to answer some 
of the most important and difficult questions in science. The output from these simulations is so voluminous 
and complex that advanced visualization technologies are necessary to interpret the calculated results. 
Even though visualization technology has progressed significantly in recent years, we are barely capable 
of visualizing and analyzing terascale data to its full extent, and petascale datasets are on the horizon. 
The Institute for Ultrascale Visualization (http://ultravis.org), funded by the US Department of Energy, 
is a 5-year project involving several universities and national laboratories to address the upcoming 
petascale visualization challenges facing computational science and engineering. The Institute is expected 
to revolutionize the very process of scientific discovery by equipping scientists with tools that shed 
light on the knowledge hidden in previously incomprehensible datasets. This talk will present four key 
technologies: parallel visualization, knowledge-based visualization, in situ visualization, and visualization 
interfaces. Important advancements made by the Institute and best practices will be introduced. The objective 
is to foster exchange between the visualization and SIGGRAPH communities. Parallel visualization is absolutely 
needed if the goal is to see the full extent of the data at the highest possible fidelity, but not without 
its own challenges especially across our diverse scientific user community. The Institute brings together 
leading experts from visualization, high­performance computing, and science application areas to make 
parallel visualization technology a commodity for SciDAC scientists and the broader community. Most of 
the data sets can be reduced to some degree without removing the most important information in the data. 
While we can always employ data compression methods such as quantization and transform coding, a more 
effective way to reduce data is physically based. That is, with scientists knowledge about some specific 
feature of interest, it is also possible to use that knowledge to effectively reduce the data. The reduction 
may significantly facilitate subsequent data transfer and visualization. We can consider this as knowledge-assisted 
visualization. So far, data visualization has been almost exclusively done as a post-processing step. 
For runtime visualization, the cost of moving the simulation output to a visualization machine could 
be too high to make interactive visualization feasible. A better approach is not to move the data, or 
to keep the data that must be moved to a minimum. That is, both simulation and visualization calculations 
run on the same parallel supercomputer so the data can be shared. Such in situ processing can render 
images directly or extract features, which are much smaller than the full raw data, to store for on-the-fly 
or later examination. As a result, reducing both the data transfer and storage costs early in the data 
analysis pipeline can optimize the overall scientific discovery process. The petascale challenge can 
be best addressed with in situ visualization. Over the past twenty years, many novel visualization techniques 
have been invented but few have been deployed in production systems and tools. Why? The UI aspect of 
these techniques were often neglected, and the results were tools that are hard to use. The Institute 
has initiated several projects on the design and deployment of appropriate user interfaces for advance 
visualization techniques. In addition, it becomes clear that next generation visualization technology 
should be built upon further exploitation of human perception to simplify visualization, advanced hardware 
features to accelerate visualization calculations, and also make use of machine learning to reduce the 
complexity, size, and high­dimensionality of data. We are not too far from petascale and exascale computing. 
The investment made by the DOE SciDAC program in ultra-scale visualization is timely and ensures that 
challenges will be addressed. The Ultravis Institute has made some success in creating and deploying 
new visualization technologies. This presentation calls for further research and experimental studies 
involving a greater community. Copyright is held by the author / owner(s). SIGGRAPH 2008, Los Angeles, 
California, August 11 15, 2008. ISBN 978-1-60558-466-9/08/0008 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1401099</article_id>
		<sort_key>670</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>52</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Planning the separation of conjoined twins with 3D medical imaging, scientific visualization and anatomic illustration]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1401032.1401099</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401099</url>
		<abstract>
			<par><![CDATA[<p>This presentation will examine the intricate details of the visual preparation and planning to surgically separate conjoined twins. One of the rarest congenital malformations, conjoined twins occur approximately 1 in 200,000 live births. Nearly 184 surgical separations have been attempted through the year 2000. The overall success rate, where both twins survive, is approximately 25%. Within the past 24 months Mayo Clinic Rochester has cared for and performed three successful separation surgeries for three different sets of conjoined twins.</p> <p>We'll explore the circumstances surrounding one of those cases and the process of how the visual study transpired during their seventy-five days of hospitalization prior to surgery. Nearly 6,000 radiographic images where acquired, two detailed volumerendered visualization studies were compiled, three different 3D stereolithographic models were developed and five individualized anatomic illustrations created from a vast array of medical images. All developed in an effort to educate a care team of over 70 people.</p> <p>The various state-of-the-art visuals compiled of the twin's anatomy provided a comprehensive "roadmap" for the surgeons, helping them delineate the anatomic structures and formulate an operate plan for a complex separation attempt. The large multidisciplinary care team needed detailed structural knowledge in order to clarify preoperative preparations. For instance, accurate three-dimensional life-size models of the twins anatomy provided critical spacial planning and the anatomic illustrations, printed large, were studied and ultimately posted inside the operating room providing an accurate reference during surgery.</p> <p>This presentation will provide in-depth insight into the complete visual approach to surgical planning, the resulting visuals and how they were utilized before, during and after the surgery.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>J.3</cat_node>
				<descriptor>Health</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.10</cat_node>
				<descriptor>Volumetric</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.8</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.4.9</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010444.10010446</concept_id>
				<concept_desc>CCS->Applied computing->Life and medical sciences->Consumer health</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010507</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Graphics recognition and interpretation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010444.10010449</concept_id>
				<concept_desc>CCS->Applied computing->Life and medical sciences->Health informatics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010225</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision tasks</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010240.10010242</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision representations->Shape representations</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1098499</person_id>
				<author_profile_id><![CDATA[81421595372]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Robert]]></first_name>
				<middle_name><![CDATA[F.]]></middle_name>
				<last_name><![CDATA[Morreale]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Mayo Clinic, Rochester, MN]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098500</person_id>
				<author_profile_id><![CDATA[81339509691]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Michael]]></first_name>
				<middle_name><![CDATA[A.]]></middle_name>
				<last_name><![CDATA[King]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Mayo Clinic, Rochester, MN]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098501</person_id>
				<author_profile_id><![CDATA[81421601118]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Jane]]></first_name>
				<middle_name><![CDATA[M. S.]]></middle_name>
				<last_name><![CDATA[Matsumoto]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Mayo Clinic, Rochester, MN]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098502</person_id>
				<author_profile_id><![CDATA[81421597610]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Christopher]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Moir]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Mayo Clinic, Rochester, MN]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098503</person_id>
				<author_profile_id><![CDATA[81100424747]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Richard]]></first_name>
				<middle_name><![CDATA[A.]]></middle_name>
				<last_name><![CDATA[Robb]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Mayo Clinic, Rochester, MN]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098504</person_id>
				<author_profile_id><![CDATA[81365591563]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Kevin]]></first_name>
				<middle_name><![CDATA[E.]]></middle_name>
				<last_name><![CDATA[Bennet]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Mayo Clinic, Rochester, MN]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Spielmann AL, Freed KS, Spritzer CE. MRI of conjoined twins illustrating advances in fetal imaging. <i>J Comput Assist Tomogr</i>. 2001;25:88--90.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Spitz L, Kiely EM. Conjoined twins. <i>JAMA</i> 2003; 289: 1307--10]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Hoyle RM. Surgical separation of conjoined twins. <i>Surg Gynecol Obstet</i>. 1990;170:549--562.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Spitz L, Kiely EM. Experience in the management of conjoined twins. <i>Br J Surg.</i> 2002;89:1188--1192.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Cywes S, Millar AJW, Rode H, Brown RA. Conjoined twins: the Cape Town experience. <i>Pediatr Surg Int.</i> 1997;12:234--248.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Spitz L, Kiely EM. Success rate for surgery of conjoined twins {letter}. <i>Lancet</i>. 2000;356:1765.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Kingston CA, McHugh K, Kumaradevan J, et al. Imaging in the preoperative assessment of conjoined twins. <i>Radiographics</i>. 2001;21:1187--1208.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Kermer C, Rasse M, Lagogiannis G, Undt G, Wagner A, Millesi W. Colour stereolithography for planning complex maxillofacial tumour surgery. <i>J Craniomaxillofac Surg.</i>. 1998 Dec;26(6):360--2.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>620396</ref_obj_id>
				<ref_obj_pid>619005</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Elliot K. Fishman, Brian S. Kuszyk, David G. Heath, Luomin Gao, Brian Cabral, Surgical Planning for Liver Resection, <i>Computer</i>, v. 29 n. 1, p. 64--72, January 1996]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Linney AD, Tan AC, Richards R, Gardener J, Grindrod S, Moss JP. Three-dimensional visualization of data on human anatomy: diagnosis and surgical planning. <i>J Audiov Media Med.</i> 1993 Jan; 16(1):4--10.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Stoker NG, Mankovich NJ, Valentino D. Stereolithographic models for surgical planning: preliminary report. <i>J Oral Maxillofac Surg.</i> 1992 May; 50(5):466--71.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Mankovich NJ, Samson D, Pratt W, Lew D, Beumer J 3rd. Surgical planning using three-dimensional imaging and computer modeling. <i>Otolaryngol Clin North Am.</i> 1994 Oct; 2 (5):875--89. Review.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Mankovich NJ, Robertson DR, Cheeseman AM. Three-dimensional image display in medicine. <i>J Digit Imaging</i>. 1990 May; 3(2):69--80.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Planning The Separation of Conjoined Twins with 3D Medical Imaging, Scienti.c Visualization and Anatomic 
Illustration Robert F. Morreale, Michael A. King, Jane M.S. Matsumoto, Christopher Moir, Richard A. Robb, 
Kevin E. Bennet Mayo Clinic, Rochester, MN Abstract: This presentation will examine the intricate details 
of the visual preparation and planning to surgically separate conjoined twins. One of the rarest congenital 
malformations, conjoined twins occur approximately 1 in 200,000 live births. Nearly 184 surgical separations 
have been attempted through the year 2000. The overall success rate, where both twins survive, is approximately 
25%. Within the past 24 months Mayo Clinic Rochester has cared for and performed three successful separation 
surgeries for three different sets of conjoined twins. We ll explore the circumstances surrounding one 
of those cases and the process of how the visual study transpired during their seventy-.ve days of hospitalization 
prior to surgery. Nearly 6,000 radiographic images where acquired, two detailed volume­rendered visualization 
studies were compiled, three different 3D stereolithographic models were developed and .ve individualized 
anatomic illustrations created from a vast array of medical images. All developed in an effort to educate 
a care team of over 70 people. The various state-of-the-art visuals compiled of the twin s anatomy provided 
a comprehensive roadmap for the surgeons, helping them delineate the anatomic structures and formulate 
an operate plan for a complex separation attempt. The large multi­disciplinary care team needed detailed 
structural knowledge in order to clarify preoperative preparations. For instance, accurate three-dimensional 
life-size models of the twins anatomy provided critical spacial planning and the anatomic illustrations, 
printed large, were studied and ultimately posted inside the operating room providing an accurate reference 
during surgery. This presentation will provide in-depth insight into the complete visual approach to 
surgical planning, the resulting visuals and how they were utilized before, during and after the surgery. 
References: SPIELMANN AL, FREED KS, SPRITZER CE. MRI of conjoined twins illustrating advances in fetal 
imaging. J Comput Assist Tomogr. 2001;25:88-90. SPITZ L, KIELY EM. Conjoined twins. JAMA 2003; 289: 1307 
10 HOYLE RM. Surgical separation of conjoined twins. Surg Gynecol Obstet. 1990;170:549-562. SPITZ L, 
KIELY EM. Experience in the management of conjoined twins. Br J Surg. 2002;89:1188-1192. CYWES S, MILLAR 
AJW, RODE H, BROWN RA. Conjoined twins: the Cape Town experience. Pediatr Surg Int. 1997;12:234-248. 
SPITZ L, KIELY EM. Success rate for surgery of conjoined twins [letter]. Lancet. 2000;356:1765. KINGSTON 
CA, MCHUGH K, KUMARADEVAN J, ET AL. Imaging in the preoperative assessment of conjoined twins. Radiographics. 
2001;21:1187-1208. KERMER C, RASSE M, LAGOGIANNIS G, UNDT G, WAGNER A, MILLESI W. Colour stereolithography 
for planning complex maxillofacial tumour surgery. J Craniomaxillofac Surg.. 1998 Dec;26(6):360-2. ELLIOT 
K. FISHMAN , BRIAN S. KUSZYK , DAVID G. HEATH , LUOMIN GAO , BRIAN CABRAL, Surgical Planning for Liver 
Resection, Computer, v.29 n.1, p.64-72, January 1996 LINNEY AD, TAN AC, RICHARDS R, GARDENER J, GRINDROD 
S, MOSS JP. Three-dimensional visualization of data on human anatomy: diagnosis and surgical planning. 
J Audiov Media Med. 1993 Jan;16(1):4-10. STOKER NG, MANKOVICH NJ, VALENTINO D. Stereolithographic models 
for surgical planning: preliminary report. J Oral Maxillofac Surg. 1992 May;50(5):466-71. MANKOVICH 
NJ, SAMSON D, PRATT W, LEW D, BEUMER J 3RD. Surgical planning using three-dimensional imaging and computer 
modeling. Otolaryngol Clin North Am. 1994 Oct; 2 (5):875-89. Review. MANKOVICH NJ, ROBERTSON DR, CHEESEMAN 
AM. Three-dimensional image display in medicine. J Digit Imaging. 1990 May;3(2):69-80. Copyright is 
held by the author / owner(s). SIGGRAPH 2008, Los Angeles, California, August 11 15, 2008. ISBN 978-1-60558-466-9/08/0008 
 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1401100</article_id>
		<sort_key>680</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>53</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Atta texana leafcutting ant colony]]></title>
		<subtitle><![CDATA[a view underground]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1401032.1401100</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401100</url>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>F.2.2</cat_node>
				<descriptor>Geometrical problems and computations</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.8</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010507</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Graphics recognition and interpretation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061.10010063</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures->Computational geometry</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061.10010063</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures->Computational geometry</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1098505</person_id>
				<author_profile_id><![CDATA[81361605368]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Carol]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[LaFayette]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Texas A&M University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098506</person_id>
				<author_profile_id><![CDATA[81100304539]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Frederic]]></first_name>
				<middle_name><![CDATA[I.]]></middle_name>
				<last_name><![CDATA[Parke]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Texas A&M University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098507</person_id>
				<author_profile_id><![CDATA[81421595816]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Carl]]></first_name>
				<middle_name><![CDATA[J.]]></middle_name>
				<last_name><![CDATA[Pierce]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[St. Lawrence University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098508</person_id>
				<author_profile_id><![CDATA[81421601035]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Tatsuya]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nakamura]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Starz Animation, Toronto]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098509</person_id>
				<author_profile_id><![CDATA[81421599116]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Lauren]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Simpson]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Texas A&M University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Leckebusch, J., 2001. Investigating the True Resolution and Three-Dimensional Capabilities of Ground-penetrating Radar Data in Archaeological Surveys: Measurements in a Sand Box, <i>Arch. Prospection</i> 8, 29--40.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Moser, J. C., 1963. Contents and Structure of Atta texana Nest in Summer, <i>Annals of the Entomological Society of America</i> 56, 3, 286--291.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Nakamura, T., 2006. <i>The Soprants: Conceptual and Technical Framework for a 3D Interactive Video Game</i>. Masters Thesis, Texas A&M University, 23--27.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Parke, F. I., 2005. Lower Cost Spatially Immersive Visualization for Human Environments, <i>Landscape and Urban Planning</i> 73, 2--3, 234--243.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Atta texana leafcutting ant colony: a view underground Carol LaFayette, MFA* Frederic I. Parke, Ph.D. 
 Carl J. Pierce Tatsuya Nakamura§ Lauren Simpsonß Associate Professor Professor M.S., P.G., Geophysics 
Software Developer Graduate Candidate Visualization Department Visualization Department St. Lawrence 
University Starz Animation Visualization Department Texas A&#38;M University Texas A&#38;M University 
Toronto Texas A&#38;M University  Figure 1. Atta Texana colony on an immersive visualization system 
1. Introduction The Atta project maps tunnels and chambers of a vast leafcutting ant colony. A Ground 
Penetrating Radar scan was translated into a 3D model that can be viewed on an immersive visualization 
system, scaling the viewer to ant size. The scanning is nondestructive and is the . rst time GPR has 
been used to map a living ant colony. To achieve this goal, the project combines the site-speci.c nature 
of an indexical system, GPR, with the ability of an algorithm to parse the data. The model retains a 
formal connection with its subject and can be distributed and viewed in many different ways. 2. Exposition 
One of Texas s smallest natives is also one of its largest: myrmecologists refer to ant colonies as superorganisms. 
Atta texana harvests tree leaves to farm a fungus in underground cavities that can spread over more than 
an acre of land and reach to great depths, with over a million ants in residence. Excavated leafcutting 
ant nests have proven large enough to contain a 3-story house. [Moser 1963] Previous attempts to model 
ant colonies have been undertaken by myrmecologist Walter Tschinkel, whose technique involves pouring 
casting material into the nest, digging it up and piecing it back together. Another method involves using 
a bulldozer to scrape away successive layers of soil and measuring the diameter of the holes. Tunnels 
collapse with this method and cannot be tracked. If measurement is the aim, either approach could be 
used. But our goal was to gain a unique view of this subterranean architecture using a method that would 
not disturb the colony. Ground Penetrating Radar (GPR) sends high frequency radar pulses from a surface 
antenna into the ground. Elapsed time between when the pulse is transmitted re.ected from buried materials 
or sediment and soil changes and when it is received, is measured. The sender and receiver are moved 
along the surface, following transects of a grid. GPR scans contain noise (soil composition, radio interference, 
and magnetic properties of substances) which can interfere with the results. [Leckebusch 2001] The data 
was . rst .ltered to reduce noise. We then differentiated voids from soil and other materials by targeting 
the velocity of radar waves in air (the speed of light 3 x 108 meters per second) in relation to other 
geologic materials (with an average velocity * e-mail: lurleen@viz.tamu.edu e-mail: parke@viz.tamu.edu 
e-mail: cpierce@stlawu.edu § e-mail: tatsuyan@acm.org ß e-mail: lrsimpson@gmail.com of 1 x 108 meters 
per second). An algorithm processed isosurfaces from density values distributed within the overall volume. 
The result formed polygons nested in layers corresponding to different densities in the GPR scans. [Nakamura 
2006] Thus, one can enable successive layers surrounding voids to hint at tunnel structure and fungus 
caches. Scale changes provided by an immersive visualization system transformed the viewer into the size 
of an ant surrounded by the tunnel architecture. [Figure 1] [Parke 2005] 3. Conclusion GPR delivers 
an indexical signal, formed by the action of radar pulses passing through substances over time and distance. 
In this way it can be loosely compared to a photograph, formed by the pattern of light striking a photosensitive 
surface. Photographer Henri Cartier-Bresson described taking a photograph as .xing a decisive moment, 
a con. uence of the artist s position in relation to the geometry of an unfolding event. Indexical signs 
offer what Roland Barthes termed the punctum, a speci.c feature of the subject that cannot be separated 
from time or place. Conversely, digital representations often rely on generalization of physical phenomena. 
Gravity, water, or terrain are simulated with algorithms, freed from substance and geographic locale. 
Due to its mathematical structure, an algorithm enables .uid recomposition of one form into another. 
In mapping an Atta nest, a connection is maintained with this particular subject deep in the soil of 
a Texas .eld, while the colony architecture is simulated in a medium distributable across time and space. 
References LECKEBUSCH, J., 2001. Investigating the True Resolution and Three-Dimensional Capabilities 
of Ground-penetrating Radar Data in Archaeological Surveys: Measurements in a Sand Box, Arch. Prospection 
8, 29-40. MOSER, J. C., 1963. Contents and Structure of Atta texana Nest in Summer, Annals of the Entomological 
Society of America 56, 3, 286-291. NAKAMURA, T., 2006. The Soprants: Conceptual and Technical Framework 
for a 3D Interactive Video Game. Masters Thesis, Texas A&#38;M University, 23-27. PARKE, F. I., 2005. 
Lower Cost Spatially Immersive Visualization for Human Environments, Landscape and Urban Planning 73, 
2-3, 234-243. &#38;#169;2007 Texas A&#38;M University. Data and documentation is freely available for 
noncommercial purposes. Copyright is held by the author / owner(s). SIGGRAPH 2008, Los Angeles, California, 
August 11 15, 2008. ISBN 978-1-60558-466-9/08/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	</section>
	<section>
		<section_id>1401101</section_id>
		<sort_key>690</sort_key>
		<section_seq_no>16</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Rigging outside the box]]></section_title>
		<section_page_from>16</section_page_from>
	<article_rec>
		<article_id>1401102</article_id>
		<sort_key>700</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>54</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Simulating the devolved]]></title>
		<subtitle><![CDATA[finite elements on WALL&#183;E]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1401032.1401102</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401102</url>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>G.1.8</cat_node>
				<descriptor>Finite element methods</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>H.5.1</cat_node>
				<descriptor>Animations</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>H.5.1</cat_node>
				<descriptor>Artificial, augmented, and virtual realities</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.6.1</cat_node>
				<descriptor>Systems theory</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.8</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010392</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Mixed / augmented reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341.10010346.10010347</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Simulation theory->Systems theory</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010507</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Graphics recognition and interpretation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003714.10003715.10003718</concept_id>
				<concept_desc>CCS->Mathematics of computing->Mathematical analysis->Numerical analysis->Computations in finite fields</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010866</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Virtual reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010866</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Virtual reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341.10010349.10011310</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Simulation types and techniques->Simulation by animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1098510</person_id>
				<author_profile_id><![CDATA[81100569226]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Geoffrey]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Irving]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Pixar Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098511</person_id>
				<author_profile_id><![CDATA[81421600504]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Ryan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kautzman]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Pixar Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098512</person_id>
				<author_profile_id><![CDATA[81100193621]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Gordon]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Cameron]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Pixar Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098513</person_id>
				<author_profile_id><![CDATA[81421601260]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Jiayi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Chong]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Pixar Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1028541</ref_obj_id>
				<ref_obj_pid>1028523</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Irving, G., Teran, J., and Fedkiw, R. 2004. Invertible finite elements for robust simulation of large deformation. <i>In Proc. of the ACM SIGGRAPH/Eurographics Symp. on Comput. Anim.</i>, ACM, 131--140.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Simulating the Devolved: Finite Elements on WALL E GeoffreyIrving* Ryan Kautzman* Gordon Cameron* Jiayi 
Chong* Pixar Animation Studios For the human characters on Pixar s WALL E, an automatic sys­tem for 
secondary ballistic motion was needed to add an extra level of realism and to lighten theburden on animators. 
This taskwas complicated by the nonphysical nature of the underlying models; the secondary motion should 
look realistic even if the primary an­imation is exaggerated and stylized. To accomplish this, we used 
tetrahedral .nite element simulation together with several control mechanisms to target the simulation 
pose towards the animation. During pre-production, the human characters went through several stages of 
development. Early versions were made of transparent, gelatinous material instead of .esh, and were unrecognizable 
as humans. This posed interesting problems for animation and sim­ulation. How does one act withafacialperformance 
ona material with no real structure? How does the simulator preserve animation detail and still allow 
the material to undergo large deformation? As the story evolved the jelly creatures became more human, 
and the requirements of the simulator changed. Detail preservation grew easier as key details in the 
face and hands were now adjacent to bones and required less secondary motion,but maintaining anima­tion 
silhouettes elsewhere in the body became more important. 1 Finite Element Simulation The initial character 
designs of the devolved humans involved ex­treme deformations for both primary and secondary motion, 
so our simulator had to behave smoothly and robustly in situations with many degenerate and inverted 
tetrahedra. We therefore chose to use the .nite element method of [Irving et al. 2004] for the internal 
physics of the material as opposed to a mass-spring system. Finite elements also enabled us to capture 
the incompressible and biphasic nature of biological tissue. Biphasic response allowed the material to 
be soft enough for interesting surface ripplingbut harden under large deformation to avoid undesirable 
mesh collapse. These phe­nomena are dif.cult to capture with mass-spring models. Starting from an animation 
sequence de.ned only on a surface mesh, we needed to add dynamic secondary motion without de­viating 
from animation poses when the character was not moving. In particular, static poses had to be well matched 
even though the shapes from animation were notphysically based. At the same time, the simulation had 
to take over and deform the character radically under suf.cientexternal impulse.To achieve this, we .rst 
diffused the target animation to the interior vertices of the mesh by solving a Laplace equation discretized 
on the tetrahedra using the surface animation as a boundary condition. We then applied two comple­mentary 
control mechanisms: a system of soft constraints pulling each simulated point towards its target, and 
a direct modi.cation to the rest state of each tetrahedron to match the current shape of the animation. 
With constraint forces alone the large changes in shape between different poses of the character would 
be constantly foughtbythe .nite element forcesinthe simulation, smoothingout deformation around joints 
and adding creases in undesirable places. Continuously altering the rest state allowed the simulation 
to re­spect these natural shape changes without damping out interesting dynamic motion. The deformation 
gradient between the original and modi.ed rest state was clamped in logarithmic space to avoid sliver 
tetrahedra in extreme poses. The soft constraint springs were integrated implicitly to allow their stiffness 
to be ramped arbitrarily high to match animation exactly in certain areas of the mesh. Since the springs 
at different points *e-mail: {irving,ryan,gocam,jiayi}@pixar.com Copyright is held by the author / owner(s). 
SIGGRAPH 2008, Los Angeles, California, August 11 15, 2008. ISBN 978-1-60558-466-9/08/0008  Figure 1: 
Final and early versions of animated characters with simulated secondary motion. c @Disney / Pixar. All 
rights reserved. are independent this did not require a linear system solve. All other integration was 
explicit for the elastic forces and implicit on the damping forces to preserve detail in the simulation. 
 2 Pipeline We built a streamlined interface for animators to run simulations and interactively adjust 
results. Real time kinematic colliders were used to give animators direct feedback during interaction 
with ob­jects, and the same geometry was then fed to the simulation for dynamic collisions. Spatially 
varying material properties were mapped from surfaces to the mesh interior via a Laplace equation and 
then adjusted withavolumetric paint tool. All simulations were run in parallel using MPI. Most simulations 
involved one or twocharacters and were restricted to adding only secondary motion. In shots with large 
crowds of in­teracting humans, the simulator also controlled the global position and orientation of each 
character. Animators created a variety of stationary animation cycles, and the position, orientation, 
and mo­mentum of the animation were continuously adjusted to match the simulation before targeting forces 
were applied. Wewould like to thank BillWise, Bill Shef.er, and Sajan Skaria for character rigging and 
enthusiastic support. References IRVING, G., TERAN, J., AND FEDKIW, R. 2004. Invertible .nite ele­ ments 
for robust simulation of large deformation. In Proc. of the ACM SIGGRAPH/Eurographics Symp. on Comput. 
Anim.,ACM, 131 140.  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1401103</article_id>
		<sort_key>710</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>55</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Large scale foliage animation for <i>The Ruins</i>]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1401032.1401103</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401103</url>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>H.5.1</cat_node>
				<descriptor>Animations</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>H.5.1</cat_node>
				<descriptor>Artificial, augmented, and virtual realities</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.6.1</cat_node>
				<descriptor>Systems theory</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.8</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341.10010349.10011310</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Simulation types and techniques->Simulation by animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341.10010346.10010347</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Simulation theory->Systems theory</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010507</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Graphics recognition and interpretation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010392</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Mixed / augmented reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010866</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Virtual reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010866</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Virtual reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1098514</person_id>
				<author_profile_id><![CDATA[81421596511]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Carsten]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kolve]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Rising Sun Pictures]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098515</person_id>
				<author_profile_id><![CDATA[81314491744]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Christoph]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sprenger]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Rising Sun Pictures]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098516</person_id>
				<author_profile_id><![CDATA[81421595810]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Malcolm]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Humphreys]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Rising Sun Pictures]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098517</person_id>
				<author_profile_id><![CDATA[81421601300]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Fred]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Chapman]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Rising Sun Pictures]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[PERLIN, Ken 2002. Improving Noise. In Computer Graphics Vol. 35 No. 3.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[MANN, Julian 2007, Angular Spring System for Skeletal Animation, Alias Maya plug-in]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Large Scale Foliage Animation for The Ruins Carsten Kolve Christoph Sprenger Malcolm Humphreys Fred 
Chapman Rising Sun Pictures  Figure 1: Hundreds of vines attacking the main protagonists in the film 
The Ruins. Diagnostic pass used to verify correct collision behaviour and final shot. &#38;#169;Dreamworks. 
All rights reserved. Carnivorous vines with an appetite for teenagers are the main digital characters 
in Dreamworks' The Ruins. Animating thousands of leaves and flowers attached to these vines posed a unique 
challenge not only did they have to sway, collide and react to external forces in a believable way but 
at times they were also required to perform in very un-plantlike ways. While background leaves were realised 
using a dynamic hair approach, leaves in the mid and foreground (of which there could be up to 850) needed 
to be more controllable. In this sketch we present the animation pipeline developed for this type of 
foliage: a modular control rig system that allowed layering of skeletal deformation derived from procedural-and 
key-framing techniques. We also show how the results of soft body simulation were incorporated into this 
work flow. 1. Skeletal Animation Layers All leaves attached to vines were animated using individual skeleton 
hierarchies. On the one hand animators demanded total control over the final result but on the other 
hand the sheer number of leaves prohibited a fully manual approach. To solve this conflict the system 
animLayers was devised. It enabled us to layer animations from different sources. The basis of the system 
were rig modules: Rig modules were realised as Autodesk Maya node networks that would allow the user 
to generate animation using different modules: FK controls for hand animating the performance, custom 
angular spring system for joints [Mann 2007] used to generate swaying motion derived from underlying 
vine movement, procedural joint channel animation utilising modulated Improved Perlin Noise [Perlin 2002] 
and a collision resolution system Each of these modules allowed blending with input joint animation. 
This enabled artists to create custom stacks of rigs for each vine appendage based on the current shot 
needs. Such a rig-stack could always be collapsed by caching the resulting performance to disk as an 
animation cache. The complex stack of rigs could then be replaced by a much lighter single cache-in rig. 
Copyright is held by the author / owner(s). SIGGRAPH 2008, Los Angeles, California, August 11 15, 2008. 
ISBN 978-1-60558-466-9/08/0008 Animation data was treated as a series of transformation matrix arrays 
describing the joint motion and be transferred from one rig module to the next using a custom Maya data 
type. To ensure that the transformations could be applied even if joint orientation would differ from 
rig to rig, they were always stored relative to a reference joint chain also used for binding the geometry. 
Specific interface nodes transformed the animation data in and out of the current rig's joint space. 
Custom user interfaces were developed to manage the multitude of rigs and animation caches used in a 
scene. With hundreds of leaves of various types being visible in a single shot, managing this list became 
one main objective for the Shot TD. 2. Mesh Driven Bones Resolving the complex leaf-leaf as well as 
leaf-prop collisions within the skeletal animation pipeline described above was initially planned to 
be done manually using an FK rig where needed a plan that turned out not to be practical when the amount 
of visible intersections became apparent. Instead a collision rig module based on Maya's Nucleus dynamics 
solver was developed. We mapped the vertex deformation produced by a soft body simulation onto a joint 
hierarchy: A mesh was driven by the incoming animation data and served as the target for a second mesh 
of equal topology deformed by the dynamics simulation. This collision mesh in turn would then be used 
by the custom node meshDrivenBones to produce joint channel animation for the output of the rig module. 
MeshDrivenBones drove the complete joint hierarchy using a combination of alignment vectors derived from 
the collision mesh. Starting at the root and working its way recursively through the skeleton, each joint 
matrix was constructed using a forward vector pointing from the current joint's base to the mean position 
of specific target vertices and an up vector derived from the mean vector of specific face normals. 
Figure 2: Complex leaf collisions resolved using the meshDrivenBones system. &#38;#169;Dreamworks. All 
rights reserved. 3. Discussion Using a modular approach to rigging simplified the way animation was 
used throughout the pipeline and gave us the flexibility to easily adjust the results at every stage 
of the process. On demand creation and collapsing of control rig stacks allowed us to work with huge 
numbers of individual vine appendages. References PERLIN, Ken 2002. Improving Noise. In Computer Graphics 
Vol. 35 No. 3. MANN, Julian 2007, Angular Spring System for Skeletal Animation, Alias Maya plug-in  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1401104</article_id>
		<sort_key>720</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>56</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Optimized multi strand beard setup for Shrek the Halls]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1401032.1401104</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401104</url>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>F.2.2</cat_node>
				<descriptor>Computations on discrete structures</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>F.2.2</cat_node>
				<descriptor>Geometrical problems and computations</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.8</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061.10010063</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures->Computational geometry</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010507</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Graphics recognition and interpretation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1098518</person_id>
				<author_profile_id><![CDATA[81309496516]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Lucia]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Modesto]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[PDI/DreamWorks]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098519</person_id>
				<author_profile_id><![CDATA[81335489731]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Daniel]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Dawson]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Kickstand LLC]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1218077</ref_obj_id>
				<ref_obj_pid>1218064</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Hadap, S. Oriented strands: dynamics of stiff multi-body system. In <i>SCA'06, 91--100</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1401105</article_id>
		<sort_key>730</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>57</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[Offset curve deformation from skeletal animation]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1401032.1401105</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401105</url>
		<categories>
			<primary_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Curve, surface, solid, and object representations</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>F.2.2</cat_node>
				<descriptor>Geometrical problems and computations</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Geometric algorithms, languages, and systems</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Animation</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061.10010063</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures->Computational geometry</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010401</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Volumetric models</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010399</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Parametric curve and surface models</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1098520</person_id>
				<author_profile_id><![CDATA[81541402356]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Arthur]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gregory]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Sony Pictures Imageworks]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098521</person_id>
				<author_profile_id><![CDATA[81537949656]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Dan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Weston]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Sony Pictures Imageworks]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Gregory, A., and Weston, D. 2008. Efficient offset curve deformation from skeletal animation. Tech. rep., Sony Pictures Imageworks.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1230107</ref_obj_id>
				<ref_obj_pid>1230100</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Kavan, L., Collins, S., &#381;&#225;ra, J., and O'Sullivan, C. 2007. Skinning with dual quaternions. In <i>I3D '07: Proceedings of the 2007 symposium on Interactive 3D graphics and games</i>, ACM, 39--46.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>280946</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Singh, K., and Fiume, E. 1998. Wires: a geometric deformation technique. In <i>Proceedings of ACM SIGGRAPH 98</i>, ACM, 405--414.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Offset Curve Deformation from Skeletal Animation Arthur Gregory Dan Weston Sony Pictures Imageworks 
 1 Introduction Skin deformation based on an underlying skeleton is a critical partof the modern animation 
pipeline. Linear blend skinning is fast butsuffers volume loss at a bend and candy wrapper pinching undertwist. 
More complex methods using dual quaternions [Kavan et al.2007] and bind-point offset from intermediate 
curves [Singh andFiume 1998] are nearly as fast and reduce these artifacts, but dualquaternions are limited 
only to rigid transformations and bind-pointtechniques continue to suffer from offset distortion, which 
increaseswith distance from the bind point, causing uneven stretching andself-intersection on the inside 
of a bend when offsets exceed the radius of curvature. Our OCD algorithm attempts to avoid this trade-off 
of cost andquality by abandoning bind-point techniques in favor of offset curves. In our method, each 
surface point gets its own separateoffset curve that intersects the surface at that point. With this 
tech­nique, we get the low cost of a bind point technique by using arc­segments (elbow, knuckles) or 
B-splines (shoulder, chest, neck),both of which are fast to evaluate. Examples of both are presentedand 
compared below. A complete discussion can be found in ourtechnical report [Gregory and Weston 2008]. 
 Undeformed Linear blend Dual quaternion Maya wire deformer OCD arc-segments OCD B-Splines  2 Basic 
Approach At bind time, we record the bind parameter (the fractional arc­length of the bind point along 
the in.uence curve) and offset neededto reproduce the bind position. This happens by design (for arc­segments) 
and by search (for B-splines). The arc-segment method creates curves by offsetting segment pairsparallel 
to the animation skeleton, then connecting them by tracingout ellipses in the halfplane of the common 
in.uence joint. Thereis exactly one smallest circular arc connecting these coplanar offsetsegments for 
which the offset curve is everywhere G1-continuous.The arc-length is known in closed form and evaluated 
at the pointof fractional arc-length given by the bind parameter. The spline method uses a cubic B-spline 
(with nonuniformly­spaced knots) because of its inherent smoothness and local control(any given point 
on a cubic B-spline being linear in and fully de­.ned by its two nearest control points on either side). 
We simplyoffset each spline control point by the same constant vector from itscorresponding in.uence 
control point position in the latter s ownlocal space. The artist is free to displace the offset curve 
along andnormal to its local twist axis, as well as parametric rescaling andsliding. At deform time, 
the surface vertex is evaluated on the offset curve at the bind parameter. There may be multiple in.uence 
curves, inwhich case the surface point deformation is calculated indepen­dently for each in.uence curve, 
then linearly blended using artistsupplied weights. Initial default weights are computed automati­cally 
based on distance. 3 Discussion We prefer the B-spline offset model over the arc-segment offsetmodel. 
It is more general, intuitive, and amenable to artist con­trol. Although it may require more tweaking 
of parameters to getthe same look as the arc-segment model out of the box on knuckleand elbow bends, 
we are investigating better methods for supplyingdefault parameter settings. When exact control over 
the look of a given pose is needed (moretypically in VFX than feature animation), the OCD base deforma­tion 
technique works well with secondary pose-space deformation.  References GREGORY, A., AND WESTON, D. 
2008. Ef.cient offset curve deformation from skeletal animation. Tech. rep., Sony PicturesImageworks. 
KAVAN, L., COLLINS, S., Z.´ ARA, J., AND O SULLIVAN, C. 2007. Skinning with dual quaternions. In I3D 
07: Proceedings of the2007 symposium on Interactive 3D graphics and games, ACM, 39 46. SINGH, K., AND 
FIUME, E. 1998. Wires: a geometric deformation technique. In Proceedings of ACM SIGGRAPH 98, ACM, 405 
414. Copyright is held by the author / owner(s). SIGGRAPH 2008, Los Angeles, California, August 11 15, 
2008. ISBN 978-1-60558-466-9/08/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	</section>
	<section>
		<section_id>1401106</section_id>
		<sort_key>740</sort_key>
		<section_seq_no>17</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Bend me break me]]></section_title>
		<section_page_from>17</section_page_from>
	<article_rec>
		<article_id>1401107</article_id>
		<sort_key>750</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>58</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Rope bridge animation system in <i>Kung Fu Panda</i>]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1401032.1401107</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401107</url>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>H.5.1</cat_node>
				<descriptor>Animations</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.6.7</cat_node>
				<descriptor>Environments</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.8</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010507</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Graphics recognition and interpretation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341.10010349.10011310</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Simulation types and techniques->Simulation by animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341.10010366.10010367</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Simulation support systems->Simulation environments</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1098522</person_id>
				<author_profile_id><![CDATA[81100269613]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Amaury]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Aubel]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[DreamWorks Animation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098523</person_id>
				<author_profile_id><![CDATA[81421594027]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Sven]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Pohle]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[PDI/DreamWorks]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098524</person_id>
				<author_profile_id><![CDATA[81421594142]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Mitch]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Cockerham]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[DreamWorks Animation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098525</person_id>
				<author_profile_id><![CDATA[81421597761]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Matt]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Steele]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[DreamWorks Animation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1401108</article_id>
		<sort_key>760</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>59</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Procedural fracturing and debris generation for <i>Kung-Fu Panda</i>]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1401032.1401108</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401108</url>
		<abstract>
			<par><![CDATA[<p>Several sequences in <i>Kung-Fu Panda</i> called for large-scale destruction that encouraged the debris to be generated by procedural methods but also allowed a high degree of artistic control. In this sketch, we outline such a method for fracturing a model into debris and a system for generating secondary debris when the large pieces collide or break apart.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>F.2.2</cat_node>
				<descriptor>Geometrical problems and computations</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Curve, surface, solid, and object representations</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.8</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010507</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Graphics recognition and interpretation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061.10010063</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures->Computational geometry</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010399</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Parametric curve and surface models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010401</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Volumetric models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1098526</person_id>
				<author_profile_id><![CDATA[81421593028]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Lawrence]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lee]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[DreamWorks Animation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098527</person_id>
				<author_profile_id><![CDATA[81421600947]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Nikita]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Pavlov]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[DreamWorks Animation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Procedural Fracturing and Debris Generation for Kung-Fu Panda Lawrence Lee and Nikita Pavlov DreamWorks 
Animation  1. Abstract Several sequences in Kung-Fu Panda called for large-scale destruction that encouraged 
the debris to be generated by procedural methods but also allowed a high degree of artistic control. 
In this sketch, we outline such a method for fracturing a model into debris and a system for generating 
secondary debris when the large pieces collide or break apart. 2. Primary debris The goal was to allow 
the user to fracture a model by quickly roughing in the size and shape of the debris pieces, and then 
procedurally filling in the final shape using the fracture system. 2.1. User input The system uses a 
3D painting program as the primary method of user input because it allows us to match the fracture pattern 
given by the art director more accurately. Discrete colors are painted on the unfractured model to specify 
regions that correspond to debris pieces. The regions can be painted with as much or as little detail 
as the situation calls for. Unpainted regions are filled in procedurally in the next step. 2.2. Cutting 
volume generation To generate the cutting volumes, we first set up a voxel grid based on the bounding 
box of the unfractured model. The unfractured surfaces are then voxelized such that the voxels are seeded 
with the painted color of the surface. Simple rules inspired by Cellular Automata (CA) concepts are used 
to grow the regions until all voxels are filled in. Finer textural details can be added by resampling 
the resulting voxel grid at higher resolution and perturbing the sampling coordinates with noise. The 
high-resolution voxels of each region are then converted to polygon meshes using the marching cube algorithm. 
2.3. Final debris piece generation Finally, the unfractured model is converted to a closed polymesh which 
inherits the texture coordinates and geometric normals. Intersecting the resulting polymesh with the 
cutters generated in section 2.2 using constructive solid geometric algorithm (CSG), we arrive at our 
final debris pieces. Procedural textures based on reference positions and normals are then applied to 
the newly exposed internal surfaces using triplanar projection. The end result would work regardless 
of how we fracture the original geometry. 3. Secondary debris We use the larger debris to procedurally 
generate smaller chunks of rock and pebbles coming off when these pieces collide and/or crack open. 
 3.1. Secondary debris from collisions For any two pieces of debris, we compute the intersecting region, 
reduce it to the exterior edge, and store that as particle data. We define each point's normal as the 
average of normals at the intersecting surfaces, storing it in the particles to determine emission direction. 
The impact force is calculated based on per-vertex velocity of the intersecting surfaces near each particle, 
which is later used to scale debris emission velocity. Figure 4: Intersection between two pieces 3.2. 
Secondary debris from separating pieces For any two pieces of debris A and B that are close enough to 
intersect, we build a table that tracks, for every vertex in A, the distance to the closest vertex in 
B. As soon as this distance exceeds a certain threshold, indicating that the surfaces are separating, 
we place a particle between the two vertices, its normal facing outward as described above. 4. Additional 
credits David Allen, Wes Burian, Saty Raghavachary. Copyright is held by the author / owner(s). SIGGRAPH 
2008, Los Angeles, California, August 11 15, 2008. ISBN 978-1-60558-466-9/08/0008   
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1401109</article_id>
		<sort_key>770</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>60</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Making statues move]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1401032.1401109</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401109</url>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>H.5.1</cat_node>
				<descriptor>Animations</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>H.5.1</cat_node>
				<descriptor>Artificial, augmented, and virtual realities</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.6.1</cat_node>
				<descriptor>Systems theory</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.8</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003752.10010061.10010063</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures->Computational geometry</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341.10010349.10011310</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Simulation types and techniques->Simulation by animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341.10010346.10010347</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Simulation theory->Systems theory</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010507</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Graphics recognition and interpretation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010392</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Mixed / augmented reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010866</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Virtual reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010866</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Virtual reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1098528</person_id>
				<author_profile_id><![CDATA[81421597609]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Jason]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bayever]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Rhythm & Hues Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098529</person_id>
				<author_profile_id><![CDATA[81421593457]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Jimmy]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gordon]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Rhythm & Hues Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098530</person_id>
				<author_profile_id><![CDATA[81421601044]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Gavin]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[McMillan]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Rhythm & Hues Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098531</person_id>
				<author_profile_id><![CDATA[81421601053]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Yogesh]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lakhani]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Rhythm & Hues Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098532</person_id>
				<author_profile_id><![CDATA[81100431086]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Ari]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Shapiro]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Rhythm & Hues Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098533</person_id>
				<author_profile_id><![CDATA[81421599719]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Joe]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Mancewicz]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Rhythm & Hues Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098534</person_id>
				<author_profile_id><![CDATA[81421601138]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>7</seq_no>
				<first_name><![CDATA[Tae-Yong]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kim]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Rhythm & Hues Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Making Statues Move Jason Bayever* Jimmy Gordon Gavin McMillan Yogesh Lakhani§ Joe Mancewicz¶ Tae-Yong 
Kiml Ari Shapiro** Rhythm &#38; Hues Studios Figure 1: Plate separation during movement. (Left image) 
The original horse. (Middle image) As the horse begins to move, the rear quarters are stretched, causing 
those plates to separate. (Right image) During a run, the plates that are near the stretched skin separates, 
while other plates that were previously stretched begin to heal. The yellow indicates a gap between the 
plates, red indicates plates that have not separated, while blue indicates fully separated plates. Exposition 
A common story element in modern .lms is to have the statue of a creature made of stone or metal suddenly 
come to life and act like the animal or creature that the statue portrayed. One inter­pretation of such 
a visual effect is that the statue-creature retains the appearance of the statue material, while simultaneously 
moving in the life-like manner of the original creature, in contrast to hav­ing the statue completely 
transform into the living creature without having any statue-like characteristics. A simple way to accomplish 
such a visual effect is to rig a character normally with deformable skin, but use a material that emulates 
the statue s material in place of the usual skin or clothing material. This simple approach will fail 
to capture the visually expected effect that the rigidity of the the statue s shell material, such as 
stone, has on the statue-creature. A different approach is to simulate the cracking apart of the statue 
shell and retaining the remnants of that shell on the statue-creature during movement. For The Mummy: 
Tomb of the Dragon Emporer, we have created the visual effect of a bronze statue of a horse coming to 
life, while retaining its statue-like appearance by retaining the look of cracked metal. Our technique 
can be replicated for any type of statue-to­creature transformation using many different materials. In 
addition, the effect can be reversed, yielding the visual effect of turning a liv­ing creature into a 
statue or having a creature be frozen or encased inside of a solid shell. Techniques Pipeline. Our approach 
utilizes a standard production pipeline. The crack patterns are .rst generated manually by a modeler. 
While it is possible to generate the crack patterns automatically, by allowing the artist to specify 
where the skin will crack gives better control over the .nal appearance of the visual effect. The horse 
rigs are then generated, and the animators then animate the horses according to the desired shot. *e-mail: 
jason@rhythm.com e-mail: jimmyg@rhythm.com e-mail: gavin@rhythm.com §e-mail: yogesh@rhythm.com ¶e-mail: 
joe@rhythm.com Ie-mail: tae@rhythm.com **e-mail: ashapiro@rhythm.com Plate Formation. After animating 
the creature, the metal plates can be formed from the crack patterns by assuming that each piece moves 
rigidly along the surface of the skin. All vertices in a plate will then move in concert relative to 
the center of the plates by aver­aging the vertex positions. Our proprietary software will then deter­mine 
when two plates will split apart by measuring the difference between the original and current areas of 
the plates. The rigidity of the plates can be controlled by interpolating between perfectly rigid plates 
(those that split apart upon any subtle movement) and perfectly elastic plates (those which stretch and 
deform based on any movement), shown in Figure 1. The plates are allowed to re­sume back to their original 
shapes based on a healing time. 2D Collisions. Once the plate behavior and elasticity has been determined, 
the plates can be visualized as moving independently along the surface of the model. These plates are 
then collided and bounced against each other along the surface of the horse. This is done by creating 
a simpli.ed 2D collision model. The collisions are calculated by projecting each set of plates onto a 
two-dimensional plane orthogonal to the normal of the plate surface. The plates will then bounce based 
on the overlap in the direction along the surface of the plate. By using 2D collisions instead of 3D 
collisions, this process can be run at interactive speeds. Metal Buckling. The plates need to appear 
to be made of metal, which will buckle and curl upon collisions. We created a deforma­tion that emulates 
metal buckling by passing sine waves through the plate based on the amount of deformation and the direction 
of collision. The artist can control: how many waves are formed, the depth of the wave, and the upwards 
or downwards curling of the edges upon collision. Accurate models of metal buckling can be generated 
with .nite element analysis. However, a simpli.ed ver­sion that considers only the deformation of the 
mesh can be better controlled and tweaked by the artist. Extensibility. The procedure for creating cracks 
on a skinned rig can be modi.ed to accommodate other types of materials. For ex­ample, cracks made of 
clay can be produced by using the same plate formation and 2D collision, but reacting to collisions by 
crumbling, rather than by buckling. Copyright is held by the author / owner(s). SIGGRAPH 2008, Los Angeles, 
California, August 11 15, 2008. ISBN 978-1-60558-466-9/08/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1401110</article_id>
		<sort_key>780</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>61</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[CrackTastic]]></title>
		<subtitle><![CDATA[fast 3D fragmentation in "The Mummy: Tomb of the Dragon Emperor"]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1401032.1401110</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401110</url>
		<abstract>
			<par><![CDATA[<p>We have developed an efficient technique for fast and production-friendly fragmentation of solid (<i>i.e.</i> closed) geometry. It is based on novel data structures and algorithms that allow us to employ fast and robust level set operations to generate fragments at very high resolutions and speeds far exceeding fully physics-based simulation techniques. Our tool, dubbed "CrackTastic", is embedded into a larger Houdini framework that adds artistic control to the fracture generation as well as augments with physics-based animations by means of rigid body dynamics (RBD) of the fragments. "CrackTastic" is derived from a vision to balance fully physics-based fracture simulations and completely manual modeling since both extremes pose major disadvantages in terms of production time and artistic flexibility. In contrast, our new framework allows a single artist to produce massive fragmentations on the order of minutes as oppose to hours or even days. "CrackTastic" has already been used in the newly released movie "The Golden Compass", but more significantly constitutes a key-technology in the upcoming movie "The Mummy, Tomb Of The Dragon Emperor" that features complex fragmentation on a very large scale.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Curve, surface, solid, and object representations</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>F.2.2</cat_node>
				<descriptor>Geometrical problems and computations</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Geometric algorithms, languages, and systems</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.8</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010507</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Graphics recognition and interpretation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061.10010063</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures->Computational geometry</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010401</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Volumetric models</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010399</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Parametric curve and surface models</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1098535</person_id>
				<author_profile_id><![CDATA[81324492244]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Ken]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Museth]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Digital Domain, Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098536</person_id>
				<author_profile_id><![CDATA[81421598300]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Michael]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Clive]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Digital Domain, Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1278804</ref_obj_id>
				<ref_obj_pid>1278780</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Museth, K., Clive, M., and Zafar, N. B. 2007. Blobtacular: Surfacing particle systems in "pirates of the caribbean 3". In <i>ACM SIGGRAPH Sketches</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1132035</ref_obj_id>
				<ref_obj_pid>1132033</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Nielsen, M. B., and Museth, K. 2006. Dynamic Tubular Grid: An efficient data structure and algorithms for high resolution level sets. <i>Journal of Scientific Computing 26</i>, 3, 261--299.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 CrackTastic Fast 3D Fragmentation in The Mummy: Tomb Of The Dragon Emperor * Ken Museth and Michael 
Clive Digital Domain, Inc. Abstract: We have developed an ef.cient technique for fast and production-friendly 
fragmentation of solid (i.e. closed) geometry. It is based on novel data structures and algorithms that 
allow us to employ fast and robust level set operations to generate fragments at very high resolutions 
and speeds far exceeding fully physics­based simulation techniques. Our tool, dubbed CrackTastic , is 
embedded into a larger Houdini framework that adds artistic con­trol to the fracture generation as well 
as augments with physics­based animations by means of rigid body dynamics (RBD) of the fragments. CrackTastic 
is derived from a vision to balance fully physics-based fracture simulations and completely manual model­ing 
since both extremes pose major disadvantages in terms of pro­duction time and artistic .exibility. In 
contrast, our new framework allows a single artist to produce massive fragmentations on the or­der of 
minutes as oppose to hours or even days. CrackTastic has already been used in the newly released movie 
The Golden Com­pass , but more signi.cantly constitutes a key-technology in the upcoming movie The Mummy, 
Tomb Of The Dragon Emperor that features complex fragmentation on a very large scale. Overview: The input 
to CrackTastic is a base-geometry (e.g. textured polygonal mesh) with associated (arbitrary) impact points 
and the output is an animation of the fracturing. The .rst step in our pipeline converts the base-geometry 
to a level set using a new robust scan-converter that can handle self-intersecting meshes. We then have 
to option to produce shells (i.e. double-walls) from the solid base-geometry. This is a highly desired 
feature for Mummy 3 and is easily accomplished by means of fast level set operations. Next, the impact 
points are used to generate so-called scatter-points . The position (and other attributes) of these scatter-points 
are com­puted by means of simpli.ed physical heuristics derived from the base-geometry and the impact 
points. We have then developed an ef.cient level set procedure to recursively generate naturally look­ing 
fragments. The characteristics of the fragments (size, shape, density etc.) are easy to controlled or 
optionally automate. As the .nal step, RBD is applied to the fragments to account for the dy­namics of 
the shattering. Overall this approach is very production­friendly since it is both intuitive and easily 
allows for artistic con­trol. However, to facilitate fast level set operations that produce tightly .tted 
fragments with arbitrarily complex topology, high res­olution and no self-intersections, we need some 
extra sauce : *Some of the materials included in this submission are place-holders for Mummy 3 shots 
that we cannot publish before the movie-release in June. DB-Grid forms the fundamental representation 
of all our geometry processing, and is best described as a highly ef.cient data struc­ture for arbitrary 
volumetric data. It employs different blocking techniques to reduce memory footprints and ensure fast 
data ac­cess. It shares several of the bene.ts of the extremely compact DT-Grid[Nielsen and Museth 2006], 
but complements with sev­eral important improvements. Whereas DT-Grid is strictly limited to (closed) 
level sets, DB-Grid can literally encode any volumetric data, including of course unclosed (i.e. none-manifold) 
surfaces or even random densities! Furthermore, DB-Grid allows for both ran­dom read and push/pop in 
constant time unlike the logarithmic ran­dom read of DT-Grid and complete absence of random push/pop. 
Thus, we can represent surfaces with DB-Grids of effective grid resolutions exceeding 80003, which is 
more then adequate for VFX production. We currently employ DB-Grid to represent high reso­lution level 
sets in CrackTastic , but we have also used it to dra­matically improve the performance of Blobtacular 
presented last year[Museth et al. 2007]. In the near future we also plan to use DB-Grid in Digital Domain 
s award-winning .uid solver (FSIM) and volume renderer (STORM) to allow for much high resolutions and 
.delity. Preliminary results with FSIM look very promising. Post-processing: Ironically DB-Grid introduces 
a new problem for in the overall pipeline; the access to very high resolutions level sets obviously result 
in the generation of meshes with very high polygon counts. To solve this problem we have also developed 
a fast mesh decimator. Additionally we have extended CrackTastic to cor­rectly transfers mesh attributes 
(i.e. texture coordinates) from the base-geometry to the decimated meshes of the .nal fragments. A combination 
of all these tools and techniques make up the Crack-Tastic pipeline which is routinely used by artists 
to fracture thou­sands of terracotta warriors in the upcoming The Mummy: Tomb Of The Dragon Emperor . 
We look forward to presenting some of these dramatic shots after the movie release in June. References 
MUSETH, K., CLIVE, M., AND ZAFAR, N. B. 2007. Blobtacular: Surfacing particle systems in pirates of the 
caribbean 3 . In ACM SIGGRAPH Sketches. NIELSEN, M. B., AND MUSETH, K. 2006. Dynamic Tubular Grid: An 
ef.cient data structure and algorithms for high resolu­tion level sets. Journal of Scienti.c Computing 
26, 3, 261 299. Copyright is held by the author / owner(s). SIGGRAPH 2008, Los Angeles, California, August 
11 15, 2008. ISBN 978-1-60558-466-9/08/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	</section>
	<section>
		<section_id>1401111</section_id>
		<sort_key>790</sort_key>
		<section_seq_no>18</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Fire, fur, and fluid]]></section_title>
		<section_page_from>18</section_page_from>
	<article_rec>
		<article_id>1401112</article_id>
		<sort_key>800</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>62</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Fire simulation and rendering in Beowulf]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1401032.1401112</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401112</url>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.6.1</cat_node>
				<descriptor>Systems theory</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.6.7</cat_node>
				<descriptor>Environments</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.8</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010341.10010366.10010367</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Simulation support systems->Simulation environments</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010507</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Graphics recognition and interpretation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341.10010346.10010347</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Simulation theory->Systems theory</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1098537</person_id>
				<author_profile_id><![CDATA[81100426615]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Magnus]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wrenninge]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Sony Imageworks]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098538</person_id>
				<author_profile_id><![CDATA[81421592239]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Vincent]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Serritella]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Sony Imageworks]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098539</person_id>
				<author_profile_id><![CDATA[81421598740]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Theo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Vandernoot]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Sony Imageworks]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098540</person_id>
				<author_profile_id><![CDATA[81100392996]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Henrik]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Falt]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Sony Imageworks]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098541</person_id>
				<author_profile_id><![CDATA[81332536029]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Patrick]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Witting]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Sony Imageworks]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Fire simulation and rendering in Beowulf Magnus Wrenninge * Vincent Serritella Theo Vandernoot Henrik 
Falt § Patrick Witting ¶ Sony Imageworks Sony Imageworks Sony Imageworks Sony Imageworks Sony Imageworks 
 1 Introduction In a world without electricity, .re proved to be an integral part of Beowulfs 900+ all-CG 
shots. Given the range of .re effects re­quired from simple torches to a .re-breating dragon we needed 
a pipeline that could support both prop-based .re as well as com­plex and heavily art-directed shots. 
Before the show was over, we had taken effects artists out of the equation for a majority of the shots 
and replaced the entire rendering solution in the process. 2 Simulation During Ghost Rider, Sony s proprietary 
.re pipeline (spFire) was developed to handle the many hundred .re simulations needed. It let effects 
TDs set up and control simulations in Houdini while leveraging off of Maya s .uid solver for the actual 
simulation. This foundation worked very well, letting us use all our existing effects animation tools 
in Houdini at the same time as Maya s fast CFD solver, built-in forces and user interface. An extensive 
amount of the .re in Beowulf consisted of background elements, so a library was set up with pre-simulated 
torches, stock­ade cauldrons etc. The artist lighting a particular shot could then easily browse this 
library and populate the scene as needed. This way, effects artists could spend their time on shots requiring 
more complex and art-directed .re setups. The most demanding .re shots on the show involved the dragon 
breaking out of its cave, setting .re to trees and bushes, then to a bridge and to Beowulf s castle. 
These shots had to be broken down into many separate buffers which were simulated separately but could 
be rendered together. In some cases, over 30 individual simulations were needed to complete a single 
shot. Proprietary cache .les from the .re simulations were also used to drive secondary animation. Particles 
were advected through the ve­ *e-mail: magnus@imageworks.com e-mail: vincent@imageworks.com e-mail: lobo@imageworks.com 
§e-mail: hfalt@imageworks.com ¶e-mail: pwitting@imageworks.com locity .elds for particle based smoke 
and embers, and geometry was scorched by .re interaction by exporting point clouds to Ren­derMan which 
would then affect surface shaders.  3 Rendering On Ghost Rider, the spFire pipeline employed custom 
PRMan DSOs to handle rendering, but the process had limitations and com­plex scenes required extensive 
Z slicing to get around the mem­ory implications of using millions of semi-transparent RiPoints. To get 
around these limitations, we added support for .re rendering to Sony s in-house volume rendering package 
Svea. Svea is integrated into Houdini (for volumetric modeling) and Katana (our in-house lighting and 
compositing package). With hundreds of .re shots in the movie, rendering speed turned out to be a especially 
important. Fire rendering is especially hard on raymarchers as the sharp .ame interface introduces much 
higher frequencies than smoke or dust would. To help with this, and also to speed up rendering in general, 
an adaptive raymarching algo­rithm was developed. A simple adaptive raymarcher is trivial to write, but 
once it had to account for holdouts, internal motion blur, camera motion blur and other factors important 
to production ren­dering, the code turned out to be much more involved. We found that using the contribution 
to the .nal pixel value as an error metric automatically incorporated all these factors, and this sped 
up ren­dering considerably. Levelsets were generated for each simulation buffer as a pre-pass and gave 
the raymarcher a tight-.tting bound on what parts of each buffer actually contained relevant information. 
The actual shader values were evaluated at each voxel and any non-zero values were marked as being inside 
the interface. The levelset was then ex­tended by the velocity vectors to account for internal motion 
blur. This simple optimization turned out to be the biggest time saver, cutting 30 to 90 percent off 
rendering times. For camera motion blur, we found that jittering the sample time at each raymarch step 
drastically reduced noise compared to shooting multiple rays for each pixel given equivalent render times. 
In effect, this bent the path taken by the ray marcher, so extra care had to be taken to ensure it worked 
with the adaptive march algorithm while still remaining strictly deterministic. Copyright is held by 
the author / owner(s). SIGGRAPH 2008, Los Angeles, California, August 11 15, 2008. ISBN 978-1-60558-466-9/08/0008 
 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1401113</article_id>
		<sort_key>810</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>63</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Countless characters and clovers]]></title>
		<subtitle><![CDATA[interpreting Dr. Seuss' style with 3D fur]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1401032.1401113</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401113</url>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>F.2.2</cat_node>
				<descriptor>Computations on discrete structures</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>F.2.2</cat_node>
				<descriptor>Geometrical problems and computations</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003752.10010061.10010063</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures->Computational geometry</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061.10010063</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures->Computational geometry</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1098542</person_id>
				<author_profile_id><![CDATA[81421598946]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Eric]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Maurer]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Blue Sky Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098543</person_id>
				<author_profile_id><![CDATA[81547377456]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Sean]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Palmer]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Blue Sky Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098544</person_id>
				<author_profile_id><![CDATA[81421601491]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Alen]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lai]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Blue Sky Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098545</person_id>
				<author_profile_id><![CDATA[81421601199]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Jamie]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Williams]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Blue Sky Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Countless Characters and Clovers: Interpreting Dr. Seuss' Style with 3D Fur Eric Maurer Sean Palmer 
Blue Sky Studios Blue Sky Studios ericm@blueskystudios.com palmer@blueskystudios.com Translating Dr. 
Seuss' Horton Hears a Who! from the printed page to an animated feature presented stylistic and technical 
challenges for Blue Sky s ray-traced voxelized fur technology. The directors embraced our fur tools to 
interpret Seuss' brush strokes, challenging us to address the scope of our furriest feature film to date. 
We furred and feathered jungle animals, groomed clothing for Whos in Whoville, emulated Seuss swooshes 
for trees and bushes, and created landscapes with windswept grasses and clovers. basic fur tools We procedurally 
generate each individual hair, point by point, allowing explicit articulation and contrasting with other 
systems which interpolate guide hairs. Samples are generated on host geometry and pruned according to 
minimum distances and probability signals. Curves are drawn at each sample from intricately defined world 
space vector fields and transformed into surface space. Our grooming interface is analogous to modular 
analog synthesis: raw signals are generated, mixed, modulated and filtered creating alternately harmonious 
and dissonant results. Fur data is written to files, which allows us to create elaborate descriptions 
without concern for time constraints, since the networks are only solved during the design cycle, and 
the final results are stored in a groom file. Animation of character and prop fur is achieved using a 
proprietary follow-through algorithm that produces offsets for the static fur that are also written to 
files. Fur is rendered by accumulating curves of position, transparency, color, and other data into a 
voxel space. The system represents complex organic forms with less memory and aliasing issues than geometry. 
 furry crowds of Whos The Whos were designed to be covered in fur which is groomed into clothing. To 
shape otherwise organic fur grooms into contrived clothing we developed rectilinear vector fields akin 
to the weft and warp of the implied weave of the cloth. Similar tools described elaborate hairdos using 
arrays of curves to articulate the basic contours as well as braiding, twisting and clumping. Crowd sequences 
with thousands of Whos presented new problems since an average Who took ten minutes to build two million 
hairs, five thousand would have taken over eight hundred hours to build ten billion hairs. We solved 
this by creating 'cheap' grooms with as little as one percent of the original hairs. We pre-processed 
crowd shots to determine a character's maximum screen space resolution in order to decide which fur files 
to load. Subsequently thousands of characters were built in hours instead of days. fields of clovers 
and LOD The scope of vast furry landscapes proved to be impossible for grooming and storing every single 
hair. While an average character contained 2-8 million hairs, even using only 1,000 hairs to represent 
each clover would put the field's hair count at several billion. File storage and i/o times were prohibitive. 
The solution was to groom a single hair representing each clover's stem and write these to disk. This 
gave artistic control of spacing, height and Alen Lai Jamie Williams Blue Sky Studios Blue Sky Studios 
alen@blueskystudios.com jamie@blueskystudios.com placement and could be reused in every shot. Using these 
as guides, clovers were built procedurally, generating hairs without the intermediate storage. This dynamic 
building allowed near­limitless amounts of fur. Variable hair counts for clovers were created based on 
the camera's position and built up or down as the camera moved. A level of detail (LOD) method managed 
these transitions. The screen area of each clover determined the number of hairs needed to create a full 
look at that distance. Our system could handle extreme camera moves that would have been prohibitive 
with a predetermined clover resolution. Dynamically building from 5 to 50,000 hairs, the resultant clovers 
had enough detail up close while reducing generation time significantly at a distance. The final number 
of hairs per shot varied from a few thousand to 2.5 billion. This flexible solution became part of an 
extensible tool set used to create generators for dandelions, grass and particle systems. bridging fur 
to effects The effects department leveraged on this work with the dandelions, the first effect in production 
requiring procedurally animated fur. We modified the generators to use world space transforms, derived 
from particle motion, as offsets to static fur from files, allowing for more dynamic uses of voxels. 
To direct clover piles eroding, we sampled the source positions of the clovers and stored relative height 
and depth signals to direct motion and interaction with characters. Our follow-through technology was 
adapted to create procedural wind on the field. An important consideration in integrating the effects 
and fur pipelines was to avoid redundant efforts. A hybrid system that used an fx node network design 
on the surface and called fur scripts within bridged the two systems. From millions of hairs in Ice Age 
2 to billions in Horton, our fur technology evolved to realize Seuss' vision. With an effective inter-department 
pipeline, it is now possible to design new tools and creative controls for a range of voxel applications 
at Blue Sky. Copyright is held by the author / owner(s). SIGGRAPH 2008, Los Angeles, California, August 
11 15, 2008. ISBN 978-1-60558-466-9/08/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1401114</article_id>
		<sort_key>820</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>64</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Atmos]]></title>
		<subtitle><![CDATA[a system for building volume shaders]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1401032.1401114</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401114</url>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.4</cat_node>
				<descriptor>Application packages</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>J.6</cat_node>
				<descriptor>Computer-aided design (CAD)</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>J.5</cat_node>
				<descriptor>Fine arts</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.8</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010469.10010470</concept_id>
				<concept_desc>CCS->Applied computing->Arts and humanities->Fine arts</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010432.10010439.10010440</concept_id>
				<concept_desc>CCS->Applied computing->Physical sciences and engineering->Engineering->Computer-aided design</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010469.10010472.10010440</concept_id>
				<concept_desc>CCS->Applied computing->Arts and humanities->Architecture (buildings)->Computer-aided design</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010469.10010471</concept_id>
				<concept_desc>CCS->Applied computing->Arts and humanities->Performing arts</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010507</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Graphics recognition and interpretation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1098546</person_id>
				<author_profile_id><![CDATA[81100139194]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Ferdi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Scheepers]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Pixar Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098547</person_id>
				<author_profile_id><![CDATA[81100420080]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Alexis]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Angelidis]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Pixar Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<article_type art_type="regular_article" />
	</article_rec>
	</section>
	<section>
		<section_id>1401115</section_id>
		<sort_key>830</sort_key>
		<section_seq_no>19</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Digital cinematography techniques]]></section_title>
		<section_page_from>19</section_page_from>
	<article_rec>
		<article_id>1401116</article_id>
		<sort_key>840</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>65</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[The BLT]]></title>
		<subtitle><![CDATA[a digital cinematographer's control center]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1401032.1401116</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401116</url>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>J.5</cat_node>
				<descriptor>Performing arts (e.g., dance, music)</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>H.5.1</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.4.1</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010469.10010471</concept_id>
				<concept_desc>CCS->Applied computing->Arts and humanities->Performing arts</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002951.10003227.10003251</concept_id>
				<concept_desc>CCS->Information systems->Information systems applications->Multimedia information systems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1098548</person_id>
				<author_profile_id><![CDATA[81365595618]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Kevin]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Thomason]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Blue Sky Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 The BLT: A Digital Cinematographer's Control Center Kevin Thomason Workbook Layout Artist Blue Sky Studios 
 One of the biggest challenges facing layout artists of 3-D animated films is keeping the creative filmmaking 
process fluid when shooting extremely complex shot sequences. On the one hand, making cinematic choices 
should happen smoothly and naturally, like the story itself. On the other hand, technically choreographing 
and orchestrating characters, cameras, and shot flow can be a daunting task for large shot sequences. 
If the camera crew does their job right, their creative efforts are rarely spotted by the audience; consequently, 
the layout process is much less likely to be showcased than the more apparent aspects of a CGI film such 
as animation, design, lighting, etc. However, a focus on the layout workflow and the tools that enable 
it is vital, since layout is the subtle but critical shot planning stage for the 3-D animated films we 
love. The Layout Workflow At Blue Sky Studios, the central hub tool that enabled layout artists to plan 
and manage their shots for Horton Hears a Who! was the bssLayoutTool ( BLT. ) With it they had the advantage 
of comprehensively shooting entire multi-shot sequences within one scene planning file. All principal 
and secondary characters, props, and set pieces for the sequence lived in the file, ready to be choreographed 
for each shot in one centralized environment. It was as if layout artists were cinematographers on location 
with a waiting cast, ready to be staged and filmed. As layout artists evolved the shot flow of their 
sequences, they had to consider not only composition within each individual shot, but also how the sequence 
flowed as a whole. One seemingly small change in any given shot might ripple through several related 
shots, requiring a rethinking of the shot plan. The BLT enabled the 3-D work environment to adapt quickly 
to the evolving plan by managing these changing concerns in one central location. This method bypassed 
the issues of a fractured creative workflow inherent in other filmmaking pipelines, where artists must 
plan each shot of an interrelated sequence separately in its own file. Initial Tool Design In its initial 
release, the BLT s primary functions were shot navigation and assignment of camera-specific geometry 
to shots for breakout to the Animation Department. The interface let layout artists jump quickly from 
one shot s camera view to another s, composing each shot from the same bank of characters and sets utilized 
for other shots in that sequence. Once the director approved camera cinematography, the BLT automatically 
assigned shot-specific assets to each shot camera, based on what fell within an extended volume of the 
camera's frustum. The BLT then generated scripts that removed the unnecessary geometry from the master 
file, leaving only that which was essential for the specific shot in a stripped-down file, ready for 
Animation. Further Development Enhancements were added to the BLT throughout production to further streamline 
the layout process. Setup time was greatly reduced by functionality that interfaced with the film's database 
tracker to automatically set up cameras with their corresponding frame ranges and track readings. Within 
minutes of starting even a complex 100+ shot sequence, a layout artist could have all that was needed 
to begin roughing out camera compositions. Experimentation with alternate shots, adjusting of the film's 
cut order, automatic creation of same-as shots, quick playback reviewing of all shots, etc., were all 
available from easily­accessed menus, essentially enabling the artist to edit the sequence cut within 
the 3-D environment. The BLT was particularly critical for disseminating up-to-the­minute shot data and 
generating informing imagery needed by the rest of production. Volumetric motion paths were auto-generated 
for each character in each shot to inform the Assembly Department where characters would be moving so 
that sets could be dressed to accommodate their action. Automated overhead renders of each camera s frustum 
mapped out the sequence s overall shot plan to inform master lighting planning. Technical layout concerns 
such as asset checking and cleanup were greatly relieved by automated troubleshooting functions that 
detected and corrected the sometimes tedious but show-stopping problems that arise in the creation of 
a complex animated sequence. This freed the artists to focus their efforts on cinematic concerns such 
as continuity management, shot composition, and controlling viewer eye direction. Future Work Future 
plans for the BLT include making it more extensible to all production departments, many of whom began 
to request that their own functionality be embedded in this formerly layout­specific tool. Simultaneous 
with this expansion, the BLT will also be more modular, so that various departments can build the tool 
on the fly to include only the functionality that suits their specific workflows. Even more comprehensive 
interfacing with the database tracker will enable all production departments on the film to access and 
generate up-to-the-minute, critical shot data down to the smallest detail. With these technical goals 
reached, Blue Sky s artists will be that much closer to a workflow that feels nearly as fluid as the 
visual story they are telling. Copyright is held by the author / owner(s). SIGGRAPH 2008, Los Angeles, 
California, August 11 15, 2008. ISBN 978-1-60558-466-9/08/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1401117</article_id>
		<sort_key>850</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>66</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[The cinematography of <i>Wall&#183;E</i>]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1401032.1401117</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401117</url>
		<categories>
			<primary_category>
				<cat_node>J.5</cat_node>
				<descriptor>Performing arts (e.g., dance, music)</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.8</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.4.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.4.9</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010383</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Image processing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010225</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision tasks</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010507</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Graphics recognition and interpretation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010469.10010471</concept_id>
				<concept_desc>CCS->Applied computing->Arts and humanities->Performing arts</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1098549</person_id>
				<author_profile_id><![CDATA[81342515410]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[John]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Warren]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Pixar Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098550</person_id>
				<author_profile_id><![CDATA[81421602249]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Jeremy]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lasky]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Pixar Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098551</person_id>
				<author_profile_id><![CDATA[81421597092]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Danielle]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Feinberg]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Pixar Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Burum, S. Ed., <i>American Cinematographer Manual 9th Ed.</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Kahrs, J., Calahan, S., Carson, D., and Poster, A. S. C., "Pixel Cinematography," ACM Siggraph Course Notes #30, 1996.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1073214</ref_obj_id>
				<ref_obj_pid>1073204</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Pellacini, F., Vidimce, K., Lefohn, A., Mohr, A., Leone, M., Warren, J.: Lpics: a hybrid hardware-accelerated relighting engine for computer cinematography. ACM Trans. Graph. 24(3): 464--470 (2005)]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1401118</article_id>
		<sort_key>860</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>67</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Indiana Jones]]></title>
		<subtitle><![CDATA[a look into the visual effects challenges and slight of hand for <i>Crystal Skull</i>]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1401032.1401118</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401118</url>
		<abstract>
			<par><![CDATA[<p>This sketch will highlight the wide range of challenges the visual effects team faced in creating the 540 visual effects shots (on screen for some 48-minutes) in this film. Although the team developed cutting edge technology to solve many of the issues that had to be dealt with they were also careful to remain true to the original films. ILM made the decision early on to utilize miniatures for a number of sequences but they weren't necessarily used as you might have thought. Constructing the short-lived town known as Doomtown used both practical miniatures and digital technology to achieve the resulting sequence. It was also necessary to develop advanced smoke and particle simulation techniques for the iconic nuclear event sequence.</p> <p>Among the challenges was creating a digital jungle. The approach, capture as much incamera on location as possible and then augment that reality with digital jungle that integrated seamlessly. What's an Indiana Jones film without treacherous creatures that the characters have to contend with? This time around it's ants... over 200,000 of them to be exact. Creating the thousands of swarming jungle ants that had to attack and interact with the actors in a believable manner was a challenge all to itself.</p> <p>To create the vast destruction necessary on such a grand scale the team used newly developed 'fracture' technology that allowed for the volumetric fracture of intricate geometry in a realistic manner. A challenge that loomed for the duration of the project was the valley destruction sequence. We'll take a look at the mixture of miniature elements, digital environment and simulated water for the film's climactic finale.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>J.5</cat_node>
				<descriptor>Performing arts (e.g., dance, music)</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.8</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.6.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010341</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010469.10010471</concept_id>
				<concept_desc>CCS->Applied computing->Arts and humanities->Performing arts</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010507</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Graphics recognition and interpretation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1098552</person_id>
				<author_profile_id><![CDATA[81100169065]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Pablo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Helman]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Industrial Light & Magic]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098553</person_id>
				<author_profile_id><![CDATA[81421597726]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Marshall]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Krasser]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Industrial Light & Magic]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098554</person_id>
				<author_profile_id><![CDATA[81331506854]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Jeff]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[White]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Industrial Light & Magic]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Indiana Jones: A look into the visual effects challenges and slight of hand for Crystal Skull Pablo 
Helman Marshall Krasser Jeff White Visual Effects Supervisor Associate VFX Supervisor Digital Production 
Supervisor Industrial Light &#38; Magic Industrial Light &#38; Magic Industrial Light &#38; Magic 1 
Introduction Much has changed in the world of visual effects over the past nineteen years and yet the 
goal with Indiana Jones and the Kingdom of the Crystal Skull was to stay true to physical, tangible action 
captured in the original trilogy. For the latest installment, ILM artists worked with Steven Spielberg 
to create the hair raising adventure that fans have come to expect while letting Steven capture as much 
as possible in­camera. While using every trick in the book to complete the work, ILM needed to develop 
completely new techniques and software to accomplish the climatic ending.  2 Abstract This sketch will 
highlight the wide range of challenges the visual effects team faced in creating the 540 visual effects 
shots (on screen for some 48-minutes) in this film. Although the team developed cutting edge technology 
to solve many of the issues that had to be dealt with they were also careful to remain true to the original 
films. ILM made the decision early on to utilize miniatures for a number of sequences but they weren 
t necessarily used as you might have thought. Constructing the short-lived town known as Doomtown used 
both practical miniatures and digital technology to achieve the resulting sequence. It was also necessary 
to develop advanced smoke and particle simulation techniques for the iconic nuclear event sequence. Among 
the challenges was creating a digital jungle. The approach, capture as much in- Copyright is held by 
the author / owner(s). SIGGRAPH 2008, Los Angeles, California, August 11 15, 2008. ISBN 978-1-60558-466-9/08/0008 
 camera on location as possible and then augment that reality with digital jungle that integrated seamlessly. 
What s an Indiana Jones film without treacherous creatures that the characters have to contend with? 
This time around it s ants over 200,000 of them to be exact. Creating the thousands of swarming jungle 
ants that had to attack and interact with the actors in a believable manner was a challenge all to itself. 
To create the vast destruction necessary on such a grand scale the team used newly developed fracture 
technology that allowed for the volumetric fracture of intricate geometry in a realistic manner. A challenge 
that loomed for the duration of the project was the valley destruction sequence. We ll take a look at 
the mixture of miniature elements, digital environment and simulated water for the film s climactic finale. 
 3 Conclusion In the end, the work on Indiana Jones and the Kingdom of the Crystal Skull proved both 
challenging and rewarding at the same time. It allowed ILM to further develop its pipeline and explore 
new and unique ways to solve problems. We hope to share the processes and experiences we had on location 
and in post production to give attendees a better understanding of our "Indiana Jones" adventure. 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	</section>
	<section>
		<section_id>1401119</section_id>
		<sort_key>870</sort_key>
		<section_seq_no>20</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Effects omlette]]></section_title>
		<section_page_from>20</section_page_from>
	<article_rec>
		<article_id>1401120</article_id>
		<sort_key>880</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>68</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Got snow? Digital meteorology effects and advancements as used for the X-files: I Want to Believe]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1401032.1401120</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401120</url>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.6.1</cat_node>
				<descriptor>Systems theory</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.6.7</cat_node>
				<descriptor>Environments</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.8</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010341.10010366.10010367</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Simulation support systems->Simulation environments</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010507</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Graphics recognition and interpretation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341.10010346.10010347</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Simulation theory->Systems theory</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1098555</person_id>
				<author_profile_id><![CDATA[81421594421]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Mat]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Beck]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Entity FX]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098556</person_id>
				<author_profile_id><![CDATA[81544749456]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[David]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Alexander]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Entity FX]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Got Snow? Digital Meteorology Effects and Advancements as Used for The X-Files: I Want to Believe Mat 
Beck, Senior Visual Effects Supervisor and Second Unit Director, Entity FX Introduction Evolving digital 
toolsets and new creative and logistical demands placed on production are resulting in the need for unprecedented 
control and manipulation of on-set conditions post shoot. When visual effects artists are tapped to change 
or create realities after the fact and there is a critical mass of these shots on the project, a streamlined 
process and automated toolset and approach is helpful. Radically Altered Environments Given the shooting 
schedule and creative parameters of The X-Files: I Want to Believe, it was clear that the visual effects 
work would require a range of radically altered environments and creation of natural phenomena along 
with complex set extensions and fully synthetic photorealistic 3D. The visual effects toolset and pipeline 
would need to accommodate varying degrees and permutations on making shot footage look more sunny, rainy, 
wet or dry. A particular challenge would be creating atmosphere and continuity through the addition of 
digital snow. Flakes R Us Rather than relying on traditional one-off approaches, a more efficient solution 
was developed to streamline effects work and address the challenge of digital weather alteration as a 
whole. The resulting system, called Frosty, generates specific weather conditions, elements and scenarios 
on command. The software, composed using Maya s MEL programming language, allows for the automatic creation 
of snow and mist elements ranging from regular falling snow to outright blizzards. Many of the system 
s controls can be adjusted in real time, which makes achieving the final desired effect easier. The application 
also has built-in functionality for melting snow, generating z­depth channels, controlling files on a 
per-particle basis and others. The X-Factor As more productions go virtual, shoot here-for-there or 
take on ambitious filming schedules, robust and innovative approaches to generating altered realities 
on the fly in the visual effects process will continue to advance and grow. The X-Files: I Want to Believe 
is an example of a live-action feature project where the capabilities of digital meteorology play a central 
behind-the­scenes role. David Alexander, CG Supervisor/3D Lead, Entity FX Copyright is held by the author 
/ owner(s). SIGGRAPH 2008, Los Angeles, California, August 11 15, 2008. ISBN 978-1-60558-466-9/08/0008 
 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1401121</article_id>
		<sort_key>890</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>69</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[DrivenShape]]></title>
		<subtitle><![CDATA[a data-driven approach for shape deformation]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1401032.1401121</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401121</url>
		<categories>
			<primary_category>
				<cat_node>I.3.5</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>F.2.2</cat_node>
				<descriptor>Geometrical problems and computations</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>H.5.1</cat_node>
				<descriptor>Animations</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061.10010063</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures->Computational geometry</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341.10010349.10011310</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Simulation types and techniques->Simulation by animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061.10010063</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures->Computational geometry</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1098557</person_id>
				<author_profile_id><![CDATA[81365596229]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Tae-Yong]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kim]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Rhythm and Hues Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098558</person_id>
				<author_profile_id><![CDATA[81365592365]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Eugene]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Vendrovsky]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Rhythm and Hues Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Goldfarb, D., and Idnani, A. 1983. A numerically stable dual method for solving strictly quadratic programs. <i>Mathematical Programming 27</i>, 1--33.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 DrivenShape -a data-driven approach for shape deformation Tae-Yong Kim* Eugene Vendrovsky Rhythm and 
Hues Studios Rhythm and Hues Studios. Figure 1: Thedore s shirt animated with DrivenShape. DrivenShape 
is a technique developed at Rhythm and Hues Studios, aimed at providing an ef.cient alternative to expensive 
deformation computations (e.g. cloth simulations). It allows users to drive defor­mation of one object 
from the shape of another object. For example, we may wish to drive shape (wrinkles and folding) of a 
pants based on how the character s legs are posed. The tool is especially useful when the paired shapes 
are highly cor­related and the space of all the possible shapes is limited. Such cases are common in 
animation, such as deformation of tight .tting clothing or muscle .exing, where deformation is almost 
determined by the pose of the character. (Try raising your arm many times, and observe how your shirts 
responds to the pose). Overview We start by anaylizing reference animations that contain both pose shape 
(driver) and its corresponding deformed (driven) shape. For a database of k frames, let us call the pose 
shape Pi, and the driven shape Di, for 1 = i = k. At runtime, we have a new pose Pnew and wish to reconstruct 
cor­responding deformation Dnew . We express the new target shape  Dnew as weighted sum of Di in the 
database, Dnew =wi *Di ,  and use Pnew to compute weights such that Pnew =wi * Pi . In a mathematical 
form, we wish to minimize IA * w -b I2 , where A s column vectors are .lled with vertices of Pi, w is 
the weights, and b is a column vector with vertices of Pnew . We seek for wi that minimizes the above 
norm, resulting in the following equations. G = AT * A (1) g T = -2 * AT * b (2) b0 = bT * b (3) IA 
* w -b I2 = w T * G * w + g T * w + b0 (4) , subject to k wi =1 (5) i=1 0 = wi = 1 (6) *e-mail: tae@rhythm.com 
e-mail:eugene@rhythm.com , which can be solved with the standard quadratic programming (QP) techniques 
[Goldfarb and Idnani 1983]. Equation 6 ensures that we don t get artifacts from negative weights. Database 
Construction We provide a tool to automatically extract N most distinct shapes based on a greedy process. 
We expand the set of pose shapes by adding at each step a shape that s the most different (by Euclidean 
measure) from all the shapes contained within the set. After N it­erations, we have a set of N shapes 
that are suf.cently different from each other. We also allow users to directly pick the pair of shapes 
they want to add. Usually, users start with small number of automated picks and add to the database as 
they wish. Collision Detection For highly deformable characters, a simple linear blend would not  match 
the target shape (in general, Pnew wi =* Pi ). One no­ticeable artifact is a deformation that intersects 
with the pose shape.  Let PDnew =wi *Pi , and D new =wi *Di . After weights are computed , we apply 
additional mapping PDnew . Pnew to DDnew as follows. For each point d in D new , we .nd the closest triangle 
from PDnew and construct a coordinate system on the closest point with one axis being the normal, and 
another axis being one edge of the triangle. Local coordinate of dlocal is then computed, and used to 
reconstruct .nal position df inal after we move points of the triangle to Pnew and update the coordinate 
system. When two geometries are close (e.g. legs crossing each other), the closest triangle can come 
from wrong side and cause popping arti­fact. We let users supply additional mapping to exclude unnecses­sary 
binding (e.g. left pants maps to left leg only). Secondary Motion and Layering Since output is directly 
mapped from the pose, we lose secondary motion that was contained in the original animation. When sec­ondary 
motions are desired, we turn back to regular simulations, but users still use DrivenShape to guide the 
simulation (e.g using spring constraints) or to rapidly provide the initial draping of cloth. It also 
seamlessly works with additional deformations such as noise-based wind effects. Often users partition 
the geometry and apply Driven-Shape to more rigid part, and simulate more dynamic part (such as hood 
of the sweater) with cloth simulator. Conclusion For 20-30 shape pairs with 5000 vertices, the system 
runs in real­time, providing rapid feedback for animators. This technique was extensively used in our 
recent production of Alvin and the Chip­munks, and we could eliminate the need for expensive cloth sim­ulations 
for about 70 percent of the shots. Although it was orig­inally developed to speed up the cloth simulation 
pipeline, users have expanded its use to anything that deforms such as muscles, facial structure, etc. 
 References GOLDFARB, D., AND IDNANI, A. 1983. A numerically stable dual method for solving strictly 
quadratic programs. Mathematical Programming 27, 1 33. Copyright is held by the author / owner(s). SIGGRAPH 
2008, Los Angeles, California, August 11 15, 2008. ISBN 978-1-60558-466-9/08/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1401122</article_id>
		<sort_key>900</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>70</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Golden Compass auroras]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1401032.1401122</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401122</url>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>H.5.1</cat_node>
				<descriptor>Animations</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.10</cat_node>
				<descriptor>Volumetric</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.6.1</cat_node>
				<descriptor>Systems theory</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.8</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010240.10010242</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision representations->Shape representations</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341.10010349.10011310</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Simulation types and techniques->Simulation by animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341.10010346.10010347</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Simulation theory->Systems theory</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010507</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Graphics recognition and interpretation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1098559</person_id>
				<author_profile_id><![CDATA[81421599830]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Nathan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ortiz]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Rhythm and Hues Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098560</person_id>
				<author_profile_id><![CDATA[81421596380]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Eric]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Horton]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Rhythm and Hues Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098561</person_id>
				<author_profile_id><![CDATA[81541020056]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Michael]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kowalski]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Rhythm and Hues Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098562</person_id>
				<author_profile_id><![CDATA[81100445884]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Jerry]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tessendorf]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Rhythm and Hues Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Golden Compass Auroras Nathan Ortiz Eric Horton MichaelKowalski JerryTessendorf Rhythm and Hues Studios* 
 Aurora veil layer with reference artwork (upper left). 1 Introduction Aurora Borealis effect for the 
North Pole sequence on The Golden Compass, originally called for 100+ shots of animated sky Auro­ras 
that needed to transition to a ground level phenomenon for the characters to cross through them as a 
portal to another world. Af­ter consulting reference footage of auroras1, we assembled them from .ve 
volumetric layers, each mimicing particular features in the footage. The various layers employed a mix 
of techiques for creating volumetric density and color from splines, noise, streaks, and particles. One 
layer used very thin, almost two dimensional gas simulations as animated solid textures for animated 
volumetric grids. The decisionto modelthe layersvolumetricallygaveusthe power and .exibility needed to 
control the range of animation and visual detail required. In this talk we deconstruct the auroras and 
examine the techniques used for each layer. 2 Volumetric Layers The Auroras are composedof.vevolumetric 
layers:(1)ABack­ground layer with large scale noise structure and color gradient; (2) a Band layer with 
rapidly animating transients of variable height; (3)A Cloud layer with noisy structure running along 
the base of the aurora; (4) a Ribbon layer with .ne vertical .laments slowly traveling; and(5)aVeil layerof 
thickgaseous curtainsofvortices and .uid motion. TheVeil layer includes a CFDgas simulation from our 
Ahab Eulerian grid Navier Stokes simulator. 3 Fluid Simulations as Solid Textures Fluid .ow along auroral 
limbs visually appears mostly two dimen­sional. We simulated these conditions with Ahab by using only 
rectangular simulation domain that was typically 128 × 256 × 5, with the short dimension perpendicular 
to primary direction of the auroral limb. The thermally drivengas simulation had both hot and *e-mail: 
{nathano, ehorton, mak, jerryt}@rhythm.com 1http://www.phys.ucalgary.ca/ trondsen/ Final aurora structure, 
combining all of the layers. cold sources in order to mix the density and produce vortices. Be­causeof 
the minimally viscousQUICK advection schemein Ahab, the vortices in the simulation persist and evolve 
for long periods of time,allowingthebuildupof visually interesting motion. Position­ing the Ahab simulation 
volume colocated with the major axes of each limb, thevolumetric densityof theVeil layerwas multiplied 
bythegas density,effectively transferingthe .uid simulationtothe veil along a slice in the shape of the 
curved limb. When the aurora reached the ground level, Ahab .uid simulations were positioned separate 
from aurora layers to emphasize more vi­olent and turbulent conditions.  4 Volume Framework The framework 
for generating and manipulatingvolumetric models and simulationsatRhythmandHuesisacustom scripting language 
called Felt. The volumetric layers were modeled, assembled, and rendered using Felt to compute the volumes 
and handle them for other software systems. This allowed the layers to be created in­dependently with 
separate resolutions and grid structures, if any. Color and density .elds generated in Felt were controlled 
via at­tributesof sparse particle systems. Thegassimulation densitywas combined with theVeil layer using 
Felt scripting as well. Copyright is held by the author / owner(s). SIGGRAPH 2008, Los Angeles, California, 
August 11 15, 2008. ISBN 978-1-60558-466-9/08/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1401123</article_id>
		<sort_key>910</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>71</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[Destruction system]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1401032.1401123</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401123</url>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>H.5.1</cat_node>
				<descriptor>Artificial, augmented, and virtual realities</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.8</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010866</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Virtual reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010507</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Graphics recognition and interpretation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010866</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Virtual reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010392</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Mixed / augmented reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1098563</person_id>
				<author_profile_id><![CDATA[81312485671]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Rachel]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Weinstein]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Industrial Light & Magic]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098564</person_id>
				<author_profile_id><![CDATA[81421594097]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Frank]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Petterson]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Industrial Light & Magic]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098565</person_id>
				<author_profile_id><![CDATA[81421600698]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Brice]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Criswell]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Industrial Light & Magic]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Destruction System Rachel Weinstein* Frank Petterson Brice Criswell Industrial Light &#38; Magic Industrial 
Light &#38; Magic Industrial Light &#38; Magic  Figure 1: Our destruction pipeline maintains the original 
geometry while giving artists control over crack and breaking patterns. 1 Introduction Simulation of 
destruction is becoming increasingly necessary as de­stroying practical models do not provide the realism 
needed when dealing with locations that are dif.cult to reproduce. For the movie Indiana Jones and the 
Kingdom of the Crystal Skull, this motivated Industrial Light &#38; Magic (ILM) to develop a new fully 
3D destruc­tion pipeline which gave the artists the ability to take a piece of geometry, destroy it and 
integrate it seamlessly into the larger sim­ulation framework, Zeno. We chose to separate our destruction 
pipeline into two parts. In the .rst step, the fracturing of the geometry is done as a preprocess. Any 
object to be destroyed is pre-scored. A primary advantage of this is that it allows the fracture iterations 
to be decoupled from the subsequent rigid body simulation. The disadvantage is that the artist needs 
to be able to come up with plausible fracture patterns before simulation. To deal with this we provide 
real-time crack pattern visualization tools. In the second step we simulate the pre­scored geometry using 
a modi.ed rigid body engine that supports clustering of geometry. Here we present this pipeline and how 
it is used. 2 Modeling The .rst step is the fracturing of the original geometry. Our method retains 
the all details of the original geometry on the exterior and automatically transfers any important information 
such as texture coordinates to the new broken pieces. The interior crack faces of the geometry can have 
variable roughness as speci.ed by the artist. Since cracks are exact and sharp, our tool also allows 
for post­process beveling of edges to handle hard edges which may look unrealistic in some situations. 
A modeling approach was chosen to allow artists to have creative freedom over the fracture patterns. 
The tools have varied levels of control. They range from very broad, such as specifying which parts can 
and cannot break in a binary fashion, to very detailed, such as painting exact crack patterns by hand. 
The out put of the pre-scoring step is a set of new geometry along with the supplemental data needed 
by the simulation step to give the appearance that he fracture is not pre-scored, such as face data and 
new texture coordinates. *e-mail: rweinst@ilm.com e-mail:frankp@ilm.com e-mail:brice@ilm.com 3 Simulation 
Once the geometry has been fractured, the next step of the pipeline is to simulate the geometry as a 
dynamic object. The pre-scored pieces are grouped together as one piece of geometry known as a cluster. 
During simulation, the cluster can break automatically upon any impact, only when forces cause suf.cient 
strain between sub­bodies, or at keyframed times. Furthermore, different areas of the clustered geometry 
can be tagged as stronger than others, requir­ing more force to break. This tagging can be performed 
manually using spatially varying inputs or automatically, making the inside of an object stronger than 
the outer edges. In addition to tagging areas of the cluster, artists can also specify a decay rate between 
pairs of bodies to allow the object to gradually break down. The information resulting from the actual 
break simulation, such as times of separation, can be used for adding additional effects like emission 
(i.e., emitting dust between pieces as they break). Such additional effects can either be done during 
the initial simulation or as a post process, further streamlining the work.ow of the artists. 4 Conclusion 
We have presented the method developed for destruction scenes at Industrial Light &#38; Magic. This pipeline 
has been demonstrated ef­fective through its use in .lms such as Indiana Jones and the King­dom of the 
Crystal Skull and Ironman and was prototyped on Trans­formers. We plan to continue adding additional 
controls as new challenges arise. Copyright is held by the author / owner(s). SIGGRAPH 2008, Los Angeles, 
California, August 11 15, 2008. ISBN 978-1-60558-466-9/08/0008 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	</section>
	<section>
		<section_id>1401124</section_id>
		<sort_key>920</sort_key>
		<section_seq_no>21</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Caspian challenges of the sequel]]></section_title>
		<section_page_from>21</section_page_from>
	<article_rec>
		<article_id>1401125</article_id>
		<sort_key>930</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>72</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[A pipeline for 800+ shots]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1401032.1401125</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401125</url>
		<abstract>
			<par><![CDATA[<p>The Chronicles of Narnia: Prince Caspian presented a new and unique challenge to London's VFX industry. The complexity and volume of Caspian forced MPC to dramatically rethink their pipeline in just 18 short months. By the end of production MPC had finalled 865 shots. This talk will document the pipeline that we developed to meet this challenge, paying particular attention to asset management, character build and lighting.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.8</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.6.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010507</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Graphics recognition and interpretation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061.10010063</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures->Computational geometry</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1098566</person_id>
				<author_profile_id><![CDATA[81100548186]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Greg]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Butler]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The Moving Picture Company]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098567</person_id>
				<author_profile_id><![CDATA[81421594914]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Anders]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Langlands]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The Moving Picture Company]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098568</person_id>
				<author_profile_id><![CDATA[81421597655]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Hannes]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ricklefs]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The Moving Picture Company]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 A Pipeline for 800+ shots Greg Butler* Anders LanglandsHannes Ricklefs The Moving Picture Company The 
Moving Picture Company The Moving Picture Company Abstract The Chronicles of Narnia: Prince Caspian presented 
a new andunique challenge to London s VFX industry. The complexityand volume of Caspian forced MPC to 
dramatically rethink theirpipeline in just 18 short months. By the end of production MPChad .nalled 865 
shots. This talk will document the pipeline thatwe developed to meet this challenge, paying particular 
attention toasset management, character build and lighting. 1 Pipeline Changes To complete a show the 
size of Prince Caspian in the time avail­able, we decided to take a multi-tier production-line approach, 
en­forcing strict rules on such things as naming conventions; buildingtools and work.ows to allow different 
disciplines to work in paral­lel wherever possible, and to automate many of the repetitive tasksartists 
encounter day to day. MPC s old asset management system revolved around the version­ing of individual 
elements such as models, rigs, and animationcaches stored in a hierarchical structure based around job, 
scene,shot, asset type, asset and version. It quickly became apparent thatthis was not going to be suf.cient 
to manage the complexity re­quired for a project like Caspian. We need a system that couldstore the relationships 
between assets and their dependencies oneach other. This would be required in order to validate functional­ity 
between assets as well as providing production and artists theability to identify the status of the asset 
within the pipeline. For ex­ample, if for a particular character a new model was released withan updated 
topology, all existing animation caches would not workwith the new model and would need to be regenerated. 
The def­inition of these relationships should allow, for example, the auto­matic generation of an animation 
cache when the animation curvesor underlying models change. To accomplish this, the concept of a Package 
was de.ned as a structured grouping of multiple typesof assets or components (e.g. Models, rigs, and 
animation curves)that could each be version controlled in its own right. Therefore aPackage consists 
of a hierarchical list of assets, the dependenciesbetween those, and the rules to govern the regeneration 
of one assetshould its upstream/downstream dependencies change. MPC s pipeline is based around a proprietary 
C++ library, knownas Muggins which contains many algorithms and data structuresuseful for 3D applications. 
Muggins interfaces with Maya, Ren­derMan, OpenGL and mentalray, among others, and is bound tothe scripting 
languages lua and python to allow dynamic creation,manipulation and visualisation of geometry and other 
3D data bothwithinMayaandthe.nalrenderers. Thesescriptinglanguage bind­ings, collectively known as Giggle 
, provide the means to de.nePackagestructures, display theseasOpenGL previewswithinMayaand as renders 
out of RenderMan. Very early in the project it became clear that given the short time­frame, additional 
features had to be added to packaging to allowa parallel work.ow between the different departments. First, 
anapprovals system was implemented in order to control the .ow ofassets through the system. Lighting 
artists would only be able togather AnimatedCharacterPkgs built from approved CharacterPkgs *e-mail: 
greg-b@moving-picture.com e-mail: hannes-r@moving-picture.com into their scene, for example. This was 
important as it let us ver­ify that an asset was working correctly before propagating it intoshots. We 
also implemented the concept of streams: separate pack­ages for different disciplines that were independent 
of each otherand contained slightly different information, This meant that artistsworking at different 
stages of the pipeline could release and updatepackages without worrying about their changes breaking 
someoneelse s scenes. Once packages were approved, the necessary infor­mation could be copied between 
streams to synchronize the datathey contained and make sure everyone was working with the latestassets. 
With so many shots to present and so many characters in each shot,preparing animation dailies to show 
the client for approval becamea laborious process using standard Maya playblasts. Thus, we usedthe OpenGL 
interface to our scripting system to quickly batch ren­der previews of dozens of shots at a time on the 
render farm. Notonly was this vital for presenting animation for client approval, itallowed us to quickly 
verify that every asset in a shot was workingbefore passing off to lighting for .nal rendering. 2 Production 
Summary Even-though one might get the impression that the pipeline wasworking smoothly it had multiple 
problems. Given the late devel­opment and roll-out of the new features, sequences that had alreadybeen 
started worked outside of the pipeline. As the short timescalenever allowed these sequences to be incorporated 
back into the newpipelineitbecametiresometosupportboththenewpipelineandtheold, temporary wor.ows. In 
addition, the sheer volume of data gen­erated and processed during this project, combined with the largenumber 
of artists working on the show, in.icted severe stress onstorage and backup mechanisms, databases, network, 
.lesystems,and MPC s proprietary webservice based infrastructure. Caspianrequired full support from IT, 
Systems and R&#38;D during the dura­tion of the project to address these issues immediately wheneverthey 
occurred and to enable MPC to deliver the movie withoutshow-stopping interruptions to services. An ongoing 
area of de­velopment is the integration of production tools into the packagingasset management system. 
Expressing the concept of a packagein a transparent form to our production teams has proved to be achallenge, 
with reports and inspection utilities either providing toomuch, or too little information. However, the 
ability of the sup­port team to script the asset management system, de.ne automa­tion rules, and setup 
parallel work.ows to generate mass updatesand detailed shot status reports as required by production 
has been a success. The Chronicles Of Narnia: Prince Caspian presented MPC witha new and unique challenge 
that tested every part of the pipelineto the limit. To achieve the high standard of work required in 
theshort timeframe we developed a new pipeline based around com­mon working practices and conventions, 
an automated asset man­agement system and parallel work.ows between disciplines. Theresult placed a strong 
emphasis on effective scene management butwas necessary to guarantee the fast delivery of high quality 
workwhilst still allowing the artist creative freedom within the remit oftheir shot. With the show complete 
we can now leverage the tech­niques developed on Caspian and give MPC a strong foundationfrom which we 
can more ef.ciently deliver future shows, exceed­ing expectations in a highly competitive market place. 
Copyright is held by the author / owner(s). SIGGRAPH 2008, Los Angeles, California, August 11 15, 2008. 
ISBN 978-1-60558-466-9/08/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1401126</article_id>
		<sort_key>940</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>73</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Aslan and Trufflehunter]]></title>
		<subtitle><![CDATA[creature creation, from follicle to chronicle]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1401032.1401126</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401126</url>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>H.5.1</cat_node>
				<descriptor>Animations</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>H.5.1</cat_node>
				<descriptor>Artificial, augmented, and virtual realities</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.6.1</cat_node>
				<descriptor>Systems theory</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.8</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003752.10010061.10010063</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures->Computational geometry</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341.10010349.10011310</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Simulation types and techniques->Simulation by animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341.10010346.10010347</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Simulation theory->Systems theory</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010507</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Graphics recognition and interpretation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010392</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Mixed / augmented reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010866</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Virtual reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010866</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Virtual reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1098569</person_id>
				<author_profile_id><![CDATA[81421600525]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Paul]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Beilby]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Framestore]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098570</person_id>
				<author_profile_id><![CDATA[81421593118]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Ian]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Comley]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Framestore]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098571</person_id>
				<author_profile_id><![CDATA[81421593712]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Mike]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Mulholland]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Framestore]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098572</person_id>
				<author_profile_id><![CDATA[81421600555]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Alex]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Rothwell]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Framestore]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098573</person_id>
				<author_profile_id><![CDATA[81421598566]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Olivier]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Soares]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Framestore]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Aslan and Trufflehunter Creature Creation, From Follicle to Chronicle Paul Beilby Ian Comley* Mike 
Mulholland§ Alex Rothwell Olivier Soares Framestore   1 Introduction CG creature work is a field in 
which the bar never stops rising, and the challenge faced by Framestore when creating their iteration 
of the magnificent lion Aslan for 'The Chronicles of Narnia: Prince Caspian', was to ensure that he was 
purer, bolder than ever, and able to interact closely with the cast to a degree previously unseen. Whilst 
we were able to draw on our previous CG fur experience and proprietary grooming tools, we soon realised 
that creating the lion's mane would require extraordinary effort and ingenuity. In particular, controlling 
the shape, movement and interaction of light within clumps of hair required a whole new approach including 
a density-based normal calculation and voxelised occlusion. Trufflehunter the badger a new major character 
to this film required a great deal of interaction with actors and props in the scene, thus creating 
his own set of development issues. We present the process of creating the creatures Aslan and Trufflehunter 
from concept through to final composite, with emphasis on the techniques we developed to generate, simulate 
and render large volumes of photo-realistic hair. 2 Groom Development Despite reprising his role in 
Narnia, we were largely developing Aslan from scratch. Framestore's HairFilters grooming system (previously 
used for the polar bears in 'The Golden Compass') was pushed to a new level to cope with the volume of 
hair and tight clump control required for the lion's mane. Using our high-level fine, clumping (aka flumping) 
controls we were able to groom the mane around a relatively small number of guide hairs around 2550 
compared to the 10,000+ used in the first film. We also deliberately divided the groom for the mane into 
separate fur descriptions. This gave us more practical version control and allowed our Grooming Department 
to work on individual descriptions in parallel. Overall, 15 fur descriptions were used to generate the 
7.6 million hairs comprising the mane. With fine face fur, a coarse belly and shaggy shell, Trufflehunter 
benefited from the same techniques, by using separate fur descriptions for the specific hair qualities. 
As a new character, we captured a lot of reference material first­hand which became extremely useful 
for look development and animation as the traditionally reclusive animals are rarely captured in sufficient 
detail on film. 3 Hair Dynamics Dynamo, another proprietary tool, was used to simulate our hair movement 
ranging from a gentle breeze, a hand running through a mane to the badger being carried through a forest 
on the shoulder of a Prince. We solved on the underlying guide hairs and also had the ability to tweak 
using a poly-shell for tuning at the lighting level. Wind simulation was handled by pre-calculating vector 
grids representing wind strength which were then plugged into Dynamo. Wind occlusion was handled using 
a fast lookup of a voxel representation of the characters. Fur dynamics inevitably required some hand 
tweaking, but by the end of production results were predictable and controllable. 4 Fur Rendering With 
Aslan's flowing mane and Trufflehunter's dense coat, a shading model which accurately produced the result 
of light interaction through hair was essential. Finding a stable normal for use in our Marschner-based 
[Marschner et al. 2003] implementation became a particular problem. Fixing the follicle orientation allowed 
us to approximate the effects of light through individual, elliptical animal hairs, but conforming the 
normal directions to provide a sufficient 'shell' for lighting our thick hair clumps required a new technique. 
We ultimately came up with a voxelisation method for generating clump normals using fur density as a 
guide. The same technique also allowed us to produce high­quality fur-to-fur occlusion and environmental 
lighting in a fraction of the time taken by other methods. MARSCHNER, S. R., JENSEN H. W. and CAMMARANO 
M. 2003. Light Scattering from Human Hair Fibers. In Proceedings of ACM SIGGRAPH 2003. *email: ian.comley@framestore.com 
§email: mike.mulholland@framestore.com Copyright is held by the author / owner(s). SIGGRAPH 2008, Los 
Angeles, California, August 11 15, 2008. ISBN 978-1-60558-466-9/08/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1401127</article_id>
		<sort_key>950</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>74</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Raging waters]]></title>
		<subtitle><![CDATA[the rivergod of Narnia]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1401032.1401127</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401127</url>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>J.5</cat_node>
				<descriptor>Performing arts (e.g., dance, music)</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.8</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.6.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010507</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Graphics recognition and interpretation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061.10010063</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures->Computational geometry</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010469.10010471</concept_id>
				<concept_desc>CCS->Applied computing->Arts and humanities->Performing arts</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1098574</person_id>
				<author_profile_id><![CDATA[81100604837]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Stephan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Trojansky]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Raging Waters: The Rivergod of Narnia Stephan Trojansky troja@scanlinevfx.com Summary: This sketch 
presents how the Rivergod in Narnia Prince Caspian was brought to life by controlling Flowline Fluidsimulations 
with a classic polygonal character-rig. We ll also show how the complete rivergod was finally rendered 
in one single pass with the implicit surface render engine of Flowline. CG Rivergod, CG Bridgedestruction, 
CG Soldier and Horse Interaction and CG River all simulated and rendered in one pass with Flowline. 
Highresolution simulation of Rivergod with Keyframe animated facial features driving the fluidsimulation 
in Flowline. Sketch: To bring the rivergod to life, we had to develop a system that allowed the artists 
to create directable fluidsimulations within Flowline. The rivergod was constantly sucking water in a 
vortex motion from the river, spiraling up inside the body and pouring down again as his arms and hair. 
This system had to be a closed system where no single drop of water would dissappear or could be cheated 
away. Everything had to fall back into the river and be used again to build the character by spinning 
up the vortex. This complex system had to work also with complex character animations of the Rivergod. 
For this, we developed a setup that based all simulation setups on a keyframe animatable polygon-character 
rig and extended Flowline with features to maintain a realistic Flow of the water with foam, splashes 
and bubbles while constantly following the keyframe motion of the character. This setup then had to interact 
with the bridge horses and soldiers. We used a similar technique to what we had done for the ship destruction 
in 300. The wood had to splinter and interact depending on the pressure in the fluid simulation. The 
horses and soldiers were keyframe animated or motioncaptured and then transistioned into a simulated 
ragdoll motion interacting with the surrounding water. To get fast turn around cycles on the shots we 
used the unique technique in Flowline to distribute a single simulations to the renderfarm. In this way 
we were able to get results in a fraction of the time that would have been needed to calculate a shot 
on a single computer. Since we did not want to use any procedural textures, each detail in the rivergod 
down to each single foam bubble with subpixel size in a final image had to be simulated. This was possible 
since Flowline allows to create different LOD's within a simulation. While the simulation was still going 
on a shot, another set of computers was able to pick up rendering. One of the core features of Flowline 
is to allow rendering of all its simulationdata in one beauty pass including water, spray, bubbles, foam, 
selfshadowing, Global illumination and caustics. In fact, the Rivergod as he appears in the final shots 
in the movie, was created with the beauty pass and just a couple of adjustment layers for the compositors, 
so that they could focus on finetuning the shot instead of rebuilding the shot from dozens of layers. 
Copyright is held by the author / owner(s). SIGGRAPH 2008, Los Angeles, California, August 11 15, 2008. 
ISBN 978-1-60558-466-9/08/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	</section>
	<section>
		<section_id>1401128</section_id>
		<sort_key>960</sort_key>
		<section_seq_no>22</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Computer animation festival talks]]></section_title>
		<section_page_from>22</section_page_from>
	<article_rec>
		<article_id>1401129</article_id>
		<sort_key>970</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>75</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[The making of an oktapodi]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1401032.1401129</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401129</url>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>H.5.1</cat_node>
				<descriptor>Animations</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>H.5.1</cat_node>
				<descriptor>Artificial, augmented, and virtual realities</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.6.1</cat_node>
				<descriptor>Systems theory</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010392</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Mixed / augmented reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341.10010349.10011310</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Simulation types and techniques->Simulation by animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341.10010346.10010347</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Simulation theory->Systems theory</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010866</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Virtual reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010866</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Virtual reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1098575</person_id>
				<author_profile_id><![CDATA[81421600673]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Julien]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bocabeille]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Gobelins l'&#233;cole de l'image]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098576</person_id>
				<author_profile_id><![CDATA[81421596111]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Fran&#231;ois-Xavier]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Chanioux]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Gobelins l'&#233;cole de l'image]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098577</person_id>
				<author_profile_id><![CDATA[81421600979]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Olivier]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Delabarre]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Gobelins l'&#233;cole de l'image]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098578</person_id>
				<author_profile_id><![CDATA[81421595781]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Thierry]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Marchand]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Gobelins l'&#233;cole de l'image]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098579</person_id>
				<author_profile_id><![CDATA[81421598406]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Quentin]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Marmier]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Gobelins l'&#233;cole de l'image]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098580</person_id>
				<author_profile_id><![CDATA[81421598839]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Emud]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Mokhberi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Gobelins l'&#233;cole de l'image]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 The Making of an Oktapodi Julien Bocabeille François-Xavier Chanioux Olivier Delabarre Thierry Marchand 
Quentin Marmier Emud Mokhberi Gobelins l école de l image 1 Introduction In making a 3D animated short 
film where the central character is an octopus, two problems stand out: how to create an appealing character, 
particularly as octopi are not considered lovable creatures, and how to execute a model and rig that 
allows the animators to bring the character to life in a short production cycle. 2 Design &#38; Modeling 
The primary challenge is how to design an octopus that can emote and be appealing. It is an animal that 
is by nature slimy, with suction cups, and a strangely shaped body. It is often portrayed as a sea monster 
or alien and raises no immediate empathy. As the mouth is not visible, the function of the eyes becomes 
critical. Because our octopus is small in size compared to the world of our story, we enlarge its eyes 
and bring them to the center of the face so that they are easily readable. This combined with expressive 
eyelids, eyebrow ridges, and gills results in an appealing design that is true to the animal yet allows 
the character to emote. In designing the body, we need to determine how many arms and suction cups are 
enough. This is similar to the problem that the artists at Disney faced during the production of 101 
Dalmations. Even though we do not have to draw the arms and suction cups for each individual frame, we 
need to consider the time required to animate them and create appealing silhouettes. We find that with 
six arms and 24 suction cups each we can achieve a design that reads as an octopus and allows the creation 
of a 3D character that animators can work with. Figure 1: Design of face, body, and suction cups Rigging 
&#38; Animation The next problem is how to build a rig for our main character that allows it to perform 
a variety of actions with its six arms. It has to be able to use them as real arms and legs, to walk 
or climb walls, both of which require contact points. In addition, it has to have the ability to perform 
more broad movements, particularly to swim and fly through the air. Furthermore, our octopus has to be 
able to curl both the tip and the entire arm, fold its arm along the central axis, and be stretchable. 
Regardless of what action is performed, the arms have to appear to be joint-less. To accomplish this, 
we use a triple chain of 32 joints layered on top of each other for each arm, with additional joints 
coming off of this chain for the suction cups. The main chain is controlled using two separate ik-splines, 
one for what we consider the hand or foot portion, and another for the remainder of the arm. Because 
we need to maintain contact points to walk and hold onto objects, we are not able to use a simple stretchable 
ik-spline. Instead, we create a 1st degree curve for the ik-spline, which enables us to stretch it and 
maintain a contact point, and we control it with a 3rd degree curve using a wire deformer. This system 
gives us the capability to use a method similar to the reverse foot lock for walking and climbing, yet 
is flexible enough to allow the arm to bend at any point and appear joint-less when needed. Figure 2: 
Examples of range of actions performed with arms Figure 3: Joint layout and fold, twist and curl ability 
of arms The other challenge that we face is how to control the suction cups. While we want the ability 
to animate each suction cup individually, we also need a system for global control. To this end, we use 
two sliders that move along each arm and control the suction cups that fall between them. Two additional 
sliders are used to control the degree of falloff. Using this system, we can control a group of suction 
cups while maintaining the ability to do local adjustments on each one. We also use the system of sliders 
to control the fold and to allow the animator to apply a local twist to a portion of the arm, as opposed 
to the standard twist that originates from the tip. Finally as our octopus is very squishy, we give the 
animator controls to squash and stretch the entire arm or just a portion of it, again by using the system 
of sliders. Figure 4: Global control of suction cups showing falloff functionality, squash &#38; stretch, 
and twist 4 Rendering Our goal in rendering is to make the final image of the octopi as appealing as 
possible. We use pastel-colored textures and a healthy amount of subsurface scattering to achieve a soft 
translucent look in our main characters, which emphasizes their fragile nature. This, combined with our 
efforts in the design and animation process, gives rise to an octopus that is, contrary to preconceived 
notions, not only cute but almost cuddly. Copyright is held by the author / owner(s). SIGGRAPH 2008, 
Los Angeles, California, August 11 15, 2008. ISBN 978-1-60558-466-9/08/0008   
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1401130</article_id>
		<sort_key>980</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>76</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Rendertime procedural feathers through blended guide meshes]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1401032.1401130</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401130</url>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>F.2.2</cat_node>
				<descriptor>Computations on discrete structures</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>F.2.2</cat_node>
				<descriptor>Geometrical problems and computations</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Curve, surface, solid, and object representations</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Geometric algorithms, languages, and systems</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061.10010063</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures->Computational geometry</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010399</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Parametric curve and surface models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010401</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Volumetric models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1098581</person_id>
				<author_profile_id><![CDATA[81421601551]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Daniel]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Seddon]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Framestore]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098582</person_id>
				<author_profile_id><![CDATA[81421598523]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Martin]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Auflinger]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Framestore]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098583</person_id>
				<author_profile_id><![CDATA[81100122636]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[David]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Mellor]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Framestore]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Rendertime Procedural Feathers Through Blended Guide Meshes Daniel Seddon Martin Auflinger David Mellor 
Framestore 1 Introduction In Monster Stork the challenge was to create a system for efficiently creating 
and grooming photo realistic feathers for a CG bird. Feather grooming can be difficult, as there are 
a number of interdependent factors to be adjusted and controlled in order to achieve the final look. 
The appearance of the individual feathers is largely determined by both continuous and localised variations 
in their size and shape across the body. At the same time, each feather can have an individual textural 
quality, in terms of colour and barb shaping. We present a method which involves creating groomed feathers 
that are then blended by painting a weighting function on to a base mesh. This gives a continuous variation 
in groom across the bird s body, allowing for control over the appearance of specific feathers as well 
as overall groom. 2 Basic Approach The basic idea for the system is that the grooming process is broken 
down in to two stages. In the first stage, a number of key feathers are chosen from the bird to be groomed. 
These feathers are then recreated with a procedural feather grooming system to give a matching appearance 
in terms of a number of pre-defined groom filter attributes. These attributes are: Splitting imitates 
the splitting seen on feathers Scraggle -random noise used to displace the barbs Tangle a scraggle that 
accumulates down the barb Clipping -takes random cuts along the length of the barbs (a) Real (b) Procedural 
 Real scanned feathers(a) and their procedurally generated copies(b). The smaller feathers have more 
scraggle / tangle and clipping, whereas the larger feathers have only splitting. This procedural process 
takes place at rendertime using a Renderman DSO. A representative Bézier grid is passed to Renderman 
in order to give the shape and size, and also the painted groom attributes. Because of the shape of the 
Bézier, a feather is easily built by first evenly distributing the barbs along the v parameter of the 
Bézier grid, and then applying the filters to give the look. The number of barbs on the feathers was 
determined by how many barbs of a given width and separation could fit on to the grid, which meant that 
the granularity could be controlled directly by these values. In all about 15 feathers were groomed for 
our bird. In the second stage, once these filters have been used to match the appearance of the feathers 
on our stork, the grooming process takes place using these feathers as a template, with the groomed feathers 
determining the size and shape of the feathers on the whole of the bird. A base mesh is used as a grooming 
surface, upon which a weighting function for each feather is painted to define what fraction of each 
of the groom feathers is present on each section of the bird. After this the size of the groom feathers 
determines the feather density in a scatter function that distributes points across the mesh. These scattered 
points then use the weighting function, along with other grooming attributes such as orientation and 
lift, to build feathers at each point from the guide feathers. Following this, there are a number of 
cleanup stages designed to deal with feather collisions and interpenetration. All of which gives a final 
output to the Renderman DSO which is a mesh of Bézier grids, enabling it to build smoothly interpolated 
but unique feathers to a variable degree of granularity for different sizes in frame. 3 Discussion The 
method used in our commercial gave us great flexibility in changing what the feathers looked like. This 
was necessary in the case of a stork, as we didn't have the option of using recognisable texturing or 
colour, and the shaping/groom of a stork's feathers is one of its characteristics, making it very sensitive 
to the size and shape of the feathers. Also, several of the shots in the commercial needed the bird to 
have an aged appearance, and being able to resize and shape the feathers with simple modelling and displacements 
meant that this was easily accomplished over a few days at the end of the project. The system could 
easily be adapted to do more colourful birds. This could be achieved partly through texture mapping for 
gross colour, but also using procedural textures for patterning, as such an approach would work well 
in tandem with our generated feathers. We did have some problems with this approach, most of which could 
be solved for future projects. The main difficulty was that the amount of groom data was very high so 
much so that we had to write a caching system for the DSO. This slowed down the work flow. In retrospect, 
the blending of the groom filter attributes could have been carried out in the DSO, removing the need 
to store much more than a few attributes per feather. Future work would look at optimising this part 
of the process. Copyright is held by the author / owner(s). SIGGRAPH 2008, Los Angeles, California, August 
11 15, 2008. ISBN 978-1-60558-466-9/08/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1401131</article_id>
		<sort_key>990</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>77</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Fat panda]]></title>
		<subtitle><![CDATA[from 2D to 3D visual design development in Kung Fu Panda]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1401032.1401131</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401131</url>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>H.5.1</cat_node>
				<descriptor>Animations</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>H.5.1</cat_node>
				<descriptor>Artificial, augmented, and virtual realities</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.6.7</cat_node>
				<descriptor>Environments</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.8</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341.10010349.10011310</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Simulation types and techniques->Simulation by animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341.10010366.10010367</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Simulation support systems->Simulation environments</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010507</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Graphics recognition and interpretation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010392</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Mixed / augmented reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010866</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Virtual reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010866</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Virtual reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1098584</person_id>
				<author_profile_id><![CDATA[81421600378]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Wes]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Burian]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Dream Works Animation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<article_type art_type="regular_article" />
	</article_rec>
	</section>
</content>
</proceeding>
