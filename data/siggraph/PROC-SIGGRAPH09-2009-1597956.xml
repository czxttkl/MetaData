<?xml version="1.0" encoding="ISO-8859-1"?>
<!DOCTYPE proceeding SYSTEM "proceeding.dtd">
<proceeding ver="6.0" ts="04/10/2010">
<conference_rec>
	<conference_date>
		<start_date>08/03/2009</start_date>
		<end_date>08/07/2009</end_date>
	</conference_date>
	<conference_loc>
		<city><![CDATA[New Orleans]]></city>
		<state>Louisiana</state>
		<country></country>
	</conference_loc>
	<conference_url></conference_url>
</conference_rec>
<series_rec>
	<series_name>
		<series_id>SERIES382</series_id>
		<series_title><![CDATA[International Conference on Computer Graphics and Interactive Techniques]]></series_title>
		<series_vol></series_vol>
	</series_name>
</series_rec>
<proceeding_rec>
	<proc_id>1597956</proc_id>
	<acronym>SIGGRAPH '09</acronym>
	<proc_desc>ACM SIGGRAPH 2009 Emerging Technologies</proc_desc>
	<conference_number>2009</conference_number>
	<proc_class>conference</proc_class>
	<proc_title></proc_title>
	<proc_subtitle></proc_subtitle>
	<proc_volume_no></proc_volume_no>
	<isbn13>978-1-60558-833-9</isbn13>
	<issn></issn>
	<eissn></eissn>
	<copyright_year>2009</copyright_year>
	<publication_date>08-03-2009</publication_date>
	<pages>26</pages>
	<plus_pages></plus_pages>
	<price><![CDATA[]]></price>
	<other_source></other_source>
	<abstract>
		<par><![CDATA[<p>SIGGRAPH 2009's Emerging Technologies presents innovative technologies and applications in many fields, including displays, robotics, input devices, and interaction techniques. The demos are available for attendees to try out and discuss with the creators.</p>]]></par>
	</abstract>
	<publisher>
		<publisher_id>PUB27</publisher_id>
		<publisher_code>ACMNY</publisher_code>
		<publisher_name>ACM</publisher_name>
		<publisher_address>2 Penn Plaza, Suite 701</publisher_address>
		<publisher_city>New York</publisher_city>
		<publisher_state>NY</publisher_state>
		<publisher_country>USA</publisher_country>
		<publisher_zip_code>10121-0701</publisher_zip_code>
		<publisher_contact>Bernard Rous</publisher_contact>
		<publisher_phone>212 869-7440</publisher_phone>
		<publisher_isbn_prefix></publisher_isbn_prefix>
		<publisher_url>www.acm.org/publications</publisher_url>
	</publisher>
	<sponsor_rec>
		<sponsor>
			<sponsor_id>SP932</sponsor_id>
			<sponsor_name>ACM Special Interest Group on Computer Graphics and Interactive Techniques</sponsor_name>
			<sponsor_abbr>SIGGRAPH</sponsor_abbr>
		</sponsor>
	</sponsor_rec>
	<categories>
		<primary_category>
			<cat_node/>
			<descriptor/>
			<type/>
		</primary_category>
	</categories>
	<chair_editor>
		<ch_ed>
			<person_id>P1617437</person_id>
			<author_profile_id><![CDATA[81100606762]]></author_profile_id>
			<orcid_id></orcid_id>
			<seq_no>1</seq_no>
			<first_name><![CDATA[Daniel]]></first_name>
			<middle_name><![CDATA[]]></middle_name>
			<last_name><![CDATA[Wigdor]]></last_name>
			<suffix><![CDATA[]]></suffix>
			<affiliation><![CDATA[Microsoft Surface, Emerging Technologies Curation]]></affiliation>
			<role><![CDATA[Conference Chair]]></role>
			<email_address><![CDATA[]]></email_address>
		</ch_ed>
	</chair_editor>
	<ccc>
		<copyright_holder>
			<copyright_holder_name>ACM</copyright_holder_name>
			<copyright_holder_year>2009</copyright_holder_year>
		</copyright_holder>
	</ccc>
</proceeding_rec>
<content>
	<article_rec>
		<article_id>1597957</article_id>
		<sort_key>10</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>1</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[AmbiKraf]]></title>
		<subtitle><![CDATA[an embedded non-emissive and fast changing wearable display]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1597956.1597957</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1597957</url>
		<abstract>
			<par><![CDATA[<p>AmbiKraf is a novel non-emissive analog fabric display that has rapid color changing capability. It is integrated into the soft fabric enabling novel animations and interactive scenarios in the normal clothes that we wear. The core novelty of this project lies within the actual implementation of a fabric embedded with Peltier junction semiconductor to form a robust and wearable fabric display. In addition, our key novelties extend to its fast, accurate and bidirectional control through in-fabric semiconductor based heating and cooling systems. This enables the capability of displaying animations and facilitating many interaction scenarios through everyday clothes.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.4.0</cat_node>
				<descriptor>Image displays</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>A.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P1617438</person_id>
				<author_profile_id><![CDATA[81385592390]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Roshan]]></first_name>
				<middle_name><![CDATA[Lalintha]]></middle_name>
				<last_name><![CDATA[Peiris]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio-NUS CUTE Center & Mixed Reality Lab Singapore]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617440</person_id>
				<author_profile_id><![CDATA[81100418633]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Adrian]]></first_name>
				<middle_name><![CDATA[David]]></middle_name>
				<last_name><![CDATA[Cheok]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio-NUS CUTE Center & Mixed Reality Lab Singapore]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617441</person_id>
				<author_profile_id><![CDATA[81317500954]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[James]]></first_name>
				<middle_name><![CDATA[Keng Soon]]></middle_name>
				<last_name><![CDATA[Teh]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio-NUS CUTE Center & Mixed Reality Lab Singapore]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617442</person_id>
				<author_profile_id><![CDATA[81100444317]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Owen]]></first_name>
				<middle_name><![CDATA[Noel Newton]]></middle_name>
				<last_name><![CDATA[Fernando]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio-NUS CUTE Center & Mixed Reality Lab Singapore]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617443</person_id>
				<author_profile_id><![CDATA[81442618216]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Wen]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yingqian]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio-NUS CUTE Center & Mixed Reality Lab Singapore]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617444</person_id>
				<author_profile_id><![CDATA[81442600979]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Andre]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lim]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio-NUS CUTE Center & Mixed Reality Lab Singapore]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617445</person_id>
				<author_profile_id><![CDATA[81474678755]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>7</seq_no>
				<first_name><![CDATA[Pan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio-NUS CUTE Center & Mixed Reality Lab Singapore]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617446</person_id>
				<author_profile_id><![CDATA[81442619321]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>8</seq_no>
				<first_name><![CDATA[Doros]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Polydorou]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio-NUS CUTE Center & Mixed Reality Lab Singapore]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617447</person_id>
				<author_profile_id><![CDATA[81442612949]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>9</seq_no>
				<first_name><![CDATA[Kian]]></first_name>
				<middle_name><![CDATA[Peng]]></middle_name>
				<last_name><![CDATA[Ong]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio-NUS CUTE Center & Mixed Reality Lab Singapore]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617439</person_id>
				<author_profile_id><![CDATA[81442614675]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>10</seq_no>
				<first_name><![CDATA[Mili]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tharakan]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio-NUS CUTE Center & Mixed Reality Lab Singapore]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1179143</ref_obj_id>
				<ref_obj_pid>1179133</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Shibutani, M., and Wakita, A. 2006. Fabcell: fabric element. In <i>SIGGRAPH '06: ACM SIGGRAPH 2006 Emerging technologies</i>, ACM, New York, NY, USA, 9.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 AmbiKraf: An embedded non-emissive and fast changing wearable display Roshan Lalintha Peiris*, Adrian 
David Cheok, James Keng Soon Teh, Owen Noel Newton Fernando Wen Yingqian, Andre Lim, Pan Yi, Doros Polydorou, 
Kian Peng Ong, Mili Tharakan Keio-NUS CUTE Center &#38; Mixed Reality Lab Singapore  1 Introduction 
AmbiKraf is a novel non-emissive analog fabric display that has rapid color changing capability. It is 
integrated into the soft fabric enabling novel animations and interactive scenarios in the normal clothes 
that we wear. The core novelty of this project lies within the actual implementation of a fabric embedded 
with Peltier junc­tion semiconductor to form a robust and wearable fabric display. In addition, our key 
novelties extend to its fast, accurate and bidirec­tional control through in-fabric semiconductor based 
heating and cooling systems. This enables the capability of displaying anima­tions and facilitating many 
interaction scenarios through everyday clothes. Fabric displays are gaining more attention as a novel 
form of wear­able displays. Researchers are looking into new ways of converting fabrics for many purposes 
ranging from information display to a medium of expression. Fabric displays are generally categorized 
as emissive and non-emissive displays. Emissive displays include a range of displays that embed lighting 
material including LEDs, electro-luminescent wires and sheets, etc. These displays, are less popular 
as a daily item of clothing as they draw too much attention, are obtrusive and distractive. Usually such 
displays are used for more speci.c functionalities or purposes such as advertising. Due to such limitations 
of emissive clothing, researchers have fo­cused on non-emissive technologies for displays. Fabcell [Shibu­tani 
and Wakita 2006], uses liquid crystal inks as a multicolor non emissive fabric display with conductive 
yarn as the source of heat to actuate the color change. Liquid crystals are able to display a range of 
colors. Thermochromic inks have also been used for non-emissive fabric displays. They have the advantage 
of being ro­bust and washable. However, thermochromic inks provide single color change, which allows 
it to only reveal an image or a color by heating. Such displays have typically used conductive yarn as 
the source of heat for actuation. Without any cooling technology for thermally actuated color change, 
these fabric displays are unidirec­tional with slow color change, as there is no way to actuate them 
back to the original color. Therefore the disadvantage is that ani­mations or fast color change is impossible 
on these thermochomic fabric displays. Overcoming these limitations such as the unsuitability of emis­sive 
displays in more social and home environments, or the non-robustness, slow and one directional change 
of current non­emissive displays, we thrive to innovate a novel concept of non­emissive fabric display 
technology which is robust for wear, can display impressive subtle animations with high speed control 
of * Contact: ambikraf@mixedrealitylab.org color change, and enable more interactive scenarios through 
fash­ion. 2 System Description AmbiKraf uses thermochromic ink as the color changing agent. A novel 
fabric consisting of light weight and small semiconductor Peltier junctions intertwined into it is used 
for temperature actu­ating. Each Peltier junction provides heating and cooling, thus en­abling bidirectional 
accurate and robust control. With the use of a tuned control circuit, a matrix of such Peltier modules 
are controlled in the cloth. We use thermochromic inks with optimized color ac­tuation temperatures to 
suit the wearers comfort and the speed of color change. Different thermochromic inks are mixed and com­bined 
with textile binder and screen printed on to the fabric thus enabling easy implementation and robustness 
of the display allow­ing wearability and washing. These two customized technologies together present 
us the fast and accurate control of a multicolor display thus presenting the capability of animated sequences 
as a breakthrough in non-emissive display technologies. 3 User Experience From simple calming animated 
displays, we demonstrate Am-biKraf s usage in novel interaction-based scenarios that enable bidi­rectional 
display based multimodal communication. To demon­strate, the users can experience bidirectional interaction 
as they send and receive color actuated interactive messages by wearing the clothes. In addition they 
can experience different color animations in their clothes as they move closer or further away from each 
other. Through these scenarios visitors can explore this novel technology as a personal and emotional 
experience. 4 Conclusion AmbiKraf is a multicolor non-emissive fabric display that is robust, fast, 
and accurate that presents novel animated and interactive fab­rics. This paves the way to new concepts 
and applications in the .eld of wearable media. By embedding such technology in a true wearable form, 
we envision it to revolutionize the society through new forms of interactive and communicative wearable. 
 References SHIBUTANI, M., AND WAKITA, A. 2006. Fabcell: fabric element. In SIGGRAPH 06: ACM SIGGRAPH 
2006 Emerging technolo­gies, ACM, New York, NY, USA, 9. Copyright is held by the author / owner(s). SIGGRAPH 
2009, New Orleans, Louisiana, August 3 7, 2009. ISBN 978-1-60558-726-4/09/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1597958</article_id>
		<sort_key>20</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>2</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Anthropomorphization of a space with implemented human-like features]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1597956.1597958</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1597958</url>
		<abstract>
			<par><![CDATA[<p>We propose an anthropomorphization method that uses attachable human-like features like eyes and arms. These features are attached to the target, making it into an anthropomorphic agent, and providing the target's information to users intuitively. The anthropomorphized space can use gestures, pointing, emotion, and expression to initiate interaction.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>H.5.2</cat_node>
				<descriptor>Interaction styles (e.g., commands, menus, forms, direct manipulation)</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>H.5.1</cat_node>
				<descriptor>Artificial, augmented, and virtual realities</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>A.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010866</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Virtual reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010392</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Mixed / augmented reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010866</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Virtual reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P1617448</person_id>
				<author_profile_id><![CDATA[81414597701]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Hirotaka]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Osawa]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617449</person_id>
				<author_profile_id><![CDATA[81384618568]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Kentaro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ishii]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Japan Science and Technology Agency, ERATO]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617450</person_id>
				<author_profile_id><![CDATA[81414599861]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Toshihiro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Osumi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617451</person_id>
				<author_profile_id><![CDATA[81314484611]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Ren]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ohmura]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617452</person_id>
				<author_profile_id><![CDATA[81100282909]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Michita]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Imai]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[ARToolKit. http://www.hitl.washington.edu/artoolkit/.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Mori, M. 1970. The uncanny valley. <i>Energy 7</i>, 4, 33--35.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Osawa, H., Ohmura, R., and Imai, M. 2009. Using attachable humanoid parts for realizing imaginary intention and body image. <i>International Journal of Social Robotics 1</i>, 1.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 § ¶ Figure 1: Anthropomophization of Several Spaces 1 Introduction We propose an anthropomorphization 
method that uses at­tachable human-like features like eyes and arms. These fea­tures are attached to 
the target, making it into an anthro­pomorphic agent, and providing the target s information to users 
intuitively. The anthropomorphized space can use ges­tures, pointing, emotion, and expression to initiate 
interac­tion. We implemented robotic human-like parts for anthropomor­phization and designed authroing 
tool that authors commu­nication contents for embodied home appliance. Users easily convert an object 
into embodied agent and create their own contents interactively to the agent by our method. 2 Features 
By our method, a user accepts a target s intention and imag­inary body image. Our proposed method, in 
comparison to the use of an independent agent, is also better at gaining the attention of users for explaining 
the functions of arti-facts[Osawa et al. 2009]. It is also possible to use metaphors for pointing at 
the lo­cation of an artifact by our method. For example, the printer can say I have a stomachache when 
it is jammed. These metaphors would be impossible to use for normal ex­planation using an independent 
agent. If we study these metaphors more closely, we might be able to use more un­derstandable expressions 
for the artifact using our method. This method also indirectly avoids an uncanny valley which is strong 
repulsion between an industrial robot and a hu­man[Mori 1970]. Using human-like features, we can verify 
separately what parts or actions of parts are positive and empathic for users and what parts or actions 
are negative. We can research anthropomorphization that is not uncanny *e-mail: osawa@ayu.ics.keio.ac.jp 
e-mail: kenta@designinterface.jp e-mail: tosihiro@ayu.ics.keio.ac.jp §e-mail: ren@ayu.ics.keio.ac.jp 
¶e-mail: michita@ics.keio.ac.jp by changing the humanoids parts, without constraining the humanoid. 
 3 Principle Our system consists of a camera, a skin-sensor, human-like eye-parts, human-like arm-parts, 
mouth-parts, and a com­puter. Our anthropomorphized object did not need to ma­nipulate other objects 
using its attahed hands. Because the target already has its own task, and our devices are used for just 
expressionism. Instead of manipulation, these devices must be simple and light so they can be easily 
attached. We developed human-like robotic devices and attached them to our target by using hook and loop 
fasteners. Our system uses a Bluetooth connection to each body part. Our sys­tem uses a visual marker-detection 
technique by ARToolKit for calculating positions of each part and pointing targets [ARToolKit ]. 4 Conclusion 
We proposed the anthropomorphization method and the au­thoring tool. In future, we conducted user study 
with non experts to evaluate our tool. Acknowledgements The .rst author was supported in part by the 
JSPS Research Fellowships for Young Scientists. This work was supported in part by Grant in Aid for the 
Global Center of Excellence Program for Center for Education and Research of Symbi­otic, Sage and Secure 
System Design from the Ministry of Education, Culture, Sport, and Technology in Japan. References ARToolKit. 
http://www.hitl.washington.edu/artoolkit/. Mori, M. 1970. The uncanny valley. Energy 7, 4, 33 35. Osawa, 
H., Ohmura, R., and Imai, M. 2009. Using at­tachable humanoid parts for realizing imaginary intention 
and body image. International Journal of Social Robotics 1, 1. Copyright is held by the author / owner(s). 
SIGGRAPH 2009, New Orleans, Louisiana, August 3 7, 2009. ISBN 978-1-60558-726-4/09/0008 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1597959</article_id>
		<sort_key>30</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>3</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Baby type robot "YOTARO"]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1597956.1597959</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1597959</url>
		<abstract>
			<par><![CDATA[<p>Everyone had experienced meet a baby in the train or elevator and unconsciously smiles, waves hands or tries to outstare him. The kindness feeling to babies is inherent in human's natural emotion. When we interact with a baby, while they still not have capacity to do things by themselves, we will need to do anything and everything to them. The human's baby is the most conspicuous example about human's instincts where everybody takes care and be tender to him.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.2.9</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>A.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010213.10010204</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Control methods->Robotic planning</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010520.10010553.10010554</concept_id>
				<concept_desc>CCS->Computer systems organization->Embedded and cyber-physical systems->Robotics</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010199.10010204</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Planning and scheduling->Robotic planning</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P1617453</person_id>
				<author_profile_id><![CDATA[81442619834]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Hiroki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kunimura]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Tsukuba]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617454</person_id>
				<author_profile_id><![CDATA[81442606217]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Chiyoko]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ono]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Tsukuba]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617455</person_id>
				<author_profile_id><![CDATA[81442595397]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Madoka]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hirai]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Tsukuba]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617456</person_id>
				<author_profile_id><![CDATA[81442615701]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Masatada]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Muramoto]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Tsukuba]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617457</person_id>
				<author_profile_id><![CDATA[81442615635]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Wagner]]></first_name>
				<middle_name><![CDATA[Tetsuya]]></middle_name>
				<last_name><![CDATA[Matsuzaki]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Tsukuba]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617458</person_id>
				<author_profile_id><![CDATA[81442595062]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Toshiaki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Uchiyama]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Tsukuba]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617459</person_id>
				<author_profile_id><![CDATA[81331504069]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>7</seq_no>
				<first_name><![CDATA[Kazuhito]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Shiratori]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Tsukuba]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617460</person_id>
				<author_profile_id><![CDATA[81451595006]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>8</seq_no>
				<first_name><![CDATA[Junichi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hoshino]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Tsukuba]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1597960</article_id>
		<sort_key>40</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>4</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[Back to the mouth]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1597956.1597960</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1597960</url>
		<abstract>
			<par><![CDATA[<p>In this paper, we focus on breathe and propose interactive system Back to the mouth. There are many studies related to respiration of breath, but our emphasis is on mouth odor. Study of smell using input is not progressing in comparison with study of smell using output. This system uses mouth odor strength as a trigger in addition to breathing action. By using mouth odor, there is possibility that a better breath device would be developed.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>H.5.1</cat_node>
				<descriptor>Artificial, augmented, and virtual realities</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>A.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010866</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Virtual reality</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010392</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Mixed / augmented reality</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010866</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Virtual reality</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P1748190</person_id>
				<author_profile_id><![CDATA[81447593668]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Takuya]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Iwamoto]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Kanazawa Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1748191</person_id>
				<author_profile_id><![CDATA[81447603110]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Yusuke]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sasayama]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Kanazawa Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1748192</person_id>
				<author_profile_id><![CDATA[81447598447]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Mitsuo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Motoki]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Kanazawa Technical College]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1748193</person_id>
				<author_profile_id><![CDATA[81448597336]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Takayuki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kosaka]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Kanazawa Technical College]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[ARToolKit. http://www.hitl.washington.edu/artoolkit/.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Mori, M. 1970. The uncanny valley. <i>Energy 7</i>, 4, 33--35.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Osawa, H., Ohmura, R., and Imai, M. 2009. Using attachable humanoid parts for realizing imaginary intention and body image. <i>International Journal of Social Robotics 1</i>, 1.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Back to the mouth Takuya Iwamoto*1, Yusuke Sasayama*2, Mitsuo Motoki*3, Takayuki Kosaka*4 \\202.223.149.27\kosaka\kosaka\....\...bmp 
system.jpg ivrc.jpg ivrc.jpg \\202.223.149.27\kosaka\iwamoto\Pics\640x480.JPG \\202.223.149.27\kosaka\iwamoto\....\DSC06855[1].jpg 
Kanazawa Institute of Technology*1,2 Kanazawa Technical College*3,4 E-mail;{takuya-i*1, satella*2, kosaka*4}@kosaka-lab.com 
 1. Introduction  In this paper, we focus on breathe and propose interactive system Back to the mouth 
. There are many studies related to respiration of breath, but our emphasis is on mouth odor. Study of 
smell using input is not progressing in comparison with study of smell using output. This system uses 
mouth odor strength as a trigger in addition to breathing action. By using mouth odor, there is possibility 
that a better breath device would be developed. 2. Back to the mouth  Back to the mouth is a system 
activated by breath and mouth odor. The user eats/drinks something to control the user s mouth odor. 
Then it is possible to kill monsters on a screen by blowing/sucking a blowgun-type device. The user plays 
this system, repeating the actions of, eating/drinking, aiming, blowing and sucking (Fig. 1).  Fig.1: 
How to play Back to the mouth .   2.1 System constitution Fig.2 shows system constitution of Back 
to the mouth . The user aims a blowgun-type device and blows the device towards the monsters. The user 
can strike the monsters only for specific mouth odor. To strike them, the user needs to control their 
mouth odor by eating or drinking during the system. There are the monsters which aren t repulsed unless 
the user s odor is weakened. Therefore, a user must weaken the user s mouth odor by drinking green tea 
or water corresponding to the monster s weak point. A flying arrow is affected by the wind as in the 
real world. Therefore, it might not go straight. There are 8 blower fans reproduce wind in the system. 
The user feels the wind from the blower fans and needs to aim accordingly. The smell that comes from 
user s mouth is diffused by the wind. We set up an IR LED in the center of a screen and an IR camera 
instrument on the blowgun to measure the direction of the device. The screen is covered by a white cloth 
to hide the IR.  Fig.2: System constitution.  3. Result  We experimented the system(Fig.4). Many 
users were amazed by the system. They found it interesting to learn that the user had to eat snacks and 
drinks in order play. Some children ate their dislike food to beat monsters. We have potential to develop 
a system which can change the likes and dislikes food. Through our studies and findings we can build 
a better mouth odor device in the future.    Fig.4: experimentation of Back to the mouth .  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1597961</article_id>
		<sort_key>50</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>5</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>5</seq_no>
		<title><![CDATA[Bloxels]]></title>
		<subtitle><![CDATA[glowing blocks as volumetric pixels]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1597956.1597961</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1597961</url>
		<abstract>
			<par><![CDATA[<p>In this paper, we propose a novel block-shaped tangible interface named Bloxel (see Figure 1). A Bloxel is a translucent cubical block that glows in full color and communicates with the neighboring Bloxels through high-speed flickers.</p> <p>Our significant accomplishment is that users can build displays with a variety of shapes by stacking hundreds of Bloxels on a tabletop surface. Each Bloxel obtains its color data from the lower Bloxel through infra-red high-speed flickers, and transfers a series of color data to the upper Bloxel. In this way, Bloxels serve as volumetric pixels which can display meaningful content as a whole. With our module-based approach, we introduce a ground-breaking display technology. Moreover, as an augmented version of children's block play, Bloxels will have a significant novel impact on the field of physical computing and tangible interfaces.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.4.10</cat_node>
				<descriptor>Volumetric</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>A.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010240.10010242</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision representations->Shape representations</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P1617466</person_id>
				<author_profile_id><![CDATA[81442599439]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Jinha]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lee]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Toky]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617467</person_id>
				<author_profile_id><![CDATA[81100527572]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Yasuaki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kakehi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617468</person_id>
				<author_profile_id><![CDATA[81100095332]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Takeshi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Naemura]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1085755</ref_obj_id>
				<ref_obj_pid>1085714</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Dunn, H. N., Nakano, H., and Gibson, J. 2003. Block jam: A tangible interface for interactive music. In NIME2003, pp. 170--177.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1401030</ref_obj_id>
				<ref_obj_pid>1400885</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Kimura, S., Oguchi, R., Tanida, H., Kakehi, Y., Takahashi, K., and Naemura, T. 2008. Pvlc projector: Image projection with imperceptible pixel-level metadata. In ACM SIGGRAPH 2008 Posters, B177.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>503438</ref_obj_id>
				<ref_obj_pid>503376</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Sharlin, E., Itoh, Y., Watson, B., Kitamura, Y., Liu, L., and Sutphen, S. 2002. Cognitive cubes: a tangible user interface for cognitive assessment. In ACM SIGCHI 2002, pp. 347--354.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Bloxels: Glowing Blocks as Volumetric Pixels Jinha Lee* Yasuaki Kakehi* Takeshi Naemura* The UniversityofTokyo 
Keio University The UniversityofTokyo Figure 1: Bloxels Figure 2: Application for Entertainment Figure 
3: Application for Education 1 Introduction In this paper, we propose a novel block-shaped tangible interface 
named Bloxel (see Figure 1). A Bloxel is a translucent cubical block that glows in full color and communicates 
with the neigh­boring Bloxels through high-speed .ickers. Our signi.cant accomplishmentis that users 
canbuild displays with a variety of shapes by stacking hundreds of Bloxels on a tabletop surface. Each 
Bloxel obtains its color data from the lower Bloxel through infra-red high-speed .ickers,and transfersa 
seriesof color data to the upper Bloxel. In this way, Bloxels serve as volumetric pixels which can display 
meaningful content as a whole. With our module-based approach, we introduce a ground-breaking display 
technology. Moreover,asan augmentedversionof children s block play, Bloxels will have a signi.cant novel 
impact on the .eld of physical computing and tangible interfaces. 2 Technical Innovations of Bloxels 
Sofar,severaltypesof block-shaped tangibledevicehavebeenpro­posed[Sharlin et al. 2002][Dunn et al. 2003]. 
Compared with these works, technical innovations of our system are as follows: First, our optical design 
for simple communication through light is crucial for the intuitive manipulation of physical blocks. 
ABloxel consists of two full color LEDs for display, nine infra-red LEDs for data transmission,aphoto 
detector,abattery andamicro controller. The infra-red LEDs are placed so as to realize the data transmission 
even when the neighboring Bloxels are not completely in contact with each other. Second, our data processing 
method enables a simple system con­.guration. While each Bloxel communicates only with the neigh­boring 
ones, 3D sensors or cameras are not necessary to track the positions of the Bloxels. Finally, to send 
signals to the base of the stacked Bloxels, we have inventeda horizontal tabletop display system. Our 
specialized DLP *e-mail: vlc@hc.ic.i.u-tokyo.ac.jp projector [Kimura et al. 2008] can emit high-speed 
.ickering sig­nals pixel by pixel to the base. This allows users to realize several kinds of applications 
as demonstrated in our video. 3 Applications We believe that Bloxels can be applied for media art works 
and en­tertainment purposes as well as for display technologies and human interfaces. By using Bloxels, 
we have already implemented some applications. One is an application for entertainment (see Figure 2). 
In this appli­cation, users can see hidden animations (e.g. .owers) by stacking Bloxels on the tabletop. 
Another application is for education (see Figure 3). In this application, Bloxel can serve as an interactive 
tutorial for a shape creation process. When stacking up Bloxels followed by signals, users can be guided 
to create speci.c shaped objects. In the future, we plan to develop much more applications by using the 
Bloxels in various situations. References DUNN,H.N.,NAKANO,H., AND GIBSON,J. 2003. Block jam: A tangible 
interface for interactive music. In NIME2003, pp. 170 177. KIMURA, S., OGUCHI, R., TANIDA, H., KAKEHI, 
Y., TAKA-HASHI, K., AND NAEMURA, T. 2008. Pvlc projector: Image projection with imperceptible pixel-level 
metadata. In ACM SIG-GRAPH 2008 Posters, B177. SHARLIN, E., ITOH, Y., WATSON, B., KITAMURA, Y., LIU, 
L., AND SUTPHEN,S. 2002. Cognitive cubes:a tangible user inter­face for cognitive assessment. In ACM 
SIGCHI 2002, pp. 347 354. Copyright is held by the author / owner(s). SIGGRAPH 2009, New Orleans, Louisiana, 
August 3 7, 2009. ISBN 978-1-60558-726-4/09/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1597962</article_id>
		<sort_key>60</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>6</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>6</seq_no>
		<title><![CDATA[CRISTAL, control of remotely interfaced systems using touch-based actions in living spaces]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1597956.1597962</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1597962</url>
		<abstract>
			<par><![CDATA[<p>The amount of digital appliances and media found in domestic environments has risen drastically over the last decade, for example, digital TVs, DVD and Blu-ray players, digital picture frames, digital gaming systems, electronically moveable window blinds, and robotic vacuum cleaners. As these devices become more compatible to Internet and wireless networking (e.g. Internet-ready TVs, streaming digital picture frames, and WiFi gaming systems, such as Nintendo's Wii and Sony's Playstation) and as networking and WiFi home infrastructures become more prevalent, new opportunities arise for developing centralized control of these myriad devices and media into so called "Universal remote controls". However, many remote controls lack intuitive interfaces for mapping control functions to the device intended being controlled. This often results in trial and error button pressing, or experimentation with graphical user interface (GUI) controls, before a user achieves their intended action.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>H.5.2</cat_node>
				<descriptor>Haptic I/O</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>H.5.2</cat_node>
				<descriptor>Input devices and strategies (e.g., mouse, touchscreen)</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>A.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003120.10003121.10003125</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10011666</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Touch screens</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10011752</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Haptic devices</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P1617469</person_id>
				<author_profile_id><![CDATA[81319500586]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Thomas]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Seifried]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Media Interaction Lab]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617470</person_id>
				<author_profile_id><![CDATA[81442613741]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Christian]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Rendl]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Media Interaction Lab]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617471</person_id>
				<author_profile_id><![CDATA[81442617361]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Florian]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Perteneder]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Media Interaction Lab]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617472</person_id>
				<author_profile_id><![CDATA[81319495417]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Jakob]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Leitner]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Media Interaction Lab]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617473</person_id>
				<author_profile_id><![CDATA[81100048383]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Michael]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Haller]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Media Interaction Lab]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617474</person_id>
				<author_profile_id><![CDATA[81329491765]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Daisuke]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sakamoto]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617475</person_id>
				<author_profile_id><![CDATA[81416606592]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>7</seq_no>
				<first_name><![CDATA[Jun]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kato]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617476</person_id>
				<author_profile_id><![CDATA[81100424140]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>8</seq_no>
				<first_name><![CDATA[Masahiko]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Inami]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617477</person_id>
				<author_profile_id><![CDATA[81100544371]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>9</seq_no>
				<first_name><![CDATA[Stacey]]></first_name>
				<middle_name><![CDATA[D.]]></middle_name>
				<last_name><![CDATA[Scott]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Waterloo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1518733</ref_obj_id>
				<ref_obj_pid>1518701</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Sakamoto, D., Honda, K., Inami, M., Igarashi, T., "Sketch and Run: A Stroke-based Interface for Home Robots," <i>Proceeding of the 27th Annual SIGCHI Conference on Human Factors in Computing Systems (CHI2009)</i>, Boston, USA (April 2009).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>143011</ref_obj_id>
				<ref_obj_pid>142750</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Tani, M., Yamaashi, K., Tanikoshi K., Futakawa M., and Tanifuji S., "Object-oriented video: interaction with real-world objects through live video," <i>Proceedings of the SIGCHI conference on Human factors in computing systems</i>, Monterey, California, USA (1992), pp. 593--599.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 CRISTAL, Control of Remotely Interfaced Systems using Touch-based Actions in Living spaces. Thomas 
Seifried, Christian Daisuke Masahiko Stacey D. Rendl, Florian Perteneder, Sakamoto, Jun Kato Inami Scott 
Jakob Leitner, Michael Haller Media Interaction Lab University of Tokyo Keio University University of 
Waterloo mi-lab@fh-hagenberg.at sakamoto@designinterface.jp inami@inami.info s9scott@uwaterloo.ca  1. 
INTRODUCTION The amount of digital appliances and media found in domestic environments has risen drastically 
over the last decade, for example, digital TVs, DVD and Blu-ray players, digital picture frames, digital 
gaming systems, electronically moveable window blinds, and robotic vacuum cleaners. As these devices 
become more compatible to Internet and wireless networking (e.g. Internet-ready TVs, streaming digital 
picture frames, and WiFi gaming systems, such as Nintendo s Wii and Sony s Playstation) and as networking 
and WiFi home infrastructures become more prevalent, new opportunities arise for developing centralized 
control of these myriad devices and media into so called Universal remote controls . However, many remote 
controls lack intuitive interfaces for mapping control functions to the device intended being controlled. 
This often results in trial and error button pressing, or experimentation with graphical user interface 
(GUI) controls, before a user achieves their intended action. To address this issue, CRISTAL (Control 
of Remotely Interfaced Systems using Touch-based Actions in Living spaces) was developed. CRISTAL simplifies 
the control of our digital devices in and around the living room. The system provides a novel experience 
for controlling devices in a home environment by enabling users to directly interact with those devices 
on a live video image of their living room using multi-touch gestures on a digital tabletop. 2. CRISTAL 
CRISTAL consists of an interactive multi-touch surface and a camera, mounted in the ceiling of the living 
room capturing the entire living room. The interactive surface is integrated into the coffee table and 
extends its functionality. The display itself is only activated on demand and still can be used as a 
normal coffee table. When activated, the interactive surface shows the live camera feed. To control the 
devices in the living room, users can directly manipulate them by touching the corresponding video­image 
(cf. [2]). Depending on the controlled device different types of input are possible. A sliding gesture 
over a floor lamp, for example, modifies the brightness of light source. On the other hand, a similar 
gesture across the floor in front of a robotic vacuum cleaner defines a path for it to follow. To watch 
a movie, a user can select a movie from a digital movie collection invoked directly on the table surface 
and drag it onto the TV in the video image. The live video image displayed on the table gives continuous 
real-time feedback to the user. Currently, the user can control the following objects in the living room: 
 Light sources: can turn on/off and dim light sources, and  set a global lighting color (i.e. set a 
warm/cold light and all light sources are adjusted accordingly).  Audio: control of volume.  TV/Projector/Music 
Player: can choose movies or music and control CD and movie playback.  Digital picture frame: can select 
a physical photo album in the video image and drag it onto the picture frame.  Robotic vacuum cleaner: 
can control the movement and  position of a wireless vacuum cleaner [1]. Summarizing, the interface 
allows a more natural and intuitive interaction experience. REFERENCES [1] Sakamoto, D., Honda, K., 
Inami, M., Igarashi, T., Sketch and Run: A Stroke-based Interface for Home Robots, Proceeding of the 
27th Annual SIGCHI Conference on Human Factors in Computing Systems (CHI2009), Boston, USA (April 2009). 
 [2] Tani, M., Yamaashi, K., Tanikoshi K., Futakawa M., and Tanifuji S., Object-oriented video: interaction 
with real­world objects through live video, Proceedings of the SIGCHI conference on Human factors in 
computing systems, Monterey, California, USA (1992), pp. 593-599. Copyright is held by the author / owner(s). 
SIGGRAPH 2009, New Orleans, Louisiana, August 3 7, 2009. ISBN 978-1-60558-726-4/09/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1597963</article_id>
		<sort_key>70</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>7</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>7</seq_no>
		<title><![CDATA[Crystal zoetrope]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1597956.1597963</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1597963</url>
		<abstract>
			<par><![CDATA[<p>The modern zoetrope was invented by William Horner in the early 19th century. This simple device generates dynamic two-dimensional (2D) animations, and was widely popular until the introduction of film in the early 20 century. Even today, this optical toy can be found for sale in stores and in use in art installations. The zoetrope has been improved to display three-dimensional (3D) animations by several artists and companies, including Gregory Barsamian [1997], Stewart Dickson [2003], and Eric Dyer [2008]. The Ghibli museum displays a 3D zoetrope in the 'Bouncing Totoro,' and Pixar Animation Studios used one for 'Toy Story'. The 3D zoetrope animation provides a feeling of realism that cannot be achieved through traditional 2D animations.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>H.5.1</cat_node>
				<descriptor>Animations</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>A.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341.10010349.10011310</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Simulation types and techniques->Simulation by animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P1617478</person_id>
				<author_profile_id><![CDATA[81100384795]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Woohun]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lee]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[KAIST Design Media Lab]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617479</person_id>
				<author_profile_id><![CDATA[81416602734]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[JinHa]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Seong]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[KAIST Design Media Lab]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Barsamian, G. 1997. Juggler, ICC collection.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1401622</ref_obj_id>
				<ref_obj_pid>1401615</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Dyer, E. 2008. Bellows: bringing digital animation into the physical world. In ACM SIGGRAPH 2008 New Tech Demos (Los Angeles, California, August 11--15, 2008). SIGGRAPH '08. ACM, New York, NY, 1-1.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Dickson, S. 2003. Technical Statement: A Three-Dimensional Zoetrope of the Calabi-Yau Cross-Section in CP4, LEONARDO, the Journal of the ISAST (International Society of Arts, Science and Technology), Volume 36, Issue 3.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Crystal Zoetrope Woohun Lee*, JinHa Seong KAIST Design Media Lab.  Figure 1. (a) A crystal block with 
animation objects, (b) The inside of the crystal zoetrope, (c) The outside of the crystal zoetrope 1. 
Introduction The modern zoetrope was invented by William Horner in the early 19th century. This simple 
device generates dynamic two­dimensional (2D) animations, and was widely popular until the introduction 
of film in the early 20 century. Even today, this optical toy can be found for sale in stores and in 
use in art installations. The zoetrope has been improved to display three­dimensional (3D) animations 
by several artists and companies,including Gregory Barsamian [1997], Stewart Dickson [2003], andEric 
Dyer [2008]. The Ghibli museum displays a 3D zoetrope inthe Bouncing Totoro, and Pixar Animation Studios 
used one for Toy Story . The 3D zoetrope animation provides a feeling ofrealism that cannot be achieved 
through traditional 2D animations. 3D zoetropes generally consist of hundreds of physical animationobjects. 
Each object must be located at a particular position inspace by means of a support. Therefore, most 3D 
zoetropes have a complicated structure and tend to be bulky. Such zoetropes are fairly large, and usually 
serve as an artistic installation in amusement parks or museums. Viewers must keep a distance fromthese 
zoetropes to enjoy the animation. What if a 3D zoetrope could be made small enough to be embedded in 
daily objects or the environment? Smaller, 3D zoetrope animations in a table, wall, or floor would allow 
the infusion of unique visual entertainment into everyday contexts. 2. Implementation We employed the 
Sub-Surface Laser Engraving (SSLE) techniqueto produce a small but elaborate animation object for building 
an embedded 3D zoetrope. This 3D printing technique is commonlyused for making fancy souvenirs, and, 
in this project, 3D animation objects were engraved directly into a crystal block using the technique. 
We designed hundreds of animation objects for each timeframe using 3D Studio Max software, and allocated 
the objects alongconcentric circles or a spiral curve using Maya embedded language. The modeling data 
was engraved into a crystal discusing an SSLE apparatus[Figure1-(a)]. The crystal block was puton a motor 
coaxially and it was surrounded by 20 5W power LEDs. The motor rotated at 120RPM and the power LEDs blinked 
on and off at 80Hz. The animation objects in the crystal block were lit by the LED lights while they 
were revolving around the axis of the motor. Sequential afterimages of the reflected objects produced 
a salient 3D animation, just like a traditional zoetrope[Figure1-(b)]. For this project, we embedded 
the crystal zoetrope into a table.The zoetrope was covered by an acrylic board with a thin lightcut-off 
film intended to clarify the animation images and allow them to be seen even in the light. We attached 
a touch sensor array just beneath the cover to allow viewers to easily control thedirection, speed, or 
brightness of the 3d animation by touching the zoetrope. This minimal interactivity helps people better 
enjoy the animation[Figure1-(c)]. 3. Conclusion We produced a miniaturized, 3D zoetrope using the SSLE 
technique and embedded it into a table. Imagine seeing a jellyfish swimming through your table when you 
visit a restaurant or a pub.Our zoetrope is different from traditional 3D zoetropes not onlydue to its 
small size and ease of production, but because using the SSLE technique allows small animation objects 
to be engraved ina single crystal block, eliminating the requirement to support multiple animation objects. 
Consequently, the technique we propose allows the animation of subtle phenomena such as fog inthe sky 
or stars in space without difficulty. For this study, we applied the crystal zoetrope only to a table.However, 
this new 3D animation technique would be equallyapplicable to other everyday objects or environments; 
lighting,walls, floors, advertising and toys could all be embedded withthese zoetropes. The crystal zoetrope 
is a new visual medium that could make everyday objects more interactive and interesting. References 
Barsamian, G. 1997. Juggler, ICC collection. Dyer, E. 2008. Bellows: bringing digital animation into 
the physical world. In ACM SIGGRAPH 2008 New Tech Demos (Los Angeles, California, August 11 - 15, 2008). 
SIGGRAPH '08. ACM, New York, NY, 1-1. Dickson, S. 2003. Technical Statement: A Three-Dimensional Zoetrope 
of the Calabi-Yau Cross-Section in CP4, LEONARDO, the Journal of the ISAST (International Societyof Arts, 
Science and Technology), Volume 36, Issue 3. * woohun.lee@kaist.ac.kr Copyright is held by the author 
/ owner(s). SIGGRAPH 2009, New Orleans, Louisiana, August 3 7, 2009. ISBN 978-1-60558-726-4/09/0008 
 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1597964</article_id>
		<sort_key>80</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>8</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>8</seq_no>
		<title><![CDATA[Digital decal]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1597956.1597964</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1597964</url>
		<abstract>
			<par><![CDATA[<p>Decal, short for 'decalcomania', is a technique of transferring graphic images from specially prepared paper to another surface upon contact. It was popularized as a decorative technique for ceramics during the 1870s. The technique was also frequently used by the Surrealists. Presently, decals are widely used for instant lettering, tattoos, and pictograms. With the simple action of rubbing, graphic images can be quickly transferred to a variety of surfaces. This motivated us to ask the following question: "Can we apply the direct and playful image transfer method of decals to pen-based computing?" Our response to this is the concept of 'digital decal'. With digital decal, ordinary paper replaces special decal films. The user can simply place a sheet of printed paper on the tablet screen, and then rub on it with a pen. The rubbed image is transferred directly to the screen in the same manner as with a traditional decal (Figure 1). Furthermore, any thin material such as colorful fabrics or leaves also can be used in place of paper.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.4.0</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>A.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010383</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Image processing</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P1617480</person_id>
				<author_profile_id><![CDATA[81100384795]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Woohun]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lee]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[KAIST Design Media Lab.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617481</person_id>
				<author_profile_id><![CDATA[81100384871]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Geehyuk]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lee]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[HCI Lab.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617482</person_id>
				<author_profile_id><![CDATA[81442601738]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Jiseok]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Song]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[HCI Lab.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617483</person_id>
				<author_profile_id><![CDATA[81350583269]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Boram]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lee]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[KAIST Design Media Lab.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617484</person_id>
				<author_profile_id><![CDATA[81452601021]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Hyunjung]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kim]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[KAIST Design Media Lab.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>985731</ref_obj_id>
				<ref_obj_pid>985692</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Ryokai, K., Marti, S., and Ishii, H. 2004. I/O brush: drawing with everyday objects as ink. In Proceedings of <i>the SIGCHI Conference on Human Factors in Computing Systems</i>(Vienna, Austria). CHI '04. ACM, New York, NY, 303--310.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1278304</ref_obj_id>
				<ref_obj_pid>1278280</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Lee, W., Pak, J., Kim, S., Kim, H., and Lee, G. 2007. TransPen&amp;MimeoPad: a playful interface for transferring a graphic image to paper by digital rubbing. In <i>ACM SIGGRAPH 2007 Emerging Technologies</i> (San Diego, California). SIGGRAPH '07. ACM, New York, NY, 23.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Digital Decal Woohun Lee1*, Geehyuk Lee2, Jiseok Song2, Boram Lee1, Hyunjung Kim1 1KAIST Design Media 
Lab.,2HCI Lab. 1. Introduction Decal, short for decalcomania , is a technique of transferring graphic 
images from specially prepared paper to another surface upon contact. It was popularized as a decorative 
technique for ceramics during the 1870s. The technique was also frequently used by the Surrealists. Presently, 
decals are widely used for instant lettering, tattoos, and pictograms. With the simple action of rubbing, 
graphic images can be quickly transferred to a variety of surfaces. This motivated us to ask the following 
question: Can we apply the direct and playful image transfer method of decals to pen-based computing? 
Our response to this is the concept of digital decal . With digital decal, ordinary paper replaces special 
decal films. The user can simply place a sheet of printed paper on the tablet screen, and then rub on 
it with a pen. The rubbed image is transferred directly to the screen in the same manner as with a traditional 
decal (Figure 1). Furthermore, any thin material such as colorful fabrics or leaves also can be used 
in place of paper. Figure 1: Traditional decal (left) and digital decal (right) 2. Implementation In 
order to implement the concept of digital decal, we built a DecalPen prototype. The prototype consists 
of two main parts: a color sensing part and a position sensing part. It uses an optical fiber bundle 
to transfer an image under the tip of the DecalPen to a color sensor. The sensor determines the average 
RGB values of the image and sends them to the computer. For position sensing, it has an LC resonator 
at the tip tuned to the tablet hardware of the computer. The location of the pen is determined at the 
frequency of 100Hz. The computer then paints pixels in the detected color where the DecalPen is pointing. 
The real-time cycles of such scanning and painting create the experience of decal, spanning the physical 
real world and the digital computing world. When DecalPen was first built, we found that the sparseness 
ofprinted pixels was much higher than we expected. Even though thescanning and printing cycle is repeated 
about 100 times a second, each printed pixel is separated by approximately 3mm in normaloperations. To 
solve this problem, we have the system print a certain number of dots around the current point with a 
Gaussian probability profile. This spray effect produces a more natural­looking picture. Moreover, it 
is effective in color mixing when strokes overlap. Severe discordance between the original image and 
the copy wasalso a problem. The main cause seemed to be the system delay between the detection of the 
stylus position and the color sensing.A basic dead-reckoning technique was used in order to overcomethe 
system delay problem. Because rubbing is a regular and periodic movement, it is possible to predict the 
position of DecalPen for a short amount of time. After system delaycompensation, the quality of copied 
image was significantly improved. For the DecalPen demo application, we designed two transfer modes: 
scanner mode and stencil mode. With the former, the user can simply copy an image onto a screen (Figure 
2- left). With the latter, on the other hand, only a particular region of the screen predefined by the 
user is painted with scanned patterns (Figure 2­right). Figure 2: Scanner mode (left) and stencil mode 
(right) 3. Conclusion DecalPen can be used as a medium of artistic expression for children as well as 
adults. The users can copy interesting graphic images to a screen intuitively by simple rubbing, and 
edit it to express their feeling or ideas. In this respect, DecalPen is similar to I/O Brush [Ryokai 
et al, 2004] in a sense. However, DecalPen provides a considerably different artistic experience from 
I/O Brush, thanks to its directness and spatial congruence in interaction. DecalPen is an extension of 
our previous work TransPen &#38; MimeoPad [Lee et al, 2007], which enables digital rubbing . If we employ 
both devices together, the directness of image transfer via DecalPen and TransPen will make the process 
of drawing and editing more effective and pleasurable. References Ryokai, K., Marti, S., and Ishii, 
H. 2004. I/O brush: drawing with everyday objects as ink. In Proceedings of the SIGCHI Conference on 
Human Factors in Computing Systems(Vienna, Austria). CHI '04. ACM, New York, NY, 303-310. Lee, W., Pak, 
J., Kim, S., Kim, H., and Lee, G. 2007. TransPen &#38; MimeoPad: a playful interface for transferring 
a graphic image to paper by digital rubbing. In ACM SIGGRAPH 2007 Emerging Technologies (San Diego, California). 
SIGGRAPH '07. ACM, New York, NY, 23. * woohun.lee@kaist.ac.kr Copyright is held by the author / owner(s). 
SIGGRAPH 2009, New Orleans, Louisiana, August 3 7, 2009. ISBN 978-1-60558-726-4/09/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1597965</article_id>
		<sort_key>90</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>9</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>9</seq_no>
		<title><![CDATA[Embodied and mediated learning in SMALLab]]></title>
		<subtitle><![CDATA[a student-centered mixed-reality environment]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1597956.1597965</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1597965</url>
		<abstract>
			<par><![CDATA[<p>In recent years, much work in K-12 educational technology has shifted away from addressing the problem of mere accessibility and toward a greater emphasis on the effective design of learning environments that make innovative use of emerging digital technologies. Contemporary research in the Learning Sciences reveals the importance of well-designed, student-centered learning environments that are situated in an appropriate context; engage today's participatory culture; speak to 21<sup>st</sup> century literacies; and enable collaborative, distributed and embodied cognition. Recent research in the Human Computer Interaction (HCI) community has yielded tools and paradigms to enable collaboration, embodied interaction, and multimodal interaction within computational frameworks.</p> <p>Looking across these disciplines we have identified three key areas of convergent research that, when applied to the design of interactive learning environments, can yield truly transformative results: <i>embodiment, computational mediation</i>, and <i>multimodality</i>. By <i>embodiment</i> we mean interactions that engage students both in mind and body, encouraging them to physically explore concepts and systems by moving within and acting upon an environment. By <i>computational mediation</i> we mean that students interact with computational technology that monitors their actions and provides real-time feedback with respect to those actions. By <i>multimodality</i> we mean interactions that encompass students' full sensory and expressive capabilities including visual, sonic, and kinesthetic.</p> <p>Despite this promise, three significant challenges can prevent such work from achieving a broad impact in real world contexts. First, due to their cost and complexity, many emerging interactive environments remain sequestered in specialized facilities, inaccessible to the students and educators who might benefit most. Conversely, many web-based and desktop applications that can readily scale to large user groups, fail to engage the capabilities of learners due to the limiting nature of the mouse/keyboard interface. Finally, all too often, educators and media designers fail to deeply collaborate. As a consequence, researchers are left to make design decisions that are not rooted in an appropriate theoretical base and educators are offered unwieldy tools that do not address the needs of their students.</p> <p>Our work (http://ame2.asu.edu/projects/emlearning) represents a significant advance in the domain of technology-enabled K-12 learning through the purposeful integration of the trans-disciplinary research described above. Specifically:</p> <p>&#8226; We have developed the Situated Multimedia Arts Learning Lab [<i>SMALLab</i>], a mixed-reality learning environment that allows learners to engage through full body 3D movements and gestures within a collaborative, computationally mediated space. <i>SMALLab</i> enables emerging interactive learning approaches to reach a broad population of students and educators in a low cost package.</p> <p>&#8226; We are guided by a <i>grounded design</i> approach to the collaborative design of learning scenarios that features ongoing, deep collaboration through a professional learning community comprised of a cohort of K-12 teachers, students, media researchers, and artists.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[K-12 education]]></kw>
			<kw><![CDATA[constructivism]]></kw>
			<kw><![CDATA[experiential media]]></kw>
			<kw><![CDATA[interactivity]]></kw>
			<kw><![CDATA[learning]]></kw>
			<kw><![CDATA[situated multimedia]]></kw>
			<kw><![CDATA[student centered learning environments]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>K.3.1</cat_node>
				<descriptor>Collaborative learning</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>K.3.1</cat_node>
				<descriptor>Computer-assisted instruction (CAI)</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010489.10010490</concept_id>
				<concept_desc>CCS->Applied computing->Education->Computer-assisted instruction</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010489.10010492</concept_id>
				<concept_desc>CCS->Applied computing->Education->Collaborative learning</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Documentation</gt>
			<gt>Experimentation</gt>
			<gt>Performance</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1617485</person_id>
				<author_profile_id><![CDATA[81100071531]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[David]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Birchfield]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Arizona State University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617487</person_id>
				<author_profile_id><![CDATA[81337488063]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Ellen]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Campana]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Arizona State University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617488</person_id>
				<author_profile_id><![CDATA[81338489053]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Sarah]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hatton]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Arizona State University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617489</person_id>
				<author_profile_id><![CDATA[81442614978]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Mina]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Johnson-Glenberg]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Arizona State University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617490</person_id>
				<author_profile_id><![CDATA[81319494744]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Aisling]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kelliher]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Arizona State University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617491</person_id>
				<author_profile_id><![CDATA[81320493440]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Loren]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Olson]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Arizona State University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617492</person_id>
				<author_profile_id><![CDATA[81442605096]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>7</seq_no>
				<first_name><![CDATA[Christopher]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Martinez]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Arizona State University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617493</person_id>
				<author_profile_id><![CDATA[81442616532]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>8</seq_no>
				<first_name><![CDATA[Philippos]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Savvides]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Arizona State University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617494</person_id>
				<author_profile_id><![CDATA[81350586523]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>9</seq_no>
				<first_name><![CDATA[Lisa]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tolentino]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Arizona State University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617486</person_id>
				<author_profile_id><![CDATA[81442611678]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>10</seq_no>
				<first_name><![CDATA[Sibel]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Uysal]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Arizona State University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Embodied and Mediated Learning in SMALLab: a student­centered mixed-reality environment David Birchfield, 
Ellen Campana, Sarah Hatton, Mina Johnson-Glenberg, Aisling Kelliher, Loren Olson, Christopher Martinez, 
Philippos Savvides, Lisa Tolentino, Sibel Uysal Arts, Media and Engineering Arizona State University 
+1.480.965.9438 dbirchfield@asu.edu Categories and Subject Descriptors K.3.1 [Computers and Education]: 
Computer Uses in Education collaborative learning, computer-assisted instruction  General Terms Algorithms, 
Documentation, Performance, Design, Experimentation. Keywords K-12 Education, Situated Multimedia, Learning, 
Experiential Media, Student Centered Learning Environments, Interactivity, Constructivism.  1. ABSTRACT 
In recent years, much work in K-12 educational technology has shifted away from addressing the problem 
of mere accessibility and toward a greater emphasis on the effective design of learning environments 
that make innovative use of emerging digital technologies. Contemporary research in the Learning Sciences 
reveals the importance of well-designed, student-centered learning environments that are situated in 
an appropriate context; engage today s participatory culture; speak to 21st century literacies; and enable 
collaborative, distributed and embodied cognition. Recent research in the Human Computer Interaction 
(HCI) community has yielded tools and paradigms to enable collaboration, embodied interaction, and multimodal 
interaction within computational frameworks. Looking across these disciplines we have identified three 
key areas of convergent research that, when applied to the design of interactive learning environments, 
can yield truly transformative results: embodiment, computational mediation, and multimodality. By embodiment 
we mean interactions that engage students both in mind and body, encouraging them to physically explore 
concepts and systems by moving within and acting upon an environment. By computational mediation we mean 
that students interact with computational technology that monitors their actions and provides real-time 
feedback with respect to those actions. By multimodality we mean interactions that encompass students 
full sensory and expressive capabilities including visual, sonic, and kinesthetic. Despite this promise, 
three significant challenges can prevent such work from achieving a broad impact in real world contexts. 
First, due to their cost and complexity, many emerging interactive environments remain sequestered in 
specialized facilities, inaccessible to the students and educators who might benefit most. Conversely, 
many web-based and desktop applications that can readily scale to large user groups, fail to engage the 
capabilities of learners due to the limiting nature of the mouse/keyboard interface. Finally, all too 
often, educators and media designers fail to deeply collaborate. As a consequence, researchers are left 
to make design decisions that are not rooted in an appropriate theoretical base and educators are offered 
unwieldy tools that do not address the needs of their students. Our work (http://ame2.asu.edu/projects/emlearning) 
represents a significant advance in the domain of technology-enabled K-12 learning through the purposeful 
integration of the trans­disciplinary research described above. Specifically: We have developed the 
Situated Multimedia Arts Learning Lab [SMALLab], a mixed-reality learning environment that allows learners 
to engage through full body 3D movements and gestures within a collaborative, computationally mediated 
space. SMALLab enables emerging interactive learning approaches to reach a broad population of students 
and educators in a low cost package.  We are guided by a grounded design approach to the collaborative 
design of learning scenarios that features ongoing, deep collaboration through a professional learning 
community comprised of a cohort of K-12 teachers, students, media researchers, and artists.  Evaluation 
and Results: As evidence of the scalability of our efforts, over the past two years we have reached over 
25,000 students, community members and teachers. A SMALLab system is currently installed on a permanent 
basis -in a large urban public high school. In collaboration with a cohort of high school teachers we 
are designing, implementing, and assessing innovative programs in earth science, chemistry, physics, 
and language arts that reach across the school community. We apply a mixed method approach to evaluation 
to assess the efficacy of SMALLab for learning. Through empirical research in the schools, we have documented 
statistically significant learning gains by students by using invariant pre-and post-tests of standards-based 
content knowledge. We have observed improved performance by teachers in SMALLab through the use of an 
observational rubric that assesses the nature of student-centered learning in the classroom. In addition, 
our research team has begun to formalize of a comprehensive design rubric with metrics that address the 
technological, pedagogical, and scalability needs of mixed-reality learning scenarios. Copyright is 
held by the author / owner(s). SIGGRAPH 2009, New Orleans, Louisiana, August 3 7, 2009. ISBN 978-1-60558-726-4/09/0008 
 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1597966</article_id>
		<sort_key>100</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>10</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>10</seq_no>
		<title><![CDATA[Funbrella]]></title>
		<subtitle><![CDATA[making rain fun]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1597956.1597966</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1597966</url>
		<abstract>
			<par><![CDATA[<p>Rain changes its complexion based on the time and the place. Each aspect of rain provides a unique impression, so experiencing it in distant places and different times would be a new way of sharing information. To represent rain, we focused on an umbrella as a user interface to connect humans and rain. Generally, people experience rain with sounds, sights, or sometimes smells; however, in our proposed system, we exploit the vibration perceived through an umbrella's handle to let people feel the rain. We implemented a vibration-giving mechanism based on a speaker and microphone not only to give vibrations but also to record them so that they reconstruct rain. Using this mechanism, we propose a novel umbrella-like user interface named "Funbrella" that can record and replay the rain with FUN (Figure 1). We developed two interesting applications. One is to feel various kinds of rain, and the other transmits rain to different places in real time. With such applications, people can enjoy rain that is usually annoying.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>H.5.1</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>A.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002951.10003227.10003251</concept_id>
				<concept_desc>CCS->Information systems->Information systems applications->Multimedia information systems</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P1617495</person_id>
				<author_profile_id><![CDATA[81442604676]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Ai]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yoshida]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Osaka University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617496</person_id>
				<author_profile_id><![CDATA[81100329599]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Yuichi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Itoh]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Osaka University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617497</person_id>
				<author_profile_id><![CDATA[81435593830]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Ryo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Fukazawa]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Osaka University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617498</person_id>
				<author_profile_id><![CDATA[81442608795]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Kazuyuki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Fujita]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Osaka University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617499</person_id>
				<author_profile_id><![CDATA[81100273942]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Yoshifumi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kitamura]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Osaka University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617500</person_id>
				<author_profile_id><![CDATA[81413592680]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Maya]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ozaki]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Osaka University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617501</person_id>
				<author_profile_id><![CDATA[81442606291]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>7</seq_no>
				<first_name><![CDATA[Tetsuya]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kikukawa]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Osaka University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617502</person_id>
				<author_profile_id><![CDATA[81100339162]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>8</seq_no>
				<first_name><![CDATA[Fumio]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kishino]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Osaka University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Funbrella: Making Rain Fun AiYoshida YuichiItoh Kazuyuki Fujita Maya Ozaki Tetsuya Kikukawa RyoFukazawa 
YoshifumiKitamura FumioKishino GraduateSchoolof InformationScienceandTechnology,OsakaUniversity*  Figure 
1: Funbrella Figure 2: Vibration-giving mechanism Figure 3: Tele-rain 1 Introduction Rain changesits 
complexion basedonthetimeandtheplace.Each aspectofrainprovidesa uniqueimpression, soexperiencingitin 
distant places and different times would be a new way of shar­ing information. To represent rain, we 
focused on an umbrella as a user interface to connect humans and rain. Generally, peo­pleexperiencerainwithsounds,sights, 
or sometimes smells;how­ever, in our proposed system, we exploit the vibration perceived through an umbrella 
s handletolet peoplefeeltherain.Weimple­mentedavibration-giving mechanismbased onaspeaker andmi­crophone 
not onlytogivevibrationsbut alsotorecordthem sothat they reconstruct rain. Using this mechanism, we propose 
a novel umbrella-likeuserinterface named Funbrella that canrecordand replaytherainwithFUN(Figure1). We 
developedtwointerest­ingapplications. Oneistofeelvarious kinds ofrain,andtheother transmits raintodifferentplacesin 
realtime. Withsuch applica­tions,people canenjoyrainthatisusually annoying. 2 Funbrella Inthis section,we 
describethedetailsofFunbrella sstructure. Peopleperceive thevibrationcausedby raindropshittingthecloth 
of the umbrella through its handle. Since the vibration s strength andtimingareirregularandcomplex,anewmethodis 
necessaryto recordandreplaythe actualvibrationoftheraindrops.Wefocused onaspeaker andamicrophonetorecordandgeneratevibrations. 
Figure2showsthe mechanismthatrecordsand generatesthevibra­tion.Two boardshavethree springs betweenthem;the 
upperhasa coil, andthe bottomhasaneodymiummagnet.AsshowninFigure 1,this mechanismisplaced betweenthecloth 
andthe umbrella s handle.Eventually,thesprings sustainthe umbrellainsteadofthe handle. WhenraindropshittheFunbrella,itssprings 
areshaken. Atthis moment,the coilandthe magnetgenerateelectromagneticinduc­tion, transferring the up-and-down 
motions to electric signals that resemble a microphone. The system can store vibration data as well.Funbrellacanalso 
generatemotionwithampli.edsignalsthat theFunbrella records. As a result,Funbrella workslike a speaker 
andmicrophone.Inthisway,Funbrella canrecordandgeneratethe actualshaking(vibration) data madebyraindrops. 
*e-mail:hi-funbrella@hi-mail.ise.eng.osaka-u.ac.jp  3 Application WithFunbrella sfeatures,weimplementedtwoapplicationsto 
en­tertainwith rain.Inthissection,we describetheseapplicationsin detail. Crazy Rain: Participants hold 
Funbrella to experience various kindsof rain thatwerestoredinadvance. In addition,wepro­vide the recorded 
sound of the rain with a speaker because the simultaneous usage of vibrations and sounds creates a feeling 
of being immersed in rain. We recorded not only normal rain with threelevelsofstrengthbutalso such impossible/unlikely 
rainsas waterfromabucketaswellasrainthatis recordedwithsnaketoys, marbles,BBbullets,and spaghetti. Throughthis 
application, par­ticipants canfeel andexperience many unusualtypesof rainwith actual vibrations. Tele-rain: 
Twoparticipants holdFunbrellasapartfrom eachother and one s rain is transmitted to the other, as a telephone. 
Figure 3shows the entire system. The electric signal recorded by thein­put side of the Funbrella is sent 
to its output side and generates the vibration. The Funbrella input can be output and vice versa, so 
Funbrella is bi-directional. This means that participants can changetheirpositionfromeither sendingto 
receivingtheinforma­tionof raindrops.Thisenables participantswhoarein Japanand theUnitedStatestosharetheirrainwitheachotherinrealtime. 
 4 Conclusion Weproposedanew umbrella-likeuserinterface, Funbrella, that recordsandreplaysdifferentkindsofrain 
using vibrations gener­atedbytheprincipleof dynamicspeakersandmicrophones. This userinterface canstoreavarietyof 
rain, andparticipantsexpe­rience not only normalbutalso crazy rainsthattheyhave never experienced before.Inaddition,twoparticipantsindifferentplaces 
can feel each other sraininrealtime. Inthefuture, we areplan­ning to improve the vibration-generating 
mechanism to store and produce many kinds of vibrations. This system might become a brandnewway of sharinginformationbecauseit 
can representthe faintpresenceofhumansor physicalobjectsormovementsassub­tle vibrations. Copyright is 
held by the author / owner(s). SIGGRAPH 2009, New Orleans, Louisiana, August 3 7, 2009. ISBN 978-1-60558-726-4/09/0008 
 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1597967</article_id>
		<sort_key>110</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>11</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>11</seq_no>
		<title><![CDATA[gCubik]]></title>
		<subtitle><![CDATA[real-time integral image rendering for a cubic 3D display]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1597956.1597967</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1597967</url>
		<abstract>
			<par><![CDATA[<p>gCubik provides a 3D visual experience, which is interactive and group-shared, by using a graspable cubic display. Users are presented with color, stereo and full motion parallax 3D scenes, viewable from any direction, without the need of special glasses. Newly designed wide field of view integral photography(IP) lens array are employed on each face of the display for its autostereoscopic effect. In our demonstration, real-time rendering of the IP images allows users to interactively manipulate the 3D objects in the scene via simple finger gestures.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.4.0</cat_node>
				<descriptor>Image displays</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>A.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P1617503</person_id>
				<author_profile_id><![CDATA[81421599381]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Roberto]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lopez-Gulliver]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[NICT Universal Media Research Center, Kyoto, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617504</person_id>
				<author_profile_id><![CDATA[81100632505]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Shunsuke]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yoshida]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[NICT Universal Media Research Center, Kyoto, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617505</person_id>
				<author_profile_id><![CDATA[81421599744]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Sumio]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yano]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[NICT Universal Media Research Center, Kyoto, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617506</person_id>
				<author_profile_id><![CDATA[81100619503]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Naomi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Inoue]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[NICT Universal Media Research Center, Kyoto, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[de Sorbier, F., Nozick, V., and Birl, V. 2008. Gpu rendering for autostereoscopic displays. In <i>3D Data Processing, Visualization and Transmission</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1278294</ref_obj_id>
				<ref_obj_pid>1278280</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Jones, A., McDowall, I., Yamada, H., Bolas, M., and Debevec, P. 2007. An interactive 360&#176; light field display. In <i>SIGGRAPH '07, ACM</i>, 13.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1544244</ref_obj_id>
				<ref_obj_pid>1544196</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Lopez-Gulliver, R., Yoshida, S., Yano, S., and Inoue, N. 2008. Toward an interactive box-shaped 3d display: Study of the requirements for wide field of view. In <i>IEEE Symposium on 3D User Interfaces</i>, 157--158.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 gCubik: Real-time Integral Image Rendering for a Cubic 3D Display Roberto Lopez-Gulliver* Shunsuke Yoshida 
Sumio Yano Naomi Inoue NICT Universal Media Research Center, Kyoto, Japan Figure 1: gCubik is an autostereoscopic 
display, graspable, interactive and suitable for multiple users 1 Abstract gCubik provides a 3D visual 
experience, which is interactive and group-shared, by using a graspable cubic display. Users are pre­sented 
with color, stereo and full motion parallax 3D scenes, view­able from any direction, without the need 
of special glasses. Newly designed wide .eld of view integral photography(IP) lens array are employed 
on each face of the display for its autostereoscopic effect. In our demonstration, real-time rendering 
of the IP images allows users to interactively manipulate the 3D objects in the scene via simple .nger 
gestures. 2 Vision Three-dimensional displays are a promising technology for natu­rally sharing 3D information 
among users. Besides visual feed­back, direct and intuitive manipulation of the 3D scenes could have 
a signi.cant impact on face-to-face collaborative tasks, with appli­cations in education and entertainment[Jones 
et al. 2007]. Our goal is to provide a graspable, group-shared, glasses-free and interactive 3D display 
which allows multiple users to naturally share 3D images as they would do with real objects. Our proto­typed 
display gives users the impression of holding, pointing and manipulating, in their hands, a 3D object 
inside a cube. We plan to propose a novel interaction paradigm that is unique and suitable for our 6-faced 
display. 3 Technical Innovations Wide .eld of view integral photography. Our display utilizes a newly 
designed IP lens array with wide .eld of view, on each of the 6 faces of a cube, allowing users for simultaneous 
multi-face view­ing from any direction. Each IP lens have a 120-degree .eld of view and provide 18×18 
directional views with smooth image transition at a holding distance of 400mm[Lopez-Gulliver et al. 2008]. 
Real-time integral photography image rendering. The rendering algorithm utilizes OpenGL s off-screen 
rendering to a texture us­ing framebuffer objects (FBO). Similar algorithms have been pro­posed[de Sorbier 
et al. 2008]. FBOs avoid context switching and *e-mail:gulliver@nict.go.jp copying from the framebuffer 
to the texture buffer thus improv­ing performance. Also, the framebuffer object s dimensions are not 
limited by the screen s dimensions. In our implementation, for each of the 6 cube s faces, we allocate 
an FBO with texture attachments with (640x2) x (480x2) pixels dimensions, up-sampling to avoid aliasing 
artifacts. And then render the 3D scene, into this texture, from each of the 1065 IP lens positions using 
a 120-degree .eld of view virtual camera onto a (18x2) x (18x2) pixel viewport corre­sponding to each 
elemental image. We use a stencil mask to avoid viewport overlapping with adjacent lens s position. We 
then simply draw a down-sampled 640x480 pixel textured quad on the window system provided framebuffer. 
Our display s 3D resolution is limited by the number of IP lenses in the array, 1065 per face. In our 
tests, we achieved up to 12 fps for a single-material, non­textured, 3500-vertex, 6500-triangle, lit 
3D model(teapot), using a 2.66Gz Quad Core2 Intel CPU with one NVidia GeForce 8800 GT graphics card. 
Interactive manipulation of IP images. Thanks to the real-time IP image rendering described above, the 
display provides interactivity via touch-panels on each of the cube s faces. With this, users can manipulate 
the 3D objects inside the cube using a virtual trackball by simply dragging they .ngers on the panels. 
By assigning speci.c actions to the corners of the panels, users can change the 3D model to be displayed, 
change its material, and more. 4 User Experience In our demonstration, users can naturally share 3D 
images among a group of friends. Moreover, users can interactively manipulate and change the visual properties 
of the 3D objects in the scene us­ing simple .nger gestures(please refer to accompanied video). We believe 
interaction with our gCubik is a fun 3D visual experience. References DE SORBIER, F., NOZICK, V., AND 
BIRI, V. 2008. Gpu rendering for autostereo­scopic displays. In 3D Data Processing, Visualization and 
Transmission. JONES, A., MCDOWALL, I., YAMADA, H., BOLAS, M., AND DEBEVEC, P. 2007. An interactive 360. 
light .eld display. In SIGGRAPH 07, ACM, 13. LOPEZ-GULLIVER, R., YOSHIDA, S., YANO, S., AND INOUE, N. 
2008. Toward an interactive box-shaped 3d display: Study of the requirements for wide .eld of view. In 
IEEE Symposium on 3D User Interfaces, 157 158. Copyright is held by the author / owner(s). SIGGRAPH 2009, 
New Orleans, Louisiana, August 3 7, 2009. ISBN 978-1-60558-726-4/09/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1597968</article_id>
		<sort_key>120</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>12</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>12</seq_no>
		<title><![CDATA[Graphical instruction for a garment folding robot]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1597956.1597968</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1597968</url>
		<abstract>
			<par><![CDATA[<p>This project proposes to use interactive graphical editing interface for an end user to give instructions to intelligent robots to complete a real world object manipulation task. Natural language is often considered as an ideal communication method for robots, but it not intuitive at specifying tasks that require visual (geometry) information. Learning from demonstration can be useful, but it is not easy to generalize a provided example into a working program. Our approach is to provide a specialized graphical editor that abstracts the target task and to have the user specify how to complete the task by performing simple editing operations (clicking and dragging). We show the effectiveness of our approach by building and testing an example application based on this concept, which is a graphical editor for teaching garment folding to a robot. This example shows that our approach is particularly effective for an end user to configure the robot behavior to satisfy their own needs, which cannot be covered by a single, pre-programmed solution for general audience.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.2.9</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>A.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>H.5.2</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003120.10003121</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010199.10010204</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Planning and scheduling->Robotic planning</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010520.10010553.10010554</concept_id>
				<concept_desc>CCS->Computer systems organization->Embedded and cyber-physical systems->Robotics</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010213.10010204</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Control methods->Robotic planning</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P1617507</person_id>
				<author_profile_id><![CDATA[81442608773]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Yuta]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sugiura]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University, Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617508</person_id>
				<author_profile_id><![CDATA[81100444444]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Takeo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Igarashi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo, Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617509</person_id>
				<author_profile_id><![CDATA[81542108956]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Hiroki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Takahashi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Waseda University, Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617510</person_id>
				<author_profile_id><![CDATA[81442614860]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Tabare]]></first_name>
				<middle_name><![CDATA[Akim]]></middle_name>
				<last_name><![CDATA[Gowon]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Harvard University, Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617511</person_id>
				<author_profile_id><![CDATA[81435595273]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Charith]]></first_name>
				<middle_name><![CDATA[Lasantha]]></middle_name>
				<last_name><![CDATA[Fernando]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University, Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617512</person_id>
				<author_profile_id><![CDATA[81100344143]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Maki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sugimoto]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University, Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617513</person_id>
				<author_profile_id><![CDATA[81100424140]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>7</seq_no>
				<first_name><![CDATA[Masahiko]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Inami]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University, Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Graphical Instruction for A Garment Folding Robot Yuta Sugiura* Takeo Igarashi Hiroki Takahashi Tabare 
Akim Gowon KMD, Keio University The University of Tokyo Waseda University Harvard University JST, ERATO, 
Tokyo JST, ERATO, Tokyo JST, ERATO, Tokyo JST, ERATO, Tokyo Charith Lasantha Fernando Maki Sugimoto 
Masahiko Inami KMD, Keio University KMD, Keio University KMD, Keio University JST, ERATO, Tokyo JST, 
ERATO, Tokyo JST, ERATO, Tokyo Figure 1: Teaching how to fold a T-shirt by means of graphical editing. 
The user folds the T-shirt on the computer via simple dragging operations (a, b). The robot then performs 
the task in the real world (c, d). 1 Introduction This project proposes to use interactive graphical 
editing interface for an end user to give instructions to intelligent robots to complete a real world 
object manipulation task. Natural language is often considered as an ideal communication method for robots, 
but it not intuitive at specifying tasks that require visual (geometry) informa­tion. Learning from demonstration 
can be useful, but it is not easy to generalize a provided example into a working program. Our ap­proach 
is to provide a specialized graphical editor that abstracts the target task and to have the user specify 
how to complete the task by performing simple editing operations (clicking and dragging). We show the 
effectiveness of our approach by building and testing an example application based on this concept, which 
is a graphical ed­itor for teaching garment folding to a robot. This example shows that our approach 
is particularly effective for an end user to con.g­ure the robot behavior to satisfy their own needs, 
which cannot be covered by a single, pre-programmed solution for general audience. 2 Exposition We propose 
to use custom-made graphical editor to give instruc­tions to the robot. The system provides a graphical 
representa­tion that abstracts the essence of target task and the user edits the graphical representation 
using direct manipulation (click and drag) (Figure1). This approach is much more effective than using 
a natu­ral language for tasks that involves visual information and is much more reliable than having 
the system inferring the procedure from a demonstration. We envision that our approach makes intelligent 
robots much more accessible for end users than they are now. We introduce an example prototype system 
to demonstrate the ef­fectiveness of the proposed approach, which is garment folding. In garment folding, 
the user .rst captures the shape of a given gar­ment into a computer and folds the captured virtual garment 
on the computer by simple click and drag operations. The resulting op­eration sequence is then transferred 
to a real folding robot and the robot performs the task according to the instruction. The system continuously 
monitors the progress of the folding process using a ceiling mounted camera and sends appropriate control 
commands to the robot by comparing the current real world con.guration and the con.gurations in the virtual 
folding procedure. * e-mail: y-sugiura@kmd.keio.ac.jp Figure 2: The result of giving folding instruction 
for handkerchief and T-shirt. 3 Conclusion Our contribution is summarized as follows. 1) We propose 
to use interactive graphical editing as a communication method for the user to teach how to perform a 
task. This method has various ad­vantages over typical methods such as conventional programming, natural 
languages, and programming by demonstration. 2) We de­scribe a working prototype system that shows the 
feasibility of the concept. Although it is crude prototype, implementation detail will be useful for 
implementing more robust practical system in the fu­ture. 3) The main technical contribution is in the 
way we design the graphical interaction. It is not literal simulation of the target task or completely 
abstract geometric editing. It appropriately abstracts the essence of the target task providing easy 
control while hiding the low-level control system details. Teaching is fun and watching the result of 
successful teaching is a ful.lling experience. The success of our experiment shows that this applies 
not only to people and pets, but also to robots. The user feels more intimacy and con.dence with robots 
that follow his or her instruction than those works without user intervention at all. Previous as-automatic-as-possible 
approach to robot control lacks this point of view, which we believe that is one of the reasons behind 
limited adoption of robot technology in daily life. We hope that our work inspires more work in this 
direction. Copyright is held by the author / owner(s). SIGGRAPH 2009, New Orleans, Louisiana, August 
3 7, 2009. ISBN 978-1-60558-726-4/09/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1597969</article_id>
		<sort_key>130</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>13</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>13</seq_no>
		<title><![CDATA[HeadSPIN]]></title>
		<subtitle><![CDATA[a one-to-many 3D video teleconferencing system]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1597956.1597969</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1597969</url>
		<abstract>
			<par><![CDATA[<p>When people communicate in person, numerous cues of attention, eye contact, and gaze direction provide important additional channels of information, making in-person meetings more efficient and effective than telephone conversations and 2D teleconferences. Two-dimensional video teleconferencing precludes the impression of accurate eye contact: when a participant looks into the camera, everyone seeing their video stream sees the participant looking toward them; when the participant looks away from the camera (for example, toward other participants in the meeting), no one sees the participant looking at them. In this work, we develop a one-to-many teleconferencing system which uses 3D acquisition, transmission, and display technologies to achieve accurate reproduction of gaze and eye contact. In this system, the face of a single remote participant is scanned at interactive rates using structured light while the participant watches a large 2D screen showing an angularly correct view of the audience. The scanned participant's geometry is then shown on the 3D display to the audience.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>H.4.3</cat_node>
				<descriptor>Computer conferencing, teleconferencing, and videoconferencing</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>A.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002951.10003260.10003282.10003286.10003291</concept_id>
				<concept_desc>CCS->Information systems->World Wide Web->Web applications->Internet communications tools->Web conferencing</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P1617514</person_id>
				<author_profile_id><![CDATA[81100556708]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Andrew]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Jones]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Southern California]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617515</person_id>
				<author_profile_id><![CDATA[81440610181]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Magnus]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Southern California]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617516</person_id>
				<author_profile_id><![CDATA[81421597317]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Graham]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Fyffe]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Southern California]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617517</person_id>
				<author_profile_id><![CDATA[81440613130]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Xueming]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Southern California]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617518</person_id>
				<author_profile_id><![CDATA[81440600975]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Jay]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Busch]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Southern California]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617519</person_id>
				<author_profile_id><![CDATA[81100542223]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Ian]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[McDowall]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Fakespace Labs]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617520</person_id>
				<author_profile_id><![CDATA[81100467115]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>7</seq_no>
				<first_name><![CDATA[Mark]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bolas]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Southern California]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617521</person_id>
				<author_profile_id><![CDATA[81100086933]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>8</seq_no>
				<first_name><![CDATA[Paul]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Debevec]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Southern California]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1276427</ref_obj_id>
				<ref_obj_pid>1276377</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Jones, A., McDowall, I., Yamada, H., Bolas, M. and Debevec, P. 2007. Rendering for an interactive 360 light field display. <i>ACM Transactions on Graphics 26</i>, 3 (July), 40: 1--40: 10.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>966458</ref_obj_id>
				<ref_obj_pid>966432</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Viola, P., and Jones, M. J. 2004. Robust real-time face detection. <i>International Journal of Computer Vision 57</i>, 2, 137--154.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Zhang, S., and Huang, P. 2006. High-resolution, real-time three-dimensional shape measuremnt. <i>Optical Engineering 45</i>, 12.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 HeadSPIN: A One-to-many 3D Video Teleconferencing System Andrew Jones* Magnus Lang* Graham Fyffe* Xueming 
Yu* Jay Busch* Ian McDowall Mark Bolas* Paul Debevec* * University of Southern California University 
of Southern California Fakespace Labs Institute for Creative Technologies School of Cinematic Arts Figure 
1: (a) An audience interacts with a remote participant (RP) rendered in 3D on an autostereoscopic display. 
(b,c) A cross-fusable stereo pair where the RP appears life-size in correct perspective, able to make 
eye contact with the members of the audience. (d) The RP looks back at the audience via geometrically 
calibrated wide-angle 2D video while being scanned, transmitted, and rendered at 30Hz. When people communicate 
in person, numerous cues of atten­tion, eye contact, and gaze direction provide important additional 
channels of information, making in-person meetings more ef.cient and effective than telephone conversations 
and 2D teleconferences. Two-dimensional video teleconferencing precludes the impression of accurate eye 
contact: when a participant looks into the camera, everyone seeing their video stream sees the participant 
looking to­ward them; when the participant looks away from the camera (for example, toward other participants 
in the meeting), no one sees the participant looking at them. In this work, we develop a one-to-many 
teleconferencing system which uses 3D acquisition, transmission, and display technologies to achieve 
accurate reproduction of gaze and eye contact. In this system, the face of a single remote partic­ipant 
is scanned at interactive rates using structured light while the participant watches a large 2D screen 
showing an angularly correct view of the audience. The scanned participant s geometry is then shown on 
the 3D display to the audience. Real-time 3D Scanning The face of the remote participant is scanned at 
30Hz using a structured light scanning system based on the phase-unwrapping technique of Zhang and Huang 
[2006]. The system uses a monochrome Point Grey Research Grasshop­per camera and greyscale video projector 
running at a frame rate of 120Hz. Generally, we found 120Hz capture to be relatively robust to artifacts 
resulting from temporal misalignment, though fast facial motion can produce waviness in the recovered 
geometry. Figure 2: Layout of the 3D Display apparatus Autostereoscopic 3D Display Our 3D display is 
similar to [Jones et al. 2007] with several key differences. First, the size, geometry, and material 
of the spinning display surface have been optimized for the display of a life-sized human face. The display 
surface has been replaced by a two-sided tent shape with symmet­rical sides made from 20cm × 25cm thin 
brushed aluminum sheet metal. We found that brushed aluminum had high re.ectivity and anisotropy making 
it an inexpensive substitute for the holographic diffuser material used by [Jones et al. 2007]. The two-sided 
shape provides two passes of a display surface to each viewer per full rotation, achieving 30Hz visual 
update for 900 rpm rotation com­pared to the 15Hz update rate achieved by [Jones et al. 2007]. A monochrome 
high speed projector from Polaris Road, Inc. projects frames at 4,320 1-bit (black or white) frames per 
second using a specially coded DVI video signal. Effectively, the display projects seventy-three unique 
views of the scene across the 180-degree .eld of view, for an angular view separation of 2.5 degrees. 
For a typical inter-pupillary distance of 65mm, this provides binocular stereo for viewing positions 
up to 1.5m away. 2D Video Feed A 90. .eld of view 2D video feed allows the re­mote participant to view 
the central audience interacting with their three-dimensional image on the 3D display. A polarized beam­splitter 
as seen in Fig. 2 is used to virtually place the camera close to the position of the eyes of the three-dimensional 
head. Crossed polarizers on the camera and beam-splitter prevent the video feed camera from seeing past 
the beamsplitter while preserving the au­dience s re.ection. The video from the aligned 3D display camera 
is transmitted to the the remote participant where it is shown on a geometrically calibrated projection 
screen. While the remote par­ticipants s view is not autostereoscopic, the screen is approximately at 
the typical distance of the audience members so that the visual disparity is approximately correct. While 
the autostereoscopic horizontal-parallax-only nature of the display will generally produce accurate horizontal 
perspective, high or low vantage points may result in a less accurate vertical gaze di­rection. To correct 
the vertical perspective, we use marker-less face detection [Viola and Jones 2004] to track viewers based 
on the 2D video feed. In this way, the display s horizontal parallax provides binocular stereo with no 
lag as the viewers move their heads hori­zontally, while vertical parallax is achieved through tracking. 
References JONES, A., MCDOWALL, I., YAMADA, H., BOLAS, M., AND DEBEVEC, P. 2007. Rendering for an interactive 
360 light .eld display. ACM Transactions on Graphics 26, 3 (July), 40:1 40:10. VIOLA, P., AND JONES, 
M. J. 2004. Robust real-time face detection. International Journal of Computer Vision 57, 2, 137 154. 
ZHANG, S., AND HUANG, P. 2006. High-resolution, real-time three-dimensional shape measurement. Optical 
Engineering 45, 12. Copyright is held by the author / owner(s). SIGGRAPH 2009, New Orleans, Louisiana, 
August 3 7, 2009. ISBN 978-1-60558-726-4/09/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1597970</article_id>
		<sort_key>140</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>14</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>14</seq_no>
		<title><![CDATA[Interactive cooking simulator]]></title>
		<subtitle><![CDATA[to understand cooking operation deeply]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1597956.1597970</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1597970</url>
		<abstract>
			<par><![CDATA[<p>We propose "Interactive Cooking Simulator" which provides users with information about physical and chemical reaction state during cooking process. This system helps users to understand theoretical operation using visual information of the physical and chemical changes that occurs during cooking process. We hope to change the traditional way of cooking that replies on speculating indirect sense over food more than robust database. We need to experience various cooking operations and the effects of each one to understand them well. However, we can't sense the effects of the cooking operations in realtime. For example, temperature inside the food ingredients during cooking can not be seen even with thermography camera. On the other hand, cooking simulator can simulate state inside the food ingredients and can present it to the users. Thus, with this we believe that the users can experience effects of cooking operations and deeply understand cooking.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[dynamical and thermal simulation]]></kw>
			<kw><![CDATA[edible computing]]></kw>
			<kw><![CDATA[real-time graphics]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>H.5.2</cat_node>
				<descriptor>Interaction styles (e.g., commands, menus, forms, direct manipulation)</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>A.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>H.5.1</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10002951.10003227.10003251</concept_id>
				<concept_desc>CCS->Information systems->Information systems applications->Multimedia information systems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P1617522</person_id>
				<author_profile_id><![CDATA[81313481766]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Fumihiro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kato]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Electro-Communications]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617523</person_id>
				<author_profile_id><![CDATA[81442605095]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Yusuke]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hanaoka]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Electro-Communications]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617524</person_id>
				<author_profile_id><![CDATA[81442601691]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Tu]]></first_name>
				<middle_name><![CDATA[Nguyen]]></middle_name>
				<last_name><![CDATA[Ngoc]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Electro-Communications]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617525</person_id>
				<author_profile_id><![CDATA[81442615754]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Danial]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Keoki]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Electro-Communications]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617526</person_id>
				<author_profile_id><![CDATA[81316489667]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Hironori]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Mitake]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tokyo Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617527</person_id>
				<author_profile_id><![CDATA[81543929256]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Takafumi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Aoki]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tokyo Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617528</person_id>
				<author_profile_id><![CDATA[81100270860]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>7</seq_no>
				<first_name><![CDATA[Shoichi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hasegawa]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Electro-Communications]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Interactive Cooking Simulator -to understand cooking operation deeply- Fumihiro Kato* Yusuke Hanaoka* 
Tu Nguyen Ngoc* Danial Keoki* Hironori Mitake Takafumi Aoki Shoichi Hasegawa* University of Electro-Communications 
* Tokyo Institute of Technology Keywords: Edible computing, Real-Time Graphics, Dynamical and thermal 
simulation 1 Introduction We propose Interactive Cooking Simulator which provides users with information 
about physical and chemical reaction state during cooking process. This system helps users to understand 
theoreti­cal operation using visual information of the physical and chemical changes that occurs during 
cooking process. We hope to change the traditional way of cooking that replies on speculating indirect 
sense over food more than robust database. We need to experience various cooking operations and the effects 
of each one to understand them well. However, we can t sense the effects of the cooking operations in 
realtime. For example, temperature inside the food ingredients during cooking can not be seen even with 
thermography camera. On the other hand, cooking simulator can simulate state inside the food ingredients 
and can present it to the users. Thus, with this we believe that the users can experience effects of 
cooking operations and deeply understand cooking. 2 Innovation What we want to eat is not usually easy 
to make. This is because cooking is a complex intertwinement of physical and chemical re­action that 
can not be seen directly with naked eyes. When ana­lyzing cooking, we use all our .ve senses to determine 
the state or condition inside the ingredients. Then we use this information to decide the next cooking 
step repeatedly. In this way, we gain sensing experience to speculate indirect information. Our system 
s advantage will support users to break through these dif.culties as an interactive simulator. By giving 
users additional information (the inside and outside of an ingredient appearances, heat, moisture con­ditions, 
etc.) to refer to a Recognition Support environment can be provided to support users to make a dish upon 
interactive images. Moreover, the users can easily undo or change the cooking process if the dish does 
not match as their imagination. By undergoing this repetition training, the users can build a strong 
basic understanding about cooking, as well as the cause of unwanted cooking results. 3 System This simulator 
consists of 3 major elements. 3D Shape Model and Dynamics Simulation  Heat, Moisture and Chemical Reaction 
Simulation  Haptic Interaction  Using the 3D Shape Model and Dynamic Simulation , we simu­late the 
movements of the food ingredients on the frying pan when moved causing the food ingredients to turn or 
roll over that liter­ally re.ects the heating process. Heat transmission within the food ingredients 
are expressed using .nite element method. Changes in *e-mail:{ fumihiro.k,hanaoka,nguyen,dan,hase} @hi.mce.uec.ac.jp 
e-mail:{ mitake,aoki} @hi.pi.titech.ac.jp heat transfer, appearance, moisture and chemical reactions 
are all simulated and modeled in Heat, Moisture and Chemical Reaction Simulation . These models are simulated 
in real time and param­eter changes are visualized. Then to enable training and learning of new cooking 
skills in cooking, real interactive interactions is pre­sented with the Haptic Interaction . These Haptic 
Interaction will directly affect the simulation results of the 3D Shape Model and Dynamic Simulation 
. Similar as real principle laws, simu­lation of cooking operation towards the end result of a dish can 
be achieved instantly proves to be a useful tool to build a strong foun­dation and experience towards 
cooking(Figure 1). Figure 1: (a): prototype system of the Interactive Cooking Simu­lator with the haptic 
device. Users can experience stir-fry cooking with additional information (temperature, appearance and 
moisture conditions). (b): Users can refer the inner state of the beef steak (temperature and appearance 
conditions). Bottom pictures show cross sections of the beef steak. 4 Vision With our system, we hope 
that even without and knowledge of cooking, anyone can observe what happens during cooking and use this 
understanding to better improve one s cooking skills, thus threshold of cooking can be overcome and anyone 
can enjoy cook­ing and moreover have a rich and healthy eating diet. If information about food ingredients 
undergoing heating process can be accessed, cooking in the real world, even though the insides or contents 
of a food ingredient cannot be seen directly, measurement of dif.cult data parameters can be simulated 
in real time and be reproduced to the user visually thus being a cooking support tool. In addition, any­one 
with cooking knowledge or skills can dynamically rearrange known recipes to create new dishes. We hope 
to bring innovation in cooking itself with our system. Finally, we believe that cooking originality at 
an individual level will increase signi.cantly with our system and cooking can be a good opportunity 
to open the century of individual creativity. Copyright is held by the author / owner(s). SIGGRAPH 2009, 
New Orleans, Louisiana, August 3 7, 2009. ISBN 978-1-60558-726-4/09/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1597971</article_id>
		<sort_key>150</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>15</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>15</seq_no>
		<title><![CDATA[An interactive "retrographic sensor" for touch, texture, and shape]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1597956.1597971</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1597971</url>
		<abstract>
			<par><![CDATA[<p>Retrographic sensing is a novel method for measuring surface texture and shape. It uses a sensor made of clear elastomer with a painted skin to non-destructively change an object's reflectance characteristics. When an object is pressed into the sensor, the painted skin conforms to the shape of the object. Viewed from behind, the object appears as a shaded surface and the shape of the surface can be estimated using photometric stereo techniques. In previous work [Johnson and Adelson 2009], we describe a method for reconstructing high-resolution 2.5D surface data from a single image of the sensor. In this work, we demonstrate an implementation of the system that runs at interactive rates.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>H.5.2</cat_node>
				<descriptor>Interaction styles (e.g., commands, menus, forms, direct manipulation)</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>H.5.1</cat_node>
				<descriptor>Artificial, augmented, and virtual realities</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>A.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010866</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Virtual reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010392</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Mixed / augmented reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010866</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Virtual reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P1617529</person_id>
				<author_profile_id><![CDATA[81442599480]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Alvin]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Raj]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Massachusetts Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617530</person_id>
				<author_profile_id><![CDATA[81100451483]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Micah]]></first_name>
				<middle_name><![CDATA[K.]]></middle_name>
				<last_name><![CDATA[Johnson]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Massachusetts Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617531</person_id>
				<author_profile_id><![CDATA[81100466478]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Edward]]></first_name>
				<middle_name><![CDATA[H.]]></middle_name>
				<last_name><![CDATA[Adelson]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Massachusetts Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Johnson, M. K., and Adelson, E. H. 2009. Retrographic sensing for the measurement of surface texture and shape. In <i>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 An interactive retrographic sensor for touch, texture, and shape Alvin Raj, Micah K. Johnson, Edward 
H. Adelson* Massachusetts Institute of Technology  (a) (b) Figure 1: (a) A cookie is pressed against 
the skin of the retrographic sensor. It is illuminated from below by three colored lights and is captured 
by a standard USB camera. (b) Using photometric stereo, we estimate the cookie s shape at interactive 
rates. 1 Introduction Retrographic sensing is a novel method for measuring surface tex­ture and shape. 
It uses a sensor made of clear elastomer with a painted skin to non-destructively change an object s 
re.ectance characteristics. When an object is pressed into the sensor, the painted skin conforms to the 
shape of the object. Viewed from be­hind, the object appears as a shaded surface and the shape of the 
surface can be estimated using photometric stereo techniques. In previous work [Johnson and Adelson 2009], 
we describe a method for reconstructing high-resolution 2.5D surface data from a single image of the 
sensor. In this work, we demonstrate an implementa­tion of the system that runs at interactive rates. 
 2 Technical details The sensor is made of a clear thermoplastic elastomer (TPE), which is both strong 
and elastic. The elastomer can be dissolved in com­mon solvents and formed into arbitrary shapes, though 
we typically keep the shape as .at as possible. The re.ective paint is made from a mixture of TPE, dissolved 
in a solvent such as toluene, and re­.ective pigments. This technique creates a re.ective skin with the 
same strength and elasticity as the body of the sensor. The paint is applied to the elastomer using an 
airbrush. The properties of the sensor determine the types of surfaces that can be measured. In particular, 
the thickness and hardness of the elastomer directly in.uence how the re.ective skin deforms as an object 
presses against it: a soft and thick sensor is useful for mea­suring objects with deep concavities, while 
a .rm and thin sensor can measure near-microscopic surface texture. The choice of BRDF of re.ective paint 
also has a large effect on the performance of the system. To measure deep objects, we have found that 
diffuse BRDFs and oblique illumination are necessary because they reduce ambiguities at extreme surface 
orientations. But diffuse paints typically blur the .ne surface detail. To see this detail (i.e., small 
variations in surface normal), we need specular paints. Specular paints, however, cause problems with 
extreme an­gles, thus there is a tradeoff between depth and detail. *email: {alvin,kimo,adelson}@csail.mit.edu 
To capture images from the retrographic sensor, we position the slab of elastomer on a sheet of glass 
and arrange colored lights (red, green, and blue) below the glass facing the sensor. The lights are arranged 
in an equilateral triangle below the sensor. We use an Imaging Source DFK 31BU03 Color CCD USB camera 
positioned below the sensor. The system is calibrated by imaging an array of spheres with known diameter. 
The spheres allow us to learn the relationship between ob­ject geometry (i.e., surface normal) and RGB 
color at every pixel. We build a lookup table to represent this mapping. For the inter­active implementation, 
we use the normals learned during calibra­tion directly and skip the surface normal re.nement step described 
in [Johnson and Adelson 2009]. The system receives real time video input from the camera which is processed 
frame-by-frame. To counter non-uniform lighting from the close proximity of the three light sources, 
we process a con­strained region of 512 × 512 pixels on the sensor. We look up the associated normals 
for each pixel, then solve the resulting Pois­son system of equations, additionally imposing a Dirichlet 
bound­ary condition so that the edges of the 512 × 512 region has zero height. The resulting height map 
is turned into a 3D mesh, which we light and render with DirectX. At present, our system runs in Windows 
XP on a 2.4Ghz Core 2 Intel processor with 2GB RAM and a GeForce 8600 GTS video card with 256 MB RAM, 
at a frame rate of approximately 20 fps. 3 Conclusion The retrographic sensor allows realtime capture 
and display of sur­face texture and shape. It may be useful in studying the interaction between human 
skin and soft materials in the context of food, cos­metics, or clothing. References JOHNSON, M.K., AND 
ADELSON,E.H.2009.Retrographicsens­ing for the measurement of surface texture and shape. In IEEE Conf. 
on Computer Vision and Pattern Recognition (CVPR). Copyright is held by the author / owner(s). SIGGRAPH 
2009, New Orleans, Louisiana, August 3 7, 2009. ISBN 978-1-60558-726-4/09/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1597972</article_id>
		<sort_key>160</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>16</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>16</seq_no>
		<title><![CDATA[Multimodal floor for immersive environments]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1597956.1597972</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1597972</url>
		<abstract>
			<par><![CDATA[<p>We have developed an interactive system that allows untethered users to experience walking on virtual ground surfaces resembling natural materials. The demonstration consists of a multimodal floor interface for providing auditory, tactile and visual feedback to users' steps. It is intended for immersive virtual and augmented reality environments (VE) that provide the impression of walking over natural ground surfaces, such as snow and ice. To date, immersive environments with interactive floor surfaces have been largely focused on visual and auditory feedback linked to a VE simulation (e.g., [Gronbaek 2007]; see also the comparative review in [Miranda and Wanderley 2006]). However, while walking in natural environments, we receive continuous, multisensory information about the nature of the ground we walk on -- the crush of dry leaves, the soft compression of grass. The static nature of floor surfaces in existing VEs typically bears little resemblance to a given natural ground material. This creates a perceptual conflict with the dynamic visual and/or auditory feedback that users are provided in the VE. This project illustrates a novel approach to reconciling such perceptual conflicts, based on multisensory feedback provided through a floor surface in response to users' steps.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>H.5.1</cat_node>
				<descriptor>Artificial, augmented, and virtual realities</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>A.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010866</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Virtual reality</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010392</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Mixed / augmented reality</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010866</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Virtual reality</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P1617532</person_id>
				<author_profile_id><![CDATA[81384604025]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Alvin]]></first_name>
				<middle_name><![CDATA[W.]]></middle_name>
				<last_name><![CDATA[Law]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[McGill University, Montreal, Canada]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617533</person_id>
				<author_profile_id><![CDATA[81442601366]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Jessica]]></first_name>
				<middle_name><![CDATA[W.]]></middle_name>
				<last_name><![CDATA[Ip]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[McGill University, Montreal, Canada]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617534</person_id>
				<author_profile_id><![CDATA[81442611488]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Benjamin]]></first_name>
				<middle_name><![CDATA[V.]]></middle_name>
				<last_name><![CDATA[Peck]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[McGill University, Montreal, Canada]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617535</person_id>
				<author_profile_id><![CDATA[81100340614]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Yon]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Visell]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[McGill University, Montreal, Canada]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617536</person_id>
				<author_profile_id><![CDATA[81100269547]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Paul]]></first_name>
				<middle_name><![CDATA[G.]]></middle_name>
				<last_name><![CDATA[Kry]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[McGill University, Montreal, Canada]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617537</person_id>
				<author_profile_id><![CDATA[81100490116]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Jeremy]]></first_name>
				<middle_name><![CDATA[R.]]></middle_name>
				<last_name><![CDATA[Cooperstock]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[McGill University, Montreal, Canada]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[F. Fontana, R. B. 2003. Physics-based sound synthesis and control. In <i>Proc. of the XIV Colloq. on Mus. Inf.</i>
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1255061</ref_obj_id>
				<ref_obj_pid>1255047</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Gronbaek, K. 2007. Igamefloor: a platform for co-located collaborative games. In <i>ACE '07</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Law, A. W., Peck, B., Visell, Y., Kry, P., and Cooperstock, J. 2008. A multi-modal floor-space for experiencing material deformation underfoot in virtual reality. In <i>Proc. IEEE HAVE'08</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1201683</ref_obj_id>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Miranda, E., and Wanderley, M. 2006. <i>New Digital Musical Instruments: Control And Interaction Beyond the Keyboard</i>. AR Editions.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Multimodal Floor for Immersive Environments AlvinW.Law JessicaW.Ip BenjaminV. Peck YonVisell PaulG.Kry 
JeremyR. Cooperstock McGill University, Montreal, Canada Figure 1: Virtual ground surfaces exhibit their 
interactive, material-dependent characteristics when walked upon, through visual, auditory, and vibrotactile 
feedback. Shown are a frozen pond simulation (left) and a snowy .eld (right). We have developed an interactive 
system that allows untethered users to experience walking on virtual ground surfaces resembling natural 
materials. The demonstration consists of a multimodal .oor interface for providing auditory, tactile 
and visual feedback to users steps. It is intended for immersive virtual and augmented reality environments 
(VE) that provide the impression of walking over natural ground surfaces, such as snowand ice.Todate, 
immer­sive environments with interactive .oor surfaces have been largely focused on visual and auditory 
feedback linked to a VE simula­tion (e.g., [Gronbaek 2007]; see also the comparative review in [Miranda 
andWanderley2006]). However,whilewalkingin nat­ural environments, we receive continuous, multisensory 
informa­tion about the nature of the ground we walk on the crush of dry leaves, the soft compression 
of grass. The static nature of .oor sur­faces in existing VEs typically bears little resemblance to a 
given natural ground material. This creates a perceptual con.ict with the dynamic visual and/or auditory 
feedback that users are provided in the VE. This project illustrates a novel approach to reconciling 
such perceptual con.icts, based on multisensory feedback provided through a .oor surface in response 
to users steps. The demonstration consists of a 6x6 foot, 36-tile .oor surface. An array of force sensors 
within the .oor (Interlink FSRs), is used to acquire the steps of users. Multimodal feedback is rendered, 
via physically-motivated models for the vibrotactile, auditory, and vi­sual responseofthe materialtothestepsofauser[F.Fontana2003; 
Law et al. 2008], and is displayed at the site of a footstep via an array of vibrotactile and audio actuators 
in the .oor, as well as top­down video projection. Figure1and the accompanying video show a smaller4 
tile system alongwitha temporary projection surface as currently set up in our lab, pending completion 
of the larger sys­tem. The parts to construct the 36 .oor tile array to be exhibited at SIGGRAPH have 
all arrived and are being assembled with planned completion in mid March. The .nal .oor consists of a 
set of rigid wooden tiles withadurable, greyre.ectivepaint. The prototype has been tested with hundreds 
of users during a McGill open house. The two demonstration simulations consist of a frozen pond and .eld 
of snow (Figure 1). In the former, users can tip-toe over the virtual pond surface and observe .sh swimming 
below.Ahard step will causetheiceto fracture, accompaniedbythe soundofice frac­turing, the appearance 
of cracks, and synchronous vibrations. The snow setting allows one to leave footsteps onto virtual snow, 
with acoustic and vibrotactile response similar to the feeling of stepping onto real snow. The latter 
simulation requires motion capture (via a Vicon MCam2 system) of the posture of the feet in order to 
render the footsteps.  Figure 2: The .oor, in the con.guration in whichit is installed in the rear-projected 
CAVE-like environment of our lab. This system introduces a novel approach to affording natural walk­ing 
activity in virtual environments, on virtual terrains. It may be used to enhance existing immersive VEs 
(Figure 2) or for aug­mented .oors in other settings. Potential future applications can be envisioned 
in areas such as immersive VE training simulations, responsive .oor-surfaces for entertainment parks, 
and interactive rehabilitation. References F. FONTANA, R. B. 2003. Physics-based sound synthesis and 
control. In Proc. of the XIV Colloq. on Mus. Inf. GRONBAEK,K. 2007. Igame.oor: a platform for co-located 
col­laborativegames. In ACE 07. LAW, A. W., PECK, B., VISELL, Y., KRY, P., AND COOPER-STOCK, J. 2008. 
A multi-modal .oor-space for experiencing material deformation underfoot in virtual reality. In Proc. 
IEEE HAVE 08. MIRANDA,E., AND WANDERLEY,M. 2006. NewDigital Musical Instruments: ControlAnd InteractionBeyondtheKeyboard. 
AR Editions. Copyright is held by the author / owner(s). SIGGRAPH 2009, New Orleans, Louisiana, August 
3 7, 2009. ISBN 978-1-60558-726-4/09/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1597973</article_id>
		<sort_key>170</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>17</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>17</seq_no>
		<title><![CDATA[Pen de touch]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1597956.1597973</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1597973</url>
		<abstract>
			<par><![CDATA[<p>We propose a pen-shaped handheld haptic display that allows haptic interactions with virtual environments by generating kinesthetic sensations on the user's fingers; the user's movements are not restricted since the device does not have mechanical linkages. Unlike conventional haptic displays that provide vibrations, which are not representative of tactile sensation, our proposed device, named "Pen de Touch" (Figure 1), provides kinesthetic sensations to the muscles in the user's fingers.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>H.5.2</cat_node>
				<descriptor>Input devices and strategies (e.g., mouse, touchscreen)</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>A.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003125</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10011666</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Touch screens</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P1617538</person_id>
				<author_profile_id><![CDATA[81421597185]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Sho]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kamuro]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617539</person_id>
				<author_profile_id><![CDATA[81331499667]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Kouta]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Minamizawa]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617540</person_id>
				<author_profile_id><![CDATA[81100173571]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Naoki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kawakami]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617541</person_id>
				<author_profile_id><![CDATA[81365592882]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Susumu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tachi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1401657</ref_obj_id>
				<ref_obj_pid>1401615</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Kyung, K. and Lee, J. 2008. wUbi-Pen: windows graphical user interface interacting with haptic feedback stylus, <i>ACMSIGGRAPH 2008 New Tech Demos</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Fiorentino, M., Uva, A. and Monno, G. 2005. The Senstylus: a novel rumble-feedback pen device for CAD application in Virtual Reality. In <i>Proceedings of WSCG 2005</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Pen de Touch Sho Kamuro*1 Kouta Minamizawa*2 Naoki Kawakami*3 Susumu Tachi*4 The University of Tokyo 
The University of Tokyo The University of Tokyo Keio University       Figure 1: Proposed device 
 Pen de Touch Figure 2: Internal structure Figure 3: Prototype system Figure 4: Use of Pen de Touch in 
a museum  We propose a pen-shaped handheld haptic display that allows haptic interactions with virtual 
environments by generating kinesthetic sensations on the user s fingers; the user s movements are not 
restricted since the device does not have mechanical linkages. Unlike conventional haptic displays that 
provide vibrations, which are not representative of tactile sensation, our proposed device, named Pen 
de Touch (Figure 1), provides kinesthetic sensations to the muscles in the user s fingers. 1. Introduction 
Various handheld haptic displays have been developed, which are more easy to use than wearable haptic 
displays because the user needs to merely grasp the device to enjoy the haptic interactions. Human haptic 
sensations include cutaneous sensations of the skin and kinesthetic sensations produced at the joints 
of fingers and arms. PHANToM [SensAble Technologies Inc.] is an example of a typical handheld haptic 
display, which enables a user to perceive kinesthetic sensations with the help of mechanical linkages, 
which are driven by multiple motors. However, this device requires to be grounded, restricting the user 
s movements within the range of the mechanical linkages. Recently developed portable handheld haptic 
displays such as wUbi-Pen [Kyung and Lee 2008] and Senstylus [Fiorentino et al. 2005] can provide haptic 
sensations without mechanical linkages. Although such ungrounded devices do not impose restrictions on 
the motion performed by users, they can provide only cutaneous sensations or periodic kinesthetic sensation. 
The wUbi-Pen requires the use of physical contacts with the screen surface and does not function if it 
is moved in mid-air; on the other hand, Senstylus can provide only vibrations, which do not satisfactorily 
represent the realistic feeling of touch. The development of an ungrounded haptic display that can provide 
continual kinesthetic sensations has not been reported thus far. 2. Method Our proposed haptic device 
is pen-shaped so that the user can hold the device in the same way as he or she would hold a writing 
pen. In order to downsize the device, we developed our device on the basis of the hypothesis that the 
kinesthetic sensations on fingers alone are sufficient to represent the sensations of touch. An ungrounded 
device can not apply an external force to the user s hand; therefore, the point of support and the point 
of application of force must be located within the hand itself. We fixed the supporting point as a point 
on the base of the index finger and applied forces to the fingertips by changing the length of the pen-shaped 
device. Therefore, we developed a haptic display for haptic augmentation, which the user could use and 
freely move his or her hands in mid-air without any restrictions that could be introduced by the use 
of mechanical linkages. Figure 2 shows the mechanism of working of our proposed device. The device consists 
of a part from where the pen is held (grip part) and a base part. When the device is held in a user s 
hand, the base part is fixed to the base of the user s index finger, which is inserted in a ring attached 
to the base part; the user grasps the grip part by tip of the index finger, the middle finger, and the 
thumb. The motion of the device is measured by using an optical motion capture system. When the tip of 
the device touches a virtual object, the grip part is pulled back toward the base part with the help 
of the motors, thereby generating the kinesthetic sensations on the skin and muscles of the user s fingers. 
Inside of the base part, three motors and strings are fixed, which pull each connecting point in the 
grip part and control the 3-DOF motion of the grip part, as shown in Figure 2. The motion parallel to 
the central axis generates pushing or pecking sensations on the fingers, and the motion perpendicular 
to the central axis generates the sensation of friction or the sensation of touching an object by the 
side of the pen. 3. Application Pen de Touch is a simple device, which can be easily applied to various 
types of conventional virtual reality environments that are not haptically augmented. Figure 3 shows 
a prototype haptic interaction system interacting with a computer graphics character. Our haptic display 
device provides the kinesthetic sensations to the users depending on the contacts between the pen tip 
and the virtual character. The virtual character then moves according to the contacts from the user, 
thereby making the virtual reality system interactive. As a result, the user experiences the feeling 
of communicating with the character, as it were actually existing in the real world. The easy accessibility 
and good representational ability of our proposed haptic device has encouraged us to further develop 
this device for use in practical systems; as the next step in this direction, we plan to construct a 
haptic interaction system for multiple users, which can be used in a public domain such as a museum (Figure 
4). References Kyung, K. and Lee, J. 2008. wUbi-Pen: windows graphical user interface interacting with 
haptic feedback stylus, ACM SIGGRAPH 2008 New Tech Demos. *1 e-mail: sho_kamuro@ipc.i.u-tokyo.ac.jp 
*2 e-mail: kouta_minamizawa@ipc.i.u-tokyo.ac.jp *3 e-mail: naoki_kawakami@ipc.i.u-tokyo.ac.jp *4 e-mail: 
tachi@tachilab.org Fiorentino, M., Uva, A. and Monno, G. 2005. The Senstylus: a novel rumble-feedback 
pen device for CAD application in Virtual Reality. In Proceedings of WSCG 2005. 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1597974</article_id>
		<sort_key>180</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>18</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>18</seq_no>
		<title><![CDATA[PhotoelasticTouch]]></title>
		<subtitle><![CDATA[transparent rubbery interface using a LCD and photoelasticity]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1597956.1597974</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1597974</url>
		<abstract>
			<par><![CDATA[<p>PhotoelasticTouch is a tabletop system designed to intuitively facilitate touch-based interaction via real objects made from transparent elastic material. The transparent elastic material (transparent rubber) provides a realistic haptic interface, which when combined with the visual content displayed on the LCD tabletop, enables a balanced coupling of the physical world and digital content. The system utilizes the photo-elastic properties of the transparent rubber to recognize when a user pushes, pulls or pinches the object, while the LCD provides the appropriate visual feedback in accordance to the stress applied to the elastic material.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>H.5.2</cat_node>
				<descriptor>Interaction styles (e.g., commands, menus, forms, direct manipulation)</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>H.5.1</cat_node>
				<descriptor>Artificial, augmented, and virtual realities</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>A.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010866</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Virtual reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010392</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Mixed / augmented reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010866</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Virtual reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P1617542</person_id>
				<author_profile_id><![CDATA[81421592614]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Toshiki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sato]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Electro-Communications]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617543</person_id>
				<author_profile_id><![CDATA[81435604554]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Haruko]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Mamiya]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Electro-Communications]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617544</person_id>
				<author_profile_id><![CDATA[81442602487]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Taro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tokui]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Electro-Communications]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617545</person_id>
				<author_profile_id><![CDATA[81100297951]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Hideki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Koike]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Electro-Communications]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617546</person_id>
				<author_profile_id><![CDATA[81365592103]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Kentaro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Fukuchi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Japan Science and Technology Agency]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1401632</ref_obj_id>
				<ref_obj_pid>1401615</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Kakehi, Y., Jo, K., Sato, K., Minamizawa, K., Nii, H., Kawakami, N., Naemura, T., and Tachi, S. 2008. Forcetile: tabletop tangible interface with vision-based force distribution sensing. In <i>SIGGRAPH 2008 new tech demos</i>, ACM.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1186161</ref_obj_id>
				<ref_obj_pid>1186155</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Kamiyama, K., Kajimoto, H., Vlack, K., Kawakami, N., Mizota, T., and Tachi, S. 2004. Gelforce. In <i>SIGGRAPH 2004 Emerging technologies</i>, ACM.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 PhotoelasticTouch: Transparent Rubbery Interface using a LCD and Photoelasticity Toshiki Sato Haruko 
Mamiya Taro Tokui Hideki Koike * Kentaro Fukuchi The University of Electro-Communications Japan Science 
and Technology Agency Figure 1: Tangible objects made from Figure 2: Tangible interface using rubbery 
Figure 3: Paint application using squeez­ Figure 4: The .lter on the camera blocks the incoming light 
from the LCD. However, elliptically polarized light induced by deformed regions of the elastic body is 
visible to the camera. PhotoelasticTouch is composed of a horizontal LCD and an over­head camera. A quarter-wavelength 
.lter is attached to entire sur­face of the LCD to convert the LCD s polarized light to circularly *e-mail:koike@acm.org 
e-mail:fukuchi@megaui.net References KAKEHI, Y., JO, K., SATO, K., MINAMIZAWA, K., NII, H., KAWAKAMI, 
N., NAEMURA, T., AND TACHI, S. 2008. Forcetile: tabletop tangible interface with vision-based force dis­tribution 
sensing. In SIGGRAPH 2008 new tech demos, ACM. KAMIYAMA, K., KAJIMOTO, H., VLACK, K., KAWAKAMI, N., MIZOTA, 
T., AND TACHI, S. 2004. Gelforce. In SIGGRAPH 2004 Emerging technologies, ACM. Copyright is held by the 
author / owner(s). SIGGRAPH 2009, New Orleans, Louisiana, August 3 7, 2009. ISBN 978-1-60558-726-4/09/0008 
 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1597975</article_id>
		<sort_key>190</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>19</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>19</seq_no>
		<title><![CDATA[Pull-navi]]></title>
		<subtitle><![CDATA[a novel tactile navigation interface by pulling the ears]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1597956.1597975</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1597975</url>
		<abstract>
			<par><![CDATA[<p>Previous studies on navigation for walking have mainly used visual or acoustic sensations, which are not intuitive and can even be dangerous because they may block visual and auditory information from the surrounding environment. Some other studies have used tactile stimulation, which is more intuitive and less annoying, on the hand or arm to generate a pseudo-pulling force [1][2][3]. However, the devices tended to become large and heavy. Maeda [4] achieved walk navigation by using Galvanic Vestibular Stimulation, but such electrical stimulation to the head has some clinical challenges for practical daily use.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>H.5.2</cat_node>
				<descriptor>Haptic I/O</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>H.5.2</cat_node>
				<descriptor>Interaction styles (e.g., commands, menus, forms, direct manipulation)</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>A.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003120.10003121.10003124</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10011752</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Haptic devices</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P1617547</person_id>
				<author_profile_id><![CDATA[81421594132]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Yuichiro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kojima]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Electro-Communications]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617548</person_id>
				<author_profile_id><![CDATA[81311484645]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Yuki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hashimoto]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Electro-Communications]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617549</person_id>
				<author_profile_id><![CDATA[81350575122]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Shogo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Fukushima]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Electro-Communications]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617550</person_id>
				<author_profile_id><![CDATA[81100131163]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Hiroyuki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kajimoto]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Electro-Communications]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[N. Nakamura, Y. Fukui: Development of Human Navigation System "HapticNavi" using GyroCube, XVth Triennial Congress of the International Ergonomics Association 2003, pp. 352--355, 2003]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1449674</ref_obj_id>
				<ref_obj_pid>1449660</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[T. Amemiya, T. Maeda: Asymmetric Oscillation Distorts the Perceived Heaviness of Handheld Objects, IEEE Transactions on Haptics, Vol. 1, No. 1, pp. 9--18, 2008.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[cabboots: http://www.freymartin.de/en/projects/cabboots]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1187315</ref_obj_id>
				<ref_obj_pid>1187297</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[T. Maeda, H. Ando, T. Amemiya, M. Inami, N. Nagaya, M. Sugimoto: Shaking The World: Galvanic Vestibular Stimulation As A Novel Sensation Interface, ACM SIGGRAPH 2005 Emerging Technologies, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Pull-Navi A novel tactile navigation interface by pulling the ears Yuichiro Kojima Yuki Hashimoto Shogo 
Fukushima Hiroyuki Kajimoto The University of Electro-Communications {y-kojima, hashimoto, shogo, kajimoto}@kaji-lab.jp 
Introduction and Method Previous studies on navigation for walking have mainly used visual or acoustic 
sensations, which are not intuitive and can even be dangerous because they may block visual and auditory 
information from the surrounding environment. Some other studies have used tactile stimulation, which 
is more intuitive and less annoying, on the hand or arm to generate a pseudo-pulling force [1][2][3]. 
However, the devices tended to become large and heavy. Maeda [4] achieved walk navigation by using Galvanic 
Vestibular Stimulation, but such electrical stimulation to the head has some clinical challenges for 
practical daily use. We propose a new method for navigating the user by pulling their ears. There are 
three merits of our method. The first merit is its naturalness. Our method requires quite a small force 
to pull the ears, and the users can be tempted to walk to the guided direction without any pain or enforced 
feelings. This is presumably because we were commonly pulled by our ears at a young age. The second merit 
is its numerous degrees of freedom (DOF). While most previous tactile navigation systems have achieved 
one or at most two DOF navigations, our new method can direct full 3-DOF directions (right, left, front, 
back, up and down). This feature is quite useful for indoor situations, such as navigation in complex 
department stores or transfer at subway stations. The final merit is its compact architecture. Since 
quite small forces are required for pulling the ears, we can use small and lightweight motors. Future 
miniaturization will be easy and we have already developed a glasses-mounted prototype to date. System 
and Experiment Our interface is composed of two clips attached to the earlobes, six DC motors to pull 
each ear in three directions, a microprocessor to control the motors and a helmet to mount the motors. 
The user wears the helmet and attaches it to the earlobes via the clips. We confirmed experimentally 
that the users were inevitably tempted to move right /left when their right/left ears were pulled right/left. 
When both ears were pulled forward/backward, the users were tempted to walk faster/slower. Interestingly, 
when both ears were pulled up/down, the users were tempted to walk up/down stairs if there were stairs 
in front of them. We have already exhibited our first prototype at a workshop and over 100 people experienced 
our interface. As a result, almost all the people moved in the navigated direction. We also confirmed 
that they kept feeling the navigating sensation when the ears were continuously pulled. This stability 
is quite useful for navigation of the user to the destination. Although there have been many studies 
on tactile navigation systems, none of the systems are currently in practical use except for some devices 
for visually handicapped people. With the advantages of the Pull-Navi, we hope that it will become the 
first successful tactile walking guidance system. Figure 1. Images of navigation by pulling the ears 
 Figure 2. Overview of our system Figure 3. Scene of experience References   [1] N. Nakamura, Y. Fukui: 
Development of Human Navigation System "HapticNavi" using GyroCube, XVth Triennial Congress of the International 
Ergonomics Association 2003, pp.352-355, 2003 [2] T. Amemiya, T. Maeda: Asymmetric Oscillation Distorts 
the Perceived Heaviness of Handheld Objects, IEEE Transactions on Haptics, Vol. 1, No. 1, pp. 9-18, 2008. 
 [3] cabboots : http://www.freymartin.de/en/projects/cabboots [4] T. Maeda, H. Ando, T. Amemiya, M. 
Inami, N. Nagaya, M. Sugimoto: Shaking The World: Galvanic Vestibular Stimulation As A Novel Sensation 
Interface, ACM SIGGRAPH 2005 Emerging Technologies, 2005.  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1597976</article_id>
		<sort_key>200</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>20</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>20</seq_no>
		<title><![CDATA[The sleighing simulator 2.0]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1597956.1597976</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1597976</url>
		<abstract>
			<par><![CDATA[<p>The speed sensation is one of the important factors that affect pleasantness for users of entertainment system such as motion ride, etc. Motion ride is a kind of entertainment systems, which is equipped with motion platform. Motion platforms are effective system to improve users' sense of "motion'. If users of the motion platform want more sense of "motion", the movement of the system should become harder. Consequently, the system may become more harmful to users.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>H.5.1</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>A.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.6.3</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010341</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002951.10003227.10003251</concept_id>
				<concept_desc>CCS->Information systems->Information systems applications->Multimedia information systems</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P1617551</person_id>
				<author_profile_id><![CDATA[81343501981]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Takuya]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nojima]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Electro-Communications]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617552</person_id>
				<author_profile_id><![CDATA[81421592605]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Yu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Okano]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Electro-Communications]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617553</person_id>
				<author_profile_id><![CDATA[81361595640]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Yoshihiko]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Saiga]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Electro-Communications]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617554</person_id>
				<author_profile_id><![CDATA[81100131163]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Hiroyuki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kajimoto]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Electro-Communications]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Gerhard A. Brecher et al., 1972. Relation of optical and labyrin-thean orientation, <i>OPTICA ACTA</i>, Vol. 19, No. 5, pp. 467--471.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1335175</ref_obj_id>
				<ref_obj_pid>1335114</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Takuya, N., Yoshihiko, S., Yu, O., Yuki, H., and Hiroyuki, K., 2007, The Peripheral Display for Augmented Reality of Self-Motion, <i>proceedings of 17<sup>th</sup> International Conference on Artificial Reality and Telexistence</i>, 27--30, November.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 The Sleighing Simulator 2.0 Takuya NOJIMA*, Yu Okano , Yoshihiko SAIGA and Hiroyuki KAJIMOTO The University 
of Electro-Communications   Figure 3: The image from user s Figure 2: The physical of prototype system 
point of view 1. Introduction The speed sensation is one of the important factors that affect pleasantness 
for users of entertainment system such as motion ride, etc. Motion ride is a kind of entertainment systems, 
which is equipped with motion platform. Motion platforms are effec­tive system to improve users' sense 
of "motion". If users of the motion platform want more sense of "motion", the movement of the system 
should become harder. Consequently, the system may become more harmful to users. The sense of "motion" 
is mainly consists of the speed sensation and the acceleration sensation. Motion platforms are the display 
 systems for the acceleration sensation. The speed sensation is mainly provided by the visual information 
from conventional visual displays which are often placed in front of users. It is well known that information 
from peripheral vision strongly affects the sense of speed and human sense of self-motion [Gerhard et 
al. 1972]. However, such conventional visual displays are often having insufficient field of view to 
stimulate the user s peri­pheral vision. By using large size display may become an easy way to solve 
that problem. Actually, the size of display has been increasing but the price of such displays has been 
decreasing these days. In spite of this fact, it is still not easy for many pri­vate consumers to equip 
with CAVE like system to increase FOV of display system. Large cost and wide space is required to setup 
such system, which is hard to cover by average private consumers. In our previous research, we have proposed 
a system called the peripheral display . That is the system for augmentation of the sense of speed in 
a cost effective and in a safe way [TAKUYA et al. 2007]. In this paper, we introduce application system 
named The Sleighing Simulator 2.0. * e-mail: tnojima@computer.org e-mail: {saiga,okano,kajimoto}@kaji-lab.jp 
 2. System Development Our peripheral display consists of LED-matrix-arrays [Figure 1] placed around 
the human visual system. This means that peri­pheral area of human vision is covered by the LED-matrix 
ar­rays [Figure 2]. The random dots on those LED-matrix arrays move from far side to near side. Such 
visual stimulus to the pe­ripheral vision can enhance user's sense of speed. As shown in Figure 3, the 
users of this system can watch outside view in­cluding display image in front of them through our system. 
Then users can feel more sense of speed by the effect of the pe­ripheral display system. Our application 
system, the sleighing simulator 2.0, consists of a sleigh like input device, a conventional display in 
front of the user, and the peripheral display. On the conventional display placed in front of the user, 
the forward vision of the sleighing simulator is provided. At the same time, users feel more sense of 
speed by the effect of the visual stimulus from the peripheral display. Then users control their sleigh 
through that sleigh like input device.  3. Acknowledgement This work was supported by Grant-in-Aid for 
Young Scientists (B)( 20700121) References Gerhard A. Brecher et al., 1972. Relation of optical and 
labyrin­thean orientation, OPTICA ACTA, Vol. 19, No.5, pp.467-471. Takuya, N., Yoshihiko, S., Yu, O., 
Yuki, H., and Hiroyuki, K., 2007, The Peripheral Display for Augmented Reality of Self-Motion, proceedings 
of 17th International Conference on Artificial Reality and Telexistence, 27-30,November. Copyright is 
held by the author / owner(s). SIGGRAPH 2009, New Orleans, Louisiana, August 3 7, 2009. ISBN 978-1-60558-726-4/09/0008 
  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1597977</article_id>
		<sort_key>210</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>21</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>21</seq_no>
		<title><![CDATA[Sound Scope headphones]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1597956.1597977</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1597977</url>
		<abstract>
			<par><![CDATA[<p>We designed the Sound Scope Headphones so that they would let users control an audio mixer through natural movements, and thus enable a musical novice to separately listen to each player's performance. The main advantage of the headphones is that they detect natural movement, such as head movement or placing a hand behind an ear, and uses the detected movements to control an audio mixer while the user listens to music. Three sensors are mounted on the headphones: a digital compass, a tilt sensor, and a distance sensor (Figure 1).</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>H.5.5</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>A.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010469.10010475</concept_id>
				<concept_desc>CCS->Applied computing->Arts and humanities->Sound and music computing</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002951.10003317.10003371.10003386.10003390</concept_id>
				<concept_desc>CCS->Information systems->Information retrieval->Specialized information retrieval->Multimedia and multimodal retrieval->Music retrieval</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P1617555</person_id>
				<author_profile_id><![CDATA[81444599496]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Masatoshi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hamanaka]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Univirsity of Tsukuba]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617556</person_id>
				<author_profile_id><![CDATA[81442595282]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[SuengHee]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lee]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Univirsity of Tsukuba]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Goudeseune, C., and Kaczmarski, H. Composing outdoor augmented-reality sound environments. In <i>In Proceedings of the International Computer Music Conference</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Warusfel, O., and Eckel, G. Listen - augmenting eeveryday environments through interactive soundscapes. In <i>In Proceedings of IEEE Workshop on VR for public consumption</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>261140</ref_obj_id>
				<ref_obj_pid>261135</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Wu, J., Duh, C., Ouhyoung, M., and Wu, J. Head motion and latency compensation on localization of 3d sound in virtual reality. In <i>In Proceedings of the ACM symposium on Virtual reality software and technology</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Figure 1: Three sensors mounted to headphones. 2 How parts are scoped The usability of our headphones 
depends on the quality of the links between the mixer manipulations and the natural movement while the 
user listens to music. We use three links. Link from the facing direction. When a user moves his head 
left­wards (rightwards), the part normally heard from the left (right) side can be heard from a frontal 
position as the digital compass de­tects the change in the direction the user faces. This allows a user, 
through natural movement, to scope the part which he wants to hear most clearly and hear it from a frontal 
position. Link from the face s angle of elevation. When there are several parts in the frontal position, 
the user might not be able to hear the desired part clearly after turning his head left or right to hear 
it from a frontal position. In such a case, the user can change the mix *e-mail:hamanaka@iit.tsukuba.ac.jp 
e-mail:lee@kansei.tsukuba.ac.jp by moving his head up or down and the tilt sensor will detect the change 
in the face s angle of elevation. By looking up or down, re­spectively, the user increases the volume 
of each part located farther away or more closely. Link from the distance between hand and ear. The distance 
sen­sor detects the motion of putting a hand behind one s ear while lis­tening to the sound from a frontal 
position. The distance between hand and ear determines the area indicating whether each part is audible. 
For example, when a user places her hand close to her ear, she can hear only the parts from a frontal 
position. When the user removes her hand, she can hear all the parts except those be­hind her. When the 
user puts her hand in a middle position, she can hear the parts located in the front half position. Therefore, 
the user can focus on a part she wants to listen to by adjusting the distance between her hand and ear. 
 3 Conclusion The Sound Scope Headphones enable the wearer to control an au­dio mixer through natural 
movements. We are now developing ap­plications for the headphones. For example, Figure 2 shows real instruments 
where the light brightness is controlled depending on each part s sound-level. This allows the user to 
understand each part s sound-level visually as well as aurally. This should help mu­sical novices who 
do not know the sound of each instrument learn the relationship between instrument and sound. Figure 
2: Lighting depending on each part s sound-level. References GOUDESEUNE, C., AND KACZMARSKI, H. Composing 
outdoor augmented-reality sound environments. In In Proceedings of the International Computer Music Conference. 
WARUSFEL, O., AND ECKEL, G. Listen -augmenting eeveryday environments through interactive soundscapes. 
In In Proceed­ings of IEEE Workshop on VR for public consumption. WU, J., DUH, C., OUHYOUNG, M., AND 
WU, J. Head motion and latency compensation on localization of 3d sound in virtual reality. In In Proceedings 
of the ACM symposium on Virtual reality software and technology. Copyright is held by the author / owner(s). 
SIGGRAPH 2009, New Orleans, Louisiana, August 3 7, 2009. ISBN 978-1-60558-726-4/09/0008 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1597978</article_id>
		<sort_key>220</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>22</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>22</seq_no>
		<title><![CDATA[The UnMousePad]]></title>
		<subtitle><![CDATA[the future of touch sensing]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1597956.1597978</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1597978</url>
		<abstract>
			<par><![CDATA[<p>Multi-touch input has been an active area of research for over two decades but has always suffered from the absence of an easily available high quality touch input device. For this reason, exciting user interfaces developed in the lab have appeared on CNN, but not on everyone's desk, computer screens, table-tops, walls and floors. What has been needed - and lacking - is a better mousetrap; an inexpensive, flexible and sensitive <i>touch imaging technology</i>.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>H.5.2</cat_node>
				<descriptor>Haptic I/O</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>H.5.2</cat_node>
				<descriptor>Input devices and strategies (e.g., mouse, touchscreen)</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>A.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003120.10003121.10003125</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10011666</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Touch screens</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10011752</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Haptic devices</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P1617557</person_id>
				<author_profile_id><![CDATA[81319500075]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Ilya]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Rosenberg]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[New York University Media Research Lab]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617558</person_id>
				<author_profile_id><![CDATA[81100250413]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Ken]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Perlin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[New York University Media Research Lab]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617559</person_id>
				<author_profile_id><![CDATA[81416609292]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Charles]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hendee]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[New York University Media Research Lab]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617560</person_id>
				<author_profile_id><![CDATA[81416604520]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Alex]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Grau]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[New York University Media Research Lab]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617561</person_id>
				<author_profile_id><![CDATA[81416601724]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Nadim]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Awad]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[New York University Media Research Lab]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 The UnMousePad -The Future of Touch Sensing Ilya Rosenberg*, Charles Hendee , Ken Perlin , Alex Grau§, 
Nadim Awad¶ New York University Media Research Lab Figure 1: Left: Writing/leaning on an UnMousePad 
and the captured force image. Right: Personal work space of the future. 1 Introduction Multi-touch input 
has been an active area of research for over two decades but has always suffered from the absence of 
an easily avail­able high quality touch input device. For this reason, exciting user interfaces developed 
in the lab have appeared on CNN, but not on everyone s desk, computer screens, table-tops, walls and 
.oors. What has been needed -and lacking -is a better mousetrap; an inexpensive, .exible and sensitive 
touch imaging technology. The UnMousePad is that better mousetrap -a novel form of input sensor that 
enables inexpensive multi-touch, pressure acquisition at both small and large form-factors. It can accurately 
measure en­tire images of pressure with continuous bilinear interpolation, per­mitting both high-frame-rate 
and high quality imaging of spatially variant pressure upon a surface. Though the use of force variable 
resistors as multiple points of con­tact input devices is not new, previous work in this area has fo­cused 
mainly on arrays of discrete and independent sensors. The key difference between the UnMousePad and previous 
technolo­gies is that it is based on Interpolating Force Sensitive Resistance (IFSR), which closely mimics 
the multi-resolution properties of hu­man skin, in which the position of a touch can be detected at .ner 
scale than the discrimination of multiple touches. The development of the UnMousePad and other IFSR based 
sen­sors and an improved understanding of their electrical properties enhances the type and quality of 
information that may be obtained in situations where entire images of pressure need to be acquired in 
real-time or in situations where multiple points of pressure need to be continuously tracked. 2 Demonstration 
For our demonstration, we plan to show how we envision ordinary people will use IFSR based sensors in 
their daily lives in the not too distant future. We plan to set up several demonstration areas with the 
following sensor form factors: 1. The future of portable electronic devices: A 2.5 x 3.5 sen­sor will 
be placed on the back of a portable electronic device *e-mail:ilya@cs.nyu.edu e-mail:perlin@cs.nyu.edu 
e-mail:chendee@gmail.com §e-mail:alexgrau@gmail.com ¶nadim.awad@gmail.com having a small screen. The 
device will be operated by press­ing on the back, keeping .ngers from obscuring and leaving smudges on 
the screen. 2. The future of electronic paper: An 8.5 x 11 sensor will be integrated below the eInk 
display of a portable ebook reader. Users will be able to intuitively .ip through pages and write on 
the electronic paper. 3. The future personal work space: A transparent 12 x 16 sen­sor will be placed 
over a 1/8 thick LCD display which will lay .at, having the appearance of an ink blotter. There will 
also be a large 30 display in front of the user with a web cam mounted on top and looking down at the 
user s hands in order to track their position. Software applications will demonstrate the power of this 
interface and its advantages over keyboards and mice. 4. The future of play: A circular 14 diameter 
hand-drum sen­sor will be mounted on a drum stage and covered with a soft rubber pad. Like a real drum, 
the drum will produce different timbres of sound depending on how it is hit, and users will be able to 
change the pitch of the drum by pressing down with one hand while hitting with the other.  For our demonstrations 
of some of the ways in which these devices can be used, we will present the following software applications: 
1. Various visualization of pressure measured by the devices. 2. Tracking of .ngers and pens; writing 
through a pad of paper. 3. Use as a traditional keyboard and mouse. 4. Six degree of freedom manipulation 
of virtual objects. 5. Painting with an overhead camera used to track hands and display their absolute 
position on the screen. 6. Sculpting of surfaces, spherical planet terrain, and clay. 7. Instruments 
including a piano, a synthesizer with continuous tones, FOF voice synthesis, a Theremin, and a hand-drum. 
 8. Animation of a water surface, faces, and virtual characters.  3 Conclusion IFSR based sensor technology 
is inherently unobtrusive, inexpen­sive, and very durable. It has a very wide range of potential appli­cations 
in many sectors of society, enabling multi-touch pressure imaging at a low cost in a wide variety of 
form factors. In our talk, demonstration and poster, we plan to show the technical aspects that underly 
the IFSR technology, and to inspire people to think about the myriad of novel form factors and exciting 
applications that are made possible by this revolutionary new technology. Copyright is held by the author 
/ owner(s). SIGGRAPH 2009, New Orleans, Louisiana, August 3 7, 2009. ISBN 978-1-60558-726-4/09/0008 
 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1597979</article_id>
		<sort_key>230</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>23</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>23</seq_no>
		<title><![CDATA[Touchable holography]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1597956.1597979</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1597979</url>
		<abstract>
			<par><![CDATA[<p>Mid-air displays which project floating images in free space have been seen in SF movies for several decades [Rakkolainen 2007]. Recently, they are attracting a lot of attention as promising technologies in the field of digital signage and home TV, and many types of holographic displays are proposed and developed. You can see a virtual object as if it is really hovering in front of you. But that amazing experience is broken down the moment you reach for it, because you feel no sensation on your hand.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>H.5.2</cat_node>
				<descriptor>Haptic I/O</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>H.5.1</cat_node>
				<descriptor>Artificial, augmented, and virtual realities</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>A.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010866</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Virtual reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010392</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Mixed / augmented reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010866</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Virtual reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10011752</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Haptic devices</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P1617562</person_id>
				<author_profile_id><![CDATA[81365594770]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Takayuki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hoshi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617563</person_id>
				<author_profile_id><![CDATA[81442600957]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Masafumi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Takahashi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617564</person_id>
				<author_profile_id><![CDATA[81421601844]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Kei]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nakatsuma]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617565</person_id>
				<author_profile_id><![CDATA[81100555829]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Hiroyuki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Shinoda]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1401616</ref_obj_id>
				<ref_obj_pid>1401615</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Iwamoto, T., Tatezono, M., Hoshi, T., and Shinoda, H. 2008. Airborne ultrasound tactile display. <i>In International Conference on Computer Graphics and Interactive Techniques ACM SIGGRAPH 2008 New Tech Demos</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Provision, 2009. Holo. http://www.provision3dmedia.com/.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1271628</ref_obj_id>
				<ref_obj_pid>1270398</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Rakkolainen, I. 2007. How feasible are star wars mid-air displays? In <i>11th International Conference Information Visualization</i>, 935--942.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Touchable Holography Takayuki Hoshi:, Masafumi Takahashi , Kei Nakatsuma , Hiroyuki Shinoda§ The University 
of Tokyo 1 Introduction Mid-air displays which project .oating images in free space have been seen in 
SF movies for several decades [Rakkolainen 2007]. Recently, they are attracting a lot of attention as 
promising tech­nologies in the .eld of digital signage and home TV, and many typesof holographic displays 
are proposedanddeveloped.You can see a virtual object as if it is really hovering in front of you. But 
that amazing experience is broken down the moment you reach for it, because you feel no sensation on 
your hand. Our objective is adding tactile feedback to the hovering image in 3D free space. One of the 
biggest issues is how to provide tactile sensation.Although tactile sensation needs contact with objectsby 
nature,theexistenceofastimulatorinthework space depressesthe appearance of holographic images. Therefore 
some kind of remote­controllable tactile sensation is needed. That is achieved by our original tactile 
display [Iwamoto et al. 2008]. The following paper explains the technologies employed for a Touchable 
Holography. 2 Principle 2.1 Holographic Display We use Holo [Provision 2009], a holographic display 
which pro­vides .oating images from an LCD by utilizing a concave mirror. The projected images .oat at 
30 cm away from the display surface. Auser can get near to the image and try to touch it. Of course, 
his .ngers pass through it with no tactile sensation. 2.2 Tactile Display Airborne UltrasoundTactile 
Display[Iwamotoetal. 2008] isa tactile display which provides tactile sensation onto the user shand. 
It utilizes the nonlinear phenomenon of ultrasound; acoustic radia­tion pressure. When an object interrupts 
the propagation of ultra­sound, a pressure .eld is exerted on the surface of the object. The acoustic 
radiation pressure P [Pa] is written as P = eE (1) where E [J/m3]is the energy density of ultrasound.e 
[-] is a con­stant ranging from 1 to 2 depending on the re.ection coef.cient at the object surface. The 
acoustic radiation pressure acts in the same direction of the ultrasound propagation. That is, roughly 
say­ing, the ultrasound pushes the object. Eq.(1) suggests that the spatial distribution of the pressure 
can be controlled by using wave .eld synthesis. When the tactile display radiates the ultrasound, the 
users can feel tactile sensation on their bare hands in free space with no direct contact. The current 
version of prototype consists of 324 ultrasound trans­ducers. The resonant frequencyis 40 kHz. The phase 
delays and amplitudes of all the transducers are controlled individually to gen­erate one focal point 
and move it three-dimensionally. The total *e-mail: star@alab.t.u-tokyo.ac.jp e-mail: masafumi@alab.t.u-tokyo.ac.jp 
e-mail: tsuma@alab.t.u-tokyo.ac.jp §e-mail: shino@alab.t.u-tokyo.ac.jp Figure 1: Developed interaction 
system. An aerial imaging system, a non-contact tactile display, andaWiimote-based hand-tracking systemare 
combined.Inthis.gure,theultrasoundisradiatedfrom above and the user feels as if a rain drop hits his 
palm. output force within the focal region is 1.6 gf. The diameter of the focal point is 20 mm. The prototype 
produce suf.cient vibrations up to1kHz. 2.3 Hand Tracking While camera-based and marker-less hand tracking 
systems are demonstrated these days, we use Wiimote (Nintendo) which has an infrared(IR) camerafor simplicity. 
Aretrore.ectivemarkeris attached on the tip of user s middle .nger. IR LEDs illuminate the markerandtwoWiimotes 
sensethe3D positionofthe.nger.Ow­ing to this hand-tracking system, the users can handle the .oating virtual 
image with their hands.  3 Application The developed system can render various virtual objects because 
not only visual but also tactile sensation is refreshable based on digitaldata.Itis usefulforvideogames,3DCADs,andsoon.Here 
we show anexampleof demos. Fig.1showsa demoin which rain dropsfall from above. When the rain drop hits 
the user s palm, he feels tactile sensation created by the ultrasound. In another demo, he sees and feels 
a small virtual creature running on his palm. References IWAMOTO, T., TATEZONO, M., HOSHI, T., AND SHINODA, 
H. 2008. Airborne ultrasound tactile display. In International Con­ference on ComputerGraphics and InteractiveTechniquesACM 
SIGGRAPH 2008NewTechDemos. PROVISION, 2009. Holo. http://www.provision3dmedia.com/. RAKKOLAINEN,I. 2007. 
How feasible are starwars mid-air dis­plays? In 11th International Conference InformationVisualiza­tion, 
935 942. Copyright is held by the author / owner(s). SIGGRAPH 2009, New Orleans, Louisiana, August 3 
7, 2009. ISBN 978-1-60558-726-4/09/0008 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1597980</article_id>
		<sort_key>240</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>24</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>24</seq_no>
		<title><![CDATA[Twinkle]]></title>
		<subtitle><![CDATA[interface for using handheld projectors to interact with physical surfaces]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1597956.1597980</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1597980</url>
		<abstract>
			<par><![CDATA[<p>Recently, many small pocket-size projectors have been developed. It is expected that in the near future, such projectors will be installed in portable devices. Meanwhile, intuitive interfaces that operate according to the user's motion have been popular. Therefore, the interfaces that can be used for accessing information using handheld projectors have been increasingly studied [Forlines et al. 2005][Cao et al. 2007]. However, these interfaces suffer from a number of problems. Some systems need motion-tracking systems in order to measure the position of the projector. Further, the surface where image is projected are limited to plain screen like a white wall.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>H.5.2</cat_node>
				<descriptor>Interaction styles (e.g., commands, menus, forms, direct manipulation)</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>A.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>H.5.1</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10002951.10003227.10003251</concept_id>
				<concept_desc>CCS->Information systems->Information systems applications->Multimedia information systems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P1617566</person_id>
				<author_profile_id><![CDATA[81421594598]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Takumi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yoshida]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617567</person_id>
				<author_profile_id><![CDATA[81100485839]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Hideaki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nii]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617568</person_id>
				<author_profile_id><![CDATA[81100173571]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Naoki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kawakami]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617569</person_id>
				<author_profile_id><![CDATA[81365592882]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Susumu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tachi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1294220</ref_obj_id>
				<ref_obj_pid>1294211</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Cao, X., Forlines, C., and Balakrishnan, R. 2007. Multiuser interaction using handheld projectors. <i>In Proceedings of the 20th Annual ACM Symposium on User interface Software and Technology</i>, 43--52.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1095046</ref_obj_id>
				<ref_obj_pid>1095034</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Forlines, C., Balakrishnan, R., Beardsley, P., v. Baar, J., and Raskar, R. 2005. Zoom-and-pick: facilitating visual zooming and precision pointing with interactive handheld projectors. <i>In Proceedings of the 18th annual ACM symposium on User interface software and technology</i>, 73--82.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Twinkle: Interface for using Handheld Projectors to Intaract with Physical Surfaces Takumi Yoshida* 
Hideaki Nii Naoki Kawakami Susumu Tachi The University of Tokyo Keio University The University of Tokyo 
Keio University Figure 1: (left):Concept of Twinkle, (center):Projected Image, (right):Prototype Device 
Keywords: handheld projector, augmented reality 1 Introduction Recently, many small pocket-size projectors 
have been developed. It is expected that in the near future, such projectors will be installed in portable 
devices. Meanwhile, intuitive interfaces that operate ac­cording to the user s motion have been popular. 
Therefore, the in­terfaces that can be used for accessing information using handheld projectors have 
been increasingly studied [Forlines et al. 2005][Cao et al. 2007]. However, these interfaces suffer from 
a number of problems. Some systems need motion-tracking systems in order to measure the position of the 
projector. Further, the surface where image is projected are limited to plain screen like a white wall. 
The purpose of our study is to propose a novel interface for in­teraction with an arbitrary physical 
plane surface; here, we de.ne interaction as the utilization of a physical surface to perform cer­tain 
tasks. We call the interface Twinkle . We de.ne a physical surface as a surface that exists in a physical 
environment and is not plain. Examples of such surfaces are a poster on a wall, .gures or characters 
on a whiteboard, and a desk on which objects are placed. When a user shines light from a handheld projector 
such as a .ash­light onto a physical surface, pictures are displayed and sounds are emitted according 
to the objects that are present on the surface and the user s motion. Figure 1 (left) shows the concept 
behind Twin­kle. Our method enables various applications. A few examples of the applications of Twinkle 
are mentioned below. First, we propose an interface for music composition and musical performance. The 
pitch of a sound is determined by the size of the object illuminated by the projector. The color of the 
object and the user s motion de­termine the tone and the volume, respectively, of the sound. The user 
can create melody and rhythm by laying out objects on a sur­face. This interface enables users to compose 
and play music on the basis of intuition, i.e., they can compose and play music even if they do not have 
knowledge of musical score. Next, we propose an AR annotation system. The system recognizes .gures or 
characters on a surface, and information is presented near theose objects. Ad­ditionally, the proposed 
interface can be used in shooting games or action games. In such games, real objects on a surface are 
regarded as obstacles. *e-mail:takumi yoshida@ipc.i.u-tokyo.ac.jp 2 System Overview The proposed system 
comprises a handheld projector and a video camera .xed to the projector. The system is quite simple and 
does not need other motion-tracking systems. The camera captures a physical surface illuminated by the 
projector. The camera has the following two roles: to estimate the user s motion and to recognize the 
features of the physical surface. The process of estimationi of the user s motion is described below. 
By recognizing the shape of the area on which the light is projected, we can calculate the pro­jector 
s position relative to the surface and the distance between the projector and the surface. Moreover, 
an optical .ow technique en­ables the estimation of the direction and velocity of the motion of the user 
s hand on a two-dimensional surface. In order to recog­nize the features of the physical surface, various 
existing image­processing methods can be used. For example, by using a labeling process and pattern recognition, 
we can estimate where the object is located on the surface and estimate its shape. These processes are 
executed in parallel and in real time. Then, images are generated according to the user s motion and 
the features of the surface, and these images are projected onto the surface by the projector. We have 
developed a prototype device and implemented several ap­plications. Figure 1 (center) shows a projected 
image when a user plays music on the interface, and (right) shows a prototype device. 3 Conclusion We 
proposed a novel interface for interaction with an arbitrary physical plane surface, where the interaction 
involves the use of handheld projector. Further, we have developed a prototype device and implemented 
several applications. References CAO, X., FORLINES, C., AND BALAKRISHNAN, R. 2007. Multi­user interaction 
using handheld projectors. In Proceedings of the 20th Annual ACM Symposium on User interface Software 
and Technology, 43 52. FORLINES, C., BALAKRISHNAN, R., BEARDSLEY, P., V. BAAR, J., AND RASKAR, R. 2005. 
Zoom-and-pick: facilitating visual zooming and precision pointing with interactive handheld pro­jectors. 
In Proceedings of the 18th annual ACM symposium on User interface software and technology, 73 82. Copyright 
is held by the author / owner(s). SIGGRAPH 2009, New Orleans, Louisiana, August 3 7, 2009. ISBN 978-1-60558-726-4/09/0008 
  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1597981</article_id>
		<sort_key>250</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>25</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>25</seq_no>
		<title><![CDATA[Versatile training field]]></title>
		<subtitle><![CDATA[the wellness entertainment system using trampoline interface]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1597956.1597981</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1597981</url>
		<abstract>
			<par><![CDATA[<p>We propose the wellness entertainment system Versatile Training Field (VTF). In this system, we use a mini trampoline as the input device. The system enables the user to move and jump freely in VR space by exaggerated movement corresponding to walking or jumping on the mini trampoline. Improvements in exercise motivation and support for continuous exercise are achieved in our system, since it is possible to enjoy strolling through a virtual space, which is usually difficult to experience, by exercising on the mini trampoline without injury to the user's joints.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>H.5.1</cat_node>
				<descriptor>Artificial, augmented, and virtual realities</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>H.5.2</cat_node>
				<descriptor>Interaction styles (e.g., commands, menus, forms, direct manipulation)</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>A.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003120.10003121.10003124</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010866</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Virtual reality</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010392</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Mixed / augmented reality</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010866</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Virtual reality</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P1617570</person_id>
				<author_profile_id><![CDATA[81100501102]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Hiroshi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Mori]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Tsukuba]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617571</person_id>
				<author_profile_id><![CDATA[81331504069]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Kazuhito]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Shiratori]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Tsukuba]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617572</person_id>
				<author_profile_id><![CDATA[81413604705]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Tomoyuki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Fujieda]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Tsukuba]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617573</person_id>
				<author_profile_id><![CDATA[81100230368]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Jun'ichi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hoshino]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Tsukuba]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Versatile Training Field: the Wellness Entertainment System using Trampoline Interface Hiroshi Mori 
Kazuhito Shiratori Tomoyuki Fujieda Jun ichi Hoshino University of Tsukuba ABSTRACT We propose the wellness 
entertainment system Versatile Training Field (VTF). In this system, we use a mini trampoline as the 
input device. The system enables the user to move and jump freely in VR space by exaggerated movement 
corresponding to walking or jumping on the mini trampoline. Improvements in exercise motivation and support 
for continuous exercise are achieved in our system, since it is possible to enjoy strolling through a 
virtual space, which is usually difficult to experience, by exercising on the mini trampoline without 
injury to the user s joints. 1. AUGMENTING TRAMPOBICS Recently, lifestyle-related diseases have become 
recognized as problems. Changes in eating habits, stress in day-to-day living, smoking, and excessive 
drinking all contribute to the development of lifestyle-related diseases. Regular exercise is needed 
to prevent these diseases. Particularly beneficial are effective and continuous exercises appropriate 
for our physical strength. Nowadays, exercise machines for home use, such as running machines or aerobics 
bicycles, are prevalent. Trampobics is an aerobics training method using a mini trampoline. It substantially 
decreases damage to the joints compared with floor-based aerobics. Moreover, it has benefits such as 
constipation prophylaxis, diarrhea prevention, and alleviation of tension in the shoulders. It enables 
us to exercise effectively in a short time by controlling the heart rate. Because of this, it is expected 
to be a lifelong exercise. 2. SYSTEM OVERVIEW Our goal is for the user to enjoy exercising, and reap 
the benefits of highly effective trampoline exercise, and to alleviate difficulties in maintaining an 
exercise program. In our proposed system, when the user exercises on a mini trampoline, he or she experiences 
moving and jumping freely in VR space. We consider the use of our system by a wide range of age groups. 
Therefore, we envision households as a typical usage environment for our system. Figure 1 shows the appearance 
of the experience. Our system is composed of an input device and an image-generation system, which consists 
of a PC, two short-focus projectors, and two large­scale screens. The input device consists of a mini 
trampoline and a position­sensitive detector (PSD) sensor, which is installed beneath the center of the 
trampoline and measures the amount of change in the trampoline bed. A PC application detects the type 
and extent of the user s movement by processing the time series of the amount of changes in the trampoline 
bed. When the application detects the user s motion and calculates the corresponding viewpoint in VR 
space, the short-focus projector projects the view onto a large- default stand walk jump (low) jump 
(high) Figure 2. Relationship of exercise and an input signal. scale screen in front of the user. This 
projected image, synchronized with the exercise, gives the user the sense that he/she is exercising within 
the virtual scenery. 3. USER EXPERIENCE We propose a method to distinguish the type of exercise and status 
of the user according to the amount of change in the trampoline bed(Figure 2). In particular, our system 
provides a user with an image of the virtual viewpoint, synchronized with the exercises and varied according 
to whether the user is standing (at rest), walking, balancing, or jumping on the trampoline. Our system 
prepares versatile exercise content in VR selected according to the interests of the user and the desired 
exercise time. Examples of such content are strolling around streets, jumping higher than the roof of 
a city building, jumping sky-high in a natural environment such as a mountainous area, playing a sport, 
having a snowball fight, playing a treasure-hunting game, and so on. Copyright is held by the author 
/ owner(s). SIGGRAPH 2009, New Orleans, Louisiana, August 3 7, 2009. ISBN 978-1-60558-726-4/09/0008 
 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1597982</article_id>
		<sort_key>260</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>26</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>26</seq_no>
		<title><![CDATA[Virtualization gate]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1597956.1597982</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1597982</url>
		<abstract>
			<par><![CDATA[<p>The virtual reality community developed solutions for immersion based on advanced display technologies, mainly head-mounted displays (HMDs) and immersive multi-projector environments like Caves. Though these environments provide an impressive sense of immersion, users are usually limited in the way they can interact with virtual objects. Their influence on the 3D world, including users appearance, is very limited, impairing the immersion experience. Limitations come from the ability to perceive data from the users. Usually in 3D environments, interactions rely on a 3D tracker providing the position, velocity and identification of a limited set of markers the user is equipped with. Avatars can be used to enforce the sense of presence, but they lack to provide detailed information about the actual user body position or visual appearance.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.4.0</cat_node>
				<descriptor>Image displays</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>H.5.2</cat_node>
				<descriptor>Screen design (e.g., text, graphics, color)</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>A.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003120.10003123</concept_id>
				<concept_desc>CCS->Human-centered computing->Interaction design</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10011666</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Touch screens</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P1617574</person_id>
				<author_profile_id><![CDATA[81381602310]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Benjamin]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Petit]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[INRIA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617575</person_id>
				<author_profile_id><![CDATA[81381592410]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Jean-Denis]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lesage]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Grenoble Universities]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617576</person_id>
				<author_profile_id><![CDATA[81100397682]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Edmond]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Boyer]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Grenoble Universities]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617577</person_id>
				<author_profile_id><![CDATA[81100252773]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Bruno]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Raffin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[INRIA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Virtualization Gate Benjamin Petit * Jean-Denis Lesage Edmond Boyer Bruno Raf.n § INRIA Grenoble 
Universities Grenoble Universities INRIA  The virtual reality community developed solutions for immersion 
based on advanced display technologies, mainly head-mounted dis­plays (HMDs) and immersive multi-projector 
environments like Caves. Though these environments provide an impressive sense of immersion, users are 
usually limited in the way they can inter­act with virtual objects. Their in.uence on the 3D world, including 
users appearance, is very limited, impairing the immersion experi­ence. Limitations come from the ability 
to perceive data from the users. Usually in 3D environments, interactions rely on a 3D tracker providing 
the position, velocity and identi.cation of a limited set of markers the user is equipped with. Avatars 
can be used to enforce the sense of presence, but they lack to provide detailed information about the 
actual user body position or visual appearance. Multiple cameras can be used to compute, in real-time, 
a 3D model of the user. It enables to get geometric and photometric data about the user. The challenge 
is to compute a good quality model at a high refresh rate. Some existing works use octree-based 3D model­ing 
algorithms. The geometrical data can be precise, but the visual appearance is not satisfactory due to 
the dif.culty to accurately tex­ture the octree model using the photometric data provided by the cameras. 
Other approaches focus on the visual appearance of the user for telepresence. They rely on point cloud 
3D modeling algo­rithms. The visual quality is signi.cantly improved. Our contribution is to associate 
multi-camera 3D modeling, phys­ical simulation and tracked HMD for a full-body immersion and presence 
in virtual worlds. 3D modeling is based on the EPHV algorithm, that provides an exact shape with respect 
to input data. The geometry enables to compute full-body collisions with virtual objects animated by 
a physical simulation. Since the algorithm is exact, it allows for a consistent texture mapping hence 
yielding qualitative models. This full-body representation can thus be ren­dered on a distant site for 
telepresence. It can also be rendered into a HMD. The user sees his 3D model superposed with his real 
body occluded by the HMD. Because the displays are hold in front of the *e-mail: benjamin.petit@inria.fr 
e-mail: jean-denis.lesage@imag.fr e-mail: edmond.boyer@inrialpes.fr §e-mail: bruno.raf.n@imag.fr  user 
s eyes, the image projection is not impaired by elements of the real world. With a .xed screen, even 
in an immersive Cave like con.guration, the user would not be able to see a virtual object in his hand 
palm as his hand would occlude the light emitted by the displays. With our approach the user sees the 
3D model of his hand and the virtual object correctly positioned in his palm. It enables a .rst-person 
viewing and occlusion-free co-located interactions. Our set-up1 is built around several components. The 
video acquisi­tion, background subtraction, segmentation, texture extraction and 3D modeling steps are 
distributed on a PC cluster to enable a real­time execution (about 20 times per second). The SOFA2 framework 
runs the physical simulation. The 3D model is injected into the simulation that manages it as a solid 
object not subject to external forces. The user wears a HMD tracked with an infrared positioning system. 
An off-line calibration process enables to align the cam­eras, the tracker and the HMD within the real 
world. The 3D model is textured by mixing the photometric data extracted from the clos­est cameras to 
the user s viewpoint. Notice that the 3D modeling system makes no assumption about the scene observed. 
One or sev­eral persons can stand in the acquisition space. It only affects the model quality and the 
computation time. The application is devel­oped on top of the FlowVR3 library, a middleware dedicated 
to high performance interactive applications. It enforces a modular pro­gramming though a hierarchical 
component model that leverages software engineering issues while enabling ef.cient executions on parallel 
architectures. We presented at the 2007 Siggraph Emerging Technologies the Grimage platform for markerless 
3D modeling. Images were ren­dered on a .xed screen positioned behind the acquisition space, pro­viding 
little immersion and only third-person interactions. The ac­quisition space was also notably smaller 
enabling only user s hands modeling. Acknowledgements We would like to thank the following per­sons for 
their fruitful contributions to the project : Laurence Boissieux, INRIA, Thomas Dupeux, INRIA, Nicolas 
Turro,INRIA, Herv´e Mathieu, INRIA, , Franois Faure, Grenoble Universities, Florent Falipou, INRIA, Micha¨el 
Adam, INRIA, Cl´ement M´enier, 4DView Solutions, Florian Geffray, 4DView Solutions. This work was partly 
funded by Agence Nationale de la Recherche, contract ANR-06-MDCA-003. 1http://grimage.inrialpes.fr 2http://www.sofa-framework.org/ 
 3http://flowvr.sf.net/  Copyright is held by the author / owner(s). SIGGRAPH 2009, New Orleans, Louisiana, 
August 3 7, 2009. ISBN 978-1-60558-726-4/09/0008 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<article_sponsors>
			<funding_agency>Agence Nationale de la Recherche</funding_agency>
			<grant_numbers>
				<grant_number>ANR-06-MDCA-003</grant_number>
			</grant_numbers>
		</article_sponsors>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
</content>
</proceeding>
