<?xml version="1.0" encoding="ISO-8859-1"?>
<!DOCTYPE proceeding SYSTEM "proceeding.dtd">
<proceeding ver="6.0" ts="04/10/2010">
<conference_rec>
	<conference_date>
		<start_date>07/27/2003</start_date>
		<end_date>07/31/2003</end_date>
	</conference_date>
	<conference_loc>
		<city><![CDATA[San Diego]]></city>
		<state>California</state>
		<country></country>
	</conference_loc>
	<conference_url></conference_url>
</conference_rec>
<series_rec>
	<series_name>
		<series_id>SERIES382</series_id>
		<series_title><![CDATA[International Conference on Computer Graphics and Interactive Techniques]]></series_title>
		<series_vol></series_vol>
	</series_name>
</series_rec>
<proceeding_rec>
	<proc_id>965333</proc_id>
	<acronym>SIGGRAPH '03</acronym>
	<proc_desc>ACM SIGGRAPH 2003 Web Graphics</proc_desc>
	<conference_number>2003</conference_number>
	<proc_class>conference</proc_class>
	<proc_title></proc_title>
	<proc_subtitle></proc_subtitle>
	<proc_volume_no></proc_volume_no>
	<issn></issn>
	<eissn></eissn>
	<copyright_year>2003</copyright_year>
	<publication_date>07-27-2003</publication_date>
	<pages>45</pages>
	<plus_pages></plus_pages>
	<price><![CDATA[]]></price>
	<other_source></other_source>
	<publisher>
		<publisher_id>PUB27</publisher_id>
		<publisher_code>ACMNY</publisher_code>
		<publisher_name>ACM</publisher_name>
		<publisher_address>2 Penn Plaza, Suite 701</publisher_address>
		<publisher_city>New York</publisher_city>
		<publisher_state>NY</publisher_state>
		<publisher_country>USA</publisher_country>
		<publisher_zip_code>10121-0701</publisher_zip_code>
		<publisher_contact>Bernard Rous</publisher_contact>
		<publisher_phone>212 869-7440</publisher_phone>
		<publisher_isbn_prefix></publisher_isbn_prefix>
		<publisher_url>www.acm.org/publications</publisher_url>
	</publisher>
	<sponsor_rec>
		<sponsor>
			<sponsor_id>SP932</sponsor_id>
			<sponsor_name>ACM Special Interest Group on Computer Graphics and Interactive Techniques</sponsor_name>
			<sponsor_abbr>SIGGRAPH</sponsor_abbr>
		</sponsor>
	</sponsor_rec>
	<categories>
		<primary_category>
			<cat_node/>
			<descriptor/>
			<type/>
		</primary_category>
	</categories>
	<chair_editor>
		<ch_ed>
			<person_id>P15779</person_id>
			<author_profile_id><![CDATA[81100334998]]></author_profile_id>
			<orcid_id></orcid_id>
			<seq_no>1</seq_no>
			<first_name><![CDATA[Alyn]]></first_name>
			<middle_name><![CDATA[P.]]></middle_name>
			<last_name><![CDATA[Rockwood]]></last_name>
			<suffix><![CDATA[]]></suffix>
			<affiliation><![CDATA[Colorado School of Mines]]></affiliation>
			<role><![CDATA[Conference Chair]]></role>
			<email_address><![CDATA[]]></email_address>
		</ch_ed>
	</chair_editor>
	<ccc>
		<copyright_holder>
			<copyright_holder_name>ACM</copyright_holder_name>
			<copyright_holder_year>2003</copyright_holder_year>
		</copyright_holder>
	</ccc>
</proceeding_rec>
<content>
	<section>
		<section_id>965336</section_id>
		<sort_key>1</sort_key>
		<section_seq_no>1</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Advanced 3D techniques]]></section_title>
		<section_page_from>1</section_page_from>
		<chair_editor>
			<ch_ed>
				<person_id>PP14033296</person_id>
				<author_profile_id><![CDATA[81543728956]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Alan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Norton]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Colorado School of Mines]]></affiliation>
				<role><![CDATA[Session Chair]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
		</chair_editor>
	<article_rec>
		<article_id>965339</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Extensible behavior simulation with motion archive]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965333.965339</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965339</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P263739</person_id>
				<author_profile_id><![CDATA[81100252511]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Shigeru]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kuriyama]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Toyohashi University of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP45025259</person_id>
				<author_profile_id><![CDATA[81336491652]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Tomohiko]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Mukai]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Toyohashi University of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P653805</person_id>
				<author_profile_id><![CDATA[81100118359]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Yusuke]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Irino]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Toyohashi University of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P653778</person_id>
				<author_profile_id><![CDATA[81536797956]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Kazuyuki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Anda]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Toyohashi University of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14134383</person_id>
				<author_profile_id><![CDATA[81100379841]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Toyohisa]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kaneko]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Toyohashi University of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>514264</ref_obj_id>
				<ref_obj_pid>514236</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[BADLER, N. I., ERIGNAC, C. A., AND LIU, Y. 2002. Virtual Humans for Validating Maintenance Procedures. Communications of the ACM 45, 7, 57--63.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>508129</ref_obj_id>
				<ref_obj_pid>508126</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[GLEICHER, M. 2001. Comparing Constraint-Based Motion Editing Methods. Graphical Models 63, 2, 107--134.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>566607</ref_obj_id>
				<ref_obj_pid>566654</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[LEE, J., CHAI, J., REITSMA, P., HODGINS, J., et al. 2002. Interactive Control of Avatars Animated With Human Motion Data. ACM Trans. on Graph. 21, 3, 491--500.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Extensible Behavior Simulation with Motion Archive Shigeru Kuriyama, Tomohiko Mukai, Yusuke Irino, Kazuyuki 
Anda, and Toyohisa Kaneko Toyohashi University of Technology 1. Introduction Humanoid animations in 
Web3D are largely created forentertainment uses such as role-playing games, but they are seldom used 
as tools for evaluating working environments [Badler et al. 2002]. Such tools have been implemented as 
plug-ins of expensive industrial CAD systems, and theyoften require elaborate manipulations in creating 
realistichuman movements with kinematics-based controls. On the other hand, use of motion capture data 
becomes a majortool for designing realistic movements with intelligent editing [Gleicher 2001] and transition 
[Lee et al. 2002]methods. Integration of such data-oriented tools with behavior modeling ones can enhance 
the quality of simulations. We therefore developed an environment that can simulate realistic human behaviors 
with an archive of motion capture data. A motion archive has been constructedby measuring 12 persons 
movements as they transfer and manipulate objects, whose total playback time reaches 6hours. Our simulation 
middleware intelligently manages the motion data and displays them with X3D humanoid animations through 
gl4Java. Our system can be flexiblyextended on the basis of XML-based behavior specification,by which 
any type of environments can be evaluated. 2. System architecture Our system consists of three layers 
of a MVC (Model, View, and Control) architecture. The model layer supplies an XML archive involving motion 
and skeleton information, extracted from BVH, and behavioral and environmental data for customizing simulations. 
The control layergenerates motions through the conversion of the motion data from behavior rules with 
extensible transformation tools, and also supplies a dynamical analyzer for evaluatingthe resulting motions. 
The view layer manages the X3Ddata of virtual humans and task environments, and supplies an information 
visualization tool using Java2D for efficiently searching for motion segments from the archive. *e-mail: 
kuriyama@ics.tut.ac.jp Copyright held by the author . Human: specifies the skeleton data file, type 
of geometric embodiment, and initial location and behavior. . Sensor: specifies the class that manages 
sensoryinformation for an environment and targeting objects. . Manipulator: specifies the class that 
reactively manipulates objects and utilized sensors. . Behavior: specifies a sequence of playback motions. 
 . Motion: specifies the resource name and frame bounds for playback and controllers by which the movements 
are automatically transformed. . Controller: specifies the class that transforms motion data with constraints 
given from sensors.  This system supplies base classes of a Java library for creating or extending sensors, 
manipulators, and controllers in order to be adapted to an environment of simulation.Figure 2 shows the 
snapshots of a simulation of assemblingin an automobile plant. Figure 2: Assembling simulation in automobile 
plant  3. Acknowledgements This work was supported in The 21st Century COE Program Intelligent Human 
Sensing , from the ministry of Education, Culture, Sports, Science and Technology. This was also funded 
in Exploratory Software Project, from Information-technology Promotion Agency. References BADLER, N.I., 
ERIGNAC, C.A., AND LIU, Y. 2002. Virtual Humans for Validating Maintenance Procedures. Communications 
of the ACM 45, 7, 57-63. GLEICHER, M. 2001. Comparing Constraint-Based Motion Editing Methods. Graphical 
Models 63, 2, 107-134. LEE, J., CHAI, J., REITSMA, P., HODGINS, J., et al. 2002. Interactive Control 
of Avatars Animated With Human Motion Data. ACM Trans. on Graph. 21, 3, 491-500. http://www.vcl.ics.tut.ac.jp/xsim/ 
 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>965338</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Flux]]></title>
		<subtitle><![CDATA[lightweight, standards-based Web graphics in XML]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965333.965338</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965338</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP28004996</person_id>
				<author_profile_id><![CDATA[81100583806]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Tony]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Parisi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Media Machines, Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[BELL, G., PARISI, T., and PESCE, M. 1995. The VRML 1.0 specification]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[WEB3D CONSORTIUM 2003. The X3D International Standard, ISO/IEC FDIS 19775:2003]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Flux: Lightweight, Standards-based Web Graphics in XML Tony Parisi Media Machines, Inc. 1 Introduction 
Standards for web-based graphics have evolved greatly from their primitive beginnings in VRML 1.0 [Bell 
1995] to the new X3D standard [Web3D 2003]. In the last several years, advances in graphics as well as 
those in networking, most notably the emergence of the Extensible Markup Language (XML), have heavily 
influenced the development of X3D. X3D embodies best practices in commercial real time graphics within 
the framework of the XML family of technologies. This paper describes one implementation of the X3D standard, 
the Flux Player from Media Machines, and the issues inherent with deploying X3D in a commercial setting. 
 2 X3D Requirements X3D poses unique challenges for creating an effective web graphics runtime system. 
X3D combines an authoring-friendly scene graph with real time playback, network delivery, an extensible 
object system and client-side programmability. The specification is platform-, device-and programming-language 
neutral, offering a wide range of choices for deployment. X3D content may originate from a variety of 
sources: a single world can be authored by multiple authors using a range of tools, and located on disparate 
servers with varying capabilities. Further, the specification is broad in scope, encompassing applications 
that include embedded web page presentations, online training applications, turnkey scientific visualization 
products, and immersive simulations. The foregoing requirements, plus the commercial need to create a 
runtime system that not only performs well but also downloads quickly and integrates seamlessly into 
the host operating environment, levy great demands on an implementation. 3 The Flux System Media Machines 
has developed its Flux technology to meet the challenges specific to deploying web-standard graphics. 
Flux consists of three interoperating components: an Engine, Web Tools, and a Media Player. These components 
operate in independent threads, providing the fastest rendering speed, network throughput and user response. 
The Flux Engine implements a real time, multiply instanced scene graph. High performance is achieved 
using an optimized Visitor traversal that de-instances the scene graph to populate an underlying render 
graph. The render graph supports fast view culling, picking and state sort-based drawing, and also provides 
a convenient abstraction layer to isolate the details of the underlying rendering library. The Web Tools 
component loads X3D content encoded in XML, VRML Classic or Binary formats. The file loaders use a common 
core based on compiler technology that is also shared with a built-in ECMAScript interpreter and the 
type system used to support X3D s extensible objects. The XML loader employs both the XML Document Object 
Model (DOM) and the Simple API for XML (SAX). The Flux Media Player provides the X3D end user experience, 
implementing user navigation and input per the specification. The Media Player is packaged together with 
the Engine and Web Tools as a small footprint plug-in (less than 700 kilobytes compressed) that can be 
hosted in Internet Explorer or Netscape, so that Flux can be easily downloaded on a just-in time basis. 
 4 Conclusion X3D incorporates recent advances in real time graphics into the XML framework of the Web. 
Media Machines Flux technology provides working proof that X3D can be implemented with high performance, 
robustness and portability and deployed in a practical fashion over the Web.  References BELL, G., PARISI, 
T., and PESCE, M. 1995. The VRML 1.0 specification WEB3D CONSORTIUM 2003. The X3D International Standard, 
ISO/IEC FDIS 19775:2003 Copyright held by the author 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>965337</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[High-level procedural shading VRML/X3D]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965333.965337</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965337</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P653769</person_id>
				<author_profile_id><![CDATA[81100637776]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Gon&#231;alo]]></first_name>
				<middle_name><![CDATA[Nuno Moutinho]]></middle_name>
				<last_name><![CDATA[de Carvalho]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Digital Design Studio - Glasgow School of Art]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>552872</ref_obj_id>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[OLANO, M., HART, J. C., HEIDRICH, W., AND MCCOOL, M. 2002. Real-Time Shading. A. K. Peters.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 High-Level Procedural Shading VRML/X3D Gonc¸alo Nuno Moutinho de Carvalho* Digital Design Studio -Glasgow 
School of Art 1 Introduction VRML/X3D is a speci.cation language for the representation of virtual worlds. 
We describe a new set of nodes that allow the use of high-level shading languages (HLSL) in VRML/X3D 
worlds. Real-time use of procedural shaders has always been a moving target for the computer graphics 
industry since the introduction of RenderMan [Olano et al. 2002]. The current generation of commodity 
graphics hardware provi­ding programmable shading functionality allows for unprecedented levels of realism. 
Unfortunately these capabilities are available only to super-users by means of a programmatic interface 
like OpenGL or Direct3D. 2 High Level Shading Languages To complicate matters further, no emerging uni.ed 
HLSL stan­dard exists. Four principal languages exist: Microsoft s DirectX 9 HLSL, Nvidia s Cg, SGI s 
ISL and the future OpenGL2.0 glslang, .rst proposed by 3Dlabs. Fortunately there is a commonality among 
all these languages; each is a high level language, closely resembling C, and the shader parameters are 
usually extended with some set of attributes. Although vertex shaders in some languages are speci.ed 
with a very low level (e.g. ISL), assembler-like language, they can still be abstracted by the above 
reasoning. It s this commonality that allows us to derive a new set of VRML/X3D nodes to expose the functionality 
to authors.  3 High-Level Procedural Shading VRML/X3D We propose three new categories of nodes: grouping 
shader nodes, shader type nodes (vertex, fragment or light) and shader parameters. A ShaderGroup node 
encompasses a sub-scene graph to which all shaders contained in the ShaderGroup .eld shader are applied. 
The .eld shader is, under VRML/X3D terminology, an exposed-Field of type MFNode, which means that shaders 
can be replaced at runtime, using the common ROUTE and Script mechanisms. It may seem tempting to simply 
replace the Appearance node with a ShaderAppearance instead of introducing a grouping node. The reason 
why this is not being considered, is because shaders are meant to replace speci.c parts of the .xed function 
pipeline (FFP), and therefore all of the current nodes can maintain their existing meaning, and be, effectively, 
extended by the shader nodes. Shaders usually come in two .avors: vertex and fragment (Some languages 
like ISL allow for light shaders. We will not treat them here). Vertex shaders replace the TnL (Transform 
and Lighting) part of the FFP, and frament shaders replace the pixel blending transformations part of 
the FFP. Only one node, Shader, is intro­duced for this category with .elds to specify the language, 
type and parameters of the shader. The parameters are, obviously, allowed to change at runtime, to allow 
for dynamic effects. * e-mail: g.carvalho@gsa.ac.uk Figure 1: Marble torus with a thin layer of glass. 
Shaders parameters are nodes that contain an exposedField of the basic type of the parameter (bool, .oat, 
int and so on). Parame­ters exist in two types: an array type and a single type. For exam­ple, .oating 
point parameters have the following representations: ShaderParameterMFFloat and ShaderParameterSFFloat. 
Shading languages allow for the speci.cation of attribute quali.ers to the parameters. Again, this is 
also possible in our extensions, with the quali.ers taking the meaning from the target language. Interfaces 
for procedural shading in commercial packages (Alias Wavefront s Maya or discreet s 3ds max) have been 
developed. Graphics card manufacturers have their own shader editors im­plemented along with such an 
interface. Authoring of procedural shaders by means of a GUI is now available. The only dif.culty that 
remains is to decide for which language to write and for which pro.le. Without a single language to choose 
from and with so many available pro.les this is the biggest challenge. Hopefully in the fu­ture, with 
the emergence of a uni.ed standard, this issue will .nally be resolved. 4 Conclusion We ve demonstrated 
how it s possible to interface the capabili­ties of today s programmable GPU s with VRML/X3D. Although 
most current browsers will not support such an interface, it is ex­pected that they will do so in the 
future. Either using the au­thor s interface or an improvement, authors of VRML/X3D worlds will be able 
to bring cinematic qualities to their designs. This interface is currently implemented and is being extended 
by the author in the Digital Design Studio s own VRML/X3D browser RealightXP. More information and examples 
can be found at http://www.digitaldesignstudio.org/shading.  References OLANO, M., HART, J. C., HEIDRICH, 
W., AND MCCOOL, M. 2002. Real-Time Shading. A. K. Peters. Copyright held by the author  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	</section>
	<section>
		<section_id>965340</section_id>
		<sort_key>2</sort_key>
		<section_seq_no>2</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Visualization]]></section_title>
		<section_page_from>2</section_page_from>
		<chair_editor>
			<ch_ed>
				<person_id>PP42053570</person_id>
				<author_profile_id><![CDATA[81327491146]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Sandy]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ressler]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[National Institute of Standards and Technology ROLESESSION CHAIR]]></affiliation>
				<role><![CDATA[Editor]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
		</chair_editor>
	<article_rec>
		<article_id>965341</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Earth's avatar]]></title>
		<subtitle><![CDATA[the Web augmented virtual earth]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965333.965341</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965341</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP14035902</person_id>
				<author_profile_id><![CDATA[81323496636]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Chris]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Thorne]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Ping Interactive Broadband pty. ltd.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P653803</person_id>
				<author_profile_id><![CDATA[81350592224]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Viveka]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Weiley]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Ping Interactive Broadband pty. ltd.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[LECLERC, Y., REDDY, M., ERIKSEN, M., BRECHT, J. AND COLLEEN, D. 2002 SRI's Digital Earth Project, Technical Note No. 560, Artificial Intelligence Center, SRI International, Menlo Park, CA. Planet 9 Studios, SF, CA.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>604532</ref_obj_id>
				<ref_obj_pid>604471</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[CHAMPION, E. 2003. Applying game design theory to virtual heritage environments, Proceedings of the 1st international conference on Computer graphics and interactive techniques in Australasia and South East Asia, Melbourne, Australia.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>274467</ref_obj_id>
				<ref_obj_pid>274450</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[CLARKE-WILLSON, S. 1998. Applying game design to virtual environments, Digital Illusion: Entertaining the Future with High Technology. ACM Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>625405</ref_obj_id>
				<ref_obj_pid>624604</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[SCHNEIDERMAN, B. November 1994. Dynamic Queries for Visual Information Seeking. IEEE Software, Volume 11, Issue 6.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Earth s Avatar the Web Augmented Virtual Earth Chris Thorne, Viveka Weiley§ Ping Interactive Broadband 
pty. ltd. 1Introduction Virtual worlds including whole earth environments have been made since the beginning 
of VR. In 1998, Vice President Al Gore described a vision called the Digital Earth" which led to a burst 
of activity. This subsided after the year 2000 when the political landscape and organizational structures 
changed. We postulate another reason for this downturn was due to a focus by some developers on proprietary, 
costly infrastructure for large Earth databases, for which applications were then sought. This approach 
can quickly either overwhelm the capacity of an organization s resources, or at least become hard to 
justify when there is no additional funding stream. Therefore, we propose as an alternative a low cost, 
open source, collaborative and highly scalable system.  Some existing development has come from organizations 
with their own map databases, intent on publishing their own content. Our system will empower other organizations 
or individuals to publish their own content into the developing Virtual Earth, and will facilitate broader 
access to it. We have started implementing these methods in our 3D Metanet Atlas Platform (3map) project. 
 2Evolution of Virtual Earth initiatives A brief history of Virtual Earth initiatives includes early 
VRML earths such as Mark Pesce's WebEarth, Rez1 and planet-earth2 (now part of 3map), Stanford Research 
Institute (SRI) s TerraVision system [Leclerc 2002], the Interagency Digital Earth Working group s Digital 
Earth Reference Model (DERM), Web Mapping Testbed (WMT), and Digital Earth Alpha Version; the Geospatial 
Applications and Interoperability (GAI) working group s GIRM -Guide for interoperable geospatial applications; 
the work of the Open GIS Consortium(OGC); and work in the Web3D Consortium's X3D, RM3D and GeoVRML working 
groups. 3The 3map project an open systems approach The 3map sample implementation is being built using 
OGC standards to promote interoperability and open access &#38; publishing, and Java and X3D/GeoVRML 
in order to facilitate broad availability of the runtime. Other delivery technologies will also be considered, 
such as Shockwave3D, Blendo on the Playstation 2, as well as MPEG-4 set-top boxes. The time-sensitive 
nature of the data, as well as the very large size of global datasets requires online delivery, preferably 
over broadband networks. We anticipate that end-users and third-party developers will find innovative 
and unexpected ways of using and extending the system, as has happened with the Word Wide Web. 3map s 
architecture will be based on low cost internetworked systems that can be easily scaled as content and 
other resource demands grow. 4 Interaction design As well as providing an open platform, 3map will broaden 
the base for Virtual Earth access and publishing through innovations in user interface. Users will be 
able to navigate effectively, filter information using structured metadata, and publish their own content 
from within the context of the virtual world. 1 http://www.ping.com.au/3map/rez or http://www.surak.com.au/~chris/vrml/RezIndex.html 
2 http://www.planet-earth.org Lessons from game design show us that navigating 3D space can be simple 
and effective if users are given appropriate affordances and constraints. Generalised VR navigation systems 
typically allow unconstrained movement in three dimensions, leading to user disorientation [Clarke-Willson, 
1998].  3map will provide a series of constrained navigation modes, appropriate to user task flow, which 
is closely linked to the user s proximity to the planet. The following modes are being developed: Fig 
1. Orbital rotate globe to select an area of interest. Fig 2. Satellite/map nadir-facing (looking straight 
down), users can move across the surface of the planet. Fig 3. Flight initially facing diagonally down, 
but with control of view angle, users can fly at a constant height above terrain. The fourth navigation 
mode is similar to VRML Walk navigation. Additionally, a target lock facility can constrain forward motion 
[Clarke-Willson, 1998]. This is particularly useful when users are placing geometry in the environment. 
In games, the environment is often a static setting or backdrop. In 3map, the setting becomes an interactive 
artifact [Champion 2003]. At any point, users can find information or filter their view using dynamic 
queries on structured metadata. This is an effective user interface technique for manipulating complex 
datasets, especially suited to geographic applications, which leads to significant improvements in user 
satisfaction and engagement over traditional text searches [Schneiderman, 1994]. In fig 2, a user is 
seeking a 3­bedroom house in a specified price range. 5 Conclusion The next generation of Virtual Earth 
implementations will enhance everyday activity with virtual experience. 3MAP is aimed at achieving this 
vision through sustainable progressive augmentation, open source development and open access &#38; publishing. 
 References LECLERC, Y., REDDY, M., ERIKSEN, M., BRECHT, J. AND COLLEEN, D. 2002 SRI's Digital Earth 
Project, Technical Note No. 560, Artificial Intelligence Center, SRI International, Menlo Park, CA. 
Planet 9 Studios, SF, CA. CHAMPION, E. 2003. Applying game design theory to virtual heritage environments, 
Proceedings of the 1st international conference on Computer graphics and interactive techniques in Australasia 
and South East Asia, Melbourne, Australia. CLARKE-WILLSON, S. 1998. Applying game design to virtual environments, 
Digital Illusion: Entertaining the Future with High Technology. ACM Press. SCHNEIDERMAN, B. November 
1994. Dynamic Queries for Visual Information Seeking. IEEE Software, Volume 11 , Issue 6. § s2003@ping.com.au 
Copyright held by the author  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>965343</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Design patterns for pseudo-3D photo collage]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965333.965343</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965343</url>
		<abstract>
			<par><![CDATA["Pseudo-3D Photo Collage", one of new wed3D techniques, is based on multiple photographs and <i>Spatial-Hyperlinks</i>. A user can design various patterns of <i>HyperPhoto-Networks</i>, for example, <i>linear, loop, tree, grid, rhizome</i> and so on. It is similar to design a website based on <i>HyperText-Networks</i>. We propose several design patterns and those applications to create highly-designed virtual sequences.]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP35024783</person_id>
				<author_profile_id><![CDATA[81100135119]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Hiroya]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tanaka]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Kyoto University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14096696</person_id>
				<author_profile_id><![CDATA[81100253521]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Masatoshi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Arikawa]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP40029034</person_id>
				<author_profile_id><![CDATA[81100609541]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Ryosuke]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Shibasaki]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1242325</ref_obj_id>
				<ref_obj_pid>1242073</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Hiroya Tanaka, Masatoshi Arikawa, Ryosuke Shibasaki: Pseudo-3D Photo Collage, Siggraph2002 Conference Abstracts and Applications, pp.317.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[http://www.photowalker.net/]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Design Patterns for Pseudo-3D Photo Collage Hiroya Tanaka* Masatoshi Arikawa Ryosuke Shibasaki Graduate 
School of Informatics, Center for Spatial Information Center for Spatial Information Kyoto University. 
Science, the University of Tokyo Science, the University of Tokyo Abstract Pseudo-3D Photo Collage , 
one of new wed3D techniques, is based on multiple photographs and Spatial-Hyperlinks. A user can design 
various patterns of HyperPhoto-Networks, for example, linear, loop, tree, grid, rhizome and so on. It 
is similar to design a website based on HyperText-Networks. We propose several design patterns and those 
applications to create highly­designed virtual sequences. 1. Introduction At the Siggraph2002 conference, 
we introduced a new web3D technique named Pseudo-3D Photo Collage [1]. This is one of image-based approaches 
for creating and browsing virtual space on the web. This virtual space can be created by simple operations 
as follows. (1) A user draws a rectangle in a set of photographs to indicate one corresponding area (Figure.1) 
 (2) The system generates a Spatial-Hyperlink, which associates two photographs with a corresponding 
area (Figure.2). (3) A user can create HyperPhoto-Networks composed of many photographs and Spatial-Hyperlinks 
(Figure.3). (4) When a user wants to browse HyperPhoto-Networks, the system calculates a geometric transformation 
and overlap two photographs. A user can traverse one photo to another with morphing animations (Figure.4). 
  Figure1: Corresponding rectangles Figure2: A set of Photographs associated with a Spatial Hyperlink 
*e-mail:hirotanaka@dl.kuis.kyoto-u.ac.jp e-mail:arikawa@csis.u-tokyo.ac.jp e-mail:shiba@csis.u-tokyo.ac.jp 
This research is partly supported by the research for the grant of Scientific Research (15017249) from 
Ministry of Education, Culture, Sports, Science and Technology of Japan. We would like to thank Prof. 
Katsumi Tanaka (Kyoto University) for valuable advice.  Copyright held by the author Figure3: HyperPhoto-Networks 
 Figure4 Animations for traversing one photo to another 2. Design Patterns A user can design various 
patterns of HyperPhoto-Networks, for example, linear, loop, tree, grid, rhizome and so on. It is similar 
to design a website based on HyperText-Networks. We made a matrix which indicates the relationships between 
patterns and those applications (Figure.5). By using this matrix, a user can create highly-designed sequences. 
We are now collecting various samples of virtual space by using our system and managing a portal website[2]. 
At the conference, we will show several attractive samples.  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>965342</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Candytop]]></title>
		<subtitle><![CDATA[a Web3D interface to visualize growth of multimedia documents]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965333.965342</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965342</url>
		<abstract>
			<par><![CDATA[Candytop is a web3D interface to visualize growth of multimedia documents along with a time line. Users can easily realize the relation among documents and catch up the context behind the projects. We use X3D VRML97 Profile for modeling and visualization.]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P11884</person_id>
				<author_profile_id><![CDATA[81100476406]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Akira]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wakita]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Sphere System Design]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P635437</person_id>
				<author_profile_id><![CDATA[81546797156]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Naofumi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yoshida]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39047567</person_id>
				<author_profile_id><![CDATA[81100538386]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Jun]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Miyazaki]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Fuji Xerox]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP77042831</person_id>
				<author_profile_id><![CDATA[81409593388]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Hiroaki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Chiyokura]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[WEB3D CONSORTIUM, 2002. EXTENSIBLE 3D, HTTP://WWW.WEB3D.ORG/X3D.HTML.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[FRERICHS, D., PIXEL PERFECT TEXTURES., HTTP://WWW.FRERICHS.NET/VRML2/PP/PIXEL_PERFECT.HTML.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[WIKI, 2002. HTTP://WWW.WIKI.ORG.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Candytop : A Web3D Interface to Visualize Growth of Multimedia Documents Akira Wakita* Naofumi Yoshida 
Sphere System Design Keio University Abstract Candytop is a web3D interface to visualize growth of multimedia 
documents along with a time line. Users can easily realize the relation among documents and catch up 
the context behind the projects. We use X3D VRML97 Profile for modeling and visualization. Project URL 
: http://www.sfc.keio.ac.jp/~wakita/candytop/ 1 Introduction 3D interfaces are often used for visualization 
of logical relationship among documents. Because users can interact with 3D objects from multiple viewpoints, 
it is suitable for visualization of complex structures that contain multiple variables. Taking advantage 
of this merit, a variety of 3D interfaces are proposed. However, a method to visualize growth of documents 
along with a time line or a method which supports easy understanding of contexts behind documents hardly 
exists. To address this problem, we propose Candytop, a 3D interface and its visualization method based 
on the metaphor of candy. The structure of candy, that is, drops are cut off from a candy tube and transparent 
drops contain grains, is able to visualize growth of multimedia documents effectively. 2 Basic Models 
 We focused on the structure of candy and the process of making candy. A candy block is stretched and 
cut into drops. The material of candy is often transparent and grains contained in a drop can be seen. 
We associate grains to multimedia documents and associate a drop to a time line segment. As a result, 
stretched candy, that is sequence of drops, is associated to a fixed-point observation. By comparing 
adjacent drops or by walking through a drop sequence, growth of documents can be shown. We have modeled 
the above structure as a 3D model as shown in figure 1. World is a 3D scene where Candy is placed. Drop 
is modeled as a short cylinder and is able to cut off from Candy. Grain, which is associated to each 
multimedia document, is modeled as a rectangle polygon. Arrow represents the relationship between two 
Grains. Tube represents growth of a Grain along with a time line. Tube can be regarded as a connectivity 
of a Grain between Drops. Tube is modeled by a skinning operation through a time line. This research 
is supported by Fuji Xerox Co.,Ltd. *e-mail: wakita@sfc.keio.ac.jp e-mail: naofumi@sfc.keio.ac.jp e-mail: 
jun.miyazaki@fujixerox.co.jp +e-mail: chiyo@sfc.keio.ac.jp Jun Miyazaki Hiroaki Chiyokura+ Fuji Xerox 
Keio University Arrow Tube Drop  Candy World Figure 1. Basic Model  3 Implementation We have implemented 
the Candytop model with X3D VRML97 Profile [1]. Textures are mapped on Grain walls. If the viewpoint 
is near a Grain, we use Pixel Perfect Textures [2] to avoid distortions and poor visuals caused by the 
point-sampling technique of texture mapping used by most software rendering engines. Figure 2 shows an 
example of an application. We have applied Candytop for the visualization of Wiki [3], a collaborative 
editing software on the Web. Our visualization can provide an at-a-glance view of multi-versions of documents 
and provide circumstances of discussion context in multimedia documents for users without the discussion 
context in the virtual space. References [1] WEB3D CONSORTIUM, 2002. EXTENSIBLE 3D, HTTP://WWW.WEB3D.ORG/X3D.HTML 
. [2] FRERICHS, D., PIXEL PERFECT TEXTURES., HTTP://WWW.FRERICHS.NET/VRML2/PP/PIXEL_PERFECT.HTML . 
[3] WIKI, 2002. HTTP://WWW.WIKI.ORG . Copyright held by the author  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	</section>
	<section>
		<section_id>965344</section_id>
		<sort_key>3</sort_key>
		<section_seq_no>3</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Exhibiting Web art]]></section_title>
		<section_page_from>3</section_page_from>
		<chair_editor>
			<ch_ed>
				<person_id>PP42054852</person_id>
				<author_profile_id><![CDATA[81339533859]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Jane]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Veeder]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[San Francisco State University]]></affiliation>
				<role><![CDATA[Session Chair]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
		</chair_editor>
	<article_rec>
		<article_id>965345</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[ArtStream]]></title>
		<subtitle><![CDATA[bringing Web art into museum culture]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965333.965345</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965345</url>
		<abstract>
			<par><![CDATA[Works that involve Internet technology have been problematic for museums to show, to the point of exclusion. ArtStream, a Web-based, curated exhibition space for Internet art, seeks to promote the work of Internet artists while bringing it into the "legitimized" world of traditional museum culture. ArtStream is the product of a collaboration between the University of Arizona Museum of art and the Treistman Center for New Media, in the University of Arizona College of Fine arts. The project was funded by a New Learning Environments grant from the university.]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P653800</person_id>
				<author_profile_id><![CDATA[81100050130]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Tom]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hapgood]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Arizona]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P174143</person_id>
				<author_profile_id><![CDATA[81100047360]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Lucy]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Petrovich]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Arizona]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14104031</person_id>
				<author_profile_id><![CDATA[81536614656]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Peter]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Briggs]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Arizona Museum of Art]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P653784</person_id>
				<author_profile_id><![CDATA[81100007355]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Mike]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Holcomb]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Arizona]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P653755</person_id>
				<author_profile_id><![CDATA[81100385302]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Charles]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Guerin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Arizona Museum of Art]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P653796</person_id>
				<author_profile_id><![CDATA[81100331035]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Shelley]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tolman]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Treistman Center]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P653783</person_id>
				<author_profile_id><![CDATA[81100505544]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>7</seq_no>
				<first_name><![CDATA[Michael]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Branch]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Treistman Center]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Bishop, Janet, 2001. Old-Fashioned Forms in Newfangled Times or Why Would Anybody in this Hip and Modern World Bother Making Paintings?. In art in Technological Times, Coerver, Chad, Ed., San Francisco Museum of Modern art, 73--75.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 ArtStream: Bringing Web Art into Museum Culture Tom Hapgood Digital Arts Design Manager Treistman Center 
for New Media University of Arizona Professor Mike Holcomb Assistant Dean of Digital Technology College 
of Fine Arts University of Arizona  Abstract Works that involve Internet technology have been problematic 
for museums to show, to the point of exclusion. ArtStream, a Web­based, curated exhibition space for 
Internet art, seeks to promote the work of Internet artists while bringing it into the legitimized world 
of traditional museum culture. ArtStream is the product of a collaboration between the University of 
Arizona Museum of Art and the Treistman Center for New Media, in the University of Arizona College of 
Fine Arts. The project was funded by a New Learning Environments grant from the university. 1 Introduction 
The further we progress into the 21st century, the less museums will be able to ignore the proliferation 
of inherently web-based Internet art as a social phenomenon. With this new online exhibit space originally 
conceptualized by Assistant Dean of Digital Arts Mike Holcomb and the University of Arizona Museum of 
Art Director Charles Guerin, UA Museum of Art is at the forefront of its peer institutions, fostering 
Internet Art for acquisition into its permanent collection along with traditional works of art.  2 Exposition 
The ArtStream experience not only exhibits the work, but also enables the viewer to browse the gallery 
space and discover the curator s intentions in creating the exhibit. The visitor also has the option 
of perusing the biographical information on the curator and featured artist(s). All interaction with 
the Macromedia Flash ­enabled site is carried out by holding the mouse over the panels, causing them 
to flip, to reveal the content behind. The viewer is also given the opportunity to ensure that he or 
she has the needed browser and plug-in technology for maximum results. The ArtStream administrative area 
enables the chosen curator to create and modify their exhibit through Web forms, with very little Web 
knowledge needed. In creating an exhibit, the curator either chooses one artist s work to highlight or 
selects a theme and presents the work of several relevant artists. A curator s statement usually accompanies 
the exhibit, expressing the reasoning behind the organization of the exhibit. e-mail: thapgood@u.arizona.edu*e-mail: 
lucy@u.arizona.edu Professor Lucy Petrovich* Guest Curator Department of Media Arts University of Arizona 
 Charles Guerin Director University of Arizona Museum of Art Peter Briggs Chief Curator University of 
Arizona Museum of Art Shelley Tolman (Administrative) Michael Branch (Graphic Design) Student workers, 
Treistman Center The first ArtStream exhibit, curated by new media artist, educator and researcher at 
the University of Arizona Lucy Petrovich, was entitled The Process of Experience and highlighted the 
projects of three well-known Net artists: Mark Napier, Mary Flanagan and Amy Alexander. This premier 
exhibit celebrates 'The Internet' as a medium of communication within its more apparent context as a 
collection of information. The Internet properties of searching and retrieving information is at the 
heart of each of these works. The user s input retrieves information from the Internet and then utilizes 
that information to create visual constructions. Each person s interaction with the exhibited works creates 
a unique experience, making the exhibit more about process than product. The work exists only through 
the user s participation with it.  Copyright held by the author  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>965346</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Framing online art]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965333.965346</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965346</url>
		<abstract>
			<par><![CDATA[Online and offline exhibition spaces are an important mechanism by which online artworks are experienced by and contextualized for audiences. Critical attention needs to be paid to the aesthetics and pragmatics underlying these 'frames' for online art.There is a range of approaches to presentations of online art. Some seek to duplicate either the structure or the look of physical exhibits in galleries and museums. Others mimic online and offline presentational forms such as search engines or magazines. Taking into account the relativity of content and context online, some of the most adventurous exhibition spaces are effectively inseparable from the works they present, and can be considered artworks themselves.While online exhibits initially sought to duplicate the form of the physical exhibition space, today we are beginning to see the reverse. The fluid and flexible aesthetic of the online exhibit is beginning to creep into the design of physical exhibits. One such example, <i>&lt;Alt&gt;DigitalMedia</i>, is a physical gallery space of digital art at the American Museum of the Moving Image.The broad debate about whether online works are to be displayed at all in physical spaces does not take into account the inherent variability of software-based art, the sheer diversity of works and genres in existence, and the notion that distinguishing online from offline experience, physical from virtual, is becoming increasingly immaterial. More relevant are discussions around how online work can be implemented in a physical context, and how the artwork itself might have both a physical and virtual embodiment. Such discussions are most productive when each work is taken on its own terms.]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P653753</person_id>
				<author_profile_id><![CDATA[81100004599]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Carl]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Goodman]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[American Museum of the Moving Image]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Framing Online Art   Carl Goodman* American Museum of the Moving Image   Abstract Online and offline 
exhibition spaces are an important mechanism by which online artworks are experienced by and contextualized 
for audiences. Critical attention needs to be paid to the aesthetics and pragmatics underlying these 
'frames' for online art. There is a range of approaches to presentations of online art. Some seek to 
duplicate either the structure or the look of physical exhibits in galleries and museums. Others mimic 
online and offline presentational forms such as search engines or magazines. Taking into account the 
relativity of content and context online, some of the most adventurous exhibition spaces are effectively 
inseparable from the works they present, and can be considered artworks themselves. While online exhibits 
initially sought to duplicate the form of the physical exhibition space, today we are beginning to see 
the reverse. The fluid and flexible aesthetic of the online exhibit is beginning to creep into the design 
of physical exhibits. One such example, <Alt>DigitalMedia, is a physical gallery space of digital art 
at the American Museum of the Moving Image. The broad debate about whether online works are to be displayed 
at all in physical spaces does not take into account the inherent variability of software-based art, 
the sheer diversity of works and genres in existence, and the notion that distinguishing online from 
offline experience, physical from virtual, is becoming increasingly immaterial. More relevant are discussions 
around how online work can be implemented in a physical context, and how the artwork itself might have 
both a physical and virtual embodiment. Such discussions are most productive when each work is taken 
on its own terms. -------------------------------------------- *e-mail:cgoodman@movingimage.us 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	</section>
	<section>
		<section_id>965347</section_id>
		<sort_key>4</sort_key>
		<section_seq_no>4</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Rich Internet applications]]></section_title>
		<section_page_from>4</section_page_from>
		<chair_editor>
			<ch_ed>
				<person_id>P653752</person_id>
				<author_profile_id><![CDATA[81543572356]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Branden]]></first_name>
				<middle_name><![CDATA[J.]]></middle_name>
				<last_name><![CDATA[Hall]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Joshua Davis Studios]]></affiliation>
				<role><![CDATA[Session Chair]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
		</chair_editor>
	<article_rec>
		<article_id>965349</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Building communities with rich Internet applications]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965333.965349</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965349</url>
		<abstract>
			<par><![CDATA[The session presents a case study of a Web site specifically designed as a RIA. The structure of the Web site, iknowbetter.com, is based upon traditional online forums wherein a community of members shares ideas and opinions within moderated or un-moderated discussion areas. Members, however, can also utilize Web cams and microphones to add audio/video streams to text-based posts. The forum provides categories (or searches) based on content/theme, geographic location, or member name. Members can engage in live audio/video discussions, create their own private online diaries, and designate specific members to view such blog-like entries.]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP14088030</person_id>
				<author_profile_id><![CDATA[81100224886]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Robert]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Reinhardt]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The Content Project]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Building Communities with Rich Internet Applications Robert Reinhardt* The Content Project [theMAKERS] 
Abstract The session presents a case study of a Web site specifically designed as a RIA. The structure 
of the Web site, iknowbetter.com, is based upon traditional online forums wherein a community of members 
shares ideas and opinions within moderated or un-moderated discussion areas. Members, however, can also 
utilize Web cams and microphones to add audio/video streams to text-based posts. The forum provides categories 
(or searches) based on content/theme, geographic location, or member name. Members can engage in live 
audio/video discussions, create their own private online diaries, and designate specific members to view 
such blog-like entries. 1 Introduction As the Web economy struggles to recover from the dotbomb era, 
several technologies have emerged to enable advanced user interactivity with shared and dynamic data 
across networked systems. Macromedia, the software company responsible for the success of the near-ubiquitous 
Flash Player, has introduced several server technologies that allow Web designers and developers to create 
a new breed of next-generation Web applications that can connect multiple users simultaneously in live 
audio, video and text environments. Such applications are called Rich Internet Applications, or RIAs. 
With a new breed of RIAs, Web developers can begin to create multi-user environments that provide immediate 
audio/video communication. These applications stand to emphasize the human quality of network communication 
over sensory deprived text-based or avatar-based community applications. 2 Defining the Goals of a Community-based 
Experience The first portion of the presentation provides an overview of the Web site s functionality. 
While the design of community sites can vary, most of them share common patterns for data organization 
and use. The planning of the site is broken down and shown via a process chart. From user interface concerns 
to client/server implementation, the site s framework is discussed. 3 Analyzing the Usage of a RIA-based 
Community Site After an overview of the site has been presented, behavior patterns of the site s members 
will be discussed, including: Use and frequency of private versus publicly accessible posts. How often 
do members contribute to personal diaries? How many public posts does the average member create? *e-mail: 
robert@iknowbetter.com Anonymous versus declared identity. Do members disclose their identity in posts? 
 Audio/video versus text-based representation. Many Internet users prefer the absence of their physicality 
in the context of online posts and discussions. What do members of this site prefer?  Marketing and 
growth of the community. Did members encourage other contacts to participate in the community?  4 Considering 
the Potential for Cross-Device Integration The current prototype of the site is designed for use on a 
desktop computer. The impact of instant access to images, audio, and text via mobile devices with respect 
to contributions to Internet community-based sites will be discussed.  Presentation Web site iknowbetter.com 
http://www.iknowbetter.com/siggraph2003/ User: s2003, Password: anim253  About the Presenter Robert 
Reinhardt, Director of Multimedia Applications for The Content Project, is internationally regarded as 
an expert on multimedia application development, particularly in Macromedia Flash. Robert is the lead 
author of the Flash Bible series and the Flash MX ActionScript Bible (Wiley), as well as the recently 
released Macromedia MX: Building Rich Internet Applications (Macromedia Press). He has developed multimedia 
courses for educational facilities in Canada and the United States and been a featured speaker at several 
Web conferences. Recent Content Project assignments Robert has led include multimedia data analysis applications 
for Nielsen s Media and Entertainment division and creating interactive advertising for Warner Bros. 
films Dreamcatcher, Kangaroo Jack, and The Matrix: Reloaded. Copyright held by the author 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>965348</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Creating usable interfaces for online applications]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965333.965348</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965348</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P653770</person_id>
				<author_profile_id><![CDATA[81100245847]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Grant]]></first_name>
				<middle_name><![CDATA[D.]]></middle_name>
				<last_name><![CDATA[Skinner]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Creating Usable Interfaces for Online Applications Grant D. Skinner* gskinner.com 1 Introduction The 
way users interact the World Wide Web is constantly evolving. The current direction is towards more usable 
and functional sites which blur the line between desktop applicationsand web sites. This step forward 
is embodied by the term RichInternet Application, which describes an online application orutility that 
includes a level of functionality and interfacecomplexity formerly ascribed only to desktop applications. 
RIAs can be developed with a number of technologies, including Java,DHTML, and perhaps most successfully, 
Macromedia Flash. Butregardless of the technology used to develop and deploy them,RIAs all share the 
common need for a usable, effective interface to present to the end-user. 2 The three components of 
a RIA interface A typical application interface has two major components thataffect its usability layout 
and functionality. This combinationtypically necessitates the involvement of two professional groupsin 
the interface creation process, designers and developers,respectively. Due to the commercial nature of 
most onlinecontent, RIAs frequently involve a third essential component branding. This necessitates 
the involvement of a group or individual familiar with the deploying company's marketingstrategies. Too 
often, companies relegate the task of developing an onlineapplication to a team representing only one 
or two of the aforementioned competencies. Creating a successful RIA demands theinput of all three. This 
presentation will discuss approaches for meeting interfaceneeds from each of these perspectives, and 
take a look at tools andtechnologies (such as Flash, Flash Components, FlashOS2, andDHTML) for generating 
well branded, usable RIA interfacesquickly and effectively. Techniques will be discussed with reference 
to the most currentexamples of exceptional online application interface designs.  3 About FlashOS version 
2 FlashOS2 provides a modular GUI framework for Rich InternetApplication development and deployment. 
It includes elementssuch as hierarchial menus, tooltips, windows, content handlingpanes, dialogs, menubars 
and more that are accessible via a welldocumented API, and are customizable with a simple cascadingstyle 
system. In addition, it handles common application taskslike resource and settings loading, as well as 
providing a centrallibrary of useful functions. *e-mail: info@gskinner.com web: http://gskinner.com/ 
 4 About the presenter Grant's varied education and experience enables him to fuse his coding prowess 
with interface design, marketing and business logic. For the past 3 years he has been developing applications 
and games in Flash, whilst trying to apply practices and approaches from established technologies, like 
Java, to Actionscript. He has been internationally recognized for his work on gskinner.com, FlashOS2 
and gModeler. He won awards in both the Technical Merit and Application categories of the Flash Film 
Festival at FlashForward2003 San Francisco, and for Technical Excellence in the 2003 Flash In The Can 
awards, where he presented on Object Oriented Concepts for Flash Applications. Grant presently operates 
on a free-lance basis, focusing on cutting edge RIA conceptualization, development and deployment.  
Copyright held by the author  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	</section>
	<section>
		<section_id>965350</section_id>
		<sort_key>5</sort_key>
		<section_seq_no>5</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Accessibility & usability]]></section_title>
		<section_page_from>5</section_page_from>
		<chair_editor>
			<ch_ed>
				<person_id>PP42053570</person_id>
				<author_profile_id><![CDATA[81327491146]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Sandy]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ressler]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[National Institute of Standards and Technology]]></affiliation>
				<role><![CDATA[Session Chair]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
		</chair_editor>
	<article_rec>
		<article_id>965351</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Making accessible Web graphics]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965333.965351</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965351</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP28014692</person_id>
				<author_profile_id><![CDATA[81100097381]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Dean]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Jackson]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[World Wide Web Consortium (W3C)]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Making Accessible Web Graphics Dean Jackson dean@w3.org World Wide Web Consortium (W3C) http://www.w3.org/ 
 The Web is a rich interactive environment it is full of text, images, audio, video and animations. 
It's also designed to be anenvironment for everybody, regardless of age, physical,intellectual and cognitive 
disabilities. This fundamental aspect ofthe web is referred to as "Accessibility". Many people think 
that accessibility simply means making webcontent available to people with disabilities, such as blind 
users.However, this is a limited view of the area. Accessibility meansmaking Web content and the Web 
experience available to as manypeople as possible, whether they have a disability such as a vision, motor 
or learning impairment, users with slow network connections and slow computers that choose "text-only" 
pages,users accessing the Web with mobile phones or PDAs with limited bandwidth, memory and screen resolution 
or softwareagents that do not "see" the web page in the same way as a human. Can Web Graphics be accessible? 
 Definitely. The vast majority of Web developers want theircontent to be available to as wide an audience 
as possible. More accessible graphical content, including raster images (JPEG, GIF,PNG), video (QuickTime, 
AVI, MPEG), vector content (PDF,SVG) and animations (Flash, SVG), is possible with a few simpletechniques. 
Some web designers think making graphics accessible requiresgenerating equivalent text-only pages or 
creating visually unappealing images. However, making the content more accessible does not lessen the 
experience for the users that do notneed accessible content, and greatly enhances the experience forthe 
users that do. Why should Web Graphics be made accessible? Many countries have passed legislation that 
requires Web sites tobe accessible. In most cases, the legislation applies to all thecontent on the site, 
including the graphics, animations and video. Beyond the legal reasons, there are many practical reasons. 
Accessible Web Graphics means that a greater portion of thepotential Web audience can access the Web 
site. Examples areblind users that can skip introduction animations and sendgraphical electronic greeting 
cards to friends. Also, designing foraccessibility typically produces a more usable Web site. You can 
also consider a search engine, such a Google, to be aparticular class of user that can benefit from accessible 
content.Most search engines primarily index textual content they cannoteasily read text within an image 
or understand the audio stream ofa video. If the graphics on a site are more accessible then searchengines 
can use the accessible content to direct users to the site. How can Web Graphics be made accessible? 
Unfortunately many of the Web Content Accessibility tips andguidelines were originally authored towards 
textual content. Manyaspects are still relevant to graphics, and there are a number ofsimple techniques 
for particular formats that can greatly enhanceaccessibility. Examples include: The most common use 
of graphics on the web is images thatare included inline within an HTML page. Most contentdevelopers 
know that the HTML image tag has a mandatoryattribute that provides a textual description of the image 
(alt­text). It's also possible to provide titles and to refer to anexternal file that has a more detailed 
description of the image  Many animation formats, such as Flash, allow the user toprovide text descriptions 
of some graphical content. Otheranimation formats, such as SVG, were designed withaccessibility in mind, 
and are therefore accessible by nature.Tools can extract the accessible content from Flash and SVG and 
display this content to the user.  Structuring graphical content provides the user with increased navigation 
and understanding. For example,defining a library of shapes and reusing the defined shapes inthe graphical 
content means a user or tool can more easilydetermine the semantics. Also, it is possible to structuretextual 
content within graphics formats so that users are ableto determine which parts of the text are more important. 
 Many video streams have embedded audio tracks. There aremany ways to make the audio accessible, captioning 
beingone of them. Some video formats allow embedding of acaptioned stream, allowing user agents to show 
the captions along with the video.  Some users have no problems seeing complex animations,but have problems 
interacting with the content (a mobilityimpairment for example). If an animation requires the user tointeract 
within a certain timeframe, then there should be a way to pause the animation in order to allow the user 
tointeract at their own pace. Also, movement confuses someusers. It should be possible to avoid (or provide 
a way toavoid) unnecessary animations.  Does accessibility reduce the impact of WebGraphics to typical 
users? No. By following the simple techniques, the typical user can stillhave the same experience as 
before. It is not necessary to removeanimations, or provide text-only sites. Graphics can be just asaccessible 
as text content. Interestingly, the effort required improving the accessibility ofMaking Web Graphics 
more accessible requires a small amount of Web Graphics is less than many would expect. With only a smalleffort 
and provides a large gain. Not only is it a legal requirementnumber of simple techniques, there is a 
large return in accessible in many countries, it also widens the possible audience of a sitecontent. 
and increases the usability. Access for all is a fundamental aspect of the Web. Copyright held by the 
author 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>965352</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[An investigation of best practices for interactive content controls]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965333.965352</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965352</url>
		<abstract>
			<par><![CDATA[This contribution presents an experimental design that seeks to answer the question of which type of interactive control works best for interactive, map-based content on the Web. The study used a matched pair's t-test and found a significant difference between a joystick/trackball-oriented interactive control versus two different control mechanisms that were based on multi-directional buttons. While the content of the study focused on map-based content, the findings have applicability to other content forms. Additionally, the quantitative approach used in this study provides a framework for work in the fields of usability and interface design as it relates to computer graphics.]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP42051043</person_id>
				<author_profile_id><![CDATA[81341494013]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[James]]></first_name>
				<middle_name><![CDATA[L.]]></middle_name>
				<last_name><![CDATA[Mohler]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Purdue University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP43125720</person_id>
				<author_profile_id><![CDATA[81339510299]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Nishant]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kothary]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Purdue University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P653792</person_id>
				<author_profile_id><![CDATA[81100072571]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Ronald]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Glotzbach]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Purdue University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>517069</ref_obj_id>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[CATO, J. 2001. User-centered web design. Boston: Addison-Wesley Professional.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>290178</ref_obj_id>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[FLEMING, J. (1998). Web Navigation: Designing the User Experience. Sebastopol, CA: O'Riley & Associated, Inc.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>272962</ref_obj_id>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[HACKOS, J., & REDISH, J. 1998. User and Task Analysis for Interface Design. New York: John Wiley & Sons.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[NEILSON, J. 1994. Usability Inspection Methods. New York: John Wiley & Sons.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 An Investigation of Best Practices for Interactive Content Controls James L. Mohler* Nishant Kothary 
Ronald Glotzbach Envision Center for Perceptualization Purdue University Purdue University Purdue University 
 Abstract This contribution presents an experimental design that seeks to answer the question of which 
type of interactive control works best for interactive, map-based content on the Web. The study used 
a matched pair s t-test and found a significant difference between a joystick/trackball-oriented interactive 
control versus two different control mechanisms that were based on multi­directional buttons. While the 
content of the study focused on map-based content, the findings have applicability to other content forms. 
Additionally, the quantitative approach used in this study provides a framework for work in the fields 
of usability and interface design as it relates to computer graphics. 1 Introduction As interactive content 
on the Web continues to grow, many are striving to use various techniques to ensure the quality of the 
interactive content, and thus, the user experience. Neilson (1994) has provided various Likert type scales 
to rate the overall quality of the web experience pertaining to usability. Many others have also focused 
upon the topic (Cato, 2001; Fleming, 1998; Hackos &#38; Redish, 1998). Yet, such scales are often subjective 
in nature, being dependent on the reliability of the rater who is using the instrument. Additionally, 
they are more global in nature, focusing on the big picture issues rather than the specifics of any individual 
element or control. As a plausible alternative, it is suggested that quantitative studies can be used 
in many instances to improve specific parts of online content, particularly when the content does not 
fit into the traditional page metaphor. This paper describes one such study in which the goal was to 
determine the best type of control to use for panning or scrolling a map. 2 Method This study presented 
each user with three sequential tests in which he or she was required to scroll across a map and click 
on four spatially-separated, clickable items. The tests were given in a random order to avoid test bias. 
The clickable items on each test were located in different, but equidistant locations on the map and 
the starting locations for the map began in the same location on each test. The time required to scroll 
and click on the four items was recorded and then a mean for each test was computed across the four items. 
These means where used to determine the differences in total time and thus the potential effectiveness 
of each type of control. The participants of this study were student volunteers (n = 38) from those enrolled 
in an introductory Web class at Purdue University.  3 Control Types One method of scrolling was based 
on a trackball design as shown in Figure 1a. The second was based on a floating palette of directional 
arrows (Figure 1b) and the third was based on controls *e-mail: jlmohler@purdue.edu e-mail: kothary@purdue.edu 
e-mail: rjglotzbach@tech.purdue.edu that were integrated around the perimeter of the map window (Figure 
1c). Upon answering some demographic questions, the participants were instructed that they had to search 
for clickable items within a map using three different sets of interactive controls. No instruction was 
given pertaining how to use each set of controls. Table 1 presents the overall 5 number summaries for 
each test with units in seconds. Table 1 Five Number Summaries Test 1 Test 2 Test 3 Max 38.447 46.775 
49.956 Q3 29.501 30.933 31.080 Md 21.813 25.535 27.246 Q1 16.728 20.939 23.249 Min 14.564 16.234 18.509 
4 Results Results of the matched pair s comparisons indicated that the trackball control yielded the 
shortest response times. When compared with the floating arrow palette, it yielded a significant difference 
of 3.42 seconds (p = .0308). When compared to the border navigation, it was 5.279 seconds faster and 
was significantly different (p = .0017). A comparison of the border navigation versus the floating arrow 
palette yielded a 1.8 second difference (favoring the arrow palette), but the result was not significant 
(p = .2617). Figure 1. Test Control Mechanisms References CATO, J. 2001. User-centered web design. Boston: 
Addison-Wesley Professional. FLEMING, J. (1998). Web Navigation: Designing the User Experience. Sebastopol, 
CA: O Riley &#38; Associated, Inc. HACKOS, J., &#38; REDISH, J. 1998. User and Task Analysis for Interface 
Design. New York: John Wiley &#38; Sons. NEILSON, J. 1994. Usability Inspection Methods. New York: John 
Wiley &#38; Sons. Copyright held by the author  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	</section>
	<section>
		<section_id>965353</section_id>
		<sort_key>6</sort_key>
		<section_seq_no>6</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Design patterns in Flash]]></section_title>
		<section_page_from>6</section_page_from>
		<chair_editor>
			<ch_ed>
				<person_id>PP14088030</person_id>
				<author_profile_id><![CDATA[81100224886]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Robert]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Reinhardt]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[[theMAKERS]]]></affiliation>
				<role><![CDATA[Session Chair]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
		</chair_editor>
	<article_rec>
		<article_id>965354</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Usability & human behavior analysis through real-time performance data]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965333.965354</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965354</url>
		<abstract>
			<par><![CDATA[Current usability testing trends involve collection of data that often lacks objectivity in the context of a testing sample (Dumas & Redish, 1999). Usability and heuristic analysis are performed on samples that are often statistically unacceptable as they are characterized by subject opinions and moods. In addition, usability studies and statistical analyses are typically accompanied by the substantial overhead of time constraints and data-collection issues. Through this Web statistical data collection application, we introduce the concept of performance data that can be collected in real-time. The Flash.NET application presented herein captures user information in real-time and sends the information to an ASP.NET application residing on the server through Flash Remoting MX, at which point the data is stored into SQL Server until the ASP.NET application is prepared to perform statistical analysis on the acquired data. The information produced from this study not only provides a framework for future usability analyses, but also a framework for predictors of human behavior in the field of computer graphics.]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P653793</person_id>
				<author_profile_id><![CDATA[81100072571]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Ronald]]></first_name>
				<middle_name><![CDATA[J.]]></middle_name>
				<last_name><![CDATA[Glotzbach]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Purdue University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP43119736</person_id>
				<author_profile_id><![CDATA[81339510299]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Nishant]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kothary]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Purdue University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>557936</ref_obj_id>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[DEITEL, H. M., DEITEL, P. J., & STEINBUHLER, K. 2001. e-Business & e-Commerce for Managers. Upper Saddle River, NJ: Prentice-Hall, Inc.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>600280</ref_obj_id>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[DUMAS, J., & REDISH, J. 1999. A Practical Guide to Usability Testing. Bristol, England: Intellect.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>601307</ref_obj_id>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[HALL, B., & WAN, S. 2002. Object-Oriented Programming with ActionScript. Indianapolis, IN: NewRiders.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[TRUCHARD, A. & KATZ-HAAS, R. 1998. Ten guidelines for user-centered web design. Usability Interface, 5(1).]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Usability &#38; Human Behavior Analysis through Real-Time Performance Data Ronald J. Glotzbach* Nishant 
Kothary Purdue University Purdue University Abstract Current usability testing trends involve collection 
of data that often lacks objectivity in the context of a testing sample (Dumas &#38; Redish, 1999). Usability 
and heuristic analysis are performed on samples that are often statistically unacceptable as they are 
characterized by subject opinions and moods. In addition, usability studies and statistical analyses 
are typically accompanied by the substantial overhead of time constraints and data-collection issues. 
Through this Web statistical data collection application, we introduce the concept of performance data 
that can be collected in real-time. The Flash.NET application presented herein captures user information 
in real-time and sends the information to an ASP.NET application residing on the server through Flash 
Remoting MX, at which point the data is stored into SQL Server until the ASP.NET application is prepared 
to perform statistical analysis on the acquired data. The information produced from this study not only 
provides a framework for future usability analyses, but also a framework for predictors of human behavior 
in the field of computer graphics. 1 Introduction The level of interactivity on the Web has exploded 
in recent years, putting web developers in a position where usability has become a primary concern. Collection 
of data through web-based applications is making it increasingly possible to run usability studies on 
truncated timelines and in some cases, as this contribution explains, gain immediate feedback. It is 
through the analysis of these data that an even more important subject emerges, human behavior prediction. 
 2 Application The application uses a hybrid design pattern that borrows from both, Model-View-Controller, 
and Model-View-Presenter. Due to the stringent nature of these patterns, the completed application is 
more flexible and customizable. The MVP Pattern allows for distributing each component of the application 
over several classes and locations (Hall &#38; Wan, 2002). Data structures may be created on the client-side 
to reduce the number of trips to the server-side model. Typically, there is not much need for maintaining 
a live connection with the database on the server. Figure 1. Entity Relationship Diagram *e-mail: rjglotzbach@tech.purdue.edu 
e-mail: kothary@purdue.edu app: http://www.rjglotzbach.com/siggraph03/index.html Figure 1 is a representation 
of the Entity Relationship Diagram which describes the database and the relationships formed to handle 
running and tracking the experiment(s), tests, test items, users, and storage of results including statistical 
data. The Flash MX application proceeds through a series of three tests, the ordinal of which is randomly 
assigned for each. Usability considerations are focused on different navigational schemes, considered 
a major guideline for user-centered web design (Truchard, &#38; Katz-Haas, 1998), as shown in Figure 
2. As the user manipulates the map, actions are recorded for later analysis. In this particular experiment, 
usability is being assessed between a trackball design (Figure 2a), floating palette (Figure 2b), and 
border navigation (Figure 2c). Figure 2. Navigation Mechanisms 3 Human Behaviors From a design standpoint, 
the database can capture multiple experiments, unlimited users, and unlimited tests. Furthermore, the 
items can be reused in various tests, allowing for slight modifications from test 1 to test 2 in order 
to discover something as simple as the preferred path from the north edge to the southeast corner (diagonal 
navigation vs. over and down). This can lead to statistical analysis of human behavior, more so than 
just a usability study. The results could also lead to the improvement of current accessibility issues 
as addressed by Deitel, Deitel, &#38; Steinbuhler (2001). 4 Conclusions and Future Work The current 
application and framework presented herein allow for cost-efficient, reliable, and timely creation and 
implementation of usability studies. This framework can be extended to accommodate other usability tests, 
as well as customized to deliver not only performance data, but also real-time reaction data such as 
path followed by subject, etc.  References DEITEL, H.M., DEITEL, P.J., &#38; STEINBUHLER, K. 2001. 
e-Business &#38; e-Commerce for Managers. Upper Saddle River, NJ: Prentice-Hall, Inc. DUMAS, J., &#38; 
REDISH, J. 1999. A Practical Guide to Usability Testing. Bristol, England: Intellect. HALL, B., &#38; 
WAN, S. 2002. Object-Oriented Programming with ActionScript. Indianapolis, IN: NewRiders. TRUCHARD, A. 
&#38; KATZ-HAAS, R. 1998. Ten guidelines for user­centered web design. Usability Interface, 5(1). Copyright 
held by the author 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	</section>
	<section>
		<section_id>965355</section_id>
		<sort_key>7</sort_key>
		<section_seq_no>7</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Java applications]]></section_title>
		<section_page_from>7</section_page_from>
		<chair_editor>
			<ch_ed>
				<person_id>PP14033296</person_id>
				<author_profile_id><![CDATA[81543728956]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Alan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Norton]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Colorado School of Mines]]></affiliation>
				<role><![CDATA[Session Chair]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
		</chair_editor>
	<article_rec>
		<article_id>965357</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[A haptic interface for the explorable virtual human]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965333.965357</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965357</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P653764</person_id>
				<author_profile_id><![CDATA[81100003949]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Elizabeth]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Prince]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Colorado School of Mines]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P653776</person_id>
				<author_profile_id><![CDATA[81100571121]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Karl]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Reinig]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Colorado]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP309738400</person_id>
				<author_profile_id><![CDATA[81481647951]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[David]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Rubinstein]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Colorado]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P15780</person_id>
				<author_profile_id><![CDATA[81100334998]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Alyn]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Rockwood]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Colorado School of Mines]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[HIKICHI, K., ARIMOTO, I., MORINO, H., SEZAKI, K., and YASUDA Y., 2002. Evaluation of Adaptation Control for Haptics Collaboration over the Internet. In Proc. of IEEE Communications Quality & Reliability (CQR) International Workshop.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[PETERSON, R., 2002. Scientists "Touch" via the Internet. In Reuters, October 30.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>258878</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[RUSPINI, D. C., KOLAROV, K., and KHATIB, O., 1997. The Haptic Display of Complex Graphical Environments. In Proceedings of SIGGRAPH 97, ACM Press / ACM SIGGRAPH/ Addison-Wesley Publishing Co, Computer Graphics Proceedings, Annual Conference Series, ACM, 345--352.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 A Haptic Interface for the Explorable Virtual Human Elizabeth Prince  Karl Reinig Colorado School 
of Mines  Center for Human Simulation Center for Human Simulation  University of Colorado 1 Introduction 
Haptic technology enhances a graphic user interface by allowing the user to interact in three dimensions 
with models on the screen, with force feedback giving the perception of touch. Recently several groups 
developed Internet based haptic systems that allow users in different parts of the world to interact 
with the same models simultaneously [Hikichi et al. 2002; Peterson 2002]. Although this system operates 
in real time, delay jitter caused by time required to transfer data inhibits the ability to apply realistic 
qualities to the individual models, as the lag in time is compensated by increasing the apparent weight 
of the model. Therefore, those users with greater lag times perceive the models as heavier than those 
with smaller lags [Hikichi et al. 2002]. While the required refresh rate of 1kHz suggests at least a 
portion of the interaction must be accomplished locally even on these other systems, transform updates 
must be handled at a server level. The haptic interface for the Explorable Virtual Human (EVH) we present 
allows the user to interact with various models taken from the Visual Human data set and the 100 micron 
knee. Our approach implements the PHANToM haptic device and can either incorporate single point collision 
as determined by the General Haptic Open Software Toolkit (GHOST®) SDK or calculate collision and force 
feedback information based on the shape and size of the haptic cursor and various properties of the models 
being viewed. In addition, collision detection allows deformations for those models representing soft 
tissues. Because the interaction occurs in real time, both the graphics and haptics run on the local 
computer while the browser enables the large data set to be managed by the server.  2 The Haptic Interface 
for the EVH The EVH was designed to provide teachers, researchers, and students a tool with which to 
communicate complex anatomical concepts. Because the EVH was designed as a teaching tool reather than 
as a haptic model, and we understood only a select group own haptic devices, the EVH functions in the 
absence of a haptic environment so that a user would not know any further functionality is available. 
Manipulation of the models can be managed either using the PHANToM device or using a simple mouse. These 
updates are recognized both by the Java side and the haptic side. In this way, the haptics, while not 
necessary, adds to the power of the tool. Initially we ran the haptics through the Java Native Interface 
(JNI) as a dynamically linked library, but we found that this system did not take advantage of our resources. 
Although the models could be loaded and touched successfully, model manipu­lation caused either a noticeable 
lag in the graphics or a lag in the refresh rate of the PHANToM, causing a timeout error. This was overcome 
by running the haptics as an independent process and communicating through sockets, as shown in Fig. 
1 and Fig. 2. One interface of the EVH allows users to choose a set of models for display, providing 
the set for both graphic and haptic rendering. Once the models have been loaded by the Java browser, 
the vertices and indices are passed to the haptic process to build the haptic models. We have implemented 
two methods for stroing the models on the haptic side. First, included in the David Rubinstein  Alyn 
Rockwood Center for Human Simulation  Colorado School of Mines  University of Colorado GHOST® SDK 
are structures which create a haptic model from vertices and triangle polygonal meshes. While simpler 
to implement, this method restricts contact between the model and the haptic cursor to a single point. 
Alternatively, we have built models using a variation of an axis aligned bounding box (AABB) tree and 
put the leaves into a matrix for fast access for the collision detection. The simulations we are working 
to implement require collision detection at more than a single point. In addition, by calculating the 
point of collision, interesting features such as deformations and textures can be added to the models 
to enhance the authenticity of the interactions. However, this method greatly increases the model loading 
time.  Figure 1. By pressing the stylus button and moving the stylus while touching a model, the user 
moves models individually. Figure 2. By pressing the stylus button and moving the stylus when no model 
is touched, the user moves the whole scene. 3 Conclusion While various other applications have been 
implemented to allow remote haptic interaction, not all applications are suitable to the delay in response 
caused by sending packets across the network. A local haptic interface for a Java browser allows access 
to large data sets stored remotely while allowing immediate response to haptic manipulation as well as 
providing convenient means for distribution.  References HIKICHI, K., ARIMOTO, I., MORINO, H., SEZAKI, 
K., and YASUDA Y., 2002. Evaluation of Adaptation Control for Haptics Collab­oration over the Internet. 
In Proc. of IEEE Communications Quality &#38; Reliability (CQR) International Workshop. PETERSON, R., 
2002. Scientists Touch via the Internet. In Reuters, October 30. RUSPINI, D.C., KOLAROV, K., and KHATIB, 
O., 1997. The Haptic Display of Complex Graphical Environments. In Proceedings of SIGGRAPH 97, ACM Press 
/ ACM SIGGRAPH/ Addison-Wesley Publishing Co, Computer Graphics Proceedings, Annual Conference Series, 
ACM, 345-352. Copyright held by the author 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>965358</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Collaborative online 3D editing]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965333.965358</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965358</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P556115</person_id>
				<author_profile_id><![CDATA[81100097986]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Irene]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Cheng]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[TelePhotogenics, Canada]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P557196</person_id>
				<author_profile_id><![CDATA[81100271503]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Matt]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bates]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[TelePhotogenics, Canada]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP15038381</person_id>
				<author_profile_id><![CDATA[81100646309]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Anup]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Basu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Alberta]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA["OneSpace", CoCreate, <u>www.cocreate.com</u>, 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA["SolidWorks", SolidWorks, <u>www.solidworks.com</u>, 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA["eDrawings", SolidWorks, solidworks.com/edrawings, 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA["SolidWorks Viewer", SolidWorks, <u>www.solidworks.com/swdocs</u>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA["3D Instant Website", SolidWorks, solidworks.com/3dinstantwebsite.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[M. Zwicker et al. "Pointshop 3D", pp 322--329, SIGGRAPH 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Collaborative Online 3D Editing Irene Cheng and Matt Bates, TelePhotogenics, Canada, research@telephotogenics.com, 
www.Zoomage3D.com Anup Basu, Dept. of CS, University of Alberta 1. Introduction and related work Figure 
1: (from left to right) Zoomage3D scanning device, a 1 ft tall nut-cracker toy and a 18,720 polygon mesh 
of the toy. In this abstract we describe software that multiple users can use over the Internet to interactively 
and simultaneously edit 3D objects. Collaborative 3D editing can be useful for correcting inaccurately 
scanned vertices, such as with a 3D scanner that can be used to create texture and mesh (Figure 1). This 
technique can also be used by digital artists and graphic designers to interactively create new 3D content 
starting with existing models. There are other collaborative 3D products, but they are designed for different 
applications. CoCreate s OneSpace [1] is a software package that provides a way to have online meetings 
that discuss a 3D object. This software includes meeting management features like meeting planning, and 
automatic generation of meeting reports. It does allow editing of the object being discussed. However, 
the OneSpace meeting is not web-based. SolidWorks "eDrawings"[3], and "SolidWorks Viewer"[4] are 3D viewers, 
but they do not allow manipulation of the shape of the object. "3D Instant Website"[5] creates a website 
to display a particular object. The generated site has the standard 3D display tools (zoom, rotate etc.), 
but again does not allow editing of the object. The Pointshop 3D software [6] proposes a set of point-based 
surface editing tools; however, this work cannot be used in an online collaborative environment. In contrast, 
the Zoomage Online Collaborative Editor (ZOCE) is a small web-based applet that not only allows a client 
to view a 3D object, but also allows multiple clients to simultaneously edit the same 3D object via a 
Java applet. While CoCreate [1] and SolidWorks [2] use AutoCAD type objects, ZOCE maintains range data 
captured from actual objects, preserving both realistic geometric and texture information on an object. 
 2. Description of technology In order to allow flexibility, either texture maps or mesh representations 
can be edited at a client site. To assist selection of a direction in which to move a vertex, an Edit 
Bar is available. This bar is pivoted at the selected vertex and can be rotated around this vertex. To 
change the location of a vertex a user simply orients the bar to the desired position then clicks on 
the bar at the new vertex location. Figures 2 and 3 show different stages of the editing process. Clients 
can choose either a texture or mesh view. Modified data is instantaneously updated at the server. Other 
clients can see the revised model within seconds. At present, we maintain a central data repository and 
allow an unlimited number of clients to edit an object simultaneously. 3D objects of standard formats 
can be added to the central database. A demonstration set up for the SIGGRAPH 2003 presentation can be 
found at: www.zoomage3d.com/Siggraph.htm. Figure 2: Start of editing: Client 1 (left) wants to modify 
shoulder, while Client 2 (right) wants to modify head. Figure 3: Modifications on client sites during 
the editing process. Note that changes made at one site are reflected at all sites.  3. Future work 
The preliminary implementation allows only a single vertex to be edited by a user at any point in time. 
In the next stage we will allow the user to define a local region that can be collectively modified by 
the editing process. Region-based editing will require the identification of faces that need to be locked 
during modification Figure 4: Faces identified as intersecting a spline curve being used for smoothing 
boundaries of a 3D scan. 4. References [1] OneSpace , CoCreate, www.cocreate.com , 2002. [2] SolidWorks 
, SolidWorks, www.solidworks.com , 2002. [3] eDrawings , SolidWorks, solidworks.com/edrawings, 2002. 
[4] SolidWorks Viewer , SolidWorks, www.solidworks.com/swdocs. [5] 3D Instant Website , SolidWorks, solidworks.com/3dinstantwebsite. 
[6] M. Zwicker et al. Pointshop 3D , pp 322-329, SIGGRAPH 2002. Copyright held by the author  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>965356</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Processing]]></title>
		<subtitle><![CDATA[a learning environment for creating interactive Web graphics]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965333.965356</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965356</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP37027875</person_id>
				<author_profile_id><![CDATA[81100463130]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Casey]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Reas]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Interaction Design Institute Ivrea, Italy]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP38026245</person_id>
				<author_profile_id><![CDATA[81100613376]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Benjamin]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Fry]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MIT Media Laboratory]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Processing: A Learning Environment for Creating Interactive Web Graphics Casey Reas Interaction Design 
Institute Ivrea, Italy c.reas@interaction-ivrea.it Introduction The Processing project introduces a 
new audience to computer programming and encourages an audience of hybrid artist/ designer/programmers. 
It integrates a programming language, development environment, and teaching methodology into a unified 
structure for learning. Its goal is to introduce program­ming in the context of electronic art and to 
open electronic art concepts to a programming audience. Unlike other popular web programming environments 
such as Flash and Director, Pro­cessing is an extension of Java and supports many of the exist­ing Java 
structures, but with a simplified syntax. The application runs locally and exports programs to Java applets, 
which may be viewed over the Internet. It is not a commercial production tool, but is build specifically 
for learning and prototyping. Concept Graphical user interfaces became mainstream nearly twenty years 
ago, but programming fundamentals are still primarily taught through the command line interface. Classes 
proceed from outputting text to the screen, to GUI, to computer graph­ics (if at all). It is possible 
to teach programming in a way that moves graphics and concepts of interaction closer to the surface. 
Making exercises created during learning viewable over the web supports the creation of a global educational 
community and provides motivation for learning. A view source method of programming enables the community 
to learn from each other. The concept of Processing is to create a text programming language specifically 
for making responsive images, rather than creating a visual programming language. The language enables 
sophisticated visual and responsive structures and has a bal­ance between features and ease of use. Many 
computer graphics and interaction techniques can be discussed including vector/ raster drawing, 2D/3D 
transformations, image processing, color models, events, network communication, information visualiza­tion, 
etc. Processing shifts the focus of programming away from technical details like threading and double-buffering 
and places emphasis on communication. Figure 1: Example images created with Processing.  Copyright 
held by the author Benjamin Fry MIT Media Laboratory fry@media.mit.edu  Programming Language/Environment 
Processing is a Java environment which translates programs written in its own syntax into Java code and 
then compiles to executable Java Applet 1.1 byte code. It includes a custom 2D/3D engine inspired by 
PostScript and OpenGL. The software is free to use and the source code will be made public. It runs on 
Windows, Mac OS X, Mac OS 9, and Linux and the software is currently in Alpha release. The Beta release 
is scheduled for Summer 2003. Processing Version 1.0 focuses on teaching basic concepts of interactive 
networked computer graphics. Processing provides three different modes of program­ming each one more 
structurally complex than the previous. In the most basic mode, programs are single line commands for 
drawing primitive shapes to the screen. In the most complex mode, Java code may be written within the 
environment. The intermediate mode allows for the creation of dynamic software in a hybrid procedural/object-oriented 
structure. It strives to achieve a balance between features and clarity, which encourages the experimentation 
process and reduces the learning curve. Skills learned through Processing enable people to learn lan­guages 
suitable for different contexts including web authoring (ActionScript), networking and communications 
(Java), micro­controllers (C), and computer graphics (OpenGL). Networked Learning The Processing website 
houses a set of extended examples and a complete reference for the language. Hundreds of students, educators, 
and practitioners across five continents are involved in using the software. An active online discussion 
board is a platform for discussing individual programs and future soft­ware additions to the project. 
The software has been used at diverse universities and institutions in cities including: Boston, New 
York, San Fransisco, London, Paris, Oslo, Basel, Brussels, Berlin, Bogota (Colombia), Ivrea (Italy), 
Manila, and Tokyo. See also: http://www.proce55ing.net  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	</section>
	<section>
		<section_id>965359</section_id>
		<sort_key>8</sort_key>
		<section_seq_no>8</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Web production creativity]]></section_title>
		<section_page_from>8</section_page_from>
		<chair_editor>
			<ch_ed>
				<person_id>PP42057872</person_id>
				<author_profile_id><![CDATA[81341487525]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Simon]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Allardice]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[United Publishing]]></affiliation>
				<role><![CDATA[Session Chair]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
		</chair_editor>
	<article_rec>
		<article_id>965360</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Flash finally hits the big screen]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965333.965360</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965360</url>
		<abstract>
			<par><![CDATA[Flash will play an integral role in the future of 2-D animation. Already the mainstay of animation for the web, Flash has slowly been flexing its production muscles on television. Studios such as Warner Bros., Comedy Central, and Cartoon Network have just given the green light to several flash productions. As flash creeps onto the small screen, artists and executives are starting to find a common middle ground to get flash feature film productions off the ground, the bottom line. Utilizing proper asset management and creative multitasking, television and feature animation work done in Flash can drive down the cost of animation to mere fractions of its traditional counterpart. The key to deriving such low costs comes from understanding how to economize the artwork, a technique that heralds back to optimizing flash animation for web delivery.]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP28017152</person_id>
				<author_profile_id><![CDATA[81100570853]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Sandro]]></first_name>
				<middle_name><![CDATA[M.]]></middle_name>
				<last_name><![CDATA[Corsaro]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Flash Finally Hits the Big Screen Sandro M. Corsaro* Abstract Flash will play an integral role in the 
future of 2-D animation. Already the mainstay of animation for the web, Flash has slowly been flexing 
its production muscles on television. Studios such as Warner Bros., Comedy Central, and Cartoon Network 
have just given the green light to several flash productions. As flash creeps onto the small screen, 
artists and executives are starting to find a common middle ground to get flash feature film productions 
off the ground, the bottom line. Utilizing proper asset management and creative multitasking, television 
and feature animation work done in Flash can drive down the cost of animation to mere fractions of its 
traditional counterpart. The key to deriving such low costs comes from understanding how to economize 
the artwork, a technique that heralds back to optimizing flash animation for web delivery. 1 Introduction 
There are numerous benefits of using programs such as Flash MX from a production standpoint in a variety 
of entertainment formats. Quantity: Can you get a bargain producing for the web, TV, and feature film 
all at once?  Cost: Price comparisons of traditional 2-D models from Television and Feature Films. 
 Time: How does using Flash affect a timetable? What about producing a TV show and movie right after 
one another?  Quality: Because Flash is so economical, will the quality of animation suffer?  The traditional 
production system of animation can be starkly contrasted with the time saving efficient workflow digital 
animation can provide to studios. The comparisons of animating with a web strategy versus a broadcast 
strategy in Flash are quite similar. Flash MX can create a very sophisticated version of limited animation. 
 2 The Evolution of Scooby Doo In 1957, the last of the big studios closed the doors of its television 
animation division. The cost of a six-minute Tom and Jerry short was escalating at over $50,000. MGM 
had no choice but to follow Disney and Warner s lead. By 1960 most animators in Hollywood found themselves 
out of work. It would be two men named William Hanna and Joseph Barbera that would not only save the 
genre but also reinvent it. The two men invented the limited animation system. The system was based on 
reusing animation, strong staging, and simple gags. It would be fifty years later the same principles 
would become the principles of web animation. By economizing the artwork Hanna Barbera *e-mail: info@sandrocorsaro.com 
dramatically drove down the cost of production and were able to bring hundreds of artists back to their 
drawing boards. It is an amazing story. What was even more amazing was my encounter with Scooby Doo creator, 
Iwao Takamoto one year ago. The very principles that drive the Flash animation system on the web are 
the ones that made Fred Flintstone a household name. When I showed him MX he got it right away, after 
all it s really a system he helped pioneer.  3 The Future of 2-D Production With studios clamoring for 
drastic budget cuts in animation in both Television and Feature work, animation is at a crossroad once 
again. Studios will be relying on programs such as Flash to help relieve this burden. Almost all animated 
television is now done of overseas much to the dismay of many Hollywood animators. In the 1980s animation 
costs were quite high, causing many studios to look toward Asia for cheaper labor. The executives got 
their wish. Now dozens of Korean studios are in full swing pumping out hundreds of shows a year. Flash 
MX has the potential to bring production back to the United States because it offers a cheaper and much 
faster way to create animation. Currently, no Korean studios are successfully utilizing Flash to its 
potential. The flash production process of involves an intangible you can not teach, creativity. The 
essence of the flash creative process hinges on each component of the workflow: story, reuse, optimization, 
animation, and character designs. Throughout the last year, I have worked on several of these innovative 
domestic projects, including Sony Pictures soon to be released Flash feature   About the Presenter 
Specializing in pure Flash animation, Sandro Corsaro has created projects for clients such as Intel, 
McDonalds, MCA Records, and Nestle. Based in Los Angeles, Sandro has worked in creative development for 
Warner Bros. Television, Sony Pictures, and Revolution Studios. He has contributed to such animated feature 
films such as The Iron Giant, Osmosis Jones, and Lil Pimp. Among Sandro s latest projects has been a 
book for New Riders Publishing entitled, The Flash Animator. He shares his distinctive techniques for 
applying traditional animation principles in the Flash MX environment. This past year Sandro has presented 
at conventions such as Siggraph, Flash Kit, Flash Forward, and The Atlantic Film Festival. A big kid 
himself, Sandro has always enjoyed working with children and fostering their desire to learn both in 
and outside of the classroom. Utilizing his artistic talents he co-founded Teachtoons.com, an interactive 
educational site for elementary school children. Copyright held by the author 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>965361</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[The art of flow]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965333.965361</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965361</url>
		<abstract>
			<par><![CDATA[I mostly focus on issues of finding forms in math and nature -systems and boundaries and using this math structure to write vector based programs or what I call "machines", to build randomly constructed algorithms and behaviors.]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP14208696</person_id>
				<author_profile_id><![CDATA[81100603647]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Joshua]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Davis]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Joshua Davis Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 The Art of Flow Joshua Davis Joshua Davis Studios Abstract I mostly focus on issues of finding forms 
in math and nature ­systems and boundaries and using this math structure to write vector based programs 
or what I call "machines", to build randomly constructed algorithms and behaviors. 1 Living with Machines 
The art form is not in the few days it takes to write the program -­but the art form is the few weeks 
I spend "living" with the work, watching it evolve, waiting to capture the moment in time, the beautiful 
accident. Much like hiking in the mountains, finding an ideal natural composition, and sitting there 
for 365 days recording sunrises, sunsets, snow, rain, death, rebirth, etc. Since with technology I don't 
have to sit 365 days with my work. I can simulate 365 days of evolution in about a few weeks. 2 Input 
and Output I once read in some essays by Jackson Pollock that he considered himself a painter, though 
at times the brush never hit the canvas. Similarly, I would consider myself a traditional artist, though 
I have no control over what my programs ("machines") output. I program the paints, the brushes, the canvas, 
the strokes, the rules and boundaries. However, it is the machine that outputs the compositions, and 
I, the artist, am in a constant state of surprise and discovery, because the machine may structure forms 
that I had never thought of to execute.  3 Control The act of creating artwork in the past has always 
been about a somewhat strict notion of control - commanding colors on certain places on the canvas. (I 
know this doesn't apply to everyone, especially Pollack) However with my work an awkward sense of chaos 
and no control is applied to every aspect of the structure. So my talk would be about the release of 
control, chaos, randomness, algorithms / behaviors and a constant slide show of print work, real time 
interactive and non-interactive web (screen) based work, and broadcast video. (Captured video of time) 
  Copyright held by the author  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	</section>
	<section>
		<section_id>965362</section_id>
		<sort_key>9</sort_key>
		<section_seq_no>9</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Standards]]></section_title>
		<section_page_from>9</section_page_from>
		<chair_editor>
			<ch_ed>
				<person_id>PP14088030</person_id>
				<author_profile_id><![CDATA[81100224886]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Robert]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Reinhardt]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[[theMAKERS]]]></affiliation>
				<role><![CDATA[Session Chair]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
		</chair_editor>
	<article_rec>
		<article_id>965364</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Advances in W3C Web graphics standards]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965333.965364</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965364</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP31039670</person_id>
				<author_profile_id><![CDATA[81100379622]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Max]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Froumentin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[World Wide Web Consortium]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Advances in W3C Web Graphics Standards Max Froumentin (mf@w3.org) World Wide Web Consortium http://www.w3.org/ 
Since 1994 the World Wide Web Consortium has published spec­i.cations designed to make a better and easier 
Web, for end-users, as well as for application developers. While the scope of W3C s work has grown to 
extend the structure of the Web itself with the Semantic Web and Web Services, an important part of the 
work remains focused on the browser and the interaction with the user. However both browser and interaction 
have evolved: the concept of browser is no longer only for desktop computers, while the concept of interaction 
is no longer restricted to mouse and keyboard input. Since the PNG image format was published as the 
.rst W3C Recommendation in 1996, other web presentation formats (HTML, CSS) have been developped, and 
new speci.cations have appeared such as SMIL for concurrent multimedia content, and SVG for in­teractive 
vector graphics. More recently work has started on a mul­timodal interaction framework, as well as a 
timed-text format for captioning web multimedia content. Interoperability Languages such as XHTML, MathML 
or SVG were designed on a common a syntactic level (XML) as well as on a common pre­sentational level 
(e.g. CSS), making it simpler to build appli­cations that integrate these language. For instance, the 
Mozilla browser or W3C s Amaya browser/editor now handle the rendering of XHTML documents including equations 
encoded in MathML and vector graphics in SVG, showing how web pages containing text, graphics and mathematics 
are rendered with unprecedented quality, and in a more accessible and user-customisable way. An­other 
example is the X-Smiles browser, which implements no less than 10 web standards, among which XHTML, XSL, 
CSS and XForms for general pagination, as well as SMIL, SVG and X3D for graphics and multimedia. While 
these projects show that the original design goal of in­teroperability of those speci.cations is ful.lled, 
further challenges appear for which current projects are chartered: De.ning pro.les While implementations 
prove the general in­teroperability of web standards, further investigation needs to be completed to 
specify .ner detail in how to mix the different lan­guages. For example, such questions as where in an 
XHTML doc­ument a synchronised animation can occur need to be explored and the answers standardised. 
The W3C recently started the work of fully characterising the XHTML+MathML+SVG pro.le by de.n­ing additional 
semantics for the mixing of the respective languages, and by providing validation tools for the compound 
language. Overcoming the limitations of plug-ins The next challenge will be to make all Web formats work 
together in browsers that han­dle mixed content through the use of plug-ins. The design of a gen­eral 
plug-in framework is required for specifying how specialised software components, such as an SVG renderer 
or an X3D engine, must interact with each other in the browser in order to satisfy in­tegration requirements, 
e.g. the propagation of CSS properties, or management of user interaction. The W3C chartered a task force 
of browser vendors to begin work on fully de.ning this framework.  Targeting new devices Another area 
of current activity is enabling the web on new devices. In order to cater for new modes of interaction 
provided by mobiles phones or PDAs, several W3C working groups have developed pro­.les of existing languages. 
These pro.les specify a subset of their original language in order to meet the requirements of the platform 
they will be used on (display size, sound, etc.) For instance, the CSS working group has published pro.les 
for presenting web content on mobile devices as well as on interactive TV. SMIL and SVG now also have 
mobile pro.les, which have re­cently been adopted by the Third Generation Partnership Project (3GPP) 
for multimedia messaging on mobile phones. The embed­ded SVG or SMIL viewers can also be used in other 
applications, such as displaying maps. Another example is the SVG Print speci­.cation, to allow for directly 
embedding SVG renderers into high­end printers, removing features such as animation while adding new 
print-speci.c capabilities. Other new activities have also started with a view to respond to the need 
to adapt web content to new interaction modes: the De­vice Independence activity is working on standard 
server-accessible means to specify the capabilities of a client device, as well on mech­anisms for web 
content adaptation. The new Multimodal Interac­tion Working Group is working on integrating multiple 
input and output methods to access web content in various situations, such as voice-controlled browsers 
in cars, or through pen input and hand­writing recognition. These new activities are being developed 
jointly with the Web Accessibility Initiative and Internationalization activities, to ensure that new 
Web Graphics standards allow for universal Web access. Figure 1: Left: SVG animation on a mobile phone 
(courtesy of ZoomOn Mobile Solutions). Right: mixed XHTML/MathML/SVG document in Amaya Copyright held 
by the author 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>965363</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Extensions of SVG for human navigation by cellular phone]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965333.965363</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965363</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P653749</person_id>
				<author_profile_id><![CDATA[81100603707]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Arei]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kobayashi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[KDDI R&D Laboratories Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P653795</person_id>
				<author_profile_id><![CDATA[81100030307]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Satoru]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Takagi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[KDDI R&D Laboratories Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP31050171</person_id>
				<author_profile_id><![CDATA[81100619503]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Naomi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Inoue]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[KDDI R&D Laboratories Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Extensions of SVG for Human Navigation by Cellular Phone Arei Kobayashi, Satoru Takagi, and Naomi Inoue 
kobayasi@kddilabs.jp KDDI R&#38;D Laboratories Inc. Introduction In Japan, cellular phone service started 
in 1987 and spread allover Japan very quickly. With GPS receivers and high resolutionLCD s implemented 
into the cellular phone, the demand forhuman navigation by cellular phone is increasing. One of thetypical 
ways that geographic information is presented for humannavigation is a map. In order to browse a map 
on a cellularphone, high interactivity such as zoom-in, zoom-out and rotation of the map are considered 
essential functions. From thispoint of view, a vector graphics format is suitable. However, theconventional 
vector graphics formatting for a geographicinformation system can only be used in a specific system, 
and isnot designed based on the architecture of the WWW. On theother hand, the W3C (World Wide Web Consortium) 
definedSVG 1.1 / Mobile (Scalable Vector Graphics) as the openWWW standard vector graphics format on 
January 14, 2003.SVG1.1/Mobile is defined for not only the PC, but also mobileterminals such as cellular 
phones. We have developed a mapbrowser software based on SVG1.1/Mobile on cellular phones.This paper 
describes requirements of human navigation bycellular phone, and then proposes extensions of SVG in order 
torealize the requirements. Requirements of Human Navigation In order to realize human navigation by 
cellular phone, the following is required as well as zoomin / zoomout and rotation of the map. -An overlay 
function of downloaded content. A cellular phone downloads some geographic information such as a map, 
point information, and location information from different servers, and then overlays and displays that 
information. -Display control function of graphic objects. In zoom-in and zoom-out of map, graphic objects 
on the map should be controlled by display scale. For example, insignificant roads for wide area map 
are displayed only after zoom-in. -Location mapping on an imprecise map. An imprecise map, for example 
the floor layout of a station, department store, theme park guide, etc. can be used for human navigation 
because of it s high visual effect. The user s current location should be identified on the imprecise 
map.  Extensions of SVG In order to realize the requirements, SVG are extended as follows. 1. Overlaying 
function of downloaded contents. This function is realized by using the Geographics Coordinate System 
specified in SVG 1.1. By using this specification, contents in different servers can define a common 
coordinate system. A map browser software can download contents from different servers, and overlay and 
display them. This function achieves realization of interoperability, which has been a major problem 
of geographic information systems for many years.  2. Display control function of graphic objects. This 
function is realized by using Namespaces in XML specification._ex: <polyline points="0 0 10 10" au:figure­visibility="20,40"/> 
_A browser software calculates a parameter (scale=boundingbox of contents * 10 / current viewport) from 
the current display scale. If 1st value<=scale<=2nd value, the graphic object is displayed. Because a 
browser software need not download a map drawn to a scale, the number of times of network connection, 
and transmission data size, can be reduced. 3. Location mapping on an imprecise map. Some datum points, 
usually more than 10 points, are described in a imprecise map beforehand. Position of each datum point, 
called local position, is corresponded to real world (longitude and latitude), called global position. 
An example of description of datum points is shown below.   <au:local>0,0 120,100 500,500</au:local> 
<au:global>135.00,35.11 135.12,35.22 135.99,35.77</au:global>  After a user s current position is measured 
by a GPS receiver equipped with a cellular phone, the nearest three datum points are selected. Then the 
user s position on the imprecise map is calculated based on transformation parameters between local position 
and global position of nearest three datum points. Conclusion This paper describes requirements of human 
navigation bycellular phone, and proposed extensions of SVG in order torealize the requirements. We have 
developed a map browsersoftware which provides the required functions. Copyright held by the author 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	</section>
	<section>
		<section_id>965365</section_id>
		<sort_key>10</sort_key>
		<section_seq_no>10</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Gaming]]></section_title>
		<section_page_from>10</section_page_from>
		<chair_editor>
			<ch_ed>
				<person_id>PP42057944</person_id>
				<author_profile_id><![CDATA[81341487525]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Simon]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Allardice]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[United Publishing]]></affiliation>
				<role><![CDATA[Session Chair]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
		</chair_editor>
	<article_rec>
		<article_id>965367</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Online gaming applications]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965333.965367</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965367</url>
		<abstract>
			<par><![CDATA[Browser-based online game development has reached a point where exciting and innovative titles can be produced and distributed to a large audience quickly and easily. With the advent of Macromedia Flash MX, and the penetration of the Flash 5 plug-in, browser-based gaming shows the promise of vast possibilities such as further exploring abandoned gameplay models and enhancing them with online multiplayer features.]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P653779</person_id>
				<author_profile_id><![CDATA[81100311989]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Kenneth]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Austin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[WDDG]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>863002</ref_obj_id>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[PENNER, ROBERT, 2002. Programming Macromedia Flash MX]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>601307</ref_obj_id>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[HALL, BRANDEN, AND WAN, S. 2002. Object Oriented Programming with Actionscript.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[MOOCK, COLIN. 2002. Actionscript for Flash MX.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>186897</ref_obj_id>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[GAMMA, E., HELM, Z., JOHNSON, R., AND VLISSIDES, J. 1995. Design Patterns: Elements of Reusable Object-Oriented Software.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Online Gaming Applications Kenneth Austin WDDG Abstract Browser-based online game development has reached 
a point where exciting and innovative titles can be produced and distributed to a large audience quickly 
and easily. With the advent of Macromedia Flash MX, and the penetration of the Flash 5 plug­in, browser-based 
gaming shows the promise of vast possibilities such as further exploring abandoned gameplay models and 
enhancing them with online multiplayer features. 1 Code Structure In commercial production of browser-based 
games, we use techniques common in development for any object orientedlanguage. The structure of the 
Flash movieclip based timeline canbe confusing, especially to a seasoned programmer. The methodwe use 
for all engine development is to have all of the codebroken up into .as files containing each specific 
class that handlethe various game functions and include them in the first frame. Allgraphics are stored 
within movieclips in a library .swf and attached with attachMovie() as needed. The game engine then setsthe 
visibility, position, and _currentframe properties of each of themovieclips as needed. Since actionscript 
is an object orientedenvironment, we are able to create pseudo classes to control game flow, graphical 
updates, user interaction, server communication, and any other functionality our engine will use.These 
classes can then be reused and repurposed for futureengines. Also taking advantage of Flash s object 
oriented nature,design patterns can be helpful tools in organizing the structure ofthe engine. A technique 
we use often is creating objects for thehandling of interaction with the user, such as mouse and keypresses. 
These objects talk to a bridge object which passes theinformation to the game engine. This decoupling 
allows theinteraction objects to be swapped out for different functionality.For example, you could swap 
out the custom keypress object for arecorded value object to create an instant replay the last gameplayed. 
The main game model does not know the difference ofwhat is controlling it. 2 Level Design Many games 
require some sort of level editor to speed up content creation and put level design in the hands of the 
designers. Platformers or adventure games are especially suited for these. Utilizing Flash s robust XML 
capabilities, level editors can be authored in Flash to write XML formatted data for use by the game 
engine. For our adventure game engine, for example, we created a mini-application with importable libraries 
of graphics, which can be dragged and dropped onto the stage, and have editable properties, such as the 
text that appears when the object is looked at, or if it can be picked up. These properties are stored 
in the level editor until the level is built, at which time the data is parsed in actionscript and written 
to XML. These level files are loaded into the engine at runtime and parsed to build each level. e-mail:kc@wddg.com 
 Figure 1. Altoids Strips Adventure Game Engine. 3 Graphics Pipeline Creating game engines in a modular, 
object oriented fashionallows for the easy integration and swapping of graphics. Thisalso allows for 
the engine development and art creation to happenat the same time, allowing for optimal time management. 
Artistscan work on .fla graphic libraries and update them as they finish.These updates will immediately 
appear in the game as it progresses, speeding approval times and creative direction. 4 Conclusion The 
world of browser-based online gaming has finally reached apoint that deserves the attention of serious 
programmers andartists. Flash s inherent capabilities, while prohibiting some genres such as first person 
shooters and large simulations, can be exploited to produce many still engaging and sought after types 
ofgames, such as rpg s, classic style adventure games, platformers,and shooters. Examining these almost 
abandoned styles of gamesshows a wonderful medium for storytelling and expression. WithFlash s extremely 
short and painless development cycle, utilizinga very small team of developers and artists, we are given 
theopportunity to deliver these games with unprecedented speed andminimum cost. Instead of simply rehashing 
old titles, we can learnfrom our gaming experiences since the classic models were released, and further 
the genre of 2D gaming in new and exciting ways.  References PENNER, ROBERT, 2002. Programming Macromedia 
Flash MX HALL, BRANDEN, AND W AN, S. 2002. Object Oriented Programming with Actionscript. MOOCK, COLIN. 
2002. Actionscript for Flash MX. GAMMA, E., HELM, Z., JOHNSON, R., AND VLISSIDES, J. 1995. Design Patterns: 
Elements of Reusable Object-Oriented Software. Copyright held by the author 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>965366</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Item.tv]]></title>
		<subtitle><![CDATA[online game of "symphonic" media]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965333.965366</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965366</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P653767</person_id>
				<author_profile_id><![CDATA[81100345260]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Genki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Shimomura]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P653804</person_id>
				<author_profile_id><![CDATA[81100500883]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Yasutaka]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Mori]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14217284</person_id>
				<author_profile_id><![CDATA[81544539656]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Satoshi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yoshida]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39041138</person_id>
				<author_profile_id><![CDATA[81335492043]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Masa]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Inakage]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Item.tv Online game of symphonic media  Genki Shimomura Keio University, Inakage Lab Yasutaka Mori 
Keio University, Inakage Lab Satoshi Yoshida Keio University, Inakage Lab Masa Inakage Keio University, 
Inakage Lab  1. Overview Item.tv is a completely new type of online game using both the web and broadcast 
programs, made in anticipation that in the near future, these two medias will be combined. A typical 
online game takes place only on the web, and item.tv is the first example in which the web and broadcast 
are fully linked to each other, creating a unique world of its own. In item.tv, the characteristics of 
these two medias were carefully considered, and we have created a game system where the players can cross 
over between multiple medias. 2. Game Overview Item.tv is a competitive item trading game, much like 
a stock market game. First, the entrants of the game contribute their ideas for a fictional product. 
Each of these ideas will become an item , and these items will be put on sale in an item.tv virtual mall. 
The players can buy items in exchange for tem , a fictitious currency used in item.tv, and they can choose 
to keep the items they have purchased as an asset, or sell them at their current value. Similar to stocks 
in the stock market, the value of items constantly fluctuate, and the players can gain tems by buying 
items that they suspect will increase in value. Item.tv can be accessed from mobile phones as well, so 
that the users can trade items even when they are away from their computers. The objective of the game 
is to increase the value of one s property through the exchange of items, and place their names in the 
ranking.  item.tv web page In order to profit from trading items, the players must carefully time 
when to buy or sell the items based on the different information available. To facilitate communication 
between the players, the item.tv web page is equipped with BBS and messengers that can be used by the 
players to exchange information by stimulating communication between the players, we are also aiming 
to create an online community. As a means to make the game more competitive and give the new entrants 
chances to catch up, events that dramatically alter the item market will be provided. This will be done 
through the use of broadcast programs. For example, a market forecast predicting the movements in the 
item market in the following week will be broadcasted, providing players with valuable information that 
will help them trade items to their advantage. By fully utilizing these information, even new entrants 
will have a chance to compete. Information in broadcast programs, as mass media, offer high reliability, 
so we can expect the users to show a high degree of reaction to it through their actions in the game. 
 The use of different medias in item.tv 3. Techniques in use In item.tv, XML is used in the user database 
and the content database. By utilizing XSLT, it is possible to take one source from XML and use it in 
the Web (HTML), mobile phones of three main carriers (i-mode[CHTML] , ez-web, J-sky), and data broadcast 
(BML). XSLT automatically detects the environment (device) from which the user is accessing, and chooses 
the content fit for that device. We have carefully considered the different characteristics of these 
devices to produce optimum effect in creating item.tv.  Automated content serving system Also, in the 
item.tv virtual mall, Macromedia Flash and PHP are used to construct the web page and the database system 
needed for item trading. 4. URL  5. Acknowledgements This project is sponsored by Iwasaki Sangyo Co.,Ltd 
and rainboware inc. -------------------------------------------- *e-mail:smoke@imgl.sfc.keio.ac.jp 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	</section>
	<section>
		<section_id>965368</section_id>
		<sort_key>11</sort_key>
		<section_seq_no>11</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Art & design]]></section_title>
		<section_page_from>11</section_page_from>
	<article_rec>
		<article_id>965371</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Spacemath]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965333.965371</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965371</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P653771</person_id>
				<author_profile_id><![CDATA[81100607027]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Gustavo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Valga&#241;&#243;n]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Spacemath Gustavo Valgañón http://www.valganon.com/spacemath Spacemath is an experimental animation 
presentation made to be shown on the internet. It is made using the Flash software program. Once the 
introductory page is clicked on the viewer is presented with .ve chapters or consecutive animations that 
require no user input and offer no interactivity. Its design and implementation attempts to evoke the 
senses: irreverent color combinations which clash and change rapidly, stains of color violently move 
in the screen seemingly without a purpose, taking over the whole browser window. Figure 1. Spacemath 
welcome screen. Since the animation is divided into .ve chapters, an added element of organization or 
control is present in what otherwise is a random and alive display. These .ve chapters feature the only 
.gurative elements present in the animation, precisely in the least important parts: the loading screens. 
While much experimental animation made with the . ash technology is realized with the actionscript language, 
this animation relies more on the frame by frame capabilities of the . ash authoring program. Instead 
of the .ash player s own scripting language The essence of the randomness and organic feel of this animation 
is achieved by manipulating vector data even before any work is done in Flash. The vector data is processed 
using scripting formulas in the Adobe Illustrator program. By connecting several Macintosh computers 
in a local area network and sharing the processing time, the impression of drawing with scalable vector 
graphics in real time was achieved. Following this process almost thirty minutes of graphics were recorded. 
Spacemath as it is shown now is the result of editing the stream of vector data. e-mail: gustavo@valganon.com 
  Figure 2. A frame from Spacemath chapter three. With a background in painting, I use a computer much 
in the same way as a camera: to acquire references that can trigger the process of creating. With a computer, 
however, instead of capturing the world around me I try to depict unknown environments which already 
exist or are otherwise susceptible of being discovered. The only requisite to view Spacemath is to use 
a browser equipped with the version 6 of the . ash player.  Copyright held by the author 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>965372</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[ArtStream]]></title>
		<subtitle><![CDATA[Web art in a curatorial setting]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965333.965372</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965372</url>
		<abstract>
			<par><![CDATA[ArtStream, a Web-based, curated exhibition space for Internet art, seeks to promote the work of Internet artists while bringing it into the "legitimized" world of traditional museum culture. Therefore, ArtStream makes it easy for a curator from a less technical background to administer an exhibit. ArtStream is the product of a collaboration between the University of Arizona Museum of Art and the Treistman Center for New Media, in the University of Arizona College of Fine Arts. The project was funded by a New Learning Environments grant from the university.]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P653800</person_id>
				<author_profile_id><![CDATA[81100050130]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Tom]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hapgood]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Arizona]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P174143</person_id>
				<author_profile_id><![CDATA[81100047360]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Lucy]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Petrovich]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Arizona]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14104031</person_id>
				<author_profile_id><![CDATA[81536614656]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Peter]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Briggs]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Arizona Museum of Art]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P653784</person_id>
				<author_profile_id><![CDATA[81100007355]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Mike]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Holcomb]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Arizona]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P653755</person_id>
				<author_profile_id><![CDATA[81100385302]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Charles]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Guerin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Arizona Museum of Art]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14040617</person_id>
				<author_profile_id><![CDATA[81100084855]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[James]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Smith]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Treistman Center for New Media]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P653758</person_id>
				<author_profile_id><![CDATA[81100453989]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>7</seq_no>
				<first_name><![CDATA[Cynthia]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Barlow]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Treistman Center for New Media]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 ArtStream: Web Art in a Curatorial Setting Tom Hapgood Professor Lucy Petrovich* Peter Briggs Digital 
Arts Design Manager Guest Curator Chief Curator Treistman Center for New Media Department of Media Arts 
University of Arizona Museum of Art University of Arizona University of Arizona Professor Mike Holcomb 
Charles Guerin James Smith, Programming Manager Assistant Dean of Digital Technology Director Cynthia 
Barlow, Network Services College of Fine Arts University of Arizona Museum of Art Manager University 
of Arizona Treistman Center for New Media Abstract ArtStream, a Web-based, curated exhibition space 
for Internet art, seeks to promote the work of Internet artists while bringing it into the legitimized 
world of traditional museum culture. Therefore, ArtStream makes it easy for a curator from a less technical 
background to administer an exhibit. ArtStream is the product of a collaboration between the University 
of Arizona Museum of Art and the Treistman Center for New Media, in the University of Arizona College 
of Fine Arts. The project was funded by a New Learning Environments grant from the university. 1 Introduction 
 Similar to traditional, physical exhibit spaces, a good deal of work has gone on behind the scenes in 
order for the public to enjoy the artwork in an ArtStream online exhibit. 2 Exposition The ArtStream 
administrative area was created with the typical Web user in mind, relying on standard Web forms in standard 
Web browsers for system interaction. (Figure 1) Information entered into these java server pages forms 
is stored in a MySQL database structure and is then displayed through dynamic Flash technology on the 
public front end. In creating an exhibit, the curator either chooses one artist s work to highlight or 
selects a theme and presents the work of several relevant artists. The curator is prompted to enter the 
Exhibit name, the curator s statement, as well as biographical information on the involved artists and 
inputting and editing the individual artwork information. In addition, the exhibit can be customized 
by picking a color scheme and uploading a poster image. (Figure 2) Special technology needs of the exhibit, 
in the form of browser plug-ins, are also requested from the curator to ensure a successful visit by 
the exhibit visitors. The curator continues to log in and administer the exhibit before the exhibit opens, 
at the same time working with the artists on obtaining the permission to show the works. Once permission 
to use the work has been obtained, the work is either uploaded onto the ArtStream server or is simply 
linked to. The existing exhibit is moved into the archive as the new exhibit becomes active. e-mail: 
thapgood@u.arizona.edu*e-mail: lucy@u.arizona.edu url: http://artstream.arizona.edu Copyright held by 
the author   
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>965373</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Typorganism]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965333.965373</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965373</url>
		<abstract>
			<par><![CDATA[Typorganism(http://www.typorganism.com) is a web-based work, which is comprised of eight communication experiments. This project's main focus is interactive kinetic typography, and I began with my metaphorical belief that "type is a life form."During the course of designing and developing this web site, the project itself has been evolved to embrace the experiment for multi-user interaction on the web.]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P653768</person_id>
				<author_profile_id><![CDATA[81100384921]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Gicheol]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lee]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MFA Computer Art, School of Visual Arts]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Lee, Gicheol. 2002. Typorganism: Communication Experiments focused on Interactive Kinetic Typography and Communal Interactivity in the Web Environment. MFA thesis, Computer Art, School of Visual Arts]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Typorganism www.typorganism.com Gicheol Lee MFA Computer Art, School of Visual Arts Abstract 2.1 Evolution 
Typorganism(http://www.typorganism.com) is a web-based work, which is comprised of eight communication 
experiments. This project s main focus is interactive kinetic typography, and I began with my metaphorical 
belief that type is a life form. During the course of designing and developing this web site, the project 
itself has been evolved to embrace the experiment for multi-user interaction on the web. 1 Introduction 
Since 1999, I have been studying programmatic motion and applying math and physics formulas to my experiments 
and design works. Meanwhile, the results of these designs appeared similar to a small life form, and 
implied some type of intelligence. If I compose a list of commands and apply that to a visual element, 
its behavior appears alive. It reacts to the user, or it moves in a way that I instructed. The movements 
are geared by a software algorithm. They are a new breed. They are life forms created with programming. 
Typorganism project was originated with my motive to demonstrate how typography can be altered by using 
recent programming techniques in the web environment. This project was started out by aiming to experiment 
with interactive kinetic typography, and ended up being a set of communication experiments. All of the 
8 pieces are in interactive environment, so user s mouse gestures, key strokes, images uploaded by users 
from their hard disk, and even text and images extracted from other web site are the input sources for 
this work. My ultimate goal of this project was to demonstrate the aesthetics of numbers, data structure, 
and form, through computation, so that I can deliver the new and engaging experience for users, and suggest 
alternative solutions for communication. Web has been being evolved as a sophisticated organism, and 
it was a big fun to use the metaphor that Type is an Organism , because type and web are inevitable communication 
medium in our life. 2 Exposition I organized the recent trend of typography in a new medium having five 
characteristics. The following are the main points that I can point out to support my belief of typography 
as a notion of organism metaphor. email: nucleus@typorganism.com Typography has been evolving through 
history. It continues to transform depending on the cultural environment and the latest technology. 2.2 
Dynamism Dynamism in typography is rooted in Futurism, and it has been developed throughout the 20th 
century s communication design history. Now we witness the actual movement of type in motion graphics 
and interactive multimedia, and its liveliness makes the typography immersive and engaging experience 
with achieving its main purpose effective communication. Figure 1. Typorganism DNA 2.3 Intelligence 
Since type is running via software s algorithm, type can have some degree of intelligence depending on 
how sophisticated it was programmed. 2.4 Responsiveness Interactive kinetic type can be thought of as 
an organism, because it is responsive to the outer stimulus. 2.5 Propagation In computation medium, 
all the work equates to data , which is composed of 0s and 1s. It can be easily replicated or distributed 
through the network.  References Lee, Gicheol. 2002. Typorganism: Communication Experiments focused 
on Interactive Kinetic Typography and Communal Interactivity in the Web Environment. MFA thesis, Computer 
Art, School of Visual Arts Copyright held by the author 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>965369</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[Flowing transformations and any wall]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965333.965369</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965369</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP14119843</person_id>
				<author_profile_id><![CDATA[81332509890]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Robert]]></first_name>
				<middle_name><![CDATA[J.]]></middle_name>
				<last_name><![CDATA[Krawczyk]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Illinois Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Flowing Transformations and Any Wall  Flowing Transformations URL: www.iit.edu/~krawczyk/s2003 This 
web page displays a continuous series of curve transformations from their linear origins with a very 
structured flow of color. The display consists for three parts: first the presentation of the faded original 
curve, then the transformation in a flowing manner, and finally the spine of the transformed curve to 
reinforce the path it traveled through. All of the curves are computed and drawn in real time, randomly 
selected from a gallery of over 300 possible curves. Each is based on a graphic interpretation of a simple 
spirolateral curve, then transformed according to one of the following methods: antiMercator, circular, 
circle inversion, harmonic mean inversion, hypocycloid, epicycloid, epitrochoid, piriform, bicorn, tear 
drop, lips, lame, or hippopede curve. Individual images in this series can be found at: www.netcom.com/~bitart 
This web page only requires a JAVA enabled internet browser. Algorithmic generators and transformations 
are used to develop a series of images that explore the unexpected from the very simple. These images, 
that at one time elicit fragments of a far off future, at the same time remind me of past ritual icons 
and symbols. The starting linear and resulting circular forms remind me of the past while the lack of 
color and the sharp edges envision a possible future. The series of images included are part of a larger 
series investigating the formation of artwork using basic mathematical concepts. This series is based 
on a mathematical figure called a "spirolateral". It is simply a square spiral with increasing length 
of turns and the turns repeating themselves. The turns can be all in one direction or certain turns can 
go the opposite direction. The most interesting of these are ones that close on themselves, not all do. 
Investigating a series of possible turning angles, number of turns, number of repeats, and trying all 
revered turns, over 10,000 spirolaterals have been identified that are closed. URL: www.iit.edu/~krawczyk/s2003 
This piece speculates on a possible replacement for Sol LeWitt's draftsperson with a web application 
for his wall drawings, a simple experiment in viewer directed art. This is a parody of the original concept 
of the wall drawing. This is not a wall drawing. This is not art by Sol LeWitt. You are not Sol LeWitt 
if you choose to execute this. This JAVA applet is not Sol LeWitt. You are the artist. Imagine an additional 
button that could send you a printed copy of the drawing you generated, size and payment to be determined. 
Imagine if such a web page could drive a real-time display the size of an actual wall or room. Changes 
to the wall could be presented from all over the world at some time interval. A send button would be 
added to queue the next arrangement. Or imagine if the wall is covered with a future version of digital 
vinyl or digital ink technology turned into a digital paint. Is Sol LeWitt's "machine for art" now complete? 
This web page only requires a JAVA enabled internet browser. LEWITT, S. 1975. Plan for a Wall Drawing, 
San Francisco Museum of Modern Art, July 4, 1975. LEWITT, S. 1981. Wall Drawing #358,, Museum of Contemporary 
Art, Chicago, Gerald S. Elliot Collection. LEWITT, S. 1999. Wall Drawing #912, Collection of Barbara 
Gladstone, Sag Harbor, New York LEWITT, S. 1969. Sentences on Conceptual Art, Art-Language1, no.1, May 
1969, 11-13. LEWITT, S. 1971. Doing Wall Drawings, Ar Now: New York 3, no.3, June 1971. Copyright held 
by the author 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>965370</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>5</seq_no>
		<title><![CDATA[FRESHMIX]]></title>
		<subtitle><![CDATA[a study in user interaction with dynamic content]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965333.965370</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965370</url>
		<abstract>
			<par><![CDATA[This project is an online blender providing refreshing interactive experience through the integration of multimedia design and creative programming to make a unique use of dynamic content.]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P653781</person_id>
				<author_profile_id><![CDATA[81100221472]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Maureen]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>558568</ref_obj_id>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Archontakis, P., Beard, D., Chua, E., Diogo, J., Doyle, P., Ellis, B., Everett-Church, J., Hall, B., Humphrey, D., Kato, R., Khilnani, N., Lee, P., Leone, S., Mahoney, B., Montalenti, A., Piper, P., Ponce, C., Prideaux, R., Roy, N., Ryder, S., and Webster, S. 2001. Flash 5 Dynamic Content Studio]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>558642</ref_obj_id>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Moock, C. 2001. ActionScript: The Definitive Guide]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>516825</ref_obj_id>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Kerman, P. 2001. ActionScripting in Flash]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Sawahata, L. 1999. Color Harmony Workbook]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>516802</ref_obj_id>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Shedroff, N. 2001. Experience Design]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 FRESHMIX: A Study in User Interaction with Dynamic Content Maureen Lin Abstract This project is an 
online blender providing refreshing interactive experience through the integration of multimedia design 
and creative programming to make a unique use of dynamic content. A. Objective The objective of my project 
is to provide a new interactive experience through the use of Flash ActionScript. The final project should 
provide an example of interaction design as an extendable mockup for commercial use. Through the proper 
graphics and interface design, users should experience no difficulty understanding the purpose of the 
program, while still experiencing unexpected interaction delivered from creative programming. B. Overview 
 My submission is an online blender built with Macromedia Flash. It is designed to provide a refreshing 
interactive experience through the integration of multimedia design and creative programming. The site 
is driven by a simple database in order to display dynamic content. Based on the user s choice of fruit, 
the program provides information such as vitamins and calories. There is an unlimited variety of choices, 
thus the user experience is unique every time the program is run. C. Features The user can move pieces 
of fruit to a collecting area and preview nutrition information such as vitamins and calories for each 
piece. If they are not satisfied with their selection they can go back to the starting section to make 
modifications. Once they are finished choosing, they can confirm their selection and start blending juice. 
Each fruit has been assigned its own color, sound, opacity, and motion. These features combine to show 
a unique blending animation based on the user s selection. Each fruit contributes its color value to 
the liquid, and adds its sound effect to the background loop. When the blending is complete, the user 
can choose from a variety of cups and decorations. The juice is poured into the cup and decorated appropriately. 
Sound effects from the chosen fruit play in the background as the recipe and nutrition facts appear. 
The user can then print the final output and return to making juice, or exit the program.  Images Screen 
Examples:  C. Start to blend customized juice  Citations and References Archontakis,P., Beard, D., 
Chua, E., Diogo, J., Doyle, P., Ellis, B., Everett-Church, J., Hall, B., Humphrey, D., Kato, R., Khilnani, 
N., Lee, P., Leone, S., Mahoney, B., Montalenti, A., Piper, P., Ponce, C., Prideaux, R., Roy, N., Ryder, 
S., and Webster, S. 2001. Flash 5 Dynamic Content Studio Moock, C. 2001. ActionScript: The Definitive 
Guide Kerman, P. 2001. ActionScripting in Flash Sawahata, L. 1999. Color Harmony Workbook Shedroff, N. 
2001. Experience Design Copyright held by the author 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>965374</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>6</seq_no>
		<title><![CDATA[VIROS]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965333.965374</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965374</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP14144127</person_id>
				<author_profile_id><![CDATA[81100407712]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Teresa]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 VIROS Teresa Lang tiger@smokingmoose.com URL: www.smokingmoose.com/3d/viros/virosFR.htm  Description: 
VIROS is an abstract interactive 3d web space, comprised of 12 environments. 1 Navigation instructions: 
Choose an environment by clicking on the buttons on the left side of the screen. When the doors open, 
move your mouse around the environment that appears. Try clicking, dragging and throwing objects. 2 
Background information: VIROS was created to exist only on the internet, there is no reason for it to 
exist anywhere else. VIROS does not mean anything and might not make any logical sense. It mixes abstract 
visuals, abstract sounds and abstract interaction to create an artistic web space to travel into, explore 
and possibly get lost in. VIROS was created using AXELedge and Dreamweaver.  3 Software requirements: 
Windows or Macintosh with Internet Explorer 5.0 browser (or higher) and the AXELplayer, which should 
download automatically when visiting the VIROS website, if not please visit: http://www.mindavenue.com/en/download/AXELplayer.html 
 Copyright held by the author  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>965375</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>7</seq_no>
		<title><![CDATA[A is for apple]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965333.965375</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965375</url>
		<abstract>
			<par><![CDATA['A is for Apple' is an interactive web artwork by David Clark. It was produced in collaboration with web designers Rob Whynot, Randy Knott, and Ron Gervais.]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP14215456</person_id>
				<author_profile_id><![CDATA[81100625462]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[David]]></first_name>
				<middle_name><![CDATA[A.]]></middle_name>
				<last_name><![CDATA[Clark]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 A is for Apple David A. Clark1 Director and Producer Abstract A is for Apple is an interactive web 
artwork by David Clark. It was produced in collaboration with web designers Rob Whynot, Randy Knott, 
and Ron Gervais.  Figure 1: A is for Apple: Apple Page 1 Introduction A is for Apple was produced using 
Macromedia Flash 5 and has been on-line since March 2002. Produced primarily as an artwork to be viewed 
on­line, it has also been exhibited as an exhibition with the original collages. Figure 2: A is for 
Apple: Main Map  2 Exposition Conceptually A is for Apple explores the array of meanings associated 
with the apple. It also reflects on the meaning of meaning and the role of language in culture. The viewer 
navigates through animated pages that are linked through a web of associations, The work touches on a 
vast array of subjects; religion, popular culture, the history of cryptography, and the ideas of language 
and psychoanalytic theory. 1 dclark@nscad.ns.ca The work was constructed using the paradigm of collage. 
Using images culled from Google image searches, collages were built and animated around sixty-five topics. 
A menu of spinning words allows you to navigate from link to link, As well, a map of the site shows the 
network of connections. This work explores the use of interactive multimedia to explore the documentary 
form in an artistic and non-linear way that reflects a web surfing mode of navigating the information. 
It also reflects the collaborative process showing how several different sensibilities can be integrated 
into the final project. Figure 3: A is for Apple: Strawberry Fields Page This work has been shown widely 
on the internet and at galleries and festivals including: Seoul Net Festival, File2002, Sao Paulo, Split 
2002, Croatia, Montreal Festival of New Cinema and New Media, International Festival of New Media, Brudenell, 
PEI, Pixxelpoint, Nova Gorica, Slovenia, Digital Clip Festival, Italy, 16th Stuttgart Filmwinter, Germany, 
Sundance On-Line Film Festival, Park City, Immedia2003, Ann Arbor, Transmediale.03, Berlin, SXSW 2003 
Interactive, Austin, American Museum for the Moving Image, New York, Images du Nouveau Monde Festival, 
Quebec City, European Media Arts Festival, Osnabruck, Videomedeja, Hungary, The Irish Museum of Modern 
Art. It won Best in Show at the SXSW 2003 Interactive Web Awards in Austin, Texas, First Prize at the 
FILE2002 in Sao Paulo, Brazil, Best Interactive at Pixxelpoint in Slovenia, and three awards at the International 
Media Arts Festival in Brudnell, Canada. Copyright held by the author  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	</section>
	<section>
		<section_id>965376</section_id>
		<sort_key>12</sort_key>
		<section_seq_no>12</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Commercial]]></section_title>
		<section_page_from>12</section_page_from>
	<article_rec>
		<article_id>965377</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Extreme animations for Web]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965333.965377</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965377</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P653748</person_id>
				<author_profile_id><![CDATA[81100496849]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Andr&#233;]]></first_name>
				<middle_name><![CDATA[Luiz]]></middle_name>
				<last_name><![CDATA[Mendon&#231;a]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Studios Pixel]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P653765</person_id>
				<author_profile_id><![CDATA[81100440120]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Felipe]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Araujo]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Studios Pixel]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Extreme Animations for Web André Luiz Mendonça¹ Studios Pixel  1. Objectives: The project of the 
site Studios Pixel had as primordial parameter the interativity and applied innovation to each action 
done by the user. We applied represented usual concepts in a new way, using artistic animations and subjective 
ideas mainly than it comes to be the content of the information that is being accessed, escaping from 
a traditional pattern that separates the art of the consulted content. Inside of this subjective concept 
we used dynamic animations with the objective of providing to the user an interest for the presentation 
forms linking the content in an entertaining and unexpected way. The interface of the site was thought 
in a way that provided the maximum of prominence for the animations that happen during the actions, we 
draw a clean layout to give emphasis exactly to its actions. The character's animations Pixelman was 
whole developed and animated in 3d but with an exit of artistic quality, releasing textures and other 
Rendering effects, insinuating that so much he as the other objects that are part of its actions was 
drawn by hands and animated in the same way. 2. Making Off: To use 3d effects and appropriate plugins 
of exit of Rendering we created the scenes in Quicktime format with the maximum of quality and with Macromedia 
Flash MX we compressed the files optimizing its size for the web pattern. ¹ e-mail: andre@studiospixel.com 
² e-mail: felipe@studiospixel.com Felipe Araujo ² Studios Pixel  We published the other effects in 
Flash MX to integrate the animations to the content and layout pre-defined for the site. The sound effects 
were published and prepared separately, than imported for Flash MX, synchronized with the animations, 
giving more form and realism to the scenes.  3. Conclusions: With the progress of the Internet were 
created differentiated alternatives to expose a content in a more creative and artistic way than properly 
informative . The use of animation techniques for web can provide the creation of an innovative atmosphere 
and fewer standardized. Based on that idea, we developed the last version of Studios Pixel site, integrating 
visual design with motion graphics and content concepts . Copyright held by the author 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>965378</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Exploring the Shockwave engine ForgeFX]]></title>
		<subtitle><![CDATA[contest page]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965333.965378</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965378</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P653760</person_id>
				<author_profile_id><![CDATA[81100445067]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[David]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Vaillancourt]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Exploring the Shockwave Engine ForgeFX Contest Page  1 About ForgeFX: Bringing together years of 
web and game development with deep 3D expertise, the ForgeFX team has created rich experiences for a 
vast number of clients and applications. Our range of projects includes games and entertainment, advertisement 
and product demonstrations, interactive simulations, and general web business applications. ForgeFX is 
committed to pushing the technology to help their clients deliver engaging digital content. 2 Practicality 
of 3D on the web: Our contest site contains examples of projects that we have completed using the Macromedia 
Shockwave engine. One of the main challenges of introducing 3D on the Web is to find practical uses for 
it that a company and client can financially gain from. Right now the most practical use is product representation. 
This gives the users a chance to interact with a company s product before purchasing. We believe that 
as the technology progresses and as average users machines get faster that many other applications will 
open up as well. Such as an architectural walk through, 3D visualization designed for database management, 
and gaming of course. 3 Our contest site: We have four product representations and three game samples 
that we have chosen to represent our work. 1. Car Maintenance This project is a 3D sample of how to 
perform a few basic car maintenance procedures. The user can interact with the car to get a better view 
of things as well as read the text script that follows the user along. 2. Furniture Assembly This project 
is a 3D assembly instruction manual for a cedar porch swing by American Woods. 3. Ornament This project 
pushed the poly count for Shockwave. It was designed for a CD-ROM application as it is 2.5MB in size 
in which could take a bit of time to download. At 40,000 polygons, this one surprised all of us. You 
can click on the center dial to start / stop the train, as well as the two smaller red buttons to make 
the horn or whistle blow. 4. Fictitious Bicycle Computer This project was developed to show how Shockwave 
could be used as a prototype to visualize products that don t exist yet. The user can interact with all 
the buttons thus changing the screen on the project. Again, this project was developed for a CD-ROM application. 
The original version had video on the screen with synced audio in which our client asked us to remove 
for the contest page. 5. Lode Runner Trailer This is an example of aggressive content in a small file 
size. It is a cut scene for the 3D version of Lode Runner. A compressed QuickTime rendered version would 
be about 30MB. This online version is under 300K. 6. Golf This shows our part of a 3D golf game. We 
re still awaiting the approval of our client to post this for the contest so it might be removed in the 
near future. 7. Stunt Racer This is our in house project of a BMXer going through a course. There are 
shortcuts you can take, but be warned, some shortcuts will take you farther back in the course. We are 
using a modified version of the ray casting to control the biker and havoc to handle real time physics. 
 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>965379</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[KDLAB v3]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965333.965379</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965379</url>
		<abstract>
			<par><![CDATA[KDLAB is an interdisciplinary firm intent on exploring the blurred boundaries between architecture, graphics, and film.Committed to ongoing research in applied computer graphics technologies and the fostering of experimentation, collaboration and media synthesis, KDLAB's projects have ranged from interactive multimedia design, to the development of digital cinematic trailers for gaming and film. The lab is also focused on photorealistc simulation, immersive VR technologies, broadcast design, information architecture and branding. KDLAB works closely with DCC software developers, aiding in the testing of emerging tools and technology.This project is the latest in an ongoing effort to document the work by KDLAB. The goal of the project was to leverage mainstream digital technologies to employ a transparent inhabitation of various projects in various mediums.]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P653761</person_id>
				<author_profile_id><![CDATA[81100446312]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Dean]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Di Simone]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[KDLAB, Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P640303</person_id>
				<author_profile_id><![CDATA[81100626962]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Joseph]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kosinski]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[KDLAB, Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 KDLAB v3 Dean Di Simone* and Joseph Kosinski KDLAB, Inc. Abstract KDLAB is an interdisciplinary firm 
intent on exploring the blurred boundaries between architecture, graphics, and film. Committed to ongoing 
research in applied computer graphics technologies and the fostering of experimentation, collaboration 
and media synthesis, KDLAB s projects have ranged from interactive multimedia design, to the development 
of digital cinematic trailers for gaming and film. The lab is also focused on photorealistc simulation, 
immersive VR technologies, broadcast design, information architecture and branding. KDLAB works closely 
with DCC software developers, aiding in the testing of emerging tools and technology. This project is 
the latest in an ongoing effort to document the work by KDLAB. The goal of the project was to leverage 
mainstream digital technologies to employ a transparent inhabitation of various projects in various mediums. 
 *e-mail: dean@kdlab.net e-mail: joseph@kdlab.net  Copyright held by the author 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	</section>
	<section>
		<section_id>965380</section_id>
		<sort_key>13</sort_key>
		<section_seq_no>13</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Community]]></section_title>
		<section_page_from>13</section_page_from>
	<article_rec>
		<article_id>965381</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Geotracker]]></title>
		<subtitle><![CDATA[a global weather snapshot]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965333.965381</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965381</url>
		<abstract>
			<par><![CDATA[geoTracker is an application that provides a visitor-based snapshot of the global weather in near-real time.As each visitor enters the application they start on a world map at their location, and their current weather is displayed. Then, they can travel around the world looking at information for other cities in other countries. Most importantly, all user's paths are visible to everyone else.]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP24016242</person_id>
				<author_profile_id><![CDATA[81100374241]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Marcos]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Weskamp]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[COLLIN MOOCKAmbient User Activity on the web. http://www.moock.org/unity/clients/ambientUserSound/docs/aboutAUS.html]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>557780</ref_obj_id>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[MARTIN DOGE & ROBIN KITCHINMapping Cyberspace - Routelege - 2001]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[M. STEKIK, D. G. BOBROW, S. LANNING, B. TARTAREarly Experiences with multi-user interfaces <u>http://www2.parc.com/istl/groups/hdi/papers/stefik-wysiwis-revised-earlyexpe.pdf</u>]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>516802</ref_obj_id>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[NATHAN SHEDROFF, Experience Design - New Riders - 2001]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Elizabeth F. Churchill, David N. Snowdon Alan J. Munro, Collaborative Virtual Environments - Springer - 2001]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 GeoTracker : a global weather snapshot Marcos Weskamp www.marcosweskamp.com Abstract geoTracker is an 
application that provides a visitor-based snapshot of the global weather in near-real time. As each visitor 
enters the application they start on a world map at their location, and their current weather is displayed. 
Then, they can travel around the world looking at information for other cities in other countries. Most 
importantly, all user's paths are visible to everyone else. 1 Introduction One of the goals of the web 
has always been to break all barriers of space and time, just to put you right there right now . Geotracker 
is a little bit ironic, it talks about that, it puts you on the same webspace with all these other people 
from around the world but it reminds you of the spacial and temporal distance that sepparates you from 
all the other participants Unlike other multi-user environments, here, all participants remain anonymous, 
only identifiable by the path, or footPrint thay they leave as they move throughout the map. As they 
move around the application visiting weather stations worldwide, they will partially reveal the current 
status of the global weather. I've always been very interested in the irony of the fact that today we 
access almost every information resouce in the internet alone, always unaware of all the people how is 
accessing the same resource at the same time. It's always the user and the screen and that's it, there's 
no human presense there. There's no way to know about all the other hundred thousand users accessing 
amazon.com with me, even less interact with them. I wanted to allow other people to become aware of the 
nature of the human factor that we don't always see on the web in a clear example where that factor, 
with the help time and space visualizations, would surface. I wanted to produce a snapshot and visualize 
the real existence and geographical and cultural diversity that the web represents in a non-verbal fashion. 
Where are the other users coming from? what time is it there? What's the environment around him like? 
 2 Exposition The application combines a light local database with several web­services and APIs from 
which it pulls data to resolve the user's geographical location and bring in the weather reports for 
each location. Users accessing the application are localized using several methods, and are first displayed 
on the map in the nearest location available for them. Once the user's geographical location is resolved, 
the application creates a "base node" which will be available for other users to access while this user 
stays connected to the application, and then searches for the nearest weather report to their geographical 
location and displays it. .-------------------------------------------- *e-mail: mail@marcosweskamp.com 
Once the localization proccess is finished, the user can move towards other weather station nodes by 
loading the available weather stations, and access more detailed statistics by clicking on a node or 
other users base stations. Figure 1. geoTracker 3 Background I was caught into multiuser content or 
computer supported cooperative work (CSCW) creation after I read Ambient User Activity on the Web by 
Collin Moock it was there when I realized about our need to see others, collaborate with them, and follow 
the trails of their activities in any kind of space. CSCW is nothing new, it has been around since the 
begining of personal computation, but it had always been accessible through desktop applications. Today, 
not only can it be accessed through a ubiquotus web browser, but the creation and development of such 
applications, which used to something only for programming gurus, has become accessible for almost everyone, 
but so far it has hardly been seen icorporated in the content that at least I browse everyday, which 
sometimes makes me forget about the tangibility and real existance of all the other people that brings 
the web alive  References COLLIN MOOCK Ambient User Activity on the web. http://www.moock.org/unity/clients/ambientUserSound/docs/ab 
outAUS.html MARTIN DOGE &#38; ROBIN KITCHIN Mapping Cyberspace Routelege -2001 M. STEKIK, D.G. BOBROW, 
S. LANNING, B. TARTAR Early Experiences with multi-user interfaces http://www2.parc.com/istl/groups/hdi/papers/stefik-wysiwis­ 
revised-earlyexpe.pdf NATHAN SHEDROFF, Experience Design New Riders 2001 Elizabeth F. Churchill, David 
N. Snowdon Alan J. Munro, Collaborative Virtual Environments Springer -2001 Copyright held by the author 
 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>965382</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[TelMeA theatre]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965333.965382</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965382</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP38025535</person_id>
				<author_profile_id><![CDATA[81100491034]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Toru]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Takahashi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[ATR Media Information Science Laboratories, 2-2-2 Hikaridai, Keihanna Science City, Kyoto 619-0288, JAPAN]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14054061</person_id>
				<author_profile_id><![CDATA[81100123349]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Yasuhiro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Katagiri]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[ATR Media Information Science Laboratories, 2-2-2 Hikaridai, Keihanna Science City, Kyoto 619-0288, JAPAN]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14191804</person_id>
				<author_profile_id><![CDATA[81100552319]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Keiko]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nakao]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[ATR Media Information Science Laboratories, 2-2-2 Hikaridai, Keihanna Science City, Kyoto 619-0288, JAPAN]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 TelMeA Theatre Toru Takahashi* Yasuhiro Katagiri Keiko Nakao ATR Media Information Science Laboratories 
2-2-2 Hikaridai, Keihanna Science City, Kyoto 619-0288, JAPAN Background Among numerous web community 
services available today, online community system such as bulletin board system enables people to communicate 
each other through the Internet without sharing time and real space. In avatar systems, on the other 
hand, people share virtual space using their embodied representatives, and at the same time are able 
to have real time communication by using of the embedded chat system. While asynchronous communication 
in bulletin board systems mainly consists of words or at the most with graphics, various gestures and 
spatial context can be used in avatar systems. Significant Features In our asynchronous communication 
system, TelMeA Theatre, avatars with synthesized voices and various gestures are available for users. 
The users, the habitants of TelMeA Theatre, not only express themselves through multimodal representation, 
such as gestures of the avatars, but also are able to open any web pages in the middle of their utterance 
and to use finger pointing at figures on opening web pages. In short, they can make their stories with 
multimodal representation, edit web pages all around the world in their stories, and exchange their stories 
in conversation threads. Because of the asynchronous features of TelMeA Theatre, the habitants can replay 
registered stories at any time. Although the conversations are built in asynchronous manner, they will 
be replayed as if avatars are playing their roles on the scene. Setup Instruction TelMeA Theatre employs 
Microsoft Agent technology for the avatar-controlling module. Therefore the service requires Windows 
machines and Microsoft Internet Explorer 5.5 or later for client terminals. When accessing our web site, 
select the privacy level of IE on Medium-High or lower and let IE accept certified ActiveX components. 
A broadband connection is preferred. If there is any English TTS system, the MS agents can utter with 
a sound. If not, the system is available for free from the Microsoft Agent web site or directly from 
the following URL: http://activex.microsoft.com/activex/controls/agent2/tv_enua.exe. *e-mail: toru@atr.co.jp 
e-mail: katagiri@atr.co.jp e-mail: nakao@atr.co.jp  Instruction for Navigation 1. What to Do First At 
the entrance of the TelMeA Theatre, users will be welcomed by eight attractive characters aligned in 
a table. Click one, and then the character selected will step forward and will ask for its nickname (see 
Figure 1). The character will become the user s avatar with which one can submit messages through. 2. 
How to See Conversations? Figure 2 shows a screen shot of TelMeA Theatre. TelMeA Theatre mainly consists 
of two parts: a thread frame on the right hand and a web browser on the left hand. The thread frame displays 
conversation threads that are already submitted by users of TelMeA Theatre. One can click any title of 
the threads and see conversations. The web browser will be used in some conversation to display web pages 
to be shared in TelMeA Theater. 3. How to Submit a Message? When clicking the Compose button located 
on the top of the thread frame, the TelMeA Editor opens to allow the users to design their messages. 
The Editor provides five types of behaviors for the user s avatar: speech, affective expression, interpersonal 
attitude, document reference, and comments on document. The behavior components can be used multiple 
times and arranged in any sequence for the users to create complex messages.  Background Materials Takahashi, 
T., Katagiri, H., and Bartneck, C. 2003. Show Me What You Mean Expressive Media for Online Communities, 
In Proceedings of the CHI2003 Workshop "Subtle Expressivity for Characters and Robots", Available at 
http://www.mis.atr.co.jp/ ~noriko/CHI2003/proceedingsCHI03Wrkshp.pdf. Takahashi, T. and Takeda, H. 2002. 
Proposal of a Script Language for Embodied Conversational Agents as Asynchronous Conversational Media, 
In Proceedings of AAMAS 2002, ACM Press, C. Castelfranchi and W. L. Johnson, Eds., ACM, 1387-1388. Takahashi, 
T. and Takeda, H. 2001. TelMeA: An Asynchronous Community System with Avatar-like Agents, In Proceedings 
of INTERACT 2001, IOS Press / Ohmsha, M. Hirose, Ed., IFIP, 480-487. Copyright held by the author 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	</section>
	<section>
		<section_id>965383</section_id>
		<sort_key>14</sort_key>
		<section_seq_no>14</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Development]]></section_title>
		<section_page_from>14</section_page_from>
	<article_rec>
		<article_id>965385</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[FastScript3D]]></title>
		<subtitle><![CDATA[a JavaScript companion to Java3D]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965333.965385</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965385</url>
		<abstract>
			<par><![CDATA[FastScript3D is a web-friendly companion to Java3D that makes it easy to get started using Java3D via JavaScript and HTML. The FastScript3D web site shows how you can create Java3D web content without having to be an experienced Java3D programmer.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Applets]]></kw>
			<kw><![CDATA[Java3D]]></kw>
			<kw><![CDATA[JavaScript]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P653787</person_id>
				<author_profile_id><![CDATA[81100531560]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Patti]]></first_name>
				<middle_name><![CDATA[Koenig]]></middle_name>
				<last_name><![CDATA[Koehler]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Jet Propulsion Laboratory, California Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[<u>http://fastscript3d.jpl.nasa.gov/</u>]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[<u>http://java.sun.com/products/java-media/3D/</u>]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 FastScript3D A JavaScript Companion to Java3D Patti Koenig Koehler Jet Propulsion Laboratory California 
Institute of Technology Abstract FastScript3D is a web-friendly companion to Java3D that makes it easy 
to get started using Java3D via JavaScript and HTML. The FastScript3D web site shows how you can create 
Java3D web content without having to be an experienced Java3D programmer. Keywords: Applets, Java3D, 
JavaScript 1 Introduction Web pages can be greatly enhanced with interactive 3D graphics. Learning how 
to create such content using Java3D applets requires knowledge of several inter-acting languages including 
HTML, JavaScript, Java and Java3D. Piecing all of the required knowledge together can be overwhelming 
at first. FastScript3D is a set of simplifying, high level commands designed to ease you into 3D web 
applets. The FastScript3D language consists of a series of simple commands for loading, animating and 
controlling the Java3D scenes from JavaScript. The system was designed to be easy to extend. For instance, 
it allows for loading popular 3D file formats and providing extra capabilities. To use Java3D within 
your web page, a Java3D applet is defined and embedded in the desired location on your web page. Your 
3D content is then created by constructing FastScript3D commands using JavaScript and sending them to 
and from the Java3D applet.  2 Example Figure 1. shows how FastScript3D looks as used with JavaScript. 
A simple applet is created of three 3D cubes, with the right and left cube rotating continuously. The 
FastScript3D commands are the capitalized text strings and are sent to the embedded Java3D applet called 
fs3d. The 3D model is defined beginning with the MODELCLEAR command and constructed with the MODELBUILD 
command. The NAME command creates a new component part and adds it into the scene under the desired parent 
part. The HINGE command defines an axis of rotation for the part. The JavaScript function sim creates 
an animation that continuously applies relative rotations to the left and right cube about their defined 
HINGE by three degrees. The demo function creates the model and activates the animation. <! A button 
is pressed to start the demo --> <input type="button" value="Press here to start applet" onclick="demo();"> 
<!--defines your embedded FastScript3D applet --> <applet align=middle name="fs3d" code="easy.class" 
archive="fs.jar" width="275" height="275"> <blockquote> <hr> If you were using a Java-capable browser, 
you would see the graphics window here. </hr>< /blockquote></applet> <SCRIPT LANGUAGE="JavaScript"> 
function model() { this.document.fs3d.parse("MODELCLEAR"); boxsize = 0.25; this.document.fs3d.parse("NAME 
redbox"); this.document.fs3d.parse("COLOR red"); this.document.fs3d.parse("HINGE 1 0 0"); this.document.fs3d.parse("MOVABLE"); 
this.document.fs3d.parse("OFFSET -0.5 0 0"); this.document.fs3d.parse("GEOMBOX " + boxsize); this.document.fs3d.parse("NAME 
greenbox"); this.document.fs3d.parse("COLOR green"); this.document.fs3d.parse("GEOMBOX " + boxsize); 
this.document.fs3d.parse("NAME bluebox"); this.document.fs3d.parse("HINGE 1 0 0"); this.document.fs3d.parse("MOVABLE"); 
this.document.fs3d.parse("COLOR blue"); this.document.fs3d.parse("OFFSET 0.5 0 0"); this.document.fs3d.parse("GEOMBOX 
" + boxsize); this.document.fs3d.parse("MODELBUILD"); } function sim() { this.document.fs3d.parse("SIMNAME 
movethem"); for (i=0; i<120; i++) { this.document.fs3d.parse("FNUM " + i + " RR redbox " + 3); this.document.fs3d.parse("FNUM 
" + i + " RR bluebox " + -3); } } function demo() { model(); sim(); this.document.fs3d.parse("PLAYAUTOON"); 
this.document.fs3d.parse("PLAYRUN");  } </SCRIPT> Figure 1. FastScript3D Example.  3 Conclusion FastScript3D 
enables users to create and control Java3D web page content via JavaScript and HTML. FastScript3D is 
easy to extend with new commands. The FastScript3D web site provides instructions, examples and source 
code to help you get started with Java3D web applets.  Web References http://fastscript3d.jpl.nasa.gov/ 
http://java.sun.com/products/java-media/3D/ e-mail: Patti.Koenig@jpl.nasa.gov Copyright held by the author 
 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>965384</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[gModeler.com]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965333.965384</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965384</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP309756600</person_id>
				<author_profile_id><![CDATA[81545990056]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Grant]]></first_name>
				<middle_name><![CDATA[D.]]></middle_name>
				<last_name><![CDATA[Skinner]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 gModeler.com 1. Overview gModeler has become a widely touted example of the future ofRich Internet 
Applications, due to it's success in bringing desktopfunctionality to a web delivered tool. As an online 
Unified Markup Language (UML) diagramming and documentation tool,it fuses an attractive visual interface 
with powerful data manipulation and organization capabilites. Built using FlashMXand Grant Skinner's 
proprietary FlashOS2 application framework,it sports an intuitive, responsive interface giving quick 
access toits many features. gModeler empowers developers working with ECMA-262 languages (ex. Javascript 
and FlashMX ActionScript), to plan anddocument their development projects online in a collaborativemanner, 
then export formatted documentation and stub code tofacilitate smoother, more productive coding. gModeler 
serves as both a powerful tool, and as inspiration fordevelopers creating the next generation of online 
content. 2. Background In February, 2002 Grant Skinner released FlashOS, a Flash 5based project that 
demonstrated an OS-like interface delivered viathe internet. While it was well received, limitations 
in Flash 5 keptit from being much more than a curiosity. With the release ofFlashMX, Grant began work 
on FlashOS2. Version 2 was developed as a modular GUI framework for rapidly building richInternet applications. 
Throughout the development of FlashOS2, Grant constantlysought ways to organize and document the complex 
project, butwas unable to find a tool that met his specific needs as a Flashdeveloper. As the project 
neared completion, a need arose for asample application, to test and demonstrate the features of the 
newframework in a classic chicken and the egg paradox, Grantdecided to use FlashOS2 to build a custom 
tool with which he could diagram and document FlashOS2. After a relatively short development period, 
gModeler was released for free online use in January of 2003. It combinedsimple UML class diagramming 
capabilities with robust documenting features, to facilitate a more efficient workflow forcreators of 
Rich Internet Applications. Since release, gModeler has been heralded as a leading exampleof the future 
of the World Wide Web, and has won awards at the FlashForward 2003SF Flash Film Festival, and Flash In 
The Can 2003. *e-mail: info@gskinner.com web: http://gskinner.com/  Figure 1. gModeler in action 3. 
Features A few of the major features of gModeler include: Intuitive, attractive application interface, 
incorporating standard desktop application UI elements, such as menus, tooltips, dialogs, custom cursors, 
keyboard shortcuts, drag scrolling (ie. dragging to the edge of the canvas area automatically scrolls 
the canvas), etc.  Only 110kb total load size, including embedded fonts  Completely cross platform 
and cross browser compatible  Has no reliance on back-end technology, all functionality is handled client-side. 
 Saves and manages diagrams on users' local drive using FlashMX's local shared objects  Exports HTML 
documentation, FlashMX action panel/reference panel documentation, and commented stub code.  Saves diagrams 
in XML format for sharing via email or the web, and for use with version control software.  Copyright 
held by the author  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	</section>
	<section>
		<section_id>965386</section_id>
		<sort_key>15</sort_key>
		<section_seq_no>15</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Education]]></section_title>
		<section_page_from>15</section_page_from>
	<article_rec>
		<article_id>965388</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Contents production techniques using Web3D]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965333.965388</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965388</url>
		<abstract>
			<par><![CDATA[The purpose of this research is to investigate the necessary tools and their functions under the theme of information design and communication in 3-dimensional space created by existing web3D technology. We have developed and verified "Aqua Place," a web contents on virtual aquarium containing 3-dimensional simulation by VRML within a virtual fish tank.Through this development and verification, we focused on the functions for observation in 3-dimensional space and on the method for revealing information about creatures as a database. In conclusion, it demands an operating condition that makes it possible to observe each creature individually as well as an information design of 3-dimensional ecosystem simulation in a virtual fish tank.]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P653790</person_id>
				<author_profile_id><![CDATA[81100490859]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Rina]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Takahashi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tokyo University of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P653785</person_id>
				<author_profile_id><![CDATA[81100072985]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Naoki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wakabayashi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tokyo University of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P653774</person_id>
				<author_profile_id><![CDATA[81100590134]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Kaori]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Aoki]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tokyo University of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P653798</person_id>
				<author_profile_id><![CDATA[81100364042]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Taichi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Watanabe]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tokyo University of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Contents Production Techniques using Web3D Rina Takahashi1 Naoki Wakabayashi2 Tokyo University of Tokyo 
University of Technology Technology Creative Lab. Abstract The purpose of this research is to investigate 
the necessary tools and their functions under the theme of information design and communication in 3-dimensional 
space created by existing web3D technology. We have developed and verified "Aqua Place," a web contents 
on virtual aquarium containing 3-dimensional simulation by VRML within a virtual fish tank. Through this 
development and verification, we focused on the functions for observation in 3-dimensional space and 
on the method for revealing information about creatures as a database. In conclusion, it demands an operating 
condition that makes it possible to observe each creature individually as well as an information design 
of 3-dimensional ecosystem simulation in a virtual fish tank. 1 The aim of the aqua project The first 
aim of this project was to open the aquarium to be visited freely by applying the VRML techniques for 
building up and have the original 3D space. We had focused on the educational function of the aquarium 
and have done the research and development on the new design way of information for the aquarium by applying 
the features of interactive contents. This time, on the constructing the aqua place, the virtual aquarium, 
we used VRML and network environment. 2 What aqua place is The aqua place consists of three parts; the 
first part is aquarium, the second is aqua library and the third is What s aqua place. Voyagers of the 
Ocean is a name of a fish tank in the aquarium part, and it shows the kinds of ocean migratory fish that 
travel in a high speed such as a Thunnus. The South China Sea shows the ecosystem around coral and rocky 
seabed. There you can enjoy various kinds of fish. The Indian Ocean is a pleasant tank simulated the 
environment of coral reef with small fish sailing around and between the coral. In the part of aqua library, 
we realized the function for observation on a basis of combination with Viewpoint and interactive 3D 
Web environment. The function for studying has also realized by the mutual linkage between VRML files 
and HTML files. This enables you to have more detailed observation that is difficult in the simulated 
fish tanks of aquarium part. You can pick up a fish sailing in the tank and observe its movement closely, 
if you like. With these functions, this content provide users to have an analytic observation points 
of view; for example, observe a target fish in close-up or from various angle. 1e-mail:rina@aqua.media.teu.ac.jp 
2e-mail:wak@aqua.media.teu.ac.jp 3e-mail: yomokina@aqua.media.teu.ac.jp 4e-mail: earth@aqua.media.teu.ac.jp 
Kaori Aoki3 Taichi Watanabe4 Tokyo University of Tokyo University of Technology Technology Figure 1. 
An example of functions for observation A creature in South China Sea and its linked detailed explanation 
 3 Developed utilities In the case of modeling multiple kinds of models such as a fish tank, including 
fish, coral and rocks, it is necessary to have a huge data of enormous numbers of polygons to represent 
their appearance. However, huge data causes difficulty to execute real time rendering process with VRML 
browsers. We have developed aqua pallet, the utility software to reduce the amount of polygons and to 
optimize the form of the polygon. It is also possible to define the texture of models with this utility. 
In the shape transformation, it is generally used the techniques of a bone animation, or a skeleton animation, 
to define the frame of a shape and show movement of transformed outward appearance along with the transformed 
movement of the frame. However, it is almost impossible to define the frame movement for the modeling 
in the VRML environment, so we developed a utility based on FFD (Free Form Deformation) method for the 
shape transformation. With this utility, we represented the movement of pectoral and tail fins of a fish 
and simulated the unique movement of sailing fish. 4 "aqua place" as a learning environment In the 3D 
simulated virtual fish tank, it is possible to have a spatial points of view as if the user dived in 
the sea. Following are the points of view and the ways for observation utilizing the 3D space: 1) observation 
from the various angle, 2) Dynamic and interactive observation points of view and 3) points of view for 
experiencing the tracking or exploring. Simulating the biological aspects and/or inhabiting ecosystem 
makes it possible to realize the interactive functions as exploring, observation or discovery. This is 
the point expected to lead us to develop an experimental learning environment within a virtual space. 
Not only for studying and observation by oneself, it is also expected to form a learning community by 
communicating with someone on that space. It would be effective to motivate users to learn. Copyright 
held by the author  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>965390</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Processing]]></title>
		<subtitle><![CDATA[a learning environment for creating interactive Web graphics]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965333.965390</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965390</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP37027875</person_id>
				<author_profile_id><![CDATA[81100463130]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Casey]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Reas]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Interaction Design Institute Ivrea, Italy]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP38026245</person_id>
				<author_profile_id><![CDATA[81100613376]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Benjamin]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Fry]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MIT Media Laboratory]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Processing: A Learning Environment for Creating Interactive Web Graphics Casey Reas Interaction Design 
Institute Ivrea, Italy c.reas@interaction-ivrea.it Introduction The Processing project introduces a 
new audience to computer programming and encourages an audience of hybrid artist/ designer/programmers. 
It integrates a programming language, development environment, and teaching methodology into a unified 
structure for learning. Its goal is to introduce program­ming in the context of electronic art and to 
open electronic art concepts to a programming audience. Unlike other popular web programming environments 
such as Flash and Director, Pro­cessing is an extension of Java and supports many of the exist­ing Java 
structures, but with a simplified syntax. The application runs locally and exports programs to Java applets, 
which may be viewed over the Internet. It is not a commercial production tool, but is build specifically 
for learning and prototyping. Concept Graphical user interfaces became mainstream nearly twenty years 
ago, but programming fundamentals are still primarily taught through the command line interface. Classes 
proceed from outputting text to the screen, to GUI, to computer graph­ics (if at all). It is possible 
to teach programming in a way that moves graphics and concepts of interaction closer to the surface. 
Making exercises created during learning viewable over the web supports the creation of a global educational 
community and provides motivation for learning. A view source method of programming enables the community 
to learn from each other. The concept of Processing is to create a text programming language specifically 
for making responsive images, rather than creating a visual programming language. The language enables 
sophisticated visual and responsive structures and has a bal­ance between features and ease of use. Many 
computer graphics and interaction techniques can be discussed including vector/ raster drawing, 2D/3D 
transformations, image processing, color models, events, network communication, information visualiza­tion, 
etc. Processing shifts the focus of programming away from technical details like threading and double-buffering 
and places emphasis on communication. Figure 1: Example images created with Processing.  Copyright 
held by the author Benjamin Fry MIT Media Laboratory fry@media.mit.edu  Programming Language/Environment 
Processing is a Java environment which translates programs written in its own syntax into Java code and 
then compiles to executable Java Applet 1.1 byte code. It includes a custom 2D/3D engine inspired by 
PostScript and OpenGL. The software is free to use and the source code will be made public. It runs on 
Windows, Mac OS X, Mac OS 9, and Linux and the software is currently in Alpha release. The Beta release 
is scheduled for Summer 2003. Processing Version 1.0 focuses on teaching basic concepts of interactive 
networked computer graphics. Processing provides three different modes of program­ming each one more 
structurally complex than the previous. In the most basic mode, programs are single line commands for 
drawing primitive shapes to the screen. In the most complex mode, Java code may be written within the 
environment. The intermediate mode allows for the creation of dynamic software in a hybrid procedural/object-oriented 
structure. It strives to achieve a balance between features and clarity, which encourages the experimentation 
process and reduces the learning curve. Skills learned through Processing enable people to learn lan­guages 
suitable for different contexts including web authoring (ActionScript), networking and communications 
(Java), micro­controllers (C), and computer graphics (OpenGL). Networked Learning The Processing website 
houses a set of extended examples and a complete reference for the language. Hundreds of students, educators, 
and practitioners across five continents are involved in using the software. An active online discussion 
board is a platform for discussing individual programs and future soft­ware additions to the project. 
The software has been used at diverse universities and institutions in cities including: Boston, New 
York, San Fransisco, London, Paris, Oslo, Basel, Brussels, Berlin, Bogota (Colombia), Ivrea (Italy), 
Manila, and Tokyo. See also: http://www.proce55ing.net  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>965389</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Bowen virtual theater]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965333.965389</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965389</url>
		<abstract>
			<par><![CDATA[A web-based virtual theater was created to provide students learning introductory theatrical concepts with a more engaging means of exploring the possibilities of set design, lighting, prop placement, and actor blocking. Web delivery without plug-ins allows a very large number of students to access the site with minimal support requirements. The system can be used as an experimentation lab, a design tool, and an in-class instructional system for illustrating concepts.]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP30028393</person_id>
				<author_profile_id><![CDATA[81339512248]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Matthew]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lewis]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Ohio State University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[ACCAD. 2003. Bowen Virtual Theater Web Site. <u>http://www.accad.ohio-state.edu/VT/</u>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[BEITLER, M. 2003. H-anim Working Group Web Site. http://h-anim.org/.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>557664</ref_obj_id>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[POLEVOI, R. 2000. Interactive Web Graphics with Shout3D. Sybex.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Bowen Virtual Theater Matthew Lewis* Advanced Computing Center for the Arts and Design Ohio State University 
 Abstract A web-based virtual theater was created to provide studentslearning introductory theatrical 
concepts with a more engagingmeans of exploring the possibilities of set design, lighting, propplacement, 
and actor blocking. Web delivery without plug-insallows a very large number of students to access the 
site withminimal support requirements. The system can be used as anexperimentation lab, a design tool, 
and an in-class instructionalsystem for illustrating concepts. 1 Introduction The Bowen Virtual Theater 
project is the result of collaboration between the Advanced Computing Center for the Arts and Design 
(ACCAD) and the Department of Theater, at The Ohio State University. The Theater Department teaches a 
broad Introduction to Theater course each quarter, with total enrollment in all sections reaching as 
high as one thousand students. As an alternative to the large lecture presentations this necessitates, 
and also as an investigation into the possibilities of distance education, the Bowen Virtual Theater 
was created to allow students to explore basic concepts and issues of theater. The project also allowed 
us to produce an infrastructure for delivering motion capture driven web3d human animation in virtual 
environments. 2 Technology and Content Development The first stage of the project was focused on the 
basics of setdesign. Different design styles can be chosen for each selectedplay. Students can see how 
a classical design might differ from amore surreal design solution, for example. Additionally, props 
can Figure 1. Virtual Theater Interface. be selected, positioned, and oriented via direct manipulation 
in the3D window. Basic lighting with intensity, color, and directionThis project received funding from 
the Technology Enhancedattributes can also be adjusted. Navigation in the space isLearning and Research 
(TELR) group of The Ohio Stateaccomplished by either manipulating the ends of a view arrow inUniversity, 
and was the product of the effort of many people, butthe 2D overhead window, or by dragging in the 3D 
window. especially Katie Whitlock, Jon Woodring, and ArunStandard walk, fly, and look modes are available. 
Somasundaram. Java-based Shout3D [Polevoi 2000] libraries were used to eliminate the need for students 
and public campus labs to   References download and install a 3D plug-in. Using Java also allowed us 
towrap our 3D content in a custom interface. All geometry wasACCAD. 2003. Bowen Virtual Theater Web Site. 
modeled in Maya and exported as VRML. http://www.accad.ohio-state.edu/VT/. The second stage of the project 
saw the addition of charactersdriven by motion capture data. Students select from differentBEITLER, M. 
2003. H-anim Working Group Web Site. virtual actors and block their positions and actions over time.http://h-anim.org/. 
Preliminary work was begun for coordinating scripted dialog withactions as well. H-anim [Beitler 2003] 
human figures werePOLEVOI, R. 2000. Interactive Web Graphics with Shout3D. authored in Maya, and our 
in-house optical motion capture systemSybex. was used to produce a wide range of motion data. *e-mail:lewis.239@osu.edu 
  Copyright held by the author 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>965387</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[Safe as mother's milk]]></title>
		<subtitle><![CDATA[the Hanford project]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965333.965387</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965387</url>
		<abstract>
			<par><![CDATA[<i>Safe As Mother's Milk: The Hanford Project</i> is a web site and physical installation exploring the atomic history of the Hanford Nuclear Reservation. For more than forty years, Hanford released radioactive materials into the environment on an uninformed public while producing plutonium for America's nuclear arsenal during the Cold War era. The project incorporates recently declassified documents and historical photographs available online through Hanford Declassified Document Retrieval System.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Hanford]]></kw>
			<kw><![CDATA[Manhattan Project]]></kw>
			<kw><![CDATA[cold war]]></kw>
			<kw><![CDATA[documentary]]></kw>
			<kw><![CDATA[education resource]]></kw>
			<kw><![CDATA[plutonium]]></kw>
			<kw><![CDATA[radiation]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP31042069</person_id>
				<author_profile_id><![CDATA[81100432752]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Kim]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Stringfellow]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Safe As Mother s Milk: The Hanford Project Kim Stringfellow Abstract Safe As Mother's Milk: The Hanford 
Project is a web site and physical installation exploring the atomic history of the Hanford Nuclear Reservation. 
For more than forty years, Hanford released radioactive materials into the environment on an uninformed 
public while producing plutonium for America's nuclear arsenal during the Cold War era. The project incorporates 
recently declassified documents and historical photographs available online through Hanford Declassified 
Document Retrieval System. Keywords: radiation, plutonium, documentary, education resource, Hanford, 
Cold War, Manhattan Project 1 Introduction The Hanford Nuclear Reservation is located on 565-square-miles 
of desert in southeastern Washington State near the Tri-Cities area of Richland, Pasco and Kennewick. 
For more than forty years, Hanford released radioactive materials into the environment on an uninformed 
public while producing plutonium for the U.S. nuclear arsenal during the Cold War era. Although the majority 
of the releases were due to activities related to production, some were also planned and intentional. 
Hanford workers, their families and other downwind residents became literal guinea pigs for radiation 
experiments that were carried out at the facility by the former Atomic Energy Commission (AEC), Department 
of Energy (DOE), the Department of Defense, and civilian sub-contractors including DuPont and General 
Electric from 1944 to 1972. Although civilians were informed of Hanford's plutonium production activities 
by the end of World War II, officials in charge kept secret the growing number of radioactive releases, 
experiments and other environmental safety hazards resulting at the facility. During the mid-1980s, increasing 
public suspicion over Hanford activities forced government agencies and their civilian sub-contractors 
to release formally classified documents through a request under the Freedom of Information Act. With 
the release of these documents in 1986, the public has been able to piece together a devastating chronicle 
of atomic weaponry production that consequently poisoned the people it was ironically meant to protect. 
Thousands of area residents from towns and farms surrounding the Hanford Site and beyond have suffered 
an array of health problems including thyroid cancers, autoimmune diseases and reproductive disorders 
that they feel are the direct result of these releases and experiments. Safe as Mother's Milk examines 
these important events through declassified historical photographs, media and documents available online 
at various government archives, including the Hanford Declassified Document Retrieval System [http://www2.hanford.gov/declass/] 
and Human Radiation Experiments Information Management System (HREX) [http://tis.eh.doe.gov/ohre/]. This 
project is designed as both a physical installation and Website, originally commissioned for the Cornish 
College of the ArtsART | ACTIVISM 2002 Visiting Artist Series. Figure 1. Downwinders screen shot from 
the web site.  2 Exposition Safe As Mother's Milk: The Hanford Project is an unofficial documentary 
of the Hanford site seen through historic declassified documents, photography and other media. The web 
site contains rigorously researched information regarding the area s history and is organized into three 
interrelated categories: Background, Releases and Repercussions. Historic photographs and news reels 
conceptualize the information in a visual manner to subtly expose deceptive activities carried out by 
the U.S. government and its civilian sub-contractors in the name of National Security at the Hanford 
site for over a forty year period. The web site also includes a visitor s forum, glossary terms and important 
links for further research. The site was created with Macromedia s Flash MX and incorporates new features 
of the program such as embedded videoclips. The site also features an interactive timeline. Author/Producer: 
Kim StringfellowE-mail: mail@kimstringfellow.com Web site url: http://www.kimstringfellow.com/hanford/ 
 Copyright held by the author  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	</section>
	<section>
		<section_id>965391</section_id>
		<sort_key>16</sort_key>
		<section_seq_no>16</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Visualization]]></section_title>
		<section_page_from>16</section_page_from>
	<article_rec>
		<article_id>965393</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Interactive 3D gene expression viewer]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965333.965393</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965393</url>
		<abstract>
			<par><![CDATA[Gene expression can be visualized in three dimensions using volume data generated through confocal microscopy. The Interactive Gene Expression viewer provides a way to view spatial relationships between different gene expression patterns and anatomic features. Web based 3D enabled technologies such as the Interactive Gene Expression viewer help to facilitate the analysis of 3D gene expression patterns and the creation of bioinformatics databases that can use this data to predict gene interaction and function.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[gene expression]]></kw>
			<kw><![CDATA[scientific visualization]]></kw>
			<kw><![CDATA[surface reconstruction]]></kw>
			<kw><![CDATA[volume data]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P653802</person_id>
				<author_profile_id><![CDATA[81100329241]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Victor]]></first_name>
				<middle_name><![CDATA[E.]]></middle_name>
				<last_name><![CDATA[Gerth]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Calgary]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P653789</person_id>
				<author_profile_id><![CDATA[81100642890]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Peter]]></first_name>
				<middle_name><![CDATA[D.]]></middle_name>
				<last_name><![CDATA[Vize]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Calgary]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[WALSER, PETER. http://www.idx3d.ch/idx3d/idx3d.html]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Interactive 3D Gene Expression Viewer Victor E. Gerth* University of Calgary, Department of Biological 
Sciences  Abstract Gene expression can be visualized in three dimensions using volume data generated 
through confocal microscopy. The Interactive Gene Expression viewer provides a way to view spatial relationships 
between different gene expression patterns and anatomic features. Web based 3D enabled technologies such 
as the Interactive Gene Expression viewer help to facilitate the analysis of 3D gene expression patterns 
and the creation of bioinformatics databases that can use this data to predict gene interaction and function. 
 Keywords: gene expression, scientific visualization, volume data, surface reconstruction 1 Introduction 
The Interactive 3D Gene Expression Viewer manipulates surface reconstructions of volume data, allowing 
a user to interactively rotate and move their viewpoint or the model using our specially tailored interface. 
Most surface reconstructions (or models) are separated into several objects. Some surface reconstructions 
contain two channels of information. Currently, all models in our collection have been obtained through 
confocal microscopy or micro CT scans. Our 3D Gene Expression Viewer utilizes the Java 1.1 compliant 
IDX3D API [Walser 2000] to create 3D projections of surface models. Since, all projections are created 
through software rendering the viewer maintains usability across many different computing platforms and 
only requires a JAVA enabled web browser. Current revisions to the Interactive 3D Gene Expression Viewer 
are maintained at http://cbrbio.ucalgary.ca/3DModels/.  2 Exposition The interface for the Interactive 
3D Gene Expression Viewer was designed to allow data stored on separate channels to be easily adjusted 
by channel association or independently. Currently, the viewer allows for individual objects, which compose 
the overall model, to have independent color, opacity, and wire frame attributes. Channel modification 
supports opacity and visibility. Color and opacity are controlled by user adjustable sliders. The viewer 
application tracks the last object the user selected, and in this manner users can adjust the appearance 
and presence of an object in the scene by adjusting the sliders, buttons, and checkboxes listed under 
the model name. Distances can be measured between the minutest of surface definitions by activating the 
interaction type to measure in the provided list box, then dragging the cursor between the two desired 
points. Each end point is highlighted and a solid line depicting the measured region is displayed, along 
with the calculated distance. *e-mail: gerth@ucalgary.ca e-mail: pvize@ucalgary.ca  Copyright held 
by the author Peter D. Vize University of Calgary, Department of Biological Sciences  In manipulate 
mode the viewer can rotate and move the scene or objects by clicking and dragging in the model view port. 
Rotation occurs when the navigation list box is set to rotation, and the user clicks and drags in the 
desired direction of rotation. Likewise, the user can control positioning with the move setting in the 
navigation list box. Positional information towards and away from the user is controlled through the 
arc zoom slider. By clicking and dragging on the Arc zoom slider in a vertical motion the user can increase 
and decrease positional increments towards or away from the user. Within the navigation feature a targeting 
mode exists. The targeting list box determines whether the viewpoint or the currently selected element 
will be manipulated through navigation interactions. The element setting for targeting allows the user 
to alter how elements are assembled in data sets. Typing in the desired data set filename into the text 
field provided can access 3D data sets stored on the web server. Clicking on the load file or merge file 
button allows the user to either completely replace or merge the currently viewed data set with another 
data set. Figure 1. The Interactive 3D Gene Expression Viewer. 3 Conclusion By providing an interface 
for visualizing and comparing 3D structures relating to gene expression and anatomy, the Internet based 
nature of the 3D Gene Expression Viewer allows a network friendly method for visualizing 3D anatomical 
and gene expression databases. Ultimately our tool addresses the need for a manner of sharing and comparing 
spatial relations existing within developing organisms.  References WALSER, PETER . http://www.idx3d.ch/idx3d/idx3d.html 
 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>965392</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[World-wide gallery for pseudo-3D photo collage]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965333.965392</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965392</url>
		<abstract>
			<par><![CDATA["Pseudo-3D Photo Collage", one of new wed3D techniques, is based on multiple photographs and <i>Spatial-Hyperlinks</i>. A user can design various patterns of <i>HyperPhoto-Networks</i>, for example, <i>linear, loop, tree, grid, rhizome</i> and so on. It is similar to design a website based on <i>HyperText-Networks</i>. We propose several design patterns and those applications to create highly-designed virtual sequences.]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP35024783</person_id>
				<author_profile_id><![CDATA[81100135119]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Hiroya]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tanaka]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[PhotoWalker Development Group, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P653797</person_id>
				<author_profile_id><![CDATA[81100522622]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Shohei]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Matsukawa]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[PhotoWalker Development Group, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP309606900</person_id>
				<author_profile_id><![CDATA[81536499956]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Akira]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wakita]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[PhotoWalker Development Group, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P653775</person_id>
				<author_profile_id><![CDATA[81100199964]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Kaoru]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Misaki]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[PhotoWalker Development Group, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P653801</person_id>
				<author_profile_id><![CDATA[81100652649]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Tom]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Saito]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[PhotoWalker Development Group, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P653772</person_id>
				<author_profile_id><![CDATA[81100600232]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Hiroki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ito]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[PhotoWalker Development Group, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1242325</ref_obj_id>
				<ref_obj_pid>1242073</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Hiroya Tanaka, Masatoshi Arikawa, Ryosuke Shibasaki: Pseudo-3D Photo Collage, Siggraph2002 Conference Abstracts and Applications, pp.317.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[http://www.photowalker.net/]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 World-Wide Gallery for Pseudo-3D Photo Collage Hiroya Tanaka, Shohei Matsukawa, Akira Wakita, Kaoru 
Misaki, Tom Saito and Hiroki Ito PhotoWalker Development Group, Japan. Abstract Pseudo-3D Photo Collage 
, one of new wed3D techniques, is based on multiple photographs and Spatial-Hyperlinks. A user can design 
various patterns of HyperPhoto-Networks, for example, linear, loop, tree, grid, rhizome and so on. It 
is similar to design a website based on HyperText-Networks. We propose several design patterns and those 
applications to create highly­designed virtual sequences. 1. Introduction At the Siggraph2002 conference, 
we introduced a new web3D Figure3: HyperPhoto-Networkstechnique named Pseudo-3D Photo Collage [1]. This 
is one of image-based approaches for creating and browsing virtual space on the web. This virtual space 
can be created by simple operations as follows. Figure4 Animations for traversing one photo to another 
 (1) A user draws a rectangle in a set of photographs to indicate one corresponding area (Figure.1) 
(2) The system generates a Spatial-Hyperlink, which associates 2. World-Wide Gallery two photographs 
with a corresponding area (Figure.2). We published our collage software, both a making tool and a (3) 
A user can create HyperPhoto-Networks composed of many browsing tool, as free software onto the web. 
246 people  photographs and Spatial-Hyperlinks (Figure.3). downloaded our software, and created and 
published virtual space. (4) When user wants to browse HyperPhoto-Networks, the We are now collecting 
various samples of virtual space by using  system ca alculates a geometric transformation and overlap 
our system. One content is from China and another is from two photographs. A user can traverse one photo 
to another England. We are also making and managing a portal with morphing animations (Figure.4). website[2].You 
can navigate attractive virtual spaces in the world. Figure2: A set of Photographs associated with a 
Spatial Hyperlink e-mail:info@photowalker.net This research is partly supported by the research for the 
grant of Scientific Research (15017249) from Ministry of Education, Culture, Sports, Science and Technology 
of Japan. We would like to thank Prof. Katsumi Tanaka (Kyoto University) for valuable advice. Copyright 
held by the author  Figure1: Corresponding rectangles  Figure5 : World map for Pseudo-3D Photo Collage. 
3. References [1] Hiroya Tanaka, Masatoshi Arikawa, Ryosuke Shibasaki: Pseudo-3D Photo Collage, Siggraph2002 
Conference Abstracts and Applications, pp.317. [2] http://www.photowalker.net/  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>965394</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Open source life project]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965333.965394</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965394</url>
		<abstract>
			<par><![CDATA[The experimental new media group - Nomads & Homesteaders explore biotechnology, artificial life systems and simulation through, the <i>Open Source Life</i> (OSL) project.OSL participants create artificially intelligent plants, designed to adapt to life in a changing environment. Once created, the virtual plant's survival is the responsibility of its "creator." Plant-like avatars represent participants, who are also provided interactive controls to promote a sense of community, resource sharing and collaboration. Outside the context of the physical installation, plants and the environment are tended to by virtual visits to the garden via the World Wide Web.This code-based environment uses open source technology in the creation of the project with hopes that it will continue to be built upon by open source developers. Nomads and Homesteaders wrote the engine of OSL in Python so that the plants can continuously grow on the Apache server. OSL also uses MySQL and PHP to store and retrieve each of the individual plant information. The interactive site is then displayed with Flash where the plants are then presented in a 3D world where the users can view their plant amongst others in the environment.Open Source Life is a collaborative project produced by the Nomads & Homesteaders: Beth Cerny, Diane Figueredo, Joey Lindsey, Flo McGarrell and Daniel Romano, with the support of Jon Cates, Ben Chang, Claire Pentecost; the Film, Video and New Media and Art and Technology Studies departments at SAIC; and the UIC Electronic Visualization Laboratory.]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P653751</person_id>
				<author_profile_id><![CDATA[81100583912]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Beth]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Cerny]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Nomads & Homesteaders]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P653763</person_id>
				<author_profile_id><![CDATA[81100266358]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Diane]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Figueredo]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Nomads & Homesteaders]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P653756</person_id>
				<author_profile_id><![CDATA[81100514789]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Chris]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Keating]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Nomads & Homesteaders]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P653773</person_id>
				<author_profile_id><![CDATA[81100048664]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Joey]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lindsey]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Nomads & Homesteaders]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P653766</person_id>
				<author_profile_id><![CDATA[81100521708]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Flo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[McGarrell]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Nomads & Homesteaders]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P653759</person_id>
				<author_profile_id><![CDATA[81100602489]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Daniel]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Romano]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Nomads & Homesteaders]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Open Source Life Project Nomads &#38; Homesteaders Abstract The experimental new media group - Nomads 
&#38; Homesteaders explore biotechnology, artificial life systems and simulation through, the Open Source 
Life (OSL) project. OSL participants create artificially intelligent plants, designed to adapt to life 
in a changing environment. Once created, the virtual plant's survival is the responsibility of its creator. 
Plant-like avatars represent participants, who are also provided interactive controls to promote a sense 
of community, resource sharing and collaboration. Outside the context of the physical installation, plants 
and the environment are tended to by virtual visits to the garden via the World Wide Web. This code-based 
environment uses open source technology in the creation of the project with hopes that it will continue 
to be built upon by open source developers. Nomads and Homesteaders wrote the engine of OSL in Python 
so that the plants can continuously grow on the Apache server. OSL also uses MySQL and PHP to store and 
retrieve each of the individual plant information. The interactive site is then displayed with Flash 
where the plants are then presented in a 3D world where the users can view their plant amongst others 
in the environment. * http://www.nomadsandhomesteaders.org/OSL/index.php Image Credit: Brenda Lopez-Silva 
Open Source Life is a collaborative project produced by the Nomads &#38; Homesteaders: Beth Cerny, Diane 
Figueredo, Joey Lindsey, Flo McGarrell and Daniel Romano, with the support of Jon Cates, Ben Chang, Claire 
Pentecost; the Film, Video and New Media and Art and Technology Studies departments at SAIC; and the 
UIC Electronic Visualization Laboratory. Copyright held by the author 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	</section>
	<article_rec>
		<article_id>965335</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Origins London-Belize Website]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965333.965335</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965335</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P243883</person_id>
				<author_profile_id><![CDATA[81100180433]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Rob]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Semper]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Exploratorium]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P653782</person_id>
				<author_profile_id><![CDATA[81100502515]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Melissa]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Alexander]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Exploratorium]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P653791</person_id>
				<author_profile_id><![CDATA[81100484044]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Robin]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Marks]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Exploratorium]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Origins London-Belize Website Rob Semper, Principal Investigator Melissa Alexander, Project Director 
Robin Marks, Web Site Producer Exploratorium Exploratorium Exploratorium Introduction Evolutionary biology 
is a tangible science: Researchers use their eyes, ears, and minds to observe, hold, and compare living 
things. We wanted our Web site, From Jungle to Lab: The Study of Life s Complexity, to bring that tangible 
experience to our audience, allowing them to see into the places where the work is done and talk directly 
with scientists. Through photos and interviews with researchers, the site follows the story of two organisms 
one insect and one plant as they are gathered from Las Cuevas Research Station in Belize, packed for 
transportation to England, and analyzed using the sophisticated tools available at the Natural History 
Museum in London. Visitors to the site meet the researchers studying these organisms (People section), 
see the tools they use in their work (Tools section), and learn how they use new knowledge to tell us 
more about the evolutionary tree connecting all living things (Ideas section). It s our Webcasts (Live 
section) and our interactive panoramas (Place section) that make this Web site a truly visceral experience. 
The live Webcasts bring visitors into real science experiences: In a special Halloween Webcast, bat researchers 
join us live from both London and Belize, and viewers online and in our museum ask questions as the scientists 
bring us up-close-and-personal with their subjects. In another Webcast, viewers watch a live feed from 
a scanning electron microscope as a pollen researcher gets a first­time look at a new species. The Place 
section features a series of interconnected 360-degree views with hot spots. These links lead to pages 
that give a sense of the research environment on many levels, using audio and video clips, as well as 
text and photos. The site s elements combine to give visitors a being there sense of life as a research 
biologist. Email: Rob Semper: robs@exploratorium.edu Melissa Alexander: melissaa@exploratorium.edu Robin 
Marks robinm@exploratorium.edu Site location: Exploratorium: www.exploratorium.edu/origins/belize-london/ 
 Image 1 : Screenshot of the entry page to Origins London-Belize Credits Rob Semper, Principal Investigator 
Melissa Alexander, Project Director Robin Marks, Web Site Producer Lowell Robinson, Web Designer Susan 
Schwartzenberg, Photographer Julie Konop, Video Producer Noel Wanner, Webcast Producer Steve Kearsley, 
Illustrations Writers: Pearl Tesler Zach Tobias Martha Steele Web developers: Robin Marks Sarah Reiwitch 
Jenny Villigran Janet Fouts Copyright held by the author 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>965334</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Seeing collection Website]]></title>
		<subtitle><![CDATA[exploratorium]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965333.965334</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965334</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P653777</person_id>
				<author_profile_id><![CDATA[81100382327]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Kathleen]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Mclean]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Co-Principal Investigator]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P653799</person_id>
				<author_profile_id><![CDATA[81100290445]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Thomas]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Humphry]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Co-Principal Investigator]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Creative Growth Art Center (Oakland, CA)]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Golden Gate National Recreation Area]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Rose Resnick LightHouse for the Blind and Visually Impaired (San Francisco, CA)]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Pacific Film Archive (UC Berkeley)]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[San Francisco Museum of Modern Art (SFMOMA)]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[The East Bay Center for the Performing Arts (Richmond, CA)]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 SEEING COLLECTION WEBSITE Exploratorium http://www.exploratorium.edu/seeing Kathleen Mclean Thomas Humphry 
Co-Principal Investigator Co-Principal Investigator Introduction We wanted the Seeing Web site to be 
more than simply an online supplement to the interactive exhibit collection: We attempted to create a 
site providing opportunities to experience and explore visual phenomena as visitors would on the exhibit 
floor, as well as presenting factual information about the collection and the visual system. The site 
contains an outline of the collection s exhibits, including an overview of the primary content areas 
of the collection (Sensing Light, Seeing Color/Depth/Motion, Seeing and Attention, Seeing in Context, 
and Interpreting Images) and an interactive map of the exhibit floor. Content areas are illustrated with 
images and descriptions of key exhibits exploring fundamental ideas in vision research and their relationships 
to real-world phenomena. The site also offers a comprehensive collection of links to external locations 
exploring visual illusions, the anatomy and physiology of both human and animal visual systems, and the 
visual arts. In addition, however, a core part of the site s design revolves around an extensive Web-based 
exhibit entitled Not Fade Away. This exhibit investigates vision not by presenting abstract visual experiences 
but by providing an intimate connection with Joel Deutsch, a writer experiencing a gradual loss of his 
ability to see. This exhibit features conversations, anecdotes, and journal entries from Joel, as well 
as a description and simulation of how retinitis pigmentosa is inexorably destroying his vision. The 
site also contains several virtual exhibits especially suited to online display exhibits investigating 
color perception and visual contrast effects, for example allowing site visitors to have some of the 
kinds of interactive experiences open to Museum visitors. Finally, dynamic visual experiences (gradually-changing 
images and backgrounds combined with noticing prompts) are incorporated into the design of the site s 
pages.  References Creative Growth Art Center (Oakland, CA) Golden Gate National Recreation Area Rose 
Resnick LightHouse for the Blind and Visually Impaired (San Francisco, CA) Pacific Film Archive (UC Berkeley) 
San Francisco Museum of Modern Art (SFMOMA) The East Bay Center for the Performing Arts (Richmond, CA) 
 Credits Hugh McDonald Senior Science Writer Charles Carlson Content Development Richard Brown Content 
Development Susan Schwartzenburg Media  Email: Kathleen Mclean kmclean@exploratorium.edu Thomas Humphry 
tomh@exploratorium.edu Copyright held by the author 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
</content>
</proceeding>
