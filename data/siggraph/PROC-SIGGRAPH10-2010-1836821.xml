<?xml version="1.0" encoding="ISO-8859-1"?>
<!DOCTYPE proceeding SYSTEM "proceeding.dtd">
<proceeding ver="6.0" ts="04/10/2010">
<conference_rec>
	<conference_date>
		<start_date>07/26/2010</start_date>
		<end_date>07/30/2010</end_date>
	</conference_date>
	<conference_loc>
		<city><![CDATA[Los Angeles]]></city>
		<state>California</state>
		<country></country>
	</conference_loc>
	<conference_url></conference_url>
</conference_rec>
<series_rec>
	<series_name>
		<series_id>SERIES382</series_id>
		<series_title><![CDATA[International Conference on Computer Graphics and Interactive Techniques]]></series_title>
		<series_vol></series_vol>
	</series_name>
</series_rec>
<proceeding_rec>
	<proc_id>1836821</proc_id>
	<acronym>SIGGRAPH '10</acronym>
	<proc_desc>ACM SIGGRAPH 2010 Emerging Technologies</proc_desc>
	<conference_number>2010</conference_number>
	<proc_class>conference</proc_class>
	<proc_title></proc_title>
	<proc_subtitle></proc_subtitle>
	<proc_volume_no></proc_volume_no>
	<isbn13>978-1-4503-0392-7</isbn13>
	<issn></issn>
	<eissn></eissn>
	<copyright_year>2010</copyright_year>
	<publication_date>07-26-2010</publication_date>
	<pages>23</pages>
	<plus_pages></plus_pages>
	<price><![CDATA[]]></price>
	<other_source></other_source>
	<abstract>
		<par><![CDATA[<p>Interactive demonstrations of innovative technologies in a broad range of applications, including displays, robotics, input devices, and interaction techniques. Emerging Technologies includes a mix of works invited by the organizers and works selected from juried submissions.</p>]]></par>
	</abstract>
	<publisher>
		<publisher_id>PUB27</publisher_id>
		<publisher_code>ACMNY</publisher_code>
		<publisher_name>ACM</publisher_name>
		<publisher_address>2 Penn Plaza, Suite 701</publisher_address>
		<publisher_city>New York</publisher_city>
		<publisher_state>NY</publisher_state>
		<publisher_country>USA</publisher_country>
		<publisher_zip_code>10121-0701</publisher_zip_code>
		<publisher_contact>Bernard Rous</publisher_contact>
		<publisher_phone>212 869-7440</publisher_phone>
		<publisher_isbn_prefix></publisher_isbn_prefix>
		<publisher_url>www.acm.org/publications</publisher_url>
	</publisher>
	<sponsor_rec>
		<sponsor>
			<sponsor_id>SP932</sponsor_id>
			<sponsor_name>ACM Special Interest Group on Computer Graphics and Interactive Techniques</sponsor_name>
			<sponsor_abbr>SIGGRAPH</sponsor_abbr>
		</sponsor>
	</sponsor_rec>
	<categories>
		<primary_category>
			<cat_node/>
			<descriptor/>
			<type/>
		</primary_category>
	</categories>
	<ccc>
		<copyright_holder>
			<copyright_holder_name>ACM</copyright_holder_name>
			<copyright_holder_year>2010</copyright_holder_year>
		</copyright_holder>
	</ccc>
</proceeding_rec>
<content>
	<article_rec>
		<article_id>1836822</article_id>
		<sort_key>10</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>1</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[360-degree autostereoscopic display]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836821.1836822</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1836822</url>
		<abstract>
			<par><![CDATA[<p>A volumetric 3D display has been a motif in many science fiction movies, and is the very image of futuristic technology. We have developed a prototype 360-degree autostereoscopic display. This display allows us to view full-color volumetric objects from all angles, as if the objects really exist. This display uses special LED light sources, and it can show 360 unique images to all directions in one-degree separations. We can sense the depth of the displayed object, because our left and right eyes are seeing different images. No special 3D glasses are needed to see the 3D image.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061.10010063</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures->Computational geometry</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2264104</person_id>
				<author_profile_id><![CDATA[81466645276]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Katsuhisa]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ito]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Sony Corporation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264106</person_id>
				<author_profile_id><![CDATA[81466645182]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Hiroki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kikuchi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Sony Corporation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264107</person_id>
				<author_profile_id><![CDATA[81466642428]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Hisao]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sakurai]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Sony Corporation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264108</person_id>
				<author_profile_id><![CDATA[81466645627]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Izushi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kobayashi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Sony Corporation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264109</person_id>
				<author_profile_id><![CDATA[81466642853]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Hiroaki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yasunaga]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Sony Corporation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264110</person_id>
				<author_profile_id><![CDATA[81466641824]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Hidenori]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Mori]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Sony Corporation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264111</person_id>
				<author_profile_id><![CDATA[81466647721]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>7</seq_no>
				<first_name><![CDATA[Kazutatsu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tokuyama]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Sony Corporation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264112</person_id>
				<author_profile_id><![CDATA[81466641162]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>8</seq_no>
				<first_name><![CDATA[Hirotaka]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ishikawa]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Sony Corporation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264113</person_id>
				<author_profile_id><![CDATA[81466648172]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>9</seq_no>
				<first_name><![CDATA[Kengo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hayasaka]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Sony Corporation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264105</person_id>
				<author_profile_id><![CDATA[81466643461]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>10</seq_no>
				<first_name><![CDATA[Hiroyuki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yanagisawa]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Sony Corporation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 360-degree Autostereoscopic Display Katsuhisa Ito, Hiroki Kikuchi, Hisao Sakurai, Izushi Kobayashi, 
Hiroaki Yasunaga, Hidenori Mori, Kazutatsu Tokuyama, Hirotaka Ishikawa, Kengo Hayasaka and Hiroyuki Yanagisawa 
Sony Corporation A volumetric 3D display has been a motif in many science fiction movies, and is the 
very image of futuristic technology. We have developed a prototype 360-degree autostereoscopic display. 
This display allows us to view full-color volumetric objects from all angles, as if the objects really 
exist. This display uses special LED light sources, and it can show 360 unique images to all directions 
in one-degree separations. We can sense the depth of the displayed object, because our left and right 
eyes are seeing different images. No special 3D glasses are needed to see the 3D image. The cylindrical 
display measures 13cm in diameter and 27cm in height. Because of its compact size, we can place the display 
unit wherever we wish. The 360-degree display has a digital video input port, and can connect to a PC 
or other devices. When the video data is supplied to the display, the moving volumetric object appears 
inside the cylinder. The 360-degree motion image is generated by a graphic processor in real time at 
30Hz frame rate. We can move and interact with the object inside the display. The display is also equipped 
with a gesture sensor which can interactively control orientation of the object with hand motions. A 
common 3D computer graphics design software can be used to prepare the volumetric 3D image data. A turn-table 
is used to capture the photographed 360-degree static image. To capture 360-degree motion images, a new 
multiple-camera system has been developed. With the view interpolation image synthesis from multiple 
cameras, we generate continuous 360-degree view-point motion images. The 360-degree display is the first 
volumetric 3D display device which features high view-point density (360 views) 3D image, 24bit full-color, 
compact size and interactive live motion with digital video interface, so far as we know. This display 
has many potential applications, such as amusement, professional visualization, digital signage, museum 
display, video games, and futuristic 3D telecommunication. Specifications Resolution 96[H]x128[V] Number 
of Views 360 Frame Rate 30 [fps] Display Color 24bit full-color Dimensions 13cm[diameter] x 27cm [H] 
 Copyright is held by the author / owner(s). SIGGRAPH 2010, Los Angeles, California, July 25 29, 2010. 
ISBN 978-1-4503-0210-4/10/0007 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2010</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1836823</article_id>
		<sort_key>20</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>2</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[3D multitouch]]></title>
		<subtitle><![CDATA[when tactile tables meet immersive visualization technologies]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836821.1836823</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1836823</url>
		<abstract>
			<par><![CDATA[<p>Multitouch tactile input, while having been in the research labs for quite some time, is just reaching the general public through well-known interfaces like mobile phones or multitouch tables. The technology, when used in the right context in the right way, is known to provide an intuitive manipulation of the synthetic -- mostly 2D -- content that is displayed on its surface.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>H.5.2</cat_node>
				<descriptor>Input devices and strategies (e.g., mouse, touchscreen)</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>H.5.2</cat_node>
				<descriptor>Theory and methods</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003120.10003121.10003126</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->HCI theory, concepts and models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003123.10011758</concept_id>
				<concept_desc>CCS->Human-centered computing->Interaction design->Interaction design theory, concepts and paradigms</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003125</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10011666</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Touch screens</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2264114</person_id>
				<author_profile_id><![CDATA[81381609918]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Jean-Baptiste]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[de la Rivi&#232;re]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Immersion SAS]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264115</person_id>
				<author_profile_id><![CDATA[81381608476]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[C&#233;dric]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kerv&#233;gant]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Immersion SAS]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264116</person_id>
				<author_profile_id><![CDATA[81381606548]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Nicolas]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Dittlo]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Immersion SAS]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264117</person_id>
				<author_profile_id><![CDATA[81466641118]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Mathieu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Courtois]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Immersion SAS]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264118</person_id>
				<author_profile_id><![CDATA[81381610813]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Emmanuel]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Orvain]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Immersion SAS]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 3D Multitouch : When Tactile Tables Meet Immersive Visualization Technologies Jean-BaptistedelaRivi`edricKerv´ere,C´egant,NicolasDittlo,MathieuCourtois,EmmanuelOrvain 
ImmersionSAS 1 Introduction Multitouch tactile input, while having been in the research labs for quite 
some time, is just reaching the general public through well-known interfaces like mobile phones or multitouch 
tables. The technology, when used in the right context in the right way, is known to provide an intuitive 
manipulation of the synthetic mostly2D contentthatisdisplayed on itssurface. Likewise, means to display 
stereoscopic images are known for decades now, but are just beginning to be understood and accepted by 
the general public thanks to very well received movies and all the announcements around the soon to be 
released 3D television. Whenrenderedproperly,stereoscopicdisplays indeed offerahuge sense of depth to 
previously .at images. Furthermore, combining stereorendering withviewpoint trackingde.nitelyprovidesanim­mersive 
visualization of3D models. The combination of the two technologies introduces speci.c con­straints that 
have not yet been dealt with nor extensively studied. We therefore propose to demonstrate our current 
two-user multi­view multitouch table prototype, letting the attendees experience thenew issuesand experiment 
withourpreliminarydevelopments. 2 Immersive Multitouch Table 2.1 Technologies Constraints Whilethe two 
technologiessharemanyobjectives,whilebothmul­titouch tables and immersive visualization are being extensively 
studied by the research community, they have to our knowledge never been combined into a single solution. 
Such a combination is indeed not that obvious, since strengths of a technology become constraintsfor 
theotherone.They morepreciselyconsist in: single viewpoint : multitouch tables are ef.cient for collabo­rativework, 
implyingthat twouserswillfaceeach other,have oppositepointof viewsonthecontent and theirright and left 
sideswillbeswitched.Onthecontrary,most stereo technolo­gies are single viewpoint only.  Viewingangle: 
multitouchtablesare intendedtobeusedhor­izontally,whilemost stereo technologiesarereadyforvertical visualization 
requiring a narrow viewing angle.  Hand and 3D content collision : the most immersive stereo visualization 
is obtained when negative parallax is used, to make the3D content appear infront and outsideofthedisplay. 
Fingers in contact with the screen ruin the depth perception, since they wronglypass through the3D objects. 
 Parallax:activatingtheheadtrackingimpliesthatthecontent willmoveaccordingto theuser shead,dynamicallychanging 
theactual3Dpoints that areprojectedbeneath the .ngers.  2.2 Preliminary Prototype Trying to take the 
multitouch tables from 2D interaction to an ef.­cient and collaborative3Dimmersiveinterface,webuilta 
two-user multiview multitouch table that we propose to demonstrate. Two­user multiview, which ensures 
each user has his own perspective correct viewpoint on thesame3D model, isobtainedby combining activeandpassivestereo 
technologiesand using sixdof sensors that help toretrievethepositionand orientationof each user shead. 
The focal plane has to be set on the tactile surface. Negative par­allax, which then leads to the most 
impressive stereo visualization with the3D content coming out ofthedisplay, isused as long asno hand 
approaches the tactile surface. When a .nger touches to the surface, themodel may slowlybe lowered toobtainapositivepar­allax, 
with the 3D content located right inside the table. Thanks to thediffuseinfraredilluminationourmultitouch 
videoanalysisalgo­rithmsrelyon,wearealsoable todetectfeaturesbeyondthesimple 2D contact points and therefore 
retrieve a hand presence even be­fore its.ngerstouchthesurfaceandbreakthedepthperception. While image 
plane interaction techniques would work quite well forasingleuser,theywouldhardly takeintoaccountthe 
twousers opposite viewpoints and the stereo positive parallax. We therefore chose to experiment with 
shooting virtual rays orthogonal to the tactile surface from each .nger in contact, which currently seems 
to strenghten the .ngers presence within the virtual world and its relationship with3D objects.  3 Demo 
3.1 The Experience We Propose Two users will experience the demo simultaneously. Both of them willvisualizeacity3D 
model on the tablefrom theirownreal view­point, in the exact same way they would do with a real mockup. 
They willadditionallybeable tomanipulate the3D model trans­lation, rotation, scale using the typical 
multitouch gestures, one userat a timesincethe3D contentissharedby allusers.They will also be able, thanks 
to a set of 2D icons, to play with the various settingsand choices parallax,stereo,head tracking,virtual 
rays... wemadeat such anearlystage inourdevelopments. 3.2 Our Objectives On theonehand,ourproposalbrings 
togethersomede.nitelyma­jor emerging technologies and offers people to get their hands on theresult.Ontheotherhand,wewant 
ourdemo toactuallygofur­ther and allow attendees to experiment with the issues one has to deal with to 
build such systems and make them usable. The devel­opments we willdemonstrate are onlypreliminary experiments 
we are conducting, and are mostly intended to make people react and trigger as muchfeedback and exchanges 
aspossible.  4 Conclusion OurETechproposalconsists inoffering attendeesa .rst experience with the combination 
of multitouch tables andimmersive visualiza­tiontechnologies,andisbuilt tohelpthemexperiment withthe.rst 
issues it introduces. We are indeed very con.dent stereoscopic vi­sualization will .nd its way within 
multitouch tables in the future, and we would like to reduce this delay by demonstrating our .rst attempts 
and triggering some preliminary exchanges on this topic at theSiggraph2010EmergingTechnologies. Copyright 
is held by the author / owner(s). SIGGRAPH 2010, Los Angeles, California, July 25 29, 2010. ISBN 978-1-4503-0210-4/10/0007 
 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2010</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1836824</article_id>
		<sort_key>30</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>3</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[A fluid-suspension, electromagnetically driven eye with video capability for animatronic applications]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836821.1836824</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1836824</url>
		<abstract>
			<par><![CDATA[<p>(Our work of the same title was initially published at "Humanoid '09" in Paris France, and should be referred to for details).</p> <p>We have prototyped a compact, fluid-suspension, electromagnetically-rotated animatronic eye. The Eye has no external moving parts, features low operating power, a range of motion and saccade speeds that can exceed that of the human eye, and an absence of frictional wear points. It supports a rear, stationary, video camera. In a special application, the eye can be separated into a hermetically sealable portion that might be used as a human eye prosthesis along with an extra-cranially-mounted magnetic drive.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.2.9</cat_node>
				<descriptor>Sensors</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.4.1</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010520.10010553.10010559</concept_id>
				<concept_desc>CCS->Computer systems organization->Embedded and cyber-physical systems->Sensors and actuators</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2264119</person_id>
				<author_profile_id><![CDATA[81466647729]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Katie]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bassett]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[IEEE]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264120</person_id>
				<author_profile_id><![CDATA[81466644489]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Marcus]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hammond]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[IEEE]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264121</person_id>
				<author_profile_id><![CDATA[81466647221]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Lanny]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Smoot]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[IEEE]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Katie Bassett, Marcus Hammond, and Lanny Smoot, Member, IEEE (Our work of the same title was initially 
published at Humanoid 09 in Paris France, and should be referred to for details). We have prototyped 
a compact, fluid-suspension, electromagnetically-rotated animatronic eye. The Eye has no external moving 
parts, features low operating power, a range of motion and saccade speeds that can exceed that of the 
human eye, and an absence of frictional wear points. It supports a rear, stationary, video camera. In 
a special application, the eye can be separated into a hermetically sealable portion that might be used 
as a human eye prosthesis along with an extra-cranially-mounted magnetic drive.  Our electromagnetic 
Eye design is a result of an iterative approach. We wanted a robotic eye installable in current animatronic 
(human-like robotic) figures that would address a number of issues. It needed to be small and self-contained 
(current animatronic eyes are driven by rods, gears and motors that fill much of the top of the head), 
realistic looking, low wear, and with fast movement capability. In addition we wanted to support computer 
vision. Two of our earlier approaches (a free floating Eye with a hidden bar magnet swiveled by an external 
magnetic field, and a magnetically actuated ball in socket design) were essentially combined in a final 
system which is based on a sphere-within-a-sphere concept. The inner sphere is clear plastic, and is 
surrounded by, and neutrally buoyant in, a clear index-matching fluid. A clear plastic outer sphere is 
also index-matched to the fluid. The inner eye is painted to look like a human eye, but we leave the 
pupil area open to admit light (much as in a real human eye). A portion of the back of the inner eye 
is also left unpainted so that incoming light can reach a rear-mounted CCD imager (analogous to the retina 
of a human eye). With this scheme, the entire Eye acts as a single spherical lens (and it is the ONLY 
lens used in the system). The inner eye has small permanent magnets mounted at its north and south, 
and east and west, equatorial poles. These are used to slew the eye around with symmetrical, low-profile, 
external electromagnetic coils. With this design, the magnification and gaze direction of the overall 
camera/eye is fixed. Due to the spherical symmetry of the overall system, even when the inner eye rotates, 
the Eye s view does not change. The pupil opening is actually behind the front surface of the combined 
spherical lens and only acts as an aperture stop. The inner eyeball floats without touching the shell, 
virtually eliminating friction and allowing low power operation and long wear life. diagramProsthetic_01 
color crop Because of the magnification of the inner eye by the curved outer shell and index matching 
fluid, the inner eye s surface appears to be the outer surface of the overall Eye assembly, so the outer 
surface of the Eye can be in contact with animatronic skin and still appear to rotate even though it 
actually does not. Medical Application In a special application, we believe it would be possible to separate 
our (hermetically sealed) Eye sphere from its magnetic drive. The sphere could serve as a prosthesis 
for a person who has lost an eye. The magnetic drive could be supplied extra-cranially (as shown in the 
figure) with a drive signal derived from the person s remaining eye to synchronize the movement of the 
prosthesis. A Fluid-Suspension, Electromagnetically Driven Eye with Video Capability for Animatronic 
Applications 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2010</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1836825</article_id>
		<sort_key>40</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>4</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[Acroban the humanoid]]></title>
		<subtitle><![CDATA[playful and compliant physical child-robot interaction]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836821.1836825</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1836825</url>
		<abstract>
			<par><![CDATA[<p>Personal robotics is predicted to arrive massively in our homes in the 21<sup>st</sup> century, as well as impact importantly our society. Yet, before this vision can be realized, a number of very hard challenges need to be addressed. Among them are challenges related to human-robot interaction. Most personal robots, from entertainment to assistive robots, will need to interact with humans in a day-to-day basis: this implies that robot should afford intuitive, safe and pleasant interactions, as well as be able to adapt robustly to all unpredicted human behaviours. Many studies and technologies have elaborated in the field of human-robot interaction. In spite of this, physical human-robot interactions, which is central and unavoidable in real-world scenarios, have been only very little studied, in particular because existing hardware, humanoids in particular, did not allow easily for both safe and compliant interactions.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.2.9</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>H.5.2</cat_node>
				<descriptor>Theory and methods</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>H.5.2</cat_node>
				<descriptor>Interaction styles (e.g., commands, menus, forms, direct manipulation)</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003120.10003121.10003126</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->HCI theory, concepts and models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003123.10011758</concept_id>
				<concept_desc>CCS->Human-centered computing->Interaction design->Interaction design theory, concepts and paradigms</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010199.10010204</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Planning and scheduling->Robotic planning</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010520.10010553.10010554</concept_id>
				<concept_desc>CCS->Computer systems organization->Embedded and cyber-physical systems->Robotics</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010213.10010204</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Control methods->Robotic planning</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2264122</person_id>
				<author_profile_id><![CDATA[81466643483]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Olivier]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ly]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[INRIA, France]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264123</person_id>
				<author_profile_id><![CDATA[81100662126]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Pierre-Yves]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Oudeyer]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[INRIA, France]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Acroban the humanoid: Playful and compliant physical child-robot interaction Olivier Ly and Pierre-Yves 
Oudeyer INRIA, France Personal robotics is predicted to arrive massively in our homes in the 21st century, 
as well as impact importantly our society. Yet, before this vision can be realized, a number of very 
hard challenges need to be addressed. Among them are challenges related to human-robot interaction. Most 
personal robots, from entertainment to assistive robots, will need to interact with humans in a day-to-day 
basis: this implies that robot should afford intuitive, safe and pleasant interactions, as well as be 
able to adapt robustly to all unpredicted human behaviours. Many studies and technologies have elaborated 
in the field of human-robot interaction. In spite of this, physical human-robot interactions, which is 
central and unavoidable in real-world scenarios, have been only very little studied, in particular because 
existing hardware, humanoids in particular, did not allow easily for both safe and compliant interactions. 
In this work, we present a lightweight humanoid robot, called Acroban, which is to our knowledge the 
first humanoid robot which is able to: 1) demonstrate playful, compliant and intuitive physical interaction 
with children; 2) move and walk dynamically while keeping its equilibrium even if unpredicted physical 
interactions are initiated by humans. The robot combines uniquely several crucial features which make 
these advances possible: Softness. The robot is soft : instead of controlling motors in a stiff manner, 
their rigidity and simulated elasticity is dynamically changed based on forces that are sensed with proprioception. 
The mechanical structure includes itself natural softness by using elastics and springs. All of this 
provides compliance to external forces due to interactions, as well as leveraging the energy of gravity: 
the robot applies principles of powered passive dynamic walkers to a wide range of movements involving 
its torso and its arms. Morphology. The robot has 32 degrees of freedom, and in particular is equipped 
with a complex semi­passive vertebral column as well as complex hip and ankle systems. The morphological 
design of these systems allows the robot an extreme robustness for keeping its balance dynamically. Coupled 
with the softness property, this allows fluid and robust physical interaction with humans while the robot 
is performing its own movements. Motor and interaction primitives. Softness and morphology are leveraged 
in an advanced system of combinable motor and interaction primitives built as dynamical systems with 
a stable and drivable attractor dynamics, as well as with a particular design of movements which create 
a strong illusion of life. We present a demonstration of the system in an entertainment human-robot interaction 
context, in particular allowing children to engage in the interaction. In this demonstration, the robot 
has a range of behaviors that it can combine and which all react intuitively, naturally and creatively 
to uncontrolled external human intervention. For example, when the robot is walking anyone can take its 
arms, like we take the arms of babies learning to walk, and drive in a fluid and transparent manner the 
robot in any direction. This is realized automatically without the need to provide the robot with any 
command, and is the result of the dynamical properties of its motor primitives and morphological properties. 
Another example is when the robot is showing a complex movement of its torso: anyone can interrupt physically 
the robot and take its arm, which will cause the robot s arm to follow the movements imposed by the human 
without falling. This is the first time the Acroban humanoid is formally presented, and it was only showed 
once so far in a robotic public exhibition of the Science Museum of Napoli, Italy, in 2009. This allowed 
us to show that it efficiently affords a new kind of physical human-robot interaction, with children 
in particular, which is at the same time playful, intuitive, compliant, fluid and robust, as shown in 
the accompanying video. Copyright is held by the author / owner(s). SIGGRAPH 2010, Los Angeles, California, 
July 25 29, 2010. ISBN 978-1-4503-0210-4/10/0007  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2010</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1836826</article_id>
		<sort_key>50</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>5</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>5</seq_no>
		<title><![CDATA[AirTiles]]></title>
		<subtitle><![CDATA[modular devices to create a flexible sensing space]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836821.1836826</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1836826</url>
		<abstract>
			<par><![CDATA[<p>There are a number of systems and devices for spacial measurement which can be used to measure bodily motion. However, these systems are usually large or fixed with the environment. In this study, we propose a novel modular device that allows users to create geometric shapes in the real world and also a flexible sensing space within the created shape. Users can freely put devices and rotate them so that a geometrical shape could appear on the floor. As the modular devices and the emitted laser beam represent the corners and sides of the shape, respectively, the device therefore does not interrupt figures or textures on the ground. The developed system provides an interactive visual/audio environment as if children could make a hand drawing on the ground or floor and play with the created shapes.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.5</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003752.10010061.10010063</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures->Computational geometry</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061.10010063</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures->Computational geometry</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2264124</person_id>
				<author_profile_id><![CDATA[81466645488]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Kazuki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Iida]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Tsukuba, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264125</person_id>
				<author_profile_id><![CDATA[81466647675]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Junki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ikeuchi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Tsukuba, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264126</person_id>
				<author_profile_id><![CDATA[81442595062]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Toshiaki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Uchiyama]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Tsukuba, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264127</person_id>
				<author_profile_id><![CDATA[81100184640]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Kenji]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Suzuki]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Tsukuba, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 AirTiles: Modular Devices to Create a Flexible Sensing Space Kazuki Iida* , JunkiIkeuchi, Toshiaki 
Uchiyama, andKenji Suzuki University of Tsukuba, Japan  Figure 1: AirTiles: Spacial measurement can 
be done by the created geometrical shapes according to the small and lightweight modules placed on the 
ground. Users can easily modify the shape which correspond to the sensing region. 1 Introduction There 
are a number of systems and devices for spacial mea­surement which can be used to measure bodily motion. 
How­ever, these systems are usually large or .xed with the envi­ronment. In this study, we propose a 
novel modular device that allows usersto create geometric shapesinthe realworld and also a .exible sensing 
space within the created shape. Users can freely put devices and rotate them so that a ge­ometrical shape 
could appear on the .oor. As the modular devices and the emitted laser beam represent the corners and 
sides of the shape, respectively, the device therefore does not interrupt .gures or textures on the ground. 
The developed system provides an interactive visual/audio environment as if children could makea hand 
drawing on the ground or .oor and play with the created shapes.  2 Overview The developed module consists 
of a microprocessor, laser­emitting module, infrared-emitting/receiving components, small position sensitive 
detector, wireless meshed network component, LED, beep speaker and battery. All equipments are installed 
in a small case as illustrated in the right of Fig­ure 1. The modules allow the user to create several 
geo­metrical shape on the ground (or wall) and realize the object sensing within the drawn shape. Each 
module is capable to emit a laser beam in one direction, and users can place these modules on the ground 
so as to create an enclosed region. The module s location and emitted laser beam correspond to the corner 
and side of the created shape. As an example, triangle shape can be drawn by three mod­ules: First, the 
user put .rst and second modules on the ground so that the emitted laser beam from .rst module could 
reach second moduleby rotating the .rst module. The *e-mail: kaz@ai.iit.tsukuba.ac.jp user then put 
third module and rotate the second module in order to reach the emitted laser beam from the second mod­ule 
to the third module. Finally, by rotating the third module to reach the emitted laser beam to the .rst 
module. Each module emits both single laser beam and infrared beam to the same direction by receiving 
an infrared beam from the other module. Therefore, the relative angle between incom­ing and outgoing 
laser beams can be calculated at each mod­ule. Once an enclosed region is created, the modules auto­matically 
detect the created shape and start to measure in the region. The modules communicate with each other 
so as to detectcomingsandgoingsintheregionbyusing positionde­tection sensors installed in each module. 
Polygonal shapes such as rectangle and pentagon, and different shape like star can be created solely 
by changing the locations of modules, as illustrated in the center of Figure 1. The created enclosed 
region is called as AirTile. 3 Performance We have conducted several experiments by using the devel­oped 
modules. For example, we used more than two groups of modules(AirTiles)and designedasimplegame for motion 
guidance. We asked users to create two AirTile on the desk­top and then LEDs of one AirTile start to 
blink on and off. When the user put his hand in the AirTile, another AirTile starts to blink. On the 
other hand, these modules can be used as a bodily exercise tool. Side stepping is a simple exercise that 
the user should shift back and forth between predeter­mined two regions. In this case, the user .rst 
create two re­gions by AirTile and simply do the exercise. As AirTiles can be used as the data logger, 
exercise quantity and timing of footwork are recorded and analyzed after theexercise. Other potential 
applications include the human-behavior measure­ment, motion guidance, and therapeutic exercise. Copyright 
is held by the author / owner(s). SIGGRAPH 2010, Los Angeles, California, July 25 29, 2010. ISBN 978-1-4503-0210-4/10/0007 
  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2010</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1836827</article_id>
		<sort_key>60</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>6</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>6</seq_no>
		<title><![CDATA[An interactive zoetrope for the animation of solid figurines and holographic projections]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836821.1836827</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1836827</url>
		<abstract>
			<par><![CDATA[<p>We demonstrate interactive zoetropes that animate holographic images or solid figurines. Unlike previously existing zoetropes, they are capable of aperiodic, interactive behavior. For example, we use them to animate talking character's mouths in real time, in response to human speech.</p> <p>Zoetropes trace their roots to the early 1800s. Initially developed as parlor entertainment, they inspired the successive-image presentation method used in modern cinema and television. They were typically spinning platters or cylinders around which 2D images were affixed. The device was spun rapidly, and included a method (e.g. slits or mirrors) to strobe the user's view of the images, freezing their translational movement, and overlaying them to create animation.</p> <p>More recently the zoetrope has enjoyed a resurgence with global LED strobe illumination replacing slits, and solid figurines replacing 2D images. These new zoetropes still show all of the figures in motion at once, and they are still only capable of depicting periodic motion; the show repeats after each revolution of the platter.</p> <p>Our interactive zoetropes however allow us to instantaneously vary the order in which the images are displayed each revolution, allowing us to change the course of the animation in real time. This supports non-repetitive and non-trivial animation using a small, finite number of images or frames. Other researchers and commercial vendors have explored video animation based on audio input. In contrast, our approach allows instantaneous interactive animation of physical objects and holograms.</p> <p>We have prototyped 3 different types of interactive zoetrope. A first example animates whimsical faces drawn on ping-pong balls (Fig. 2) affixed to a small rotating platform. A rotary shaft encoder and associated electronics (fig 3) determine which figurine gets lit, based on an audio input level.</p> <p>In the case of this "Ping Pong Ball Zoetrope," as a person speaks into a microphone (Fig. 4) the whimsical ping-pong ball character mimics their talking mouth movement.</p> <p>A second version of our device spins a holographic plate (fig. 5) manufactured by Holorad Inc. The hologram (fig. 6) has 8 frames of animation of a "monster" character. Each frame has 42 depth planes, and is read out depending on the angle of an external light source to the back face of the transparent hologram. We rapidly rotate the hologram in its plane while strobing a high power LED controlled by an analog of the electronics used in the previous incarnation. The effect is that a luminous 3D image floats in front of the hologram, and appears to speak (moves face and mouth) in sync with a person speaking into a microphone.</p> <p>A final version (fig. 7) retains the hologram but instead of spinning it, we essentially "spin" its lighting. Fixed LEDs at 45 degree angles around the hologram are lit synchronously to the instantaneous volume level of a person speaking into a microphone, and again animate the monster.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Animation</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.6.8</cat_node>
				<descriptor>Animation</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010341.10010349.10011310</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Simulation types and techniques->Simulation by animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2264128</person_id>
				<author_profile_id><![CDATA[81466647221]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Lanny]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Smoot]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Disney Research, Glendale, CA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264129</person_id>
				<author_profile_id><![CDATA[81466647729]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Katie]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bassett]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Disney Research, Glendale, CA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264130</person_id>
				<author_profile_id><![CDATA[81430613187]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Stephen]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hart]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Holorad, Salt Lake City, UT]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264131</person_id>
				<author_profile_id><![CDATA[81466640813]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Daniel]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Burman]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Holorad, Salt Lake City, UT]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264132</person_id>
				<author_profile_id><![CDATA[81466648234]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Anthony]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Romrell]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Holorad, Salt Lake City, UT]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 An Interactive Zoetrope for the Animation of Solid Figurines and Holographic Projections Lanny Smoot, 
Katie Bassett, Stephen Hart*, Daniel Burman*, Anthony Romrell* Disney Research, 1401 Flower St., Glendale, 
CA 91201, USA *Holorad, 2929 South Main St, Salt Lake City, UT 84115, USA (Our Siggraph 10 presentation 
will be based on a paper of the above title given at IDW 09 in Miazaki Japan) ABSTRACT We demonstrate 
interactive zoetropes that animate holographic images or solid figurines. Unlike previously existing 
zoetropes, they are capable of aperiodic, interactive behavior. For example, we use them to animate talking 
character's mouths in real time, in response to human speech. Zoetropes trace their roots to the early 
1800s. Initially developed as parlor entertainment, they inspired the successive-image presentation method 
used in modern cinema and television. They were typically spinning platters or cylinders around which 
2D images were affixed. The device was spun rapidly, and included a method (e.g. slits or mirrors) to 
strobe the user s view of the images, freezing their translational movement, and overlaying them to create 
animation.  Fig. 3 Fig. 2 Fig. 1     More recently the zoetrope has enjoyed a resurgence with 
global LED strobe illumination replacing slits, and solid figurines replacing 2D images. These new zoetropes 
still show all of the figures in motion at once, and they are still only capable of depicting periodic 
motion; the show repeats after each revolution of the platter. Our interactive zoetropes however allow 
us to instantaneously vary the order in which the images are displayed each revolution, allowing us to 
change the course of the animation in real time. This supports non-repetitive and non-trivial animation 
using a small, finite number of images or frames. Other researchers and commercial vendors have explored 
video animation based on audio input. In contrast, our approach allows instantaneous interactive animation 
of physical objects and holograms. We have prototyped 3 different types of interactive zoetrope. A first 
example animates whimsical faces drawn on ping-pong balls (Fig. 2) affixed to a small rotating platform. 
A rotary shaft encoder and associated electronics (fig 3) determine which figurine gets lit, based on 
an audio input level.  Fig. 4      Fig. 7 Fig. 6  Fig. 5  In the case of this Ping Pong Ball 
Zoetrope, as a person speaks into a microphone (Fig. 4) the whimsical ping-pong ball character mimics 
their talking mouth movement. A second version of our device spins a holographic plate (fig. 5) manufactured 
by Holorad Inc. The hologram (fig. 6) has 8 frames of animation of a monster character. Each frame has 
42 depth planes, and is read out depending on the angle of an external light source to the back face 
of the transparent hologram. We rapidly rotate the hologram in its plane while strobing a high power 
LED controlled by an analog of the electronics used in the previous incarnation. The effect is that a 
luminous 3D image floats in front of the hologram, and appears to speak (moves face and mouth) in sync 
with a person speaking into a microphone. A final version (fig. 7) retains the hologram but instead 
of spinning it, we essentially spin its lighting. Fixed LEDs at 45 degree angles around the hologram 
are lit synchronously to the instantaneous volume level of a person speaking into a microphone, and again 
animate the monster. We would plan to demonstrate the Ping-Pong Ball Zoetrope, and the stationary hologram 
interactive zoetrope at Siggraph 10. 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2010</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1836828</article_id>
		<sort_key>70</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>7</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>7</seq_no>
		<title><![CDATA[beacon 2+]]></title>
		<subtitle><![CDATA[networked socio-musical interaction]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836821.1836828</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1836828</url>
		<abstract>
			<par><![CDATA[<p>This work proposes an environment for socio-musical interaction where people can generate sounds and play music by feet in collaboration with each other. We have been developing a new musical interface, <i>beacon</i>. Each beacon produces laser beams lying on the ground and rotating around it. Audio sounds are then produced when the beams pass individual performer's foot. As the users are able to change the sound pitch and length according to the foot location and angles facing the instrument, their bodily motion and foot behavior can be translated into sound and music in an intuitive manner. In addition, two different types of beacons are connected via network. Throughout the performance, walking, dancing and stepping around the devices, users could feel and make his/her presence felt in a distant place, and also they can easily and intuitively understand how it works.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>H.5.2</cat_node>
				<descriptor>Interaction styles (e.g., commands, menus, forms, direct manipulation)</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>H.5.2</cat_node>
				<descriptor>Auditory (non-speech) feedback</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003120.10003121.10003128.10010869</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction techniques->Auditory feedback</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2264133</person_id>
				<author_profile_id><![CDATA[81466648352]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Takahiro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kamatani]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Tsukuba, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264134</person_id>
				<author_profile_id><![CDATA[81442595062]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Toshiaki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Uchiyama]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Tsukuba, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264135</person_id>
				<author_profile_id><![CDATA[81100184640]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Kenji]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Suzuki]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Tsukuba, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 beacon 2+: Networked Socio-Musical Interaction Takahiro Kamatani*, Kenji Suzuki , Toshiaki Uchiyama 
, University of Tsukuba, Japan Figure 1: beacon 2+ -new interfaces for networked socio-musical interaction. 
Interaction with the beacon (left), beacon 1 (center) and, beacon 2 (right). 1 Introduction This work 
proposes an environment for socio-musical inter­action where people can generate sounds and play music 
by feet in collaboration with each other. We have been devel­oping a new musical interface, beacon. Each 
beacon pro­duces laser beams lying on the ground and rotating around it. Audio sounds are then produced 
when the beams pass in­dividual performer s foot. As the users are able to change the sound pitch and 
length according to the foot location and angles facing the instrument, their bodily motion and foot 
behavior can be translated into sound and music in an intu­itive manner. In addition, two different types 
of beacons are connected via network. Throughout the performance, walk­ing, dancing and stepping around 
the devices, users could feel and make his/her presence felt in a distant place, and also theycan easily 
and intuitively understand how it works. 2 Overview The developed interface consists of a loudspeaker, 
a small­size computer, 60 line laser modules,2 laser range .nders, dial andbuttons interface, and battery. 
All equipments are installedinacylinder shaped metal interface as illustratedin Figure 1. The laser beams 
are used not only to mark the cur­rent location to produce the soundbut also to assist musical interaction. 
At the bottom of the instrument, two laser range­.nders are installed and used for the distance measurement 
to performers, in particular those foot positions and its angles every 100 ms at the height of1 cm from 
the ground. The installed range-.nder has 4[m] measuring range with 99% range accuracy, and also has 
a 240 degree angle of view for each. We used two range-.nders in order to obtain omni­directional distance 
map every time. *e-mail:buhii314@gmail.com e-mail: uchi@kansei.tsukuba.ac.jp e-mail:kenji@ieee.org Regarding 
the laser emission, beacon 1 and 2 have differ­ent styles: (i) 60 red laser beam emissions in a sequential 
manner, (ii) green laser emission based on thegalvanometer. Inthe beacon1,upto4red laser beamswith equiangularly­spaced 
directions are lying on the ground and rotating during the performance. The rotation speed of laser beams 
can be controlled between 40bpm and 100bpm, which corresponds to the musical tempo. On the other hand, 
in the beacon 2, the green laser beams are projected on the ground by the re­.ectionofthe installed lensandgalvanometer.Agreen 
laser beam is re.ected by the lens which is rotated by the motor. beacon is, therefore, able to show 
the user s location on the other side by using another laser beam in addition to the ro­tating beams 
which indicate the time line. The beacon1sim­ply uses the red laser beams according to the other slocation, 
andthe beacon2is capabletoshowthe other s footprintby using galvanometer-based laser projection as illustrated 
in the right .gure of Figure 1. 3 Performance The beacon generates sounds when the beams passed indi­vidual 
performers as if each user could be a musical note. Also two beacons are connected via internet and people 
in a distant place can share the generated music simultane­ously. In the demonstration, participants 
can not only gen­erate soundsby using these feet aroundthedevicesbut also can feel the other s presence 
thorough both visual and au­ditory feedbacks from laser beam projected on the ground and generated sound. 
This novel interface thus can be used for the physical exercise or recreation with fun. Moreover, by 
arranging small objects around the instrument, a variety of sound will be produced like a environmental 
music box. The proposed interface also provides a new artistic expres­sion for spatial designers. This 
round-shape interface does nothaveanydirectional characteristicsand playsakeyrole ofgathering people 
foraffective communication. Copyright is held by the author / owner(s). SIGGRAPH 2010, Los Angeles, California, 
July 25 29, 2010. ISBN 978-1-4503-0210-4/10/0007  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2010</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1836829</article_id>
		<sort_key>80</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>8</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>8</seq_no>
		<title><![CDATA[Beyond the surface]]></title>
		<subtitle><![CDATA[3D interactions for tabletop systems]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836821.1836829</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1836829</url>
		<abstract>
			<par><![CDATA[<p>Current tabletop systems are designed to sense 2D interactions taken place <i>on the tabletop surface</i>, such as finger touches and tangible objects. The ability to interact <i>above the tabletop surface</i> makes it possible to support 3D interactions. For example, an architect can examine a 2D blueprint of a building shown on the tabletop display while inspecting 3D views of the building by moving a mobile display above the tabletop. Recent approaches to localize objects in 3D requires visible markers or the use of embedded sensors [Song et al. 2009]. The use of visible markers often interferes with the content users are focusing on, limiting its usefulness and applicability.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[3D interaction]]></kw>
			<kw><![CDATA[infra-red projection]]></kw>
			<kw><![CDATA[tabletop system]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>K.6.1</cat_node>
				<descriptor>Life cycle</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>K.7.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003456.10003457</concept_id>
				<concept_desc>CCS->Social and professional topics->Professional topics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003456.10003457.10003490.10003491</concept_id>
				<concept_desc>CCS->Social and professional topics->Professional topics->Management of computing and information systems->Project and people management</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2264136</person_id>
				<author_profile_id><![CDATA[81407593501]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Li-Wei]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Chan]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[National Taiwan University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264137</person_id>
				<author_profile_id><![CDATA[81466645166]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Hsiang-Tao]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[National Taiwan University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264138</person_id>
				<author_profile_id><![CDATA[81444600184]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Hui-Shan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kao]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[National Taiwan University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264139</person_id>
				<author_profile_id><![CDATA[81466643323]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Home-Ru]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[National Taiwan University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264140</person_id>
				<author_profile_id><![CDATA[81443595574]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Ju-Chun]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ko]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[National Taiwan University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264141</person_id>
				<author_profile_id><![CDATA[81460645188]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Mike]]></first_name>
				<middle_name><![CDATA[Y.]]></middle_name>
				<last_name><![CDATA[Chen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[National Taiwan University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264142</person_id>
				<author_profile_id><![CDATA[81406599584]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>7</seq_no>
				<first_name><![CDATA[Jane]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hsu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[National Taiwan University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264143</person_id>
				<author_profile_id><![CDATA[81100111382]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>8</seq_no>
				<first_name><![CDATA[Yi-Ping]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hung]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[National Taiwan University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1518726</ref_obj_id>
				<ref_obj_pid>1518701</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Song, H., Grossman, T., Fitzmaurice, G., Guimbretiere, F., Khan, A., Attar, R., and Kurtenbach, G. 2009. Penlight: combining a mobile projector and a digital pen for dynamic visual overlay. In <i>CHI '09</i>, ACM, 143--152.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 1 2 Figure 2: The system architecture. in Fig. 2a, the color projector provides visible content to the 
view­ers, while the IR projector delivers content that is invisible to human eyes but can be captured 
by IR cameras. Two IR cameras are in­stalled under the table to detect .nger touches directly on the 
table­top surface. In addition, IR cameras are embedded in objects that need to be localized above the 
tabletop surface. The programmable IR projector is used for two purposes. First, it projects a uniform 
white screen for multi-touch detection and recognizing tangible objects placed on the surface, similar 
to other direct-illuminated (DI) tabletop systems. Second, it projects special marker patterns to enable 
IR cameras to estimate their 3D positions relative to the tabletop. In order to simultaneously perform 
2D detection and 3D localiza­tion, we propose an adaptive IR projection framework, based on input from 
the IR cameras under and above the table (Fig. 2b). By default, the IR projection only projects marker 
patterns, allowing for 3D localization. When the IR cameras under the table detect objects on the surface, 
the IR projector will project a white region enclosing the foregrounds. This enables IR cameras to accurately 
detect .nger touches and tangible objects. In addition, when the IR cameras above the table detects the 
marker patterns, they report the quality of the patterns perceived. The IR marker patterns adaptively 
change in size based on the reported quality and distance of the cameras. 3 Interaction Metaphors We 
propose three interaction metaphors (Fig. 1). iView is com­posed of a tablet computer with an attached 
IR camera, which is an intuitive tool to see 3D content from different perspective. iL­amp is composed 
of a projector with an IR camera that projects high-resolution content on the surface, mimicking the 
use of a desk lamp. Similarly, iFlashlight is a mobile version of iLamp, facilitat­ing information explorations 
and cooperative tasks. References SONG, H., GROSSMAN, T., FITZMAURICE, G., GUIMBRETIERE, F., KHAN, A., 
ATTAR, R., AND KURTENBACH, G. 2009. Pen­light: combining a mobile projector and a digital pen for dy­namic 
visual overlay. In CHI 09, ACM, 143 152. Copyright is held by the author / owner(s). SIGGRAPH 2010, Los 
Angeles, California, July 25 29, 2010. ISBN 978-1-4503-0210-4/10/0007 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2010</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1836830</article_id>
		<sort_key>90</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>9</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>9</seq_no>
		<title><![CDATA[Camera-less Smart Laser Projector]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836821.1836830</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1836830</url>
		<abstract>
			<par><![CDATA[<p>The 'Smart Laser Projector' (SLP) is a modified laser-based projector capable of displaying while simultaneously using the laser beam (at the same or different wavelength or polarization) as a LIDAR probe gathering information about the projection surface (its borders, 3d shape, relative position and orientation, as well as fine texture and spectral reflectance). This information can then be used to correct perspective warp, perform per-pixel contrast compensation, or even reroute the scanning/projecting path altogether (for tracking, feature discovery or barcode reading for instance). We demonstrate here raster-scan and vector graphics applications on two different prototypes. The first relies on a pair of galvanomirrors, and is used for demonstrating simultaneous tracking and display on the palm of the hand, depth-discriminating active contours (for spatially augmented reality surveying), and interactive games. The other relies on a single 2-axis MEMS mirror working in resonant mode, and is used to demonstrate edge enhancement of printed material and 'artificial fluorescence' - all with perfect projection-to-real-world registration by construction.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.4.1</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.4.7</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.4.8</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010225.10010227</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision tasks->Scene understanding</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2264144</person_id>
				<author_profile_id><![CDATA[81100408023]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Alvaro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Cassinelli]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264145</person_id>
				<author_profile_id><![CDATA[81458645328]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Alexis]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Zerroug]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264146</person_id>
				<author_profile_id><![CDATA[81100363385]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Yoshihiro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Watanabe]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264147</person_id>
				<author_profile_id><![CDATA[81100353752]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Masatoshi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ishikawa]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264148</person_id>
				<author_profile_id><![CDATA[81100351175]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Jussi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Angesleva]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The Berlin University of the Arts]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1056851</ref_obj_id>
				<ref_obj_pid>1056808</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Cassinelli, A., Perrin, S., and Ishikawa, M. 2005. Smart laser-scanner for 3d human-machine interface. In <i>CHI '05</i>, 1138--1139.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1665144</ref_obj_id>
				<ref_obj_pid>1665137</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Cassinelli, A., Kuribara, Y., Ishikawa, M., and Manabe, D. 2009. scorelight. In <i>SIGGRAPH ASIA '09 Art Gallery &amp; Emerging Technologies: Adaptation</i>, 15--15.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1198715</ref_obj_id>
				<ref_obj_pid>1198555</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Raskar, R., van Baar, J., Beardsley, P., Willwacher, T., Rao, S., and Forlines, C. 2005. ilamps: geometrically aware and self-configuring projectors. In <i>SIGGRAPH '05: ACM SIGGRAPH 2005 Courses</i>, 5.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Camera-less Smart Laser Projector Alvaro Cassinelli*Jussi Angesleva , Alexis Zerroug, Yoshihiro Watanabe, 
Masatoshi Ishikawa The University of Tokyo The Berlin University of the Arts 1 INTRODUCTION The Smart 
Laser Projector (SLP) is a modi.ed laser-based pro­jector capable of displaying while simultaneously 
using the laser beam (at the same or different wavelength or polarization) as a LIDAR probe gathering 
information about the projection surface (its borders, 3d shape, relative position and orientation, as 
well as .ne texture and spectral re.ectance). This information can then be used to correct perspective 
warp, perform per-pixel contrast com­pensation, or even reroute the scanning/projecting path altogether 
(for tracking, feature discovery or barcode reading for instance). We demonstrate here raster-scan and 
vector graphics applications on two different prototypes. The .rst relies on a pair of galvano­mirrors, 
and is used for demonstrating simultaneous tracking and display on the palm of the hand, depth-discriminating 
active con­tours (for spatially augmented reality surveying), and interactive games. The other relies 
on a single 2-axis MEMS mirror working in resonant mode, and is used to demonstrate edge enhancement 
of printed material and arti.cial .uorescence -all with perfect projection-to-real-world registration 
by construction. Figure 1: Collinear beams for display and measurement  2 DEMONSTRATION The SLP is 
a mix between a laser projector and a laser scanner (Fig.1). It is a smart projector in the sense of 
[Raskar et al. 2005], but presents some signi.cant advantages with respect to a classical camera-enhanced 
projector, the most important being unnecessary camera/projector calibration (scanner and projector share 
the same intrinsic and extrinsic parameters), markerless 3d tracking by di­rect laser range.nding, and 
.nally possible precise measurement of the surface s spectral re.ectance. Additional advantages come 
from the laser projection technology itself: large depth of .eld, variable resolution, simple and compact 
optical system (no imag­ing optics), energy ef.ciency for long-range projection (ideal for large-scale 
outdoor applications). The SLP is also capable of de­tecting and tracking objects or .ngers over the 
projection surface, making it an interesting platform for implementing ubiquitous in­teractive displays. 
The Smart Laser Scanner [Cassinelli et al. 2005] and scoreLight installation [Cassinelli et al. 2009] 
can be seen as special implementations of the SLP in vector-graphics mode. We present here two new vector-graphic 
demonstrations highlighting the ability of the SLP to extract geometrical features from images or 3d 
objects: depth-discriminating active contours for AR survey­ing and interactive laser games based on 
the simulation of geomet­rical optics (Fig.2). A second MEMS-based prototype is capable of *e-mail: cassinelli.alvaro@gmail.com 
jussi.angesleva@iki..  Figure 3: Edge detection and enhancement in raster mode displaying raster-scan 
images; it uses an infrared laser for sensing, and a collinear red laser for displaying. Fig.3 shows 
a laser gen­erated outer glow to enhance text legibility. Other applications will be demonstrated during 
the show, including direct visualiza­tion of IR watermarks, vein enhancement (by exploiting the arti.­cial 
.uorescence principle) and direct visualization of polarization changes. Future applications of the SLP 
may include dermatology (enhancement of super.cial veins, direct visualization of anoma­lous polarization 
induced by cancerous cells, and energy-ef.cient targeted phototherapy), non-destructive control (visualization 
of microscopic scratches or mechanical stress), authentication (by ex­ploiting arti.cial .uorescence) 
and in general all sort of augmented reality applications using any available surface for projecting 
laser icons but also full-.edged raster scan images when required. In this sense, our short term goal 
is to develop a wearable MEMS-based prototype capable of transforming the space around the wearer into 
an interactive laser-based AR environment -a sort of laser aura . References CASSINELLI, A., PERRIN, 
S., AND ISHIKAWA, M. 2005. Smart laser-scanner for 3d human-machine interface. In CHI 05, 1138 1139. 
CASSINELLI, A., KURIBARA, Y., ISHIKAWA, M., AND MAN-ABE, D. 2009. scorelight. In SIGGRAPH ASIA 09 Art 
Gallery &#38; Emerging Technologies: Adaptation, 15 15. RASKAR, R., VAN BAAR, J., BEARDSLEY, P., WILLWACHER, 
T., RAO, S., AND FORLINES, C. 2005. ilamps: geometrically aware and self-con.guring projectors. In SIGGRAPH 
05: ACM SIGGRAPH 2005 Courses, 5. Copyright is held by the author / owner(s). SIGGRAPH 2010, Los Angeles, 
California, July 25 29, 2010. ISBN 978-1-4503-0210-4/10/0007  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2010</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1836831</article_id>
		<sort_key>100</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>10</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>10</seq_no>
		<title><![CDATA[Colorful Touch Palette]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836821.1836831</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1836831</url>
		<abstract>
			<par><![CDATA[<p>Previously, pictures were painted using tools such as crayons or even by hand. Surfaces such as canvases or walls, provided the tactile sensations of the drawing surface while painting. However, this tactile experience has got lost because of advances in computer graphics software. Besides, a conventional multi-touch interface [1] can not provide tactile sensation. We propose a novel interactive painting interface called <i>"Colorful Touch Palette"</i> that may help us to rediscover our creativity. The user can touch the canvas having the electrode, select or blend tactile textures of their choice, draw a line, and experience the tactile sensations of painting as shown in Figure 1. Various tactile textures can be created by blending textures as paints. This interface can be used to design complex spatial tactile patterns for haptic-friendly products. Moreover, this system can be potentially used to create novel tactile paintings.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>H.5.2</cat_node>
				<descriptor>Input devices and strategies (e.g., mouse, touchscreen)</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003125</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10011666</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Touch screens</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2264149</person_id>
				<author_profile_id><![CDATA[81466647546]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Yuki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hirobe]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264150</person_id>
				<author_profile_id><![CDATA[81421594598]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Takumi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yoshida]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264151</person_id>
				<author_profile_id><![CDATA[81331496816]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Shinobu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kuroki]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264152</person_id>
				<author_profile_id><![CDATA[81331499667]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Kouta]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Minamizawa]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264153</person_id>
				<author_profile_id><![CDATA[81331503619]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Katsunari]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sato]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264154</person_id>
				<author_profile_id><![CDATA[81365592882]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Susumu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tachi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1179159</ref_obj_id>
				<ref_obj_pid>1179133</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Han, J. Y. 2006. Multi-touch interaction wall. ACM SIGGRAPH Emerging Technology.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>962721</ref_obj_id>
				<ref_obj_pid>962710</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Kajimoto, H., Inami, M., Kawakami, N., and Tachi, S. 2004 SmartTouch: Electric skin to touch the untouchable. IEEE CG&amp;A 24-36-43]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>835832</ref_obj_id>
				<ref_obj_pid>580521</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Kajimoto, H., Kawakami, N., Maeda, T., and Tachi, S. 2001 Electrocutaneous Display as an Interface to a Virtual Tactile World, IEEE Virtual Reality Conf., Yokohama, Japan.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Colorful Touch Palette Yuki Hirobe* Shinobu Kuroki* Katsunari Sato* Takumi Yoshida* Kouta Minamizawa 
 Susumu Tachi *The University of Tokyo Keio University Figure 1: Concept drawing Figure 2: Tactual 
Cap 1. Introduction Previously, pictures were painted using tools such as crayons or even by hand. Surfaces 
such as canvases or walls, provided the tactile sensations of the drawing surface while painting. However, 
this tactile experience has got lost because of advances in computer graphics software. Besides, a conventional 
multi-touch interface [1] can not provide tactile sensation. We propose a novel interactive painting 
interface called Colorful Touch Palette that may help us to rediscover our creativity. The user can touch 
the canvas having the electrode, select or blend tactile textures of their choice, draw a line, and experience 
the tactile sensations of painting as shown in Figure 1. Various tactile textures can be created by blending 
textures as paints. This interface can be used to design complex spatial tactile patterns for haptic-friendly 
products. Moreover, this system can be potentially used to create novel tactile paintings. 2. Technical 
Innovations To realize the concept of this interface, following features are required. (1) Providing 
various types of tactile sensation that the user can recognize comfortably. (2) Creating new tactile 
textures by blending original textures. (3) Providing tactile feedback according to the motion and posture 
of the finger.  With regarding to feature (1), the previous electro-tactile stimulation system [2] could 
provide only uniform rough textures and could not provide grating convex patterns with a resolution higher 
than the electrode interval. We improved the technique of electro-tactile stimulation of spatial patterns, 
and we archived various patterns of tactile sensations. We provided various degrees of roughness by changing 
the intensity and controlling the distribution and variances of each electrode. We also virtually increased 
the spatial resolution by changing the stimulus points faster than the fingertip movements as occasion 
demanded, instead of synchronizing both movements. We designed the blending method of tactile textures 
for realizing feature (2). It is known that each polar stimulus can produce different sensations: sense 
of vibration by anodic stimulus and sense of pressure by cathodic stimulus [3]. To calculate the stimuli 
of the blended tactile textures, we defined a pressure model Mp(x,y) and a vibration model Mv(x,y) for 
each tactile texture and * e-mail: {yuki_hirobe,shinobu_kuroki,katsunari_sato, takumi_yoshida}@ipc.i.u-tokyo.ac.jp 
 e-mail: {kouta, tachi}@tachilab.org Figure 3: Touch Palette Figure 4: Texture Canvas (a) 2D canvas (b) 
3D canvas described a texture as T(Mp,Mv). When the user blends two different tactile textures (T1(Mp1, 
Mv1) and T2(Mp2,Mv2)), the blended texture (Tb(Mpb, Mvb)) can be calculated as follows: Mpb(x, y) = Mp1(x, 
y).Mp2(x, y) Mvb(x, y) = Mpb(x, y).(Mv1(x, y).Mv2(x, y)) For feature (3), we implemented temporal patterns 
of electro-tactile feedback. In previous works, the special distribution of electrode intensities in 
the finger pad was not controlled according to the human motion. Here, we measured the movement and the 
contact state of the finger, and used the data to provide tactile feedback according to the velocity 
and pressure of the finger. 3. System Configurations This system is composed of a Tactual Cap , Touch 
palette and Texture Canvas . Tactual Cap : a cap shaped device, as shown in Figure 2, that consists of 
a high-density electrode matrix for providing tactile feedback, a pressure sensor for estimating the 
contact state of the fingertip, and tracking markers for detecting the fingertip posture. Touch Palette 
: a palette shaped display that consists of a touch panel. Several types of visuo-tactile textures called 
tactile colors are arranged as shown in Figure 3. Texture Canvas : we propose two types of canvases 
a 2D canvas and a 3D canvas, as shown Figure 4. The 2D canvas consists of a touch panel monitor mounted 
on an easel. It is used for painting a tactile picture. The 3D canvas consists of a solid screen, a projector, 
and motion tracking cameras. It is used for surface prototyping. The user blendes the tactile colors 
together on the Touch Palette using a finger with the Tactual Cap, which creates various tactile colors. 
Then the user touches and draws a visuo-tactile painting on the Texture Canvas at will with tactile feedback. 
The user can appreciate the visuo-tactile picture he/she painted on the basis of the tactile feedback. 
The painted tactile pictures are archived, and other users can enjoy them with stimulating tactile sensations. 
 References [1] Han, J.Y. 2006. Multi-touch interaction wall. ACM SIGGRAPH Emerging Technology. [2] Kajimoto, 
H., Inami, M., Kawakami, N., AND Tachi, S. 2004 SmartTouch: Electric skin to touch the untouchable. IEEE 
CG&#38;A 24:36-43 [3] Kajimoto, H., Kawakami, N., Maeda, T., AND Tachi, S. 2001 Electrocutaneous Display 
as an Interface to a Virtual Tactile World, IEEE Virtual Reality Conf., Yokohama,Japan . Copyright is 
held by the author / owner(s). SIGGRAPH 2010, Los Angeles, California, July 25 29, 2010. ISBN 978-1-4503-0210-4/10/0007 
 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2010</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1836832</article_id>
		<sort_key>110</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>11</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>11</seq_no>
		<title><![CDATA[FuSA<sup>2</sup> touch display]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836821.1836832</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1836832</url>
		<abstract>
			<par><![CDATA[<p>Touching, stroking and pulling are important ways to communicate with fibratus material. Above all, stroking is one of the most representative ways to interact with fibratus material because stroking lets people feel its direction, hardness and thickness. We propose a fibratus visual and tactile display, "FuSA<sup>2</sup> Touch Display" (Figure 1) that gives those tactile sensations and visual feedback. In the proposed system, the visual display and multi-touch input detection technique are integrated into simple construction using plastic optical fiber (POF) bundles. We implement a multi-touch detection technique based on POF's feature and camera image, without using additional sensors. The proposed system provides the image in response to the touch input.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>H.5.2</cat_node>
				<descriptor>Input devices and strategies (e.g., mouse, touchscreen)</descriptor>
				<type>S</type>
			</primary_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003120.10003121.10003125</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10011666</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Touch screens</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003125</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10011666</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Touch screens</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2264155</person_id>
				<author_profile_id><![CDATA[81466645886]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Kosuke]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nakajima]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Osaka University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264156</person_id>
				<author_profile_id><![CDATA[81100329599]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Yuichi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Itoh]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Osaka University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264157</person_id>
				<author_profile_id><![CDATA[81442604676]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Ai]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yoshida]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Osaka University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264158</person_id>
				<author_profile_id><![CDATA[81351601788]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Kazuki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Takashima]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Osaka University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264159</person_id>
				<author_profile_id><![CDATA[81100273942]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Yoshifumi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kitamura]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tohoku University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264160</person_id>
				<author_profile_id><![CDATA[81100339162]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Fumio]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kishino]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Kwansei Gakuin University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Figure 1: FuSA2 Touch Display Figure 2: System construction Figure 3: Touch detection 1 Introduction 
For stable input detection, the whole area of the upper-side surface always needs to be supplied an even 
bright light from the projector. Touching, stroking and pulling are important ways to communicate The 
surface, however, is not always given an even light because with .bratus material. Above all, stroking 
is one of the most rep­ of the unevenness of the projected image s brightness. Therefore, resentative 
ways to interact with .bratus material because stroking projection images are switched alternately between 
visual feedback lets people feel its direction, hardness and thickness. We propose and bright monochromatic 
light for touch input detection at a speed a .bratus visual and tactile display, FuSA2 Touch Display 
(Fig­ high enough to avoid the .icker. To capture the re.ected light from ure 1) that gives those tactile 
sensations and visual feedback. In the the only monochromatic light, we set a liquid-crystal shutter 
in front proposed system, the visual display and multi-touch input detection of the camera. This shutter 
opens and closes in synchronization technique are integrated into simple construction using plastic op­ 
with projection image switching in order to prevent the camera from tical .ber (POF) bundles. We implement 
a multi-touch detection capturing the re.ected light of visual feedback. technique based on POF s feature 
and camera image, without us­ ing additional sensors. The proposed system provides the image in response 
to the touch input. 3 Application We implement simple and delightful application with this system. 2 
FuSA2 Touch Display When users touch or stroke the .bratus display, the touched areas change the color. 
The colored area follows the stroke, and fades FuSA2 Touch Display has simple structure using POF bundles, 
a camera and a projector as shown in Figure 2. In order to make the away in time. Users can get a tactile 
feeling of the .bratus and see the visual feedback depending on stroking speed and touched size. .exible 
.bratus display, we use POFs that are long, thin and .exible like hairs. The upper-side surface of the 
display consists of arrayed 4 Conclusion POF bundles. A half of .bers in each bundle are led to the surface 
in front of a camera and arrayed in the same order as the upper-side We proposed .bratus multi-touchable 
diaplay FuSA2 Touch Dis­ surface. In the same manner, the other .bers are led to and arrayed play . The 
proposed system with simple construction using plastic in front of a projector. optical .ber (POF) budnles, 
a camera and a projector can show vi­ sual feedback and detect multi-touch input by leveraging diffusion 
In this construction, the system displays images on upper-side sur­ of the projection light on the human 
skin. In this way, users can face by a projector, and detects multi-touch input by using the pro­ touch 
and stroke the information directly via the .bratus materials. jection light. The system projects image 
to the projector-side sur- In the future, this system can be applied to various situation, for face of 
the POF bundles. The projected light comes out from the example, robot skins, grass .elds of a stadium 
and so on. As a next upper-side surface. As shown in Figure 3, when users touch the step of this work, 
we are planning to enlarge the size of the display, upper-side surface, the light is re.ected diffusely 
and gets into the to make the resolution higher, and to develop a new input technique POFs led to the 
camera-side surface. The re.ected light comes out to sense a force on the .bers. from POFs that are corresponding 
to the touched area. The camera captures this light from the camera-side surface, and the system rec­ 
ognizes the touch input from lighting area. This recognition process includes some image processing for 
noise reduction. email:hi-fusa@hi-mail.ise.eng.osaka-u.ac.jp e-mail:kitamura@riec.tohoku.ac.jp e-mail:kishino@kwansei.ac.jp 
 Copyright is held by the author / owner(s). SIGGRAPH 2010, Los Angeles, California, July 25 29, 2010. 
ISBN 978-1-4503-0210-4/10/0007 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2010</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1836833</article_id>
		<sort_key>120</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>12</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>12</seq_no>
		<title><![CDATA[Gesture-world technology]]></title>
		<subtitle><![CDATA[3D hand pose estimation system for unspecified users using a compact high-speed camera]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836821.1836833</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1836833</url>
		<abstract>
			<par><![CDATA[<p>This technology allows people to control devices such as computers, communications devices, household appliances, and robots by means of everyday gestures without using sensors or controllers, which employs the high-speed and high-accuracy computer vision technology capable of estimating the human hand and arm poses captured by a compact high-speed camera.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>H.5.2</cat_node>
				<descriptor>Input devices and strategies (e.g., mouse, touchscreen)</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>H.5.2</cat_node>
				<descriptor>Theory and methods</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>H.5.2</cat_node>
				<descriptor>Interaction styles (e.g., commands, menus, forms, direct manipulation)</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003120.10003121.10003124</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003126</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->HCI theory, concepts and models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003123.10011758</concept_id>
				<concept_desc>CCS->Human-centered computing->Interaction design->Interaction design theory, concepts and paradigms</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10011666</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Touch screens</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003125</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2264161</person_id>
				<author_profile_id><![CDATA[81319493088]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Kiyoshi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hoshino]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Tsukuba]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264162</person_id>
				<author_profile_id><![CDATA[81421597791]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Motomasa]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tomida]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Tsukuba]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264163</person_id>
				<author_profile_id><![CDATA[81319502317]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Takanobu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tanimoto]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Tsukuba]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1184035</ref_obj_id>
				<ref_obj_pid>1183918</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[
K. Hoshino and T. Tanimoto: Real time search for similar hand images from database for robotic hand control, IEICE Transactions on Fundamentals of Electronics, Communications and Computer Sciences, E88-A, 10, pp. 2514--2520, 2005.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1401626</ref_obj_id>
				<ref_obj_pid>1401615</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[
K. Hoshino and M. Tomida: Copycat arm, SIGGRAPH 2008, New Tech Demos, Full Conference DVD-ROM, 2164, p. 1, 2008.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[
K. Hoshino and M. Tomida: 3D hand pose estimation using a single camera for unspecified users, Journal of Robotics and Mechatronics, 21, 6, pp. 749--757, 2009.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Gesture-World Technology - 3D hand pose estimation system for unspecified users using a compact high-speed 
camera - Kiyoshi Hoshino Motomasa Tomida 1 Introduction This technology allows people to control devices 
such as computers, communications devices, household appliances, and robots by means of everyday gestures 
without using sensors or controllers, which employs the high-speed and high-accuracy computer vision 
technology capable of estimating the human hand and arm poses captured by a compact high-speed camera. 
The technology behind our exhibit will change standard practices with regard to the mechanical operation 
of equipment such as information devices and robots. Because fingers are articulated structures, they 
can assume many complex shapes, which often result in the problem of self-occlusion. Despite their small 
size relative to the rest of the body, fingers are also capable of moving in a wide 3D space, due to 
movements of the arms or the body. For these reasons, it has not been easy to estimate hand poses by 
non-contact means, using a monocular camera or a pair of cameras at close-range. In recent years, however, 
high-speed cameras have become more compact and inexpensive. Therefore, if we can achieve fast and accurate 
3D hand pose estimation, using only camera images (in other words, without sensors) and without the need 
to strictly fix the camera position, we expect that the technology will be applied in a wide range of 
areas. Examples of such applications could include gesture-based computer operation, virtual games, remote 
control without a remote controller, digital archiving of artisan skills, and remote robot control. The 
need to attach sensors or find and use special controllers will disappear. Our system focuses on achieving 
highly accurate hand pose estimation for unspecified users by constructing an enormous database including 
bone thickness and length, joint range of motion, and habitual finger movements, by thoroughly reducing 
the dimensionality of the image features in the data set used for comparison with the input hand images. 
If the image features that express each hand pose were of extremely low dimensionality, it would be possible 
to prepare a database that includes differences among people. We reduce the dimensionality to 64 or less, 
or 1/25th of the original image features. With an image feature reduced to 1/25th, it is clearly possible 
to upload data for 25 times as many hand poses to the memory. Takanobu Tanimoto (University of Tsukuba) 
In the Conference, we will show you a virtual clay art system, with which you can form 3D models by moving 
your hand and arm as desired, and/or a remote control system of a robot, with which you can control a 
manipulator by your hand and arm movement similar to movements in your dairy living. 2 System configuration 
Using CG editing software, it is relatively easy to generate an enormous hand pose database [1]. We have 
a data set of about 30,000 features [2] thus fur, and if we increase the size of the database by 25 times, 
to 750,000 features, by thoroughly reducing the dimensionality of the image features in the data sets, 
we will still be able to upload it to a memory. By varying parameters such as bone thickness and length 
and finger bending habits (in other words, individual differences in joint range of motion and angles 
of bending within that range), we will generate a hugely varied and diverse collection of hand poses. 
Using the above method, we have achieved accurate hand pose estimation even where there are individual 
differences. The next step is to increase the speed. Our team has previously proposed a classification 
of features to allow for narrowing a search in terms of wrist rotation, thumb bending, and four fingers 
bending [3]. We can use this classification of features to perform a first-stage filtering, to be followed 
by a second stage of precise similarity matching. This 2-stage searching will make it possible to find 
the most similar pose from a database of roughly one million features at about 150 fps. References [1] 
K.Hoshino and T.Tanimoto: Real time search for similar hand images from database for robotic hand control, 
IEICE Transactions on Fundamentals of Electronics, Communications and Computer Sciences, E88-A, 10, pp.2514-2520, 
2005. [2] K.Hoshino and M.Tomida: Copycat arm, SIGGRAPH 2008, New Tech Demos, Full Conference DVD-ROM, 
2164, p.1, 2008. [3] K.Hoshino and M.Tomida: 3D hand pose estimation using a single camera for unspecified 
users, Journal of Robotics and Mechatronics, 21, 6, pp.749-757, 2009.  Fig. 1: Virtual clay art system. 
hoshino@esys.tsukuba.ac.jp http://www.kz.tsukuba.ac.jp/~hoshino/ Copyright is held by the author / owner(s). 
SIGGRAPH 2010, Los Angeles, California, July 25 29, 2010. ISBN 978-1-4503-0210-4/10/0007  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2010</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1836834</article_id>
		<sort_key>130</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>13</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>13</seq_no>
		<title><![CDATA[Haptic canvas]]></title>
		<subtitle><![CDATA[dilatant fluid based haptic interaction]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836821.1836834</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1836834</url>
		<abstract>
			<par><![CDATA[<p>Some kinds of substances naturally attract people to touch them. The slurry made from water and starch is one of the haptically fascinating substances, and presents the amusing but mysterious sensation like playing in the mud when you were in childhood. The mysterious haptic sensation comes from the property of the fluid, "dilatancy", which is the change in the state from liquid-like to solid-like according to the external force. The purpose of this study is to establish the new haptic interaction by presenting both direct touch and mechanically variable haptic sensations with the slurry made from water and starch-i.e., "dilatant fluid".</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>H.5.2</cat_node>
				<descriptor>Haptic I/O</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>H.5.2</cat_node>
				<descriptor>Interaction styles (e.g., commands, menus, forms, direct manipulation)</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003120.10003121.10003124</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10011752</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Haptic devices</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2264164</person_id>
				<author_profile_id><![CDATA[81466647007]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Shunsuke]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yoshimoto]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Osaka University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264165</person_id>
				<author_profile_id><![CDATA[81466640900]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Yuki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hamada]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Osaka University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264166</person_id>
				<author_profile_id><![CDATA[81430676815]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Takahiro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tokui]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Osaka University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264167</person_id>
				<author_profile_id><![CDATA[81466648513]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Tetsuya]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Suetake]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Osaka University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264168</person_id>
				<author_profile_id><![CDATA[81317494648]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Masataka]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Imura]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Osaka University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264169</person_id>
				<author_profile_id><![CDATA[81100594294]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Yoshihiro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kuroda]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Osaka University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264170</person_id>
				<author_profile_id><![CDATA[81100518076]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>7</seq_no>
				<first_name><![CDATA[Osamu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Oshiro]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Osaka University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[
Blake, J., and Gurocak, H. B. 2009. Haptic glove with mr brakes for virtual reality. <i>IEEE/ASME Transactions on Mechatronics 14</i>, 5, 606--615.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[
Han, Y. M., Kang, P. S., Sung, K. G., and Choi, S. B. 2007. Force feedback control of a medical haptic master using an electrorheological fluid. <i>Journal of Intelligent Material Systems and Structures 18</i>, 12, 1149--1154.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[
Saga, S., and Deguchi, K. 2009. Dilatant fluid based tactile display (2nd report). In <i>Proceedings of the 2009 JSME Conference on Robotics and Mechatoronics</i>, 2P1--L04.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Haptic Canvas: Dilatant .uid based haptic interaction Shunsuke YOSHIMOTO,* Yuki HAMADA, Takahiro TOKUI, 
Tetsuya SUETAKE, Masataka IMURA, Yoshihiro KURODA, Osamu OSHIRO Osaka University 1 Introduction Some 
kinds of substances naturally attract people to touch them. The slurry made from water and starch is 
one of the haptically fascinating substances, and presents the amusing but mysterious sensation like 
playing in the mud when you were in childhood. The mysterious haptic sensation comes from the property 
of the .uid, dilatancy , which is the change in the state from liquid­like to solid-like according to 
the external force. The purpose of this study is to establish the new haptic interaction by present­ing 
both direct touch and mechanically variable haptic sensations with the slurry made from water and starch-i.e., 
dilatant .uid . Haptic Canvas is a haptic entertainment system presented by dilatant .uid with the controlling 
grove. Like painting a pic­ture, users can blend haptic paints to create a new haptic sensa­tion by touching 
virtual haptic paints represented in the shallow pool .lled with dilatant .uid, thus coloring and drawing 
a hap­tic picture. The haptic paints called Haptic Primary Colors are stickiness , hardness and roughness 
sensations represented by the proposed haptic glove. Users can have the experience of distinct multimodal 
haptic sensations created by the controlling grove with the dilatant .uid. 2 Innovation The requirements 
of the system are both direct interaction with the viscoelastic liquid and the control of the dramatic 
change in the state to present variable sensations. Previous haptic de­vices using other functional .uid 
(e.g. Electrorheological .uid or Magnetorheological .uid) [Han et al. 2007; Blake and Guro­cak 2009] 
are not suf.cient for the former, while the tactile dis­play using dilatant .uid by using vibrations 
[Saga and Deguchi 2009] is not suf.cient for the latter. This study proposes a new mechanism for the 
haptic interaction by using jamming of the .uid caused by suction of the water. Jamming is the mecha­nism 
by which particulate material can make transition between a liquid-like and a solid-like state with external 
energy. We inten­tionally cause the jamming by using the sucking structure with .ltration of the particle 
of the starch. Furthermore, we develop the haptic glove which enables users to make direct interaction 
with the viscoelastic liquid and to feel the dramatic change of the substance state in their hand by 
attaching the structure to the .ngertip of the glove. The mechanism of the presenting haptic sensation 
is as follows. First, users attach the sucking tube with the .lter of the parti­cle to their .ngertip, 
and put the .nger into the dilatant .uid. Next, during suction, the particles are gathering around the 
.l­ter, making the concentration of the slurry higher thus leading the jamming. Because the density determines 
the viscosity of the liquid, we can mechanically control the viscosity of the dila­tant .uid by controlling 
the amount of the accumulated particles. Finally, the friction between the accumulated particle around 
the .ngertip and the precipitated particle at the bottom of the pool works as a break when the .nger 
moves. The proposed haptic device has four advantages. 1) The de­vice enables users to touch directly 
with the dilatant .uid and *e-mail:yoshimoto@bpe.es.osaka-u.ac.jp Figure 1: Haptic Canvas: Red, green 
and blue correspond to sticki­ness, hardness and roughness respectively. When users touch the color, 
they can feel the haptic sensation varying with brightness value. to present distinct sensation of the 
viscoelastic .uid, like slimi­ness sensation so that exhibits the haptic entertainment. 2) The glove 
is small and light because of the simple structure consists of the tube and the .lter. 3) The device 
can present large force because the force is presented according to user-intended hand movement. 4) Some 
other sensory modalities can be presented by considering the structure. We can set both sucking/ejecting 
pressure and duration to control the amount of the accumu­lated particles. We found that the sensations 
that the device can present are stickiness , hardness and roughness sensations based on the preliminary 
qualitative analysis of the device. 3 Interaction Fig.1 is the scene of the demonstration of Haptic 
Canvas sys­tem. Users enjoy the visual and haptic interaction of haptic color projected on the canvas 
with the haptic glove. The haptic primary colors are generated by touching the sources of haptic sensation 
(haptic paints) arranged on the canvas and utilized to blend the sensations. Users can move the produced 
haptic pri­mary colors and blend them by visually contacting each other. The intensity of the light is 
directly presenting the intensity of the haptic sensation. The intensity of the primary haptic sensa­tions 
varies with intensity of the colors. User paint a haptic pic­ture by drawing haptic color with feeling 
the haptic sensation. Haptic Canvas system will show that the dilatant .uid based haptic device expands 
the possibility of haptic entertainment.  References BLAKE, J., AND GUROCAK, H. B. 2009. Haptic glove 
with mr brakes for virtual reality. IEEE/ASME Transactions on Mechatronics 14, 5, 606 615. HAN, Y. M., 
KANG, P. S., SUNG, K. G., AND CHOI, S. B. 2007. Force feedback control of a medical haptic master using 
an electrorheological .uid. Journal of Intelligent Material Systems and Structures 18, 12, 1149 1154. 
SAGA, S., AND DEGUCHI, K. 2009. Dilatant .uid based tac­tile display (2nd report). In Proceedings of 
the 2009 JSME Conference on Robotics and Mechatoronics, 2P1 L04. Copyright is held by the author / owner(s). 
SIGGRAPH 2010, Los Angeles, California, July 25 29, 2010. ISBN 978-1-4503-0210-4/10/0007  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2010</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1836835</article_id>
		<sort_key>140</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>14</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>14</seq_no>
		<title><![CDATA[Head-mounted photometric stereo for performance capture]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836821.1836835</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1836835</url>
		<abstract>
			<par><![CDATA[<p>Head-mounted cameras are an increasingly important tool for capturing an actor's facial performance. Such cameras provide a fixed, unoccluded view of the face. The resulting imagery is useful for observing motion capture dots or as input to existing video analysis techniques. Unfortunately current systems are typically affected by ambient light and generally fail to record subtle 3D shape changes between expressions Artistic interventions is often required to cleanup and map the captured performance onto a virtual character. We have developed a system that augments a head-mounted camera with LED-based photometric stereo. The system allows observation of the face independent of the ambient light and records per-pixel surface normal. Our data can be used to generate dynamic 3D geometry, for facial relighting, or as input of machine learning algorithms to accurately control an animated face.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.4.8</cat_node>
				<descriptor>Photometry</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.2.10</cat_node>
				<descriptor>Intensity, color, photometry, and thresholding</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.8</cat_node>
				<descriptor>Stereo</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010225</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision tasks</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010225.10010227</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision tasks->Scene understanding</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2264171</person_id>
				<author_profile_id><![CDATA[81100556708]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Andrew]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Jones]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Southern California]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264172</person_id>
				<author_profile_id><![CDATA[81421597317]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Graham]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Fyffe]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Southern California]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264173</person_id>
				<author_profile_id><![CDATA[81440613130]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Xueming]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Southern California]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264174</person_id>
				<author_profile_id><![CDATA[81470641515]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Alex]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ma]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Southern California]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264175</person_id>
				<author_profile_id><![CDATA[81440600975]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Jay]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Busch]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Southern California]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264176</person_id>
				<author_profile_id><![CDATA[81100467115]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Mark]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bolas]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Southern California]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264177</person_id>
				<author_profile_id><![CDATA[81100086933]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>7</seq_no>
				<first_name><![CDATA[Paul]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Debevec]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Southern California]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>378090</ref_obj_id>
				<ref_obj_pid>378040</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[
Cootes, T. F., Edwards, G. J., and Taylor, C. J. 2001. Active appearance models. <i>IEEE Trans. Pattern Anal. Mach. Intell. 23</i>, 6, 681--685.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2383925</ref_obj_id>
				<ref_obj_pid>2383894</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[
Malzbender, T., Wilburn, B., Gelb, D., and Ambrisco, B. 2006. Surface enhancement using real-time photometric stereo and reflectance transformation. In <i>Rendering Techniques 2006: 17th Eurographics Workshop on Rendering</i>, 245--250.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[
Woodham, R. 1980. Photometric method for determining surface orientation from multiple images. <i>Optical Engineering 19</i>, 1, 139--144.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Head-mounted Photometric Stereo for Performance Capture Andrew Jones Graham Fyffe Xueming Yu Alex Ma 
Jay Busch Mark Bolas Paul Debevec University of Southern California Institute for Creative Technologies 
 (a) (b) (c) (d) (e) Figure 1: (a) Lighting conditions (3 directions and ambient only) captured at 120fps 
(b) Recovered surface albedo, independent of ambient light (c) Recovered surface normals (d) Geometry 
recovered by integrating surface normals (e) 3D facial rig driven by albedo and normals. Head-mounted 
cameras are an increasingly important tool for cap­turing an actor s facial performance. Such cameras 
provide a .xed, unoccluded view of the face. The resulting imagery is useful for observing motion capture 
dots or as input to existing video anal­ysis techniques. Unfortunately current systems are typically 
af­fected by ambient light and generally fail to record subtle 3D shape changes between expressions Artistic 
interventions is often required to cleanup and map the captured performance onto a vir­tual character. 
We have developed a system that augments a head­mounted camera with LED-based photometric stereo. The 
system allows observation of the face independent of the ambient light and records per-pixel surface 
normal. Our data can be used to generate dynamic 3D geometry, for facial relighting, or as input of machine 
learning algorithms to accurately control an animated face. We use a Point Grey Grasshopper camera mounted 
20 cm from the face (Fig. 2). A ring of 12 individually controlled LEDs encircles the lens. The LEDs 
repeat a sequence of three illumination patterns, followed by an unlit frame to record and subtract ambient 
light (Fig. 1a); crossed linear polarizers on the lights and lens attenuate spec­ular re.ection from 
the face. The camera and lights run at 120 fps, yielding albedo and normal estimates (Fig. 1b,c) at 30 
fps. ramps across the twelve LEDs, rotated at 0, 120, and 240 degrees. Using the full ring of lights 
provides more even illumination, re­duces shadow artifacts, and emits light from a wider area to increase 
actor comfort; our video shows results using three individual LEDs for comparison. LED .icker can be 
greatly reduced by switching lightings patterns at a faster framerate, while using shorter expo­sure 
time skip extra frames. To further eliminate distraction from the lights, we built a second lighting 
rig using invisible infrared LEDs, leveraging the broad spectral sensitivity of the camera. This provided 
similar results at the expense of some detail in the photo­metric normals due to increased subsurface 
scattering. To correct for subject motion, we compute optical .ow between similar illumination patterns 
to temporally align each set of pat­terns. Our light sources are close to the face, violating the assump­tion 
of distant illumination. To compensate, we compute per-pixel lighting directions relative to a plane 
approximating the face. To estimate 3D performance geometry, we integrate the surface nor­mals using 
Gaussian belief propagation. As expected, the geometry (Fig. 1d) suffers from low-frequency distortions 
yet reveals expres­sive performance detail in 3D. The results can be used as input to a machine learning 
algorithm to drive a facial rig with the perfor­mance after an initial training phase. For an initial 
test, we use an active appearance model [Cootes et al. 2001] to .nd blendshape weights for a given set 
of albedo and normal maps. Our initial re­sults are restricted to phonemes (Fig. 1e) and we are working 
to extend this algorithm to animating the entire face. Initial results show that analysis of normals 
and albedo provides smoother ani­mation than analysis of albedo alone. We are also working to mini­mize 
system weight, which should not be signi.cantly greater than existing rigs as the LEDs weigh just a few 
grams each. References COOTES, T. F., EDWARDS, G. J., AND TAYLOR, C. J. 2001. Active appearance models. 
IEEE Trans. Pattern Anal. Mach. Intell. 23, 6, 681 685. MALZBENDER, T., WILBURN, B., GELB, D., AND AMBRISCO, 
B. 2006. Surface enhancement using real-time photometric stereo and re.ectance transformation. In Rendering 
Techniques 2006: 17th Eurographics Workshop on Rendering, 245 250. WOODHAM, R. 1980. Photometric method 
for determining surface orientation from multiple images. Optical Engineering 19, 1, 139 144. Copyright 
is held by the author / owner(s). SIGGRAPH 2010, Los Angeles, California, July 25 29, 2010. ISBN 978-1-4503-0210-4/10/0007 
  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2010</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1836836</article_id>
		<sort_key>150</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>15</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>15</seq_no>
		<title><![CDATA[In-air typing interface for mobile devices with vibration feedback]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836821.1836836</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1836836</url>
		<abstract>
			<par><![CDATA[<p>Recently the miniaturization of mobile devices has progressed and such devices are difficult to have input interface that has wide operation area on their surface. Conventional input interface on a cell phone, such as a touch panel or keypad, has limited operation area. There has been many approaches to handle this problem, but they require users to wear some physical devices[Harrison and Hudson 2009] or to use in some specific environments[Roeber et al. 2003].</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>H.5.2</cat_node>
				<descriptor>Input devices and strategies (e.g., mouse, touchscreen)</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>H.5.2</cat_node>
				<descriptor>Theory and methods</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>H.5.2</cat_node>
				<descriptor>Haptic I/O</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10011752</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Haptic devices</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003126</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->HCI theory, concepts and models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003123.10011758</concept_id>
				<concept_desc>CCS->Human-centered computing->Interaction design->Interaction design theory, concepts and paradigms</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10011666</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Touch screens</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003125</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2264178</person_id>
				<author_profile_id><![CDATA[81466647861]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Takehiro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Niikura]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264179</person_id>
				<author_profile_id><![CDATA[81466647546]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Yuki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hirobe]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264180</person_id>
				<author_profile_id><![CDATA[81100408023]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Alvaro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Cassinelli]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264181</person_id>
				<author_profile_id><![CDATA[81100363385]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Yoshihiro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Watanabe]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264182</person_id>
				<author_profile_id><![CDATA[81100009594]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Takashi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Komuro]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264183</person_id>
				<author_profile_id><![CDATA[81100353752]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Masatoshi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ishikawa]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>964604</ref_obj_id>
				<ref_obj_pid>964568</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Baker, S., and Matthews, I. 2004. Lucas-kanade 20 years on: A unifying framework. <i>International Journal of Computer Vision 56</i>, 3, 221--255.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1622199</ref_obj_id>
				<ref_obj_pid>1622176</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Harrison, C., and Hudson, S. E. 2009. Abracadabra: Wireless, high-precision, and unpowered finger input for very small mobile devices. In <i>Proceedings of the 22nd Annual ACM Symposium on UIST</i>, ACM, 121--124.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>765944</ref_obj_id>
				<ref_obj_pid>765891</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Roeber, H., Bacus, J., and Tomasi, C. 2003. Typing in thin air the canesta projection keyboard - a new method of interaction with electronic devices. <i>CHI03 extended abstracts on Human factors in computing systems</i>, 712--713.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Figure 1: Prototype System Image Capture and Preprocessing Small operation area on the surface of mobile 
devices is a signif­icant problem. We used a camera with a wide-angle lens so that we can operate it 
in a wide 3D space. Since the images obtained through a wide-angle lens is distorted, we applied distortion 
cor­rection to the obtained images. As a result, the angle of view of 90 degrees is acquired. Since the 
.ngertip near the camera moves fast in the image, a high-frame-rate camera is used for stable tracking. 
Using skin color extraction, the .ngertip region image is extracted. *e-mail:Takehiro Niikura@ipc.i.u-tokyo.ac.jp 
 3D Fingertip Tracking In this interface, the 3D position of .ngertip is estimated by track­ing in order 
to gain high-accuracy position. When a .ngertip moves in 3D space, the shape of the .ngertip in the image 
is trans­formed in various ways. Four parameters are estimated to track the user s .ngertip: translation 
along the plane perpendicular to the camera s optical axis, rotation around the optical axis and scale 
change. The Lucas-Kanade Algorithm is used to estimate these pa­rameters[Baker and Matthews 2004]. Before 
tracking starts, users have to register their .ngertip image as a template. Since the scale change of 
the .ngertip is inversely proportional to the distance be­tween .nger and camera, the 3D position of 
the .ngertip can be estimated. Detection of Keystroke Action and Vibration Feedback In desirable input 
interface, user can type letters quickly when user get accustomed to the system. We de.ned the following 
gesture as keystroke action. The gesture is slightly moving a .ngertip back and forth in the direction 
of the camera s optical axis, like typing with a physical keyboard. In this system, we utilized the difference 
of the dominant frequency of the .ngertip s scale in images so that a keystroke action can be detected. 
To realize input interface in which we can type letters fast, tactile feedback is important. With tactile 
feedback, users can quickly rec­ognize that the input action of the users is detected. From this re­spect, 
we attached a vibration motor on the back side of the display which vibrates for a short time when a 
keystroke action is detected. 3 Applications In-air Typing Keyboard is an application in which user 
can type let­ters in the air. The pointer of software keyboard moves according to the position of the 
.ngertip. If the pointer is located on the tar­get key and a keystroke action is detected, the target 
character is input. By utilizing the depth information of the .ngertip position, other application examples 
can also be constructed: Zooming Pic­ture Viewer and 3D Painter. In Zooming Picture Viewer, the user 
can zoom and scroll the picture on the display with the 3D position of the .ngertip. The user can draw 
lines in 3D space with .ngertip using 3D position directly in 3D painter. References BAKER, S., AND 
MATTHEWS, I. 2004. Lucas-kanade 20 years on: A unifying framework. International Journal of Computer 
Vision 56, 3, 221 255. HARRISON, C., AND HUDSON, S. E. 2009. Abracadabra: Wire­less, high-precision, 
and unpowered .nger input for very small mobile devices. In Proceedings of the 22nd Annual ACM Sym­posium 
on UIST, ACM, 121 124. ROEBER, H., BACUS, J., AND TOMASI, C. 2003. Typing in thin air the canesta projection 
keyboard -a new method of interaction with electronic devices. CHI03 extended abstracts on Human factors 
in computing systems, 712 713. Copyright is held by the author / owner(s). SIGGRAPH 2010, Los Angeles, 
California, July 25 29, 2010. ISBN 978-1-4503-0210-4/10/0007 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2010</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1836837</article_id>
		<sort_key>160</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>16</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>16</seq_no>
		<title><![CDATA[Lumino]]></title>
		<subtitle><![CDATA[tangible building blocks based on glass fiber bundles]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836821.1836837</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1836837</url>
		<abstract>
			<par><![CDATA[<p>We present <i>luminos</i>, tangible building blocks that allow users to assemble physical 3D structures on a tabletop computer. All luminos are tracked using the table's built-in camera, including those luminos located on top of other luminos. To enable this, each lumino contains not only a fiducial marker, but also a glass fiber bundle that allows the camera to "see through it". Applications include a tangible image editing application, a board game, and a tangible 3D construction kit.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Physically based modeling</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010352.10010379</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation->Physical simulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003741.10003742.10003745</concept_id>
				<concept_desc>CCS->Mathematics of computing->Continuous mathematics->Topology->Geometric topology</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2264184</person_id>
				<author_profile_id><![CDATA[81100137268]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Patrick]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Baudisch]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Hasso-Plattner-Institute, Potsdam, Germany]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264185</person_id>
				<author_profile_id><![CDATA[81460645003]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Torsten]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Becker]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Hasso-Plattner-Institute, Potsdam, Germany]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264186</person_id>
				<author_profile_id><![CDATA[81460656798]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Frederik]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Rudeck]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Hasso-Plattner-Institute, Potsdam, Germany]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1731916</ref_obj_id>
				<ref_obj_pid>1731903</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Bartindale, T. and Harrison, C. Stacks on the Surface: Resolving Physical Order Using Fiducial Markers With Structured Transparency. In <i>Proc. Tabletop '09</i>, 4 pages.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1753500</ref_obj_id>
				<ref_obj_pid>1753326</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Baudisch, P. Becker, T, and Rudeck, F. Lumino: Tangible Blocks for Tabletop Computers Based on Glass Fiber Bundles. To appear in <i>Proceedings of CHI 2010</i>, 10 pages.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>258715</ref_obj_id>
				<ref_obj_pid>258549</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Ishii, H., Ullmer, B. Tangible Bits: Towards Seamless Interfaces Between People, Bits and Atoms. In <i>Proc. CHI '97</i>, 234--241.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Lumino: Tangible Building Blocks Based on Glass Fiber Bundles  Patrick Baudisch Torsten Becker Frederik 
Rudeck Hasso-Plattner-Institute, Potsdam, Germany Abstract We present luminos, tangible building blocks 
that allow users to assemble physical 3D structures on a tabletop computer. All lu-minos are tracked 
using the table s built-in camera, including those luminos located on top of other luminos. To enable 
this, each lumino contains not only a fiducial marker, but also a glass fiber bundle that allows the 
camera to see through it . Applica-tions include a tangible image editing application, a board game, 
and a tangible 3D construction kit. 1 Introduction Physical building blocks allow quick construction 
and manipula-tion of structures through two-handed tactile interaction [ISHII &#38; ULLMER,1992]. However, 
traditional tracking mechanisms, such as magnetic tracking make it hard to manage large numbers of ob-jects 
a prerequisite for complex tangible applications. Tabletop computers based on diffuse illumination, in 
contrast, make it easy to track visual barcodes, aka fiducial markers, placed on their surface. Unfortunately, 
they do not allow tracking build-ing block constructions, because (1) blocks located directly on the 
table surface prevent its camera from seeing blocks stacked on top and (2) the table s built-in diffuser 
makes objects unrecognizably blurry, unless they are in direct contact with the table surface. So far, 
researchers have therefore only been able to track stacks of foil-like objects [BARTINDALE, T. &#38; 
HARRISON, 2009].  Figure 1: Luminos are tangible building blocks that allow the underlying diffuse illumination 
table to track their 3D arrangement. The shown lu-mino construction kit supports users with suggestions, 
such as the light-ning-shaped overhang warning icon. Callout: The shown type of lumino contains a marker 
strip and a slanted glass fiber bundle. 2 Luminos Luminos (Figure 1) overcome these limitations [BAUDISCH, 
BECKER, &#38; RUDECK, 2010]. Each block contains a glass fiber bundle, i.e., a large number of parallel, 
vertical glass fibers. As illustrated by Figure 2, this allows the table (here Microsoft Sur-face) to 
see through luminos. On its way back down to the dif-fuser, light reflected off the marker is confined 
within the fibers; this prevents the marker image from blurring, so that the camera in the table sees 
a crisp image. To prevent a vertical arrangement of markers from occluding each other, the glass fiber 
bundles inside of luminos are never fully vertical. In the design shown in Figure 1, for example, glass 
fiber bundles are slanted. This causes each marker image to be projected onto the space next to the marker 
below it, rather than directly onto it. The round luminos in Figure 2 demagnify marker images; this causes 
the markers of stacked blocks to appear as concentric rings.  Figure 2: (a) These round luminos use 
a ring-shaped marker and contain a tapered glass fiber bundle. (b) When stacked, the marker on top shows 
up in the center. (c) How it works: the glass fiber bundle prevents the light reflected off a marker 
from spreading on its way to the diffuser. While luminos extend the concept of fiducial markers to 3D, 
they preserve many of the benefits of regular tabletop markers: luminos are unpowered, self-contained 
objects that require no calibration, making it easy to maintain a large number of them. 3 Applications 
We have created three demo applications. In lumino checkers, luminos allow the table to tell men from 
kings (Figure 2a, b). Image Touch-Up (Figure 3) allows users to improve digital photos by manipulating 
stacks of dials placed on top of a photo. A single dial allows removing color casts. An additional dial 
on top allows adjusting contrast, and so on. Stamping other images with a stack of dials applies the 
correction to other images.  Figure 3: (a) The bottom dial removes a color cast; the top dial adjusts 
contrast. (b) Applying the same effect to other images by stamping . The lumino construction kit (Figure 
1), finally, allows users to put together building block constructions. The kit is a simple en-visionment 
of how a future tangible application might support architects. Luminos allow the table to keep track 
of the prototypes and design explorations on the table as the user is creating them. This allows the 
table to take on the role of an assistant, here a civil engineer. The system logs construction activities, 
checks the soundness of the hypothetical building, displays piece lists and running totals of construction 
cost, and informs the user about potential flaws and construction alternatives. References BARTINDALE, 
T. AND HARRISON, C. Stacks on the Surface: Resolving Physical Order Using Fiducial Markers With Structured 
Transparency. In Proc. Tabletop 09, 4 pages. BAUDISCH, P. BECKER, T, AND RUDECK, F. Lumino: Tangible 
Blocks for Tabletop Computers Based on Glass Fiber Bundles. To appear in Pro-ceedings of CHI 2010, 10 
pages. ISHII, H., ULLMER, B. Tangible Bits: Towards Seamless Interfaces Be-tween People, Bits and Atoms. 
In Proc. CHI 97, 234 241. 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2010</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1836838</article_id>
		<sort_key>170</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>17</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>17</seq_no>
		<title><![CDATA[Matrix LED unit with pattern drawing and extensive connection]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836821.1836838</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1836838</url>
		<abstract>
			<par><![CDATA[<p>Matrix LED unit, the array of small LEDs, is widely used for display systems, such as indicator in public transportation vehicles and toys. In these cases, the patterns displayed on the matrix LED unit is stored in the memory card or the host computer systems, and it is impossible for the user to draw the patterns on the matrix LED displays. It is known that LED can also be used as photo sensor, since LED is a kind of the diode, which produces the electric current by the incident light[1]. This paper describes the matrix LED unit system for pattern display and interacting with users. The user can draw patterns by using light source, such as laser pointer, which is implemented by using LED arrays both for displaying and light sensing. Each unit has the communication channel for connected neighboring unit, which enables the system to extend the larger display areas by connecting the units as desired. The drawn pattern is morphed by user's interaction, which is enabled by the tilt sensor equipped in each unit. The pattern morphing is also performed by scrolling patterns across the connected units or so-called 'life game' pattern transition.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>H.5.2</cat_node>
				<descriptor>Interaction styles (e.g., commands, menus, forms, direct manipulation)</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>H.5.2</cat_node>
				<descriptor>Input devices and strategies (e.g., mouse, touchscreen)</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003120.10003121.10003125</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10011666</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Touch screens</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2264187</person_id>
				<author_profile_id><![CDATA[81100214479]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Junichi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Akita]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Kanazawa University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1029653</ref_obj_id>
				<ref_obj_pid>1029632</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[J. Lee <i>et al.</i>, Proc. of UIST2004, pp. 123--126, 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 1 Introduction Matrix LED unit, the array of small LEDs, is widely used for display systems, such as 
indicator in public transporta­tion vehicles and toys. In these cases, the patterns dis­played on the 
matrix LED unit is stored in the memory card or the host computer systems, and it is impossible for the 
user to draw the patterns on the matrix LED dis­plays. It is known that LED can also be used as photo 
sensor, since LED is a kind of the diode, which produces the electric current by the incident light[1]. 
This paper describes the matrix LED unit system for pattern display and interacting with users. The user 
can draw patterns by using light source, such as laser pointer, which is im­plemented by using LED arrays 
both for displaying and light sensing. Each unit has the communication channel for connected neighboring 
unit, which enables the system to extend the larger display areas by connecting the units as desired. 
The drawn pattern is morphed by user s inter­action, which is enabled by the tilt sensor equipped in 
each unit. The pattern morphing is also performed by scrolling patterns across the connected units or 
so-called life game pattern transition. 2 System Implementation The following functions are pointed 
out in the system de­sign. 1. Pattern drawing capability by light sources, such as laser pointer, by 
using LED s light sensitivity. 2. Connecting neighboring units, with communication channel. 3. Unit 
size is set to be equal to the size of matrix LED unit, in order to realize condensed arrange of the 
units.  Figure 1 shows the developed matrix LED unit, which is named as LED Tile (LT). The micro controller 
(CY8C29466, Cypress) and tri-axis acceleration sensor (MMA7361L, Freescale) are mounted on the board, 
whose size is equal to the size of the matrix LED unit. The LT unit has four connectors at the four edges 
of the unit for extending the unit connections and display area, as shown in Fig. 2. The communication 
between the neighboring LTs is carried out in the asynchronous half-duplex serial protocol, which is 
used for pattern morphing across the LTs. The LED produces the weak electric current by incident light, 
and this fact can be used for light sensing, as well as emitting light for displaying. The LTs controls 
the matrix LED unit as the light sensing, followed by the light emit­ting. Light sensing time and light 
emitting time is set as 1 milli second for one column of LED matrix. (a) (b) Figure 2: Connection of 
LTs 3 Pattern Morphing The following three functions of pattern morphing are im­plemented in the current 
LT system. Flowing: The drawn pattern .ows across the con­nected LTs when the user tilts the LTs.  
Scrolling: The drawn pattern scrolls for the speci.ed direction across the connected LTs. Speed and direc­tion 
can be changed by tilt sensor.  Life Game: The drawn pattern makes transition based on the life game 
rule, a kind of cellular au­tomata.  References [1] J.Lee et al., Proc. of UIST2004, pp.123 126, 2004. 
Copyright is held by the author / owner(s). SIGGRAPH 2010, Los Angeles, California, July 25 29, 2010. 
ISBN 978-1-4503-0210-4/10/0007 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2010</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1836839</article_id>
		<sort_key>180</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>18</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>18</seq_no>
		<title><![CDATA[Meta cookie]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836821.1836839</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1836839</url>
		<abstract>
			<par><![CDATA[<p>So far, gustatory information has rarely been studied in relation to computers, even though there are lots of studies on visual, auditory, haptic and olfactory information. This scarcity of research on gustatory information has several reasons. One reason is that gustatory sensation is based on chemical signals, whose functions have not been fully understood yet. Another reason is that perception of gustatory sensation is affected by other factors, such as vision, olfaction, thermal sensation, and memories. Thus, complexity of cognition mechanism for gustatory sensation as described above makes it difficult to build up a gustatory display.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>J.3</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010444</concept_id>
				<concept_desc>CCS->Applied computing->Life and medical sciences</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010444</concept_id>
				<concept_desc>CCS->Applied computing->Life and medical sciences</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2264188</person_id>
				<author_profile_id><![CDATA[81331500452]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Takuji]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Narumi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264189</person_id>
				<author_profile_id><![CDATA[81466640651]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Takashi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kajinami]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264190</person_id>
				<author_profile_id><![CDATA[81100172183]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Tomohiro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tanikawa]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264191</person_id>
				<author_profile_id><![CDATA[81100257385]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Michitaka]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hirose]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[AR cookies: http://mikeclaremikeclare.com/index.php?/systems/ar-cookies/]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Meta Cookie Takuji Narumi , Takashi Kajinami , Tomohiro Tanikawa , Michitaka Hirose Graduate School 
of Engineering,School of Engineering, Graduate School of Information Graduate School of Information the 
University of Tokyo the University of Tokyo Science and Technology, Science and Technology, the University 
of Tokyo the University of Tokyo Fig. 1 Marked Cookie Fig. 2 Meta Cookie 1. Introduction So far, gustatory 
information has rarely been studied in relation to computers, even though there are lots of studies on 
visual, auditory, haptic and olfactory information. This scarcity of research on gustatory information 
has several reasons. One reason is that gustatory sensation is based on chemical signals, whose functions 
have not been fully understood yet. Another reason is that perception of gustatory sensation is affected 
by other factors, such as vision, olfaction, thermal sensation, and memories. Thus, complexity of cognition 
mechanism for gustatory sensation as described above makes it difficult to build up a gustatory display. 
Our research utilizes the complexity of cognition mechanism for gustatory sensation, in order to create 
a pseudo-gustatory display, which induces cross-modal effect by presenting scent. Therefore, for the 
realization of a novel gustatory display system, we aim to establish the method for letting humans feel 
various tastes without changing chemical substances by changing only accompanying information. In this 
paper, we propose the method to change perceived taste of a cookie when they eat by changing appearance 
and smell with augmented reality technology. 2. Meta Cookie: Pseudo-Gustation System We made the system 
to change perceived taste of a cookie by overlaying visual and olfactory information onto a real cookie 
with an AR marker and named it Meta Cookie (Fig. 2). Meta Cookie consists of four parts: a marked plain 
cookie, a marker detection unit, a overlaying visual information unit and an olfactory display. In this 
system, a user wears visual and olfactory display system (Fig. 3). The marker detection unit detects 
the marked cookie and calculates the 6DOF position of the cookie and a distance between the cookie and 
the nose of a user. The user can choose one cookie which he/she wants to eat from multiple kinds of cookies. 
A texture (a photograph of a cookie) and a scent of the cookie which the user selected are Fig. 3 Visual 
and Olfactory Display overlaid onto the cookie based on the calculated position information. We made 
a detectable plain cookie by a camera for this system by reference to Clare s AR Cookie [1]. Since Clare 
uses chocolate for the black part of AR cookie, his AR Cookie is chocolate-flavored. Therefore there 
is a chance that this flavor of chocolate interferes with cross-modal effect which our system evokes 
to change the perceived taste of a cookie. Consequently, we use a plain cookie on the market and print 
a AR marker on it by using branding iron. Fig. 1 illustrates our marked cookie. A camera and ARToolkit 
are used for the marker detection unit. Visual and olfactory display system consists of a HMD and an 
air pump- type olfactory display. A Video see-through HMD displays an appearance of several types of 
cookies (a chocolate cookie, strawberry cookie, tea cookie and so on) on the marked plain cookie based 
on the calculated position information by using ARToolkit. This visual effect let users feel that they 
are eating a selected cookie although they are just eating a marked cookie. Moreover, the air pump-type 
olfactory display produces a scent of the selected cookie. The olfactory display can eject fresh air 
and seven kinds of scented air. And it can control strength of these scents in 127 levels. In Meta Cookie 
, the strength of the produced scent is decided on the basis of the calculated position information. 
Nearer the marked cookie from the user s nose, stronger scent ejects from the olfactory display. This 
olfactory information evokes cross-modal effect between olfaction and gustation and let users feel that 
they are eating a flavored cookie although they are just eating a plain cookie. Currently more than a 
dozen people tried Meta Cookie and almost all of them answered that they feel a change of taste of the 
plain cookie by using our system. References [1]AR cookies: http://mikeclaremikeclare.com/index.php?/systems/ar-cookies/ 
email: {narumi, kaji, tani, hirose}@cyber.t.u-tokyo.ac.jp Copyright is held by the author / owner(s). 
SIGGRAPH 2010, Los Angeles, California, July 25 29, 2010. ISBN 978-1-4503-0210-4/10/0007  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2010</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1836840</article_id>
		<sort_key>190</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>19</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>19</seq_no>
		<title><![CDATA[QuintPixel]]></title>
		<subtitle><![CDATA[multi-primary color display systems]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836821.1836840</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1836840</url>
		<abstract>
			<par><![CDATA[<p>Today's conventional <i>Liquid Crystal Displays</i> (LCD) are assembled with three primary colors (red, green, and blue: RGB). Further, the most common color gamut standards are defined in RGB-primary colors. Such a system lacks the capability to reproduce all of the real-surface colors; for example, sRGB cannot cover Pointer's real-surface color dataset which consists of the measurement over the existing colors in the world except self-luminous objects [Pointer 1980] at most of the hue angles (see Figure 1 right). However, recent developments on display devices have made it possible to have wider color gamut than before. However, as long as there are only three primary colors, such display system still cannot cover the real surface colors efficiently. An example of relatively wide gamut (DCI 2005) in Figure 1 still shows poor coverage ratio against Pointer's dataset.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Color, shading, shadowing, and texture</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>H.5.2</cat_node>
				<descriptor>Screen design (e.g., text, graphics, color)</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003120.10003123</concept_id>
				<concept_desc>CCS->Human-centered computing->Interaction design</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10011666</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Touch screens</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010384</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Texturing</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2264192</person_id>
				<author_profile_id><![CDATA[81490642709]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Kazunari]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tomizawa]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[SHARP Corporation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264193</person_id>
				<author_profile_id><![CDATA[81365592580]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Akiko]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yoshida]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[SHARP Corporation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264194</person_id>
				<author_profile_id><![CDATA[81466646995]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Kohzoh]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nakamura]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[SHARP Corporation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264195</person_id>
				<author_profile_id><![CDATA[81466641193]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Yasuhiro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yoshida]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[SHARP Corporation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[
Chino, E., Tajiri, K., Kawakami, H., Ohira, H., Kamijo, K., Kaneko, H., Kato, S., Ozawa, Y., Kurumisawa, T., Inoue, K., Endo, K., Moriya, H., Aragaki, T., and Murai, K. 2006. Development of wide-color-gamut mobile displays with four-primary-color LCDs. In <i>SID DIGEST</i>, 1221--1224.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[
Pointer, M. R. 1980. The gamut of real surface colours. <i>Color Research and Application 5</i>, pp. 145--155.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[
Yang, Y.-C., Song, K., Rho, S., Rho, N.-S., Hong, S., Deul, K. B., Hong, M., Chung, K., Choe, W., Lee, S., Kim, C. Y., Lee, S.-H., and Kim, H.-R. 2005. Development of six primary-color LCD. In <i>SID DIGEST</i>, 1210--1213.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 § Figure 1: Left: Color gamut comparison of sRGB, DCI, and our QuintPixel against Pointer s dataset 
in xy chromaticity diagram. Right: Coverage ratios against Pointer s dataset at every 10 de­grees of 
hue angle (.). The outermost circle represents 100% cov­erage. 2 Multi-Primary Color Display Systems 
The Multi-Primary Color (MPC) systems solve this problem with one or more additional sub-pixels. There 
have been presented several ideas of MPC systems such as [Yang et al. 2005; Chino et al. 2006], however, 
they neither considered to reproduce the real­surface colors nor had wide-enough color gamut. In this 
paper, we introduce our MPC display system QuintPixel which employs yellow and cyan sub-pixels besides 
RGB. This achieves over 99% coverage ratio against Pointer s dataset. As seen in Figure 1 (left), large 
numbers of colors in yellow and cyan regions are missing in RGB-primary system. QuintPixel in­troduces 
additional yellow and cyan sub-pixels but does not enlarge the area of the overall pixel. By decreasing 
the area per one sub­pixel, we balance high luminance reproduction with real-surface color reproduction. 
QuintPixel also consists of two red sub-pixels for compensating its low luminance reproduction. Additionally, 
the most preferable layout of six sub-pixels are computed by us­ing spatio-chromatic Fourier analysis. 
Finally, QuintPixel leads to *tomizawa.kazunari@sharp.co.jp akiko.yoshida@sharp.co.jp nakamura.kohzoh@sharp.co.jp 
§yoshida.yasuhiro@sharp.co.jp over 99% in the coverage ratio against Pointer s dataset (see Fig­ure 
1 right). It may reproduce the colors of sun.ower s yellow, golden mask of Tutankhamen s mummy, emerald 
green sea, and pigment colors. Such colors are located out of the color gamut of conventional display 
devices. Figure 2: QuintPixel display (left) and its comparable conventional LCD (right). Note much deeper 
reproduction of yellow. Figure 2 shows a screenshot of our QuintPixel emerging technology prototype in 
60-inch size with the resolution of 1920 × 1080 pixels with its comparable conventional display1. Note 
that QuintPixel re­produces much deeper yellow than the conventional LCD. An input signal is given in 
xvYCC format for both displays. Then, if some colors are located out of a display s color gamut, those 
colors are clipped onto a display s gamut for both. QuintPixel may take not only xvYCC but also a conventional 
RGB format. 3 Further Bene.ts and Applications Because most input signals are still in RGB-based primaries, 
there exists a system of equations with three equations and .ve unknowns for reproducing a given color 
in QuintPixel. This mathematically leads to in.nitely many solutions of the combination of primary colors. 
It is a strong advantage of MPC display systems. Power-saving There exist the combinations of primaries 
such that the primaries turn on as least as possible by using a simple linear programming method. Pseudo-super 
resolution MPC display systems consist of much more sub-pixels than RGB-primary display systems. Those 
sub­pixels can be used for sub-pixel rendering for pseudo-super reso­lution which increases perceptual 
resolution of a display device. Rendering improvement at different viewing angles For a given color in 
a pixel, MPC takes the combination of primaries which reproduce the color with the smallest perceptual 
difference in dif­ferent viewing angles. References CHINO, E., TAJIRI, K., KAWAKAMI, H., OHIRA, H., 
KAMIJO, K., KANEKO, H., KATO, S., OZAWA, Y., KURU- MISAWA, T., INOUE, K., ENDO, K., MORIYA, H., ARAGAKI, 
T., AND MURAI, K. 2006. Development of wide-color-gamut mobile displays with four-primary-color LCDs. 
In SID DIGEST, 1221 1224. POINTER, M. R. 1980. The gamut of real surface colours. Color Research and 
Application 5, pp. 145 155. YANG, Y.-C., SONG, K., RHO, S., RHO, N.-S., HONG, S., DEUL, K. B., HONG, 
M., CHUNG, K., CHOE, W., LEE, S., KIM, C. Y., LEE, S.-H., AND KIM, H.-R. 2005. Development of six primary-color 
LCD. In SID DIGEST, 1210 1213. 1These are the photos of actual displays and both color gamuts are re­duced 
as seen on print or display. However, their relative color saturation differences are still presented 
in this .gure. Copyright is held by the author / owner(s). SIGGRAPH 2010, Los Angeles, California, July 
25 29, 2010. ISBN 978-1-4503-0210-4/10/0007  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2010</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1836841</article_id>
		<sort_key>200</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>20</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>20</seq_no>
		<title><![CDATA[RePro3D]]></title>
		<subtitle><![CDATA[full-parallax 3D display using retro-reflective projection technology]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836821.1836841</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1836841</url>
		<abstract>
			<par><![CDATA[<p>Motion parallax is important to recognize the depth of a 3D image. In recent years, many 3D display methods that enable parallax images to be seen with the naked eye have been developed. In addition, there has been an increase in research to design interfaces that enable humans to intuitively interact with and operate 3D objects using their hands. However, realizing 3D object interaction as if the user is actually touching the object in the real world is quite difficult. One of the reasons for this is that the screen shape in conventional methods is restricted to a flat panel. In addition, it is difficult to achieve a balance between displaying the 3D image and sensing the user input. Therefore, we propose a novel full-parallax 3D display system that is suitable for interactive 3D applications. We call this system RePro3D. Our approach is based on a retro-reflective projection technology[Inami et al. 2000]. A number of images from a projector array are projected onto the retro-reflective screen. When a user looks at the screen through a half mirror, he or she, without the use of glasses, can view a 3D image that has motion parallax. We can choose the screen shape depending on the application. Image correction according to the screen shape is not required. Consequently, we can design a touch-sensitive soft screen, a complexly curved screen, or a screen with an automatically moving surface. RePro3D has a sensor function to recognize the user input. Some interactive features, such as operation of 3D objects, can be achieved by using it.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.4.8</cat_node>
				<descriptor>Depth cues</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010225.10010227</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision tasks->Scene understanding</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2264196</person_id>
				<author_profile_id><![CDATA[81421594598]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Takumi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yoshida]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264197</person_id>
				<author_profile_id><![CDATA[81421597185]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Sho]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kamuro]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264198</person_id>
				<author_profile_id><![CDATA[81331499667]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Kouta]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Minamizawa]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264199</person_id>
				<author_profile_id><![CDATA[81100485839]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Hideaki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nii]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264200</person_id>
				<author_profile_id><![CDATA[81365592882]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Susumu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tachi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>835773</ref_obj_id>
				<ref_obj_pid>832288</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Inami, M., Kawakami, N., Sekiguchi, D., Yanagida, Y., Maeda, T., and Tachi, S. 2000. Visuo-haptic display using head-mounted projector. In <i>Proceedings of IEEE Virtual Reality 2000</i>, 233--240.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 RePro3D: Full-parallax 3D Display using Retro-re.ective Projection Technology Takumi Yoshida, Sho Kamuro,Kouta 
Minamizawa, Hideaki Nii,and SusumuTachi The UniversityofTokyo* KeioUniversity  Figure 1: The 3D object 
that is projected onto the retro-re.ective screen can be seen from a number of viewpoints with motion 
parallax, and the user can operate this object using his/her hands. 1 Introduction Motion parallax is 
important to recognize the depth of a 3D im­age. Inrecentyears,many3Ddisplaymethodsthatenableparallax 
images to be seen with the naked eye have been developed. In ad­dition, there has been an increase in 
research to design interfaces that enable humans to intuitively interact with and operate 3D ob­jects 
using their hands. However, realizing 3D object interaction as if the user is actually touching the object 
in the real world is quitedif.cult. Oneofthereasonsforthisisthatthescreenshapein conventional methods 
is restricted to a .at panel. In addition, it is dif.cult toachieveabalance between displaying the 3D 
imageand sensing the user input. Therefore, we propose a novel full-parallax 3D display system that is 
suitable for interactive 3D applications. We call this system RePro3D. Our approach is based on a retro­re.ective 
projection technology[Inami et al. 2000]. A number of imagesfromaprojectorarrayareprojectedontotheretro-re.ective 
screen. When a user looks at the screen through a half mirror, he or she, without the use of glasses, 
can view a 3D image that has motion parallax. We can choose the screen shape depending on the application. 
Image correction according to the screen shape is not required. Consequently, we can design a touch-sensitive 
soft screen, a complexly curved screen, or a screen with an automati­cally moving surface. RePro3D has 
a sensor function to recognize the user input. Some interactive features, such as operation of 3D objects, 
canbe achieved byusing it. 2 Method When images from a number of projectors are projected onto a retro-re.ector, 
there is a strong re.ection of light in the direction of each projection lens. Using a half mirror, a 
same number of viewpoints are made at symmetrical points to the half mirror. By arrangingmanyprojectorsinamatrix,a3Dimagecanbeseenfrom 
multiple viewpoints. In order to show smooth motion parallax, the densityoftheprojectorarraymustbesuf.cientlyhigh. 
Preliminary experimental results show that the distance between two adjacent projectors should be lesser 
than 1.5 cm. Commercially available projector arrays do not have adjacent projectors at such small dis­tancesbecauseoftheresultingincreaseinsizeandcost. 
Therefore, we have developed a high-density projector array by arranging a number of projection lenses 
in a matrix on a high-luminance LCD. Figure2showstheopticalsystemofRePro3D.Thesystemconsists ofanumberoflenses,anLCD,ahalfmirror,andaretro-re.ectoras 
*e-mail: {takumi yoshida, sho kamuro}@ipc.i.u-tokyo.ac.jp e-mail: {kouta, nii, tachi}@tachilab.org Figure 
2: Optical system con.guration of RePro3D. a screen. Lenses are located at an appropriate shift from 
the image area on the LCD so that the projection areas due to each projection lens may overlap. Shield 
plates are placed between the lenses to prevent light for other viewpoints from entering the lens. RePro3D 
can produce vertical and horizontal motion parallax; however, it cannotproducemotionparallaxinthefront-backdirection. 
Weuse an infrared camera as a sensor to recognize the user input. Infrared rays are emitted onto the 
screen through the half mirror near the camera, and the camera captures the re.ected rays. Because the 
retro-re.ector causes an intense re.ection of the rays, a clear dis­tinctionispossiblebetweenthescreenandotherobjectssuchasthe 
user s hands. 3 Prototype Implementation Results We have developed a prototype of our proposed system. 
The lumi­nanceofthehigh-luminanceLCDis1000 cd/m 2 . Thisareahas40 viewpoints, each with a size of 120 
× 70 mm. The image resolu­tionofeachviewpoint was 60 × 60pixels. Weused an IEEE-1394 camera with a resolution 
of 640 × 480 pixels for sensing the user input. Figure1showstheresultsofdisplaying3Dobjectsusingthe prototypesystem. 
Distortionlessparallaximagesweredisplayedon acurvedsurfacefrom40viewpoints. Userswereabletointuitively 
operate the 3D object by moving their hands. We plan to develop devices that have higher resolution and 
to create interactive appli­cations for demonstration of these devices. References INAMI, M., KAWAKAMI, 
N., SEKIGUCHI, D., YANAGIDA,Y., MAEDA,T., AND TACHI, S. 2000. Visuo-haptic display using head-mountedprojector. 
In Proceedings of IEEE Virtual Reality 2000,233 240. Copyright is held by the author / owner(s). SIGGRAPH 
2010, Los Angeles, California, July 25 29, 2010. ISBN 978-1-4503-0210-4/10/0007  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2010</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1836842</article_id>
		<sort_key>210</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>21</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>21</seq_no>
		<title><![CDATA[Shaboned display]]></title>
		<subtitle><![CDATA[an interactive substantial display using soap bubbles]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836821.1836842</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1836842</url>
		<abstract>
			<par><![CDATA[<p>From childhood, we often play with bubbles. We find various aesthetic elements in a series of actions of soap bubbles: appearing, expanding, floating, bursting and disappearing. This time, we utilize the movements of soap bubbles as a pixel of an image and propose a novel interactive substantial display named "Shaboned Display." (see Figure 1)</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>H.5.2</cat_node>
				<descriptor>Interaction styles (e.g., commands, menus, forms, direct manipulation)</descriptor>
				<type>S</type>
			</primary_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003120.10003121.10003124</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2264201</person_id>
				<author_profile_id><![CDATA[81331494719]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Shiho]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hirayama]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264202</person_id>
				<author_profile_id><![CDATA[81100527572]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Yasuaki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kakehi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1179137</ref_obj_id>
				<ref_obj_pid>1179133</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Nakamura, M., Inaba, G., Tamaoki, J., Shiratori, K., and Hoshino, J. 2006. Bubble cosmos. In <i>SIGGRAPH Emerging Technologies</i>, ACM, No. 3.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1400971</ref_obj_id>
				<ref_obj_pid>1400885</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Suzuki, R., Suzuki, T., Ariga, S., Iida, M., and Arakawa, C. 2008. "ephemeral melody": music played with wind and bubbles. In <i>SIGGRAPH Posters</i>, ACM, No. 80.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Shaboned Display: An Interactive Substantial Display Using Soap Bubbles Shiho Hirayama* Yasuaki Kakehi 
Keio University Keio University Figure 1: Shaboned Display 1 Introduction From childhood, we often play 
withbubbles.We .ndvarious aes­thetic elementsinaseriesof actionsof soapbubbles: appearing,ex­panding, 
.oating,bursting and disappearing. This time, we utilize themovementsofsoapbubblesasapixelofanimageand 
propose a novel interactive substantial display named Shaboned Display. (see Figure 1) Mainly in the 
.eld of media art, many artists and designers have also used bubbles as a tool for expression. However, 
in most of the previous systems,bubbles .oat freely and randomlyin air.For example, in Bubble Cosmos 
[Nakamura et al. 2006] , participants can interact with projection images by breaking .oating bubbles 
with white smoke. ephemeral melody [Suzuki et al. 2008] also usebubbles as an interface. In this piece, 
musics are created ac­cordingtothe collisionsofbubbles .oating randomly.Onthe other hand, our Shaboned 
Display can show images by controlling the sizeofeachsoapbubble arrangedina matrixinaplane.Asforthe substantial 
displays, various types of material have also applied to construct visual images. One of the features 
of such display sys­tems is that each pixel has a physical form and we can touch them freely. In our 
system, we can use each soapbubble not only asa pixel of a display but also an input tool. By exploding 
the soap bubbles, the displayed images change interactively.  2 Design of Shaboned Display In our Shaboned 
Display,We offer three innovative points as fol­lows. Firstly, this system can show images with soap 
bubbles arranged in a matrix in a plane. Figure 2 shows the system design of the Shaboned Display.Inthe 
current implementation,10x10air tubes are arranged in a plane. Underneath tubes, air pumps are attached. 
By controlling the volume and timing of air .ow, it can manipulate the size and shape of each soapbubble 
freely. Note that this sys­temdoesnotallowthesebubblesto.oatinairanditalso controls contracting of bubbles. 
By keeping bubbles expanding and con­tracting, this display can present various images such as characters 
or .gures. *e-mail: hirayama@sfc.keio.ac.jp e-mail: ykakehi@sfc.keio.ac.jp Figure 2: System Design Secondly,wedevelopeda 
systemfor creatingasoapbubble auto­matically. By using this function, even if some soap bubbles are broken 
by winds or users .ngers, this system can make the .lm rapidly again andkeep showing images. In addition, 
this system can also break the soapbubbles intentionally. Conversely, this dis­play can show imagesby 
popping someof soapbubbles. Thirdly, this display can work as an interactive system. More con­cretely, 
this system can detect a explosion of .lm by using an elec­trical approach.By attaching electrodesonthesurfaceofsoapbub­bleandtheedgeoftheairvent,thissystemcan 
detecttheexplosion event by sensing the ohmic value of the circuit. Furthermore, this system can also 
detect users actions for example hand gestures by using a camera. 3 Applications and Future Works We 
have already developed the prototype system with 10 x 10 air vents and implemented some applications 
using this display. Firstly, this display works as an ambient information board. Of course, the image 
displayed on it is affected from environmental factors such as winds. Audiences can observe the digital 
informa­tion and analog phenomena simultaneously. Secondly, we devel­oped an interactive application 
on this system. In this application, theeventsofbubblebursting are enhanced.Forexample, when one soapbubbleisexplodedby 
winds or users .ngers,bubbles around it are alsobursted sequentially like ripples in conjunction with 
the event. In other application, this system generates audio feedback whenexplosions happen. Thus, audiences 
can enjoythe intentional of unintentional phenomena through these various effects. In the future, we 
are going to implement this system in various scales and settings such as on a vertical surface. In addition, 
we alsoplanto propose more interactionsusingthesoapbubblepixels. References NAKAMURA, M., INABA, G., 
TAMAOKI, J., SHIRATORI, K., AND HOSHINO, J. 2006. Bubble cosmos. In SIGGRAPH Emerging Technologies,ACM, 
No. 3. SUZUKI,R.,SUZUKI, T.,ARIGA, S.,IIDA,M., AND ARAKAWA, C. 2008. ephemeral melody : music played 
with wind and bubbles. InSIGGRAPH Posters,ACM, No. 80. Copyright is held by the author / owner(s). SIGGRAPH 
2010, Los Angeles, California, July 25 29, 2010. ISBN 978-1-4503-0210-4/10/0007  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2010</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1836843</article_id>
		<sort_key>220</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>22</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>22</seq_no>
		<title><![CDATA[Slow display]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836821.1836843</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1836843</url>
		<abstract>
			<par><![CDATA[<p>How can we show our 16 megapixel photos from our latest trip on a digital display? How can we create screens that are visible in direct sunlight as well as complete darkness? How can we create large displays that consume less than 2W of power? How can we create design tools for digital decal application and intuitive-computer aided modeling?</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.3</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2264203</person_id>
				<author_profile_id><![CDATA[81309492813]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Daniel]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Saakes]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Camera Culture MIT Media Lab and Delft University of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264204</person_id>
				<author_profile_id><![CDATA[81466644269]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Kevin]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Chiu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Camera Culture MIT Media Lab]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264205</person_id>
				<author_profile_id><![CDATA[81466642103]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Tyler]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hutchison]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Camera Culture MIT Media Lab]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264206</person_id>
				<author_profile_id><![CDATA[81466647698]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Biyeun]]></first_name>
				<middle_name><![CDATA[M.]]></middle_name>
				<last_name><![CDATA[Buczyk]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Camera Culture MIT Media Lab]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264207</person_id>
				<author_profile_id><![CDATA[81316489433]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Naoya]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Koizumi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University / JST ERATO]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264208</person_id>
				<author_profile_id><![CDATA[81100424140]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Masahiko]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Inami]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University / JST ERATO]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264209</person_id>
				<author_profile_id><![CDATA[81100022847]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>7</seq_no>
				<first_name><![CDATA[Ramesh]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Raskar]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Camera Culture MIT Media Lab]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Slow Display Daniel Saakes1,2 Kevin Chiu1 Tyler Hutchison1 Biyeun M. Buczyk1 Naoya Koizumi3 Masahiko 
Inami3 Ramesh Raskar1 1Camera Culture MIT Media Lab 2Delft University of Technology 3Keio University 
/ JST ERATO Figure 1: With a programmable trade-off between resolution and refresh rate, the slow display 
can achieve an array of effects. Monostable materials reduce the requirement for power and allow practical 
emissive/re.ective viewable, high resolution displays. Our display is visible both in direct sunlight 
and darkness (left), is deployable on 3D surfaces (middle), and provides up to 16 megapixels of resolution 
(right). 1 Introduction How can we show our 16 megapixel photos from our latest trip on a digital display? 
How can we create screens that are visible in direct sunlight as well as complete darkness? How can we 
create large displays that consume less than 2W of power? How can we cre­ate design tools for digital 
decal application and intuitive-computer aided modeling? We introduce a display that is high resolution 
but updates at a low frame rate, a slow display . We use lasers and monostable light-reactive materials 
to provide programmable space-time reso­lution. This refreshable, high resolution display exploits the 
time decay of monostable materials, making it attractive in terms of cost and power requirements. Our 
effort to repurpose these materials involves solving underlying problems in color reproduction, day­night 
visibility, and optimal time sequences for updating content. 2 Design The slow display consists of a 
low power near UV laser (11mW, 405nm) and a laser projector that scans over a surface painted with commercially 
available monostable light-reactive materials. The resolution of the slow display is limited by laser 
scanner movements and laser spot properties, but is not dependent on the particle size of the light-sensitive 
material. We use photochromic materials with decay times of minutes for re­.ective applications and phosphorescent 
materials with decay times of hours for emissive applications. By applying both materials to the same 
display, we create a hybrid emissive/re.ective display, as shown in .gure 1 (far left). When mixed with 
traditional ink, we create hybrid static/dynamic displays. To avoid activation by am­bient light (sunlight), 
we place a UV .lter over the screen and rear project content. When applied to modeling materials, shown 
in .gure 1 (left), the monostable particles provide persistent projected decals that de­form with shape 
changes without additional updates and occlusion issues encountered in projected augmented reality. 
3 Applications Applications for the slow display include large, always-on informa­tion displays with 
slow updating content, such as .ight data maps and outdoor billboards. The slow display can be easily 
retro.tted into current designs for laser TVs by simply replacing the screen. Since the technology is 
largely projector dependent, projection sur­faces can consist of complex 3D forms, allowing any object 
to be­come a low energy, ubiquitous peripheral display. Hybrid emis­sive/re.ective displays are also 
ideal for dynamic exit signs, which would remain visible without applying additional energy due to the 
long decay time of the display s monostable materials. Applied to paper and clay, the monostable materials 
provide persis­tent projected decals for children s toys and interactive design appli­cations that merge 
traditional physical modeling and digital design. Shape changes to physical models can be captured by 
a deformed projected pattern, which updates a CAD model and maintains local and frame to frame correspondence. 
Likewise, the slow display is equally applicable to everyday objects and environments by allowing users 
to skin their surroundings daily or hourly. 4 Future Work The decay time of today s commercially available 
monostable ma­terials is a limiting factor in our slow display prototype. We hope to employ newly-developed 
nanomaterials and proteins with longer and more controllable decay times in the future. Additionally, 
our current displays are monochromatic, though we have already begun construction of a persistent color 
display using a novel mixture of phosphorescent and .uorescent materials in an aperture grille. The opportunities 
for creating novel displays using remote activa­tion of monostable or bistable materials are immense. 
Our goal is to demonstrate the slow display concept, research practical appli­cations and solutions, 
and take initial steps in solving underlying challenges. Copyright is held by the author / owner(s). 
SIGGRAPH 2010, Los Angeles, California, July 25 29, 2010. ISBN 978-1-4503-0210-4/10/0007  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2010</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1836844</article_id>
		<sort_key>230</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>23</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>23</seq_no>
		<title><![CDATA[Touch Light Through the Leaves]]></title>
		<subtitle><![CDATA[a tactile display for light and shadow]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836821.1836844</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1836844</url>
		<abstract>
			<par><![CDATA[<p>You can feel something good and comfortable when you turn your palms up and the light falling onto your palms through the trees. "Touch Light Through the Leaves" begins from an imagination which we can touch the light through the leaves. We propose "Touch Light Through the Leaves". It is a novel tactile camera display which can change visible information into tactile. It is not only a sensor of light and shadow but also a tactile display. You can feel the transition of light and shadow by your palms directly with it. You can touch the light and also can be touched by the light. Sound-Lens [Iwai 2001] is based on a similar idea. You can hear light with it. It can change light to sound.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[interaction]]></kw>
			<kw><![CDATA[light and shadow detection]]></kw>
			<kw><![CDATA[sensor]]></kw>
			<kw><![CDATA[tactile display]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>H.5.2</cat_node>
				<descriptor>Input devices and strategies (e.g., mouse, touchscreen)</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>H.5.2</cat_node>
				<descriptor>Haptic I/O</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10011752</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Haptic devices</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003125</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10011666</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Touch screens</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2264210</person_id>
				<author_profile_id><![CDATA[81100461884]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Kunihiro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nishimura]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264211</person_id>
				<author_profile_id><![CDATA[81100183761]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Yasuhiro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Suzuki]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264212</person_id>
				<author_profile_id><![CDATA[81100257385]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Michitaka]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hirose]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1358748</ref_obj_id>
				<ref_obj_pid>1358628</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Hashimoto, Y., and Kajimoto, H. 2008. A novel interface to present emotional tactile sensation to a palm using air pressure. <i>Proceeding of the 26th annual CHI conference (CHI2008)</i>, 2703--2708.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Iwai, T., 2001. Sound-lenz.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Touch Light Through the Leaves: A Tactile Display for Light and Shadow Kunihiro Nishimura* Yasuhiro 
Suzuki Michitaka Hirose The University of Tokyo The University of Tokyo The University of Tokyo  Figure 
1: Left: Picture of Touch Light Through the Leaves, Center Left: Tactile display with vibration units, 
Upper Ceter Right: Detection result of light and shadow by the camera and determination of vibration 
units, Lower Center Right: Side view of the tactile display, Left: System overview Keywords: tactile 
display, interaction, sensor, light and shadow detection 1 Introduction You can feel something good and 
comfortable when you turn your palms up and the light falling onto your palms through the trees. Touch 
Light Through the Leaves begins from an imagination which we can touch the light through the leaves. 
We propose Touch Light Through the Leaves . It is a novel tactile camera display which can change visible 
information into tactile. It is not only a sensor of light and shadow but also a tactile display. You 
can feel the transition of light and shadow by your palms directly with it. You can touch the light and 
also can be touched by the light. Sound-Lens [Iwai 2001] is based on a similar idea. You can hear light 
with it. It can change light to sound. In terms of a tactile display, there are several kinds of tactile 
display. For making tactile sensation to a palm, the way to use an air pres­sure with a speaker is proposed. 
An idea to make a tactile sensation to a palm is the same as our proposal [Hashimoto and Kajimoto 2008]. 
In terms of a tactile display for conveying information, the Optacon (Optical Tactile Converter) is one 
example. It consists of a small camera and pin display. It can convey information for blind people. We 
developed this idea and generate an entertainment and art device. 2 Our Approach and Implementation 
This touch display can change from visible information to tactile information. It can transmit the light 
and shadow to your palm di­rectly as a tactile. It consists of a camera and vibration units with in the 
same device. The camera can detect light and shadow and vibration units change them into tactile with 
image processing and control of each units with vibration motors. The size of this display is like a 
palm size in order to hold by a hand. The weight of this dis­play is about 800g. This portability is 
required to experience under *e-mail: kuni@cyber.t.u-tokyo.ac.jp e-mail:yasusay@rcast.u-tokyo.ac.jp e-mail:hirose@cyber.t.u-tokyo.ac.jp 
various conditions. The overview of system is three parts: 1) Tactile display: a camera and a tactile 
display, 2) PC: image processing and determination of vibration units, 3) Control unit: controlling vibration 
units. Each part is connected by USB or MIDI. A user bring only a tactile dis­play for experience. For 
a tactile display, we put 85 vibration units at the bottom of the device. For the vibration units, we 
used 85 vibration motors (vibra­tion motor: 6dl05wa by Linkman). One unit consists a vibration motor 
with a plastic polyvinyl chloride cap with 6.0mm diameter. Each unit has 6.5 mm diameter. Each unit is 
distributed with 9.0 mm pitch. With 85 vibration units, it can stimulate a whole palm. On the top of 
display, we put screen material in order to make shadow on it. A camera detects this shadow and light. 
For a cap­tured image, we do noise reduction, binarize, and determine vi­bration points. The area of 
shadow or light determines vibration points. When you want to touch shadow, vibration units of shadow 
area will vibrate. When you want to touch light, vibration system works opposite by a parameter. 3 Results 
and Discussion People who experienced this display had weird and new feeling because they can feel light 
or shadow directly. In their daily life, light and shadow are perfectly ordinary. When they feel light 
and shadow directly on their palms, it becomes a tactile sensor, which reminds them of light and shadow. 
For our future work, we want to develop more lightweight and wireless system for experiencing Touch Light 
Through the Leaves everywhere. References HASHIMOTO, Y., AND KAJIMOTO, H. 2008. A novel interface to 
present emotional tactile sensation to a palm using air pres­sure. Proceeding of the 26th annual CHI 
conference (CHI2008), 2703 2708. IWAI, T., 2001. Sound-lenz. Copyright is held by the author / owner(s). 
SIGGRAPH 2010, Los Angeles, California, July 25 29, 2010. ISBN 978-1-4503-0210-4/10/0007  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2010</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
</content>
</proceeding>
