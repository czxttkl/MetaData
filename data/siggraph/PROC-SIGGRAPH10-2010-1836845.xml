<?xml version="1.0" encoding="ISO-8859-1"?>
<!DOCTYPE proceeding SYSTEM "proceeding.dtd">
<proceeding ver="6.0" ts="04/10/2010">
<conference_rec>
	<conference_date>
		<start_date>07/26/2010</start_date>
		<end_date>07/30/2010</end_date>
	</conference_date>
	<conference_loc>
		<city><![CDATA[Los Angeles]]></city>
		<state>California</state>
		<country></country>
	</conference_loc>
	<conference_url></conference_url>
</conference_rec>
<series_rec>
	<series_name>
		<series_id>SERIES382</series_id>
		<series_title><![CDATA[International Conference on Computer Graphics and Interactive Techniques]]></series_title>
		<series_vol></series_vol>
	</series_name>
</series_rec>
<proceeding_rec>
	<proc_id>1836845</proc_id>
	<acronym>SIGGRAPH '10</acronym>
	<proc_desc>ACM SIGGRAPH 2010 Posters</proc_desc>
	<conference_number>2010</conference_number>
	<proc_class>conference</proc_class>
	<proc_title></proc_title>
	<proc_subtitle></proc_subtitle>
	<proc_volume_no></proc_volume_no>
	<isbn13>978-1-4503-0393-4</isbn13>
	<issn></issn>
	<eissn></eissn>
	<copyright_year>2010</copyright_year>
	<publication_date>07-26-2010</publication_date>
	<pages>156</pages>
	<plus_pages></plus_pages>
	<price><![CDATA[]]></price>
	<other_source></other_source>
	<abstract>
		<par><![CDATA[<p>Posters offer a light-weight, low-tech method for presenting student, in-progress, and late-breaking work. They are displayed throughout the conference for attendees to browse at their leisure, and poster authors meet and discuss their work with attendees during Poster Sessions.</p> <p>Poster topics range from applications of computer graphics to novel interactive techniques to in-depth research on specific topics. Posters also present work submitted to the ACM Student Research Competition.</p>]]></par>
	</abstract>
	<publisher>
		<publisher_id>PUB27</publisher_id>
		<publisher_code>ACMNY</publisher_code>
		<publisher_name>ACM</publisher_name>
		<publisher_address>2 Penn Plaza, Suite 701</publisher_address>
		<publisher_city>New York</publisher_city>
		<publisher_state>NY</publisher_state>
		<publisher_country>USA</publisher_country>
		<publisher_zip_code>10121-0701</publisher_zip_code>
		<publisher_contact>Bernard Rous</publisher_contact>
		<publisher_phone>212 869-7440</publisher_phone>
		<publisher_isbn_prefix></publisher_isbn_prefix>
		<publisher_url>www.acm.org/publications</publisher_url>
	</publisher>
	<sponsor_rec>
		<sponsor>
			<sponsor_id>SP932</sponsor_id>
			<sponsor_name>ACM Special Interest Group on Computer Graphics and Interactive Techniques</sponsor_name>
			<sponsor_abbr>SIGGRAPH</sponsor_abbr>
		</sponsor>
	</sponsor_rec>
	<categories>
		<primary_category>
			<cat_node/>
			<descriptor/>
			<type/>
		</primary_category>
	</categories>
	<chair_editor>
		<ch_ed>
			<person_id>P2264213</person_id>
			<author_profile_id><![CDATA[81100553787]]></author_profile_id>
			<orcid_id></orcid_id>
			<seq_no>1</seq_no>
			<first_name><![CDATA[Cindy]]></first_name>
			<middle_name><![CDATA[]]></middle_name>
			<last_name><![CDATA[Grimm]]></last_name>
			<suffix><![CDATA[]]></suffix>
			<affiliation><![CDATA[Washington University in St. Louis]]></affiliation>
			<role><![CDATA[Conference Chair]]></role>
			<email_address><![CDATA[]]></email_address>
		</ch_ed>
	</chair_editor>
</proceeding_rec>
<content>
	<section>
		<section_id>1836846</section_id>
		<sort_key>10</sort_key>
		<section_seq_no>1</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Animation]]></section_title>
		<section_page_from>1</section_page_from>
	<article_rec>
		<article_id>1836847</article_id>
		<sort_key>20</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>1</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[A 3-D flowering simulation based on botany characteristics and random generation algorithm]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836845.1836847</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1836847</url>
		<abstract>
			<par><![CDATA[<p>In recent years, virtual reality is experiencing a rapid development, which is also applied in plant morphology simulation. With a variety dynamic process of blooming, you are not only able to create a virtual landscape but to decorate a virtual space. In the internet, people can enjoy flowering anytime and anywhere to promote flower exhibitions and business. This paper focuses on the botanical characteristics of blooming process and creates a vivid effect based on Bezier curves and surfaces theory.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.6.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010341</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2264214</person_id>
				<author_profile_id><![CDATA[81339538448]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Junfeng]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yao]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[SoftWare School of Xiamen University, China]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264215</person_id>
				<author_profile_id><![CDATA[81474681067]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Xiaobiao]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Xie]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[SoftWare School of Xiamen University, China]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264216</person_id>
				<author_profile_id><![CDATA[81466645515]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Ming]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Zhang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[SoftWare School of Xiamen University, China]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264217</person_id>
				<author_profile_id><![CDATA[81456634285]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Hui]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Zhang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[SoftWare School of Xiamen University, China]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264218</person_id>
				<author_profile_id><![CDATA[81451592765]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Andy]]></first_name>
				<middle_name><![CDATA[Ju An]]></middle_name>
				<last_name><![CDATA[Wang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Southern Polytechnic State University in Atlanta]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1073253</ref_obj_id>
				<ref_obj_pid>1073204</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Ijiri T, Owada S. Floral diagrams and inflorescences: Interactive flower modeling using botanical structural constraints. <i>ACM Trans. on Graph</i>, 2005, 24(3):720--726.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 A 3 ­ D F l o w e r i n g S i m u l a t i o n B a s e d o n B o t a n y C h a r a c t e r i s t i c 
s a n d R a n d o m G e n e r a t i o n A l g o r i t h m 11112 Junfeng Yao, Xiaobiao Xie , Ming Zhang, 
Hui Zhang, Andy Ju An Wang1 SoftWare School of Xiamen University, China 2 Southern Polytechnic State 
University in Atlanta F i g . 1 . D i f f e r e n t S t a g e o f B l o o m i n g P r o c e s s 1 I 
n t r o d u c t i o n In recent years, virtual reality is experiencing a rapid development, which is 
also applied in plant morphology simulation. With a variety dynamic process of blooming, you are not 
only able to create a virtual landscape but to decorate a virtual space. In the internet, people can 
enjoy flowering anytime and anywhere to promote flower exhibitions and business. This paper focuses on 
the botanical characteristics of blooming process and creates a vivid effect based on Bezier curves and 
surfaces theory. In this paper the botany characteristics of various types of flowers are analyzed, 
daffodils are taken as a representative, then a mathematical model is built, and a dynamic blossom process 
is achieved with OpenGL programming technology, and finally the simulation effect of other types of flowers 
are exhibited., some prospects for future work are discussed too. 2 O u r A p p r o a c h A general 
complete flower includes six components, namely, pedicel, receptacle, calyx, corolla, stamen group androecia 
and gynoecia. The pedicel and receptacle is equivalent to the branch part[Ijiri and Owada 2005], while 
other four parts are equivalent to metamorphosis leaves. flowers with four full parts are called complete 
ones, otherwise they are incomplete. This paper focuses on how to simulate the flowering process, it 
is necessary to study the physical characteristics of an opening flower, i.e., what changes occur during 
the blossom process. Changes happened in blooming are: tilting angle of the axis and flower petals become 
larger gradually as flowers open up, which is the most important change. The most obvious and important 
one is the inclination rate reflected in the axial and flower petals as the flowers open become larger, 
usually from 5 degrees to 85 degrees, this change is not a linear process, it cannot be directly described 
using linear equations, botany information suggested that the inclination of petals at first at a more 
rapid pace, after reaching a certain extent, then the increase pace slowed down. We can use Fig 2 as 
follows: In Fig2, G says the size of the angle, T represents time. It email: yao0010@xmu.edu.cn, jwang@spsu.edu 
can be seen that G increases rapidly at first, then slowly. Firstly the key points of Bezier surface 
is modified, using a three­dimensional array to store coordinates of key points, then a random texture 
is generated with recycling calculation and texture information is stored in an array. F i g 2 C u r 
v e o f r e l a t i o n b e t w e e n t i m e a n d i n c l i n a t i o n 3 R e s u l t As mentioned 
above, the most important feature of flower blossom is the changing inclination, which is the key element 
to mathematical modeling. The variable angle in the class Petal indicates the final inclination during 
blossom process and should also be a random value, so angle=75 + rand()%10, which means angle values 
should be from 75 to 85. Variable r represents the inclination of the current moment; variable dr represents 
the inclination increment to the next time moment. When r is less than half value of the angle, dr increases 
linearly; when the value of r is greater than or equal to the half value of angle, dr decreases linearly. 
Different Stage of Blooming Process is shown in Fig.1. R e f e r e n c e s Ijiri T, Owada S. Floral 
diagrams and inflorescences: Interactive flower modeling using botanical structural constraints. ACM 
Trans. on Graph, 2005,24(3):720-726. Copyright is held by the author / owner(s). SIGGRAPH 2010, Los Angeles, 
California, July 25 29, 2010. ISBN 978-1-4503-0210-4/10/0007  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1836848</article_id>
		<sort_key>30</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>2</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[A development environment for designing interactive characters with sensorimotor models]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836845.1836848</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1836848</url>
		<abstract>
			<par><![CDATA[<p>Recent progress of interactive techniques brought intuitive and physical interaction with characters in entertainment field such as console games. Conventional motion generation method requires preparing an enormous number of motion patterns in order to implement various reactions of characters. Therefore, for an easy way of implementation of various reactions, virtual creatures with sensorimotor models will become useful. Virtual creatures [Mitake et al. 2007] are characters with sensorimotor models generate motion in physics simulation environment.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[UI]]></kw>
			<kw><![CDATA[motion design]]></kw>
			<kw><![CDATA[virtual creature]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.4</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2264219</person_id>
				<author_profile_id><![CDATA[81466645937]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Shunsuke]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Matsuyama]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Electro-Communications]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264220</person_id>
				<author_profile_id><![CDATA[81316489667]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Hironori]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Mitake]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tokyo Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264221</person_id>
				<author_profile_id><![CDATA[81100270860]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Shoichi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hasegawa]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Electro-Communications]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1599401</ref_obj_id>
				<ref_obj_pid>1599301</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[
Aoki, T., Mitake, H., Hasegawa, S., and Sato, M. 2009. Haptic ring: touching virtual creatures in mixed reality environments. In <i>SIGGRAPH '09: SIGGRAPH '09: Posters</i>, ACM, New York, NY, USA, 1--1.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[
Blender.org. http://www.blender.org/.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[
Mitake, H., Hasegawa, S., Koike, Y., and Sato, M. 2007. Reactive virtual human with bottom-up and top-down visual attention for gaze generation in realtime interactions. <i>Virtual Reality Conference, IEEE 0</i>, 211--214.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 A Development Environment for Designing Interactive Characters with Sensorimotor Models Shunsuke Matsuyama 
* Hironori Mitake The University of Electro-Communications P &#38; I lab. Tokyo Institute of Technology 
Shoichi Hasegawa The University of Electro-Communications Keywords: virtual creature,motion design, 
UI 1 Introduction Recent progress of interactive techniques brought intuitive and physical interaction 
with characters in entertainment .eld such as console games. Conventional motion generation method requires 
preparing an enormous number of motion patterns in order to im­plement various reactions of characters. 
Therefore, for an easy way of implementation of various reactions, virtual creatures with sen­sorimotor 
models will become useful. Virtual creatures [Mitake et al. 2007] are characters with sensorimotor models 
generate mo­tion in physics simulation environment. It consists of 4 main com­ponents: 1. Articulated 
rigid body model which decides characteristics of dynamics. 2. Models of external sensors such as visual, 
tactile, auditory sensors. 3. AI scripts which decide behaviors from output of sensor mod­els. 4. Motor 
controllers which generate motions to implement the behavior.  However, there are a lot of parameters 
such as spring-damper co­ef.cients and range of visual .eld. It is dif.cult to predict the re­sulting 
motions from these parameters. Thus, we propose a novel development environment that can smoothly design 
characters with sensorimotor models. 2 Innovation For creating a character with sensorimotor models, 
the proposed en­vironment has two following features in addition to usual modeling and animation tools: 
The .rst one is visualization of sensorimotor models and their pa­rameters. For example, a visual sensory 
model has parameters of the visual .eld. These parameters represent three-dimensional space. Thus, simple 
adjustment based on numerical values is trou­blesome. By visualizing with 3D computer graphics, we propose 
an environment in which intuitive adjustment becomes possible. The second is motion con.rmation. When 
we adjust parameters, their changes are immediately applied to motion of character inside developing 
environment. This feature provides smooth con.rma­tion of changing of character motions caused by parameter 
adjust­ment. Simulators of physics and sensorimotor models are necessary to con.rm the reactive motions 
of virtual creatures. Therefore, the *e-mail:matsuyama@hi.mce.uec.ac.jp e-mail:mitake@hi.pi.titech.ac.jp 
e-mail:hase@hi.mce.uec.ac.jp Figure 1: system overview proposing environment provides integration between 
a character design system and simulators. In this research, we have built a de­velopment environment 
that can provide both character body shape design and parameter adjustment, based on Blender[Blender.org 
] s combination of design environment and game engine. We inte­grated an in house physics simulator and 
sensorimotor models of virtual creatures into Blender. Accordingly, many users who get used with Blender 
can easily create virtual creatures. 3 Vision Recently, in areas such as video game, augmented reality 
and 3D input devices are increasing rapidly[Aoki et al. 2009]. Hence, in applications using these devices, 
character needs a huge number of reactions. As the proposing environment supports designers to easily 
create and tuning characters, we expect that it will be utilized widely in the near future. References 
AOKI, T., MITAKE, H., HASEGAWA, S., AND SATO, M. 2009. Haptic ring: touching virtual creatures in mixed 
reality environ­ments. In SIGGRAPH 09: SIGGRAPH 09: Posters, ACM, New York, NY, USA, 1 1. BLENDER.ORG. 
http://www.blender.org/. MITAKE, H., HASEGAWA, S., KOIKE, Y., AND SATO, M. 2007. Reactive virtual human 
with bottom-up and top-down visual at­tention for gaze generation in realtime interactions. Virtual Re­ality 
Conference, IEEE 0, 211 214. Copyright is held by the author / owner(s). SIGGRAPH 2010, Los Angeles, 
California, July 25 29, 2010. ISBN 978-1-4503-0210-4/10/0007  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1836849</article_id>
		<sort_key>40</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>3</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[A framework for GPU accelerated needle insertion simulation using meshfree methods]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836845.1836849</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1836849</url>
		<abstract>
			<par><![CDATA[<p>The simulation of needle insertion is an important research area that has many applications in robotic and image guided brachytherapy cancer treatment, biopsies, and neurosurgery. Modeling of soft tissue plays an important role in the needle insertion simulation, but the use of Finite Element Method is complicated due to the need for remeshing in the neighbourhood of the needle tip. We are proposing to use a meshfree method for the tissue deformation modeling, in which new tissue nodes are added on the needle shaft as the needle is inserted into the tissue. In addition, we have utilized Nvidia's CUDA technology to accelerate the methods used in our framework.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[deformable object]]></kw>
			<kw><![CDATA[meshfree methods]]></kw>
			<kw><![CDATA[needle insertion simulation]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.6.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061.10010063</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures->Computational geometry</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2264222</person_id>
				<author_profile_id><![CDATA[81466648540]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Aria]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Shahingohar]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Western Ontario]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264223</person_id>
				<author_profile_id><![CDATA[81436594036]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Roy]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Eagleson]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Western Ontario]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1531394</ref_obj_id>
				<ref_obj_pid>1576246</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Chentanez, N., Alterovitz, R., Ritchie, D., Cho, L., Hauser, K. K., Goldberg, K., Shewchuk, J. R., and O'Brien, J. F. 2009. Interactive simulation of surgical needle insertion and steering. In <i>Proceedings of ACM SIGGRAPH 2009.</i>]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1028542</ref_obj_id>
				<ref_obj_pid>1028523</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[M&#252;ller, M., Keiser, R., Nealen, A., Pauly, M., Gross, M., and Alexa, M. 2004. Point based animation of elastic, plastic and melting objects. In <i>SCA '04: Proceedings of the 2004 ACM SIGGRAPH/Eurographics symposium on Computer animation</i>, Eurographics Association, Aire-la-Ville, Switzerland, Switzerland, 141--151.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1780278</ref_obj_id>
				<ref_obj_pid>1780201</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Zhu, Y., Magee, D. R., Ratnalingam, R., and Kessel, D. 2007. A training system for ultrasound-guided needle insertion procedures. In <i>MICCAI (1)</i>, 566--574.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 A Framework for GPU Accelerated Needle Insertion Simulation using Meshfree Methods  Aria Shahingohar* 
UniversityofWestern Ontario Abstract The simulation of needle insertion is an important research area 
that has manyapplications in robotic and image guided brachytherapy cancer treatment, biopsies, and neurosurgery. 
Modeling of soft tis­sue plays an important role in the needle insertion simulation,but the use of Finite 
Element Method is complicated due to the need for remeshing in the neighbourhood of the needle tip. We 
are propos­ing to use a meshfree method for the tissue deformation modeling, in which new tissue nodes 
are added on the needle shaft as the nee­dle is inserted into the tissue. In addition, we have utilized 
Nvidia s CUDA technology to accelerate the methods used in our frame­work. Keywords: Deformable Object, 
Needle Insertion Simulation, Meshfree Methods  Our Framework Zhu et. al.[Zhu et al. 2007] uses a localized 
mass-spring model to simulate needle tissue interaction. They assume that the needle only causes deformation 
in the region local to the insertion path. Therefore, once the needle is inside, a cylindrical deforming 
.eld centered at the predicted insertion path (i.e., the current direction of the needle) is created 
in the form of a tetrahedral mesh. In [Chentanez et al. 2009] the Finite Element Method is used to model 
tissue deformations during needle insertion and the condensation method is used to achieve higher frame 
rates. However, there is a fundamental challenge when simulating needle insertion with FEM and that is 
the fact that boundary conditions can only be applied on the nodes. Since the needle tip can pass through 
elements, the tissue should be constantly remeshed around the tip to ensure it is always aligned with 
a node. Remeshing is complicated and costly, especially if the quality of the new mesh should be preserved.[Chentanez 
et al. 2009] In our proposed framework, meshfree methods are used for the simulation. Our main contribution 
is that instead of remeshing or snapping the nodes, we add nodes to the system. Adding a node is done 
in such a way that it does not impose anyarti.cial strain in the tissue. In our framework, for each node 
a set of arrays are generated to store several variables associated with the simulation. These variables 
are stored for all nodes and include the position, velocity, elastic force, list of neighbours and the 
weights of each neighbour. The original positionsof the nodes are alsokept sinceitis required in most 
methods. If we decide to add a node to our model (for example if the needle is penetrated more than a 
threshold) we have to make a list of neighbours for it and calculate the weight for each of its neighbour 
nodes. Moreover, the new node should be added to the list of neighbours of all nodes in its vicinity 
and the corresponding weight should also be updated. On the other hand, in each meshfree method some 
values should be calculated onthe original undeformed shape. Forexamplein[M¨ uller et al. . T 2004] A-1 
=( j xij xij wij )-1 is pre-computed for all the nodes at the beginning of the simulation. This value 
should be *e-mail: ashahing@uwo.ca e-mail:eagleson@uwo.ca Figure 1: A screenshot of needle insertion 
simulation in our frame­work. calculated for the newly added node and it should be updated for all other 
affected nodes (neighbours of the new node). In order to recalculatethesevalues,the positionofthenewlyaddednodeinthe 
reference mesh is needed. The position of the newly added node in the reference mesh(orthe material space)is 
obtainedbyan approx­imation based on the position of the newnode in the deformed state. In order to be 
able to remove the added nodes due to pulling the needle out, two stacks are de.ned to hold the node 
neighbours and nodeweights arrays.Everytimeanodeisadded, current arraysare pushed into the stack before 
updating, and the stack is popped if the last added node is deleted. We have successfully modeled a non-de.ecting 
needle with this technique.We usea stick slip method for the friction between nee­dle shaft and nodes. 
The newly added nodes are constrained to move with the needle in the stick state. A node goes to the 
slip mode if the elastic force on it exceeds a threshold. In the slip mode boundary condition is enforced 
by projecting each node to the nee­dle shaft trajectory. Since most meshfree methods can be performedfaster 
if they are executed in parallel, our framework utilizes Nvidia s CUDAtech­nologyto acceleratethe calculationsbytransferring 
their core com­putations to the GPU. Our results show that frame rates can be im­proved to more than 
20 times using GPU. References CHENTANEZ, N., ALTEROVITZ, R., RITCHIE, D., CHO, L., HAUSER, K. K., GOLDBERG, 
K., SHEWCHUK, J. R., AND O BRIEN, J. F. 2009. Interactive simulation of surgical nee­dle insertion and 
steering. In Proceedings of ACM SIGGRAPH 2009. ¨ M., AND ALEXA, M. 2004. Point based animation of elastic, 
plastic and melting objects. In SCA 04: Proceedings of the 2004 ACM SIGGRAPH/Eurographics symposium on 
Computer an­imation, Eurographics Association, Aire-la-Ville, Switzerland, Switzerland, 141 151.  MULLER, 
M., KEISER, R., NEALEN, A., PAULY, M., GROSS, ZHU, Y., MAGEE, D. R., RATNALINGAM, R., AND KESSEL,D. 2007. 
Atraining system for ultrasound-guided needle insertion procedures. In MICCAI (1), 566 574. Copyright 
is held by the author / owner(s). SIGGRAPH 2010, Los Angeles, California, July 25 29, 2010. ISBN 978-1-4503-0210-4/10/0007 
  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1836850</article_id>
		<sort_key>50</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>4</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[A skinning technique considering the shape of human skeletons]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836845.1836850</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1836850</url>
		<abstract>
			<par><![CDATA[<p>We propose a skinning technique to improve expressive power of Skeleton Subspace Deformation (SSD) by adding the influence of the shape of skeletons to the deformation result by post-processing.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2264224</person_id>
				<author_profile_id><![CDATA[81466645654]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Hirofumi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Suda]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Waseda University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264225</person_id>
				<author_profile_id><![CDATA[81442600865]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Kentaro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yamanaka]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Waseda University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264226</person_id>
				<author_profile_id><![CDATA[81100066959]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Shigeo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Morishima]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Waseda University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>111168</ref_obj_id>
				<ref_obj_pid>111154</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Magnenat-Thalmann, N., and Thalmann, D. "Humanbody deformations using joint-dependent local operatorsand finite element theory", <i>In making them move: mechanics, control, and animation of articulated figures.</i> Morgan Kaufmann, 1991. pp.243--262.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1599481</ref_obj_id>
				<ref_obj_pid>1599470</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Rohmer D., Hahmann S., Cani M.-P. "Exact volume preserving skinning with shape control", ACM SIGGRAPH Symposium on Computer Animation. 2009.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 A Skinning Technique Considering the Shape of Human Skeletons Hirofumi SUDAi Kentaro YAMANAKA Shigeo 
MORISHIMA Waseda University 1. Abstract We propose a skinning technique to improve expressive power of 
Skeleton Subspace Deformation (SSD) by adding the influence of the shape of skeletons to the deformation 
result by post­processing. 2. Introduction It is general that the deformation of 3DCG character model 
is expressed by motion of the skeleton. The representative example of the transformation method is SSD[1]. 
SSD is a technique to deform character by linear blending of transform matrices with weight for each 
skeleton. The expression is as follows (1). S'= ib =1wiTiT0,i -1S0 (1) -1 S0 is a vertex on the original 
surface in rest pose, T0,i is the transform from the surface to the skeletal flame in the rest pose, 
T is the transform from the skeletal flame to the surface containing S0 in the current pose, wi is the 
weight for each skeleton and S ' is the vertex on the surface in current pose. SSD is fast to compute 
and most common technique, on the other hand, the deformation with SSD can t get enough expressive power. 
For example, the defects called Joint-Collapse , Candy-Wrapper happen. To solve this problem, several 
techniques as post-process interpolation have been proposed. For example, Damien et al proposed a shape 
interpolation technique with volume preservation [2]. This technique can interpolate the original surface 
in arbitrary direction. On the other hand, it isn t suite for part that need more detail like human joints. 
So, we focus on the part where joints shape appear remarkably, and propose the technique for improving 
expressive power by considering influence of bone shape. i 3. Modeling the skeleton motion from Mocap 
data Reproducing the internal skeleton movement is important in expressing real shape of human body. 
So, we focused on the forefinger as a part to which shape of joint remarkably appears and obtained coordinate 
data of characteristic points of each bone from motion capture system. From the obtained data, we estimate 
the center of rotation about the phalanx proximalis, phalanx media and phalanx distalis from optimization 
by Simulated Annealing. Finally, we generate the skeletal motion by applying these to the bone models. 
 Skin Si ' NjBj Figure 1: Scenery of capturing Figure 2: Expression of projected motion data. bones. 
4. Representation of skin shape around joint To represent the skin shape around the joint, we bury the 
bone models under the skin and reflect its shape in the original surface. When the bones project over 
the skin, we find the intersection of a straight line which extends from the vertex of the skin model 
to the direction of normal vector and the mesh of the skeleton model like the following expressions (2). 
After that, we transform the mesh according to that (Figure 2). -N .(S -B ) j j j t =i Nj -ni S'= S + 
nt (2) i iii Si is the vertex on the original surface, ni is its normal vector, and Bj is one of the 
vertices that compose a polygon of the bone model. Nj is the normal vector of this polygon. Figure 3 
shows the result that applied our method in the right hand forefinger. (a) is the result of applying 
the SSD only, (b) is the result of applying the our method, (c) is the real photography . In the (a), 
the outside of the joint is transformed smoothly. On the other hand, the result in (b) is more similar 
to real shape. Figure 3: Comparison of SSD (a), our method (b) and real photography (c). 5. Conclusion 
In this work, we proposed a skinning technique to improve representation around the joint which reflects 
the shape of skeletons remarkably by post-process deformation. As future work, we should construct the 
more accurate skeleton model and evaluate accuracy by measuring accurate positions of skin and skeletons 
with MRI. We also need to consider the interpolation of the inside of the joint which have not taken 
up in the present study to improve the expression. References [1] Magnenat-Thalmann, N., and Thalmann, 
D. Humanbody deformations using joint-dependent local operatorsand finite element theory , In making 
them move:mechanics, control, and animation of articulated figures. Morgan Kaufmann, 1991. pp.243-262. 
[2] Rohmer D., Hahmann S., Cani M.-P. Exact volume preserving skinning with shape control , ACM SIGGRAPH 
Symposium on Computer Animation. 2009. i email: hirofumism_0319@ruri.waseda.jp Copyright is held by the 
author / owner(s). SIGGRAPH 2010, Los Angeles, California, July 25 29, 2010. ISBN 978-1-4503-0210-4/10/0007 
  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1836851</article_id>
		<sort_key>60</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>5</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>5</seq_no>
		<title><![CDATA[Carnival]]></title>
		<subtitle><![CDATA[a modular framework for automated facial animation]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836845.1836851</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1836851</url>
		<abstract>
			<par><![CDATA[<p>Facial animation is difficult to do convincingly. The movements of the face are complex and subtle, and we are innately attuned to faces. It is particularly difficult and labor-intensive to accurately synchronize faces with speech. A technology-based solution to this problem is automated facial animation. There are various ways to automate facial animation, each of which drives a face from some input sequence. In <i>performance-driven animation</i>, the input sequence may be either facial motion capture or video of a face. In <i>automatic lip-syncing</i>, the input is audio (and possibly a text transcript), resulting in facial animation synchronized with that audio. In <i>audio-visual text-to-speech synthesis</i> (AVTTS), only text is input, and synchronous auditory and visual speech are synthesized.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Animation</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.4</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2264227</person_id>
				<author_profile_id><![CDATA[81474689903]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Michael]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Berger]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Edinburgh]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264228</person_id>
				<author_profile_id><![CDATA[81335491640]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Gregor]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hofer]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Edinburgh]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264229</person_id>
				<author_profile_id><![CDATA[81541360956]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Hiroshi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Shimodaira]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Edinburgh]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[
Streeting, S., 2010. Ogre: Object-oriented graphics rendering engine. http:://www.ogre3d.org.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Carnival: A Modular Framework for Automated Facial Animation Michael Berger* Gregor Hofer Hiroshi Shimodaira 
University of Edinburgh (a) (b) Figure 1: (a) Schematic of a Visualizer, a real-time, modular animation 
component that is a key class in the Carnival system. The Visualizer consists of a control interface 
and an OGRE scene. The control interface comprises a set of deformation parameters (DPs) (blue squares), 
which may be bound to the current time point in a parameter time series, or to other DPs by linking functions. 
Ultimately, DPs link to mesh deformers (red squares) in the OGRE scene. (b) GUI window for a Visualizer, 
with manual slider controls for the deformation parameters. 1 Introduction Facial animation is dif.cult 
to do convincingly. The movements of the face are complex and subtle, and we are innately attuned to 
faces. It is particularly dif.cult and labor-intensive to accurately synchronize faces with speech. A 
technology-based solution to this problem is automated facial animation. There are various ways to automate 
facial animation, each of which drives a face from some input sequence. In performance-driven animation, 
the input se­quence may be either facial motion capture or video of a face. In automatic lip-syncing, 
the input is audio (and possibly a text tran­script), resulting in facial animation synchronized with 
that audio. In audio-visual text-to-speech synthesis (AVTTS), only text is in­put, and synchronous auditory 
and visual speech are synthesized. The problem with such solutions is that they bring together soft­ware 
and data formats from different .elds in particular speech technology and graphics technology that are 
not well integrated. The typical relationship between these areas is exempli.ed for au­tomatic lip sync 
in Figure 2. First, speech motion parameters are generated from audio input; second, these must be adapted 
for use in animating a facial model using animation software. There is no single framework relating these 
processes. Thus: Figure 2: Process .ow for automatic lip sync. The interface is laborious and slow. 
Parameter tracks must be converted into appropriate animation curves and imported into animation software, 
where they are usually rendered of.ine. *e-mail: m.a.berger@sms.ed.ac.uk e-mail: ghofer@inf.ed.ac.uk 
e-mail: h.shimodaira@ed.ac.uk The lack of live connection between the speech and rendering pipelines 
means it is dif.cult to backtrace animation problems to earlier processing stages, and dif.cult to see 
the effect of corrections to those stages in animation output.  There is no standard control interface 
for different facial mod­els, so the adaptation process must be repeated in each case.  2 Our solution 
We introduce a software framework for automated facial anima­tion, called Carnival, which places speech 
and graphics compo­nents within a single object-oriented system. It provides fast and automatic end-to-end 
processing; real-time animation and linked display of speech representations and rendering for instantaneous 
feedback/feed-forward information; and standardized object inter­faces for easy integration of new components. 
The core of our solution is a platform-independent C++ API. One of the main features of the API is a 
class called Visualizer, which is a modular, real-time rendering engine. A Visualizer provides a uniform 
interface to speech parameter generation, in the form of deformation parameters (DPs), which have the 
range [0, 1] for uni­directional or [-1, 1] for bidirectional deformations, with rest state 0. DPs may 
be hierarchically related by link functions. Abstractly, a Visualizer is essentially an image decoder, 
converting a vector of deformation parameters into an image. Our .rst implementation of the Visualizer 
class is based on OGRE (Object-Oriented Graphics Rendering Engine) [Streeting 2010] (Figure 1), and can 
accommo­date any facial model created in standard animation packages. The Carnival framework also provides 
functionality for memory management; processing pipelines; a stack of undoable opera­tions; and synchronous 
output of audio, video and animation.  References STREETING, S., 2010. Ogre: Object-oriented graphics 
rendering engine. http:://www.ogre3d.org. Copyright is held by the author / owner(s). SIGGRAPH 2010, 
Los Angeles, California, July 25 29, 2010. ISBN 978-1-4503-0210-4/10/0007  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1836852</article_id>
		<sort_key>70</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>6</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>6</seq_no>
		<title><![CDATA[Controlled metamorphosis of animated meshes using polygonal-functional hybrids]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836845.1836852</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1836852</url>
		<abstract>
			<par><![CDATA[<p>Polygonal models are widely used in computer animation. Static polygonal models are commonly animated using an underlying skeleton controlling the deformation of the mesh. This technique known as skeletal animation allows the artist to produce complex animation sequences in a relatively easy way. However, performing complex transitions between arbitrary animated meshes remains a challenging problem. There is a set of established techniques to perform metamorphosis (3D morphing) between static 3D meshes [Lazarus and Verroust 1998], but most of these can not be easily applied to animated meshes. The approach presented in this poster allows us to produce with great ease metamorphosing transitions between animated meshes of arbitrary topology using polygonal functional hybrids [Kravtsov et al. 2010].</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061.10010063</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures->Computational geometry</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2264230</person_id>
				<author_profile_id><![CDATA[81442614407]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[D.]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kravtsov]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Bournemouth University, UK]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264231</person_id>
				<author_profile_id><![CDATA[81100594560]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[O.]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Fryazinov]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Bournemouth University, UK]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264232</person_id>
				<author_profile_id><![CDATA[81100169564]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[V.]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Adzhiev]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Bournemouth University, UK]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264233</person_id>
				<author_profile_id><![CDATA[81100137963]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[A.]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Pasko]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Bournemouth University, UK]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264234</person_id>
				<author_profile_id><![CDATA[81100459280]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[P.]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Comninos]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Bournemouth University, UK]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Kravtsov, D., Fryazinov, O., Adzhiev, V., Pasko, A., and Comninos, P. 2010. Embedded implicit stand-ins for animated meshes: a case of hybrid modelling. <i>Comput. Graph. Forum 29</i>, 1, 128--140.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Lazarus, F., and Verroust, A. 1998. Three-dimensional metamorphosis: a survey. <i>The Visual Computer 14</i>, 8/9, 373--389.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Pasko, A., Adzhiev, V., Sourin, A., and Savchenko, V. 1995. Function representation in geometric modeling: Concepts, implementation and applications. <i>The Visual Computer</i>, 11, 429--446.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Controlled Metamorphosis of Animated Meshes Using Polygonal-Functional Hybrids Kravtsov D.*, Fryazinov 
O., Adzhiev V., Pasko A. and Comninos P. NCCA, Bournemouth University, UK 1 Introduction Polygonal models 
are widely used in computer animation. Static polygonal models are commonly animated using an underlying 
skeleton controlling the deformation of the mesh. This technique known as skeletal animation allows the 
artist to produce complex animation sequences in a relatively easy way. However, perform­ing complex 
transitions between arbitrary animated meshes remains a challenging problem. There is a set of established 
techniques to perform metamorphosis (3D morphing) between static 3D meshes [Lazarus and Verroust 1998], 
but most of these can not be easily applied to animated meshes. The approach presented in this poster 
allows us to produce with great ease metamorphosing transitions between animated meshes of arbitrary 
topology using polygonal­functional hybrids [Kravtsov et al. 2010]. 2 Method Outline The key idea of 
our method is the integration of polygonal and functional objects [Pasko et al. 1995] within one model. 
This in­volves approximating the animated meshes by convolution surfaces that are de.ned by a set of 
line segments. This is done using a single pose of each mesh. The actual metamorphosis is then performed 
between the generated convolution surfaces. Thus, for the meta­morphosis between the source and destination 
animated meshes we perform: 1. A smooth transition from the animated source mesh to its functional approximation; 
 2. A continuous transition from the functional approximation of the source mesh to the functional approximation 
of the desti­nation mesh; 3. A transition from the functional approximation of the destina­tion mesh 
to the animated destination mesh.  In order to produce a smooth transition from the mesh to the con­volution 
surface (step 1), we project the vertices of the mesh onto the approximating convolution surface. We 
use the per-vertex skin­ning and normal information to retrieve an appropriate position for every vertex 
of the mesh on the resulting convolution surface. Af­ter the projection step, every vertex is assigned 
an offset to its po­sition on the convolution surface as well as the resulting normal vector. Then we 
use this information to perform the local deforma­tion of the skinned mesh. When the positions and normals 
of all the vertices are aligned with the convolution surfaces we switch from the polygonal object to 
the functional object. In step 2, we have the functional approximations of both animated meshes and we 
can employ different methods to generate the intermediate shapes. This method can be a straightforward 
FRep metamorphosis, space­time blending or a complex user-controlled transition employing the skeletons 
de.ning the convolution surfaces. The result of this metamorphosis is a functional object approximating 
the animated destination mesh. In step 3, we apply an inverse deformation to that applied in step 1. 
Since we perform the metamorphosis using FReps all topological * dkravtsov@bournemouth.ac.uk Figure 
1: Metamorphosis changes are handled automatically and we do not need to specify any additional constraints 
on the topology of the original meshes. We have implemented our approach in MayaTM . Our plug-in al­lows 
the animator to produce metamorphosis sequences with great ease. The animator can .ne-tune the metamorphosis 
using a low resolution model in near-real time, while the .nal sequence is pro­duced using a higher resolution 
model. Moreover, when the param­eters of the transition have been evaluated, real-time playback of the 
metamorphosis sequence can be performed using either geom­etry shaders or CUDA SDK. The resolution of 
the model can be varied depending on the available hardware resources. We believe that the incorporation 
of techniques such as this and the underlying hybrid modelling technology into existing mod­elling software 
and games engines will greatly enhance the ability of artists to generate complex models and animations. 
 References KRAVTSOV, D., FRYAZINOV, O., ADZHIEV, V., PASKO, A., AND COMNINOS, P. 2010. Embedded implicit 
stand-ins for animated meshes: a case of hybrid modelling. Comput. Graph. Forum 29, 1, 128 140. LAZARUS, 
F., AND VERROUST, A. 1998. Three-dimensional metamorphosis: a survey. The Visual Computer 14, 8/9, 373 
389. PASKO, A., ADZHIEV, V., SOURIN, A., AND SAVCHENKO, V. 1995. Function representation in geometric 
modeling: Con­cepts, implementation and applications. The Visual Computer, 11, 429 446. Copyright is 
held by the author / owner(s). SIGGRAPH 2010, Los Angeles, California, July 25 29, 2010. ISBN 978-1-4503-0210-4/10/0007 
  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1836853</article_id>
		<sort_key>80</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>7</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>7</seq_no>
		<title><![CDATA[Data driven in-betweening for hand drawn rotating face]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836845.1836853</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1836853</url>
		<abstract>
			<par><![CDATA[<p>In anime production, some key-frames are drawn by artist precisely and then a great number of in-betweening frames are drawn by assistants' hands. However, it is seriously time-consuming and skilled work to draw many characters especially including face rotation. In this paper, we propose an automatic in-betweening technique for rotating face of hand drawn character only from a front image and a diagonal image (Fig.1). Baxter [2009] represented generating in-betweening using image morphing technique. However, their approach doesn't consider reflecting the artist's style and touch. Accordingly, we represent reflecting style and touch using morphing technique trained by his own database and introduced especially to generate a rotational in-betweening faces. This database contains center of gravity of each part (right eye, left eye, nose, mouth, eyebrow) and the contours on the facial image.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.8</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010507</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Graphics recognition and interpretation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2264235</person_id>
				<author_profile_id><![CDATA[81466647938]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Hiroaki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gohara]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Waseda University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264236</person_id>
				<author_profile_id><![CDATA[81421601671]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Shiori]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sugimoto]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Waseda University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264237</person_id>
				<author_profile_id><![CDATA[81100066959]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Shigeo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Morishima]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Waseda University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1592286</ref_obj_id>
				<ref_obj_pid>1591897</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[William Baxter et al., "Compatible Embedding for 2D Shape Animation", IEEE Transactions Visualization and Computer Graphics, vol. 15, Issue 5, p867--879, 2009.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Data Driven In-betweening for Hand Drawn Rotating Face Hiroaki GOHARA Shiori SUGIMOTO  Waseda University 
Shigeo MORISHIMA 1. Introduction In anime production, some key-frames are drawn by artist precisely 
and then a great number of in-betweening frames are drawn by assistants hands. However, it is seriously 
time-consuming and skilled work to draw many characters especially including face rotation. In this paper, 
we propose an automatic in-betweening technique for rotating face of hand drawn character only from a 
front image and a diagonal image (Fig.1). Baxter [2009] represented generating in-betweening using image 
morphing technique. However, their approach doesn t consider reflecting the artist's style and touch. 
Accordingly, we represent reflecting style and touch using morphing technique trained by his own database 
and introduced especially to generate a rotational in-betweening faces. This database contains center 
of gravity of each part (right eye, left eye, nose, mouth, eyebrow) and the contours on the facial image. 
 2. Database Construction Each artist has a style and touch and we focused on the center of gravity 
of facial parts which are reflecting artist s style. Accordingly, we construct the database that has 
facial features like angle of the face ., artist s label i, center of gravity of each part (right eye, 
left eye, nose, mouth, eyebrow)  (j: parts index) calculated by hand plotted points (Fig.2) and feature 
points on facial      contour (k: points index). gi,j,. represents orthogonal projection.  2,,,,,,,,,,,,,,)()(milimilimijijiFFFFFGg.................. 
......mouthnoseeyebroweyeml)4,2()5,1(,)6,0(),( (1) min,,max,,min,,,,,,jijijijijigggg...... (2) Our 
system calculate blend ratios ai,j(.) which is interpolated by ai,j,. using cubic spline interpolation. 
 3. Morphing Method  Feature points  (i: input image index, j: parts index, k: points index) are warped 
by Eq.3. The number of feature points is 98. (3). The morphed texture is generated by mixing front image 
and diagonal using alpha-blending.     Fig.1 Blend ratio for each angle of the face Fig.2 Hand 
plotted points (degree) (a) Input (b) Output      (c) Front (a)Diagonal (b) Outputs  Fig.3 
Closed-test    Fig.4 images used for database     (a) Diagonal (b) Outputs (c) Front  Fig.5 
Open-test    Fig.6 Target hand drawn images  4. Test results We perform closed-test using key images 
shown in Fig.3(a),(c) and ai,j(.)(Fig.4). Fig.3(b) shows generated in-betweening images. This test represents 
that in-betweening can be generated by only two textures we input. Subsequently, we perform open-test 
by using Fig.5(a),(c) as input images and we get Fig.5(b) as a result. 5. Conclusion We perform both 
closed-test and open-test. Closed-test shows that generated in-betweening images in Fig.3(b) are similar 
to target images in Fig.4. As a result, in between image can be gene by only two hand drawn textures. 
Subsequently, open-test shows that generated in-betweening images in Fig.5(b) are similar to target images 
in Fig.6. This results shows that our system can generate in-betweening image of another character drawn 
by same artist. And the in-betweening reflects artist s style and touch in the database. This system 
can handle various facial images and interpolate in-between. Reference William Baxter et al., Compatible 
Embedding for 2D Shape Animation , IEEE Transactions Visualization and Computer Graphics, vol. 15, Issue 
5, p867-879, 2009. 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1836854</article_id>
		<sort_key>90</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>8</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>8</seq_no>
		<title><![CDATA[Fluid-body simulations using vortex particle operations]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836845.1836854</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1836854</url>
		<abstract>
			<par><![CDATA[<p>Modeling continuous media such as fluids remains an elusive goal for interactive simulations. Fluids are particularly challenging because of the complexity imparted by the non-linear equations of motion, and the difficulty in creating stable simulations that retain spatial detail.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.6.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061.10010063</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures->Computational geometry</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2264238</person_id>
				<author_profile_id><![CDATA[81421595159]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Michael]]></first_name>
				<middle_name><![CDATA[J.]]></middle_name>
				<last_name><![CDATA[Gourlay]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Central Florida]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Fluid-Body Simulations using Vortex Particle Operations Michael J. Gourlay University of Central Florida 
/ Electronic Arts mgourlay@fiea.ucf.edu  1 Introduction Modeling continuous media such as fluids remains 
an elusive goal for interactive simulations. Fluids are particularly challenging because of the complexity 
imparted by the non-linear equations of motion, and the difficulty in creating stable simulations that 
retain spatial detail. Content creators for video games and other interactive simulations are familiar 
with using particle systems to model fluid-like systems such as smoke, but those systems traditionally 
lack the complexity and response to passing bodies that real fluids have. We aim to enhance particle 
systems so that effects authors can continue to use familiar systems, and provide a better approximation 
of fluid-like motion which has adequate performance for use in interactive, real-time simulations. Fluids 
move chaotically and chaos has appeal in visual effects. Particles that move under the influence of vortex 
systems with 3 or more vortices behave chaotically. Vortex simulations introduce appealing fluid-like 
chaotic motion into particle systems with minimal cost. We present an algorithm that exploits vortex 
particle methods to introduce fluid-like chaotic motion into traditional particle systems, yielding a 
fast, steerable and interactive fluid simulation. 1.1 Parallelized vorton methods for fluid motion Our 
technique, the code for which we provide publically, treats fluid simulation as operations incorporated 
into a canonical library of particle operations typically available in existing commercial particle systems. 
We therefore discretize vortices as particles (not filaments or sheets). Using the map-reduce paradigm 
and Intel Threading Building Blocks, we implemented scalably parallelized versions of several solvers, 
including integral (direct summation, treecode and a novel fast monopole) and differential (Jacobi, red-black 
Gauss-Seidel with successive over-relaxation and multi-grid) methods. For fluid-body interactions we 
introduce a novel amortized localized approach to obtain a global solution. 2 Methods Reusing and enhancing 
an existing particle system provides an economy of effort and familiarity with existing tools important 
features in commercial game development. The simulation includes several phases, each implemented as 
particle operations: 1. Compute velocity from vorticity. 2. Compute stretching and tilting, viscous 
diffusion etc. 3. Apply other, canonical, particle operations. 4. Advect particles (vorton and tracer). 
 5. Compute fluid-body interactions.  While several of these steps appear in other fluid simulations, 
our approach has two key differences: Each of these steps occurs as a particle operation, reusing an 
existing particle system simulation, so tools already in-place can be reused, retaining the workflow 
familiar to effects authors. Also, step 3 allows the fluid simulation operations to cooperate with existing 
particle operations, such as wind and forces, permitting effects authors to steer the flow beyond the 
restraints of accurate physical simulation. Furthermore, as material quantities, vortex particle play 
multiple roles, including sources of velocity, carriers of momentum and markers for visualization. So 
although other techniques exist that combine fluid simulations with particle systems, this treatment 
provides additional economy due to using particles as the impetus for fluid motion, in addition to their 
traditional roles in visual effects. 3 Results Results include practical interactive visual effects and 
canonical flow scenarios that demonstrate the viability of the algorithm. The user can control the motion 
of rigid bodies and move them through fluid and the fluid pushes and spins objects, all in real time. 
Validation studies indicate the relative accuracy of the six different solvers. Test cases simulate and 
render at least 60 frames per second even when using only a single core, with nearly linear scaling up 
to over a dozen cores. Comparisons with other vortex methods reveal our technique runs faster, with lower 
time complexity and better scaling with the number of particles. 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1836855</article_id>
		<sort_key>100</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>9</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>9</seq_no>
		<title><![CDATA[High speed 3D shape and motion capturing system]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836845.1836855</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1836855</url>
		<abstract>
			<par><![CDATA[<p>A 3D motion capturing and reconstructing system at high speed is presented. The system utilizes the fringe projection technique with one modified DLP projector, one camera and a computing unit to provide real-time reconstruction of forty-two 3D frames per second with the relative accuracy of 1/5000.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.4.8</cat_node>
				<descriptor>Motion</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.8</cat_node>
				<descriptor>Tracking</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010225.10010227</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision tasks->Scene understanding</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245.10010253</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems->Tracking</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010238</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->Motion capture</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352.10010380</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation->Motion processing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352.10010238</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation->Motion capture</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2264239</person_id>
				<author_profile_id><![CDATA[81466643434]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Dung]]></first_name>
				<middle_name><![CDATA[A.]]></middle_name>
				<last_name><![CDATA[Nguyen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The Catholic University of America]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264240</person_id>
				<author_profile_id><![CDATA[81466644590]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Zhaoyang]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The Catholic University of America]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[
S. Gorthi and P. Rastogi, "Fringe projection techniques: Whither we are?", Optics and Lasers in Engineering &#60;b&#62;48&#60;/b&#62;, 133--140 (2010).
]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 High Speed 3D Shape and Motion Capturing System Dung A. Nguyen* , Zhaoyang Wang The Catholic University 
of America Figure 1: Demonstration of a single 3D view result of a human body from three different angles 
1 Abstract A 3D motion capturing and reconstructing system at high speed is presented. The system utilizes 
the fringe projection technique with one modi.ed DLP projector, one camera and a computing unit to provide 
real-time reconstruction of forty-two 3D frames per second with the relative accuracy of 1/5000. 2 Introduction 
Fringe projection pro.lometry (FPP) is one of the leading tech­niques for 3D shape measurement and 3D 
imaging in a variety of .elds. Despite the tremendous development of the FPP technique in the last two 
decades, there are a few practical challenges restrict­ing the broader applications of the technique, 
[Gorthi and Rastogi 2010]. For instance, the capability to measure multiple objects with complex shapes 
in a real-time manner is one of those challenges, and it is highly demanded by numerous applications. 
In this paper, a novel and robust 3D imaging system is presented, and the system is capable of providing 
high speed, high accuracy, and full-.eld 3D imaging of multiple objects. A basic FPP-based 3D imaging 
and shape measurement system usually contains a digital projector, a digital camera, and a computer. 
During the 3D imaging, a set of fringe patterns are projected onto the surfaces of the objects of in­terest. 
The surface height/depth information is naturally encoded into the distorted fringe patterns, which are 
captured by the camera for further processing to get back the height of 3D objects. 3 High Speed 3D 
Imaging A single-chip Digital Light Processing (DLP) projector is utilized with the colorwheel removed; 
this leads to gray scale image projec­tion for any color image. Meanwhile, a fast speed CMOS camera is 
employed to capture each RGB channel of the image. An external triggering circuit is developed as a microcontroller 
to control and synchronize the capturing process of the fast speed camera with the projecting process 
of the DLP projector. Therefore, three images *e-mail: 65nguyen@cardinalmail.cua.edu e-mail:wangz@cua.edu 
 can be obtained in each projection cycle which will signi.cantly increases the speed of 3D measurement. 
In 3D imaging, both the in-plane and out-of plane dimensions must be determined. Due to the fact the 
2D in-plane dimension can be directly calculated from the corresponding digital image through a simple 
transformation with acknowledgment of the camera-object distance, the primary task of the 3D imaging 
is to rigorously de­termine the out-of-plane height and depth information. It has been shown that the 
governing equation of the out-of-plane shape deter­mination can be expressed as: 1+ c1f +(c2 + c3f)i 
+(c4 + c5f)j z = (1) d0 + d1f +(d2 + d3f)i +(d4 + d5f)j where z is the out-of-reference-plane height 
or depth at pixel (i, j), f is the projection fringe phase at the same point, and the coef.­cients c1 
-c5 and d0-d5 are constants determined by the geomet­rical and other system parameters. The above approach 
based on arbitrary and generalized setup of system components is very easy to implement; more importantly, 
it can cope with the numerous un­certainties in practice of FPP. 4 Results To demonstrate the validity 
and applicability of the system, a few experiments have been carried out and the highest speed obtain 
is 42fps at the relative accuracy of 1/5000. We have also con.rmed that if higher accuracy is required 
for other application, relative acu­racy of 1/15,000 is achievable at the speed of 8fps. The above .gure 
demonstrates a result from three different angles of 3D contructed model of a student. A visual inspection 
shows that the 3D images have a very good match with the actual objects. The experiments thus demonstrate 
the practicability of the presented FPP approach or 3D imaging of object(s) complex shapes. References 
S. Gorthi and P. Rastogi, Fringe projection techniques: Whither we are? , Optics and Lasers in Engineering 
48, 133 140 (2010). Copyright is held by the author / owner(s). SIGGRAPH 2010, Los Angeles, California, 
July 25 29, 2010. ISBN 978-1-4503-0210-4/10/0007  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1836856</article_id>
		<sort_key>110</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>10</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>10</seq_no>
		<title><![CDATA[Lions and tigers and bears]]></title>
		<subtitle><![CDATA[investigating cues for expressive creature motion]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836845.1836856</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1836856</url>
		<abstract>
			<par><![CDATA[<p>A digital creature's performance can be thought of as a combination of specifically defined motion and form; a combination that allows the viewer to comprehend the creature's action and intent. Computer graphics offers a variety of methods for defining motion including key-frame animation, data-driven action, rule-based and physically-based motion. However, all of these methods can be complex and time-consuming to implement. Essentially, most computer animation methods force the animator to think about motion at a low-level of abstraction. To create animation tools that simplify the process of creating expressive motion, we need to allow animators to work at a high-level of abstraction. We need determine the minimal elements of form and motion that visually communicate a maximal amount of information about an actor's identity or intentions. By attaching small reflective objects to joint pivot locations and recording at high contrast [Johansson 1973] developed a method for isolating motion from form as a collection of particles, now commonly known as a Point-Light Display (PLD). Manipulating this minimized visual information can even affect the perceived gender of PLD walkers. Cutting [1978] found that exaggerating the movement of points representing the hips and shoulders can bias gender recognition. The goal of our study was to investigate whether viewers use similar visual information to recognize expressive characteristics in animal motion PLDs as when viewing full representations and discover how it might be possible to use that visual information to influence the viewer's perception.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.4.8</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010225.10010227</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision tasks->Scene understanding</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2264241</person_id>
				<author_profile_id><![CDATA[81442618475]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Meredith]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[McLendon]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Texas A&M University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264242</person_id>
				<author_profile_id><![CDATA[81100069081]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Ann]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[McNamara]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Texas A&M University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264243</person_id>
				<author_profile_id><![CDATA[81332515066]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Tim]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[McLaughlin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Texas A&M University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264244</person_id>
				<author_profile_id><![CDATA[81442601881]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Ravindra]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Dwivedi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Texas A&M University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Cutting, J., Proffitt, D., and Kozlowski, L. 1978. A biomechanical invariant for gait perception. <i>J. Experimental Psychology: Human Perception and Performance 4</i>, 3, 357--372.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Johansson, G. 1973. Visual perception of biological motion and a model for its analysis. <i>Perception &amp; Psychophysics 14</i>, 2, 201--211.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Mather, G., and West, S. 1993. Recognition of animal locomotion from dynamic point-light displays. <i>Perception 22</i>, 7, 759--766.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Lions and Tigers and Bears: Investigating cues for expressive creature motion * MeredithMcLendon AnnMcNamara 
TimMcLaughlin RavindraDwivedi§ Department ofVisualization -TexasA&#38;MUniversity Figure 1: A single 
frame, from left to right, eye-tracked data over full representation and over the Point-Light Display. 
1 Perception of Point Light Displays A digital creature s performance can be thought of as a combina­tion 
of speci.cally de.ned motion and form; a combination that allows the viewer to comprehend the creature 
s action and intent. Computergraphics offers a variety of methods for de.ning motion including key-frame 
animation, data-driven action, rule-based and physically-based motion. However, all of these methods 
can be complex andtime-consumingto implement. Essentially, most com­puter animation methods force the 
animator to think about motion at a low-level of abstraction. To create animation tools that simplify 
theprocessof creating expressivemotion,weneed toallowanima­tors to work at a high-level of abstraction. 
We need determine the minimal elements of form and motion that visually communicate a maximal amount 
of information about an actor s identity or in­tentions. By attaching small re.ective objects to joint 
pivot loca­tions and recording athigh contrast[Johansson1973] developed a methodfor isolating motionfromformasacollectionofparticles, 
now commonlyknown as aPoint-LightDisplay(PLD).Manipulat­ingthisminimized visualinformationcanevenaffect 
theperceived gender of PLD walkers. Cutting [1978] found that exaggerating the movement of points representing 
the hips and shoulders can bias gender recognition. The goal of our study was to investigate whether 
viewers use similar visualinformation to recognize expres­sive characteristics in animal motion PLDs 
as when viewing full representations and discover how it might be possible to use that visualinformation 
toin.uencetheviewer sperception. 2 Experiment &#38; Results Our experimental design was inspired by 
the work of Mather and West[1993] using animal motioncaptured insideview,where the kinematic motion is 
most apparent. We used 30 video sequences, creating a correspondingPLD representationfor each animal 
s mo­tion. Three separate experiments were conducted, eachfocusing on recognitionof asingle traitpair:(1) 
Heavy orLight;(2) Predator orPrey;(3) Young orOld. Threegroupsof .veparticipants,ran­domly assigned, 
were eyetracked while viewing the videos. After viewing each video, participants were asked to make a 
judgment on a .ve-point Likert Scale with either word from the trait pair at the extremes of the scale. 
After viewing all of the PLD videos the same process was repeated for the full video presentations. The 
participant s responses can be analyzed in terms of both correct­ness and agreement. We compared each 
animal species average * mgmiller@viz.tamu.edu ann@viz.tamu.edu timm@viz.tamu.edu §ravindra@viz.tamu.edu 
 mass against the average mass of a human to determine the cor­rectness for Experiment 1. The PLD representation 
was correctly identi.ed as heavy or light 30% of the time, and identi.cation of the full representation 
achieved 52% correctness. For Experiment 2, we judged correct identi.cation as a predator or prey by 
deter­mining an animal s biological classi.cation as a member of order Carnivora.Thisresulted ina50% 
correctnessratefor thePLD rep­resentationand87%for thefullrepresentation.Judging correctness forExperiment3wasnearly 
impossibleforall30 videossince there wasnoway toknoweach animal sage.Threevideoscontained ob­viously 
juvenile animals, but only one of the three was correctly identi.ed asyoung. Results from a Wilcoxon 
rank-sum test show that for 93% of the animals inExperiment1,87%inExperiment2,and70%inExperi­ment3, there 
isastatisticallystrongprobability that theresultsfrom both presentation types have the same distribution 
of responses. This means that similar information is conveyed using both PLD an full presentation. To 
investigate where this information is con­centrated within the videos, we examined .xations recorded 
using aneye tracker.Whilenearlyallofthe .xations in thePLDsoccured within our de.ned regions of interest, 
as would be expected due to the extreme contrast in the PLDs, 15 of the full videos had 25% of their 
.xations occur in the regions of interest with .ve videos matching 70% or better. Further analysis of 
the eye gaze data for animals thatproducedbothcorrectnessand agreement revealedthat participants focused 
on similar regions in both the full videos and thePLD representations,withmuch of thegaze locationsoccuring 
near the head and shoulder regions. In future research, we plan to isolate theseregionsand analyze theirballistic 
information. Results from these experiments are guiding the development of a framework for an animation 
system in which the fundamental de­scriptorsof motionareexpressive identitycues: minimallyde.ned structures 
ofform and motion which reliably communicate charac­ter traits. References CUTTING, J., PROFFITT, D., 
AND KOZLOWSKI, L. 1978. A biomechanicalinvariantforgaitperception.J. Experimental Psy­chology: Human 
Perception and Performance 4,3,357 372. JOHANSSON, G. 1973. Visual perception of biological motion and 
a model for its analysis. Perception &#38; Psychophysics 14, 2, 201 211. MATHER, G., AND WEST, S. 1993. 
Recognition of animal lo­comotion from dynamic point-light displays. Perception 22, 7, 759 766. Copyright 
is held by the author / owner(s). SIGGRAPH 2010, Los Angeles, California, July 25 29, 2010. ISBN 978-1-4503-0210-4/10/0007 
  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1836857</article_id>
		<sort_key>120</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>11</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>11</seq_no>
		<title><![CDATA[Lip synchronization by acoustic inversion]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836845.1836857</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1836857</url>
		<abstract>
			<par><![CDATA[<p>Talking computer animated characters are a common sight in video games and movies. Although doing the mouth animation by hand gives the best results it is not always feasible because of cost or time constraints. Therefore producing lip animation automatically is highly desirable. The problem can therefore be phrased as mapping from speech to lip animation or in other words as an acoustic inversion. In our work we propose a solution that takes a sequence of input frames of speech and maps it directly to an output sequence of animation frames. The key point is that there is no need for phonemes or visemes which cuts one step in the usual lip synchronization process.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[facial animation]]></kw>
			<kw><![CDATA[lip synchronization]]></kw>
			<kw><![CDATA[neural networks]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>H.5.5</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.4.8</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010225.10010227</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision tasks->Scene understanding</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002951.10003317.10003371.10003386.10003390</concept_id>
				<concept_desc>CCS->Information systems->Information retrieval->Specialized information retrieval->Multimedia and multimodal retrieval->Music retrieval</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010469.10010475</concept_id>
				<concept_desc>CCS->Applied computing->Arts and humanities->Sound and music computing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2264245</person_id>
				<author_profile_id><![CDATA[81335491640]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Gregor]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hofer]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Edinburgh]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264246</person_id>
				<author_profile_id><![CDATA[81327491248]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Korin]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Richmond]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Edinburgh]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264247</person_id>
				<author_profile_id><![CDATA[81474689903]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Michael]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Berger]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Edinburgh]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Tokuda, K., Yoshimura, T., Masuko, T., Kobayashi, T., and Kitamura, T. 2000. Speech parameter generation algorithms for HMM-based speech synthesis. In <i>Proc. ICASSP</i>, 1315--1318.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Lip Synchronization by Acoustic Inversion Gregor Hofer* Korin Richmond Michael Berger University of 
Edinburgh University of Edinburgh University of Edinburgh  Figure 1: The word how as generated from 
our model (left) and the rendered equivalent sample frames (right). The red contours indicate the .nal 
frame, with frames going back in time represented with progressively lighter grayscale contours. Keywords: 
lip synchronization, facial animation, neural networks 1 Introduction Talking computer animated characters 
are a common sight in video games and movies. Although doing the mouth animation by hand gives the best 
results it is not always feasible because of cost or time constraints. Therefore producing lip animation 
automatically is highly desirable. The problem can therefore be phrased as map­ping from speech to lip 
animation or in other words as an acoustic inversion. In our work we propose a solution that takes a 
sequence of input frames of speech and maps it directly to an output sequence of animation frames. The 
key point is that there is no need for phonemes or visemes which cuts one step in the usual lip synchro­nization 
process. 2 Speech to motion mapping method At the heart of our mapping model is the mixture density 
network (MDN). The MDN can be considered as combining a trainable re­gression function (typically a non-linear 
regressor such as an ar­ti.cial neural network) with a probability density function. In our work, we 
have been using a multilayer perceptron (MLP) as a train­able non-linear regressor and a Gaussian mixture 
model (GMM). An illustration is shown in Fig. 2. The role of the MLP is to take an input vector in one 
domain (x, acoustic features in this case) and map to the control parameters (priors, means and variances) 
of the pdf over the domain of the target parameters (t, motion features). In this way, the MDN offers 
a model of probability density over the target domain conditioned on the input domain, p(t|x). Once trained, 
we input a sequence of acoustic feature vectors for an ut­terance and get as output a sequence of pdfs 
over the static motion features and their delta and deltadeltas. We then apply a maximum likelihood parameter 
generation algorithm (MLPG)[Tokuda et al. 2000] to this sequence of pdfs in order to obtain a single, 
most prob­able trajectory which optimizes the constraints between the distri­butions of static, delta 
and deltadelta features. This trajectory then drives the animation. Trajectories can be generated in 
real time from the speech frames using a small amount of context frames. The model was trained *e-mail: 
ghofer@inf.ed.ac.uk e-mail:korin@cstr.ed.ac.uk e-mail:m.a.berger@sms.ed.ac.uk  Figure 2: The mixture 
density network we use combines a mul­tilayer perceptron and Gaussian mixture model. We input speech 
frames and get distributions over animation frames. on the .rst 4 PCA components of 28 tracked points 
around the mouth. The total amount of training data was 207 utterances (~ 25 min.). The input speech 
features were the .rst 25 mel cepstrum co­ef.cients, which are standard speech recognition features. 
A short perceptual evaluation was carried out with 17 subjects judging 5 ut­terances. No signi.cant difference 
between the preference for ani­mation from the original data (46%) and animation from the model (54%) 
was found, which is a promising result to further develop MDNs for lip synchronization. References TOKUDA, 
K., YOSHIMURA, T., MASUKO, T., KOBAYASHI, T., AND KITAMURA, T. 2000. Speech parameter generation al­gorithms 
for HMM-based speech synthesis. In Proc. ICASSP, 1315 1318. Copyright is held by the author / owner(s). 
SIGGRAPH 2010, Los Angeles, California, July 25 29, 2010. ISBN 978-1-4503-0210-4/10/0007  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1836858</article_id>
		<sort_key>130</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>12</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>12</seq_no>
		<title><![CDATA["Mod-Leg" a modular legged robotic system]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836845.1836858</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1836858</url>
		<abstract>
			<par><![CDATA[<p>The Modular Legged robotic system [1] "Mod-Leg" presented here has been bio-inspired from a Snake's vertebrae and a caterpillar's legged structure. The system can be configured to a 4-legged robotic dog, a hexapod, a caterpillar and a Snake robot. This robot's novel design achieves compliance to the terrain using a combination of legs and electronically actuated universal spine. A unique simulator has been designed for this purpose. Some of the things we learned while developing this robotic system have been presented below.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.2.9</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010520.10010553.10010554</concept_id>
				<concept_desc>CCS->Computer systems organization->Embedded and cyber-physical systems->Robotics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010213.10010204</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Control methods->Robotic planning</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010199.10010204</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Planning and scheduling->Robotic planning</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2264248</person_id>
				<author_profile_id><![CDATA[81466647874]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Sriranjan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Rasakatla]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Robotics Research Lab, IIIT-H]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264249</person_id>
				<author_profile_id><![CDATA[81458642185]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[K.]]></first_name>
				<middle_name><![CDATA[Madhava]]></middle_name>
				<last_name><![CDATA[Krishna]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Robotics Research Lab, IIIT-H]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264250</person_id>
				<author_profile_id><![CDATA[81536764056]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Bipin]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Indurkhya]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Robotics Research Lab, IIIT-H]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Demo reel of the robot is here: http://www.youtube.com/watch?v=Wf9Y-lB_t7s]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Mod-Leg A modular Legged robotic system Sriranjan Rasakatla K Madhava Krishna Bipin Indurkhya Robotics 
Research Lab Robotics Research Lab Robotics Research Lab IIIT-H IIIT-H IIIT-H infibit@gmail.com mkrishna@iiit.ac.in 
bipin@iiit.ac.in  (a) (b) (c) (d) Figure 1: (a) Shows the generic modular legs with 2 d.o.f spinal joints 
linkage. (b) Hexapod configuration climbing over a slope (c) 4 legged dog configuration walking and climbing 
over a slope (d) Snake configuration with just the robotic spine and no legs. 1. Abstract The Modular 
Legged robotic system [1] Mod-Leg presented here has been bio-inspired from a Snake s vertebrae and a 
caterpillar s legged structure. The system can be configured to a 4-legged robotic dog, a hexapod, a 
caterpillar and a Snake robot. This robot s novel design achieves compliance to the terrain using a combination 
of legs and electronically actuated universal spine. A unique simulator has been designed for this purpose. 
Some of the things we learned while developing this robotic system have been presented below. 2. System 
Features. a) We developed an API (modleg API) for Sketchy Physics. The integrated environment of Google 
Sketchup with Sketchy Physics had limitations due to the inbuilt minimal ruby interface. This avoided 
programming the virtual models/characters in the physics simulation for full fledged kinematic algorithms. 
So a ruby plugin which would dump the physics engine parameters like motor velocities, torque, joints 
exerted by the piston, servo angle values into a shared memory was written. A DLL was then written with 
functions to read and write the values from the shared memory. Using this ruby plugin and a DLL as the 
bridge a game developer or a researcher looking for a good design and physics simulation package could 
read the values from the shared memory. This approach provided an API to sketchy physics independent 
of the language (as all one needs to do is read and write into the shared memory). So a programmer using 
Java, C, C++, Ruby or python etc could use the API. This was formerly not possible with Google Sketchup 
running sketchy Physics, b) Also using the design idea of a modular leg with 2 d.o.f spinal joints we 
developed templates for a Snake robot, a caterpillar robot, a 6 legged Hexpod and a 4 legged robotic 
dog. This can be adopted in games and simulation environments c) The virtual model in the simulator was 
closely modeled to the real prototype in both design and API. This ensured that the kinematic algorithms 
tested in the simulation engine worked exactly the same on the prototype robot during real time tests. 
The common API ensured that no further porting was required to transfer the sequence of actions of the 
virtual model to the real robot. This unified development approach greatly reduced design time, development 
time and saved resources (battery power and replacements for worn out parts) required for the real time 
tests. We feel this approach would also help in bringing virtual models/characters to robotic reality 
(life). Also simulations were done to make the robot swim in water (a buoyant environment).The tripod 
gait in hexapod configuration of the robot did not show good forward velocity. The integrated simulation 
and design environment enabled us to observe and change the paddle design then and there it self. Also 
the swimming algorithm was changed using the modleg API to get more forward velocity. d) One can interact 
with the virtual robot and the real prototype from a single point GUI. We call this the universal simulator 
and controller. An inverse kinematic stabilization algorithm on terrain with changing and unpredictable 
slopes was written for the robot. The algorithm adjusts the hip to toe lengths of the robot for making 
it stable. An accelerometer was used as an input device to vary the slope of a test platform on which 
the robotic dog was resting in the physics simulation. Later during real time tests when the accelerometer 
was mounted on the platform it was observed that the real robot stabilized itself with negative gains. 
This was possible because of the universal controller built developed on a common API. e) The maximum 
possible stiffness of the spinal cord of the robot in 6 legged and caterpillar configuration was set 
to a threshold value. This universal spine made the virtual models show compliance to uneven terrain. 
This did not require complex inverse kinematic programming by taking into consideration the varying geometries 
of the terrain. The compliance was achieved using a combination of legs and the universal spinal cord. 
This idea of compliance can thus be used in games for templates like Snakes, caterpillars and hexapods. 
The compliance to terrain was also observed on the real robot while climbing slopes up and down. f) The 
robot can be controlled from a Laptop s USB port using an Zigbee RF modem. The robot also uses CRC based 
error checking protocol to overcome communication errors and still move under noisy RF conditions. The 
robot is equipped with onboard lithium polymer power with over current and over voltage protection for 
the servos. 3.References : [1]Demo reel of the robot is here: http://www.youtube.com/watch?v=Wf9Y-lB_t7s 
SIGGRAPH-2010 Copyright is held by the author / owner(s). SIGGRAPH 2010, Los Angeles, California, July 
25 29, 2010. ISBN 978-1-4503-0210-4/10/0007  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1836859</article_id>
		<sort_key>140</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>13</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>13</seq_no>
		<title><![CDATA[Motion scoring]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836845.1836859</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1836859</url>
		<abstract>
			<par><![CDATA[<p>The one-way road leading music to motion has many manifestations. Choreographers build dance movements to match music features, subway passengers tap their feet following their iPod song hits, and the number one rule at any party is: if you wanna dance, dance to the music!</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2264251</person_id>
				<author_profile_id><![CDATA[81464649658]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Adriana]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Schulz]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Institute of Pure and Applied Mathematics]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264252</person_id>
				<author_profile_id><![CDATA[81466640832]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Marcelo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Cicconet]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Institute of Pure and Applied Mathematics]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264253</person_id>
				<author_profile_id><![CDATA[81100202267]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Luiz]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Velho]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Institute of Pure and Applied Mathematics]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1085753</ref_obj_id>
				<ref_obj_pid>1085714</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[
Dobrian, C., and Bevilacqua, F. 2003. Gestural control of music using the vicon 8 motion capture system. In <i>NIME '03.</i>
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[
Kim, G., Wang, Y., and Seo, H. 2007. Motion control of a dancing character with music. <i>Computer and Information Science, ACIS International Conference on 0</i>, 930--936.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>566605</ref_obj_id>
				<ref_obj_pid>566570</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[
Kovar, L., Gleicher, M., and Pighin, F. 2002. Motion graphs. In <i>SIGGRAPH '02</i>, ACM, New York, NY, USA.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Motion Scoring Adriana Schulz Marcelo Cicconet Luiz Velho VISGRAF Laboratory, Institute of Pure and 
Applied Mathematics  1 Motivation The one-way road leading music to motion has many manifesta­tions. 
Choreographers build dance movements to match music fea­tures, subway passengers tap their feet following 
their iPod song hits, and the number one rule at any party is: if you wanna dance, dance to the music! 
This relation has also been used to create motion in order to bring characters to life. The large human 
motion database, made avail­able by the widespread of Motion Capture technologies, has en­couraged researchers 
to investigate new methods for creating ani­mations from pre-recorded movements. In this context, music 
is a natural guiding method for motion synthesis [Kim et al. 2007]. But why not the other way around? 
In .lm scoring, for instance, music is composed based on visual action, and in video games, the intensity 
of the soundtrack varies according to the player s perfor­mance. Hence, the direction motion-to-music 
is feasible. How­ever, to the best of our knowledge, motion has only been used to guide music composition 
using low level commands [Dobrian and Bevilacqua 2003]. The intention of this work is to introduce minimal 
conditions that allow a dance-guided music composition. In what follows, we de­scribe a method in which 
a measure-synchronous motion graph is used as a tool to guide the concatenation of musical phrases for 
the composition of a new song. 2 Score Building A central issue that has to be studied is how to relate 
motion to music beats. A computational analysis of how abrupt changes in the movement direction and acceleration 
are related to music rhythm requires the establishment and quanti.cation of a large number of parameters. 
We circumvent this problem by exploring the human ability to instinctively relate both motion and music 
signals. In our approach, MoCap is done while a dancer performs to the songs in the database, automatically 
connecting these signals. The .st step is to compose samba style songs, each with a different melodic 
instrument. The songs are metrically organized in mea­sures, which determine the size of the melodic 
phrases. The latter are composed based on high level parameters, such as the number of notes and the 
current chord being played. While a dancer performs to each of the music pieces, motion cap­ture technology 
is used to record the movements. This way, the motion is automatically synchronized with the music and, 
hence, can be naturally segmented according to the melodic phrases. A measure-synchronous motion graph 
is then built by observing segment boundaries similarities. We used the similarity metric de­scribed 
in [Kovar et al. 2002]. The end of each motion phrase is connected to the .ve nearest segments, creating 
a directional weighted graph. A restricted random walk on this graph creates a new motion. We observed 
that two restrictions are appropriate for the resulting mu­sic to resemble samba style pieces. First, 
musical instruments should not vary too frequently. In our implementation, this is guar­anteed by dynamically 
varying the weights of the graph edges dur­ing the walk. Second, it is important to observe intensity 
variations between large music blocks during composition. We used three dis­tinct blocks, and motion 
phrases were annotated accordingly. Dur­ing motion synthesis, the user can specify the number of phrases 
desired for each block and the order in which they occur. The large number of connections in the graph 
is combined with a weighting system to prevent the walk from reaching the last phrase of each clip, which 
would be a dead end. Nevertheless, after the total number of phrases has been concatenated, a restricted 
graph search allows quickly reaching the last phrase. Once the new dance is synthesized, the corresponding 
melody score is composed by following the high level parameters corresponding to the segments used. The 
chord progression is kept and the drum loops are adjusted to match the generated motion. We notice that 
in the samba style, feet movements are more intense in the pres­ence of the tambourine. This behavior 
is automatically preserved by observing the presence of drums in the original music phrases. 3 Future 
Directions This work indicates that motion can be used to create music in non-trivial ways. Further directions 
in this .eld include exploring more sophisticated kinds of musically-consistent motion segmenta­tions 
and analyses. The relation between melody and mood is well known by music theorists and the association 
between mood and motion can be inferred from the concept of body language. There­fore, understanding 
mood as a link between melody and motion would be an interesting topic for further investigation. References 
DOBRIAN, C., AND BEVILACQUA, F. 2003. Gestural control of music using the vicon 8 motion capture system. 
In NIME 03. KIM, G., WANG, Y., AND SEO, H. 2007. Motion control of a dancing character with music. Computer 
and Information Sci­ence, ACIS International Conference on 0, 930 936. KOVAR, L., GLEICHER, M., AND PIGHIN, 
F. 2002. Motion graphs. In SIGGRAPH 02, ACM, New York, NY, USA. Copyright is held by the author / owner(s). 
SIGGRAPH 2010, Los Angeles, California, July 25 29, 2010. ISBN 978-1-4503-0210-4/10/0007  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1836860</article_id>
		<sort_key>150</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>14</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>14</seq_no>
		<title><![CDATA[Multi-level segmentation of dance motion by piecewise regression]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836845.1836860</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1836860</url>
		<abstract>
			<par><![CDATA[<p>It has been recognized that a technique to divide a raw motion-capture data stream of a dance into segments on the time axis is needed [Sonoda 2008]. In particular, the extraction of the higher-level information such as the hierarchical segmentation-structure is a subject of growing interest at the present time. In this study, the authors attempt to develop a method to segment dance motion in a multi-level style, namely in a hierarchical fashion.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.4.8</cat_node>
				<descriptor>Motion</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.6</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010352.10010238</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation->Motion capture</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352.10010380</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation->Motion processing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245.10010248</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems->Video segmentation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010238</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->Motion capture</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245.10010247</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems->Image segmentation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2264254</person_id>
				<author_profile_id><![CDATA[81331499988]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Takeshi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Miura]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Akita University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264255</person_id>
				<author_profile_id><![CDATA[81319497817]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Kazutaka]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Mitobe]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Akita University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264256</person_id>
				<author_profile_id><![CDATA[81319494392]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Takaaki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kaiga]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Warabi-za Co., Ltd.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264257</person_id>
				<author_profile_id><![CDATA[81319504710]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Takashi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yukawa]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[North Asia University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264258</person_id>
				<author_profile_id><![CDATA[81319502599]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Toshiyuki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Taniguchi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Akita University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264259</person_id>
				<author_profile_id><![CDATA[81100502669]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Hideo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tamamoto]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Akita University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264260</person_id>
				<author_profile_id><![CDATA[81319504709]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>7</seq_no>
				<first_name><![CDATA[Noboru]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yoshimura]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Akita University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[
McGee, V. E. et al. 1970. Piecewise Regression, <i>Journal of the American Statistical Association</i>, 65, 331, 1109--1125.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[
National Folk Dance Federation of Japan, ed. 2007. <i>Furusato no Min'y&#333; I</i>, National Folk Dance Federation of Japan (in Japanese).
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[
Preston, V. 1963. <i>A Handbook for Modern Educational Dance</i>, Macdonald &amp; Evans Ltd.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[
Shigeizumi, Y. et al. 1981. The History of Folkdance "AKITA ONDO", <i>Memoirs of the Faculty of Education, Akita University, Educational Science</i>, 31, 114--126 (in Japanese).
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[
Sonoda, M. et al. 2008. Segmentation of Dancing Movement by Extracting Features from Motion Capture Data, <i>Journal of the IIEEJ</i>, 37, 3, 303--311.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Multi-Level Segmentation of Dance Motion by Piecewise Regression Takeshi Miura Kazutaka Mitobe Takaaki 
Kaiga Takashi Yukawa Toshiyuki Taniguchi Hideo Tamamoto Noboru Yoshimura Akita University Warabi-za 
Co., Ltd. North Asia University 1. Introduction It has been recognized that a technique to divide a 
raw motion- capture data stream of a dance into segments on the time axis is needed [Sonoda 2008]. In 
particular, the extraction of the higher­level information such as the hierarchical segmentation-structure 
is a subject of growing interest at the present time. In this study, the authors attempt to develop a 
method to segment dance motion in a multi-level style, namely in a hierarchical fashion. 2. Multi-Level 
Segmentation of Dance Motion  A human-body model with skeletal structure is used in this study. Body 
motions are described using the exponential maps of the joint angles. The components of the exponential 
map corre- spond to the respective joint movements as shown in Figure 1. v = [ vx vy vz ]T : Exponential 
map v/|v| : Unit vector (direction of rotation axis) |v| : Rotation angle vx : Flexion/Extension vy : 
Internal/External rotation vz : Adduction/Abduction Number of joints :14 Number of variables:42 (3 components 
× 14 joints) Figure 1: Model of a human body. There are two types of levels in the segmentation of dance 
motion: the physical level and the semantic level [Sonoda 2008]. On the physical level, moments at each 
of which the change of the aspect of movement occurs are selected as the division points. Such moments 
can be extracted by searching the local-minimum /maximum points in the time-series data of the exponential 
map as shown in Figure 2. Local minimum/maximum Aspect of movement : change vx Flexion Extension  
Division point on the physical level. t Figure 2: Change of the aspect of movement. It is well known 
that searching the local-minimum/maximum points by detecting the sign change of the differential coefficient 
of raw time-series data is too sensitive to small fluctuations. The technique of piecewise linear regression 
[McGee 1970] is used to guarantee the robustness against the fluctuations; the influence of them is absorbed 
in the regression process. The number of divi- sion points is determined by maximizing the following 
formula: K -1 42 V1 = 1 ..|am (k +1) -a (k) | (1) m K -1 k=1 m=1 where a (k) is the slope for the mth 
variable at the kth division m point, derived as shown in Figure 2, and K is the number of divi- sion 
points (the first frame is added), respectively. The parameter adjustment frequently required in the 
extraction of the local minimum/maximum is not needed in the above procedure. miura@ipc.akita-u.ac.jp, 
kaiga@warabi.or.jp, yukawa@nau.ac.jp The stillness of the body is evaluated in the semantic-level segmentation 
since the completion of each motion in a dance is recognized by the stillness [Preston 1963]. The physical-level 
division points each of which gives the local minimum of the following formula are selected as those 
on the first semantic level: 42 s(k) =.|am (k -1)| (2) m=1 Division points on the higher semantic levels 
are selected in the same way, namely by extracting the local minimums at each level. 3. Result Figure 
3 shows an example of the multi-level segmentation of dance motion. The motion-data stream was acquired 
in the per­formance of the Japanese folk dance Ondo of Nishimonai Bon Odori by a motion capture system 
with magnetic sensors. The obtained physical level consists of 35 segments. At the first semantic level, 
most of the division points agree with those in Ref. [Shigeizumi 1981] which shows the division of the 
whole motion of Ondo into the unit gestures. The structure of the second semantic level indicates the 
relationship between the motion and the musical accompaniment [NFDFJ 2007]. Physical level Semantic 
levels : Undetected between phrases between motives Corresponding to the musical form of the accompaniment 
[NFDFJ 2007]. Figure 3: Multi-level segmentation (Japanese folk dance Nishimonai Bon Odori, Ondo). 4. 
Conclusion The above result shows that the present method is effective in the multi-level segmentation. 
It should also be noted that the present method is applicable to any dance without adjustment. Acknowledgment 
Part of this study was supported by the Strategic Information and Communications R&#38;D Promotion Programme 
(SCOPE) of the Ministry of Internal Affairs and Communications of Japan. McGee, V. E. et al. 1970. Piecewise 
Regression, Journal of the American Statistical Association, 65, 331, 1109-1125. National Folk Dance 
Federation of Japan, ed. 2007. Furusato no Min yo I, National Folk Dance Federation of Japan (in Japanese). 
Preston, V. 1963. A Handbook for Modern Educational Dance, Macdonald &#38; Evans Ltd. Shigeizumi, Y. 
et al. 1981. The History of Folkdance AKITA ONDO , Memoirs of the Faculty of Education, Akita University, 
Educational Science, 31, 114-126 (in Japanese). Sonoda, M. et al. 2008. Segmentation of Dancing Movement 
by Extracting Features from Motion Capture Data, Journal of the IIEEJ, 37, 3, 303-311. Copyright is held 
by the author / owner(s). SIGGRAPH 2010, Los Angeles, California, July 25 29, 2010. ISBN 978-1-4503-0210-4/10/0007 
 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1836861</article_id>
		<sort_key>160</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>15</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>15</seq_no>
		<title><![CDATA[Optimization of cloth simulation parameters by considering static and dynamic features]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836845.1836861</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1836861</url>
		<abstract>
			<par><![CDATA[<p>Realistic drape and motion of virtual clothing is now possible by using an up-to-date cloth simulator, but it is even difficult and time consuming to adjust and tune many parameters to achieve an authentic looking of a real particular fabric. Bhat et al. [2003] proposed a way to estimate the parameters from the video data of real fabrics. However, this projects structured light patterns on the fabrics, so it might not be possible to estimate the accurate value of the parameters if fabrics have colors and textures. In addition to the structured light patterns, they use a motion capture system to track how the fabrics move. In this paper, we will introduce a new method using only a motion capture system by attaching a few markers on fabric surface without any other devices. Moreover, animators can easily estimate the parameters of many kinds of fabrics with this method. Authentic looking and motion of simulated fabrics are realized by minimizing error function between captured motion data and synthetic motion considering both static and dynamic cloth features.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.6.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061.10010063</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures->Computational geometry</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2264261</person_id>
				<author_profile_id><![CDATA[81466642917]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Shoji]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kunitomo]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Waseda University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264262</person_id>
				<author_profile_id><![CDATA[81447596181]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Shinsuke]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nakamura]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Waseda University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264263</person_id>
				<author_profile_id><![CDATA[81100066959]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Shigeo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Morishima]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Waseda University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>846282</ref_obj_id>
				<ref_obj_pid>846276</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[K. S. Bhat, C. D. Twigg, J. K. Hodgins, P. K. Khosla, Z. Popovic and S. M. Seitz. "Estimating Cloth Simulation Parameters from Video", SCA, pp37--51, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Optimization of Cloth Simulation Parameters by Considering Static and Dynamic Features Shoji KUNITOMOi 
Shinsuke NAKAMURA Shigeo MORISHIMAi School of Science and Engineering Waseda University 1 Introduction 
 Realistic drape and motion of virtual clothing is now possible by using an up-to-date cloth simulator, 
but it is even difficult and time consuming to adjust and tune many parameters to achieve an authentic 
looking of a real particular fabric. Bhat et al. [2003] proposed a way to estimate the parameters from 
the video data of real fabrics. However, this projects structured light patterns on the fabrics, so it 
might not be possible to estimate the accurate value of the parameters if fabrics have colors and textures. 
In addition to the structured light patterns, they use a motion capture system to track how the fabrics 
move. In this paper, we will introduce a new method using only a motion capture system by attaching a 
few markers on fabric surface without any other devices. Moreover, animators can easily estimate the 
parameters of many kinds of fabrics with this method. Authentic looking and motion of simulated fabrics 
are realized by minimizing error function between captured motion data and synthetic motion considering 
both static and dynamic cloth features. 2 Capture Motion Data of Fabrics As a trial, four typical variations 
of fabrics are examined: silk, denim, cotton and synthetic fiber that is composed of polyester 65% and 
rayon 35%. 81 markers with about 5cm interval are attached to each square swatch of fabric with the size 
of 50cm by 50cm. Two markers on the top corner are considered as points of action. A swatch is swung 
back and forth, and markers are tracked by using 14 VICON motion capture cameras which record with 120 
fps. 500 frames are used to estimate parameters. 3 Error Function We observed that when impressions 
of real and simulated fabrics look similar, their shapes, moving speed and bending behavior play an important 
role. Therefore, we focused on marker coordinate to evaluate shape, marker velocity to evaluate speed 
and curvature to evaluate how fabrics bend for each corresponding vertex. Then the error function is 
defined by eq(1), RS RS N ax -x +fv -v +r{KR -K S + KR -K2 S }E =f ii ii 1i 1i 2ii (1) N i where N 
is a number of vertex, xi is an average of coordinates in all frames, vi is an average of velocity in 
all frames, K1i is an average of curvature for vertical direction, K2i is an average of curvature for 
horizontal direction. Note that we calculate the neighborhood of the vertex using Catmull-Rom Splines 
when we calculate the curvatures. In addition, a=1, f= 0.5 and r= 2 in this paper.  4 Optimizing Parameters 
 In this paper, NVIDIA® PhysX was used as a cloth simulator. In PhysX, there are many parameters we can 
adjust but if a collision between cloth and rigid bodies is not considered, only five parameters should 
be adjusted: stretching stiffness, bending stiffness, damping coefficient, density and thickness. Note 
that shear stiffness wasn t estimated just because there isn t such a parameter in PhysX. The value of 
those parameters are decided by minimizing the error function in eq(1) by simulated annealing. ie-mail: 
time-traveler@akane.waseda.ac.jp ie-mail: shigeo@waseda.ac.jp  Copyright is held by the author / owner(s). 
SIGGRAPH 2010, Los Angeles, California, July 25 29, 2010. ISBN 978-1-4503-0210-4/10/0007  (a) manually 
tuned  (b) real fabrics (c) after optimized Figure 1: Optimization result of silk (a) manually tuned 
 (b) real fabrics (c) after optimized Figure 2: Optimization result of denim (a) manually tuned (b) 
real fabrics (c) after optimized Figure 3: Optimization result of cotton (a) manually tuned  (b) real 
fabrics  (c) after optimized Figure 4: Optimization result of synthetic fiber  5 Results We show the 
results of silk in figure 1, denim in figure 2, cotton in figure 3 and synthetic fiber in figure 4. Note 
that pictures which are the same types of fabric are compared in the same time. We reproduced the silk 
swelling and got a soft impression. On the other hand, the denim gave us a hard impression after we optimized. 
The cotton stretched unnaturally at the edge of the top, but it is improved after optimized. The synthetic 
fiber moved similar to the real one especially at the edge of the bottom. 6 Conclusion We proposed 
an optimization framework for identifying the parameters of cloth simulation from a motion capture system. 
In consequence, we successfully reproduced the motion of colored and designed fabrics. In addition, it 
is notable that this method doesn t depend on the types of simulator. As a future work, we believe that 
it is possible to estimate parameters even though we use a fewer number of markers. Along with this idea, 
we also need to define better error function by using these results so that we can get better results 
which behave more similar to the real fabrics. Finally, our goal is to apply estimated parameters to 
more complex simulation, for example shirts and pants. References K.S. Bhat, C.D. Twigg, J.K. Hodgins, 
P.K. Khosla, Z. Popovic and S.M. Seitz. Estimating Cloth Simulation Parameters from Video , SCA, pp37­51, 
2003. 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1836862</article_id>
		<sort_key>170</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>16</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>16</seq_no>
		<title><![CDATA[Real-time compositing framework for interactive stereo fMRI displays]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836845.1836862</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1836862</url>
		<abstract>
			<par><![CDATA[<p>This research concentrates on providing high fidelity animation, only achievable with offline rendering solutions, for interactive fMRI-based experiments. Virtual characters are well established within the film, game and research worlds, yet much remains to be learned about which design, stylistic or behavioural factors combine to make a believable character. The definition of believability depends on context. When designing and implementing characters for entertainment, the concern is making believable characters that the audience will engage with. When using virtual characters in experiments, the aim is to create characters and synthetic spaces that people respond to in a similar manner to their real world counterparts. Research has shown that users show empathy for virtual characters. However, uncanny valley effects -- ie dips in user impressions -- can arise: behavioural fidelity expectations increase alongside increases in visual fidelity and vice versa. Often, characters used within virtual environments tend to be of fairly low fidelity due to technological constraints including rendering in real-time (Garau et al. 2003). This problem is addressed here by using non-linear playback and compositing of pre-rendered high fidelity sequences.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.4.0</cat_node>
				<descriptor>Image displays</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2264264</person_id>
				<author_profile_id><![CDATA[81335496706]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Fiona]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Rivera]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Sussex, UK]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264265</person_id>
				<author_profile_id><![CDATA[81100063706]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Phil]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Watten]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Sussex, UK]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264266</person_id>
				<author_profile_id><![CDATA[81474668331]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Patrick]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Holroyd]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Sussex, UK]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264267</person_id>
				<author_profile_id><![CDATA[81466648437]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Felix]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Beacher]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Sussex, UK]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264268</person_id>
				<author_profile_id><![CDATA[81100567689]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Katerina]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Mania]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Technical University of Crete, Greece]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264269</person_id>
				<author_profile_id><![CDATA[81466642337]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Hugo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Critchley]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Sussex, UK]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>642703</ref_obj_id>
				<ref_obj_pid>642611</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[
Garau, M., et. al (2003). <i>The impact of avatar realism and eye gaze control on perceived quality of communication in a shared immersive virtual environment.</i> In <i>CHI.</i> 2003: ACM Press.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[
Milgram, S. (1963). "<i>Behavioral Study of Obedience". Journal of Abnormal and Social Psychology</i> 67: 371--378.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[
Vinayagamoorthy, V., et. al (2005). <i>Building Characters: Lessons Drawn from Virtual Environments.</i> Toward Social Mechanisms of Andriod Science: A CogSci Workshop 119--12
]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Real-Time Compositing Framework for Interactive Stereo fMRI Displays Fiona Rivera Phil Watten Patrick 
Holroyd Felix Beacher Katerina Mania, Hugo Critchley University University University of Sussex, UK of 
Sussex. UK of Sussex, UK This research concentrates on providing high fidelity animation, only achievable 
with offline rendering solutions, for interactive fMRI-based experiments. Virtual characters are well 
established within the film, game and research worlds, yet much remains to be learned about which design, 
stylistic or behavioural factors combine to make a believable character. The definition of believability 
depends on context. When designing and implementing characters for entertainment, the concern is making 
believable characters that the audience will engage with. When using virtual characters in experiments, 
the aim is to create characters and synthetic spaces that people respond to in a similar manner to their 
real world counterparts. Research has shown that users show empathy for virtual characters. However, 
uncanny valley effects ie dips in user impressions can arise: behavioural fidelity expectations increase 
alongside increases in visual fidelity and vice versa. Often, characters used within virtual environments 
tend to be of fairly low fidelity due to technological constraints including rendering in real-time (Garau 
et al. 2003). This problem is addressed here by using non-linear playback and compositing of pre-rendered 
high fidelity sequences. Previous research into evaluating whether virtual characters placed in immersive 
collaborative environments fulfil their role, is limited to acquiring ratings of pleasantness or user 
fidelity impressions through self-report after the experience has occurred (Vinayagamoorthy 2005). It 
is challenging to derive neuroscientific correlates of subjective feelings and emotions when interacting 
with virtual characters. The ultimate goal of this framework is to explore whether natural and artificial 
characters of varied fidelity engage common perceptual or neuroscientific mechanisms. Such input is non-obtrusive 
and is derived at the same time as the experience occurs. However, it is not straightforward to provide 
synthetic stimuli to be displayed in fMRI displays due to the infrastructural and technical demands. 
Until now such investigations utilized still pictures or non stereoscopic 3D. In order to address such 
requirements, the framework puts forward a sophisticated real-time compositing system which takes input 
from a variety of media inclusive of graphics, video or a combination. The novel computational framework 
also enables stereo viewing while immersed in an fMRI scanner allowing for user interactivity throughout. 
The system presented here is a real-time nonlinear video compositing engine designed to play back and 
composite video on demand. The aim is to produce the effect of real-time interactive 3D animation where, 
in reality, long render cycles are required in order to generate high quality images. The framework is 
written in Cocoa, the native application program environment for the OS X operating system, and is reliant 
on the Quartz graphics subsystem. Whereas the logic is written in code, all of the compositing has been 
developed with Quartz Composer: the graphical environment for building graphics applications in OS X. 
Quartz Composer takes uncompressed video frames from the video playout back-end and renders them onto 
the screen for presentation. The use of Quartz Composer significantly reduces development time and introduces 
little overhead. To provide interactivity from pre-rendered video clips, the video sequences are designed 
to seamlessly combine with each other while individual parts of the scene are composited at play-out 
time. University Technical University University of Sussex, UK of Crete, Greece of Sussex, UK The framework 
presented enables strictly controlled experimental paradigms for neuroimaging. Neuroimaging experiments 
benefit from utilizing synthetic stimuli in the form of virtual characters instead of real world confederates 
since the synthetic stimuli are not bound by the same ethical constraints. User interactions as well 
as the actions of synthetic characters are synchronised to the fMRI scanner by using trigger information. 
A frequency modulated audio signal is generated at prescribed times within the experimental phase. The 
audio signal is fed into a biometric recorder, which also records heart beat information, scanning synchronisation 
information etc. Simultaneously to this, a log is generated marking the exact time the sync pulses are 
sent to the biometric recorder as well as logs for user interactions. As the final output is generated 
from pre-rendered video sequences, the temporal resolution is guaranteed. There will be no animation 
glitches or artefacts distracting the engagement of the participant. The system presented is being used 
to investigate neuro-correlates of empathy in addition to ways of creating believable characters. An 
economic game, combined with Milgram s original experimental scenario, is interactively played in an 
fMRI scanner (Milgram 1963). The system presented incorporates digital characters, thereby eliminating 
any discrepancies between performances (Fig. 1). The scene comprises a digital character who sits behind 
a desk and offers money to a participant located in an fMRI scanner. Indications of the amounts given 
and received are represented by score boards. As the final output is in stereo, a video for each number 
is generated for each required depth, while the system chooses the appropriate clip to be composited 
at play-back time. Expected neuro-imaging data will serve as objective measures of believability of animated 
characters in relation to communicating empathic responses. Such experiments showcase whether common 
perceptual and neuroscientific mechanisms are triggered by stereo-rendered synthetic characters of varied 
realism. Figure 1: Scene displayed in the fMRI scanner GARAU, M., et. al (2003). The impact of avatar 
realism and eye gaze control on perceived quality of communication in a shared immersive virtual environment. 
In CHI. 2003: ACM Press. MILGRAM, S. (1963). "Behavioral Study of Obedience". Journal of Abnormal and 
Social Psychology 67: 371 378. VINAYAGAMOORTHY, V., et. al (2005). Building Characters: Lessons Drawn 
from Virtual Environments. Toward Social Mechanisms of Andriod Science: A CogSci Workshop 119-12 Copyright 
is held by the author / owner(s). SIGGRAPH 2010, Los Angeles, California, July 25 29, 2010. ISBN 978-1-4503-0210-4/10/0007 
 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1836863</article_id>
		<sort_key>180</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>17</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>17</seq_no>
		<title><![CDATA[SocioCrowd]]></title>
		<subtitle><![CDATA[a social-network-based framework for crowd simulation]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836845.1836863</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1836863</url>
		<abstract>
			<par><![CDATA[<p>The goal of crowd simulation is to produce potential collective behaviors by simulating the movement process of a number of characters or agents. Some famous models are proposed to simulate crowd, including social force (e.g. [Helbing 2000]), cellular automata (e.g. [Chenny 2004]), and rule-based models (e.g. [Reynolds 1987]). Others use physiological (e.g. locomotion, energy level) and psychological (e.g. impatience, personality attributes) traits of agents to trigger heterogeneous behaviors [Pelechano 2007]. However, existing approaches do not consider the real-world social interactions among agents, and thus are unable to produce social-dependent scenarios. In this work, we propose to leverage the underlying social network, which captures social relationships among agents, for crowd simulation. A novel social-network-based framework, SocioCrowd, is developed (figure 1(a)) shows the virtual world). Based on SocioCrowd, we simulate three social-based scenarios, including community-guided flocking, following leading persons, and spatio-social information spreading. They display certain real-world social behaviors which are hardly modeled by existing methods. To lift the performance, our SocioCrowd is implemented by pure Java with GPU programming in ways of GSGL and JCUDA.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>H.5.3</cat_node>
				<descriptor>Web-based interaction</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.6.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010341</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010868</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Web-based interaction</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2264270</person_id>
				<author_profile_id><![CDATA[81413602545]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Cheng-Te]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Li]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[National Taiwan University, Taipei, Taiwan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264271</person_id>
				<author_profile_id><![CDATA[81466643657]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Hsun-Ping]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hsieh]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[National Taiwan University, Taipei, Taiwan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264272</person_id>
				<author_profile_id><![CDATA[81430653174]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Tsung-Ting]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kuo]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[National Taiwan University, Taipei, Taiwan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264273</person_id>
				<author_profile_id><![CDATA[81430596514]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Shou-De]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[National Taiwan University, Taipei, Taiwan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1028553</ref_obj_id>
				<ref_obj_pid>1028523</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Chenney, S. 2004. Flow Tiles. ACM SIGGRAPH/Eurographics Symposium on Computer Animation 2004. 233--242.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Helbing, D., Farkas, J., and Vicsek, T. 2000. Simulating dynamical features of escape panic. Nature, 407, 487--490.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>956769</ref_obj_id>
				<ref_obj_pid>956750</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Kempe, D., Kleinberg, J., and Tardos E. 2003. Maximizing the spread of influence through a social network. ACM SIGKDD 2003, 137--146.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Newman, M. E. J. 2004. Fast algorithm for detecting community structure in networks. Physical Review, E 69, 066133.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1272705</ref_obj_id>
				<ref_obj_pid>1272690</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Pelechano, N., Allbeck, J. M., and Badler N. I. 2007. ACM SIGGRAPH/Eurographics Symposium on Computer Animation 2007, 99--108.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>37406</ref_obj_id>
				<ref_obj_pid>37401</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Reynolds, C. W. 1987. Flocks, herds and schools: A distributed behavior model. ACM SIGGRAPH 1987, 25--34.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Wasserman, S. and Faust, K. 1994. Social network analysis. Cambridge University Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 SocioCrowd: A Social-Network-Based Framework for Crowd Simulation Cheng-Te Li *  Hsun-Ping Hsieh Tsung-Ting 
Kuo Shou-De Lin Graduate Institute of Networking and Multimedia, National Taiwan University, Taipei, 
Taiwan  Figure 1. (a) Simulated result, (b) network view, (c) no community effect, (d) community flocking 
in 2D mode, (e) information spreading in 3D mode. 1. Introduction The goal of crowd simulation is to 
produce potential collective behaviors by simulating the movement process of a number of characters or 
agents. Some famous models are proposed to simulate crowd, including social force (e.g. [Helbing 2000]), 
cellular automata (e.g. [Chenny 2004]), and rule-based models (e.g. [Reynolds 1987]). Others use physiological 
(e.g. locomotion, energy level) and psychological (e.g. impatience, personality attributes) traits of 
agents to trigger heterogeneous behaviors [Pelechano 2007]. However, existing approaches do not consider 
the real-world social interactions among agents, and thus are unable to produce social­dependent scenarios. 
In this work, we propose to leverage the underlying social network, which captures social relationships 
among agents, for crowd simulation. A novel social-network-based framework, SocioCrowd, is developed 
(figure 1(a)) shows the virtual world). Based on SocioCrowd, we simulate three social­based scenarios, 
including community-guided flocking, following leading persons, and spatio-social information spreading. 
They display certain real-world social behaviors which are hardly modeled by existing methods. To lift 
the performance, our SocioCrowd is implemented by pure Java with GPU programming in ways of GSGL and 
JCUDA. 2. SocioCrowd Framework Our SocioCrowd framework consists of the following two parts. Basic Architecture. 
We employ a traditional flocking model [Reynolds 1987], which is composed of separation, alignment, and 
cohesion rules, as the basic architecture of SocioCrowd. We further introduce two probabilities to control 
the movement status of agents. One changes agent s status from walk to stop, the other from stop to walk. 
In default setting, agents walk and stop randomly. Social Network Construction. To exploit the social 
relationships in SocioCrowd, we collect real data from DBLP1, which is an online database for Computer 
Science Bibliography. We construct the co­authorship social network, in which each node (i.e., agent) 
stands for an author and each link represents co-authorship between two authors. An example social subgraph 
around the author Ming Ouhyoung is shown in Figure 1(b). 3. Social-based Simulation Based on SocioCrowd, 
we propose to simulate three scenarios: Community-Guided Flocking. In a social network, a community can 
be regarded as a tightly intra-connected and loosely inter­connected subgraph. We use the Fast Newman 
algorithm [Newman 2004] for community detection. For agents in diverse communities, we draw different 
colors on both network view and 2D mode. Besides, to integrate communities within simulation, we then 
design two probabilities, pcand p-c, to control the potential an agent talks to others of the same community 
and of different communities respectively. The simulating results are shown in Figure 1(c), where pc=1 
and p-c=0. It can be observed that agents belong to the same communities tend to flock. For figure 1(d) 
where pc=0.5 and p-c=1, agents belong to different communities have higher chance to flock. Note that 
only partial snapshots are created due to space limit. Following Leading Persons. A general society usually 
has leaders who are chased by some followers. Our SocioCrowd simulates this kind of scenario by identifying 
the central individuals as the leading ones in a social network. Three centrality measures (i.e., degree, 
closeness, and eigenvector) [Wasserman 1994] are provided to find central individuals from different 
viewpoints. During simulation, we use a probability pl which allows an individual to follow a leader 
given the leader appears in the visible region. A higher probability implies the leader attracts more 
followers. Spatio-social Information Spreading. Here we would like to simulate how information is propagated 
from a single agent to agents around the environment. In conventional crowd simulation, agents spread 
messages only to others who are close one another in space. In real-world cases, however, people do not 
necessary communicate with spatially-closed individuals but rather socially­closed ones. With underlying 
social network, individuals are capable of interacting with socially-close ones. Our SocioCrowd combines 
spatial and social clues to perform such information spreading. We adopt the linear threshold model [Kempe 
2003] as the spreading strategy, where an agent is influenced if the summation of the influence probabilities 
of its spatial and social neighbors is larger than a given threshold. A snapshot of influenced agents 
drawn in red cylinders is shown in Figure 1(e). 4. Conclusion and Discussion We propose SocioCrowd leveraging 
real social networks to provide potential interactions among agents for crowd simulation. We also present 
three social-based simulations that can be further integrated to explore more social-based emerging behaviors. 
Our experiments show that real-world social-based behaviors can be generated through turning up the designed 
parameters.  References Chenney, S. 2004. Flow Tiles. ACM SIGGRAPH/ Eurographics Symposium on Computer 
Animation 2004. 233-242. Helbing, D., Farkas, J., and Vicsek, T. 2000. Simulating dynamical features 
of escape panic. Nature, 407, 487-490. Kempe, D., Kleinberg, J., and Tardos E. 2003. Maximizing the 
spread of influence through a social network. ACM SIGKDD 2003, 137-146. Newman, M.E.J. 2004. Fast algorithm 
for detecting community structure in networks. Physical Review, E 69, 066133. Pelechano, N., Allbeck, 
J.M., and Badler N.I. 2007. ACM SIGGRAPH/ Eurographics Symposium on Computer Animation 2007, 99-108. 
Reynolds, C.W. 1987. Flocks, herds and schools: A distributed behavior model. ACM SIGGRAPH 1987, 25-34. 
 Wasserman, S. and Faust, K. 1994. Social network analysis. Cambridge University Press. Copyright is 
held by the author / owner(s). * E-mail: d98944005@csie.ntu.edu.tw SIGGRAPH 2010, Los Angeles, California, 
July 25 29, 2010. ISBN 978-1-4503-0210-4/10/0007 1 DBLP: http://www.informatik.uni-trier.de/~ley/db/ 
 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1836864</article_id>
		<sort_key>190</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>18</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>18</seq_no>
		<title><![CDATA[Spinnability simulation of viscoelastic fluid]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836845.1836864</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1836864</url>
		<abstract>
			<par><![CDATA[<p>One of the most challenging issues of computer graphics is to represent the behavior of fluid. Visualizing the fluid behavior requires to solve Navier-Stokes equations, which take huge amount of time so that some researches use many super computers for the simulation, and others utilize the GPU performance. The common fluid is Newtonian that can be described by a single constant value of viscosity, and there are many researches related to Newtonian. On the other hand, there is another type of fluid called non-Newtonian that cannot be described easily, and one of non-Newtonians is viscoelactic fluid. Viscoelastic fluid has the characteristics of both viscosity of fluid and elasticity of solid, and it is difficult to represent the behavior of viscoelastic fluid. [Goktekin et al. 2004] represented the behavior of viscoelastic fluid. His technique is based on Eulerian methods and added elastic terms to Navier-stokes equations, which govern fluid behavior. [Clavet et al. 2005] used particle method for representing fluid behavior. Particle method can represent fine behavior of the fluid such as rain drops, fountains, clay manipulation. Their researches could visualize many types of behavior of viscoelastic fluid, however, they cannot represent the spinnability, which has three characteristics: 1) it stretches very thin as if it is a string, 2) the radius is getting smaller gradually from the both ends and the center part has the least radius, and 3) it shrinks rapidly as if it is a rubber.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.6.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010341</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2264274</person_id>
				<author_profile_id><![CDATA[81474656107]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Nobuhiko]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Mukai]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tokyo City University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264275</person_id>
				<author_profile_id><![CDATA[81466645275]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Kentaro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ito]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tokyo City University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264276</person_id>
				<author_profile_id><![CDATA[81542382456]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Masashi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nakagawa]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tokyo City University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264277</person_id>
				<author_profile_id><![CDATA[81540193556]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Makoto]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kosugi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tokyo City University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1073400</ref_obj_id>
				<ref_obj_pid>1073368</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Clavet, S., Beaudoin, P., and Poulin, P. 2005. Particle-based viscoelastic fluid simulation. <i>Proceedings of the 2005 ACM SIGGRAPH/Eurographics Symposium on Computer Animation</i>, 219--228.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1015746</ref_obj_id>
				<ref_obj_pid>1186562</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Goktekin, T. G., Bargteil, A. W., and O'Brien, J. F. 2004. A method for animating viscoelastic fluids. <i>Proceedings of the 2004 SIGGRAPH</i>, 463--468.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Spinnability Simulation of Viscoelastic Fluid Nobuhiko Mukai ., Kentaro Ito, Masashi Nakagawa, and Makoto 
Kosugi Tokyo City University Figure 1: A sequence of images showing the spinnability property of viscoelastic 
.uid. (a) The initial state, where the middle part is viscoelastic .uid and the both end are solid. (b) 
The .uid is being lifted as the top solid is moving. (c) The .uid is stretched just before its string 
is broken. (d) The state just after the string is broken. 1 Introduction One of the most challenging 
issues of computer graphics is to rep­resent the behavior of .uid. Visualizing the .uid behavior requires 
to solve Navier-Stokes equations, which take huge amount of time so that some researches use many super 
computers for the simula­tion, and others utilize the GPU performance. The common .uid is Newtonian that 
can be described by a single constant value of vis­cosity, and there are many researches related to Newtonian. 
On the other hand, there is another type of .uid called non-Newtonian that cannot be described easily, 
and one of non-Newtonians is viscoelac­tic .uid. Viscoelastic .uid has the characteristics of both viscosity 
of .uid and elasticity of solid, and it is dif.cult to represent the be­havior of viscoelastic .uid. 
[Goktekin et al. 2004] represented the behavior of viscoelastic .uid. His technique is based on Eulerian 
methods and added elastic terms to Navier-stokes equations, which govern .uid behavior. [Clavet et al. 
2005] used particle method for representing .uid behavior. Particle method can represent .ne behavior 
of the .uid such as rain drops, fountains, clay manipula­tion. Their researches could visualize many 
types of behavior of viscoelastic .uid, however, they cannot represent the spinnability, which has three 
characteristics: 1) it stretches very thin as if it is a string, 2) the radius is getting smaller gradually 
from the both ends and the center part has the least radius, and 3) it shrinks rapidly as if it is a 
rubber. In this paper, we describe a method of animating the spinnabil­ity behavior of viscoelastic .uid. 
The method calculates Cauchy s equations of motion, and constitutive equations of viscoelastic .uid and 
solvent by using MPS (Moving Particle Semi-implicit) method as the governing equation of motion. In addition, 
surface tension is integrated for representing the property of spinnability. 2 Method The method uses 
the following equations. Equation of continuity: dp = (1) dt .e-mail: mukai@cs.tcu.ac.jp Cauchy s equations 
of motion with surface tension: dv dt= 1 p(+G+ 1 p"1Æn(= pI+' Constitutive equations of viscoelastic 
.uid and solvent: (2) (3) '='s+'p (4) Giesekus model: 's= TsD (5) 'p+ 'p +a Tp 'p 'p=TpD 'p= d'p dt 
'p 'p (6)(7) = v D= 1 + (8)  where, pis density, tis time, vis velocity, (is stress tensor, G is 
gravity, "is curvature, 1is surface tension coef.cient, Æis delta function, nis normal vector of surface, 
pis pressure, Iis unit ma­trix, 'sis solvent stress, 'pis viscoelastic stress, Tsis solvent vis­cosity, 
Dis velocity tensor by deformation, is relaxation time, a is constant, and Tpis viscoelastic viscosity. 
Finally, we could simulate the spinnability of viscoelastic .uid with the proposed method. Analysis time 
by MPS was 155 ms per step, polygon generation time by marching cube method was 280 ms, and rendering 
time with OpenGL was 16 ms for the model composed of 2,744 .uid particles and 5,007 solid particles. 
 References CLAVET,S.,BEAUDOIN,P., AND POULIN, P. 2005. Particle­based viscoelastic .uid simulation. 
Proceedings of the 2005 ACM SIGGRAPH/Eurographics Symposium on Computer Ani­mation, 219 228. GOKTEKIN,T.G., 
BARGTEIL,A. W., AND O BRIEN, J. F. 2004. A method for animating viscoelastic .uids. Proceedings of the 
2004 SIGGRAPH, 463 468.  Copyright is held by the author / owner(s). SIGGRAPH 2010, Los Angeles, California, 
July 25 29, 2010. ISBN 978-1-4503-0210-4/10/0007  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1836865</article_id>
		<sort_key>200</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>19</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>19</seq_no>
		<title><![CDATA[Warping the space around an animated object]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836845.1836865</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1836865</url>
		<abstract>
			<par><![CDATA[<p>The creation of animation clips and the tweaking of existing character animation is often a tedious, time-consuming task, especially in large-scale CG productions. Traditionally these tasks were achieved by animators manually changing multiple keyframe values for all the relevant animation rig controls. Starting with the idea that a single animated object (e.g. a rig control) essentially defines a time-varying curve in 3D space - where the control points are defined by the keyframe values of the translation channels - we introduce a modeling approach for deforming these conceptual 3D curves. We will talk about our current implementation based on FFDs (Free Form Deformations) as described in [Sederberg and Parry 1986], but we strongly believe the same approach can have many other usages (e.g. modeling tools, collisions and obstacle avoidance).</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Animation</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2264278</person_id>
				<author_profile_id><![CDATA[81466648661]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Daniele]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Federico]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Dr. D. Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264279</person_id>
				<author_profile_id><![CDATA[81466647316]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Damien]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Fagnou]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Moving Picture Company]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264280</person_id>
				<author_profile_id><![CDATA[81466642831]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Tom]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Reed]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Moving Picture Company]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>15903</ref_obj_id>
				<ref_obj_pid>15886</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Sederberg, T. W., and Parry, S. R. 1986. Free-form deformation of solid geometric models. <i>SIGGRAPH Comput. Graph. 20</i>, 4, 151--160.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>218422</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Witkin, A., and Popovi&#263;, Z. 1995. Motion warping. <i>Computer Graphics 29</i>, Annual Conference Series, 105--108.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Warping the space around an animated object Daniele Federico* Damien Fagnou Tom Reed Dr.D Studios Moving 
Picture Company Moving Picture Company  Figure 1: (a) A warped animation cycle. (b) A cycle deformed 
with bezier (green) and linear (red) interpolations. 1 Introduction The creation of animation clips 
and the tweaking of existing character animation is often a tedious, time-consuming task, especially 
in large-scale CG productions. Traditionally these tasks were achieved by animators manually changing 
multiple keyframe values for all the relevant animation rig controls. Starting with the idea that a single 
animated object (e.g. a rig control) essentially de.nes a time-varying curve in 3D space -where the control 
points are de.ned by the keyframe values of the trans­lation channels -we introduce a modeling approach 
for deforming these conceptual 3D curves. We will talk about our current implementation based on FFDs 
(Free Form Deformations) as described in [Sederberg and Parry 1986], but we strongly believe the same 
approach can have many other usages (e.g. modeling tools, collisions and obstacle avoidance). 2 The 
space warping approach Motion warping was demonstrated in [Witkin and Popovi´c 1995], where the authors 
describe an approach for warping the motion of an animated character starting with a pre-existing cycle 
and providing new animator-de.ned key poses as inputs. Their algorithm then modi.es rotation and position 
values (when needed) for the skeleton joints in order to pass through the keyframe poses. The approach 
we propose is based on applying operators (e.g. deformers) directly on the 3D space in which our animated 
objects are moving. This leads to a variation of the paths and orientations of the objects over time. 
In order to .nd new translation values for each object we map positions from the untouched 3D space to 
the deformed one. In the same way, the axes of the transformation matrices are deformed and then orthogonalized 
before performing any conversion to Euler angles or quaternions. In our system we have so far chosen 
not to consider any scaling factors. At MPC as a proof of concept, our initial implementation entailed 
applying FFDs on existing animation clips in order to create new motions. These resulting motions were 
then used both in ALICE (MPC s crowd engine) and directly in the animators Maya sessions. The tool can 
be used on one or more characters at the same time with a minimal impact on the scene s performance. 
The lattice for the evaluated character is generated by the tool itself in order to completely encompass 
the controls for the entire input animation. If an object is evaluated outside of the lattices boundaries 
un­predictable results may occur. After the .rst tests we found that for modifying the animation for 
a character, it is *e-mail: df@danielefederico.it e-mail:damien-f@moving-picture.com e-mail:tom-r@moving-picture.com 
enough to apply FFDs only to the controls which move in world space, such as IK or global controls. However 
our tool also works well for .nding new local values for par­ented transforms. In fact our tool always 
generates a new orthogonalized world matrix, which in this case must be multiplied by the inverse of 
the parent matrix; therefore, this approach can be also used with FK controls. Our algorithm works with 
both bezier and linear interpolations. The former is used for a smoother result, while the latter is 
employed whenever the user needs more control over the character. An undesired effect may occur whenever 
the lattice cells get too stretched, in fact this can lead to the typical foot-skate effect. Our tool 
cannot .x this situation; what we have done instead is to provide users with some visual warnings to 
indicate when the cells shrink or become too large. In this way undesired skating feet can be pre­vented. 
Once the animator is happy with the achieved result, a baking process is run. The baking process attempts 
to change only the values of existing key frames to align the original animation with the deformed one, 
but will fall back to baking entire channels when necessary. We provide other utilities for simplifying 
the baked animation curves to our animators, so they won t have the heavy task of cleaning the result. 
This approach has been proved to be very powerful for two reasons: 1. its evaluation is never dependent 
on the next or previous con.gurations for the considered object. This leads to a solution which depends 
only on the current input matrix and the shape of the lattice. 2. it provides local control over motion. 
Each CV of the lattice can be animated and affect only certain zones of our characters at a certain time. 
 We have used this tool in production on both primary and secondary characters achiev­ing very satisfactory 
results. Speci.cally, our implementation has been proved to be very suitable for modifying basic animation 
cycles into blocking and rough animations for hero characters. Whereas animators would previously spend 
a day or more gener­ating new cycles, we have observed them achieving the same result in only two hours 
using this tool. Thus far, we have found this solution to be ef.cient and fairly computationally inex­pensive. 
Looking forward, we believe this approach could achieve its best usability and performance in conjunction 
with a good animation layer system. References SEDERBERG, T. W., AND PARRY, S. R. 1986. Free-form deformation 
of solid geo­metric models. SIGGRAPH Comput. Graph. 20, 4, 151 160. WITKIN, A., AND POPOVI C´, Z. 1995. 
Motion warping. Computer Graphics 29, Annual Conference Series, 105 108. Copyright is held by the author 
/ owner(s). SIGGRAPH 2010, Los Angeles, California, July 25 29, 2010. ISBN 978-1-4503-0210-4/10/0007 
  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	</section>
	<section>
		<section_id>1836866</section_id>
		<sort_key>210</sort_key>
		<section_seq_no>2</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Art]]></section_title>
		<section_page_from>20</section_page_from>
	<article_rec>
		<article_id>1836867</article_id>
		<sort_key>220</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>20</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Artwork evolution]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836845.1836867</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1836867</url>
		<abstract>
			<par><![CDATA[<p>Creating digital artwork requires a lot of time, talent, and effort from artists and programmers. It takes artists hours to design pleasing artwork and programmers even more time as they develop and debug complex graphics shaders. One way to aid in the creation of complex art is to use evolutionary computing called genetic programming. Genetic programming can be used to create mathematical expressions that can be rendered as an image. The image can be used as a texture in a 3D scene or as a starting point for additional artwork.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>J.5</cat_node>
				<descriptor>Fine arts</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.8</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010469.10010471</concept_id>
				<concept_desc>CCS->Applied computing->Arts and humanities->Performing arts</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010469.10010470</concept_id>
				<concept_desc>CCS->Applied computing->Arts and humanities->Fine arts</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010507</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Graphics recognition and interpretation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2264281</person_id>
				<author_profile_id><![CDATA[81466647245]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Paul]]></first_name>
				<middle_name><![CDATA[D.]]></middle_name>
				<last_name><![CDATA[Solt]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Rochester Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Ebert, D. S., Musgrave, F. K., Peachey, D., Perlin, K., and Worley, S. 2003. <i>Texturing and Modeling: A Procedural Approach</i>, third ed. Morgan Kaufmann Publishers Inc., San Francisco, CA, USA, ch. 19, 20, 546--615.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>122752</ref_obj_id>
				<ref_obj_pid>122718</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Sims, K. 1991. Artificial evolution for computer graphics. In <i>SIGGRAPH '91: Proceedings of the 18th annual conference on Computer graphics and interactive techniques</i>, ACM, New York, NY, USA, 319--328.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Artwork Evolution Figure 1: Evolved artwork 1 Introduction and Motivation Creating digital artwork 
requires a lot of time, talent, and effortfrom artists and programmers. It takes artists hours to design 
pleas­ing artwork and programmers even more time as they develop anddebug complex graphics shaders. One 
way to aid in the creationof complex art is to use evolutionary computing called genetic pro­gramming. 
Genetic programming can be used to create mathemati­cal expressions that can be rendered as an image. 
The image can beused as a texture in a 3D scene or as a starting point for additionalartwork. Over the 
past two decades there have been numerous experimentsusinggeneticprogramming.Sims [Sims1991]introducedthetopicto 
the graphics community and explored computer generated art.His work was developed on a CM-2 supercomputer 
using a vari­ant of Lisp. At the time, computing power was expensive and veryfew people had access to 
supercomputers. The limitations due tohardware constraints meant that average people were not exposedto 
computer generated art. Musgrave [Ebert et al. 2003] demon­strated that advances in computing had made 
genetic art accessibleon personal computers. Genetic textures and genetic programmingwere used to create 
planets with plants, landscapes, and skies. This poster presents an application that enables the creation 
of digi­talartwork on cell phones and other mobile devices. The power thatwas once only available in 
a supercomputer is now inside a hand­held device that .ts in a pocket. Users can view, create, and shareartwork 
without any artistic or technical skill. Each person gen­erates a unique genetic pool of images and can 
incorporate geneticartworkfromfriends andstrangers toevolveadditional images. Theability to share artwork 
enables a social network where people cancollaborate to create artwork using the touch of a .nger. 2 
Approach There are several different starting points for evolution. Both per­sonal photos and evolved 
images can be used to create new images.A user can take a picture of a tree using a mobile device camera 
andevolve the photo. Or a user can select a set of previously evolvedimagesand photostouseasparentsfortheevolutionprocess. 
Theseimages canbe chosen fromtheusersfavoritecollections, friendcol­lections, or top rated collections 
from around the world. Any im­age can be used as a starting point for evolution and allow users toevolve 
new artwork at any point in time. For example, two different *e-mail: pds2352@rit.edu users could generate 
images and then share the evolved images be­tween the mobile devices to create complex images that the 
worldhas never seen. The ability to casually evolve, share, and collab­orate on creating artwork will 
provide a unique experience in thecreation of genetic art that has not been done before. 3 Implementation 
and Future Work The application is built on a cross-platform library and runs on theiPhoneandiPad. Allofthecomputation 
isexecuted onthedevice tocreateimagesinnearrealtime. Inthecurrentdesign allcalculationsare performed 
on the CPU, which can render an image within twoto .fteen seconds. A future improvement will be to transfer 
thebulk of the calculations to the GPU where it can be processed inparallel. Additional image processing 
techniques can be added tothe system in order to create more complex and visually stimulatingartwork. 
Lastly, images created on a device could be shared onsocial networks in order to increase exposure to 
genetic art. Figure 2: Artwork Evolution application on the iPad  References EBERT,D. S., MUSGRAVE,F. 
K., PEACHEY,D., PERLIN,K., AND WORLEY, S. 2003. Texturing and Modeling: A Procedu­ral Approach, third 
ed. Morgan Kaufmann Publishers Inc., SanFrancisco, CA, USA, ch. 19, 20, 546 615. SIMS, K. 1991. Arti.cial 
evolution for computer graphics. InSIGGRAPH 91: Proceedings of the 18th annual conference onComputergraphics 
andinteractivetechniques, ACM,NewYork, NY, USA, 319 328. Copyright is held by the author / owner(s). 
SIGGRAPH 2010, Los Angeles, California, July 25 29, 2010. ISBN 978-1-4503-0210-4/10/0007  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1836868</article_id>
		<sort_key>230</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>21</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Breathe brush]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836845.1836868</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1836868</url>
		<abstract>
			<par><![CDATA[<p>This paper is about the study on an artwork, a black-and white drawing that has been expressed through a digital algorithm. Black-white drawings were popular during the Chosun era (1392--1910) reigned by kings and officials. The Oriental fine art, pursuing harmony with nature, is expressed in a moderate and restrained way, hence anyone would find it very soft and thus readily acceptable. Unlike the western paintings that fill the canvus to the very full, the oriental paintings treat even the blank space as a part making up a balanced painting. This artwork features Daegum, the decent traditional musical instrument which used to be played in loyal palaces or guest rooms of prestigious officials' residences, and a bamboo which was a frequent motive of gentlemen's paintings in the past. Daegum and the bamboo, expressed in a modern style in this work, make people appreciate the life that is full and rich. So, one can say they have been used here to make this "well-being art."</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.4</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2264282</person_id>
				<author_profile_id><![CDATA[81436596513]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Young-Mi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kim]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Chung-Ang University, Seoul, Korea]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264283</person_id>
				<author_profile_id><![CDATA[81539974456]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Jong-Soo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Choi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Chung-Ang University, Seoul, Korea]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Breathe Brush Young-Mi Kim Graduate School of Advanced Imaging Science, Multimedia, and Film, Chung-Ang 
University, Seoul, Korea Jong-Soo Choi Graduate School of Advanced Imaging Science, Multimedia, and Film, 
Chung-Ang University, Seoul, Korea  ABSTRACT This paper is about the study on an artwork, a black-and 
white drawing that has been expressed through a digital algorithm. Black-white drawings were popular 
during the Chosun era (1392 1910) reigned by kings and officials. The Oriental fine art, pursuing harmony 
with nature, is expressed in a moderate and restrained way, hence anyone would find it very soft and 
thus readily acceptable. Unlike the western paintings that fill the canvus to the very full, the oriental 
paintings treat even the blank space as a part making up a balanced painting. This artwork features Daegum, 
the decent traditional musical instrument which used to be played in loyal palaces or guest rooms of 
prestigious officials residences, and a bamboo which was a frequent motive of gentlemen s paintings in 
the past. Daegum and the bamboo, expressed in a modern style in this work, make people appreciate the 
life that is full and rich. So, one can say they have been used here to make this well-being art. 1. 
Introduction   email: frontier@imagelab.cau.ac.kr email: jschoi@cau.ac.kr Based on the concept of 
this work, the bamboo was painted according to the strength and the length of the breath of a Daegum 
player. Of the four gracious plants, this work took the bamboo as its motive. Bending little by little 
but not being broken to-tally, the bamboo was selected as it also keeps its leaves green throughout the 
four seasons, empty inside and pointing up to the heaven, which resembles the ancient gentlemen s spirit. 
The bamboo forests, being full of nature s spirit and believed to cleanse body and mind, were considered 
good places for meditation in leisure by Oriental people like Koreans, Chinese and Japanese. In the era 
of digitalization, how-ever, bamboos seemed to stay just a traditional symbol. As people seek well-being 
in the modern culture, black-and-white paintings and calligraphies have been gradually received and now 
are being established as a culture on their own. The very tool that draws the bamboo is also made of 
bamboo and is called Daegum. In other words, the painting is not drawn on a paper using a brush but drawn 
by Daegum play, that is, according to the strength and the length of the breath of a Daegum player. The 
sound of Daegum is clear and deep, just like the spirit of the upright, in-corruptible gentlemen who 
cared only about their nation and the king. Daegum is one of the representative musical instruments that 
were played in palaces during Chosun Dynasty. Literary men learned how to write or paint, and also, how 
to play Daegum. It was taken as it was believed to cultivate good minds, just like the four gracious 
plants. In the Oriental culture, if a person plays a musical instrument, another person would paint a 
picture according to what he feels as he listens to the music and tries to figure out the music player 
s thoughts and then they would talk about it together. In this work, the Daegum player is the audience 
and the computer paints the picture in accordance with the music played. 2. Design development  Being 
a traditional musical instrument, Daegum is played out in a sitting posture so that the player can calm 
himself/herself down. Just like a flute, it is blown from aside. Chwigoo (the position where lips are 
placed on) should not be covered completely by lips but only by the lower lip so that the breath can 
be blown into it. A bamboo is hollow inside and therefore the sound resonates from within and comes out 
of Chwi-goo. So, the ultra tiny microphone is installed inside Chwigoo, so that the computer receives 
and recognizes the strength and length of the breath of a player, and then classifies them into 15 different 
sounds. The strength of the breath determines the shading of the ink and the length of the breath determines 
the length of the bamboo s knot. 3. Conclusions   Having been played by high-ranking gentlemen in Chosun 
Dynasty, Daegum, these days, has still a limited access only by those who major in Korean traditional 
music. There were many trials and errors due to the limited experience with Daegum. Without given musical 
notes, the artist s intention had to be understood. In order to arrive at the image and the shape intended 
for the work, the concept of the software had to be understood, which is also necessary to comprehend 
i.e. social policies and cultural attempts that are realized through software. 4. Acknowledgements  
 This work was supported by Korean Research Foundation under BK21 project, Seoul R&#38;BD Program (TR080601), 
Seoul Future Contents Convergence (SFCC) Cluster established by Seoul R&#38;BD Program (10570).  Figure 
1. A bamboo painting drawn by Breath.  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<article_sponsors>
			<funding_agency>Korean Research Foundation under BK21 project, Seoul R&#38;BD Program</funding_agency>
			<grant_numbers>
				<grant_number>TR080601</grant_number>
			</grant_numbers>
		</article_sponsors>
		<article_sponsors>
			<funding_agency>Seoul Future Contents Convergence (SFCC) Cluster established by Seoul R&#38;BD Program</funding_agency>
			<grant_numbers>
				<grant_number>10570</grant_number>
			</grant_numbers>
		</article_sponsors>
	</article_rec>
	<article_rec>
		<article_id>1836869</article_id>
		<sort_key>240</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>22</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Digital writing ductus]]></title>
		<subtitle><![CDATA[a visual representation of individual writing styles]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836845.1836869</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1836869</url>
		<abstract>
			<par><![CDATA[<p>I present a custom software for typing experiences that opposed to linear word processing renders visible individual writing styles on a personal computer using responsive typography in order to achieve a unique and personal representation of text analogous to handwriting.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[design]]></kw>
			<kw><![CDATA[digital ductus]]></kw>
			<kw><![CDATA[dynamic typography]]></kw>
			<kw><![CDATA[human computer interface]]></kw>
			<kw><![CDATA[interaction design]]></kw>
			<kw><![CDATA[writing styles]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>H.5.2</cat_node>
				<descriptor>Interaction styles (e.g., commands, menus, forms, direct manipulation)</descriptor>
				<type>S</type>
			</primary_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003120.10003121.10003124</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2264284</person_id>
				<author_profile_id><![CDATA[81466643746]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Daniel]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tauber]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Free University of Bolzano]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Hugh Aldersey-Williams, Peter Hall, T. S. P. A. 2008. <i>Design and the Elastic Mind.</i> The Museum of Modern Art, New York.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Maeda, J. 2004. <i>Creative code: Aesthetics + Computation.</i> Thames and Hudson.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1296181</ref_obj_id>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Reas, C., and Fry, B. 2007. <i>Processing. A Programming Handbook for Visual Designers and Artists.</i> The MIT Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Digital writing ductus: A visual representation of individual writing styles Daniel Tauber* Free University 
of Bolzano Abstract I present a custom software for typing experiences that opposed to linear word processing 
renders visible individual writing styles on a personal computer using responsive typography in order 
to achieve a unique and personal representation of text analogous to handwriting. CR Categories: H.5.2 
[User Interfaces]: Interaction styles ; Keywords: Design, Interaction design, Human computer inter­face, 
Dynamic typography, Digital ductus, writing styles 1 Introduction Any common computer keyboard is modeled 
after the typewriter keyboard. All key presses are reported to the controlling software, for example 
a word processor. However coinciding advantages are rarely used for design/artistic purposes. The automated 
typeset­ting process is on the personal computer is still based on traditional (static) conventions of 
typewriting. I created a custom software for typing experiences, that opposed to linear word processing, 
explores the unique capabilities of digitality for aesthetic explorations [Maeda 2004]. I designed a 
system that is subject to individual user input, hence creating an unique piece of text that is inspired 
by the qualities of handwriting. A text written by hand contains along the linguistic semantic level 
a second level of meta information including traces of personality, the individual style of writing and 
even the persons moods. In the past years text is written in large part using a keyboard on a personal 
computer producing linear, optimized and standardized text. In the modern digital world opportunities 
to experiment and play with type infor­mally as one would in other contexts are lacking. 2 My Approach 
Common changes to the appearance of text are of user generated nature and conventional manner. These 
are basically the typeface used and font size plus a selected font weight. These are static, pre­selected 
by the user. An exception is the typeface Beowolf by Just von Rossum and Erik van Blokland. Beowolf is 
created with a ran­domization routine. Using postscript each letter is printed unique. The random nature 
of the process however does not take individ­ual input into account. [Reas and Fry 2007] My project applies 
direct and real-time feedback through interactive typography soft­ware. New writing and reading experiences 
are created while every single letter is subject to change. Visual premises important for my ambition 
where: to preserve legi­bility to an acceptable degree. To obtain a clear visual impact and to use familiar 
metaphors for the modi.cations. I selected base cate­gories for activities and operations that are unique 
to digital writing. These are Deleting, Rhythm and Keystroke. Deleting means the ability to erase what 
has been written. Deleted letters change to grey but stay visible in the background. *e-mail: info@danieltauber.com 
Rhythm is the typing sequence and progression rate produced by all key presses including pauses. Rhythm 
is shown through modi.­cation of tracking, leading and spacing between letters, words and sentences. 
Keystroke is de.ned as an approximated force applied by the user to press a singular key. Implementations 
for Keystroke where: text­size, a shift of the baseline, a second shadow offset and a change in fontweight 
regulated by the force applied. Technical assumptions de.ned for my project are: to use a common keyboard 
and personal computer (In the test scenarios a Apple Mac-Book Pro was deployed). The software has to 
run operating system independent to achieve maximum distribution. Possibility to export vector output 
to use for scale independent typographic design. The conceptual brainchild for this project was to render 
design more human. That means including space for imperfections and rough edges [Hugh Aldersey-Williams 
2008]. Technological possibilities are to often just seen from a functionalist human-centered design 
perspective. [2008] 3 Conclusion and Future Work I have introduced a new system of individual and personal 
repre­sentation of text using responsive typography. My project investi­gates the possibilities of a 
digital produced one-time existence test. It seems suitable for a variety of applications, longing from 
type writing training to a toolkit for sociological surveys and artistic ty­pographic design. It draws 
on an individual and personal repre­sentation of text that has rarely explored as a means of responsive 
typography before. In future work i will study in depth how differ­ent factors and modi.cations in.uence 
the perception of text and conduct user study.  Acknowledgements Degree project by Daniel Tauber for 
the exam session 2009, 09.2 at the Faculty of Design and Art of the Free University of Bolzano. Supervisor: 
Antonino Benincasa, Second supervisor: Matteo Maria Moretti References HUGH ALDERSEY-WILLIAMS, PETER 
HALL, T. S. P. A. 2008. Design and the Elastic Mind. The Museum of Modern Art, New York. MAEDA, J. 2004. 
Creative code: Aesthetics + Computation. Thames and Hudson. REAS, C., AND FRY, B. 2007. Processing. A 
Programming Hand­book for Visual Designers and Artists. The MIT Press. Copyright is held by the author 
/ owner(s). SIGGRAPH 2010, Los Angeles, California, July 25 29, 2010. ISBN 978-1-4503-0210-4/10/0007 
  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1836870</article_id>
		<sort_key>250</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>23</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[Ink-and-wash painting oriental cymbidium drawn with the tip of the fingers]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836845.1836870</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1836870</url>
		<abstract>
			<par><![CDATA[<p>During the old days in the orient, people used to wipe cymbidium leaves or painted cymbidium for mental training by having a cymbidium always by their side. Through the act of wiping cymbidium leaves with utmost care, a cymbidium instilled with ancient philosophical ideas is visualized, and just as God breathed life into human nostrils and created a living life form, if a breath is breathed into a cymbidium flower, a cymbidium flower with an excellent fragrance is visualized. This work is an interactive visualization of an oriental cymbidium using modern technology which our oriental ancestors painted for mental training.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>J.5</cat_node>
				<descriptor>Fine arts</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.8</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010469.10010471</concept_id>
				<concept_desc>CCS->Applied computing->Arts and humanities->Performing arts</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010469.10010470</concept_id>
				<concept_desc>CCS->Applied computing->Arts and humanities->Fine arts</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010507</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Graphics recognition and interpretation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2264285</person_id>
				<author_profile_id><![CDATA[81436596513]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Young-Mi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kim]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Chung-Ang University, Seoul, Korea]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264286</person_id>
				<author_profile_id><![CDATA[81539974456]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Jong-Soo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Choi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Chung-Ang University, Seoul, Korea]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Ink-and-Wash Painting Oriental Cymbidium Drawn with the Tip of the Fingers Young-Mi Kim Graduate School 
of Advanced Imaging Science, Multimedia, and Film, Chung-Ang University, Seoul, Korea Jong-Soo Choi Graduate 
School of Advanced Imaging Science, Multimedia, and Film, Chung-Ang University, Seoul, Korea  ABSTRACT 
During the old days in the orient, people used to wipe cymbidium leaves or painted cymbidium for mental 
training by having a cymbidium always by their side. Through the act of wiping cymbidium leaves with 
utmost care, a cymbidium instilled with ancient philosophical ideas is visualized, and just as God breathed 
life into human nostrils and created a living life form, if a breath is breathed into a cymbidium flower, 
a cymbidium flower with an excellent fragrance is visualized. This work is an interactive visualization 
of an oriental cymbidium using modern technology which our oriental ancestors painted for mental training. 
 1. Introduction   email: frontier@imagelab.cau.ac.kr email: jschoi@cau.ac.kr However, entering the 
brilliant digital era, it is a fact that traditional values and culture are treated as invaluable and 
neglected. Therefore, there is a need to active utilize and fuse modern media as a means of succeeding 
and fostering our tradition. The purpose of this thesis is to make the original meaning of the ink-and-wash 
painting, the most aesthetic oriental painting, stand out as well as naturally blending it into the lives 
of modern people by having a different form of expression. It is the intention of this paper to study 
the potential of expressing ink-and-wash painting through interaction, and present a direction that can 
coincide with modern paintings by developing ink-and-wash painting from a traditional aspect through 
analyzing the theories and techniques instilled in my works. 2. Ink-and-Wash Painting  Asians has been 
living with cymbidium as if it is a close friend, and an oriental cymbidium is always placed in noble 
places. The act of wiping each cymbidium leaves with human hands as if sweeping it off is not to maintain 
cleanliness but rather to uphold the spirit instilled within the cymbidium. In other words, if one's 
heart is troubled and there is a lot on one s mind, mental training was carried out with a careful act 
of wiping cymbidium leaves with a calm mind by having a cymbidium close to oneself. This act has great 
meaning as it plays a role of interacting for drawing cymbidium painting. While the meaning of a cymbidium 
painting is also similar to the act of wiping a cymbidium, it is instilled with more meaning. The artist 
always conducts close observation of the subject before painting it. However, the oriental method of 
sketch is not depicting the subject while looking at it in detail but making a sketch by memory apart 
from the subject. Oriental drawing technique is expanding the artist s creative space while depending 
on visual memory and shape memory by making the most use of one s subjectivity and imagination breaking 
away from the restraint of the real subject. What is important is drawing with creativity yet keeping 
several rules. The curves and bold and think lines of a cymbidium must be painted, and its leaves must 
shake in the wind and have bones. In this work, cymbidium leaves are visualized divided up into leaves 
curved according to the angle of wiping up or down and cymbidium leaves blowing in the wind. 3. Design 
Production  When the angle of the curve communicated through the cymbidium leaf is big, the cymbidium 
leaf in the painting is drawn as curved leaves or leaves blowing in the wind. A total of 6~16 cymbidium 
leaves are drawn up, and if one breathes air into a flower after drawing a cymbidium leaf, a floral axis 
and flower is drawn. It takes in the strength of the breath and when it is strong, a full blossomed flower 
is visualized and when it is weak, a flower with a peak is visualized. When a lot of cymbidium leaves 
are drawn, only one flower is drawn, and when relatively less leaves are drawn, 2~3 flowers are drawn 
up. When the interactive work is finished, a caption or a poem that goes well with the style of the painting 
is written in the remaining blank space, and a red seal is stamped. When the interactive work is finished, 
a caption or a poem that goes well with the style of the painting is written in the remaining blank space, 
and a red seal is stamped on the painting. All of these methods communicate that the painting was produced 
in the same way cymbidium paintings were painted in the old days. 4. Conclusions  This is an interactive 
well-being design made so children can engage in mental training physically and mentally through this 
artwork and naturally have an interest in the Ethnographic aspect instilled in the work. Ink-and-wash 
painting representing oriental art is a collection of a diverse range of art that can be enjoyed along 
with the beauty of poems, calligraphies, and seals within the painting. This is an artwork for examining 
the spirituality and plastic perception of oriental cymbidium, finding and succeeding a modern way of 
expression which was completed by achieving mutual harmony between analog contents and digital technology. 
This is an interactive art that can complete the artwork only through the active action of the viewer 
and not a work which can be viewed one-sidedly by achieving a consensus between the ancestors and modern 
people. 5. ACKNOWLEDGMENTS  This work was supported by Korean Research Foundation under BK21 project, 
Seoul R&#38;BD Program(TR080601), Seoul Future Contents Convergence (SFCC) Cluster established by Seoul 
R&#38;BD Program(10570).  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<article_sponsors>
			<funding_agency>Korean Research Foundation under BK21 project, Seoul R&#38;BD Program</funding_agency>
			<grant_numbers>
				<grant_number>TR080601</grant_number>
			</grant_numbers>
		</article_sponsors>
		<article_sponsors>
			<funding_agency>Seoul Future Contents Convergence (SFCC) Cluster established by Seoul R&#38;BD Program</funding_agency>
			<grant_numbers>
				<grant_number>10570</grant_number>
			</grant_numbers>
		</article_sponsors>
	</article_rec>
	<article_rec>
		<article_id>1836871</article_id>
		<sort_key>260</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>24</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>5</seq_no>
		<title><![CDATA[GPS comics]]></title>
		<subtitle><![CDATA[<i>seeing thru walls</i>]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836845.1836871</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1836871</url>
		<abstract>
			<par><![CDATA[<p>In the digital era, the comics medium is transported from print to computer screen. Current digital comics (web comics or online comics) are confined to computer screen and use the affordances of digital medium in a limited way. GPS Comics: <i>Seeing thru Walls</i> is a GPS based comics story that expands the comic canvas and explores the idea of location-based comics. In <i>Seeing thru Walls</i>, in order to receive the meaning in a comic frame the player must experience a sensory detail (a smell, sound, breeze or an object) in her surroundings in the physical world. The concept of location-based comics is an unexplored idea and gives artists new meaning making strategies.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2264287</person_id>
				<author_profile_id><![CDATA[81331503053]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[&#214;zge]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Samanci]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of California, Berkeley]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264288</person_id>
				<author_profile_id><![CDATA[81328490418]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Anuj]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tewari]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of California, Berkeley]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[
McCloud, S. 2006. <i>Reinventing Comics.</i> New York: HarperCollins Books, 222.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1836872</article_id>
		<sort_key>270</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>25</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>6</seq_no>
		<title><![CDATA[Musophobia]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836845.1836872</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1836872</url>
		<abstract>
			<par><![CDATA[<p>Graphic artists have a wide variety of applications to use for digital painting. Although each application has its own solution to enhance the user experience, most of them rely on the same standard feature; a single brush, which is completely dependent on user input for location. Although this is required for a fully controlled painting process, making small changes on this feature yields unpredictable results. My proposal for an alternate brush paradigm is using multiple brushes (as seen in the application "PD Particles"), which are not completely under control but rather moving within trajectories with random deviations, simultaneously. The trajectories are defined by controllable parameters and the user input. Since the rate of obedience to user input is dependant on the parameters, users can define the rate of deviation and thus switch between finger painting and generative painting, without changing the set of tools.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.4</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.8</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010507</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Graphics recognition and interpretation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2264289</person_id>
				<author_profile_id><![CDATA[81466648124]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Cem]]></first_name>
				<middle_name><![CDATA[Sina]]></middle_name>
				<last_name><![CDATA[Cetin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1199347</ref_obj_id>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Dave Shreiner, Mason Woo, J. N., and Davis, T. 1995. <i>OpenGL(R) Programming Guide: The Official Guide to Learning OpenGL.</i> Addison-Wesley Professional.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Ritchie, D., 2006. Pd particles, August. http://www.thebest3d.com/pdp/.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Musophobia Cem Sina Cetin* 1 Introduction Graphic artists have a wide variety of applications to use 
for dig­ital painting. Although each application has its own solution to enhance the user experience, 
most of them rely on the same stan­dard feature; a single brush, which is completely dependent on user 
input for location. Although this is required for a fully controlled painting process, making small changes 
on this feature yields un­predictable results. My proposal for an alternate brush paradigm is using multiple 
brushes (as seen in the application PD Particles ), which are not completely under control but rather 
moving within trajectories with random deviations, simultaneously. The trajecto­ries are de.ned by controllable 
parameters and the user input. Since the rate of obedience to user input is dependant on the parameters, 
users can de.ne the rate of deviation and thus switch between .nger painting and generative painting, 
without changing the set of tools. 2 Exposition Musophobia is a multi-touch painting application, where 
the user controls multiple brushes simultaneously. These brushes are in the form of .oating particles 
with .xed rest locations. Particles are al­ways visible on the canvas, but they do not paint unless the 
painting is selected. Particles always respond to user interaction, whether or not the painting mode 
is on. When the interaction ends, particles return to their rest locations. The application runs in several 
states, which changes the interaction behavior of the particles. There are 5 modes of interaction, which 
are namely blob , spin , orbit , fall and the default, rest state. The rest state shows the particles 
at their .xed locations. When the user touches the screen, nearby particles escape from the point of 
touch. In blob state, the particles cluster around the point of touch at a .xed location relative to 
the touch position. Cumulatively, par­ticles act as a single brush in this mode. In spin state, particles 
also cluster around the point of touch, but they revolve around the ori­gin (i.e. the touch location) 
at random distances and speeds. Orbit state causes the particles to travel on circles, around random 
cen­ters, radii and speeds. Fall state dislodges particles from their cur­rent locations and let them 
fall in the direction of gravity. States of interaction work mutually exclusively. When two of these 
states are active, one of them overrides the other. In addition to these mutually exclusive interaction 
states, there are three separate visualizations. Triad visualization creates triangles by using 3 nearest 
points in the rest state, Arrow replaces particles with speed and direction sensitive arrowheads and 
Hide state makes the points invisible. The combinations of these two sets of interaction and visualization 
de.ne the resulting behavior. Each particle is attributed with a number of parameters. These pa­rameters 
are randomly generated during the .rst initialization of the canvas and used for animation traits like 
speed during spin state or distance from center at blob state. New position of a point is calcu­lated 
relative to the location on the previous frame, so that the con­tinuity of the animation is not compromised. 
Since the motions are coherent in consecutive frames, the particles are guaranteed to paint continuous, 
.uent paths, no matter at what frequency the interaction states are changed. While the paths are continuous 
for each unique particle, the combined outcome forms harmonious patterns. Fol­ *e-mail: i.am@cemsinacetin.com 
lowing image shows the path of 10 and 999 particle respectively, during transition from blob state to 
orbit state. Figure 1: Trajectories during blob to orbit transition Although the interaction variables 
are generated at initialization, the user is granted the ability to tweak these values to control the 
rate of random behaviour and basic appearance options. Change of appear­ance involves the particle amount, 
size and color options, where the user can either select prede.ned colors from pickers, or force par­ticles 
to continuously change their hue (the rainbow mode). The remaining options, such as the spin radius, 
or blob radius, help the user to de.ne the boundaries for the particles to appear within spe­ci.c states. 
For instance, the lower the spin radius is, the denser the particles will be in a de.ned area, which, 
in other words, forces them to collapse into a smaller area and act more like a brush. In addition, the 
auto hide option makes the particles invisible if there are no touch events, during spin and blob modes. 
Using auto hide, forces the particles to behave like a regular .nger painting; touching and dragging 
the .nger makes the particles visible, while moving the .nger away stops painting again. 3 Conclusion 
Replacing the single brush painting with a system of particles with parameterized, random behaviors makes 
it possible to create gener­ative images and .nger painting within the same canvas, by using the same 
tools. Although such system makes it more dif.cult to practice classical painting methods, it offers 
ef.cient and easy ways to build up generative patterns to include in an artistic work. Musophobia is 
developed for and currently available on Apple s iPhone and iPad. Musophobia has over 5,000 users globally 
and the promising results of this adaptation show that, with a larger touch screen device and thus a 
canvas, features of Musophobia will be easier to use and will make it possible to create more complex, 
more detailed works. References DAVE SHREINER, MASON WOO, J. N., AND DAVIS, T. 1995. OpenGL(R) Programming 
Guide: The Of.cial Guide to Learn­ing OpenGL. Addison-Wesley Professional. RITCHIE, D., 2006. Pd particles, 
August. http://www.thebest3d.com/pdp/. Copyright is held by the author / owner(s). SIGGRAPH 2010, Los 
Angeles, California, July 25 29, 2010. ISBN 978-1-4503-0210-4/10/0007  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1836873</article_id>
		<sort_key>280</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>26</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>7</seq_no>
		<title><![CDATA[Photochromic canvas drawing with patterned light]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836845.1836873</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1836873</url>
		<abstract>
			<par><![CDATA[<p>Drawing tools using digital technology can stimulate creativity in people. For example, the Wacky Brush tool in KidPix can produce effects (such as a line of dripping paint or a line of shapes) that cannot be obtained using ordinary paper and brushes [Hickman 1991]. This feature makes it easy for people to draw pictures having a combination of patterns. Such software, however, has so far been used only with electronic displays such as LCDs and PDPs. In this paper, we propose a mechanism that would allow the user to draw such pictures while using paper as a canvas instead of electronic displays. With this mechanism, a variety of patterns can be made to appear along lines traced out by the user by moving an electronic paint brush over paper. The advantages of using paper in this way include a high degree of freedom in shape and size as well as portability.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.6</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2264290</person_id>
				<author_profile_id><![CDATA[81100411762]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Tomoko]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hashida]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264291</person_id>
				<author_profile_id><![CDATA[81100527572]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Yasuaki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kakehi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264292</person_id>
				<author_profile_id><![CDATA[81100095332]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Takeshi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Naemura]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Kid Pix#8482; 1991. Software by Craig Hickman. Novato, CA: Broaderbund Software 1991.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Irie, M. and Mohri, M. 1988. Thermally irreversible photochromic systems. Reversible photocyclization of diarylethene derivatives <i>The Journal of Organic Chemistry</i>, 53(4), 803--808.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1501853</ref_obj_id>
				<ref_obj_pid>1501750</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Osada, A., Takeshita, S., Miyahara, M. and Inakage M. 2008. KAMI CHAT In <i>Proceedings of Advances in Computer Entertainment Technology</i>, 403.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Photochromic Canvas:Drawing with Patterned Light Tomoko Hashida* Yasuaki Kakehi Takeshi Naemura The 
University of Tokyo Keio University The University of Tokyo Figure 1: Writing with light Figure 2: Effects 
of patterned light Figure 3: Effects of periodic light 1 Introduction Drawing tools using digital technology 
can stimulate creativity in people. For example, the Wacky Brush tool in KidPix can produce effects (such 
as a line of dripping paint or a line of shapes) that can­not be obtained using ordinary paper and brushes 
[Hickman 1991]. This feature makes it easy for people to draw pictures having a com­bination of patterns. 
Such software, however, has so far been used only with electronic displays such as LCDs and PDPs. In 
this pa­per, we propose a mechanism that would allow the user to draw such pictures while using paper 
as a canvas instead of electronic displays.With this mechanism, a variety of patterns can be made to 
appear along lines traced out by the user by moving an electronic paint brush over paper. The advantages 
of using paper in this way include a high degree of freedom in shape and size as well as porta­bility. 
 2 Photochromic Canvas To achieve the above objectives, we focused our attention on pho­tochromic material 
in which the appearance and disappearance of color can be controlled by light. In this regard, KAMI CHAT 
has been proposed as a means of controlling the generation of color by placing paper coated with photochromic 
material over an array of LEDs [Osada et al.2008].We here adopt photochromic mate­rial that can maintain 
a color-forming state and propose a system that does not place limitations on where the paper is placed. 
In this system, moving a handheld projector like a paint brush over paper coated with this photochromic 
material results in a reaction between the material and the projected light and a change in pa­per color. 
Effects like those of Wacky Brush can be obtained by projecting patterned light from the handheld projector. 
We call our system Photochromic Canvas. The photochromic material used here is diarylethene (DAE-BT, 
YAMADA CHEMICAL CO., LTD.)[Irie et al.1988]. Shining ultra­violet light on DAE-BT changes it color to 
pink and shining visible light on DAE-BT makes it transparent. Thus, if we coat white paper with DAE-BT 
and expose it to ultraviolet light to make it pink, we can move a handheld projector connected to a personal 
computer over the paper like a paint brush to make the DAE-BT transparent *e-mail: hashida@nae-lab.org 
e-mail: ykakehi@sfc.keio.ac.jp e-mail: naemura@nae-lab.org and reveal the white paper underneath according 
to the pattern of projected visible light. This process makes for many possibilities. For example, the 
user can create a line combining different motifs even when tracing out one simple line through the synergetic 
effect achieved by changing the rhythm or speed of the drawing hand and the pattern of the projected 
light over time. 3 Experimental Results Experimental results are shown in Figure1.These results show 
that the color of the paper changes due to a reaction with the light emit­ted from the projector. The 
results of using patterned light are shown in Figure2. Here, the shape of a T-shirt and the outline of 
a building were prepared as preliminary sketches and the projected pattern (star of certain size/orientation, 
human silhouette, etc.) was changed every second. Thus, by moving the projector once a second, the user 
can draw patterns as if using a stamp. The results of periodically projecting simple patterns from the 
pro­jector are shown in Figure3. The letters shown here consist of cir­cles of the same diameter formed 
by switching between ON and OFF every second and circles that alternate between two diameter values. 
Thus, while the user need only draw simple characters, the end result is characters with embellishments. 
 4 Conclusion We proposed and implemented a photochromic canvas. This sys­tem, in principle, requires 
no detection of projector position or pa­per alignment and allows the user to draw as desired anywhere 
on paper making it intuitively easy to use. In future research, we plan to expand photochromic material 
to real-world-oriented interfaces. Acknowledgements This research has been supported by the Kayamori 
Foundation of Informational Science Advancement. References PIXTM KID 1991. Software by Craig Hickman. 
Novato, CA:Broaderbund Software 1991. IRIE, M.AND MOHRI, M. 1988. Thermally irreversible pho­tochromic 
systems. Reversible photocyclization of diarylethene derivatives The Journal of Organic Chemistry, 53(4), 
803-808. OSADA, A.,TAKESHITA, S.,MIYAHARA, M. AND INAKAGE M. 2008. KAMI CHAT In Proceedings of Advances 
in Computer Entertainment Technology, 403. Copyright is held by the author / owner(s). SIGGRAPH 2010, 
Los Angeles, California, July 25 29, 2010. ISBN 978-1-4503-0210-4/10/0007  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1836874</article_id>
		<sort_key>290</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>27</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>8</seq_no>
		<title><![CDATA[Self-assembled art]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836845.1836874</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1836874</url>
		<abstract>
			<par><![CDATA[<p>Code (computer software and the technologies that it enables) is changing fundamentally how human beings interact with each other, and think about themselves and the world. It is a medium through which artists are increasingly expressing themselves. Code can serve as the tool which the artist uses to produce their work, or more interestingly, the artist takes the role of programmer and designs and implements an algorithm that generates the work of art. Thus, the artist's ideas are filtered and constrained through the filter of code, whose limitations and capabilities shape and inform the consequent artistic vision. For example, Casey Reas through his {Software} Structures takes verbal descriptions of processes to produce visual components and turns them into programs[Reas 2009]. In what follows, a Turing-universal model of some natural and manmade phenomena, Self-Assembly, is adapted to the automatic creation of visual art.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[algorithmic art]]></kw>
			<kw><![CDATA[simulation]]></kw>
			<kw><![CDATA[visualization]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>J.5</cat_node>
				<descriptor>Fine arts</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.8</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010469.10010471</concept_id>
				<concept_desc>CCS->Applied computing->Arts and humanities->Performing arts</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010469.10010470</concept_id>
				<concept_desc>CCS->Applied computing->Arts and humanities->Fine arts</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010507</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Graphics recognition and interpretation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2264293</person_id>
				<author_profile_id><![CDATA[81100126621]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Russell]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Deaton]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Arkansas]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[John, K., and Bar, M. 2005. Alternative mechanisms of structuring biomembranes: Self-assembly versus self-organization. <i>Phys. Rev. Lett. 95</i>, 198101.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Reas, C., 2009. Software{Structures}. http://www.whitney.org/arport/commissions/softwarestructures/.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Winfree, E., Liu, F., Wenzler, L. A., and Seeman, N. C. 1998. Design and self-assembly of two-dimensional DNA crystals. <i>Nature 394</i>, 539--544.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Self-Assembled Art Russell Deaton* Computer Science and Computer Engineering University of Arkansas 
 (a) Self-Assembly (b) Simulation of Self-Assembly (c) Visualization of Self-Assembly Keywords: algorithmic 
art, simulation, visualization 1 Introduction Code (computer software and the technologies that it enables) 
is changing fundamentally how human beings interact with each other, and think about themselves and the 
world. It is a medium through which artists are increasingly expressing themselves. Code can serve as 
the tool which the artist uses to produce their work, or more interestingly, the artist takes the role 
of programmer and designs and implements an algorithm that generates the work of art. Thus, the artist 
s ideas are .ltered and constrained through the .lter of code, whose limitations and capabilities shape 
and inform the consequent artistic vision. For example, Casey Reas through his {Software} Structures 
takes verbal descriptions of processes to pro­duce visual components and turns them into programs[Reas 
2009]. In what follows, a Turing-universal model of some natural and man­made phenomena, Self-Assembly, 
is adapted to the automatic cre­ation of visual art. 2 Self-Assembly Self-assembly is a process of construction 
through speci.c, local­ized interactions among component parts that result in organized structures[John 
and Bar 2005]. Self-assembly is capable of pro­ducing complex structure and function (e.g. ribosomes), 
and seems to be ubiquitous in the world around us. Self-assembly frequently is mentioned as a potential 
method for nanoscale manufacturing, and is capable of universal computation[Winfree et al. 1998]. Self­assembly 
can be modeled as a collection of agents that interact with each other through very speci.c, localized 
interactions. Other sys­tems, from physical to theoretical, share this fundamental character. In these 
systems, there is some potential for programming the in­teractions to achieve a structure with some usefulness, 
application, or aesthetic appeal, be it through learning, evolution, or design (Fig­ure (a)). Agents 
are labeled with P = {1, 2, 3, 4, 5, 6, 7}, and in­teractions between agents with K = {a, b, c, d, e, 
f}. Compatible *e-mail: rdeaton@uark.edu interactions, i.e. the labels match, form edges between particles 
producing a graph. 3 Self-Assembled Art To investigate the ability of Self-assembly to produce visual 
works, a simulation was programmed in the Processing system. A simula­tion of the system is show in Figure 
(b). Agents move about a two­dimensional .eld through random Brownian motion. When they encounter other 
agents, and if they have compatible interactions, the agents bind together. To produce the visualization, 
different types of interactions were programmed to render different types of visual elements, for instance, 
trace the line between agents, draw an arc from the center of the line between agents to the 90. axis, 
or draw a circle at the center of the line connecting agents. This type of visualization produced the 
image in Figure (c). 4 Conclusion Self-assembly is a computational model for a variety of man-made and 
natural systems. By choosing agents and interactions, the artist can program a framework for a particular 
work while the ultimate product remains nondeterministic. Thus, self-assembly is a con­structor for not 
only man-made and natural structure, but also po­tentially for art. References JOHN, K., AND BAR, M. 
2005. Alternative mechanisms of struc­turing biomembranes: Self-assembly versus self-organization. Phys. 
Rev. Lett. 95, 198101. REAS, C., 2009. Software{Structures}. http: //www.whitney.org/arport/commissions/ 
softwarestructures/. WINFREE, E., LIU, F., WENZLER, L. A., AND SEEMAN, N. C. 1998. Design and self-assembly 
of two-dimensional DNA crys­tals. Nature 394, 539 544. Copyright is held by the author / owner(s). SIGGRAPH 
2010, Los Angeles, California, July 25 29, 2010. ISBN 978-1-4503-0210-4/10/0007  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1836875</article_id>
		<sort_key>300</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>28</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>9</seq_no>
		<title><![CDATA[Shaboned display]]></title>
		<subtitle><![CDATA[an interactive substantial display using soap bubbles]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836845.1836875</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1836875</url>
		<abstract>
			<par><![CDATA[<p>From childhood, we often play with bubbles. We find various aesthetic elements in a series of actions of soap bubbles: appearing, expanding, floating, bursting and disappearing. This time, we utilize the movements of soap bubbles as a pixel of an image and propose a novel interactive substantial display named "Shaboned Display." (see Figure 1)</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.4.0</cat_node>
				<descriptor>Image displays</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2264294</person_id>
				<author_profile_id><![CDATA[81331494719]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Shiho]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hirayama]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264295</person_id>
				<author_profile_id><![CDATA[81100527572]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Yasuaki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kakehi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1179137</ref_obj_id>
				<ref_obj_pid>1179133</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Nakamura, M., Inaba, G., Tamaoki, J., Shiratori, K., and Hoshino, J. 2006. Bubble cosmos. In <i>SIGGRAPH Emerging Technologies</i>, ACM, No. 3.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1400971</ref_obj_id>
				<ref_obj_pid>1400885</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Suzuki, R., Suzuki, T., Ariga, S., Iida, M., and Arakawa, C. 2008. "ephemeral melody": music played with wind and bubbles. In <i>SIGGRAPH Posters</i>, ACM, No. 80.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Shaboned Display: An Interactive Substantial Display Using Soap Bubbles Shiho Hirayama* Yasuaki Kakehi 
Keio University Keio University Figure 1: Shaboned Display 1 Introduction From childhood, we often play 
withbubbles.We .ndvarious aes­thetic elementsinaseriesof actionsof soapbubbles: appearing,ex­panding, 
.oating,bursting and disappearing. This time, we utilize themovementsofsoapbubblesasapixelofanimageand 
propose a novel interactive substantial display named Shaboned Display. (see Figure 1) Mainly in the 
.eld of media art, many artists and designers have also used bubbles as a tool for expression. However, 
in most of the previous systems,bubbles .oat freely and randomlyin air.For example, in Bubble Cosmos 
[Nakamura et al. 2006] , participants can interact with projection images by breaking .oating bubbles 
with white smoke. ephemeral melody [Suzuki et al. 2008] also usebubbles as an interface. In this piece, 
musics are created ac­cordingtothe collisionsofbubbles .oating randomly.Onthe other hand, our Shaboned 
Display can show images by controlling the sizeofeachsoapbubble arrangedina matrixinaplane.Asforthe substantial 
displays, various types of material have also applied to construct visual images. One of the features 
of such display sys­tems is that each pixel has a physical form and we can touch them freely. In our 
system, we can use each soapbubble not only asa pixel of a display but also an input tool. By exploding 
the soap bubbles, the displayed images change interactively.  2 Design of Shaboned Display In our Shaboned 
Display,We offer three innovative points as fol­lows. Firstly, this system can show images with soap 
bubbles arranged in a matrix in a plane. Figure 2 shows the system design of the Shaboned Display.Inthe 
current implementation,10x10air tubes are arranged in a plane. Underneath tubes, air pumps are attached. 
By controlling the volume and timing of air .ow, it can manipulate the size and shape of each soapbubble 
freely. Note that this sys­temdoesnotallowthesebubblesto.oatinairanditalso controls contracting of bubbles. 
By keeping bubbles expanding and con­tracting, this display can present various images such as characters 
or .gures. *e-mail: hirayama@sfc.keio.ac.jp e-mail: ykakehi@sfc.keio.ac.jp Figure 2: System Design Secondly,wedevelopeda 
systemfor creatingasoapbubble auto­matically. By using this function, even if some soap bubbles are broken 
by winds or users .ngers, this system can make the .lm rapidly again andkeep showing images. In addition, 
this system can also break the soapbubbles intentionally. Conversely, this dis­play can show imagesby 
popping someof soapbubbles. Thirdly, this display can work as an interactive system. More con­cretely, 
this system can detect a explosion of .lm by using an elec­trical approach.By attaching electrodesonthesurfaceofsoapbub­bleandtheedgeoftheairvent,thissystemcan 
detecttheexplosion event by sensing the ohmic value of the circuit. Furthermore, this system can also 
detect users actions for example hand gestures by using a camera. 3 Applications and Future Works We 
have already developed the prototype system with 10 x 10 air vents and implemented some applications 
using this display. Firstly, this display works as an ambient information board. Of course, the image 
displayed on it is affected from environmental factors such as winds. Audiences can observe the digital 
informa­tion and analog phenomena simultaneously. Secondly, we devel­oped an interactive application 
on this system. In this application, theeventsofbubblebursting are enhanced.Forexample, when one soapbubbleisexplodedby 
winds or users .ngers,bubbles around it are alsobursted sequentially like ripples in conjunction with 
the event. In other application, this system generates audio feedback whenexplosions happen. Thus, audiences 
can enjoythe intentional of unintentional phenomena through these various effects. In the future, we 
are going to implement this system in various scales and settings such as on a vertical surface. In addition, 
we alsoplanto propose more interactionsusingthesoapbubblepixels. References NAKAMURA, M., INABA, G., 
TAMAOKI, J., SHIRATORI, K., AND HOSHINO, J. 2006. Bubble cosmos. In SIGGRAPH Emerging Technologies,ACM, 
No. 3. SUZUKI,R.,SUZUKI, T.,ARIGA, S.,IIDA,M., AND ARAKAWA, C. 2008. ephemeral melody : music played 
with wind and bubbles. InSIGGRAPH Posters,ACM, No. 80. Copyright is held by the author / owner(s). SIGGRAPH 
2010, Los Angeles, California, July 25 29, 2010. ISBN 978-1-4503-0210-4/10/0007  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1836876</article_id>
		<sort_key>310</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>29</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>10</seq_no>
		<title><![CDATA[SMA motion display]]></title>
		<subtitle><![CDATA[<i>plant</i>]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836845.1836876</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1836876</url>
		<abstract>
			<par><![CDATA[<p>Recently there has been demand for display equipment capable of advanced expressions in spatial design. For example, there is the Adobe Interactive Wall at Union Square (New York City, 2007), and the Zero Energy Media Wall of greenPIX (Beijing, 2008) using LEDs placed on the whole facade. The simple display of information contents is becoming insufficient, and more appealing spatial designs combining information content with interactive art expression are becoming more important. In this paper, we propose a shape memory alloy motion display (SMD), a novel piece of display equipment taking advantage of the existence of an actual object. Then, we introduce an interactive art work <i>plant</i> based on SMD technology.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.4.0</cat_node>
				<descriptor>Image displays</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2264296</person_id>
				<author_profile_id><![CDATA[81442619290]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Akira]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nakayasu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Kyushu University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264297</person_id>
				<author_profile_id><![CDATA[81331505568]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Kiyoshi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tomimatsu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Kyushu University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[
Hirose, M. Digital Public Art Supported by Advanced Media Technology. In Journal of Information Processing Society of Japan, IPSJ (2007), Vol48, No12, pp1335--1342.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>674740</ref_obj_id>
				<ref_obj_pid>645968</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[
Wisneski, C., Ishii, H., Dahley, A., Gorbet, M., Brave, S., Ullmer, B. and Yarin, P. Ambient Displays: Turning Architectural Space into an Interface between People and Digital Information. In Proceedings of the International Workshop on Cooperative Buildings (CoBuild '98), Springer (1998), pp22--32.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1665191</ref_obj_id>
				<ref_obj_pid>1665137</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[
Nakayasu, A. and Tomimatsu, K. Himawari plant robot: creature expression using shape-memory-alloy actuator crowd robots. In the Emerging Technologies of ACM SIGGRAPH ASIA 2009, ACM Press (2009).
]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 SMA Motion Display: plant Akira NakayasuKiyoshi TomimatsuGraduate School of Design, ADCDUFaculty of 
DesignKyushu UniversityKyushu Universityinfo@ander.jp tomimatsu@design.kyushu-u.ac.jp 1. Introduction 
Recently there has been demand for display equipment capable of advanced expressions in spatial design. 
For example, there is the Adobe Interactive Wall at Union Square (New York City, 2007), and the Zero 
Energy Media Wall of greenPIX (Beijing, 2008) using LEDs placed on the whole facade. The simple display 
of informa­tion contents is becoming insufficient, and more appealing spatial designs combining information 
content with interactive art expres­sion are becoming more important. In this paper, we propose a shape 
memory alloy motion display (SMD), a novel piece of display equipment taking advantage of the existence 
of an actual object. Then, we introduce an interactive art work plant based on SMD technology. 2. What's 
SMA Motion Display (SMD)? SMD does not display images with a combination of changes in light as in visual 
displays, instead the elements corresponding to light dots in visual displays are replaced by objects 
(actuators). We do not aim to display high density information as is done with images, but realize abstract 
expressions with physical movements and changes in shape. We are working on devices that give visual 
expressions and a feeling of creature-like existence from soft, creature-like movements. Applications 
include interactive walls (e.g. Fig. 1) as Digital Public Art [1], digital signage using visual expressions, 
Ambient Displays [2], and the moving dolls in theme parks combining 3D structure impossible with images 
and robotics. We built Himawari [3] as an application to robotics.  3. plant The plant (Fig. 2) is 
an interactive installation inspired by the vision of grass blowing in the wind. It is also created by 
using the core technology in SMD. 169 artificial leaves are controlled by using the shape memory alloy 
actuators we developed. All each leaf is independently controlled and reacts to a hand's movement and 
moves slowly. The sound also changes when reacting to a hand's movement. The foliage in the darkness 
creates a fantastic space. The plant provides users with the presence of plants and a comfortable interaction 
by the slow movements of leaves. The control system is composed of an infrared camera, a control program, 
an electric circuit, and shape memory alloy actuators. Images from the infrared camera above the plant 
are analyzed using the control program and are converted into control signals for the electric circuit. 
Control signals are converted into signals for pluse width modulation using the electric circuit, which 
is developed with a PIC microcontroller. The heat of each shape memory alloy actuator is controlled by 
controlling the 169ch voltage values, and 169 leaves are driven.  4. Conclusion and Future Work This 
paper has described the possibility of a SMD through making the interactive installation plant as a piece 
of art. However, slowness of response and number of actuators did not fully demon­strate the expressions. 
We believe a higher degree of interaction allows presentation of expressions that are easier to understand. 
We will resolve these issues in the future, and will develop devices with more expressional capabilities 
and reproducibility through fundamental experiments on SMDs. References [1] Hirose, M. Digital Public 
Art Supported by Advanced Media Technology. In Journal of Information Processing Society of Japan, IPSJ 
(2007), Vol48, No12, pp1335-1342. [2] Wisneski, C., Ishii, H., Dahley, A., Gorbet, M., Brave, S., Ullmer, 
B. and Yarin, P. Ambient Displays: Turning Architectural Space into an Interface between People and Digital 
Information. In Proceedings of the International Workshop on Cooperative Buildings (CoBuild '98), Springer 
(1998), pp22-32. [3] Nakayasu, A. and Tomimatsu, K. Himawari plant robot: creature expression using shape-memory-alloy 
actuator crowd robots. In the Emerging Technologies of ACM SIGGRAPH ASIA 2009, ACM Press (2009). Copyright 
is held by the author / owner(s). SIGGRAPH 2010, Los Angeles, California, July 25 29, 2010. ISBN 978-1-4503-0210-4/10/0007 
 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1836877</article_id>
		<sort_key>320</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>30</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>11</seq_no>
		<title><![CDATA[The online customer-built WEB3D middleware system for arts and crafts]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836845.1836877</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1836877</url>
		<abstract>
			<par><![CDATA[<p>Recently, Xiamen University and Flying Information Technology Co., Ltd worked together and completed the development of The Online Custom-Built WEB3D Middleware System for Arts and Crafts, which will perform as a product 3D design and display center, its main features include the product demonstration background change, 3-Dimension design, 3-Dimension product display, product component reorganization and product material replacement.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>H.5.3</cat_node>
				<descriptor>Web-based interaction</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>J.5</cat_node>
				<descriptor>Fine arts</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010469.10010470</concept_id>
				<concept_desc>CCS->Applied computing->Arts and humanities->Fine arts</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010868</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Web-based interaction</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010469.10010471</concept_id>
				<concept_desc>CCS->Applied computing->Arts and humanities->Performing arts</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2264298</person_id>
				<author_profile_id><![CDATA[81339538448]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Junfeng]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yao]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Xiamen University, China]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264299</person_id>
				<author_profile_id><![CDATA[81474681067]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Xiaobiao]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Xie]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Xiamen University, China]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264300</person_id>
				<author_profile_id><![CDATA[81466642604]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Fengchun]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Flying Information Technology Co., Ltd]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264301</person_id>
				<author_profile_id><![CDATA[81466646423]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Xufa]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ji]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Flying Information Technology Co., Ltd]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264302</person_id>
				<author_profile_id><![CDATA[81466644463]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Xiaoyan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Flying Information Technology Co., Ltd]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264303</person_id>
				<author_profile_id><![CDATA[81451592765]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Andy]]></first_name>
				<middle_name><![CDATA[Ju An]]></middle_name>
				<last_name><![CDATA[Wang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Southern Polytechnic State University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[
Chen, Qi; Yao, Junfeng, Research and design of 3D presentation system based on J2ME, <i>Journal of Information and Computational Science</i>, 2008, Vol5(1):p241--24.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 T h e O n l i n e C u s t o m e r ­ B u i l t W E B 3 D M i d d l e w a r e S y s t e m f o r A r t 
s a n d C r a f t s 1 1 2 2 2 3 J u n f e n g Y a o , X i a o b i a o X i e , F e n g c h u n L i n 
, X u f a J i , X i a o y a n L i n , A n d y J u A n W a n g 1 SoftWare School,Xiamen University, China 
2Flying Information Technology Co.,Ltd 3Southern Polytechnic State University, USA F i g . 1 . 3 ­Dimension 
craftwork customizing interface 1 I n t r o d u c t i o n Recently, Xiamen University and Flying Information 
Technology Co.,Ltd worked together and completed the development of The Online Custom­Built WEB3D Middleware 
System for Arts and Crafts, which will perform as a product 3D design and display center, its main features 
include the product demonstration background change, 3­Dimension design, 3­Dimension product display, 
product component reorganization and product material replacement. 2 D e s i g n P r i n c i p l e Platform 
adheres to customized design, focusing on the user's favors and interaction [Chen and Yao 2008], users 
can customize remotely to the manufacturers the needed commodity s parts, colors, and materials, etc, 
which has a unique business value. Users can even participate in their own product design in an effort 
to make product highlight their personality, bringing great creative fun. At present, the Internet industry 
is suffering from economic "storm" test, companies have shifted more attention to e­commerce and Internet 
marketing. Under the B2C e­commerce sales model, Flying Information Technology Co.,Ltd successfully provided 
a good e­commerce platform for traditional industries enterprises as a third­party company in Fujian 
Province, and this can gives enterprises more market opportunities, direct or indirect interests. Now, 
www.narkii.com, a 3­Dimension dynamic e­commerce platform, has been developed especially for the footwear, 
umbrellas, clothing, crafts, digital, mechanical and electrical products and many other categories of 
products. 3 O u r A p p r o a c h Step 1: Use Autodesk 3DS Max; Rhinoceros 3­Dimension authoring software 
to make handicrafts sample model; and in accordance with the composition of the sample texture segment 
model components. Step 2: According to the sample model material demand to render the various component 
parts in order to achieve a clear material texture. Step 3: Use the current popular 3­Dimension engine 
technology tool to let the model display in the form of 3­dimension web page. Step 4: Use the JAVA programming 
language to achieve 3D engine technology and web interfaces and data connections. The button on the interface 
web page can control the material and color of various component parts of the model, background change, 
and component parts replacement and so on by simple clicking. R e f e r e n c e s CHEN, QI; YAO, JUNFENG, 
Research and design of 3D presentation system based on J2ME, Journal of Information and Computational 
Science,2008,Vol5(1):p241­24. email: yao0010@xmu.edu.cn, jwang@spsu.edu Copyright is held by the author 
/ owner(s). SIGGRAPH 2010, Los Angeles, California, July 25 29, 2010. ISBN 978-1-4503-0210-4/10/0007 
 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1836878</article_id>
		<sort_key>330</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>31</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>12</seq_no>
		<title><![CDATA[The Orikao Method]]></title>
		<subtitle><![CDATA[3D scene reconstruction from Japanese beauty portraits]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836845.1836878</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1836878</url>
		<abstract>
			<par><![CDATA[<p>For more than 1,300 years, beauty portraits have continued to be painted in Japan and virtually all have been stylized to a very unrealistic extent.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.4.5</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245.10010254</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems->Reconstruction</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2264304</person_id>
				<author_profile_id><![CDATA[81435605129]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Yuka]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kubo]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264305</person_id>
				<author_profile_id><![CDATA[81538270056]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Hiroyuki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Shindo]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264306</person_id>
				<author_profile_id><![CDATA[81100511385]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Koichi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hirota]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 The Orikao Method: 3D Scene Reconstruction from Japanese Beauty Portraits Yuka Kubo Hiroyuki Shindo 
Koichi Hirota The University of Tokyo The University of Tokyo The University of Tokyo kubo@media.k.u-tokyo.ac.jp 
m086705@h.k.u-tokyo.ac.jp k-hirota@k.u-tokyo.ac.jp For more than 1,300 years, beauty portraits have 
continued to be painted in Japan and irt all all ha e been st li ed to to be painted in Japan and virtually 
all have been stylized to aa very unrealistic extent. This paper discusses a method for the transformation 
into a 3D scene that has the characteristics of stylized exaggeration found in Japanese beauty portraits. 
To develop this method, we quantitatively identified the characteristics of stylized exaggeration of 
facial features in Japanese beauty portraits and created a program for generating the facial image feature 
of the stylized exaggeration of various Japanese beauty pportraits from a volunteer s pphotoggrapph automaticallyy. 
This research will suggest a new and simple method for transformation of facial images based on Japanese 
art history and also make a contribution to the non-photorealistic techniques. Characteristics of Stylized 
Exaggeration in Japanese Beauty Portraits Analyzing 200 images of Japanese beauty portraits from 8th 
th ith tilf thh f century to the present with particular focus on theshape of the eyes and cheek, we 
understand that the history of stylized beauty portraits in Japan can be divided into four periods: In 
the 8th century (1st period), beautiful women drawn on murals and byobu screens are portrayed with narrow 
eyes and full cheeks. In the late 17th century (2nd period), ukiyo-e woodblock prints featured up-angled 
eyes, closer to the center of the face. In the 1900s (3rd period), beautiful women in illustrations began 
to feature drooping eyes positioned far (4th apart. the women in girls apart. InIn the 19501950ss (4 
period), period), beautiful beautiful women in girls comic books began to feature large eyes and small 
cheeks. Fig.1 Characteristics of stylized exaggeration  The Orikao Method for Reconstruction from Various 
Beauty Portraits The The OrikaoOrikao MethodMethod isis inspired inspired fromfrom Origami Origami uses 
Origami . Origami uses a "folding" method to turn a 2-dimensional single sheet of paper into a variety 
of 3-dimensional shapes. Similarly, the Orikao Method uses a "folding" approach to transform a basic 
human face drawn on 2-dimensional paper into a 3­dimensional scene reconstruction from four types of 
faces, each of which has the characteristics found in Japanese beauty portraits. The Orikao Method is 
as follows: In order to reproduce the characteristics of a beauty portrait from the 1st pperiod,, we 
need to fold alongg the foldingg lines as showninFig.2-.. Similarly, in order to reproduce the characteristics 
of a beauty portrait from the 2nd period, 3rd period, and 4th period, respectively, we need to fold along 
the folding lines as shown in Fig. 2-.,Fig.2-. and Fig. 2-.. To determine the folding angles, we derive 
the rotation matrix on the folding axis, that maps out the equivalent characteristic points in the original 
facial image to the characteristic points in the target beauty portrait images.  Fig.2 Process of the 
Orikao Method  The Orikao Method Program The process of the program is as follows: First, read in the 
original photograph and identify the characteristic points. Second, read in a beauty portrait you wish 
to use as the target image and identify its characteristic points. Then, the original photograph is folded 
along the folding lines by angles that are derived automatically from the target beauty portrait. This 
results in the generation of the face that has the characteristics of stylized exaggeration as found 
in Japanese beauty portraits.  Copyright is held by the author / owner(s). SIGGRAPH 2010, Los Angeles, 
California, July 25 29, 2010. ISBN 978-1-4503-0210-4/10/0007  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	</section>
	<section>
		<section_id>1836879</section_id>
		<sort_key>340</sort_key>
		<section_seq_no>3</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Design]]></section_title>
		<section_page_from>32</section_page_from>
	<article_rec>
		<article_id>1836880</article_id>
		<sort_key>350</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>32</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[24X7@PHL: Codify]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836845.1836880</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1836880</url>
		<abstract>
			<par><![CDATA[<p>"24X7ATPHL: Codify" is an investigation into the novel usage of time-based animation software and procedural modeling as a method for visualizing time-based quantitative data via the construction of a qualitative, two-dimensional rendering. Treated as an experiment in the extrusion and aggregation of time-based qualitative instances, "24X7ATPHL: Codify" slows down and composites the accumulated information of seven days traffic (customer pickup and drop off) at an international airport; visualizing information in such a way as to not only notate the generations and changes in patterns, but also to show the beauty that can be found in data while unlocking the emergent potential for design. "Codify" makes use of the accumulation of NURBS geometries as a methodology for understanding the specific conditions of movement created by the interaction of existing architecture and user, the results of which are currently being used to develop everything from the design of several furniture pieces to that of a new cladding system for the Philadelphia International Airport.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.8</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010507</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Graphics recognition and interpretation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2264307</person_id>
				<author_profile_id><![CDATA[81320495814]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Robert]]></first_name>
				<middle_name><![CDATA[B.]]></middle_name>
				<last_name><![CDATA[Trempe]]></last_name>
				<suffix><![CDATA[Jr]]></suffix>
				<affiliation><![CDATA[Temple University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 24X7@PHL: Codify Robert B Trempe Jr     Tyler School of Art Architecture Department, Temple University 
    trempe@temple.edu 1. Introduction 24X7ATPHL: Codify is an investigation into the novel usage 
of time-based animation software and procedural modeling as a meth­od for visualizing time-based quantitative 
data via the construction of a qualitative, two-dimensional rendering. Treated as an experi­ment in the 
extrusion and aggregation of time-based qualitative instances, 24X7ATPHL: Codify slows down and composites 
the accumulated information of seven days traffic (customer pickup and drop off) at an international 
airport; visualizing information in such a way as to not only notate the generations and changes in patterns, 
but also to show the beauty that can be found in data while unlocking the emergent potential for design. 
Codify makes use of the accumulation of NURBS geometries as a methodology for understanding the specific 
conditions of movement created by the interaction of existing architecture and user, the results of which 
are currently being used to develop everything from the design of several furniture pieces to that of 
a new cladding system for the Philadelphia International Airport. 2. Data Articulation and Animation 
The investigation began with the collection of basic user data from the Philadelphia International Airport, 
specifically the amounts of users being dropped off and picked up from the airport at sequential periods 
through a seven day cycle. Taken into account were issues of congestion, construction, bottlenecks, and 
other typical traffic-based disruptions. The collected data operated as the set of parameters for both 
the generation and manipulation of a POP Network in Houdini 9. The birth rate of the particle system 
was regulated by the basic user data with the birth rate adjusted according to time of day and number 
of users, key framing sampled periods of time at sequential intervals on the timeline. A NURBS sphere 
was constrained to the POP Network to give the particles a basic shape and volume, aiding both in their 
visualiza­tion (for tracking of movement) as well as for identifying shape deviations based on forces 
applied. Dynamics in the form of force POPs, attractor POPs, drag POPs, and spring SOPs were applied 
to the construction as methods of notating conditions of congestion and bottlenecking, construction, 
and other anomalies that effected the flow of traffic both in and out of the airport. Sequential frames 
of the completed simulation were then exported in IGES format using a geometry ROP for the purposes of 
render­ing and post-processing using a combination of Bunkspeed Hyper­shot and Adobe Photoshop. 3. Qualitative 
Reconstruction: Accumulation The sequential IGES files were imported into Rhinoceros and com­posited 
into one master file using a common reference point as a datum for the placement of each file, remembering 
that each IGES file notated one moment of time within the seven day sequence. The process of compositing 
accomplished the task of merging all of the different time segments into one time stream, a 2-D, time-lapse 
construction of the seven day cycle. In this vein we are no longer looking at individual frames but an 
aggregate, composited in such a way as to display a change of state over the seven day cycle. It is within 
this respect that the division of movement comes into play as the seemingly random determination of the 
number of sequences to be composited becomes more logical when the out­put is examined. The resultant 
information was then examined by importing the master file into Bunkspeed Hypershot and rendering the 
result out as a composite animation, an aggregate render that combines each sequential IGES file into 
one construction. 4. Emergent Patterning Several pattern types result from the analysis of the composite, 
each of which has countless potential towards varying types of architectural solutions from programmatic 
to formal gestures. In looking at the composite as a whole, we are clearly able to witness how the organization 
of elements within the whole shifts from start (at left) to finish (at right.) As all traffic moves from 
left to right within the analysis (and airport) we are able to determine where the moments of organized 
congestion, disorganized congestion, and freedom of movement occur. Potential new entrance and exit points 
can therefore be reprogrammed back into the animation in an attempt to balance or tune the model in such 
a way as to bring more regularity to the movement of users within the airport. If we choose to look at 
the composite for its formal architectural merits (or potentials), we are able to witness several potential 
for­mal design logics. An example of this can be seen in the nature of the warping of the individual 
particles based on the forces present. At the left portion of the composite, we see several of the particles 
organizing into an overlapping pattern that could be translated into a cladding system of glass or metal 
whereby repetitive panels overlap one another in such a way as to (conceptually) reflect the dynamics 
of movement found in the composite while (physically) protecting users from the elements. These same 
geometries can also be plugged back into a new procedural model and used (along with other ergonomic 
parameters) towards the design of furniture that relfects the same geometric qualities. 5. Conclusion 
24X7ATPHL: Codify is meant as an experiment into the visu­alization of qualitative data into quantitative 
output, using proce­dural modeling and time-based software to visualize complex data in ways not possible 
through other means. The potential outcomes of such visualization have massive ramifications on design 
poten­tials, from the logic of analysis and planning to the purely plutonic and graphic.  Figure 01 
Rendered detail from 24X7@PHL: Codify.  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1836881</article_id>
		<sort_key>360</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>33</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Bloom]]></title>
		<subtitle><![CDATA[an interactive, organic visualization of starred emails]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836845.1836881</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1836881</url>
		<abstract>
			<par><![CDATA[<p>Bloom uses the metaphor of a desktop plant to remove task management from the already overloaded inbox and into a more human environment. When tasks in the inbox are starred, the email information is sent to an external touchscreen that then grows a flower for that specific task. The flower is activated on touch and the text of the email is displayed. Plucking the flower---touching, holding, then flicking the flower---removes that item from the task list.</p> <p>A large number of tools exist for managing tasks. Bloom is different in that it uses an organic, passive metaphor for visual display. Instead of having a series of piling text, whether in physical or digital form, Bloom does not visually overwhelm. A single task is as visually appealing as fifty. Additionally, although numerous email visualizations also exist much of this work has to do with overall inbox visualization and/or the display of relationships [1]. There is also precedence in using metaphor to visualize email as seen in Kjen Wilkens' <i>Mail Garden.</i> Bloom is distinct in both its focus on task management and our intent at full integration with existing email systems.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[email]]></kw>
			<kw><![CDATA[metaphorical display]]></kw>
			<kw><![CDATA[task management]]></kw>
			<kw><![CDATA[visualization]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2264308</person_id>
				<author_profile_id><![CDATA[81466642914]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Amy]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Martin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[California College of the Arts, San Francisco, CA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264309</person_id>
				<author_profile_id><![CDATA[81100098733]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Wendy]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ju]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[California College of the Arts, San Francisco, CA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1133346</ref_obj_id>
				<ref_obj_pid>1133265</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Perer, A. and Smith, M. 2006. Contrasting Portraits of Email Practices: Visual approaches to reflection and analysis. <i>Proceedings of AVI '06, The Working Conference on Advanced Visual Interfaces.</i> pp. 389--395.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1466572</ref_obj_id>
				<ref_obj_pid>1466570</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Ducheneaut, N. and Watts, L. 2005. In Search of Coherence: A Review of E-Mail Research. <i>Human-Computer Interaction</i> 20.1 pp. 11--48.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>238530</ref_obj_id>
				<ref_obj_pid>238386</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Whittaker, S. and C. Sidner. 1996. Email overload: exploring personal information management of email. <i>Proceedings of CHI'96, Conference on Human Factors in Computing Systems</i>, ACM, NY. pp. 276--283.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Corston-Oliver, S., Ringger, E., Gamon, M. and Campbell, R. 2004. Integration of Email and Task Lists. <i>Proceedings of CEAS '04, Collaboration, Electronic messaging, Anti-Abuse and Spam Conference</i>, AAAI, pp. 134--135.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Bloom: An Interactive, Organic Visualization of Starred Emails Amy Martin California College of the 
Arts 1111 Eighth Street San Francisco, CA 94107 amartin2@cca.edu Abstract Bloom uses the metaphor of 
a desktop plant to remove task man­agement from the already overloaded inbox and into a more human environment. 
When tasks in the inbox are starred, the email infor­mation is sent to an external touchscreen that then 
grows a flower for that specific task. The flower is activated on touch and the text of the email is 
displayed. Plucking the flower touching, holding, then flicking the flower removes that item from the 
task list. A large number of tools exist for managing tasks. Bloom is differ­ent in that it uses an organic, 
passive metaphor for visual display. Instead of having a series of piling text, whether in physical or 
digi­tal form, Bloom does not visually overwhelm. A single task is as visually appealing as fifty. Additionally, 
although numerous email visualizations also exist much of this work has to do with overall inbox visualization 
and/or the display of relationships [1]. There is also precedence in using metaphor to visualize email 
as seen in Kjen Wilkens Mail Garden. Bloom is distinct in both its focus on task management and our intent 
at full integration with existing email systems. Keywords: Email, task management, visualization, metaphorical 
display 1 Introduction Over the past thirty years researchers have created a dense field of quantitative 
inquiry [2]. Media artists have also created a large amount of email-related work that reflects an experimental 
and po­etic perspective. By combining metaphors with existing quantitative work Bloom begins to open 
a space for questioning between these sets of research. Email has expanded well beyond its original communication 
role to include emergent functions such as task management [3]. Although task management within email 
is a deeply addressed problem, some of the proposed solutions are not integrated into the email system 
themselves, requiring users to exit the inbox and fill out a form [4]. Other solutions use yet more language 
for a user to process. Bloom uses preattentive markers such as color and location to passively let the 
user know when she has things to do. It is also integrated directly into the inbox such that no additional 
behaviors are required from the user. 2 Exposition Derived from one person s emails over a week-long 
period, Bloom mimics an inbox to demonstrate full integration with existing star­ring systems. As individual 
messages are starred as to-do list items, a series of arrays are populated with message information including 
title, date and sender. Each bloom is colored-coded into four cat­egories: Money In/Out, School, Personal 
and Informational. These categories of messages cluster together on screen. The initial criteria used 
by Bloom to classify emails were determined by hand, but we anticipate that we will be able to apply 
natural language processing and filters to automatically generate categories in the near future. Wendy 
Ju California College of the Arts 1111 Eighth Street San Francisco, CA 94107 wju@cca.edu  The Bloom 
display is exhibited on an external monitor, which brings into a more human environment. Readers can 
call up important emails can by touching Bloom s touch screen, and unimportant e­mails can also be quickly 
pruned out. When the user selects a flower, it grows in size and spins. Text about the message is dis­played 
above the flower. Another touch of the same flower closes the text window, stops the spin and shrinks 
the flower back down to normal size. Pruning the plant, i.e., marking things as done on the to-do list, 
uses mouse behaviors on the touch screen to simulate plucking the flower off the plant. These high level 
interactions allow people to quickly address their incoming e-mail without having to delve into the potential 
distractions of their inbox. While managers, for example, need to see, respond and delegate tasks as 
a primary function of their job, employees whose workflow involves creating tend to be more distracted 
by the constant stream of email [3]. The gentle, passive display of information allows that employee 
to complete her work without distraction, achieving a level of engagement that is currently very difficult 
to attain and sustain. 3 References [1] Perer, A. And Smith, m. 2006. Contrasting Portraits of Email 
Practices: Visual approaches to reflection and analysis. Proceed­ings of AVI 06, The Working Conference 
on Advanced VisualInterfaces. pp. 389-395. [2] ducheneAut, n. And WAttS, L. 2005. In Search of Coherence: 
A Review of E-Mail Research. Human-Computer Interaction 20.1 pp. 11-48. [3] WhittAker, S. And c. Sidner. 
1996. Email overload: exploring personal information management of email. Proceedings ofCHI 96, Conference 
on Human Factors in Computing Systems, ACM, NY. pp. 276-283. [4] corSton-oLiver, S., ringger, e., gAmon, 
m. And cAmPbeLL, r. 2004. Integration of Email and Task Lists. Proceedings of CEAS 04, Collaboration, 
Electronic messaging, Anti-Abuse and Spam Conference, AAAI, pp. 134-135. Copyright is held by the author 
/ owner(s). SIGGRAPH 2010, Los Angeles, California, July 25 29, 2010. ISBN 978-1-4503-0210-4/10/0007 
 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1836882</article_id>
		<sort_key>370</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>34</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[DECO]]></title>
		<subtitle><![CDATA[a designing editor for line stone decoration]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836845.1836882</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1836882</url>
		<abstract>
			<par><![CDATA[<p>Line stone decoration is popular with young people. They enjoy applying line stone decorations to personal goods, such as notebook PCs, mobile phones and digital cameras. However, novices often find line stone decorations difficult to design, as the user must consider stroke length, stone width, and stone spacing. Hence, many people employ off-the-shelf design sheets or have their items decorated by an in-store professional. We have developed an interactive designing editor for line stone decoration. The user interactively draws freeform strokes on the canvas, as shown in Figure 1. The system then automatically generates a virtual line stone image (Fig. 1(b)), in which none of the stones overlap. Various off-line methods have been proposed for designing such decorations (e.g., tile mosaics [Hausner 2001]). Using our system, the user can create the designs interactively at the computer. Our system also creates a physical stencil pattern to help novice users to construct real line stone (Fig. 1 (c), (d)).</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.4</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2264310</person_id>
				<author_profile_id><![CDATA[81421598161]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Yuki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Igarashi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Tsukuba, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Graphtec. Craft ROBO.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383327</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Hausner, A. 2001. Simulatiing decorative mosaics. In <i>Proceedings of the 28th annual conference on Computer graphics and interactive techniques</i>, 573--580.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1073323</ref_obj_id>
				<ref_obj_pid>1073204</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Igarashi, T., Moscovich, T., and Hughes, J. F. 2005. As-rigid-as-possible shape manipulation. <i>ACM Transactions on Computer Graphics 24</i>, 3, 1134--1141.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 : A Designing Editor for Line Stone Decoration Yuki Igarashi* University of Tsukuba, Japan / JSPS Research 
Fellow  a cutter plotter Figure 1: Overview of our system. A user draws free-form strokes on the canvas. 
The system automatically generates a virtual line stone image. The user constructs a physical pattern, 
using a cutting plotter. Finally, the user employs the pattern to place the stones. 1 Introduction Line 
stone decoration is popular with young people. They enjoy ap­plying line stone decorations to personal 
goods, such as notebook PCs, mobile phones and digital cameras. However, novices often .nd line stone 
decorations dif.cult to design, as the user must con­sider stroke length, stone width, and stone spacing. 
Hence, many people employ off-the-shelf design sheets or have their items dec­orated by an in-store professional. 
We have developed an interac­tive designing editor for line stone decoration. The user interac­ tively 
draws freeform strokes on the canvas, as shown in Figure 1. Figure 2: Screenshot of the DECO system. 
The system then automatically generates a virtual line stone image (Fig. 1(b)), in which none of the 
stones overlap. Various off-line methods have been proposed for designing such decorations (e.g., tile 
mosaics [Hausner 2001]). Using our system, the user can create the designs interactively at the computer. 
Our system also creates a physical stencil pattern to help novice users to construct real line (a) The 
user can pull an existing stroke. stone (Fig. 1 (c), (d)). 2 User Interface Figure 2 shows our prototype 
system. The system is implemented (b) The user loads an image and places stones to trace the image. asaJavaTM 
program with internal vector representations. When Figure 3: Operations. the user draws a stroke, the 
system automatically places stones on the stroke without overlap. Strokes are represented as polylines, 
so that the user can easily modify a stroke to move, erase, change the colors of the stones, or change 
the scale. Existing strokes are took 10-20 min, and production of the decoration required 30-60 modi.ed 
using a pull tool [Igarashi et al. 2005], as shown in Figure min. 3(a). During a pull operation, the 
system interactively updates the positions of the stones and the number of stones on a stroke. The user 
can load a preexisting image and place stones to create a design in the shape of the image (Fig. 3(b)). 
Flood-.ll operations are also available. Once the design has been completed, the system supports the 
pro­duction process. The user is shown the required number of stones Figure 4: Results using our system 
and real line stones. and the total production time, and the resulting line stone image is exported in 
vector format (Figure 1(c)). The DXF dataset is sup- References ported. The user can then cut out a pattern 
using a cutter plotter [Graphtec ]. The holes in this pattern facilitate the placement of the GRAPHTEC. 
Craft ROBO. line stones on the design, as shown in Figure 1(d). In this way, even a novice user can easily 
create real line stones. HAUSNER, A. 2001. Simulatiing decorative mosaics. In Proceed­ings of the 28th 
annual conference on Computer graphics and 3 Results interactive techniques, 573 580. We used our system 
to create some line stone designs and actual line IGARASHI,T.,MOSCOVICH,T., AND HUGHES, J. F. 2005. stone 
decorations, as shown in Figure 4. A design session typically As-rigid-as-possible shape manipulation. 
ACM Transactions on *e-mail:yukim@acm.org Computer Graphics 24, 3, 1134 1141.  Copyright is held by 
the author / owner(s). SIGGRAPH 2010, Los Angeles, California, July 25 29, 2010. ISBN 978-1-4503-0210-4/10/0007 
 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1836883</article_id>
		<sort_key>380</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>35</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[Designing a new toy to fit other toy pieces]]></title>
		<subtitle><![CDATA[a shape-matching toy design based on existing building blocks]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836845.1836883</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1836883</url>
		<abstract>
			<par><![CDATA[<p>Shape-matching toys are popular items for infants, and consist of boxes with many holes in different shapes along with corresponding blocks of the same shapes. To play with the toy, an infant finds and inserts a block matching the shape of a particular hole. It is difficult to design new shape-matching toys based on existing blocks. We assume that the user performs such design as shown in Fig. 1 (e) based on existing building blocks like those shown in Fig. 1 (a). The construction of the toy body can be roughly divided into three steps: gather the parts, lay them out on a wooden board and trace them using a pencil, and saw the wooden board. This manual method is straightforward, but errors cannot be rectified and it is also unsuitable for mass production. Accordingly, we propose the use of a laser cutter (e.g., Commax Laser System) or a cutting plotter (e.g., Craft ROBO). Today, services are available that allow the user to send a vector dataset to a company and have the corresponding wooden board returned to them.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>J.7</cat_node>
				<descriptor>Consumer products</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010481.10003558</concept_id>
				<concept_desc>CCS->Applied computing->Operations research->Consumer products</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2264311</person_id>
				<author_profile_id><![CDATA[81421598161]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Yuki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Igarashi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Tsukuba]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264312</person_id>
				<author_profile_id><![CDATA[81100184469]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Hiromasa]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Suzuki]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[
Bradski, G., and Kaehler, A., 2008. Learning openCV: Computer vision with the openCV library.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>53392</ref_obj_id>
				<ref_obj_pid>53387</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[
Carlson, S. 1988. Sketch-based image coding of gray-level images. <i>Signal Processing 15</i>, 57--83.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Figure 1: 1 Introduction Shape-matching toys are popular items for infants, and consist of boxes with 
many holes in different shapes along with correspond­ing blocks of the same shapes. To play with the 
toy, an infant .nds and inserts a block matching the shape of a particular hole. It is dif­.cult to design 
new shape-matching toys based on existing blocks. We assume that the user performs such design as shown 
in Fig. 1 (e) based on existing building blocks like those shown in Fig. 1 (a). The construction of the 
toy body can be roughly divided into three steps: gather the parts, lay them out on a wooden board and 
trace them using a pencil, and saw the wooden board. This man­ual method is straightforward, but errors 
cannot be recti.ed and it is also unsuitable for mass production. Accordingly, we propose the use of 
a laser cutter (e.g., Commax Laser System) or a cutting plotter (e.g., Craft ROBO). Today, services are 
available that allow the user to send a vector dataset to a company and have the corre­sponding wooden 
board returned to them. We propose a computer-based method for novices to enable the de­sign of a new 
toy to .t another existing toy. The method involves designing a construction diagram for new toy on a 
photograph of the existing toy using a computer (Fig. 1 b). The user .rst takes a photograph of the existing 
toy on a checkerboard, then designs the form of the new toy on the photograph using a sketching interface. 
To take the photograph on the checkerboard, the system automati­cally .ts the designed toy to the real 
measurements and exports the results in vector form. The system supports SVG and DXF formats. Finally, 
the user cuts the shapes from a real wooden board using a cutter plotter or laser printer. 2 System 
Overview Figure 1 shows a real toy made using the proposed method. Our goal is to design a new shape-matching 
toy (Fig. 1 e) from existing building blocks (Fig. 1 a). The user .rst prints a checkerboard and takes 
a photograph of the existing building blocks Oexisting on it. The user draws free-form strokes on the 
input photograph shapes (line, rectangle and ellipse), and exports the user-input strokes in vector form 
as shown Fig. 1(d). The checkerboard is used for scale adjustment. Although various methods have been 
proposed for feature point extraction in 2D [Carlson 1988; Bradski and Kaehler 2008], no method currently 
enables precise extraction. Accordingly, we adopt manual input by the user. The system .rst extracts 
the grid of the checkerboard, and the user can easily mark the corners of the grid using the extracted 
edge detection image (Fig. 2 c). The user has to mark corners at e-mail:yukim@acm.org Figure 3: Comparison 
of timing data 3 Evaluation We tried the system with three test-users who designed a shape­matching 
toy as shown in Fig.1. Figure 3 shows a comparison of our method and existing software (Adobe Illustrator). 
In our method, the user .rst takes a photograph of the building blocks on a checkerboard, then draws 
strokes to create a diagram for the shape-matching toy on the photograph and marks its corners. In the 
existing method, the user .rst measures the building blocks and draws a construction diagram for the 
shape-matching toy. It is only necessary to mark 5 - 6 points from the results of the user study. It 
took about 10 - 13 seconds, and the test users reported that they found the system easy to use. References 
BRADSKI, G., AND KAEHLER, A., 2008. Learning openCV: Com­puter vision with the openCV library. CARLSON, 
S. 1988. Sketch-based image coding of gray-level im­ages. Signal Processing 15, 57 83. Copyright is 
held by the author / owner(s). SIGGRAPH 2010, Los Angeles, California, July 25 29, 2010. ISBN 978-1-4503-0210-4/10/0007 
 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1836884</article_id>
		<sort_key>390</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>36</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>5</seq_no>
		<title><![CDATA[Easy-Tagging Cam]]></title>
		<subtitle><![CDATA[using social tagging to augment memory]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836845.1836884</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1836884</url>
		<abstract>
			<par><![CDATA[<p>Easy-Tagging Cam (or ETC) is a digital image recording system equipped with multiple shutter buttons. This system enables users to capture and tag photographs simultaneously. This function allows the user to be set free from tagging tasks. The users enable to develop re-useable photo storage continuously. This system also utilizes a life-log system thereby aiding the easy retrieval of information.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>H.5.1</cat_node>
				<descriptor>Artificial, augmented, and virtual realities</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>H.5.3</cat_node>
				<descriptor>Web-based interaction</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010392</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Mixed / augmented reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010868</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Web-based interaction</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010866</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Virtual reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010866</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Virtual reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2264313</person_id>
				<author_profile_id><![CDATA[81365592127]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Koh]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sueda]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264314</person_id>
				<author_profile_id><![CDATA[81466644909]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Kazushi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kotani]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Digital Hollywood]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264315</person_id>
				<author_profile_id><![CDATA[81100008564]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Jun]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Rekimoto]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Bower, et al., Selectivity of learning caused by affective state, Journal of Experimental Psychology: General, 110, 451--473, 1981]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Easy-Tagging Cam ~Using social tagging to augment memory~ Koh SUEDA Kazushi KOTANI §Jun REKIMOTO University 
of Tokyo Digital Hollywood University of Tokyo/Sony CSL Inc. Abstract Easy-Tagging Cam (or ETC) is a 
digital image recording systemequipped with multiple shutter buttons. This system enables users tocapture 
and tag photographs simultaneously. This function allowsthe user to be set free from tagging tasks. The 
users enable to develop re-useable photo storage continuously. This system also utilizes a life-log system 
thereby aiding the easy retrieval of information.  Figure 1:This system enables users to capture and 
tagphotographs simultaneously Introduction Nowadays, it is possible to record our daily activities by 
usingdigital cameras with large storage capacity. These snapshots, whichare large in number, should be 
easily retrieved when stored using alife-log system. At the same time, these snapshots required re-usability 
that is easy to search (or find). Tagging photographs helps in easy retrieval. Tagging is a powerfulmethod 
to search our records. However, tagging a large number ofphotographs is time consuming. In this paper, 
we propose a system using which photographs can becaptured and tagged simultaneously. This system has 
multipleshutter buttons that can be used to tag photographs. Each buttons areassigned tag. When the user 
presses a shutter button, the corresponding tag is assigned to the captured photograph. This system allows 
the users to be set free from tagging tasks. Usage Examples The main features of this system are as follows: 
1. Easy tagging (press the appropriate shutter button fortagging) 2. Easy retrieval of past experiences 
 3. Keeping the users motivation to record and classify theirdaily activity  ETC is equipped with multiple 
shutter buttons that can be used to tagphotographs according to the mood or environment of the subject. 
Figure 1 shows an example of an image captured using the proposedsystem and viewed on an Apple iPhone. 
The user tags thephotographs by pressing the appropriate button. Further, thephotographs can be viewed 
and sent to a photo storage service suchas Flickr using the tags (Figure 2). Figure 2: The image of 
multiple tagged shutters. The user ofsystem enables to add tags to their photographs by cheesing andpressing 
the button. In addition, these tags from the real worldcan be used as a database of collective intelligence 
of the realworld on the basis of the subjective aspects of the users. Discussion The usage of a life-log 
system enhances memory. It is known thathuman is tagging to their experiments (including moods,environments, 
and time) when these are encoded. These encodedmemories are reserved as metadata that used a trigger 
of recognitionlike adding tags to data. According to the mood congruency effect, individuals can retrieve 
information more easily when it has the same emotional content as their current emotional state [1].Therefore, 
displaying similar situations will result in faster retrievalof information. In the proposed system, 
for easy retrieval of records, photographs can be tagged according to the mood of the subjects. Therefore 
the system enables to provide information that is similarto the users past experiments. These tags allow 
us to supportrecollection and reuse our life logs. In addition, these tags can beused as a database of 
collective intelligence of the real world on thebasis of the subjective aspects of the users. Further, 
the photographsare tagged according to the mood or environment of the user. However, this system requires 
more easy accessibility and usabilityfor end users to realize the concept (Figure 3). The main features 
ofthis system are easy tagging and retrieval of information. Further,this system can be used on a social 
tagging platform in the real world.For instance, the platform can be a tags database that is for assigningtags 
from the real world to ETC. Figure 3: The concept of memory enhancement using ETC References [1] Bower, 
et al., Selectivity of learning caused by affective state, Journal of Experimental Psychology: General, 
110, 451-473, 1981 info@ching-dong.com , dhkotani@gmail.com,§rekimoto@acm.org Copyright is held by the 
author / owner(s). SIGGRAPH 2010, Los Angeles, California, July 25 29, 2010. ISBN 978-1-4503-0210-4/10/0007 
 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1836885</article_id>
		<sort_key>400</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>37</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>6</seq_no>
		<title><![CDATA[Floral melody]]></title>
		<subtitle><![CDATA[flower arrangement as music interface]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836845.1836885</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1836885</url>
		<abstract>
			<par><![CDATA[<p>Flower arrangement is one of famous traditional arts in Japan, and being enjoyed across the world now. People have been created the atmosphere in a room or represented one's mind by flower arrangement.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>H.5.2</cat_node>
				<descriptor>Graphical user interfaces (GUI)</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>H.5.5</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010469.10010475</concept_id>
				<concept_desc>CCS->Applied computing->Arts and humanities->Sound and music computing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010865</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Graphical user interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002951.10003317.10003371.10003386.10003390</concept_id>
				<concept_desc>CCS->Information systems->Information retrieval->Specialized information retrieval->Multimedia and multimodal retrieval->Music retrieval</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2264316</person_id>
				<author_profile_id><![CDATA[81460650924]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Hiroki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yamada]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Floral Melody: Flower Arrangement as Music Interface HirokiYamada* University ofTokyo 1 Introduction 
Flower arrangement is one offamous traditional arts in Japan, and being enjoyed across the world now. 
People have been created the atmosphereina roomor representedone smindby.ower arrange­ment. Meanwhile, 
with the advancement of technology, it has become easy for people to enjoy listening music or composing 
by them­selves. Flower arrangement and music are similar in that both art forms create atmosphereina 
roomor representone smind, thesetwo arts havebeen different matters.Toprovidea bridge for .ower arrange­ment 
between music, the author developed Flower Arrangement Interface , which enables userstoenjoymusicbyarranging.owers 
in a vase. This study aims to provide people novel form of .ower arrangement and music experience.  
2 System Architecture The Floral Melody system architectureisas follows;PassiveIC(In­tegrated Circuit) 
tags are embedded in each stem of .ower. These IC tags are very small and thin (51.5mm height, 1.5mm 
width, 0.25mm thickness), so they.t intoa stemofa .ower.Aglassvase isputonawoodytable.AnRFID(RadioFrequencyIDenti.cation) 
antenna is set under the table to detect tabletop IC tags. The thick­ness of the table is enough thin 
so that the RFID antenna does not Figure 2: IC tag embedded in a stem arranged in advance by converting 
combination of .owers into a track number (forexample, when .ower A,B,andE areinvase and CandDare outside, 
this pattern of combination is converted into binary number 11001 once, and then converted into decimal 
num­ber 25). The system sends the decided track number to audio player software and music changes. 3.2 
As a tool for audio synthesis When the system works as a tool for composition, real-time audio synthesis 
software (MaxMSP, ChucK) creates sound from combi­nation of .owers. Some .owers strikes a note and others 
have a function of acoustic effects.  4 User Experience Figure 1: Floral Melody System Overview  3 
Application The author developed two types of application of this system; an audio player and a tool 
for audio synthesis. 3.1 As an audio player When the system works as an audio player, the combination 
of .owers decide which track is played. When the combination of .owers in a vase is changed, the system 
chooses a track from a list *e-mail: yamadaman@cyber.t.u-tokyo.ac.jp Figure 3: Floral Melody System 
in a cafe The authorexhibitedthesystem(inanaudioplayermode)inacafe as an interior accessory for visitors. 
Almost all people who ex­perienced the system asked the author about the mechanism of detection of .owers. 
This indicates that most people didn t no­tice the presenceof detection system and this systemkeeps enough 
naturalness of .ower arrangement even after embedding IC tags. Copyright is held by the author / owner(s). 
SIGGRAPH 2010, Los Angeles, California, July 25 29, 2010. ISBN 978-1-4503-0210-4/10/0007   
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1836886</article_id>
		<sort_key>410</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>38</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>7</seq_no>
		<title><![CDATA[How would you like to live?]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836845.1836886</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1836886</url>
		<abstract>
			<par><![CDATA[<p>"How Would You Like To Live" is a graphical articulation manifest from user sensory "wishes" supplied by an architectural client building a new home. It was crafted to help the designer in under-standing the needs of the client through emergent, patterned, non 1:1 results. Through the use of a parametrically-driven procedural network with parametric inputs supplied by the client, a graphical "depiction" of the user's hopes, dreams, and senses towards the occupation of domestic space was generated.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.8</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010507</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Graphics recognition and interpretation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2264317</person_id>
				<author_profile_id><![CDATA[81320495814]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Robert]]></first_name>
				<middle_name><![CDATA[B.]]></middle_name>
				<last_name><![CDATA[Trempe]]></last_name>
				<suffix><![CDATA[Jr]]></suffix>
				<affiliation><![CDATA[Temple University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 How Would You Like To Live? Robert B Trempe Jr     Tyler School of Art Architecture Department, 
Temple University     trempe@temple.edu Background How Would You Like To Live is a graphical articulation 
mani­fest from user sensory wishes supplied by an architectural client building a new home. It was crafted 
to help the designer in under­standing the needs of the client through emergent, patterned, non 1:1 results. 
Through the use of a parametrically-driven procedural network with parametric inputs supplied by the 
client, a graphical depiction of the user s hopes, dreams, and senses towards the occupation of domestic 
space was generated. The HWYLtL project was inspired by and undertaken in re­sponse to the work of Dutch 
Architect Lars Spuybroek, specifical­ly two projects entitled Off the Road - 5 Speed and myLight, both 
of which made usage of parametric modeling techniques as formal articulation tools. In the case of Off 
the Road, dynamic forces within an animated environment operated as shaping tools for a housing development 
at multiple scales, while in my­Light simple pulling and pushing actions within a plutonic model shaped 
the light to a customers specifications. While innovative in their specific usage of parametric design, 
both projects suffered from the employment of a visual, theoretical, and technical design pipeline as 
a method for transporting initial data to constructed work. In this way, neither project made full use 
of parametric information as an investigatory AND generative design tool. HWYLtL seeks to exploit this 
design pipeline from the begin­ning through technical advances in procedural modeling tied to visual 
acuity and interpolation, proposing connections from con­cept to execution. The first step (the mapping 
of quantitative data) serves as instruction our output for every other aspect in the design pipeline. 
This output has countless potential, operating as Rorschach Test for the changes in client sensory perception 
and attitude towards space over time, as a graphical instigator for the designer (using the formal results 
to unlock emergent design potential), and even as instructions for CNC production by using the resultant 
vector patterns as paths for mills and laser/plasma cutters in the building of parts for the house. The 
usage of a procedural network enhances the creation, analysis, and output of this investigation in that 
the flexible nature of the network can be exploited in multiple manners, each responding to different 
parts of the design process. Mechanics and Technical On the client end, a simple survey (created in Microsoft 
Excel) re­garding qualities of space was filled out once a week for 9 weeks by a potential architectural 
client. Questions given to the client dealt with various qualities of space such as the importance of 
a bedroom in your life, the brightness of a living space, and level of inward / outward focus for a workspace. 
The results, all scored from 1 to 10 (1 as a low value and 10 as a high value for each question), were 
then filtered through a series of if/then statements in Excel as a method of pre-conditioning the information 
in such a way as to ease their integration into the procedural network in McNeel Rhinoceros and Grasshopper. 
On the designer end, a basic point grid was positioned in space (via Rhinoceros) with specific columns 
of points correspond­ing to specific types of spaces (spaces for work and production, relaxation, and 
rest). Each column of points generated a simple NURBS curve (via Grasshopper) all of which, in turn, 
drove the generation of a NURBS surface. A second NURBS surface was then offset from the first, with 
vector lines connecting the corresponding UV points between surfaces. These resultant lines became the 
graphic tracings made visible as the end result of each survey. Answers provided by the client survey 
adjusted the offset distance between surfaces as well as the z-axis data for the points that drove the 
curves that (in turn) drove the surfaces. In this somewhat Rube Goldberg-sek procedural model, data drove 
the generation of three-dimensional information with the procedural network transforming this information 
into two-di­mensional results, all done live and on the fly (via the connection established in Grasshopper). 
Each week s results could be im­mediately linked to the model, with each week s results affecting a singular 
instance of the model. As new results were obtained, a new iteration was superimposed on the previous 
weeks result, building a time-based composite of the client s decisions. As these mappings were to be 
read in a somewhat analytical fashion, the removal of perspective-based information was paramount if 
one was to be able to understand relational changes throughout the model. Conceptual and Concrete Power 
There are many potential outputs from a study such as this, with results being used as strategies for 
understanding new potentials in architectural space planning, the qualitative nature of the spaces themselves, 
and even the methods by which the work is then constructed. Conceptually, How Would You Like to Live 
offers the designer an emergent design tool, a graphic that presents information via a non 1:1 manner 
in such a way as to inspire new architecture. The designer is able to examine changes in the clients 
wants based on changes in the patterning (both overall and localized) within the mapping. In this way, 
the resulting mapping operates (formally) as a palimpsest for the sensory experiences associated with 
domesticity and ultimately the ways in which the client wishes to occupy space. The designer still keeps 
control (IE designs the project rather than the computer taking over) in that the designer first designs 
the procedural network and also operates as the investigator of the results, interpreting the information 
as they see fit. As the procedural network is flexible, the designer is afforded the ability to tune 
the results ever-so-slightly to clarify information where needed. As a concrete tool of construction, 
the basic procedural network can be augmented and added to as a method for moving from conceptual design 
to that of design fabrication. The lines of the resultant mappings can easily be manipulated and converted 
(via additions to the procedural network) to toolpaths for use by CNC routers, plasma and laser cutters, 
each helping in the production of architectural elements for the resultant house. Procedural filter­ing 
can also be added to the existing network to help in shaping the results to operate more in a three-dimensional 
manner, aiding in the fabrication of elements such as wall panels, windows, and other architectural systems 
now being driven by automated con­struction methods.  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1836887</article_id>
		<sort_key>420</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>39</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>8</seq_no>
		<title><![CDATA[Mathmorph]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836845.1836887</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1836887</url>
		<abstract>
			<par><![CDATA[<p>The name "Math-Morph" combines the notion of "mathematic" with the notion of "morphology. This project focuses on the study of "mathematic" as an embedded variability of spatial arrangement with procedural model. The influence of digital media and information technology on architectural education and practice is increasingly evident. Digital technology has reconditioned the design process that establishes new processes and techniques of fabrication. This reconditioning has influenced how we operate as architects. Today, architectural design and building construction are increasingly aided by and dependent on digital technology. These technologies allow architects to foresee the appearance and predict the performance of proposed buildings. Mathmorph proposes an interdisciplinary research in digital fabrication of unconventional 3D forms on a conceptual design level in order to explore their features in interacting with people and their potentials of being used as architectural forms. It describes an experimental approach which facilitates 3D form generation, visualization and fabrication.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061.10010063</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures->Computational geometry</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2264318</person_id>
				<author_profile_id><![CDATA[81466647362]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Ming]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tabg]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Savannah collage of Art and Design]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264319</person_id>
				<author_profile_id><![CDATA[81466641838]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Jonathon]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Anderson]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of North Carolina Greensboro]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Mathmorph The name "Math-Morph" combines the notion of mathematic with the notion of morphology. This 
project focuses on the study of mathematic as an embedded variability of spatial arrangement with procedural 
model. The influence of digital media and information technology on architectural education and practice 
is increasingly evident. Digital technology has reconditioned the design process that establishes new 
processes and techniques of fabrication. This reconditioning has influenced how we operate as architects. 
Today, architectural design and building construction are increasingly aided by and dependent on digital 
technology. These technologies allow architects to foresee the appearance and predict the performance 
of proposed buildings. Mathmorph proposes an interdisciplinary research in digital fabrication of unconventional 
3D forms on a conceptual design level in order to explore their features in interacting with people and 
their potentials of being used as architectural forms. It describes an experimental approach which facilitates 
3D form generation, visualization and fabrication. ( new poster with architecture design projects added) 
First, a series of computer scripts were generated using computer algorithms, and mathematic equations. 
Secondly, a series of 3D models were generated by importing these algorithms and mathematic equations 
into 3D programs. These computer models were fabricated as physical prototypes by the Stratasys FDM systems, 
CNC machine, and laser cut machines. The purpose of this part is bi-fold. It does not only inspire designers 
to use unconventional 3D forms in architectural design, which has traditionally been restrained by difficulties 
in design and visualization, but also test the possibility of these unconventional 3D forms in being 
manufactured as physical prototypes. The use of these mathematically driven forms can generate porous 
structures that are non-site-specific and allow for maximum heat gain/loss and natural wind-flow. By 
interlocking two forms the generation of natural program issues solve themselves; for example a mix-use 
program naturally forms based on the two independent forms. The computational approach to design allows 
for two areas of interest in the architectural field to combine: digital form finding and digital fabrication. 
A series of abstract sculpture designed with the focus on its potential transformative spatial layout 
was also explored. The generation of an abstract mathematic form using equations was studied and showed 
that the unlimited possibility of interlocking / intertwining between solid form and void space emerges. 
We adapted several variables to control the repetition and resolution of these interlocking spaces, by 
an exhaustive combination of several variables values. From a large number of outcomes, only several 
ideal spatial arrangement solutions were selected by reviewers and then used as the genotype for the 
next operation. After exporting this parametric model into 3D programs, the continuation to building 
its procedural network was allowed though a non-linear information model. A sequence of deformation and 
control nodes were added. This additive information evolved independently in order to yield a more fabrication 
friendly form. As a result, we created a high degree of complexity and explored the dynamic possibilities 
of spatial arrangement with relatively simple input information. The method was used in the winning project 
of Evolo Skyscraper 2010 design competition. In this process, the information model demonstrated itself 
with a great power and an unlimited potential of form exploration from sets of parameters. The reviewers 
selected the desired control nodes and manipulate them to create the new spatial organization and proving 
that a parametric model can be optimized by a fabrication limitation. In the final step, a slice node 
was introduced into the network as a static representation for laser cutting. The contouring process 
produced the file documentation that was needed to digitally fabricate the form. The parameters of the 
laser cutter and 3D printing are well integrated into the information model. Another input variable, 
time, as the 4th dimension, was also added to snapshot all the layout possibilities into a motion. Expressions 
were evolved and various spatial arrangements were produced as the value of time was smoothly animated. 
Hundreds of the contour lines for laser cutting were captured into a single morphing animation. Copyright 
is held by the author / owner(s). SIGGRAPH 2010, Los Angeles, California, July 25 29, 2010. ISBN 978-1-4503-0210-4/10/0007 
 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1836888</article_id>
		<sort_key>430</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>40</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>9</seq_no>
		<title><![CDATA[Rebecca Findlay's Pixel Pusher]]></title>
		<subtitle><![CDATA[poster abstract]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836845.1836888</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1836888</url>
		<abstract>
			<par><![CDATA[<p>Pixel Pusher represents a humorous symbol for all people who work with pixels on a daily basis such as teachers, students, or some other creative in the field relating them to the rigorous, time-consuming labor of a construction worker.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.6</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2264320</person_id>
				<author_profile_id><![CDATA[81466645070]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Rebecca]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Findlay]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Rebecca Findlay s Pixel Pusher Poster Abstract Pixel Pusher represents a humorous symbol for all people 
who work with pixels on a daily basis such as teachers, students, or some other creative in the field 
relating them to the rigorous, time-consuming labor of a construction worker. This poster is purposely 
designed and illustrated to achieve the look and feel of a construction sign you may see on the road, 
but tailored to look aesthetically pleasing using bright colors on a neutral white background. The idea 
is to make the audience who identifies with this technology feel connected and to enjoy the humor and 
unusual setting the symbol is implying. The poster represents a pictograph using simple illustrations 
for a quick, univeral read. This approach is innovative because most people do not associate people who 
work digitally with construction workers because of the physical nature of the job. Pixel pushers do 
enjoy getting their hands dirty and wiping sweat off their brow in much of the same fashion as construction 
workers just in a different environment. This poster reflects my design philosophy using strong concepts, 
simple illustrations and exploration of different materials. I solely created the theme, concept and 
all of the illustrations in the poster while under the guidance and criticism of fellow colleagues and 
professors and expanded on this to create an actual three-dimensional sign.  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1836889</article_id>
		<sort_key>440</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>41</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>10</seq_no>
		<title><![CDATA[Reactive architecture]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836845.1836889</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1836889</url>
		<abstract>
			<par><![CDATA[<p>This architectural system explores the idea of using a parametric interface that reacts and changes based on user input while reproducing a series of affects (defined in psychology as the experience of emotion or feeling) on the user. The affects are predetermined, based on real world examples, and the system is designed in accordance. The overall premise for this project is to explore how tangible affects can be represented through parameters where the results are only visualized through the computer.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>H.5.2</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003120.10003121</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2264321</person_id>
				<author_profile_id><![CDATA[81466647517]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Sophia]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sobers]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1836890</article_id>
		<sort_key>450</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>42</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>11</seq_no>
		<title><![CDATA[Temperature design display device to use peltier elements and liquid crystal thermograph sheet]]></title>
		<subtitle><![CDATA["Thermo-Pict neo"]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836845.1836890</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1836890</url>
		<abstract>
			<par><![CDATA[<p>"Thermo-Pict neo" is a design apparatus produced by applying temperature visualization technology linked to an information display with the use of a thermograph sheet. Thermography is used to visualize the surface temperature of objects through their depiction as colors. This technology has been used primarily in the medical and research fields. Thermography display colors come in a wide range of hues and brightness that enables quick visualization of any object's surface temperature distribution. Use of this technology will be attempted as a tool in the production of design displays. [Fig.1]</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.4.0</cat_node>
				<descriptor>Image displays</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2264322</person_id>
				<author_profile_id><![CDATA[81100274861]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Kumiko]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kushiyama]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tokyo Metropolitan University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264323</person_id>
				<author_profile_id><![CDATA[81319488008]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Tetsuaki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Baba]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tokyo Metropolitan University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264324</person_id>
				<author_profile_id><![CDATA[81472650104]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Kouki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Doi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[National Institute of Special Needs Education]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264325</person_id>
				<author_profile_id><![CDATA[81319500609]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Shinji]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sasada]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Japan Electronics Collage]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1180027</ref_obj_id>
				<ref_obj_pid>1179849</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Kushiyama, K., Inose, M., Yokomatsu, R., Fujita, K., Kitazawa, T., Tamura, M., and Sasada, S. 2006. Thermoesthesia: about collaboration of an artist and a scientist. In <i>ACM SIGGRAPH 2006 Sketches</i>]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1599328</ref_obj_id>
				<ref_obj_pid>1599301</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Takada, Higurashi, Suzuki, Ohta, Baba, Kushiyama, Thermo-Pict: In <i>ACM SIGGRAPH 2009 Poster</i>]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Temperature Design Display device to use Peltier elements and liquid crystal thermograph sheet Thermo-Pict 
neo Kumiko KUSHIYAMA(1) , Tetsuaki Baba(1) Kouki Doi (2) Shinji SASADA(3) 1)Tokyo Metropolitan University 
2)National Institute of Special Needs Education 3)Japan Electronics Collage 1.Introduction Thermo-Pict 
neo is a design apparatus produced by applying temperature visualization technology linked to an information 
display with the use of a thermograph sheet. Thermography is used to visualize the surface temperature 
of objects through their depiction as colors. This technology has been used primarily in the medical 
and research fields. Thermography display colors come in a wide range of hues and brightness that enables 
quick visualization of any object s surface temperature distribution. Use of this technology will be 
attempted as a tool in the production of design displays. [ Fig.1] Fig. 1: Thermo-Pict neo One typical 
product that uses Peltier elements is a visual tactile display known as Thermoesthesia [1] This is an 
interactive product in which the positions of the tester s fingers are detected by a touch panel, and 
simultaneous control of the temperature detected by Peltier elements and the CG image projected on the 
display can be performed. One limitation of this product is the requirement of a projector, which results 
in a restriction of the display area size. In the present study, visualization of information was performed 
using a thermograph sheet, which enables the display even in a well lit area using compact sized instrumentation. 
2. System configuration 2.1 Outline For Thermo-Pict neo , which utilizes 80 Peltier elements, the image 
on the liquid crystal thermograph sheet continuously changes. After being touched by an individual, the 
image is further modified according to the body temperature detected. Also, an individual using this 
technology is able to sense the temperature change caused by Peltier elements. Thus the Thermo-Pict neo 
product utilizes displays based on both visual and tactile senses. [Fig.2]  2.2 Implementation For development 
of our product, a liquid crystal thermograph sheet (C-Task Company) was used. This device responds to 
a temperature between 20°C ~ 32°C, maintaining a black hue at temperatures below 20°C and a bluish green 
hue at temperatures above 32°C. In all, 7 temperature-sensitive colors can be displayed: black, reddish 
brown, yellowish green, blue, purple, green and bluish green. For this study, the display was made by 
first placing a total of 80 Peltier elements in a 8x 10 configuration occupying a 15 mm x 15 mm area 
on an aluminum sheet (to dissipate heat) and then covering the Peltier elements with a 130 mm x 170 mm 
liquid crystal thermograph sheet. The surface of the thermograph sheet may be touched and the image on 
the sheet changes according to the person s body temperature. From this arrangement, the direction of 
the current flowing through the Peltier elements can be determined. Images which are temperature-responsive 
can therefore be shown on the display. Numbers, letters and pictures are programmed as images. Fig.3 
demonstration  3. Future outlook We are currently developing this product for applications to universal 
product designs such as wall surfaces, public bulletin boards, and table-top sign systems. In this production, 
we developed technology for a tactile display unit which is able to express a novel form of tactile communication 
for everyday life-use. We envision that the present creation will stimulate development in many other 
fields and provide a venue for new inventive creation. References [1] Kushiyama, K., Inose, M., Yokomatsu, 
R., Fujita, K., Kitazawa, T., Tamura, M., and Sasada, S. 2006. Thermoesthesia: about collaboration of 
an artist and a scientist. In ACM SIGGRAPH 2006 Sketches [2]Takada,Higurashi,Suzuki,Ohta,Baba,Kushiyama, 
Thermo-Pict :In ACM SIGGRAPH 2009 Poster Fig.2 System outline Copyright is held by the author / owner(s). 
SIGGRAPH 2010, Los Angeles, California, July 25 29, 2010. ISBN 978-1-4503-0210-4/10/0007   
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1836891</article_id>
		<sort_key>460</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>43</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>12</seq_no>
		<title><![CDATA[Tiny Dreamy Stories]]></title>
		<subtitle><![CDATA[interactive paper book capable of changing the storylines]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836845.1836891</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1836891</url>
		<abstract>
			<par><![CDATA[<p>In this paper, the authors propose Tiny Dreamy Stories, which uses a traditional paper book as an interface to experience digital contents, so that it can keep the affordances of paper books while adding electronic augmentation. The aim of this study is to achieve both highly computer-supported contents and natural interface, e.g., highly efficient combination of physical and digital world. With Tiny Dreamy Stories, every person (especially who is not good at operating computers) can enjoy rich digital contents just by flipping pages.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.8</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010507</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Graphics recognition and interpretation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2264326</person_id>
				<author_profile_id><![CDATA[81460650924]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Hiroki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yamada]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Figure 2: Woody book covers, Figure 1: Embedded IC tagin a paper pages, and woody rings page. with magnets 
to bind pages. Process of creating storylines by shuf.ing pages goes as follows: Once you take off all 
pages from the binder, you can see back­ *e-mail: yamadaman@cyber.t.u-tokyo.ac.jp branches 24. starts 
to play. 3 Figure 3: Guess how the system detects individual pages. How manytimes people played with 
shuf.ing will tell how people can enjoy this system. While some people tried to shuf.e pages only once, 
225 people shuf.ed pages more than once. The reason why people played with this over and over are as 
follows; Some said it is just a fun to play with. Others said theytried to .nd the mechanism of change 
of storylines by shuf.ing pages. This re­sult indicates that the page-shuf.ing system provides enjoyment 
to people at some level. Although further experiments are needed to validate the utility of the page-shuf.ing 
system. Figure 4: How many times people played with the shuf.ing system. Copyright is held by the author 
/ owner(s). SIGGRAPH 2010, Los Angeles, California, July 25 29, 2010. ISBN 978-1-4503-0210-4/10/0007 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1836892</article_id>
		<sort_key>470</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>44</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>13</seq_no>
		<title><![CDATA[TRENDS]]></title>
		<subtitle><![CDATA[a content-based information retrieval system for designers]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836845.1836892</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1836892</url>
		<abstract>
			<par><![CDATA[<p>The TRENDS European project aimed at developing an image and text retrieval engine in order to support the activity of the designers in the early stages of their design process [TRENS 2007]. The study of the designers' activity has led us to the production of an image database in which designers will find inspirational material. A content-based image search engine has been elaborated, starting from recommendations taken from the methodology employed by the designers in their activity, to end with a complete system incorporating image retrieval technologies and various tools to extract relevant information from these images.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>H.3.3</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10002951.10003317</concept_id>
				<concept_desc>CCS->Information systems->Information retrieval</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2264327</person_id>
				<author_profile_id><![CDATA[81466644525]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Jieun]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kim]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Arts et M&#233;tiers ParisTech, France]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264328</person_id>
				<author_profile_id><![CDATA[81466645496]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Carole]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bouchard]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Arts et M&#233;tiers ParisTech, France]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264329</person_id>
				<author_profile_id><![CDATA[81314491242]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Jean-Francois]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Omhover]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Arts et M&#233;tiers ParisTech, France]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264330</person_id>
				<author_profile_id><![CDATA[81350587506]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Ameziane]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Aoussat]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Arts et M&#233;tiers ParisTech, France]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[
TRENDS 2007, Meta-deliverable 1 -- State of the art, available on www.trendsproject.org
]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 TRENDS: A content-based information retrieval system for designers Jieun Kim, Carole Bouchard, Jean-Francois 
Omhover, Ameziane Aoussat Arts et Métiers ParisTech, France *email : {jieun.kim, Carole.Bouchard, Jean-Francois.Omhover, 
Ameziane.Aoussat}@paris.ensam.fr  (a) (b) (c) Figure 1 : (a) Image search by example (b) Semantic mapping 
(c) Pallets 1. Introduction The TRENDS European project aimed at developing an image and text retrieval 
engine in order to support the activity of the designers in the early stages of their design process 
[TRENS 2007]. The study of the designers activity has led us to the production of an image database in 
which designers will find inspirational material. A content-based image search engine has been elaborated, 
starting from recommendations taken from the methodology employed by the designers in their activity, 
to end with a complete system incorporating image retrieval technologies and various tools to extract 
relevant information from these images. 2 TRENDS functionalities The TRENDS system proposes much functionality 
that answers to specific methodological needs. The first family of tools, namely search functionalities, 
related to image retrieval. The second family of tools, namely advanced functionalities for design, related 
to the categorization of images, and their statistical analysis. We provide a list of the functionalities 
in the following. Image retrieval functionalities: -image search by text: the user inputs some keywords, 
semantic adjectives or concepts; -image search by example: the user selects an image as a query, similar 
images are found in the database (figure 1a); -mixed text and image search: a combination of keywords 
and images is used as a request -relevance feedback: the system refines the query results using positive 
and negative examples pointed out by the user.  Image processing features: -semantic mapping: using 
the semantic adjectives appearing in a set of results, the system builds up a mapping (see figure 1b) 
-pallets: using specific harmony rules, the user can extract the pallets of colors and textures appearing 
in a set of images (see figure 1c) -grouping: based on visual features, the user can automatically categorize 
a set of images into subsets for extracting trends -statistics on image / text search: using text and 
image search, the system can show in real time the list of sectors in which similar images can be found. 
Currently the resulting database of TRENDS system is 300 Go large with 1,888,525 files (JPG format) after 
filtering and categorized in 25 sectors.  3 TRENDS Specificities The specificities of TRENDS that reflects 
research advances and technological innovation can be summarized according to two main axes from a user 
perspective and from a system perspective. From a user perspective, TRENDS system is specific by its 
orientation towards designers. This point of reference is important in the way it will directly impact 
the interface and the system. Indeed some authors already explained that for creative activities like 
design the proposed search functionality should enable a more or less open way from very open or even 
random, to targeted and enabling to get precise details. From a system perspective, TRENDS system uses 
early fusion of visual features and offers the user the possibility to activate any combination of them: 
the semantic search performed both for text and images, the ability of the system to generate pallets 
extracted from images or sets of images and in this way to support digitally the elaboration of trendboards, 
the generation of statistics automatically linked with the query parameters, the information about consumers, 
and the image modifier.  References TRENDS 2007, Meta-deliverable 1 State of the art, available on 
www.trendsproject.org Copyright is held by the author / owner(s). SIGGRAPH 2010, Los Angeles, California, 
July 25 29, 2010. ISBN 978-1-4503-0210-4/10/0007  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1836893</article_id>
		<sort_key>480</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>45</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>14</seq_no>
		<title><![CDATA[TYPEFACE]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836845.1836893</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1836893</url>
		<abstract>
			<par><![CDATA[<p>The design of typefaces is founded upon principles from the days of metal type, when creating individual fonts was a laborious process and constrained by physical requirements. Most digital type design follows those same conventions, even though fonts are now drawn with vectors and pixels. Fonts are still largely based on historical references and are created in the context of publishing.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.7.2</cat_node>
				<descriptor>Photocomposition/typesetting</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010476.10010477</concept_id>
				<concept_desc>CCS->Applied computing->Computers in other domains->Publishing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2264331</person_id>
				<author_profile_id><![CDATA[81466640691]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Mary]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Huang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Copenhagen Institute of Interaction Design (CIID)]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 TYPEFACE . Mary HuangCopenhagen Institute of Interaction Design (CIID) Figure 1: Some type anatomy 
and terminology. 1 Introduction The design of typefaces is founded upon principles from the days of 
metal type, when creating individual fonts was a laborious process and constrained by physicalrequirements. 
Most digital type design follows thosesame conventions, even though fonts are now drawn with vectors 
and pixels. Fonts are still largely based on historical references and are created in the context of 
publishing. Technology gives us opportunities to make type designmore spontaneous and personal. TYPEFACE 
is software that uses camera vision to translate facial dimensions into generative type design. 2 Approach 
Letters are drawn from mathematically generated curvescontrolled by distinct variables that determine 
such characteristics as x-height, bowl curvature, stroke weight, and slant. This design approach allows 
for a more natural,handwriting-like rendering. Not only does this contrast with the geometric qualities 
generative type experiments tend to take, but also challenges the conventions of typing versus writing. 
Since the type generation runs live, typing a note in the software accumulates a range of natural variation. 
experience. In exploring the potential of now readilyaccesible interactive technologies, we not only 
canredefine basic standards of computing like fontformats, but we can redefine our expectations of gital 
communication. Copyright is held by the author / owner(s). SIGGRAPH 2010, Los Angeles, California, July 
25 29, 2010. ISBN 978-1-4503-0210-4/10/0007  Figure 2: Screen capture from the program.  3 Implementation 
The software uses open-source software for facial recognition and blob detection. It finds blobs associated 
with the face, eyes, and mouth of the user and uses therespective values to determine the variables in 
the type design. Mapping the face variables to the type variables can have a number of different implementations 
for varying effects. For instance, in the current model, smiling or moving the mouth increases the width 
of theletters, and widening the eyes and mouth will draw letters with larger x-height. If you yawn, the 
letters will yawn with you. Individual typefaces can be saved and exported as pdf files. Since the software 
is built with open-source tools, this makes the project easily accessible for continued development and 
exploration. TYPEFACE was originally intended as a typographic photobooth installation in which visitors 
step in front of a backdrop and interact with the program running on acomputer with a webcam, and then 
a typographicphotostrip can be printed. Previous exhibition experiencehas revealed that people largely 
enjoy manipulating thetype by making faces. As the project progressed, therewas much interest in a standalone 
application that people could try on their own. Digital typography becomes an engaging individual 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	</section>
	<section>
		<section_id>1836894</section_id>
		<sort_key>490</sort_key>
		<section_seq_no>4</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Games]]></section_title>
		<section_page_from>46</section_page_from>
	<article_rec>
		<article_id>1836895</article_id>
		<sort_key>500</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>46</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[<i>Agritainment</i>]]></title>
		<subtitle><![CDATA[3D collaborative space for training agricultural experience with entertainment elements]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836845.1836895</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1836895</url>
		<abstract>
			<par><![CDATA[<p>We propose an Agritainment (agriculture with entertainment) framework, where users can learn how to cultivate plants and to breed livestock. To make an agricultural training joyful, we implement 3D collaborative space for training agricultural experience, which transforms monotonous training experience into realistic experience. Technical details include (1) multi-user networking, (2) realistic plant grow-up modeling, and (3) story-telling approach for immersive experience.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[agriculture]]></kw>
			<kw><![CDATA[crop/livestock]]></kw>
			<kw><![CDATA[multi-user]]></kw>
			<kw><![CDATA[simulation]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>H.5.3</cat_node>
				<descriptor>Collaborative computing</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003130</concept_id>
				<concept_desc>CCS->Human-centered computing->Collaborative and social computing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Human Factors</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2264332</person_id>
				<author_profile_id><![CDATA[81474678249]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Hwan-Soo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yoo]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Seoul, Jeon-Nong, Seoul, South Korea]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264333</person_id>
				<author_profile_id><![CDATA[81322498167]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Seong-Whan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kim]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Seoul, Jeon-Nong, Seoul, South Korea]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[
Tom L. Richard, Van Ouwerkerk, and Rob Anex. 2007. I-FARM - A decision tool for biomass production. 29th Symposium on Biotechnology for Fuels and Chemicals
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[
Christopher L. 2010, http://www.simagri.com
]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Agritainment: 3D Collaborative Space for Training Agricultural Experience with Entertainment Elements 
  Hwan-Soo Yoo and Seong-Whan Kim Department of Computer Science, University of Seoul, Jeon-Nong, Seoul, 
South Korea 130-740* (a)         (b)          (c)          (d)  Snapshot: 
(a) crop simulator with sowing seed, (b) crop simulator with disease and insect simulator, (c) economic 
simulator with buying seeds (d) map tool Abstract We propose an Agritainment (agriculture with entertainment) 
framework, where users can learn how to cultivate plants and to breed livestock. To make an agricultural 
training joyful, we implement 3D collaborative space for training agricultural experience, which transforms 
monotonous training experience into realistic experience. Technical details include (1) multi-user networking, 
(2) realistic plant grow-up modeling, and (3) story-telling approach for immersive experience. Keywords: 
agriculture, simulation, multi-user, crop/livestock 1. Introduction and Motivation *E-mail:{novalite, 
swkim7} @ uos.ac.kr Farming simulation and game experience has been presented by various formats. On-line 
agriculture simulation game provides textual information with photo images on farm working [Christophe 
L.2010], and on-line agriculture simulator has been focused on mass simulation of farming and breeding 
[Tom L. Richard et al. 2007]. However, users cannot experience what is really farming and breeding by 
non-realistic text and photo images in such simulators and image based games. In this paper, we implement 
a 3D virtual collaborative space for agriculture training using multi-user on-line game framework. To 
make it more realistic, we design and implement a simulation engine, which supports (1) L-system generator 
with smooth interpolation scheme for plant model, (2) historic environment simulation for weather, disease, 
and insect simulation, and economic changes. 2. Our Approach Agritainment system is based on our 3D game 
engine platform, and the details of client and server engines/tools are as follows. (1) Client engine: 
it comprise of OpenGL based 3D simulation engine, network interaction engine, GUI interface, and quest 
and tutorial engine, (2) Server and SNS (social networking system) engine: it comprises of database and 
multi-user server module, XML parser (we implement XML based agriculture simulators), network module, 
search module, and user management module including visiting users farm management. Based on our 3D game 
engine development, we focus on storyline design and realistic farming simulation. The details are as 
follows. (1) Storyline design tool: it comprise of farm map tool, characterization tool (for user and 
NPC animation), automatic UI design generation tool, and resource packing tool, (2) Crop simulator: we 
provide six most popular crops (carrot, potato, pepper, strawberry, cabbage, melon, pumpkin and sweet 
potato). We simulate installing pillar, sowing seed, fertilizing, thinning out, weeding, tie-up crops 
and harvesting process based on crop database, (3) Livestock simulator: we simulate three livestock (cow, 
pig and chicken) based on livestock database. Livestock can be vaccinated to prevent disease and user 
can clean animal husbandry for keeping livestock clean. (4) Weather simulator: based on the user selection 
on the specific area in South Korea, we simulate temperature and precipitation effect using last ten 
years of historic weather data for the area, (5) Disease and insect simulator: we define an activation 
condition for disease and insect occurrence, and once met, we simulate dieback, downy mildew, plague, 
anthracnose, tobacco budworm and click beetle based on disease and insects database. User can buy insecticide 
and spray it to cure crops, (6) Fertilizer simulator: we simulate nitrogen, phosphates and potash fertilizer. 
user can buy and shot fertilizer, (7) Business simulator: we simulate user s buying seed and breed, user 
s selling crops and adult livestock, user s storing harvested crops for selling it at proper price. We 
use 2010 s wholesale crop price and livestock price of Korea, 3. Evaluation We tested our system with 
sample group who have strong interest in farming, and the result show that most users can learn the basic 
agricultural knowledge without fatigue or give-up. References [1] TOM L. RICHARD, VAN OUWERKERK, AND 
ROB ANEX. 2007. I-FARM - A decision tool for biomass production. 29th Symposium on Biotechnology for 
Fuels and Chemicals [2] CHRISTOPHER L.2010, http://www.simagri.com 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1836896</article_id>
		<sort_key>510</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>47</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Basic elements on game design for interactive museum exhibitions]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836845.1836896</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1836896</url>
		<abstract>
			<par><![CDATA[<p>Recent results in educational research suggest the benefits of creating learning atmospheres in which students actively engage with the material as well as other classmates [1]. The idea of creating such an environment using a multiplayer mobile game represents a natural extension of the ubiquitous audio guides offered by most museums today.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>K.8.0</cat_node>
				<descriptor>Games</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10011007.10010940.10010941.10010969.10010970</concept_id>
				<concept_desc>CCS->Software and its engineering->Software organization and properties->Contextual software domains->Virtual worlds software->Interactive games</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002951.10003227.10003251.10003258</concept_id>
				<concept_desc>CCS->Information systems->Information systems applications->Multimedia information systems->Massively multiplayer online games</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010476.10011187.10011190</concept_id>
				<concept_desc>CCS->Applied computing->Computers in other domains->Personal computers and PC applications->Computer games</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2264334</person_id>
				<author_profile_id><![CDATA[81466640799]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Liliana]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Vega]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Instituto Tecnol&#243;gico de Culiac&#225;n, Sinaloa, M&#233;xico]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264335</person_id>
				<author_profile_id><![CDATA[81466644571]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Griselda]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ledezma]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Instituto Tecnol&#243;gico de Culiac&#225;n, Sinaloa, M&#233;xico]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264336</person_id>
				<author_profile_id><![CDATA[81466643579]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Anayeli]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hidalgo]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Instituto Tecnol&#243;gico de Culiac&#225;n, Sinaloa, M&#233;xico]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264337</person_id>
				<author_profile_id><![CDATA[81466646860]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Eduardo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ruiz]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Instituto Tecnol&#243;gico de Culiac&#225;n, Sinaloa, M&#233;xico]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264338</person_id>
				<author_profile_id><![CDATA[81466648522]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Omar]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Pinto]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Instituto Tecnol&#243;gico de Culiac&#225;n, Sinaloa, M&#233;xico]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264339</person_id>
				<author_profile_id><![CDATA[81320494143]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Ricardo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Quintero]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Instituto Tecnol&#243;gico de Culiac&#225;n, Sinaloa, M&#233;xico]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264340</person_id>
				<author_profile_id><![CDATA[81318499106]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>7</seq_no>
				<first_name><![CDATA[Leopoldo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Zepeda]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Instituto Tecnol&#243;gico de Culiac&#225;n, Sinaloa, M&#233;xico]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[ILCE- Instituto Latinoamericano de la Comunicaci&#243;n Educativa. http://www.ilce.edu]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Project in collaboration with Centro de Ciencias de Sinaloa. http://www.ccs.net.mx]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Basic Elements on Game Design for Interactive Museum Exhibitions Liliana Vega, Griselda Ledezma, Anayeli 
Hidalgo, Eduardo Ruiz, Omar Pinto, Ricardo Quintero, Leopoldo Zepeda. Instituto Tecnológico de Culiacán. 
Juan de Dios Batiz s/n. CP 80220. Sinaloa, México. {vega.itc@salazarvega.net} 1 Introduction Recent 
results in educational research suggest the benefits of creating learning atmospheres in which students 
actively engage with the material as well as other classmates [1]. The idea of creating such an environment 
using a multiplayer mobile game represents a natural extension of the ubiquitous audio guides offered 
by most museums today. Aventura evolutiva is a videogame prototype [2], platform genre, with five progressive 
levels that summarizes human evolution main stages. The avatar transforms it-self from Dryopithecus (the 
first hominid), into Homo sapiens facing many situations that need to be solved with the appropriate 
tools and activities of each species. 2 Exposition The most interesting questions raised by this work 
involve the relationship between devices and elements within the game and specific pedagogic objectives. 
In this section, we introduce the design process used to convey these issues into the game design of 
our prototype: (1) Content Selection. We defined a template for the analysis and selection of exhibitions 
to identify candidates more likely to become a video game. (See template 1). Main criteria include educational 
potential, appeal, importance of topic and exhibition. Museum section: Exhibition name: Information Quality 
Exhibition Aestetics Goal: Of two different nature: entertainment or academic and pedagogic. Game mechanics: 
In terms of game rules, player actions and universe objects, as well as interactions. Global score: (from 
1 to 10.) Template 1. Exhibitions analysis for videogames candidates. (2) Content Quality Assurance. 
We conducted a research to document and validate sources and ensure the validity and certainty of the 
information used in the game´s narrative. (3) Game Design was achieved through the definition of game 
mechanics and rules, narrative based on a coherence literary script, and game concept design through 
a storyboard. (4) Application development based on SCRUM methodology, Use Cases and User Stories as complementary 
techniques to identify requirements. In addition, we used the standard IEEE 830-1998 to prioritize those. 
  3 Conclusions Videogame effectiveness was measured in two ways: by means of the mini-games that were 
incorporated at the end of each level of the videogame and by an experiment conducted on a total sample 
of 30 children, ranging from 10 to 13 years old. Randomly formed two groups of 15 children where those 
who play with the game, managed to retain more information than those who do not played. These result 
proved that the use of a videogame to communicate information allows the player to acquire and develop 
skills that will facilitate the production of new knowledge. However, more research needs to be done 
to ensure and validate proper knowledge acquisition and definition of an assessment system within the 
videogame. [1] ILCE- Instituto Latinoamericano de la Comunicación Educativa. http://www.ilce.edu [2] 
Project in collaboration with Centro de Ciencias de Sinaloa. http://www.ccs.net.mx Copyright is held 
by the author / owner(s). SIGGRAPH 2010, Los Angeles, California, July 25 29, 2010. ISBN 978-1-4503-0210-4/10/0007 
  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1836897</article_id>
		<sort_key>520</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>48</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Cognitive games as therapy for children with FAS]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836845.1836897</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1836897</url>
		<abstract>
			<par><![CDATA[<p>Therapies that help restore abilities in individuals with brain damage are being investigated to help individuals with FAS. These methods focus on rehabilitation and exercises for the brain which improve specific cognitive capacities. We present <i>Cognitive Carnival</i>, a computer game therapy based on cognitive exercises, designed to improve the child's motivation and engagement of the tasks. Three minigames were developed, each based on improving one of three cognitive prinicples: executive function, continuous performance, and working memory. These minigames will be used in controlled therapy sessions with neuropsychologists for children with FAS to determine their effectiveness as a rehabilitative tool.</p> <p>Fetal Alcohol Syndrome (FAS) is a disorder that is caused by the ingestion of alcohol during pregnancy. Alcohol is a teratogen (substance that is toxic to the developing brain) and can result in abnormal brain development (brain damage). Children with FAS are faced with numerous obstacles, including significant problems with executive functions, attention, memory, and language. These conditions impede children with FAS from succeeding in school and living normal lives.</p> <p>There is estimated to be 0.5 to 2.0 children diagnosed with FAS per 1,000 births in the United States during the 1980's and 1990's [May and Gossage 2001]. It especially prevalent in remote communities. There is no cure. However, therapies that help restore abilities in individuals with brain damage are being explored to help individuals with FAS. These methods focus on rehabilitiation by means of an intervention by psychological professionals. The therapies are able to leverage the brain's plasticity to improve cognitive function [Neu 2002]. While adult brains show low levels of plasticity, children have more neurons and their brains continue to grow into their early 20's. Consequently, neurogenesis can be leveraged by supervised mental exercise.</p> <p>Classical therapy involves a trained therapist visiting local school, often times for a single child, to administer the therapy. The therapy itself consists of a set of exercises which the child preforms. This model of therapy, while effective, is inefficient and often times impractical for many areas. Additionally, the therapy has no builtin reward system and often times the therapist will offer the child candy or a small prize which provides little engagement with the tasks themselves.</p> <p>The minigames are intended for a controlled environment where a child with FAS is supervised by a neuroscientist. Over a course of weeks, the child will play each minigame, progressing through the difficulty levels as their abilities increase.</p> <p>A game-based therapy has multiple advantages over traditional exercises. Games tend to be more engaging than paper exercises. They also can accommodate built-in reward and motivation systems, instead of requiring the alternative of real-world incentives as the sole motivation for completing the tasks. Possibly the the most significant advantage is the ability to easily distribute the system using the Internet. This allows it to easily reach remote areas, where FAS is prevalent.</p> <p>Our therapy targeted three cognitive abilities: continuous performance, working memory, and executive function. Four minigames were created, each embodying at least one of the fundamental cognitive abilities affected by FAS. We decided to divide the therapy into minigames as each minigame could focus primarily on a single cognitive principle. This narrowed focus allowed neuropsychologists to measure progress on particular abilities. This also allowed for separate starting ability parameters for the children.</p> <p>The minigames focused on three cognitive principles typically impaired by FAS: continuous performance, working memory, and executive function. Continuous performance is the ability to sustain a consistent focus to an ongoing task continuing over an extended time period. Children without this ability may be at a major disadvantage in learning settings to their healthy counterparts. Working memory is the ability to temporarily store and manipulate information. It is an important basis for complex cognitive processes. Executive function is the ability to plan, problem solve, and make decisions.</p> <p>Each minigame has built-in difficulty levels, and the ability to manually adjust parameters for an individually-tailored therapy. Levels are then subdivided into repeated trials of the same parameters. The user progresses through each trial, receiving positive and negative feedback as appropriate, according to his or her performance. Upon the completion of a session, the user is presented with his or her results and a progress plot of their recent trials.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.2.1</cat_node>
				<descriptor>Medicine and science</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>K.8.0</cat_node>
				<descriptor>Games</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010476.10011187.10011190</concept_id>
				<concept_desc>CCS->Applied computing->Computers in other domains->Personal computers and PC applications->Computer games</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10011007.10010940.10010941.10010969.10010970</concept_id>
				<concept_desc>CCS->Software and its engineering->Software organization and properties->Contextual software domains->Virtual worlds software->Interactive games</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010444</concept_id>
				<concept_desc>CCS->Applied computing->Life and medical sciences</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002951.10003227.10003251.10003258</concept_id>
				<concept_desc>CCS->Information systems->Information systems applications->Multimedia information systems->Massively multiplayer online games</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Human Factors</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2264341</person_id>
				<author_profile_id><![CDATA[81464674772]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[David]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bartle]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Victoria]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264342</person_id>
				<author_profile_id><![CDATA[81335496578]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Sam]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Rossoff]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Victoria]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264343</person_id>
				<author_profile_id><![CDATA[81350579283]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[David]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Whittaker]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Victoria]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264344</person_id>
				<author_profile_id><![CDATA[81100057328]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Bruce]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gooch]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Victoria]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264345</person_id>
				<author_profile_id><![CDATA[81466647174]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Kim]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kerns]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Victoria]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264346</person_id>
				<author_profile_id><![CDATA[81466647580]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Jenny]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[MacSween]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Victoria]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[May, P. A., and Gossage, J. P. 2001. Estimating the prevalence of fetal alcohol syndrome. <i>Alcohol Research and Health 25</i>, 3, 159.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[2002. Neurogenesis in adult primate neocortex: an evaluation of the evidence. <i>Nature Reviews Neuroscience</i>, 3, 465--71.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Piselli, P., Claypool, M., and Doyle, J. 2006. <i>Relating Cognitive Models of Computer Games to User Evaluations of Entertainment</i>. Master's thesis, Worcester Polytechnic Institute.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1209307</ref_obj_id>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Prensky, M. 2001. <i>Digital Game-Based Learning</i>. McGraw-Hill.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Whittaker, D. 2009. <i>Dynamic Database for Game Development</i>. Master's thesis, University of Victoria.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Cognitive Games as Therapy for Children with FAS David Bartle* Kim Kerns¶ Sam Rossoff Jenny MacSweenl 
David Whittaker Department of Psychology Bruce Gooch§ University of Victoria Department of Computer Science 
University of Victoria Abstract Therapies that help restore abilities in individuals with brain dam­age 
are being investigated to help individuals with FAS. These methods focus on rehabilitation and exercises 
for the brain which improve speci.c cognitive capacities. We present Cognitive Car­nival, a computer 
game therapy based on cognitive exercises, de­signed to improve the child s motivation and engagement 
of the tasks. Three minigames were developed, each based on improving one of three cognitive prinicples: 
executive function, continuous performance, and working memory. These minigames will be used in controlled 
therapy sessions with neuropsychologists for children with FAS to determine their effectiveness as a 
rehabilitative tool. Fetal Alcohol Syndrome (FAS) is a disorder that is caused by the ingestion of alcohol 
during pregnancy. Alcohol is a teratogen (substance that is toxic to the developing brain) and can result 
in abnormal brain development (brain damage). Children with FAS are faced with numerous obstacles, including 
signi.cant problems with executive functions, attention, memory, and language. These conditions impede 
children with FAS from succeeding in school and living normal lives. There is estimated to be 0.5 to 
2.0 children diagnosed with FAS per 1,000 births in the United States during the 1980 s and 1990 s [May 
and Gossage 2001]. It especially prevalent in remote communities. There is no cure. However, therapies 
that help restore abilities in individuals with brain damage are being explored to help individuals with 
FAS. These methods focus on rehabilitiation by means of an intervention by psychological professionals. 
The therapies are able to leverage the brain s plasticity to improve cognitive function [Neu 2002]. While 
adult brains show low levels of plasticity, children have more neurons and their brains continue to grow 
into their early 20 s. Consequently, neurogenesis can be leveraged by supervised mental exercise. Classical 
therapy involves a trained therapist visiting local school, often times for a single child, to administer 
the therapy. The ther­apy itself consists of a set of exercises which the child preforms. This model 
of therapy, while effective, is inef.cient and often times impractical for many areas. Additionally, 
the therapy has no built­in reward system and often times the therapist will offer the child candy or 
a small prize which provides little engagement with the tasks themselves. The minigames are intended 
for a controlled environment where a child with FAS is supervised by a neuroscientist. Over a course 
of weeks, the child will play each minigame, progressing through the dif.culty levels as their abilities 
increase. *e-mail: dwbartle@uvic.ca e-mail:tzenes@gmail.com greytone@gmail.com §brucegooch@gmail.com 
¶e-mail:kkerns@uvic.ca Ie-mail:macsween@uvic.ca A game-based therapy has multiple advantages over tradi­tional 
exercises. Games tend to be more engaging than paper exercises. They also can accommodate built-in reward 
and moti­vation systems, instead of requiring the alternative of real-world incentives as the sole motivation 
for completing the tasks. Possibly the the most signi.cant advantage is the ability to easily distribute 
the system using the Internet. This allows it to easily reach remote areas, where FAS is prevalent. Our 
therapy targeted three cognitive abilities: continuous performance, working memory, and executive function. 
Four minigames were created, each embodying at least one of the fundamental cognitive abilities affected 
by FAS. We decided to divide the therapy into minigames as each minigame could focus primarily on a single 
cognitive principle. This narrowed focus allowed neuropsychologists to measure progress on particular 
abilities. This also allowed for separate starting ability parameters for the children. The minigames 
focused on three cognitive principles typically im­paired by FAS: continuous performance, working memory, 
and ex­ecutive function. Continuous performance is the ability to sustain a consistent focus to an ongoing 
task continuing over an extended time period. Children without this ability may be at a major disad­vantage 
in learning settings to their healthy counterparts. Working memory is the ability to temporarily store 
and manipulate infor­mation. It is an important basis for complex cognitive processes. Executive function 
is the ability to plan, problem solve, and make decisions. Each minigame has built-in dif.culty levels, 
and the ability to man­ually adjust parameters for an individually-tailored therapy. Levels are then 
subdivided into repeated trials of the same parameters. The user progresses through each trial, receiving 
positive and negative feedback as appropriate, according to his or her performance. Upon the completion 
of a session, the user is presented with his or her re­sults and a progress plot of their recent trials. 
 References MAY, P. A., AND GOSSAGE, J. P. 2001. Estimating the prevalence of fetal alcohol syndrome. 
Alcohol Research and Health 25, 3, 159. 2002. Neurogenesis in adult primate neocortex: an evaluation 
of the evidence. Nature Reviews Neuroscience, 3, 465 71. PISELLI, P., CLAYPOOL, M., AND DOYLE, J. 2006. 
Relating Cognitive Models of Computer Games to User Evaluations of Entertainment. Master s thesis, Worcester 
Polytechnic Institute. PRENSKY, M. 2001. Digital Game-Based Learning. McGraw-Hill. WHITTAKER, D. 2009. 
Dynamic Database for Game Develop­ment. Master s thesis, University of Victoria. Copyright is held by 
the author / owner(s). SIGGRAPH 2010, Los Angeles, California, July 25 29, 2010. ISBN 978-1-4503-0210-4/10/0007 
  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1836898</article_id>
		<sort_key>530</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>49</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[Improving program productivity, performance and portability through a high level language for graphics and game development]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836845.1836898</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1836898</url>
		<abstract>
			<par><![CDATA[<p>Our work focuses on the area of using a high level language to improve program productivity, performance and portability. In general, this has been an area of intense research. There are a number of previous efforts including ZPL [Chamberlain and et al 2004], X10/Fortress/Chapel from IBM/SUN/Cray [Weiland 2007], Intel's CT/RapidMind [McCool 2006] and parallel VSIPL++ [Lebak and et al 2005] to name a few. However, while these languages do great things in simplifying parallel implementation of code, extensions beyond that are limited. The primary exception to this is VSIPL++ which implements several high level functions useful to the signal processing community. While most of these languages can be used to implement graphics or game related algorithms if necessary, none of them attempt to provide a platform that makes such development particularly easy. On the other hand, high level engines such as Renderman and Unreal provide the wanted abstractions but with little or no guarantees about extensibility, portability, or parallel performance. Our research focuses on adapting the parallel VSIPL++ API from the signal processing community to the graphics and game development environment.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.4</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.6</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2264347</person_id>
				<author_profile_id><![CDATA[81466640605]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[James]]></first_name>
				<middle_name><![CDATA[R.]]></middle_name>
				<last_name><![CDATA[Geraci]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Square Enix Research Center]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264348</person_id>
				<author_profile_id><![CDATA[81466648575]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Erek]]></first_name>
				<middle_name><![CDATA[R.]]></middle_name>
				<last_name><![CDATA[Speed]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Square Enix Research Center]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Chamberlain, B. L., and et al. 2004. The high-level parallel language ZPL improves productivity and performance. Proceedings of the IEEE International Workshop on Productivity and Performance in High-End Computing.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Lebak, J., and et al. 2005. Parallel VSIPL++: An open standard software library for high-performance parallel signal processing. <i>Proceedings of the IEEE 93</i>, 2 (February), 313--330.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[McCool, M. D. 2006. Data-parallel programming on the Cell BE and the GPU using the RapidMind development platform. GSPx Multicore Applications Conference, Santa Clara.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Weiland, M. 2007. Chapel, Fortress and X10: novel languages for HPC. Tech. rep., EPCC, The University of Edinburgh, October.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Improving program productivity, performance and portability through a high level language for graphics 
and game development James R. Geraci Erek R. Speed Square Enix Research Center* 1 Introduction Our work 
focuses on the area of using a high level language to im­prove program productivity, performance and 
portability. In gen­eral, this has been an area of intense research. There are a number of previous efforts 
including ZPL [Chamberlain and et al 2004], X10/Fortress/Chapel from IBM/SUN/Cray [Weiland 2007], Intel 
s CT/RapidMind [McCool 2006] and parallel VSIPL++ [Lebak and et al 2005] to name a few. However, while 
these languages do great things in simplifying parallel implementation of code, extensions beyond that 
are limited. The primary exception to this is VSIPL++ which implements several high level functions useful 
to the sig­nal processing community. While most of these languages can be used to implement graphics 
or game related algorithms if necessary, none of them attempt to provide a platform that makes such devel­opment 
particularly easy. On the other hand, high level engines such as Renderman and Unreal provide the wanted 
abstractions but with little or no guarantees about extensibility, portability, or par­allel performance. 
Our research focuses on adapting the parallel VSIPL++ API from the signal processing community to the 
graph­ics and game development environment. The choice of parallel VSIPL++ as the starting point bears 
some discussion. Our goal was to have the base language provide as much of the needed parallel framework 
as possible while being easy to extend into our goal domain. VISPL++ s map construct .lls the .rst requirement, 
though it is unclear if it does it best. For instance, Intel s CT/Rapidmind abstracts away even this 
map construct by as­signing data to computing units dynamically with an ef.cient run­time system. However, 
for a game development environment, such an add-on would be a large step away from current programming 
paradigms and possibly a hindrance to the various closed systems in the gaming world. Moreover, because 
VSIPL++ is an open API with capable extensible implementations, something none of the other options can 
provide, it is a ready target for language research and development. VSIPL++ s implementation of a similar 
level of functionality in its primary domain provides an excellent example of what one might expect while 
building a domain speci.c exten­sion of VSPIL++. 2 Contributions We contribute three main results: we 
extended the VSIPL++ API with a ray/triangle intersection function, made data maps distribute across 
an architecture s compute units instead of just its processors as is presently done, and demonstrate 
the performance, productivity and portability of the API by implementing a Monte Carlo path tracer that 
can run on any one of 4 different platforms without any code modi.cation. First, we extended the VSIPL++ 
API with a ray/triangle intersec­tion function. This is signi.cant as it is the basis function for many 
algorithms in rendering and conceptually similar to the ob­ject/object intersections used in physics. 
Thus, by showing we can implement a high level cross platform API for rendering, we have also shown that 
it is theoretically possible to do the same for * e-mail:geraci,speed@square-enix.com physics. Our function 
runs on parallel x86 processors (or Pow­erPC processors), the Cell Broadband Engine (PS3) across multi­ple 
Synergistic Processor Elements, or on a GPU using the stream-ing/CUDA units of the GPU. Next, we modi.ed 
how parallel VSIPL++ maps data. Presently, parallel VSIPL++ maps data across system processors. This 
hides system coprocessors from developers. We modify VSIPL++ maps to run across a set of homogeneous 
computational units. These computational units could be from any number of different com­putational architecture 
families. For example, they could be x86 processors, the streaming units on a Graphics Processing Unit, 
or the Synergistic Processor Elements on a Cell Broadband Engine. We demonstrate the usefulness of our 
API by implementing a Monte Carlo path tracer in VSIPL++ for games and run it on 4 extremely different 
hardware architectures without changing any code. We present performance results for each platform and 
dis­cuss implementation dif.culties we encountered when writing at such a high level. 3 Conclusions 
We conclude our presentation with a discussion of future work. We re primarily left with one question: 
What is the optimal set of high level functions for the domain? Each added function is an opportunity 
for both usefulness and bloat that harms usability. We discuss which functions we think should be included 
and solicit feedback. References CHAMBERLAIN, B. L., AND ET AL. 2004. The high-level parallel language 
ZPL improves productivity and performance. Proceed­ings of the IEEE International Workshop on Productivity 
and Performance in High-End Computing. LEBAK, J., AND ET AL. 2005. Parallel VSIPL++: An open stan­dard 
software library for high-performance parallel signal pro­cessing. Proceedings of the IEEE 93, 2 (February), 
313 330. MCCOOL, M. D. 2006. Data-parallel programming on the Cell BE and the GPU using the RapidMind 
development platform. GSPx Multicore Applications Conference, Santa Clara. WEILAND, M. 2007. Chapel, 
Fortress and X10: novel languages for HPC. Tech. rep., EPCC, The University of Edinburgh, Octo­ber. Copyright 
is held by the author / owner(s). SIGGRAPH 2010, Los Angeles, California, July 25 29, 2010. ISBN 978-1-4503-0210-4/10/0007 
  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1836899</article_id>
		<sort_key>540</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>50</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>5</seq_no>
		<title><![CDATA[iPad mini-games connected to an educational social networking website]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836845.1836899</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1836899</url>
		<abstract>
			<par><![CDATA[<p>The goal of the project was to design an integrated system for the California Academy of Sciences that combined new technology (iPads in our case) with a social-networking based website to promote educational learning geared towards middle-school students. The experience begins when museum visitors create profiles on the California Academy of Sciences website. Initially they are able to personalize a limited number of characteristics of their avatars. Once they visit the museum, they play mini-games on iPad kiosks to accumulate points on their accounts. We developed five different educational mini-games, focusing on the areas of climate change, astronomy, evolution, and the food chain. The points gained on the iPad mini-games can then be redeemed at home by returning to the California Academy of Sciences website. Accessing the website from home allows the user to further personalize an avatar, learn more facts, and compare their scores on the mini-games and their avatar with those of their peers. Points can be redeemed to upgrade the avatar's available attributes and attires. By extending the museum experience to home and through increased level of social network interaction, learning is reinforced over a longer period of time. In a user testing session with 50 students of the target demographic age, 72% said they would be interested in redeeming their points online. They also had the opportunity to write a fact that they learned from the game. 67% of students that played the camouflage game (n=12) were able to state a fact that they learned from the game. Utilizing new technologies like the iPad is an opportunity to increase the number of initial users that create profiles on a new educational socialnetworking game website. Future research can focus on determining the extent of the educational effect of a system like this. For example, how might the experience of learning at the museum, enforcing that learning at home, and repeating through return visits to the website affect retention of facts?</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[edutainment]]></kw>
			<kw><![CDATA[enhanced museum experience]]></kw>
			<kw><![CDATA[new genres of entertainment technology]]></kw>
			<kw><![CDATA[social-networking]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>K.8.0</cat_node>
				<descriptor>Games</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>H.5.3</cat_node>
				<descriptor>Web-based interaction</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010476.10011187.10011190</concept_id>
				<concept_desc>CCS->Applied computing->Computers in other domains->Personal computers and PC applications->Computer games</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10011007.10010940.10010941.10010969.10010970</concept_id>
				<concept_desc>CCS->Software and its engineering->Software organization and properties->Contextual software domains->Virtual worlds software->Interactive games</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010868</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Web-based interaction</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002951.10003227.10003251.10003258</concept_id>
				<concept_desc>CCS->Information systems->Information systems applications->Multimedia information systems->Massively multiplayer online games</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2264349</person_id>
				<author_profile_id><![CDATA[81466646693]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Madhuri]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Koushik]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Carnegie Mellon University, Entertainment Technology Center, Pittsburgh, PA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264350</person_id>
				<author_profile_id><![CDATA[81466646000]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Eun]]></first_name>
				<middle_name><![CDATA[Jung]]></middle_name>
				<last_name><![CDATA[Lee]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Carnegie Mellon University, Entertainment Technology Center, Pittsburgh, PA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264351</person_id>
				<author_profile_id><![CDATA[81466647265]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Laura]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Pieroni]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Carnegie Mellon University, Entertainment Technology Center, Pittsburgh, PA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264352</person_id>
				<author_profile_id><![CDATA[81416604325]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Emily]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sun]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Carnegie Mellon University, Entertainment Technology Center, Pittsburgh, PA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264353</person_id>
				<author_profile_id><![CDATA[81466641116]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Chun-Wei]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yeh]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Carnegie Mellon University, Entertainment Technology Center, Pittsburgh, PA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>2153683</ref_obj_id>
				<ref_obj_pid>2153634</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[
Martin, J., Trummer, C. Personalized multimedia information system for museums and exhibitions. In: Intelligent Technologies for Interactive Entertainment. LNCS, vol. 3814, pp. 332--335. Springer, Heidelberg (2005).
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1082021</ref_obj_id>
				<ref_obj_pid>1081992</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[
His, S., Fait, H. RFID enhances visitors' museum experience at the Exploratorium. In: Communications of the ACM, vol (48), pp. 60--65. ACM: New York (2005).
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1581088</ref_obj_id>
				<ref_obj_pid>1581073</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[
Jarvinen, A. Game design for social networks: interaction design for playful dispositions. In: ACM SIGGRAPH Symposium on Video Games, pp. 95--102. ACM: New York (2009).
]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 iPad mini-games Connected to an Educational Social Networking Website Madhuri Koushik1 , Eun Jung Lee1, 
Laura Pieroni1, Emily Sun1, Chun-Wei Yeh1 [authors listed in alphabetical order] 1 Carnegie Mellon University, 
Entertainment Technology Center, 700 Technology Drive, Pittsburgh, PA 15219 Abstract Keywords: edutainment, 
new genres of entertainment technology, social-networking, enhanced museum experience. The goal of the 
project was to design an integrated system for the California Academy of Sciences that combined new technology 
(iPads in our case) with a social-networking based website to promote educational learning geared towards 
middle-school students. The experience begins when museum visitors create profiles on the California 
Academy of Sciences website. Initially they are able to personalize a limited number of characteristics 
of their avatars. Once they visit the museum, they play mini-games on iPad kiosks to accumulate points 
on their accounts. We developed five different educational mini-games, focusing on the areas of climate 
change, astronomy, evolution, and the food chain. The points gained on the iPad mini-games can then be 
redeemed at home by returning to the California Academy of Sciences website. Accessing the website from 
home allows the user to further personalize an avatar, learn more facts, and compare their scores on 
the mini-games and their avatar with those of their peers. Points can be redeemed to upgrade the avatar 
s available attributes and attires. By extending the museum experience to home and through increased 
level of social network interaction, learning is reinforced over a longer period of time. In a user testing 
session with 50 students of the target demographic age, 72% said they would be interested in redeeming 
their points online. They also had the opportunity to write a fact that they learned from the game. 67% 
of students that played the camouflage game (n=12) were able to state a fact that they learned from the 
game. Utilizing new technologies like the iPad is an opportunity to increase the number of initial users 
that create profiles on a new educational social­networking game website. Future research can focus on 
determining the extent of the educational effect of a system like this. For example, how might the experience 
of learning at the museum, enforcing that learning at home, and repeating through return visits to the 
website affect retention of facts? References 1. Martin, J., Trummer, C. Personalized multimedia information 
system for museums and exhibitions. In: Intelligent Technologies for Interactive Entertainment. LNCS, 
vol. 3814, pp. 332-335. Springer, Heidelberg (2005). 2. His, S., Fait, H. RFID enhances visitors museum 
experience at the Exploratorium. In: Communications of the ACM, vol (48), pp. 60-65. ACM: New York (2005). 
 3. Jarvinen, A. Game design for social networks: interaction design for playful dispositions. In: ACM 
SIGGRAPH Symposium on Video Games, pp. 95-102. ACM: New York (2009).  Copyright is held by the author 
/ owner(s). SIGGRAPH 2010, Los Angeles, California, July 25 29, 2010. ISBN 978-1-4503-0210-4/10/0007 
 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1836900</article_id>
		<sort_key>550</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>51</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>6</seq_no>
		<title><![CDATA[ThermoGame]]></title>
		<subtitle><![CDATA[video game interaction system that offers dynamic temperature sensation to users]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836845.1836900</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1836900</url>
		<abstract>
			<par><![CDATA[<p>Today, many researchers reports studies about haptic, tactile or tangible art and entertainment. Particularly about temperature sensation, few interaction system has ever been presented because of it does not have good responsiveness. In this study, we shall design the video game interaction system that uses temperature sensation to users. First of all we investigate the relation of the rapidity of temperature change and user response time by using prototyped controller. Our game controller can offer temperature to users dynamically according to game situations. As a result, It was able to propose a basis of interaction system to take the temperate sensation to the game interaction.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>K.8.0</cat_node>
				<descriptor>Games</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>H.5.2</cat_node>
				<descriptor>Haptic I/O</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010476.10011187.10011190</concept_id>
				<concept_desc>CCS->Applied computing->Computers in other domains->Personal computers and PC applications->Computer games</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10011007.10010940.10010941.10010969.10010970</concept_id>
				<concept_desc>CCS->Software and its engineering->Software organization and properties->Contextual software domains->Virtual worlds software->Interactive games</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10011752</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Haptic devices</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002951.10003227.10003251.10003258</concept_id>
				<concept_desc>CCS->Information systems->Information systems applications->Multimedia information systems->Massively multiplayer online games</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2264354</person_id>
				<author_profile_id><![CDATA[81319488008]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Tetsuaki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Baba]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tokyo Metropolitan University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264355</person_id>
				<author_profile_id><![CDATA[81100274861]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Kumiko]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kushiyama]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tokyo Metropolitan University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264356</person_id>
				<author_profile_id><![CDATA[81472650104]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Kouki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Doi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[National Institute of Special Needs Education]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Rein, H. 1925. Uber die topographie der warmempfindung. In <i>Beziehungen zwischen Innervation und torishen Endorganen.</i>, Zeitschrift fur Biologie, vol. 82, 513--535.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 ThermoGame: Video Game Interaction System that Offers Dynamic Temperature Sensation to Users Tetsuaki 
BABA. Kumiko KUSHIYAMA Kouki DOI Tokyo Metropolitan University Tokyo Metropolitan University National 
Institute of Special Needs Education 1 INTRODUCTION Today, many researchers reports studies about haptic, 
tactile or tan­gible art and entertainment. Particularly about temperature sensa­tion, few interaction 
system has ever been presented because of it does not have good responsiveness. In this study, we shall 
design the video game interaction system that uses temperature sensation to users. First of all we investigate 
the relation of the rapidity of temperature change and user response time by using prototyped controller. 
Our game controller can offer temperature to users dy­namically according to game situations. As a result, 
It was able to propose a basis of interaction system to take the temperate sensation to the game interaction. 
 2 EXPERIMENT Figure1 shows the graph of amount of temperature change per sec­ond of our prototype game 
controller. By using the controller, we have an user test in order to know how long time users take to 
get noticed from the moment of heating/cooling. Figure 2 shows the result of user experiment about response 
time. The heating re­sponse takes about twice times of cooling response. This is due to difference of 
the number of cold sensation and hot sensation on the palm[Rein 1925]. There are two peltier elements 
on the both side of the controller. Figure 3 is an screenshot from thermo cam-era(APISTE, FSV-7000), 
which shows the left side of the controller is heated by the peltier element. Figure 1: The graph shows 
the relation between difference of tem­perature per second of heating and cooling.  Figure 2: User response 
time to temperature sensation from peltier elements on the con­troller Figure 3: Screenshot from thermo-camera 
while an user hold the heated controller. .e-mail: baba@sd.tmu.ac.jp e-mail:kushi@sd.tmu.ac.jp e-mail:doi@nise.go.jp 
 Copyright is held by the author / owner(s). SIGGRAPH 2010, Los Angeles, California, July 25 29, 2010. 
ISBN 978-1-4503-0210-4/10/0007  3 IMPLEMENTATION Figure 4 shows our controller device. As we mentioned 
it, it has two peltier elements on the both side. The controller is connected to PC via RS-232 and controlled 
by game software. After these steps, we have made video games, one is to use heating, the other is cooling.Figure 
5 shows screenshots of the video game. For ex­ample, one game contents which is named Eruption Jumping 
is an robot action game. A player operates a robot. When the robot is near by the heating or cooling 
position, the controller offers tem­perature sensation to the player according to the situation.  Figure 
4: left:Front of the controller,right:A peltier element (15x15[mm]) on the side.  Figure 5: left:Screenshot 
of menu,right:Game example:Eruption Jumping 4 GOAL The goal of our study is to establish the basis of 
temperature sen­sation interaction system with peltier elements in the .eld of enter­tainment. As a future 
work, we shall create a new type of game system that enables blind users to play by using temperature 
sensa­tion. References REIN, H. 1925. Uber die topographie der warmemp.ndung. In Beziehungen zwischen 
Innervation und torishen Endorganen., Zeitschrift fur Biologie, vol. 82, 513 535. 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	</section>
	<section>
		<section_id>1836901</section_id>
		<sort_key>560</sort_key>
		<section_seq_no>5</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Hardware]]></section_title>
		<section_page_from>52</section_page_from>
	<article_rec>
		<article_id>1836902</article_id>
		<sort_key>570</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>52</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[3D audio-visual display using mobile devices]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836845.1836902</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1836902</url>
		<abstract>
			<par><![CDATA[<p>For providing high presence, many kinds of display system have been developed [Hughes et al. 2005]. Typical examples are 3-dimensional display and multi-channel surround speaker system. Moreover, 3D movie such as "Avatar" or 3D TV have been brought to the market. However, these high realistic displays for visual, sound, or both, were usually composed of fixed and expensive equipment.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2264357</person_id>
				<author_profile_id><![CDATA[81466642698]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Kotaro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Takahashi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Graduate School of Engineering Kanazawa Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264358</person_id>
				<author_profile_id><![CDATA[81442605405]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Tomohito]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yamamoto]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[College of Information Science and Human Communication Kanazawa Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1100872</ref_obj_id>
				<ref_obj_pid>1100858</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[
Hughes, C. E., Stapleton, C. B., Hughes, D. E., and Smith, E. M. 2005. Mixed reality in education, entertainment, and training. <i>IEEE Computer Graphics and Applications 25</i>, 6, 24--30.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[
Takahashi, K., Ikeda, S., and Yamamoto, T. 2009. Light aural display using network connected multiple computers. In <i>HCI International 2009 - Posters</i>, Springer, 401--405.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 3D Audio-Visual Display Using Mobile Devices Kotaro Takahashi * Graduate School of Engineering Kanazawa 
Institute of Technology Tomohito Yamamoto College of Information Science and Human Communication Kanazawa 
Institute of Technology 6RXQG6RXQG &#38;OLHQW ,QIRUPDWLRQ  6RXQG FOLHQWV FDOFXODWH WKH VWDQGDUG YROXPH 
YDOXH .%. E\ WKH IROORZLQJ IRUPXOD % _ +6 &#38;+ . &#38;6 _ QIRUPDWLR 7KH UHSURGXFWLRQ YROXPH LV 
FDOFXODWHG LQ WKH IROORZLQJ IRUPXOD  %  9ROXPH>G%@ 0D[ YROXPH>G%@  ORJ % .  . % . 0RWLRQ 6RXQG 
&#38;OLHQW  9ROXPH>G%@ 0D[ YROXPH>G%@ . %  LQ 5HDO :RUOG  Figure 1: System Overview Figure 2: Method 
of Sound Reproduction Figure 3: Manipulation of Virtual Object 1 Introduction For providing high presence, 
many kinds of display system have been developed [Hughes et al. 2005]. Typical examples are 3­dimensional 
display and multi-channel surround speaker system. Moreover, 3D movie such as Avatar or 3D TV have been 
brought to the market. However, these high realistic displays for visual, sound, or both, were usually 
composed of .xed and expensive equipment. On the other hand, with the development of mobile and miniatur­ization 
technology, potable devises such as a netbook or a smart phone have been widespread. These devises enabled 
a user to ac­cess information anytime and anyplace. If it is possible to build the high presence system 
by such portable devises, user can experience the high presence every time and everywhere. In this research, 
using multiple mobile devises connected to a lo­cal area network, we developed the 3D audio-visual display 
system which could adjust the scale and constitution to user s environment.  2 Our Approach Our system 
is composed of multiple terminals such as a PC or an iPhone as shown in Figure 1. Each terminal is connected 
to LAN and these are divided into one server, one visual client and multiple sound clients. In this system, 
at .rst, the server simulates a vir­tual space which is designed freely by a user. After simulation, 
the server transmits the information of the virtual space to terminals by UDP multicast. Then, terminals 
present the high presence to a user using received information. To reproduce spacial sound, sound clients 
calculate distance decre­ment of sound using three positions (a listener s position, a sound source s 
position and a sound client s position in Figure 2). Using these positions, the system calculates the 
volume in real time and reproduces a spatial sound by the volume difference of each client [Takahashi 
et al. 2009]. To represent spacial view, visual client uses a head mount display and a web-camera. For 
presenting spacial view, the visual client de­ *e-mail: k-takahashi@eagle-net.ne.jp e-mail:tyama@neptune.kanazawa-it.ac.jp 
 tects user s posture by head tracking sensor, and re.ects it to view­ing angle in the virtual space. 
Then, the spacial mixture view which is composed of the virtual space and a real view from web-camera 
is displayed to a user through the HMD. Using marker tracking, a user can also operate the virtual object 
as shown in Figure 3. The visual client recognizes a marker in web-camera s view. Then the visual client 
analyzes a position of the marker and re.ects it to the object s position. Moreover, our system can represent 
the virtual space to multi-user simultaneously using the global sever which is placed on the inter­net. 
To present the virtual space to multi-user, the global server sim­ulates the virtual space, and sends 
it to servers using TCP. Servers adjust received information to own client s constitution and trans­mit 
it to own clients. This system is basically implemented as a software level , and al­lows any device 
to become the terminal if the device can be con­nected to network. Therefore, using devices such as desktop, 
lap­top, or smart phone which have been widespread, our system can be built relatively inexpensive. 
3 Future works Present system uses earth magnetism sensor for mixture of virtual and real world. In future 
work, for improving consistency between virtual and real world, the method called monoSLAM will be im­plemented. 
Also, present aural display represents surround using only the volume difference of each speaker. For 
higher expression, additional sound elements such as reverberant and re.ected sound should be considered. 
 References HUGHES, C. E., STAPLETON, C. B., HUGHES, D. E., AND SMITH, E. M. 2005. Mixed reality in education, 
entertainment, and training. IEEE Computer Graphics and Applications 25, 6, 24 30. TAKAHASHI, K., IKEDA, 
S., AND YAMAMOTO, T. 2009. Light aural display using network connected multiple computers. In HCI International 
2009 -Posters, Springer, 401 405. Copyright is held by the author / owner(s). SIGGRAPH 2010, Los Angeles, 
California, July 25 29, 2010. ISBN 978-1-4503-0210-4/10/0007  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1836903</article_id>
		<sort_key>580</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>53</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[A work-efficient GPU algorithm for level set segmentation]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836845.1836903</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1836903</url>
		<abstract>
			<par><![CDATA[<p>We present a novel GPU level set segmentation algorithm that is both work-efficient and step-efficient. Our algorithm has <i>O</i>(log <i>n</i>) step-complexity, in contrast to previous GPU algorithms [Lefohn et al. 2004; Jeong et al. 2009] which have <i>O</i>(<i>n</i>) step-complexity. Moreover our algorithm limits the active computational domain to the minimal set of changing elements by examining both the temporal and spatial derivatives of the level set field. We apply our algorithm to 3D medical images (Figure 1) and demonstrate that our algorithm reduces the total number of processed level set field elements by 16x and is 14x faster than previous GPU algorithms with no reduction in segmentation accuracy.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.1</cat_node>
				<descriptor>Graphics processors</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.6</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245.10010247</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems->Image segmentation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010389</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Graphics processors</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245.10010248</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems->Video segmentation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2264359</person_id>
				<author_profile_id><![CDATA[81407594046]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Mike]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Roberts]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Calgary]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264360</person_id>
				<author_profile_id><![CDATA[81100306501]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Mario]]></first_name>
				<middle_name><![CDATA[Costa]]></middle_name>
				<last_name><![CDATA[Sousa]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Calgary]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264361</person_id>
				<author_profile_id><![CDATA[81466640463]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Joseph]]></first_name>
				<middle_name><![CDATA[Ross]]></middle_name>
				<last_name><![CDATA[Mitchell]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Calgary]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Harris, M., Sengupta, S., and Owens, J. D. 2007. Parallel prefix sum (scan) with CUDA. In <i>GPU Gems 3</i>, H. Nguyen, Ed. Addison Wesley, Aug.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1639194</ref_obj_id>
				<ref_obj_pid>1638611</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Jeong, W.-K., Beyer, J., Hadwiger, M., Vazquez, A., Pfister, H., and Whitaker, R. T. 2009. Scalable and interactive segmentation and visualization of neural processes in EM datasets. <i>IEEE Trans. Vis. Comp. Graphics 15</i>, 6.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2231905</ref_obj_id>
				<ref_obj_pid>2231877</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Lefohn, A. E., Kniss, J. M., Hansen, C. D., and Whitaker, R. T. 2004. A streaming narrow-band algorithm: Interactive computation and visualization of level sets. <i>IEEE Trans. Vis. Comp. Graphics 10</i>, 4.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 A Work-Ef.cient GPU Algorithm for Level Set Segmentation Mike Roberts* Mario Costa Sousa* Joseph Ross 
Mitchell* University of Calgary  Figure 1: The progression of our algorithm while segmenting the brain 
matter in a 2563 head MRI with a signal-to-noise ratio of 11. Our algorithm interactively computes this 
segmentation in 7 seconds 14× faster than previous GPU algorithms with no reduction in accuracy. Abstract 
We present a novel GPU level set segmentation algo­rithm that is both work-ef.cient and step-ef.cient. 
Our algorithm has O(log n) step-complexity, in contrast to previous GPU algo­rithms [Lefohn et al. 2004; 
Jeong et al. 2009] which have O(n) step-complexity. Moreover our algorithm limits the active com­putational 
domain to the minimal set of changing elements by ex­amining both the temporal and spatial derivatives 
of the level set .eld. We apply our algorithm to 3D medical images (Figure 1) and demonstrate that our 
algorithm reduces the total number of pro­cessed level set .eld elements by 16× and is 14× faster than 
pre­vious GPU algorithms with no reduction in segmentation accuracy. Introduction Identifying distinct 
regions in images a task known as segmentation is an important task in computer vision and medi­cal 
imaging. The GPU narrow band algorithm for level set segmen­tation can compute highly accurate segmentation 
results for noisy medical images and dramatically reduces computation times com­pared to optimized CPU 
implementations. However the GPU nar­row band solver we tested took over 100 seconds converge on the 
brain matter in a 2563 head MRI on an Nvidia GTX 280 (Figure 2). This limitation constrains clinical 
applications and motivates our work-ef.cient algorithm. The GPU narrow band algorithm avoids unnecessary 
computation by only updating .eld elements near the level set surface. We make the observation that even 
computations near the level set surface can be avoided in regions where the level set .eld has locally 
converged. This observation motivates our novel method of tracking the active computational domain. Our 
Algorithm We de.ne the level set .eld value of an element x as f(x). For each iteration, we initialize 
the active computational domain for the following iteration to be empty. Then for each cur­rently active 
element a, we check to see if Vf(a) =0 and if there are any neighboring elements n around a (including 
a itself) such that df(n) dt=0 where t is simulation time. We add all such elements to the active computational 
domain for the following iteration. We summarize our GPU algorithm for generating a new dense list of 
active elements from an old one as follows: (1) output new active elements in parallel such that each 
thread outputs all the new active elements in the neighborhood around one old active element; (2) remove 
all the duplicate active elements from step 1; (3) compact all the unique new active elements from step 
2 into a new dense list. In step 1 we output the neighbors along each cardinal direction into separate 
buffers. This guarantees that each buffer contains no du­plicate elements. In step 2 we tag a 3D scratchpad 
buffer at all the *email: {mlrobert,smcosta,rmitch}@ucalgary.ca Figure 2: The active computational domain 
size (top) and speed (bottom) of our algorithm and the GPU narrow band algorithm while segmenting the 
brain matter in a 2563 head MRI. Both algo­rithms produced equally accurate segmentations. left neighbors. 
For all the right neighbors we check if they re al­ready tagged in the 3D scratchpad: if so we remove 
them remove them from the right neighbor buffer; if not we tag them in the 3D scratchpad. We repeat this 
process for all neighbor buffers. This process does not require sorting or any additional per-thread 
syn­chronization primitives (e.g atomic memory operations) since there are no duplicate elements in each 
neighbor buffer. In step 3 we use a work-ef.cient and step-ef.cient stream compaction algorithm [Harris 
et al. 2007] to compact each neighbor buffer. References HARRIS, M., SENGUPTA, S., AND OWENS, J. D. 2007. 
Parallel pre.x sum (scan) with CUDA. In GPU Gems 3, H. Nguyen, Ed. Addison Wesley, Aug. JEONG, W.-K., 
BEYER, J., HADWIGER, M., VAZQUEZ, A., PFISTER, H., AND WHITAKER, R. T. 2009. Scalable and interactive 
segmentation and visualization of neural processes in EM datasets. IEEE Trans. Vis. Comp. Graphics 15, 
6. LEFOHN, A. E., KNISS, J. M., HANSEN, C. D., AND WHITAKER, R. T. 2004. A streaming narrow-band algorithm: 
Interactive computation and visualization of level sets. IEEE Trans. Vis. Comp. Graphics 10, 4. Copyright 
is held by the author / owner(s). SIGGRAPH 2010, Los Angeles, California, July 25 29, 2010. ISBN 978-1-4503-0210-4/10/0007 
  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1836904</article_id>
		<sort_key>590</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>54</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[ATI Stream Profiler]]></title>
		<subtitle><![CDATA[a tool to optimize an OpenCL kernel on ATI Radeon GPUs]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836845.1836904</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1836904</url>
		<abstract>
			<par><![CDATA[<p>Modern GPUs have been shown to be highly efficient machines for data-parallel applications such as graphics, image, video processing, or physical simulation applications. For example, a single ATI Radeon#8482; HD 5870 GPU has a theoretical peak of 2.72 teraflops (10<sup>12</sup> floating-point operations per second) with a video memory bandwidth of 153.6 GB/s. While it is not difficult to port CPU algorithms to run on GPUs, it is extremely challenging to optimize the algorithms to achieve teraflops performance on GPUs. Only a select few expert engineers with the application domain expertise, a deep understanding of the modern GPU architecture, and an intimate knowledge of shader compiler optimization can program GPUs close to their optimal capabilities. Many developers are content with several folds of improvements rather than one or several orders of magnitude acceleration compared to their optimized CPU implementations.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.1</cat_node>
				<descriptor>Graphics processors</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010389</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Graphics processors</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Performance</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2264362</person_id>
				<author_profile_id><![CDATA[81100288316]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Budirijanto]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Purnomo]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Advanced Micro Devices, Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264363</person_id>
				<author_profile_id><![CDATA[81100267589]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Norman]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Rubin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Advanced Micro Devices, Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264364</person_id>
				<author_profile_id><![CDATA[81100541080]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Michael]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Houston]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Advanced Micro Devices, Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 ATI Stream Pro.ler: a Tool to Optimize an OpenCL Kernel on ATI Radeon GPUs Budirijanto Purnomo* Norman 
Rubin Michael Houston Advanced Micro Devices, Inc.  Figure 1: ATI Stream Pro.ler is a run-time OpenCLTM 
pro.ler developed as a Microsoft R ® Visual Studio 2008 plugin. 1 Introduction Modern GPUs have been 
shown to be highly ef.cient machines for data-parallel applications such as graphics, image, video process­ing, 
or physical simulation applications. For example, a single ATI RadeonTM HD 5870 GPU has a theoretical 
peak of 2.72 tera.ops (1012 .oating-point operations per second) with a video memory bandwidth of 153.6 
GB/s. While it is not dif.cult to port CPU al­gorithms to run on GPUs, it is extremely challenging to 
optimize the algorithms to achieve tera.ops performance on GPUs. Only a select few expert engineers with 
the application domain exper­tise, a deep understanding of the modern GPU architecture, and an intimate 
knowledge of shader compiler optimization can program GPUs close to their optimal capabilities. Many 
developers are con­tent with several folds of improvements rather than one or several orders of magnitude 
acceleration compared to their optimized CPU implementations. In this work, we share several important 
lessons we learned in the process of developing ATI Stream Pro.ler (shown in Figure 1) for OpenCLTM, 
an open standard tool for programming parallel appli­cations on many-core architectures. 2 ATI Stream 
Pro.ler The core functionality of ATI Stream Pro.ler is its ability to present a minimal set of meaningful 
and relevant performance counters de­rived from thousands of hardware raw signals supported by ATI GPUs. 
We highlight several important lessons for optimizing OpenCLTM kernel on ATI RadeonTM 5000 series GPUs. 
2.1 ALU Optimization One important measure of kernel performance is the effective rate of .oating-point 
computation (ALU) compared to the peak theoret­ical rate of the GPU. We show the utilization of the single-instruction 
multiple-data (SIMD) units in these two performance counters: ALUBusy and ALUPacking. The former is the 
rate of instruction processed by the SIMD units and the latter is the utilization of the .ve-issue architecture 
in the SIMD. Multiplying the values from these two counters gives you the percentage of the SIMD utilization. 
For ex­ample, 70% ALUBusy and 50% ALUPacking indicate the kernel is performing at 35% of the peak theoretical 
rate. Low ALUBusy indi­cates either not enough work is scheduled or ALU units are stalled *e-mail: Budirijanto.Purnomo@amd.com 
e-mail:Norman.Rubin@amd.com e-mail:Michael.Houston@amd.com due to data latency. To improve ALUPacking, 
developers can struc­ture their codes to use more vector operations and/or reducing long dependency computation 
chains. 2.2 Global Memory Optimization Since most kernels are memory-bound, it is important to optimize 
accesses to the global memory (video memory). We show the amount of data fetched from the global memory 
(FetchMem) and the amount of data written to the global memory (FastPath and CompletePath). FastPath 
and CompletePath denote the two mem­ory paths in the hardware for writing data. The former is an op­timized 
hardware path but supports only simple operations. The latter supports additional advanced operations 
including atomics and sub-32-bit (byte/short) data transfers. In our experiments, we observed an effective 
bandwidth of 20 GB/s for the CompletePath compared to 100+ GB/s for the FastPath when moving a block 
of data. Two reasons for the performance difference: (1) additional atomics data transferred for the 
CompletePath, and (2) the maxi­mum bus utilization between the shader unit and the memory unit for the 
CompletePath is 25% compared to the 100% for the Fast-Path. To improve performance, we suggest performing 
atomics as partial reductions in the local memory and running a .nal kernel pass to combine the results. 
When working with the image objects, developers need to adapt the access pattern to the data layout of 
the objects: tiled for the image objects versus linear for the buffer objects. We show the percentage 
of fetches that hit the L1 cache for the image objects (L1CacheHit). 2.3 Local Memory Optimization You 
can also reduce the data transfer to the global memory by uti­lizing the local memory (local data share, 
or LDS). This memory unit has higher bandwidth (2 terabytes/s on the ATI RadeonTM HD 5870) but limited 
size (32 KB per SIMD) compared to global mem­ory bandwidth and size. To utilize the local memory ef.ciently, 
developers need to minimize bank con.ict accesses that will be processed serially by the units. We show 
the impact of the bank con.icts in terms of time spent in the GPU in the LDSBankCon.ict counter.  3 
Future Work Currently, we are working on expanding the counter sets and inves­tigating the counter usage 
for automatic bottleneck detection. AMD, and combinations thereof, and ATI Radeon are trademarks of Advanced 
Micro Devices, Inc. The names of actual products mentioned herein may be the trademarks of their respective 
owners. Copyright is held by the author / owner(s). SIGGRAPH 2010, Los Angeles, California, July 25 
29, 2010. ISBN 978-1-4503-0210-4/10/0007  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1836905</article_id>
		<sort_key>600</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>55</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[Birds-eye view ray scan system for flatbed autostereoscopic displays]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836845.1836905</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1836905</url>
		<abstract>
			<par><![CDATA[<p>We have proposed a flatbed autostereoscopic display using the one-dimensional (1-D) integral imaging (II) method [Hirayama 2006]. 1-D cylindrical lens array (lenticular sheet) is used in the 1D-II display, making it possible to observe a three-dimensional (3-D) image with the horizontal parallax ray. The flatbed autostereoscopic display system brought about a more effective stereoscopic experience than the conventional upright display. In the flatbed display configuration, observers perceive displayed objects as if they exist on a table, because it has real depth matching with a horizontal plane and uses bird's-eye view configuration.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Raytracing</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010372.10010374</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering->Ray tracing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2264365</person_id>
				<author_profile_id><![CDATA[81421601648]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Yoshiharu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Momonoi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Core Technology Center, Toshiba Corporation Visual Products Company]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264366</person_id>
				<author_profile_id><![CDATA[81421595127]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Masahiro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sekine]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Corporate Research & Development Center, Toshiba Corporation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264367</person_id>
				<author_profile_id><![CDATA[81421596499]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Tatsuo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Saishu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Corporate Research & Development Center, Toshiba Corporation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264368</person_id>
				<author_profile_id><![CDATA[81100323314]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Yasunobu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yamauchi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Corporate Research & Development Center, Toshiba Corporation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[
Y. Hirayama, et al.: "Flatbed-type Autostereoscopic Display Systems Using Integral Imaging Method," Proceedings of IEEE International Conference on Consumer Electronics (ICCE), 2006.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1400932</ref_obj_id>
				<ref_obj_pid>1400885</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[
M. Sekine, et al.:"3-D Image Synthesis Adaptive for Autostereoscopic Displayusing Scan-type Ray Acquisition System", ACM SIGGRAPH 2008 Posters, (2008)
]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Birds-eye View Ray Scan Systemfor Flatbed AutostereoscopicDisplays Yoshiharu Momonoi* MasahiroSekine 
 TatsuoSaishu YasunobuYamauchi Core TechnologyCenter, Toshiba Corporation VisualProductsCompany Corporate 
Research &#38; Development Center, Toshiba Corporation  1 Introduction We have proposed aflatbed autostereoscopic 
display usingthe one-dimensional (1-D) integral imaging (II) method [Hirayama 2006]. 1-D cylindrical 
lensarray (lenticular sheet) is used in the 1D-II display, making it possibleto observea three-dimensional 
(3-D) image with the horizontal parallax ray. The flatbed autostereoscopic display system brought about 
a more effective stereoscopicexperiencethan theconventional upright display.In the flatbed display configuration, 
observers perceive displayed objects as if they exist on a table, because it has real depth matching 
with a horizontal plane and uses bird's-eyeview configuration. In this work, we introducea newly developed 
prototype ofa rotatable ray scansystem for the flatbed autostereoscopicdisplay system. This system realizes 
notonly fast acquisition ofmultiple high-density light rays but also a rotating mechanismof the scanning 
surface witha case.Therefore, it can capture photographic contents havingan angle of depression suitable 
for flatbed autostereoscopic display. 2 Bird s-eye View RayScan System We have proposed thesynthesis 
method adapting 3-D images basedonvarious specifications of autostereoscopicdisplayusing the scan-type 
ray acquisition system and reported the experimental results [Sekine 2008]; however, the system did not 
support tilted capturing for birds-eye view. We madeseveral improvements to thescan-type ray acquisitionsystem. 
Figure 1 shows the new prototype scanner.The main feature of thenew system is the rotatablescanning unit 
for capturing objects with depression angle.Thescanning unit of the new system canbe easily rotated with 
the case, because itis only a quarter of the weight of the old system. Basic elements ofthe new system 
are the sameasthose ofthe old system: a fixed camera, ascanning cylindrical lens, a fixed mirror, and 
two scanningmirrors.In this system, multipleraysr(x,.) can be acquired at 1,800 positions (x) with scan-pitch 
of 0.2 mm between scanning ranges of 360 mm. An image captured ata certain scanning position corresponds 
to the different raysof 110 directions (.) in a range of 50 degrees.The max scan speed is6 mm/sec, and 
the capturing of whole photographyof a 3-D object takes oneminute.  3 Adaptive for flatbed-type 3-D 
ImageSynthesis Since the rays acquiredby this system areveryhigh density and have angle of depression, 
an adaptive 3-D image can be synthesized for flatbed autostereoscopic displays. By controlling the method 
ofthe ray sampling Ri,j,k in each position (i: cylindrical lensarraynumber, j: pixel line number in acertain 
cylindrical lens, k: vertical pixel number), different depression angle fof 3-D imagescan be synthesized. 
Figure 2 shows examples of thefollowing types of ray sampling. (a) Ri,j,k = r(xi ,.j, yk) :Basicraysampling(f=0) 
 (b) Ri,j,k = r(xi+ a(f,k).j,.j, y k) : Depth correction. (c) Ri,j,k = r(xi,.j, (b(f) yk+c (f))/( d(f) 
yk+e(f))) :Vertical perspective correction  Where xi ,yk and .j arebasic sampling position and anglewithout 
depression in a position i, j, k. Usually both (b) and (c) are *e-mail:yoshiharu.momonoi@toshiba.co.jp 
Copyright is held by the author / owner(s). SIGGRAPH2010, Los Angeles, California, July25 29, 2010. ISBN 
978-1-4503-0210-4/10/0007 necessary processes for flatbed 3-D image synthesis. By changing the depth 
parameter ain the sampling method shown in (b), the 3-D images that have variousdepthsand focal positions 
can be synthesized. In flatbed 3-D image synthesis, focal positions areshifted to eachvertical position 
dependingon depression angle f. The depthcorrection parameter a is a function of vertical position k 
and depression anglef. By changing the viewing vertical perspective parametersb, c, d and e in the sampling 
method shown in(c),the 3-D images that have various vertical perspective views can be synthesized. The 
ray acquisitiondata in a certain cylindrical lenshasvertical perspective. In flatbed 3-D image synthesis,it 
is necessary to correct vertical perspective parametersb, c, d and e. Case Mirror Mirror Mirror Camera 
f Cylindrical Lens Scan y . x ' Figure 1:Birds-eyeView Ray Scan System. l, n , m RR l , n- 1,m R 
l- 1 , n , m R l, n+1, m RR l, n- 1 , m l,n , m R l, n- 1, m R l, n, m l, n+ 1, m R l, n + 1, m R R 
l+1, n - 1, m k  Sc an ing Cy lin dri c al Len s Len ti c u l a r S he et on Li qui d Cr y s t a l 
D i s pl a y x  . y lm ln , . , m ) r ( x l i j  R i, j , k : R a y s R a diate d fr om t h e D 
i s p lay R a y Sam p l i ng o f j in i = l Ra y Sa m p l i n g o f j in i = l  y y f k  k ( c 
)  R a y Ac qui s i t i on D i spl a y e d ( a )( b ) Figure 2: 3-D image synthesis with ray sampling. 
 (a) Beforecorrection (b)After correction Figure3: Output to 1-D II autostereoscopic display.  4 Results 
Weacquired rays of an object and synthesized 3-D images. Figure 3 (a)shows the output picturesof synthesized 
3-D images on 1-D II autostereoscopic display before correction. Figure 3 (b)shows the output picturesafter 
correction by using equations (a), (b) and (c). Thissystem makes it possible to represent the raysof 
a 3-D real object adaptively for flatbed autostereoscopic displays. References Y. Hirayama, et al.: 
Flatbed-type Autostereoscopic Display Systems Using Integral Imaging Method, Proceedings of IEEEInternational 
Conference on Consumer Electronics (ICCE), 2006. M. Sekine, et al.:"3-D Image SynthesisAdaptive for Autostereoscopic 
Displayusing Scan-type RayAcquisition System",ACM SIGGRAPH 2008 Posters, (2008)  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1836906</article_id>
		<sort_key>610</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>56</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>5</seq_no>
		<title><![CDATA[Camera-based calibration for scalable immersive rendering]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836845.1836906</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1836906</url>
		<abstract>
			<par><![CDATA[<p>Immersive multi-projector displays with dozens of projectors are becoming easier to build as projection technology proliferates. We envision scenarios such as a classroom of students with individual projectors, and informal groups of people with projector-equipped mobile devices, in which computer-generated imagery can be generated and displayed in real time. Ideally, the resolution of the multi-projector display should grow with the number of projectors, and support arbitrarily wide displays (to 4&pi; steradians).</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.4.1</cat_node>
				<descriptor>Camera calibration</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010234</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->Camera calibration</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2264369</person_id>
				<author_profile_id><![CDATA[81547781556]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[William]]></first_name>
				<middle_name><![CDATA[C.]]></middle_name>
				<last_name><![CDATA[Thibault]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[California State University, East Bay]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1515684</ref_obj_id>
				<ref_obj_pid>1515609</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[
Eilemann, S., Makhinya, M., and Pajarola, R. 2009. Equalizer: A scalable parallel rendering framework. <i>IEEE Transactions on Visualization and Computer Graphics 15</i>, 436--452.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>319370</ref_obj_id>
				<ref_obj_pid>319351</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[
Raskar, R., Brown, M., Yang, R., Chen, W., Welch, G., Towles, H., Seales, B., and Fuchs, H. 1999. Multiprojector displays using camera-based registration. In <i>IEEE Visualization 99.</i>
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[
Yuen, N. P. Y., and Thibault, W. C. 2008. Inexpensive immersive projection. In <i>Proceedings of IEEE Virtual Reality 2008</i>, 237--240.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Camera-based Calibration for Scalable Immersive Rendering * William C. Thibault California State University, 
East Bay Immersive multi-projector displays with dozens of projectors are becoming easiertobuildas projection 
technology proliferates.We envision scenarios such as a classroom of students with individual projectors, 
and informal groupsof peoplewithprojector-equipped mobiledevices, in which computer-generated imagery 
can be gen­erated and displayed in real time. Ideally, the resolution of the multi-projector display 
should grow with the number of projectors, and support arbitrarily wide displays (to 4p steradians). 
Our approach attempts to minimize the resampling error possible in Raskar s two-pass algorithm [Raskar 
et al. 1999] by .ttingtightly the frustum used to render the scene in the algorithm s .rst pass to the 
portion of the .eld-of-view for which the projector is responsi­ble. (We assume a .xed viewpoint, at 
the location of the calibrat­ing camera. Each projector s responsibility is to render the part of the 
scene visible from the viewpoint, which can have an arbi­trary extent if no assumptions are made about 
projector placement or display surface shape. We assume each projector s responsibil­ity is small enough 
to be rendered with a conventional perspective projection, although this canbeextendedto arbitrary .elds-of-view 
using non-linear projection[Yuen and Thibault 2008].) Care must be taken when using two-pass rendering 
for immersive displays if resampling errors are to be minimized. The simplest ap­proach to immersive 
rendering is to render to a cube-map texture in the .rst pass, and apply the texture to a model of the 
screen ge­ometryinthe second pass.However, cube-mapfaces are limitedto the maximum texture supportedby 
the GPU.Forextremely high­resolution displays, using cube-maps will not scalebeyondacertain number of 
pixels, while the portion of the .eld-of-view contributed by each projector decreases as more projectors 
are added. With our technique, each projector frustum is automatically computed to match the associated 
projector s contribution to the overall .eld­of-view of the display. For scenarios such as gigapixel 
displays createdwithlarge numbersof projectors, cubemapsfailtoprovide the resolution needed. Our technique, 
together with the scalable rendering architecture of Equalizer[Eilemann et al. 2009], allows for creating 
immersive displays with extremely high resolution and good interactive performance, without requiring 
precise physical alignment of projectors. Equalizer and similar systems typically handle displaysbuilt 
from tilesof .at screens (walls andCAVEs) well, but Equalizer s display con.guration speci.cations provide 
added generality that makes possible the creation of arbitrarily ori­ented, overlapping display segments. 
We use this .exibility to cre­ate Equalizer con.gurations from our camera-based projector cal­ibration 
data. Physical projectors need only casual alignment, and can be placed at arbitrary locations. Our camera-based 
approach is applicable to immersive displays of arbitrary .eld of view. During calibration, temporally-coded 
struc­tured light patterns are used to establish a sparse set of projector­camera correspondences. If 
the .eld of view exceeds that of the camera, the calibration can be repeated with a rotated camera, and 
the correspondences used to solve for the rotation. The cam­era coordinates of each set of per-projector 
correspondences are then mapped to direction vectors based on a .sheye lens calibra-tion[Yuen and Thibault 
2008]. We use the results of the projector calibration to create the frustum used for each projector 
s render­ing, as well as a .oating-point texture to be used for the second­ *e-mail: william.thibault@csueastbay.edu 
pass warp. The frustum is determined as the smallest .eld-of-view centered at the mean camera direction 
of the projected features that includes allof the correspondences for thegiven projector.We use the Equalizer 
framework [Eilemann et al. 2009] to handle cluster synchronization and rendering: frusta speci.cations 
are written to an Equalizer con.guration .le, and the framework is extended to transparently implement 
the details of the two-pass algorithm. The .rst pass renders to an FBO, and a fragment program is used 
in the second pass to resample the rendered frame. Figure 1: Per-projector camera frusta. Figure 2:Apanoramic 
image displayed on our system. References EILEMANN, S., MAKHINYA, M., AND PAJAROLA, R. 2009. Equalizer: 
A scalable parallel rendering framework. IEEE Transactions on Visualization and Computer Graphics 15, 
436 452. RASKAR, R., BROWN, M., YANG, R., CHEN, W., WELCH, G., TOWLES, H., SEALES, B., AND FUCHS, H. 
1999. Multi­projector displays using camera-based registration. In IEEE Vi­sualization 99. YUEN, N. P. 
Y., AND THIBAULT, W. C. 2008. Inexpensive im­mersiveprojection. In Proceedings of IEEE Virtual Reality 
2008, 237 240. Copyright is held by the author / owner(s). SIGGRAPH 2010, Los Angeles, California, July 
25 29, 2010. ISBN 978-1-4503-0210-4/10/0007  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1836907</article_id>
		<sort_key>620</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>57</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>6</seq_no>
		<title><![CDATA[Car tracking and vibration test rig using Neo-Freerunner]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836845.1836907</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1836907</url>
		<abstract>
			<par><![CDATA[<p>Here we present our idea of using a cell-phone (the Neo Freerunner) for tracking a Car's location using GPS and measuring the road's quality using the accelerometer in the cell-phone. Neo-Freerunner is an open source Linux phone by Open Moko Inc. The phone can run many flavors of linux like Android, Qt, SHR etc. Here the implementation was done in SHR.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.4.8</cat_node>
				<descriptor>Tracking</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010225.10010227</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision tasks->Scene understanding</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245.10010253</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems->Tracking</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2264370</person_id>
				<author_profile_id><![CDATA[81466647874]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Sriranjan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Rasakatla]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[IIIT-H]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264371</person_id>
				<author_profile_id><![CDATA[81466642465]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Kashyap]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kompella]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MGIT]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264372</person_id>
				<author_profile_id><![CDATA[81466647163]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Krishna]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Koundinya]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MGIT]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Video showing the car's test run and GPS tracking. http://www.youtube.com/watch?v=rtS_pU9kyp8]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Video of screen capture showing the accelerations measured http://www.youtube.com/watch?v=p5fUorY2ekM]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Car tracking and Vibration Test rig using Neo-Freerunner Sriranjan Rasakatla Kashyap Kompella Krishna 
Koundinya IIIT-H MGIT MGIT infibit@gmail.com kashyap7kompella@gmail.com cosmonautkk@gmail.com  (a) (b) 
(c) Fig 1(a) Shows the laptop running the car vibration visualization software 1(b) shows GPS location 
in Google earth 1(c) shows the test car with 1. Abstract Here we present our idea of using a cell-phone 
(the Neo Freerunner) for tracking a Car s location using GPS and measuring the road s quality using the 
accelerometer in the cell-phone. Neo-Freerunner is an open source Linux phone by Open Moko Inc. The phone 
can run many flavors of linux like Android, Qt, SHR etc. Here the implementation was done in SHR. 2. 
System Description (a) (b) Fig 2(a) Shows the software architecture. 2(b)shows system schematic The 
phone s GPS server is invoked and the NMEA sentences are read from a particular socket number. For reading 
the GPS data and forwarding it to the serial COM port we developed a python code. These values were sent 
over the hardware serial port using a USB to RS232 converter connected to the phones mini USB port. At 
the laptop s end another RS232-USB converter is used. The values are read using Goops software and plotted 
onto Google earth. There was even an option to provide the GPS values to google earth on a particular 
port number but that did not work all the time and hence wasn t reliable. The accelerometer values were 
read from the phone and forwarded over the Bluetooth serial bridge between the phone and the laptop. 
At the computer s end the Ax, Ay and Az acceleration values were plotted as individual graphs. The pitch, 
yaw and roll were calculated from the inverse tangents of values along the respective acceleration axes. 
Thess were then visualized as a 3D cube s rotation. (a) (b) (c)  (d) (e) (f) Fig: 3(a) Processing Visualization 
3(b)Car brakes 3(c)Smooth Road 3(d)Bad road quality 3(e)Speed Breaker 3(f)End of journey 3. How is this 
System helpful? To our knowledge there is no tool to qualitatively and quantitatively measure the road 
s condition. The video analysis of the acceleration graphs revealed where the road was smooth, where 
there were bumps and speed breakers . More is the vibration along the respective axis in acceleration 
more bad was the road quality. Especially this test was more suited for the rugged road conditions here 
in India. The cube s vibration and sudden jerks were the same as that of the car. We feel this form of 
visualization is qualitatively better that just saying how good or bad the road is verbally. Also we 
were able to visually distinguish between the car applying brakes, the start and the end of the journey. 
We also feel this is one of the useful features of a cellphone which has not been explored much. The 
accelerometer can also be used to measure vibration of an automobile. The amount of vibration as measured 
by the accelerometer is not only a function of the road s quality but also depends on the quality of 
vehicles suspension mechanism. One need not install an external GPS to the car but connect a cellphone 
s GPS to the Car s computer to get GPS data. The same system can be implemented on any two wheeled vehicle 
like a motor-bike or a bi-cycle. 3.References : [1]Video showing the car s test run and GPS tracking 
. http://www.youtube.com/watch?v=rtS_pU9kyp8 [2]Video of screen capture showing the accelerations measured 
 http://www.youtube.com/watch?v=p5fUorY2ekM SIGGRAPH-2010 Copyright is held by the author / owner(s). 
SIGGRAPH 2010, Los Angeles, California, July 25 29, 2010. ISBN 978-1-4503-0210-4/10/0007  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1836908</article_id>
		<sort_key>630</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>58</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>7</seq_no>
		<title><![CDATA[Chroma keying between integral photography images]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836845.1836908</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1836908</url>
		<abstract>
			<par><![CDATA[<p>Chroma keying is a well-known technique for mixing two images in which a specific color of the foreground image is made transparent. When this technology is applied to integral photography (IP) images, each of which is a textured image in which images taken from hundreds of angles are integrated, it is very useful. IP is an ideal 3D display method because parallax in all directions can be obtained without the need for wearing special glasses. Moreover, it needs only simple hardware consisting of an LCD and a fly's eye lens. In particular, in the case of the extended fractional view (EFV) method [1][2], an inexpensive ready-made fly's eye lens can be used. Animation is also possible by displaying frames successively. However, creation of IP images is computationally intensive because multi-viewpoint rendering, in which a large number of images are observed from hundreds of viewpoints, is necessary. Therefore, we developed a chroma keying technology to reduce the processing time. By creating the foreground IP images and the background IP images separately and combining them later, the processing time could be reduced greatly, especially when the background was stationary.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.4.1</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.4.7</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2264373</person_id>
				<author_profile_id><![CDATA[81421598106]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Kazuhisa]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yanaka]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Kanagawa Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264374</person_id>
				<author_profile_id><![CDATA[81448599733]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Akifumi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Momose]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Kanagawa Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264375</person_id>
				<author_profile_id><![CDATA[81448600591]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Masahiko]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yoda]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Kanagawa Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[
Kazuhisa Yanaka: "Integral photography using hexagonal fly's eye lens and fractional view {6803-58}," Proc. of Electronic Imaging 2008 (included in Proc. of SPIE on CD-ROM, Volumes 6803--6822).
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1666808</ref_obj_id>
				<ref_obj_pid>1666778</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[
Masahiko Yoda, Akufumi Momose, and Kazuhisa Yanaka: "Moving integral photography using a common digital photo frame and fly's eye lens," SIGGRAPH ASIA 2009 Poster.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Chroma Keying between Integral Photography Images Kazuhisa Yanaka Akifumi Momose Masahiko Yoda Kanagawa 
Institute of Technology 1 Introduction Chroma keying is a well-known technique for mixing two images 
in which a specific color of the foreground image is made transparent. When this technology is applied 
to integral photography (IP) images, each of which is a textured image in which images taken from hundreds 
of angles are integrated, it is very useful. IP is an ideal 3D display method because parallax in all 
directions can be obtained without the need for wearing special glasses. Moreover, it needs only simple 
hardware consisting of an LCD and a fly s eye lens. In particular, in the case of the extended fractional 
view (EFV) method [1][2], an inexpensive ready-made fly s eye lens can be used. Animation is also possible 
by displaying frames successively. However, creation of IP images is computationally intensive because 
multi-viewpoint rendering, in which a large number of images are observed from hundreds of viewpoints, 
is necessary. Therefore, we developed a chroma keying technology to reduce the processing time. By creating 
the foreground IP images and the background IP images separately and combining them later, the processing 
time could be reduced greatly, especially when the background was stationary. 2 Croma keying between 
IP images Figure 1 shows an example of a croma keying. First, a foreground (FG) IP image (a) and a background 
(BG) IP image (b) were synthesized separately by using the existing method [1][2]. Here, the background 
of the FG IP image must be uniformly colored with a color that is not used in the object. In this case, 
the color was blue. Second, a key signal (c) was made from the FG IP image by extracting the specific 
colored area. Third, the Final IP image (d) was made by combining the two IP images based on the key 
signal. (a) Foreground IP image  (b) Key signal (c) Background IP image (d) Final IP image Figure 
1 Example of croma keying    3 Experiment A 12.1-inch laptop PC (Fujitsu FMV-R8250, number of pixels 
is 1280 H × 800 V, dot pitch = 0.204 mm) and a fly s eye lens (Fresnel Technologies No. 360 but the external 
size is enlarged, lens pitch = 1 mm) were used in the experiment as e-mail: yanaka@ic.kanagawa-it.ac.jp 
shown in Figure 2. The experiment showed that high quality 3D animation with full parallax was possible. 
The image quality of the IP image synthesized by the chroma keying was almost equal to that of the IP 
image made by our conventional method without using the chroma keying. And the processing time could 
be decreased to about one third. This technology is also applicable to a new interactive 3D display system 
in which only a foreground object is moved by keyboard operation as shown in Figure 3. To keep the z-coordinate 
of the moving object constant, the amount of horizontal and vertical motion is restricted to integer 
multiples of Lx and Ly respectively as shown in Figure 4. However, errors after the decimal point were 
caused because the lens pitch was not an integer multiple of the pixel pitch in the EFV method. To avoid 
the accumulation of this error, the amount of motion was calculated by the real number, and it was rounded 
off to the nearest integer at the last stage. As a result, smooth motion of the foreground object was 
achieved. Convex lens of fly s eye lens LCD Pixel  Py ~ 0.20 0.25 mm Px ~ 0.20 0.25 mm Lx =1 mm Figure 
4 Pixel pitch of LCD and lens pitch of fly s eye lens  4 Conclusion A new chroma keying technology for 
IP images was proposed, and it was shown that the technology was useful for reducing the processing time. 
In addition, it was also shown that it could be applied to a new interactive display. This technology 
seems to be applicable to games using full-parallax 3D displays. References [1] Kazuhisa Yanaka: Integral 
photography using hexagonal fly s eye lens and fractional view [6803-58], Proc. of Electronic Imaging 
2008 (included in Proc. of SPIE on CD-ROM, Volumes 6803-6822). [2] Masahiko Yoda, Akufumi Momose, and 
Kazuhisa Yanaka: Moving integral photography using a common digital photo frame and fly's eye lens, SIGGRAPH 
ASIA 2009 Poster. Copyright is held by the author / owner(s). SIGGRAPH 2010, Los Angeles, California, 
July 25 29, 2010. ISBN 978-1-4503-0210-4/10/0007  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1836909</article_id>
		<sort_key>640</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>59</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>8</seq_no>
		<title><![CDATA[Content-adaptive parallax barriers for automultiscopic 3D display]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836845.1836909</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1836909</url>
		<abstract>
			<par><![CDATA[<p>We optimize the performance of automultiscopic barrier-based displays, constructed by stacking a pair of LCD panels. To date, such displays have conventionally employed heuristically-determined parallax barriers, containing a fixed array of slits or pinholes, to provide view-dependent imagery. While recent methods adapt barriers to one or more viewers, we show that both layers can be adapted to the multi-view content as well. The resulting <i>content-adaptive parallax barriers</i> increase display brightness and frame rate. We prove that any 4D light field created by dual-stacked LCDs is the tensor product of two 2D mask functions. Thus, a pair of 1D masks only achieves a rank-1 approximation of a 2D light field. We demonstrate higher-rank approximations using temporal multiplexing.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2264376</person_id>
				<author_profile_id><![CDATA[81319495323]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Douglas]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lanman]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Brown University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264377</person_id>
				<author_profile_id><![CDATA[81442601918]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Matthew]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hirsch]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MIT Media Lab]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264378</person_id>
				<author_profile_id><![CDATA[81466646505]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Yunhee]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kim]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MIT Media Lab]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264379</person_id>
				<author_profile_id><![CDATA[81100022847]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Ramesh]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Raskar]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MIT Media Lab]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Kim, Y., Kim, J., Kang, J.-M., Jung, J.-H., Choi, H., and Lee, B. 2007. Point light source integral imaging with improved resolution and viewing angle by the use of electrically movable pinhole array. <i>Optics Express 15</i>, 26, 18253--18267.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Konrad, J., and Halle, M. 2007. 3-D displays and signal processing. <i>IEEE Signal Processing Magazine 24</i>, 6, 97--111.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Lee, D. D., and Seung, H. S. 1999. Learning the parts of objects by non-negative matrix factorization. <i>Nature 401</i>, 788--791.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Content-Adaptive Parallax Barriers for Automultiscopic 3D Display Douglas Lanman* Matthew Hirsch Yunhee 
Kim Ramesh Raskar Brown University MIT Media Lab MIT Media Lab MIT Media Lab  Figure 1: Automultiscopic 
3D display with content-adaptive parallax barriers. (Left) A 4D light .eld, represented as a set of oblique 
projections of a synthetic scene, is displayed using our dual-stacked LCD prototype, with corresponding 
photographs shown in the overlaid region. (Right) A pair of content-adaptive parallax barriers, drawn 
from a rank-15 decomposition of the reshaped 4D light .eld matrix. Such masks allow increased display 
brightness and frame rate, when compared to conventional parallax barriers [Konrad and Halle 2007]. Abstract 
We optimize the performance of automultiscopic barrier-based dis­plays, constructed by stacking a pair 
of LCD panels. To date, such displays have conventionally employed heuristically-determined parallax 
barriers, containing a .xed array of slits or pinholes, to pro­vide view-dependent imagery. While recent 
methods adapt barriers to one or more viewers, we show that both layers can be adapted to the multi-view 
content as well. The resulting content-adaptive par­allax barriers increase display brightness and frame 
rate. We prove that any 4D light .eld created by dual-stacked LCDs is the tensor product of two 2D mask 
functions. Thus, a pair of 1D masks only achieves a rank-1 approximation of a 2D light .eld. We demon­strate 
higher-rank approximations using temporal multiplexing. 1 Content-Adaptive Parallax Barriers We de.ne 
a pair of 2D masks f[i, j] and g[k, l], corresponding to the images displayed on the front and rear LCD 
panels, respectively. A 2D slice of the 4D light .eld is given by the outer product L[i, k]= f[i] . g[k]= 
f[i]g T[k]. (1) Similarly, the complete 4D light .eld is given by the tensor product L[i, j, k, l]= f[i, 
j] . g[k, l]. (2) These expressions imply a .xed mask pair only produces a rank­1 approximation of a 
2D light .eld matrix. To our knowledge, this limitation has not been previously described for dual-layer 
displays. Conventional parallax barriers result in reduced spatial resolution and image brightness. Recently, 
translated barriers have been pro­posed to eliminate spatial resolution loss [Kim et al. 2007]; here, 
a high-speed LCD sequentially displays a series of translated bar­riers. If the complete mask set is 
displayed faster than the .icker fusion threshold, no spatial resolution loss will be perceived. *e-mail: 
dlanman@brown.edu We generalize the concept of temporal multiplexing by considering all possible mask 
pairs. Any sequence of T mask pairs represents (at most) a rank-T decomposition of a 2D light .eld matrix 
as TT TT L[i, k]= ft[i] . gt[k]= ft[i]gt T[k]. (3) t=1 t=1 Thus, time-multiplexed light .eld display 
can be cast as a matrix (or more generally a tensor) approximation problem. Speci.cally, the light .eld 
matrix must be decomposed as L FG, (4) where F and G are Ni×T and T ×Nk, respectively. Since each mask 
must be non-negative, we seek a decomposition such that 1 arg min IL - FGI2 (5) W, for F, G = 0. 2 F,G 
Unlike conventional barriers, we allow a .exible .eld of view tuned to one or more viewers by specifying 
elements of the weight matrix W. General 4D light .elds are handled by reordering them as 2D matrices, 
whereas 2D masks are reordered as vectors. Equation 5 can be solved using non-negative matrix factoriza­tion 
[Lee and Seung 1999], with typical results shown above. In conclusion, we propose the resulting content-adaptive 
parallax barriers as a generalization of conventional barriers, in which both layers are jointly optimized 
depending on the multi-view content.  References KIM, Y., KIM, J., KANG, J.-M., JUNG, J.-H., CHOI, H., 
AND LEE, B. 2007. Pointlightsourceintegralimagingwithimproved resolution and viewing angle by the use 
of electrically movable pinhole array. Optics Express 15, 26, 18253 18267. KONRAD, J., AND HALLE, M. 
2007. 3-D displays and signal processing. IEEE Signal Processing Magazine 24, 6, 97 111. LEE, D.D., AND 
SEUNG,H.S.1999.Learningthepartsofobjects by non-negative matrix factorization. Nature 401, 788 791. Copyright 
is held by the author / owner(s). SIGGRAPH 2010, Los Angeles, California, July 25 29, 2010. ISBN 978-1-4503-0210-4/10/0007 
  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1836910</article_id>
		<sort_key>650</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>60</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>9</seq_no>
		<title><![CDATA[I want my virtual friends to be life size!]]></title>
		<subtitle><![CDATA[adapting Second Life to multi-screen projected environments]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836845.1836910</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1836910</url>
		<abstract>
			<par><![CDATA[<p>Second Life (SL) is a popular 3D online virtual world designed for human interaction (also known as a MUVE, or multi-user virtual environment). It typically supports 60--70 thousand concurrent users. The assets and physical environments within SL are easy to create and use, and the environments themselves are very much part of the human interaction experience. However, the typical means of accessing SL is through a single computer screen, which lessens the immersion that is inherent in such a rich 3D world. Because of this, the SL virtual world is a good candidate for adaptation to large scale immersive displays such as a CAVE#8482; or other multi projector systems.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>H.5.1</cat_node>
				<descriptor>Artificial, augmented, and virtual realities</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010392</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Mixed / augmented reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010866</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Virtual reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010866</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Virtual reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Human Factors</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2264380</person_id>
				<author_profile_id><![CDATA[81361599015]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Kip]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Haynes]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[USC's Institute for Creative Technologies]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264381</person_id>
				<author_profile_id><![CDATA[81100180689]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Jacquelyn]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Morie]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[USC's Institute for Creative Technologies]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264382</person_id>
				<author_profile_id><![CDATA[81442617673]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Eric]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Chance]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[USC's Institute for Creative Technologies]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>566639</ref_obj_id>
				<ref_obj_pid>566654</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Humphreys, G. Houston, M. Ng, R. Frank, R. Ahern, S. Kirchner, J. Klosowski, P. Chromium: A Stream Processing Framework for interactive Rendering on Clusters. In <i>ACM Transaction on Graphics</i> 2002]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Berger, F. Lindinger, C. Ziegler, W. Vrizer -- Using Arbitrary OpenGL Software in the CAVE or other Virtual -- Environments. In <i>IEEE Virtual Reality</i> 2004]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 I Want My Virtual Friends to be Life Size! Adapting Second Life to Multi-Screen Projected Environments 
 Kip Haynes* Jacquelyn Morie Eric Chance USC s Institute for Creative Technologies  Overview Second 
Life (SL) is a popular 3D online virtual world designed for human interaction (also known as a MUVE, 
or multi-user virtual environment). It typically supports 60-70 thousand concurrent users. The assets 
and physical environments within SL are easy to create and use, and the environments themselves are very 
much part of the human interaction experience. However, the typical means of accessing SL is through 
a single computer screen, which lessens the immersion that is inherent in such a rich 3D world. Because 
of this, the SL virtual world is a good candidate for adaptation to large scale immersive displays such 
as a CAVETM or other multi projector systems. We wanted to provide a solution that would utilize direct 
communi­cation between SL viewers and have minimal dependence on 3rd party libraries or interfere with 
the OpenGL rendering pipeline While there are several ongoing efforts to adapt SL to a stereo­scopic 
display, there are no freely available native adaptations to a CAVETM like or other multi-screen environment 
at the time of this writing. There are several methods to distribute OpenGL based applications onto multiple 
computers and displays such as Chro­mium, [Humphreys et al. 2002] or VRizer [Berger et al. 2004]. However, 
most of these streaming applications either require changes to the rendering code or are no longer available 
or sup­ported. Approach Our implementation uses multiple concurrent SL logins across an unlimited number 
of machines. A client-server relationship exists between the SL viewer that the user interacts with (leader) 
and all other viewers (followers). Inside the open source SL viewer, each viewer synchronizes whenever 
updateCamera() is called. Simulta­neously the leader will broadcast the position, rotation and focus 
of the camera. Figure 2: Diagram of interaction between SL viewers  Data and Display Synchronization 
The follower viewers receive the position of the leader camera and synchronization signal. Each follower 
reads its assigned rotation *e-mail: haynes@ict.usc.edu Copyright is held by the author / owner(s). 
SIGGRAPH 2010, Los Angeles, California, July 25 29, 2010. ISBN 978-1-4503-0210-4/10/0007 from a local 
config file and performs the proper translation and rotation. A customized 2D rotation was written to 
handle the trans­lation and rotation of the camera about the X axis. Some simple real time configuration 
functions have been added to assist in achieving the proper orientation and alignment. Additionally, 
each node reads its own FOV (field of view) info from a local config, because the Second Life FOV adjustment 
is difficult to set with any accuracy (slider bar). The above described approach provides a well synchronized 
multi­viewer environment using direct communication between the cli­ents. However, Second Life consists 
of multiple regions or sims, which are sections of land that are often controlled by different servers. 
Initial tests revealed that when the leader crosses a sim boundary, the followers remain mapped to the 
original sim coordi­nates and show the camera positions in the old sim. This is because the local coordinates 
of each region or sim in Second Life only form a square grid whose coordinates number from 0 255 meters. 
This required a fix on the native scripting side, enabling the follower avatars to subtly follow the 
leader around the environment and into the next sim. Crossing sim boundaries is an elusive and undocu­mented 
process, and essentially means that a user is switching from one simulation server to another. To solve 
this problem, we created a vehicle object that the follower avatars could sit on and ride in order to 
follow the leader s camera around. In order to achieve a proper sim crossing, the movement of the leader 
is always cached and used as a trajectory for the follower vehicle to be able to cross the sim. In most 
cases this works well, but if the leader zigzags back and forth across a boundary, the follower can miscalculate 
the tra­jectory.  Requirements Our implementation uses the Message Passing Interface (MPI) to handle 
all data and frame synchronization, remote launching and session management. It is multi-platform and 
is considered a low latency interconnect for high performance distributed systems. Limitations Currently 
the system is specifically designed for a large, three screen VR theater and only supports vertical rotation 
of the follower cameras. However, it would be very simple to add additional rota­tions for additional 
display environments. Also due to the rapid prototype of this project, the user currently has limited 
control of some of the native Second Life camera functions. This will be made available in a future release. 
 References HUMPHREYS, G. HOUSTON, M. NG, R. FRANK, R. AHERN, S. KIRCHNER, J. KLOSOWSKI, P. Chromium: 
A Stream Proc­essing Framework for interactive Rendering on Clusters. In ACM Transaction on Graphics 
2002 BERGER, F. LINDINGER, C. ZIEGLER, W. VRizer --Using Arbitrary OpenGL Software in the CAVE or other 
Virtual --Envi­ronments. In IEEE Virtual Reality 2004  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1836911</article_id>
		<sort_key>660</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>61</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>10</seq_no>
		<title><![CDATA[Implementation of a tabletop 3D display based on light field reproduction]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836845.1836911</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1836911</url>
		<abstract>
			<par><![CDATA[<p>A tabletop is a useful shared space for diverse collaborative tasks. If the tabletop is considered to be interface, then expression through visual sensation, especially 3D images, is an important way to engage the principal human sense. Many 3D displays that can be observed from any direction have been proposed in recent years. However, some techniques force to wear special glasses and restrict the positions from which 3D images can be viewed [Kitamura et al. 2001]. Other glasses-free 3D displays employ obstructive apparatus on the table [Jones et al. 2007].</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.4.0</cat_node>
				<descriptor>Image displays</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2264383</person_id>
				<author_profile_id><![CDATA[81100632505]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Shunsuke]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yoshida]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Universal Media Research Center]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264384</person_id>
				<author_profile_id><![CDATA[81421599744]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Sumio]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yano]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Universal Media Research Center]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264385</person_id>
				<author_profile_id><![CDATA[81100486048]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Hiroshi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ando]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Universal Media Research Center]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1278294</ref_obj_id>
				<ref_obj_pid>1278280</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[
Jones, A., McDowall, I., Yamada, H., Bolas, M., and Debevec, P. 2007. An interactive 360&#176; light field display. In <i>ACM SIGGRAPH '07 E-Tech.</i>, 13.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383285</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[
Kitamura, Y., Konishi, T., Yamamoto, S., and Kishino, F. 2001. Interactive stereoscopic display for three or more users. In <i>ACM SIGGRAPH '01</i>, 231--240.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[
Yoshida, S., Yano, S., and Ando, H. 2010. Prototyping of glasses-free table-style 3-d display for tabletop tasks. In <i>SID 2010</i>, 16.1.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Implementation of a Tabletop 3D Display Based on Light Field Reproduction Shunsuke YOSHIDA, Sumio YANO 
and Hiroshi ANDO* NICT (National Institute of Information and Communications Technology), Universal Media 
Research Center  Figure 1: Photos of produced 3D images shot from three different angles (Utah teapot 
and Stanford bunny with real paper crane) 1 Introduction A tabletop is a useful shared space for diverse 
collaborative tasks. If the tabletop is considered to be interface, then expression through visual sensation, 
especially 3D images, is an important way to en­gage the principal human sense. Many 3D displays that 
can be observed from any direction have been proposed in recent years. However, some techniques force 
to wear special glasses and restrict the positions from which 3D images can be viewed [Kitamura et al. 
2001]. Other glasses-free 3D displays employ obstructive apparatus on the table [Jones et al. 2007]. 
In this paper, we propose a novel glasses-free 3D display based on the concept of light .eld reproduction. 
We then implement a prototype that is designed to overcome the stated limitations and preserve natural 
interaction for tabletop tasks. 2 Our Approach We propose a method of reproducing a light .eld of a 
certain vol­ume on the table and creating a ring-shaped viewing area around the table. It employs a special 
optical device as a screen and an array of projectors. The screen is shaped as a cylinder or cone and 
features a special optical function for the incoming rays: it diffuses in the direction of the edge line 
of the shape and passes straight in the direction of the circumference. The projector array projects 
ap­propriate individual images onto a side surface of the screen from different directions. Each pixel 
of the images forms numerous rays passing through each projection center and the projected point of the 
pixel. These directional rays and the characteristic of the screen reproduce the light .eld according 
to the following principle. On a vertical plane, a ray enters the screen and spreads out at a certain 
angle. One of the diffused lights of the ray goes straight to the eye of the viewer. On a horizontal 
plane, the directions of rays produced from the series of projection centers are not changed after they 
pass the screen. A bunch of rays coming from different pro­jectors are concentrated and form an integrated 
image on the retina. In other words, when a certain point on the screen is viewed from different horizontal 
directions, the point is observed with different color and luminosity. This means multiple viewers can 
obtain indi­vidual stereoscopic images from any position around the table. 3 Implementation We have 
studied methods to create the conical-screens and exam­ined the directional optical characteristics [Yoshida 
et al. 2010]. In this work, we describe an approach to implementing the 3D display *e-mail:{shun, yano.s, 
h-ando}@nict.go.jp Side view Top view Series of projection centers Eye position Eye positions  Figure 
2: Con.guration of a prototype to reproduce a light .eld by employing optical devices, including a conical 
device and a new cylindrical-shaped screen that has wider diffusion power than its predecessors. We assembled 
the projection units of 96 micro LCD projectors (VGA of 8 lumens) and installed them in a circular manner 
at a pitch of 14.6 mm and a projection distance of 870 mm. To compute the color and luminosity of each 
image s pixel, we employed a ray tracing algorithm based on the geometric optics of Figure 2. Figure 
1 shows trial 3D images in several con.gurations. As a result, this prototype covered one third of the 
ideal viewing area of 360. . The image quality was a trade-off between the diffusion power of the screen 
and the pitch of the projectors. Currently, we are improving quality by analyzing those parameters in 
detail. This novel 3D display .oats virtual objects on a .at tabletop sur­face like a centerpiece. Multiple 
viewers can observe the 3D from all around the table without the use of 3D glasses. Our entire 3D imaging 
mechanism is installed underneath the table, and it does not employ any noisy or easily broken driving 
parts. It keeps the tabletop area clear and does not disturb collaborative work and nat­ural communications. 
 References JONES, A., MCDOWALL, I., YAMADA, H., BOLAS, M., AND DEBEVEC, P. 2007. An interactive 360. 
light .eld display. In ACM SIGGRAPH 07 E-Tech., 13. KITAMURA,Y., KONISHI,T., YAMAMOTO, S., AND KISHINO, 
F. 2001. Interactive stereoscopic display for three or more users. In ACM SIGGRAPH 01, 231 240. YOSHIDA, 
S., YANO, S., AND ANDO, H. 2010. Prototyping of glasses-free table-style 3-d display for tabletop tasks. 
In SID 2010, 16.1. Copyright is held by the author / owner(s). SIGGRAPH 2010, Los Angeles, California, 
July 25 29, 2010. ISBN 978-1-4503-0210-4/10/0007  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1836912</article_id>
		<sort_key>670</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>62</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>11</seq_no>
		<title><![CDATA[Low cost 3D perception sensors]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836845.1836912</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1836912</url>
		<abstract>
			<par><![CDATA[<p>Many sensors like the laser range finder, stereo vision cameras which help in building a depth perception of the world around it in 3D are very costly. Here I present the designs and prototypes of few 3D perception sensors which have been built low cost using components off the shelf. These perception sensors use structured infrared light projection. The design is miniature compared to other 3D sensors like LIDAR, Laser scanner and Time of flight cameras.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.2.9</cat_node>
				<descriptor>Sensors</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010520.10010553.10010559</concept_id>
				<concept_desc>CCS->Computer systems organization->Embedded and cyber-physical systems->Sensors and actuators</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2264386</person_id>
				<author_profile_id><![CDATA[81466647874]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Sriranjan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Rasakatla]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[International Institute of Information Technology-Hyderabad, India]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[
Song Zhang, Piesen S. Huang,"High-resolution real-time three-dimensional shape measurement" Optical Engineering45(12), 123601 (Decemeber 20060
]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Low Cost 3D perception sensors Sriranjan Rasakatla Robotics Research Lab, International Institute of 
Information Technology-Hyderabad, India Many sensors like the laser range finder, stereo vision cameras 
which help in building a depth perception of the world around it in 3D are very costly. Here I present 
the designs and prototypes of few 3D perception sensors which have been built low cost using components 
off the shelf. These perception sensors use structured infrared light projection. The design is miniature 
compared to other 3D sensors like LIDAR, Laser scanner and Time of flight cameras. The method involves 
illuminating the object with fringes which are sinusoidal phase shifted. From a series of images of the 
object illuminated with traveling fringes the phase information of each and every pixel can b e obtained. 
This pixel level phase information can be mapped to the depth. Also in most of the structured light projects 
the cost of the projector increases the total cost of experimental setup. To overcome this, a new low 
cost fringe projector was designed using an IR LED. It is named the nano-projector compared to the pocket 
sized micro projectors. Initially it was thought to use fringes formed by a laser pointer with diffraction 
grating plates but it requires the fabrication of grating plates which is expensive. Also the use of 
laser could damage the observer s eyes while experimenting or while using the sensor. To capture IR light 
a webcamera with its IR filter removed and fitted with visible light filter was used. The cost of micro-film 
development was more than the camera s cost and hence low cost alternatives were looked into for making 
the fringe films. The best among such was to print the fringes onto a transparent plastic sheet using 
a Laser printer. The problem was that the intensity of the fringes formed should to obey the equation 
below[1] I1(x, y)=I (x, y)+I (x, y)cos[F(x, y)-.). When the resulting fringe pattern was magnified the 
fringe lines were in a zigzagged fashion . This happened because the print head of the printer tried 
to approximate its position to print the image. To prevent this, lines which were of finite intensity 
(either 255 or 0) and that were in integral multiples of pixel thickness were drawn and printed. Now 
the print head s position approximation did not occur. Lines of varying thickness were printed . More 
are the fringes in the film more will be the detail in the reconstructed 3D model. One more disadvantage 
of increasing the number of fringe lines within the limited area of the projection film is that it comes 
at the cost of reduced line thickness; as a result the projected fringes will not form properly. Among 
all the fringe patterns tried and tested, fringes with 4 lines per mm were chosen. There are two reasons 
for which the fringe pattern formed using IR light was better. One, the Laser printer heats the sheet 
a bit, so the density of the print material at the center of the printed pixel was more than the density 
at the corner. For visible light this was dark enough to be opaque and for IR it was translucent. Thus 
even the constructed 3D point cloud was better using IR light than the visible light. In general longer 
wavelength of light diffracts more than the shorter wavelength. The infrared light diffracted more than 
the shorter wavelengths in the visible light and thus the edges of the fringes formed by infrared light 
were more blurred than the visible light. This thus resulted in a better 3D point cloud using IR light.The 
code was developed in C using OpenCv. As miniature as the camera and LEDs can get so will the sensor. 
Also by using more powerful IR LEDs higher projection distance can be achieved. The IR LEDs used here 
were able to * e-mail: infibit@gmail.com project the pattern up to a maximum distance of 2 meters as 
detected by the webcamera. Also the current consumption of the sensor is very low compared to a Laser 
Range finder. Figure 1.a-1.c show the exploded view, parts used and the fringes printed. 1.d-1.f show 
the illuminated object and the 3D point cloud in GNUplot.1.g shows the design of a 3D webcamera using 
nano 3 phase projectors, 1.h shows the design of sensor for autonomous cars,1.i shows camera capsule 
design. Each LED s peak current is 15mA and the camera consumes not more than 500mA.Bigger laser range 
finders like the LMS-200 weigh 4500 gm and have a power consumption of 20W. Applications: 1)A 3D web 
camera capable of generating a 3D point cloud has been designed which just costs 2 dollars higher that 
the web camera used. It can be used in gaming projects like Natal for getting depth. The smallest sensor 
presented here weighs only 23 gm.2)The same sensor can be used in autonomous cars.. Because of the simple 
design of the sensor it can be fitted into a car s headlight.3)A camera pill design is also presented 
here which compared to the existing camera pill s would help in getting 3D scans of the body s internals 
(like intestine and esophagus).Use of 3 phase nano-projection will revolutionize medical imaging and 
diagnosis.4)The sensor because of it miniature design can be fitted into the end effecter of a robots 
arm. For example it can be used on exploratory rovers arm to get 3D scan of a rock. The rove could then 
use this information to position a tool head to drill the rock. References. [1]Song Zhang,Piesen S.Huang 
, High-resolution real-time three­dimensional shape measurement Optical Engineering45(12),123601 (Decemeber 
20060 Copyright is held by the author / owner(s). SIGGRAPH 2010, Los Angeles, California, July 25 29, 
2010. ISBN 978-1-4503-0210-4/10/0007  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1836913</article_id>
		<sort_key>680</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>63</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>12</seq_no>
		<title><![CDATA[Moving Slit Light Field Display]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836845.1836913</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1836913</url>
		<abstract>
			<par><![CDATA[<p>This paper describes the design of a parallax based Moving Slit Light Field Display (MSLFD). A MSLFD shows multi parallax images, by using vertical slits in an opaque cylinder surrounding multiple static flat panel displays. It allows viewers looking towards the cylinder to see an image from any position. Currently, various forms of 3D display have been developed and flat panel 3D display has been in practical use for some time. But commercial 3D displays only show a stereogram. On the other hand, Light Field Displays have been developed for displaying the dense ray information of the space, [Endo et al. 2005; Jones et al. 2007]. These displays use a "Parallax barrier" to control the ray direction to the observer. It shows parallax images without the use of an eye glass. We describe a system to reduce the size of the display by using two dimensional Organic Light- Emitting Diodes (OLED) and a rounding slit. OLEDs can act as the dense light source array and it can be controlled line by line. This system proposes a method to synchronize the movement of OLED's line and movement of slits. It can show many images in multi orientation (Figure 1(a)). In this paper we explain the principal method of design and how to expand the resolution and views of a MSLFD.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.4.0</cat_node>
				<descriptor>Image displays</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2264387</person_id>
				<author_profile_id><![CDATA[81100485839]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Hideaki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nii]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[National University of Singapore]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264388</person_id>
				<author_profile_id><![CDATA[81326492532]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[James]]></first_name>
				<middle_name><![CDATA[Teh Keng]]></middle_name>
				<last_name><![CDATA[Soon]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[National University of Singapore]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264389</person_id>
				<author_profile_id><![CDATA[81100418633]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Adrian]]></first_name>
				<middle_name><![CDATA[David]]></middle_name>
				<last_name><![CDATA[Cheok]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[National University of Singapore]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1187314</ref_obj_id>
				<ref_obj_pid>1187297</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Endo, T., Kawakami, N., and Tachi, S. 2005. Seelinder: The cylindrical lightfield display. In <i>SIGGRAPH '05: ACM SIGGRAPH 2005 Emerging technologies</i>, ACM, New York, NY, USA.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1276427</ref_obj_id>
				<ref_obj_pid>1275808</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Jones, A., McDowall, I., Yamada, H., Bolas, M., and Debevec, P. 2007. Rendering for an interactive 360&#176; light field display. In <i>SIGGRAPH '07: ACM SIGGRAPH 2007 papers</i>, ACM, New York, NY, USA, 40.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Moving Slit Light Field Display Hideaki Nii* James Teh Keng Soon Adrian David Cheok KEIO-NUS Cute center 
KEIO-NUS Cute center KEIO-NUS Cute center National University of Singapore National University of Singapore 
National University of Singapore  Figure 1: (a)The slitismovingaroundtheOrganicLightEmittingDiodes(OLED)andthelightfromOLEDpassthisslittotheviewer 
s eye(Lookingfromtopside),(b)Con.gurationofdisplay(Lookingfromside),(c)Prototypedisplay,(d)Left(red) 
andRight(green)Images 1 Introduction This paper describes the design of a parallax based Moving Slit 
Light Field Display(MSLFD). A MSLFD shows multi parallax im­ages, by using vertical slits in an opaque 
cylinder surrounding mul­tiple static .at panel displays. It allows viewers looking towards the cylinder 
to see an image from any position. Currently, various forms of 3D display have been developed and .at 
panel 3D dis­play has been in practical use for some time. But commercial 3D displays only show a stereogram. 
On the other hand, Light Field Displays have been developed for displaying the dense ray infor­mation 
of the space, [Endo et al. 2005; Jones et al. 2007]. These displays use a Parallax barrier to control 
the ray direction to the observer. It shows parallax images without the use of an eye glass. We describe 
a system to reduce the size of the display by using two dimensional Organic Light-Emitting Diodes (OLED) 
and a round­ing slit. OLEDs can act as the dense light source array and it can be controlled line by 
line. This system proposes a method to synchro­nize the movement of OLED s line and movement of slits. 
It can show many images in multi orientation (Figure 1(a)). In this paper we explain the principal method 
of design and how to expand the resolution and views of a MSLFD. 2 System Description Our system con.guration 
is as shown in Figure 1(b). The controlled line of OLEDs follows the slit position. When the positional 
rela­tion is the desired orientation, the light ray pass through this slit to viewer s eye. The OLED 
can .ash a speci.ed line immediately making the system a light .eld display. As mentioned, the slit type 
3D display which MSLFD is based on can show images with re­stricted resolution. How to expand the horizontal 
view: Resolution is depen­dent on the slit movement speed and LED display .icker rate. The basic system 
.ashes once per pixel movement of the slit. The system can be programmed to .ash up to two times per 
pixel movement to increase its resolution. *e-mail: nii@mixedrealitylab.org e-mail:james.tks@mixedrealitylab.org 
e-mail:adriancheok@mixedrealitylab.org How to expand the vertical view:We expand the vertical view by 
shifting the light ray direction by varying sub-pixel amounts. This is done by adding different angled 
glasses be­hind the slits as shown in Figure 1(b). A normal slit shows the original image to the viewer 
s eye. By adding the angled glass behind the slit, we show the image shifted by a number of sub-pixels 
in relation to the eye.  How to have multiple view points: We use normal 2D dis­plays, but drawing a 
single line at any time, moving according to position of slit relative to the line of sight of the viewer. 
 We developed a proof of concept prototype display by using a dot matrix LED array unit (8 by 8 resolution, 
red and green) and 100mm diameter cylinder with 10 slits (Figure 1(c)). This LED unit is driven the same 
way as an OLED line by line. The signal is generated by the CPU board (XC-1 by XMOS). The display shows 
two images at the same time and each image resolution is 16 by 16 and refresh rate is over 30Hz (Figure 
1(d)).  3 Conclusion The main contribution of this system is to demonstrate a relatively simple system 
using 2D LED array to reconstruct 3D images with­out using an eye glass. Compared to previous systems, 
this system only has 2D LED and moving slits. It can achieve high display res­olution despite using relatively 
low resolution LED arrays and by displaying only two lines to a viewer at any time. Also, by using moving 
slits, combined with high resolution OLED panel, we can achieve real MSLFD system.  References ENDO,T.,KAWAKAMI,N., 
AND TACHI, S. 2005. Seelinder: The cylindrical light.eld display. In SIGGRAPH 05: ACM SIGGRAPH 2005 Emerging 
technologies,ACM, New York, NY, USA. JONES,A., MCDOWALL,I., YAMADA,H., BOLAS,M., AND DEBEVEC, P. 2007. 
Rendering for an interactive 360. light .eld display. In SIGGRAPH 07: ACM SIGGRAPH 2007 papers, ACM, 
New York, NY, USA, 40. Copyright is held by the author / owner(s). SIGGRAPH 2010, Los Angeles, California, 
July 25 29, 2010. ISBN 978-1-4503-0210-4/10/0007  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	</section>
	<section>
		<section_id>1836914</section_id>
		<sort_key>690</sort_key>
		<section_seq_no>6</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Image/Video Processing]]></section_title>
		<section_page_from>64</section_page_from>
	<article_rec>
		<article_id>1836915</article_id>
		<sort_key>700</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>64</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[A backward compatible HDR encoding scheme]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836845.1836915</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1836915</url>
		<abstract>
			<par><![CDATA[<p>Two-layer encoding schemes for HDR images (and video) can not only reduce the storage requirements, but more importantly they can also ensure backward compatibility during transition from LDR to HDR age. The first layer is a tone-mapped LDR image, which can be shown on existing displays. The second layer is another LDR image and contains the residual information lost in tone-mapping, which can be used by HDR applications.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.4.2</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010395</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image compression</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2264390</person_id>
				<author_profile_id><![CDATA[81451595530]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Ishtiaq]]></first_name>
				<middle_name><![CDATA[Rasool]]></middle_name>
				<last_name><![CDATA[Khan]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[A*STAR Institute for Infocomm Research, Singapore]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[
{Ward05} Ward G., and Simmons M. 2005. JPEG-HDR: A Backwards-Compatible, High Dynamic Range Extension to JPEG. <i>Proceedings of the Thirteenth Color Imaging Conference</i>.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1141946</ref_obj_id>
				<ref_obj_pid>1179352</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[
{Mantiuk06} Mantiuk R., Efremov A., and Myszkowski K. 2006. Backward Compatible High Dynamic Range MPEG Video Compression. SIGGRAPH.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1287926</ref_obj_id>
				<ref_obj_pid>1287835</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[
{OKUDA07} Okuda M., and Adamai N. 2007. Two-Layer Coding Algorithm for High Dynamic Range Images based on Luminance Compensation. <i>Journal of Visual Communication and Image Representation 18</i>, 5, 377--386.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 A Backward Compatible HDR Encoding Scheme Ishtiaq Rasool Khan A*STAR Institute for Infocomm Research 
, Singapore Two-layer encoding schemes for HDR images (and video) can not only reduce the storage requirements, 
but more importantly they can also ensure backward compatibility during transition from LDR to HDR age. 
The first layer is a tone-mapped LDR image, which can be shown on existing displays. The second layer 
is another LDR image and contains the residual information lost in tone-mapping, which can be used by 
HDR applications. In a scheme proposed by Ward et al [Ward05], the second layer is ratio of the original 
HDR image to the first layer LDR image. Mantiuk et al [Mantiuk06] used a reconstruction function that 
approximates the HDR image as function of the first layer image, and the residual information is stored 
in the second layer. Mantiuk et al noted that using the ratio image by Ward et al was equivalent to approximating 
the HDR image as a linear function (which did not fit the data very well) of the front layer LDR image. 
They showed that their reconstruction function which consists of 256 x-y points in which y is the average 
of all HDR values that map to a single LDR value x, better fits the data. At expense of a slight overhead 
of the reconstruction function, details of the HDR image can be better preserved at the same bit rate. 
Okuda et al [Okuda07] achieved further improvement by using a more compact reconstruction using Hill 
function that fits the data optimally in least square sense. Considering each column of the HDR vs LDR 
plot as 1-D sequence, the effect of using reconstruction functions is simply shifting the origin of the 
sequence from 0 to the value of the reconstruction function R0 in that column. This does not actually 
reduce the dynamic range of the data. The data in [Rmin, Rmax] range is simply shifted to [Rmin R0, Rmax 
R0] range. However, if the reconstruction function fits the data well, most of the values in the second 
layer image will be smaller after the shift, and therefore can be represented more accurately in the 
same number of bits. The better the reconstruction function fits the data, better would be details preserved 
in the second layer. Our method is inspired by the methods mentioned above. Starting with the reconstruction 
function used by Mantiuk et al, but instead of keeping average values for all 256 bins, we find their 
piecewise linear (PWL) approximation. A few key-points cleverly selected and joined by linear segments 
can represent the function quite accurately. This step is meant not just to reduce the storage overhead, 
but more importantly, PWL approximation helps in reducing the dynamic range of the second layer image. 
In our approximation, each segment has larger slope than the previous. Q2 P2 LDR Now consider a point 
P1 shown in figure above. It is at distance d1 from the segment right below it. Mantiuk et al and Okuda 
et al store this distance in the second layer. It can however be noted that d2, the distance of the point 
from the segment on its right, is e-mail: irkhan@i2r.a-star.edu.sg Copyright is held by the author / 
owner(s). SIGGRAPH 2010, Los Angeles, California, July 25 29, 2010. ISBN 978-1-4503-0210-4/10/0007 
 smaller than d1. We store this distance d2 in the second layer. While reversing the process to retrieve 
actual distance, the first guess would be Q1, which is above the segment below by distance d2. However 
Q1 has a distance d3 from the segment at right which is smaller than d2, and that disqualifies Q1. Therefore, 
the point will be moved to the next option P1. This procedure works for all points lying above the curve, 
because the slopes of the linear segments are in increasing order. For points lying below the curve, 
this does not work. For example, both P2 and Q2 will have same value stored in the second layer. This 
is because the segments can extend in the space below the curve and intersect each other. As a result, 
a point can be at the same vertical and/or horizontal distance from multiple lines. Therefore, for the 
points below the curve, we store the actual vertical distances. The only exception is the last segment. 
The points below it can be identified uniquely and restored irrespective of which distance (vertical 
or horizontal) is stored. We assign positive sign to the distances above or left of the curve and negative 
to those below or on right side of it.  The figure shown above gives a comparison of our proposed scheme 
(bottom) with that of Mantiuk (top). HDR luminance as function of LDR luminance for memorial church image 
and the reconstruction functions (red curves) are shown at left. Note that our function consists of very 
few points (8 in this example) compared to 256 points of Mantiuk. The residual data that need to be stored 
in the second layers are shown at right. Note that the dynamic range of our second layer is several times 
lower than that obtained by Mantiuk s method. Therefore, the information can be P1  stored more accurately 
(smaller rounding error) in our scheme. Q1 HDR d2 References d1 [Ward05] WARD G., AND SIMMONS M. 2005. 
JPEG-HDR: A Backwards-Compatible, High Dynamic Range Extension to JPEG. Proceedings of the Thirteenth 
Color Imaging Conference. [Mantiuk06] MANTIUK R., EFREMOV A., AND MYSZKOWSKI K. 2006. Backward Compatible 
High Dynamic Range MPEG Video Compression. SIGGRAPH. [OKUDA07] OKUDA M., AND ADAMAI N. 2007. Two-Layer 
Coding Algorithm for High Dynamic Range Images based on Luminance Compensation. Journal of Visual Communication 
and Image Representation 18, 5, 377-386.  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1836916</article_id>
		<sort_key>710</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>65</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[A real-time video 2D-to-3D with the bilateral grid]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836845.1836916</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1836916</url>
		<abstract>
			<par><![CDATA[<p>The new data structure, the bilateral grid, was presented by Jiawen et al. to make bilateral filter algorithm become simple implementation. Based on the data structure, the GPU CUDA-based optimization is proposed to have more efficiency in using GPU shared memory and massive multithreading. Meanwhile, a commercial application, the video 2d to 3d conversion which was presented by Ludovic et al. is also re-designed by applying the proposed CUDA-based bilateral grid three times to obtain better 3D quality in real-time. Depth map are created and modified by adjusting bilateral grid parameters.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2264391</person_id>
				<author_profile_id><![CDATA[81448599154]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Chun-Te]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Industrial Technology Research Institute, Taiwan, ROC]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264392</person_id>
				<author_profile_id><![CDATA[81466646670]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Wei-Hao]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Huang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Industrial Technology Research Institute, Taiwan, ROC]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264393</person_id>
				<author_profile_id><![CDATA[81466646582]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Chih-Hao]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Liu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Industrial Technology Research Institute, Taiwan, ROC]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264394</person_id>
				<author_profile_id><![CDATA[81448599089]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Wei-Jia]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Huang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Industrial Technology Research Institute, Taiwan, ROC]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264395</person_id>
				<author_profile_id><![CDATA[81448598796]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Kai-Che]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Liu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Industrial Technology Research Institute, Taiwan, ROC]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264396</person_id>
				<author_profile_id><![CDATA[81466648127]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Ludovic]]></first_name>
				<middle_name><![CDATA[J.]]></middle_name>
				<last_name><![CDATA[Angot]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Industrial Technology Research Institute, Taiwan, ROC]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1276506</ref_obj_id>
				<ref_obj_pid>1276377</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[
Jiawen Chen, Sylvain Paris, Fr&#233;do Durand, Real-time edge-aware image processing with the bilateral grid, ACM Transactions on Graphics (TOG), v.26 n.3, July 2007
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[
Ludovic J. Angot, Wei-Jia Huang, Kai-Che Liu. 2010. A 2D to 3D video and image conversion technique based on a bilateral filter. Proceedings of SPIE Vol. 7526.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1836917</article_id>
		<sort_key>720</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>66</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[A stereo one-shot multi-band camera system for accurate color reproduction]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836845.1836917</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1836917</url>
		<abstract>
			<par><![CDATA[<p>In the digital archiving for cultural heritage preservation, in the medical field, and in some industrial fields, high-fidelity reproduction of color, gloss, texture, and shape are very important. Multiband or full-spectrum imaging technology is a solution for accurate color reproduction. Although several types of multi band camera systems have been developed [Yamaguchi 2000, Tominaga 2000, Helling 2004, Hashimoto 2008], all of them are multi-shot systems and they cannot take images of moving objects. Ohsawa et al. [2004] have developed a six-band HDTV camera system. However, the system requires very expensive customized equipment. In order to make multiband technology pervasive, equipment costs must be reduced and the systems have to be able to take images of moving objects. To meet these requirements, we developed a novel multiband image capturing system that combines multiband and stereo imaging techniques. This system can acquire both spectral color information and depth information at the same time. In this paper, we focus on the generation of six-band images from a pair of stereo image.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.4.8</cat_node>
				<descriptor>Stereo</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.9</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010225</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision tasks</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2264397</person_id>
				<author_profile_id><![CDATA[81466642662]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Masaru]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tsuchida]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[NTT Corporation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264398</person_id>
				<author_profile_id><![CDATA[81365594058]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Toru]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Takahashi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tohoku University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264399</person_id>
				<author_profile_id><![CDATA[81416604303]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Koichi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ito]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tohoku University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264400</person_id>
				<author_profile_id><![CDATA[81100599057]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Takahito]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kawanishi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[NTT Corporation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264401</person_id>
				<author_profile_id><![CDATA[81100230754]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Junji]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yamato]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[NTT Corporation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264402</person_id>
				<author_profile_id><![CDATA[81100590294]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Takafumi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Aoki]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tohoku University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Yamaguch, M., et al., 2000. Natural Vision: Visual Telecommunication based on Multispectral Technology. <i>In proceedings of International Display Workshop 2000</i>, 1115--1118.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>877448</ref_obj_id>
				<ref_obj_pid>876866</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Tominaga, S., et al., 2000. Object Recognition by Multi-Spectral Imaging with a Liquid Crystal Filter. <i>In proceedings of Conference on Pattern Recognition, vol.1, 708--711</i>
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Helling, S., et al., 2004. Algorithms for spectral color stimulus reconstruction with a seven-channel multispectral camera. <i>In proceedings of Second European Conference on Colour in Graphics, Imaging, and Vision (CGIV2004), 254--258</i>
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Hashimoto, M., 2008. Two-Shot type 6-band still image capturing system using Commercial Digital Camera and Custom Color Filter. <i>In proceedings of Fourth European Conference on Colour in Graphics, Imaging, and Vision (CGIV2008)</i>
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Ohsawa, K., et al., 2004. Six-band HDTV camera system for spectrum-based color reproduction. <i>Journal of Imaging Science and Technology, 48, 2, 85--92.</i>
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Takita, H., et al., 2003. High-accuracy image registration based on phase-only correlation. <i>IEICE Transaction of Fundamentals, Vol. E86-A, no.8, 1925--1934.</i>
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Pratt, W. K., et al., 1976. Spectral estimation techniques for the spectral calibration of a color image scanner. <i>Applied Optics, OSA, 15, 73--75.</i>
]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 A stereo one-shot multi-band camera system for accurate color reproduction Masaru Tsuchida *, Toru Takahashi 
**, Koichi Ito **, Takahito Kawanishi *, Junji Yamato *, Takafumi Aoki ** * NTT Corporation, ** Tohoku 
University 1. Introduction In the digital archiving for cultural heritage preservation, in the medical 
field, and in some industrial fields, high-fidelity reproduction of color, gloss, texture, and shape 
are very important. Multiband or full-spectrum imaging technology is a solution for accurate color reproduction. 
Although several types of multi band camera systems have been developed [Yamaguchi 2000, Tominaga 2000, 
Helling 2004, Hashimoto 2008], all of them are multi-shot systems and they cannot take images of moving 
objects. Ohsawa et al. [2004] have developed a six-band HDTV camera system. However, the system requires 
very expensive customized equipment. In order to make multiband technology pervasive, equipment costs 
must be reduced and the systems have to be able to take images of moving objects. To meet these requirements, 
we developed a novel multiband image capturing system that combines multiband and stereo imaging techniques. 
This system can acquire both spectral color information and depth information at the same time. In this 
paper, we focus on the generation of six-band images from a pair of stereo image. 2. Stereo one-shot 
six-band camera system Our system consists of two commercial digital cameras, a custom interference filter, 
and color reproduction software. The filter is mounted in front of the lens of one camera (see top-center 
photo of Fig.1). The spectral transmittance of the interference filter and spectral sensitivities of 
the camera used in experiments are shown in Fig.2. The filter cuts off the left sides (i.e., the short-wavelength 
domain) of the peaks of both the blue and red in original spectral sensitivity of camera. It also cuts 
off the green's right side (i.e., the long-wavelength domain). The camera with the filter captures a 
specialized three-band image; the other camera captures a an ordinary RGB color image. 3. Image transformation 
and generating six-band image The shape of the image captured with the interference filter is adjusted 
to that of the other image by projective transformation. To calculate transformation parameters, correspondent 
points between the two images are detected by using the Phase-Only Correlation (POC) method [Takita 2003]. 
POC is a scale- and rotation-invariant pattern detection method that uses phase information. POC also 
has robustness against color. General detection methods cannot work well in cases where the color balance 
between two images is quite different. The resultant two three-band images are combined in to a six-band 
image. 4. Experimental results As target objects, we used old paint drawn on cloth. These cloth, but 
their surface is gently undulating and uneven. Before starting the experiments, the spectral sensitivity 
of camera, illumination spectrum, and monitor characters (primary color and tone curves) were measured. 
First, we took two images of the objects using the proposed system at the same time (see Fig.1). Here, 
we used raw data images from the image sensor, which is why the color balance of the image captured without 
the interference filter looks incorrect. Second, the image transformation process was carried out and 
a six-band image was generated. Next, the six-band image was converted into a spectral reflectance image 
through Wiener estimation [Pratt 1976]. Finally, the spectral reflectance image was converted into a 
RGB image by using the illumination spectrum of observation and monitor characters. The used for image 
reshaping in the experiments, this method is currently available for only almost flat objects. To applythe 
proposed system for three-dimensional objects, a non-linear transformation algorithm should be implemented, 
which remains as future work. In addition, the method could be extended by using three or morecamera 
systems. References YAMAGUCH, M., et al., 2000. Natural Vision: Visual Telecommunication based on Multispectral 
Technology. In proceedings of International Display Workshop 2000,1115-1118. TOMINAGA, S., et al., 2000. 
Object Recognition by Multi-SpectralImaging with a Liquid Crystal Filter. In proceedings of Conference 
on Pattern Recognition, vol.1, 708-711 HELLING, S., et al., 2004. Algorithms for spectral color stimulus 
reconstruction with a seven-channel multispectral camera. In proceedings of Second European Conference 
on Colour in Graphics, Imaging, and Vision (CGIV2004), 254-258 HASHIMOTO, M., 2008. Two-Shot type 6-band 
still image capturing system using Commercial Digital Camera and Custom Color Filter. In proceedings 
of Fourth European Conference on Colour in Graphics, Imaging, and Vision (CGIV2008), OHSAWA, K., et 
al., 2004. Six-band HDTV camera system forspectrum-based color reproduction. Journal of Imaging Science 
and Technology, 48, 2, 85-92. TAKITA, H., et al., 2003. High-accuracy image registration based onphase-only 
correlation. IEICE Transaction of Fundamentals, Vol. E86-A, no.8, 1925-1934. PRATT, W. K., et al., 1976. 
Spectral estimation techniques for the spectral calibration of a color image scanner. Applied Optics, 
OSA, 15, 73 75. Image capturing system  Image captured with filter Color reconstructed image 6-band 
image sensitivity resultant RGB image was compared with the paint and the image Figure.1 generated 
by the two-shot six-band camera system [Hashimoto 2008]. The image obtained with the proposed method 
is the same color as the 1 0.9 paint and has the same quality as the image obtained by conventional 0.8 
 0 methods. No registration errors remain among the band image transmittance 0.7 0.6 generated by the 
proposed method. 0.5 0.4 5. Summary 0.3 A novel one-shot multiband image capturing system wasintroduced 
and effectiveness of the method shown in 380 480 580 680 780 experiments. Although the projective transformation 
was Wavelength [nm] Wavelength [nm] *e-mail: tsuchida@cs.brl.ntt.co.jp Figure.2 Copyright is held by 
the author / owner(s). SIGGRAPH 2010, Los Angeles, California, July 25 29, 2010. ISBN 978-1-4503-0210-4/10/0007 
 0.2 0.1 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1836918</article_id>
		<sort_key>730</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>67</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[Belief propagation optical flow for high-resolution image morphing]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836845.1836918</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1836918</url>
		<abstract>
			<par><![CDATA[<p>Over the last decade, considerable progress has been made on the so-called early vision problems. We present an optical flow algorithm for image morphing that incorporates recent advances in feature matching, energy minimization, stereo vision and image segmentation. At the core of our flow estimation we use Efficient Belief Propagation for energy minimization. While state-of-the-art algorithms only work on thumbnail-sized images, our novel feature downsampling scheme in combination with a simple, yet efficient data term compression can cope with high-resolution data. The incorporation of SIFT features into data term computation further resolves matching ambiguities, making long-range flows possible. We detect occluded areas by evaluating the symmetry of the flow fields, we further apply Geodesic matting to automatically inpaint these regions.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[2D morphing & warping]]></kw>
			<kw><![CDATA[belief propagation]]></kw>
			<kw><![CDATA[optical flow]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061.10010063</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures->Computational geometry</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2264403</person_id>
				<author_profile_id><![CDATA[81365598480]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Christian]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lipski]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[TU Braunschweig]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264404</person_id>
				<author_profile_id><![CDATA[81331498092]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Christian]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Linz]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[TU Braunschweig]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264405</person_id>
				<author_profile_id><![CDATA[81100477906]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Marcus]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Magnor]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[TU Braunschweig]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1507622</ref_obj_id>
				<ref_obj_pid>1507617</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[
Bai, X., and Sapiro, G. 2009. Geodesic matting: A framework for fast interactive image and video segmentation and matting. <i>IJCV 82</i>, 2, 113--132.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1138220</ref_obj_id>
				<ref_obj_pid>1138215</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[
Felzenszwalb, P., and Huttenlocher, D. 2006. Efficient belief propagation for early vision. In <i>IJCV</i>, vol. 70, 41--54.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1478176</ref_obj_id>
				<ref_obj_pid>1478172</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[
Liu, C., Yuen, J., Torralba, A., Sivic, J., and Freeman, W. T. 2008. Sift flow: Dense correspondence across different scenes. In <i>ECCV '08: Proceedings of the 10th European Conference on Computer Vision</i>, 28--42.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[
Smith, B., Zhang, L., and Jin, H. 2009. Stereo matching with nonparametric smoothness priors in feature space. <i>Computer Vision and Pattern Recognition, IEEE Computer Society Conference on 0</i>, 485--492.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Belief Propagation Optical Flow for High-Resolution Image Morphing Christian Lipski, Christian Linz, 
Marcus Magnor* Computer Graphics Lab, TU Braunschweig  Figure 1: Belief Propagation Optical Flow. Left 
to right: two 1920 × 1080 input images A and B, optical .ow from A to B, .ow symmetry and rendered result. 
We speci.cally tailored our optical .ow for image morphing in the presence of large motion and occlusions. 
We incorporate recent advances in computer vision to produce visually convincing results. Abstract Over 
the last decade, considerable progress has been made on the so-called early vision problems. We present 
an optical .ow algo­rithm for image morphing that incorporates recent advances in fea­ture matching, 
energy minimization, stereo vision and image seg­mentation. At the core of our .ow estimation we use 
Ef.cient Be­lief Propagation for energy minimization. While state-of-the-art al­gorithms only work on 
thumbnail-sized images, our novel feature downsampling scheme in combination with a simple, yet ef.cient 
data term compression can cope with high-resolution data. The incorporation of SIFT features into data 
term computation further resolves matching ambiguities, making long-range .ows possible. We detect occluded 
areas by evaluating the symmetry of the .ow .elds, we further apply Geodesic matting to automatically 
inpaint these regions. Keywords: optical .ow, belief propagation, 2D Morphing &#38; Warping 1 Motivation 
With renewed research on early-vision problems, many promis­ing strategies have evolved that cope with 
these often ill-posed tasks. However, optical .ow-based image warping/morphing is still a challenging 
problem, especially when the input images feature long-range motion and large occluded areas. With the 
increasing availability of high-resolution content, the requirements for corre­spondence estimation between 
images are further increased. High resolution images often exhibit many ambiguous details, where their 
low resolution predecessors only show uniformly colored ar­eas. While incorrect correspondences in the 
low-res images are not conceivable, high resolution images suffer from blurring or ghost­ing artifacts 
in these regions. Liu et al. [2008] recently proposed an optical .ow for matching images possibly showing 
different scene content. We pick up on their idea to incorporate dense SIFT feature descriptors, yet 
we use them for a different purpose. While they identify visually similar regions in low-resolution images, 
we use them as a descriptor for .ne detail in high-resolution images. * e-mail:{lipski,linz,magnor}@cg.cs.tu-bs.de 
 2 Our Approach We cast the computation of optical .ow as a discrete labeling prob­lem and use Ef.cient 
Belief Propagation for energy minimization, as proposed by Felzenszwalb et al. [2006]. In order to match 
.ne structural details in two images, we compute a SIFT descriptor for each pixel. To avoid ambiguous 
descriptors and to speed up compu­tation, we choose one descriptor in a n × n (typically n =4) grid cell 
as its representative. We compute an initial lower resolution .ow on images that are downsampled by factor 
n. The 131-dimensional descriptor of each pixel is a combination of the mean color (3-dimensional) and 
the representative SIFT de­scriptor of this cell (128-dimensional). The L1-norm of this vec­tor describes 
dissimilarity between two pixels. While the original Belief Propagation implementation by Felzenszwalb 
et al. [2006] might not retain crisp borders due to the grid-based message pass­ing scheme, we employ 
a non-grid-like regularization technique as proposed by Smith et al. [2009]. As memory consumption of 
Be­lief Propagation on this scale is still too high for long-range cor­respondence estimation, we use 
a simple minima-preserving data term compression. During Belief Propagation, a symmetry term ensures 
consistent results. Occluded regions are identi.ed and in­painted: Assuming that each occluded area is 
surrounded by two independently moving regions, we use Geodesic Matting [Bai and Sapiro 2009] to propagate 
.ow information. The resulting .ow is upsampled to its original size and re.ned locally. References 
BAI, X., AND SAPIRO, G. 2009. Geodesic matting: A framework for fast interactive image and video segmentation 
and matting. IJCV 82, 2, 113 132. FELZENSZWALB, P., AND HUTTENLOCHER, D. 2006. Ef.cient belief propagation 
for early vision. In IJCV, vol. 70, 41 54. LIU, C., YUEN, J., TORRALBA, A., SIVIC, J., AND FREEMAN, W. 
T. 2008. Sift .ow: Dense correspondence across different scenes. In ECCV 08: Proceedings of the 10th 
European Con­ference on Computer Vision, 28 42. SMITH, B., ZHANG, L., AND JIN, H. 2009. Stereo matching 
with nonparametric smoothness priors in feature space. Com­puter Vision and Pattern Recognition, IEEE 
Computer Society Conference on 0, 485 492. Copyright is held by the author / owner(s). SIGGRAPH 2010, 
Los Angeles, California, July 25 29, 2010. ISBN 978-1-4503-0210-4/10/0007  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1836919</article_id>
		<sort_key>740</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>68</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>5</seq_no>
		<title><![CDATA[Blind-folded recognition of bank notes on the mobile phone]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836845.1836919</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1836919</url>
		<abstract>
			<par><![CDATA[<p>The identification of a bank note's value is a non-trivial task for the blind and the visually impaired. A popular approach adopted by many countries in order to facilitate the visually impaired, is the impression of a high-contrast, large-print region on their bank notes. Additionally, an approach used to facilitate the blind population is the impression of unique tactile marks on bank notes. However, even when tactile marks or different sizes (e.g. Euros) are used, blind and visually impaired people have practical difficulties in identifying them.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>C.5.3</cat_node>
				<descriptor>Portable devices (e.g., laptops, personal digital assistants)</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.8</cat_node>
				<descriptor>Object recognition</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245.10010251</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems->Object recognition</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010476.10011187</concept_id>
				<concept_desc>CCS->Applied computing->Computers in other domains->Personal computers and PC applications</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2264406</person_id>
				<author_profile_id><![CDATA[81100024120]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Stavros]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Papastavrou]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[VI Scientific Ltd]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264407</person_id>
				<author_profile_id><![CDATA[81466647305]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Demetris]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hadjiachilleos]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[VI Scientific Ltd]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264408</person_id>
				<author_profile_id><![CDATA[81339530469]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Georgios]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Stylianou]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[VI Scientific Ltd]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1414551</ref_obj_id>
				<ref_obj_pid>1414471</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[X. Liu, A Camera Phone Based Currency Reader for the Visually Impaired, ASSETS 2008.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1639688</ref_obj_id>
				<ref_obj_pid>1639642</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[R., Parlouar, F., Dramas, M. J-M, Mace, C. Jouffrais, Assistive Device for the Blind Based on Object Recognition: an Application to Identify Currency Bills, ASSETS 2009.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Note Teller 2, Brytech Inc., http://www.brytech.com/noteteller/]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Blind­Folded Recognition of Bank Notes on the Mobile Phone Stavros Papastavrou Demetris Hadjiachilleos 
Georgios Stylianou VI Scientific Ltd VI Scientific Ltd VI Scientific Ltd 1. Introduction The identification 
of a bank note s value is a non­trivial task for the blind and the visually impaired. A popular approach 
adopted by many countries in order to facilitate the visually impaired, is the impression of a high­contrast, 
large­print region on their bank notes. Additionally, an approach used to facilitate the blind population 
is the impression of unique tactile marks on bank notes. However, even when tactile marks or different 
sizes (e.g. Euros) are used, blind and visually impaired people have practical difficulties in identifying 
them. Therefore alternative methods and products for bank note recognition were proposed [1, 2, 3]. Even 
though they are mobile, unfortunately these don t recognize all bills of a single currency [1], cannot 
easily generalize to other currencies [1, 2] or are expensive and impractical for everyday use [3]. In 
this talk, we introduce a novel method for bank note recognition suitable for the blind that can so far 
recognize the Euro, Zloty and most of the US Dollar denominations. This method combines computer vision 
and pattern recognition techniques; it is fully portable and runs in real time on smartphones (symbian, 
windows mobile and iphone) with recognition speed 0.1 seconds. Furthermore, we have designed it such 
that is robust under different lighting conditions, and truly accessible to the blind by providing a 
trainer that helps the blind user to position the mobile phone over the correct side of the bank note. 
Finally, the output is read to the user by a screen reader which is always pre­installed in the user 
s mobile phone. 2. Method Overview We briefly describe the method using as an example the Euro bank 
note recognition. In the case of the US Dollar, Polish Zloty or other currencies, the method is or can 
be adjusted accordingly. Given an input RGB image (fig. 1, left), first we convert it to an intensity 
image (or gray scale image). A crucial step is the use of a novel adaptive segmentation algorithm to 
convert the intensity image to a black and white image. This algorithm allows the recognition of the 
currency s value under several different lighting environments (from very bright to near dark). The algorithm 
is: Given the intensity image we traverse every line in the image and compute the maxima and minima of 
the line s intensity poly­line. Using these we create the threshold T=(Max+Min)/2 where, Max is the average 
of the maxima and Min is the average of the minima. Using T we split the pixels to two groups: If I>T 
the pixel becomes white, if I<T the pixel becomes black, where I denotes the pixel s intensity.  Figure 
1. The left column shows three different input images. The right column shows the automatic detection 
of the number and the recognition outcome. From the resulting black and white image (fig. 1, right), 
it is relatively easy to locate the horizontal and vertical boundaries of the bank note, use them to 
locate the bottom left corner of the currency s value which is used to compute the number s height. Using 
the number s height we estimate its maximum width. The recognition of the currency s value is done by 
intersecting the digits using vertical scan­lines that generate a unique pattern for each digit. Every 
scan­line returns one of the following patterns B (1 intersection), BWB (2 intersections), BWBWB (3 intersections) 
or no intersections. The patterns B, W mean that there was an intersection with black pixels or white 
pixels, respectively. In addition for every intersection we save the number of pixels and the y­position 
of intersection. A no intersection pattern means white space and signals the end of a number. In addition, 
we can validate the intersections and disjoint the numbers if necessary (fig. 1, rows 1, 3). The patterns 
for the digits one, zero, two and five (that exist in Euro bank notes) are B, BW1B, BW1BW2B, BW1BW2B, 
respectively. As the digits two and five produce the same pattern, we distinguish them by comparing the 
white pixel areas (A) of W1 and W2. When A(W1)>A(W2), then the number is five. When A(W1)<A(W2), then 
the number is two. Finally we reconstruct the number from the digits. References [1] X. Liu, A Camera 
Phone Based Currency Reader for the Visually Impaired, ASSETS 2008. [2] R., Parlouar, F., Dramas, M. 
J­M, Mace, C. Jouffrais, Assistive Device for the Blind Based on Object Recognition: an Application to 
Identify Currency Bills, ASSETS 2009. [3] Note Teller 2, Brytech Inc., http://www.brytech.com/noteteller/ 
Copyright is held by the author / owner(s). SIGGRAPH 2010, Los Angeles, California, July 25 29, 2010. 
ISBN 978-1-4503-0210-4/10/0007  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1836920</article_id>
		<sort_key>750</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>69</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>6</seq_no>
		<title><![CDATA[Fast efficient algorithm for enhancement of low lighting video]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836845.1836920</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1836920</url>
		<abstract>
			<par><![CDATA[<p>We describe a novel and effective video enhancement algorithm for low lighting video. The algorithm works by first inverting the input low-lighting video and then applying an image de-haze algorithm on the inverted input. To facilitate faster computation and improve temporal consistency, correlations between temporally neighboring frames are utilized. Simulations using naive implementations of the algorithm show good enhancement results and 2x speed-up as compared with frame-wise enhancement algorithms, with further improvements in both quality and speed possible.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[de-hazing]]></kw>
			<kw><![CDATA[invert]]></kw>
			<kw><![CDATA[low lighting video enhancement]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.4.3</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2264409</person_id>
				<author_profile_id><![CDATA[81466643043]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Xuan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Dong]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Wuxi Jinnang Ltd.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264410</person_id>
				<author_profile_id><![CDATA[81453642946]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Yi]]></first_name>
				<middle_name><![CDATA[(Amy)]]></middle_name>
				<last_name><![CDATA[Pang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tsinghua University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264411</person_id>
				<author_profile_id><![CDATA[81540560156]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Jiangtao]]></first_name>
				<middle_name><![CDATA[(Gene)]]></middle_name>
				<last_name><![CDATA[Wen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tsinghua University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1099926</ref_obj_id>
				<ref_obj_pid>1099539</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Hau Ngo, Li Tao, Ming Zhang, Adam Livingston and Vijayan Asari. A Visibility Improvement System for Low Vision Drivers by Nonlinear Enhancement of Fused Visible and Infrared Video. Proceedings of the 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2005, pp. 25]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Night Driver, Making Driving Safer at Night, Raytheon Company, available at: http://www.nightdriversystems.com/nightdriver.html]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Fast Ef.cient Algorithmfor EnhancementofLow Lighting Video Xuan Dong* Yi(Amy)Pang Jiangtao(Gene)Wen 
Wuxi Jinnang Ltd. Tsinghua University Tsinghua University (a) (b) (c) (d) (e) Figure 1: (a) The input 
low lighting video frame I. (b) The inverted video frame R: R is obtained by inverting the input low-lighting 
video frame I. (c) The marked video frame: pixels with low intensity in at least one color (RGB) channel 
are marked in green. (d) The de-haze video frame J: J is obtained by applying the adapted de-haze algorithm 
on the inverted video frame R. (e) The .nal output video frame E: E is obtained by inverting the de-haze 
video frame J. Abstract We describeanovelandeffective video enhancement algorithmfor low lightingvideo.The 
algorithmworksby .rstinvertingtheinput low-lighting video and then applying an image de-haze algorithm 
ontheinverted input.Tofacilitatefaster computationandimprove temporal consistency, correlations between 
temporally neighboring frames are utilized. Simulations using naive implementations of the algorithm 
show good enhancement results and 2x speed-up as compared with frame-wise enhancement algorithms, with 
further improvements in both quality and speed possible. Keywords: low lighting video enhancement,invert,de-hazing 
1 Introduction As video cameras become increasingly widely deployed, the prob­lem of video enhancement 
for low lighting video has also be­come increasingly acute. This is because although camera and video 
surveillance systems are expected to work in all lighting and weather conditions, the majority of these 
cameras were not de­signed for low-lighting usage, and therefore the resulted poor cap­ture quality often 
renders the video unusable for critical applica­tions. Although infrared cameras are capable of enhancing 
visibil­ity in low-lighting conditions [1] and [2], theysuffer from the com­mon limitation that objects 
must have a temperature higher than their surroundings to be visible. In many cases where the critical 
object has a temperature similar to the background, e.g. a big hole on the road, infrared cameras are 
not very useful. Conventional video enhancement techniques such as histogram equalization may notworkwelleitherinmanycases, 
especiallyforrealtime process­ing of video sequences. *e-mail: dongxuan8811@gmail.com e-mail:pangy@mails.tsinghua.edu.cn 
e-mail:jtwen@tsinghua.edu.cn Inthispaper,we proposeanovel,simpleandeffectiveenhancement algorithm for 
low lighting video. We show that after inverting the input, pixels in the background regions of the inverted 
low-lighting video usually have high intensities in all color (RGB) channels while those of foreground 
regions usually have at least one color channel whose density is low. This is very similar to video cap­tured 
in hazy weather conditions. Therefore, we can apply state­of-the-art image de-hazing algorithms to the 
frames in the inverted video sequence for enhancement. We show that the combination of inverting the 
input and then performing de-hazing on the inver­sion resultisasimplebuteffective algorithmforlow-lighting 
video enhancement. This process is conceptually illustrated in Figure 1. To improve the ef.ciencyof the 
algorithm and the temporal consis­tency of the output video, we utilize correlations between tempo­rally 
neighboring video frames.In particular,we storethevaluesof key algorithm parameters for each processed 
frame, and perform motion estimation between neighboring frames. If the pixels in the current frame are 
determined to be suf.ciently similar to pix­els in a previously processed frame, the stored values are 
used for the corresponding pixels in the current frame, thereby by-passing a signi.cant portion of the 
computation. Simulation results us­ing naive implementations of the algorithm show 2x speed-up as compared 
with the conventional frame-by-frame approach. By im­proving the implementation of the algorithm, especially 
by using fast motion search algorithms, further improvements in both qual­ity and speed can be achieved. 
Please kindly .nd the demo video at http://media.cs.tsinghua.edu.cn/~multimedia/siggraph2010.htm.  References 
[1] Hau Ngo, LiTao,Ming Zhang, Adam Livingston andVijayan Asari.AVisibilityImprovement System forLowVision 
Drivers by Nonlinear EnhancementofFusedVisible and InfraredVideo. Proceedings of the 2005 IEEE Computer 
Society Conference on ComputerVision andPattern Recognition,2005, pp.25 [2] Night Driver, Making Driving 
Safer at Night, Raytheon Company, available at: http://www.nightdriversystems.com/nightdriver.html Copyright 
is held by the author / owner(s). SIGGRAPH 2010, Los Angeles, California, July 25 29, 2010. ISBN 978-1-4503-0210-4/10/0007 
  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1836921</article_id>
		<sort_key>760</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>70</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>7</seq_no>
		<title><![CDATA[LOD +]]></title>
		<subtitle><![CDATA[augmenting LOD with skeletons]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836845.1836921</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1836921</url>
		<abstract>
			<par><![CDATA[<p>Until now computer graphic researchers have tried to solve visualization problems introduced by the size of meshes. Modern tools produce large models and hardware is not able to render them in full resolution. For example, the digital Michelangelo project extracted a model with more than one billion polygons. One can notice hardware has become more and more powerful but meshes have also become more and more complex. To solve this issue, people have worked on many solutions. We can find solutions based on space subdivision, or based on visibility of objects like the use of a Z-buffer. But in 1976, Clark [Clark 1976] introduces the level of detail concept (LOD). The principle of LOD is the construction of several versions of the same 3D model at different resolutions. This is achieved by removing some object features. Luebke provides in [Luebke 1997] a very complete survey of LOD algorithms. The main issue with the simplification is that the mesh does not preserve appearance of the original mesh. Indeed, important features tend to disappear. For example, with the Quadric Error Metrics (QEM) algorithms and the cow mesh, the tail, horn and other characteristic points merge with the mesh at a low resolution. Our approach allows the simplified mesh to preserve important details.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.6</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2264412</person_id>
				<author_profile_id><![CDATA[81466648458]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Lange]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Benoit]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[LIRMM, Montpellier - France]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264413</person_id>
				<author_profile_id><![CDATA[81490658926]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Rodriguez]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nancy]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[LIRMM, Montpellier - France]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>360354</ref_obj_id>
				<ref_obj_pid>360349</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Clark, J. H. 1976. Hierarchical geometric models for visible surface algorithms. <i>Commun. ACM 19</i>, 10, 547--554.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>258849</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Garland, M., and Heckbert, P. S. 1997. Surface simplification using quadric error metrics. In <i>SIGGRAPH '97: Proceedings of the 24th annual conference on Computer graphics and interactive techniques</i>, ACM Press/Addison-Wesley Publishing Co., New York, NY, USA, 209--216.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Luebke, D. 1997. A survey of polygonal simplification algorithms.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Reeb, G. 1946. Sur les points singuliers d'une forme de complment intgrable ou d'une fonction numrique. <i>Comptes Rendus de L'Acadmie des Sances 222</i>, 847--849.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Tierny, J., Vandeborre, J.-P., and Daoudi, M. 2006. 3d Mesh Skeleton Extraction Using Topological and Geometrical Analyses. In <i>14th Pacific Conference on Computer Graphics and Applications (Pacific Graphics 2006)</i>, 85--94.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 LOD +: Augmenting LOD with skeletons LANGE Benoit* RODRIGUEZ Nancy LIRMM LIRMM Montpellier -France 
Montpellier -France (a) Hand at full resolution (b) Hand at a lower resolu-(c) Hand with 10% of edges 
(d) Hand with LOD+ tion Figure 1: Hand model 1 Introduction Until now computer graphic researchers have 
tried to solve visual­ization problems introduced by the size of meshes. Modern tools produce large models 
and hardware is not able to render them in full resolution. For example, the digital Michelangelo project 
ex­tracted a model with more than one billion polygons. One can no­tice hardware has become more and 
more powerful but meshes have also become more and more complex. To solve this issue, people have worked 
on many solutions. We can .nd solutions based on space subdivision, or based on visibility of objects 
like the use of a Z-buffer. But in 1976, Clark [Clark 1976] introduces the level of detail concept (LOD). 
The principle of LOD is the construction of several versions of the same 3D model at different resolutions. 
This is achieved by removing some object features. Luebke provides in [Luebke 1997] a very complete survey 
of LOD algorithms. The main issue with the simpli.cation is that the mesh does not preserve appearance 
of the original mesh. Indeed, important features tend to disappear. For example, with the Quadric Error 
Metrics (QEM) al­gorithms and the cow mesh, the tail, horn and other characteristic points merge with 
the mesh at a low resolution. Our approach al­lows the simpli.ed mesh to preserve important details. 
 2 Our Approach Our approach is based on Reeb graph theory. A Reeb graph is a data structure that extracts 
the critical points of a surface and pro­duces a skeleton. This kind of simple structure is commonly 
use to classify a mesh. Skeleton are also used in mesh animation. The most important use for a Reeb graph 
is shape analysis ([Tierny et al. *lange.benoit@lirmm.fr nancy.rodriguez@lirmm.fr 2006]). In fact this 
solution is the most powerful method to extract object critical points. In our approach we extract the 
Reeb graph in pre processing. In fact the complexity of Reeb graph extraction is cost expensive. Once 
the skeleton has been computed, we merge it on the mesh only when the model has been too much degenerated. 
At present, the solution is based on a threshold but in the future, we hope to .nd a more adapted metric. 
The simpli.cation algo­rithm used is [Garland and Heckbert 1997]. Our solution is called LOD+; it is 
one of the .rst solutions to improve LOD low resolution meshes. The .rst results which allow good shape 
recognition. The parts of the mesh who are decimate with other algorithms are kept. The main issue of 
our solution are the bones of the skeleton because they cross the mesh. We need also to improve skeleton 
visualiza­tion. Thickness of the bones made necessary to de.ne a covering strategy. We have tried some 
solutions (cylinders and boxes) but this affects performance. It stays one of the main open problems 
to solve in LOD+. 3 Acknowledgment This research was supported by the URBSIM company and the LRI (Languedoc-Roussillon 
Incubation). References CLARK, J. H. 1976. Hierarchical geometric models for visible surface algorithms. 
Commun. ACM 19, 10, 547 554. GARLAND, M., AND HECKBERT, P. S. 1997. Surface simpli.ca­tion using quadric 
error metrics. In SIGGRAPH 97: Proceed­ings of the 24th annual conference on Computer graphics and interactive 
techniques, ACM Press/Addison-Wesley Publishing Co., New York, NY, USA, 209 216. LUEBKE, D. 1997. A survey 
of polygonal simpli.cation algo­rithms. REEB, G. 1946. Sur les points singuliers d une forme de compl­ment 
intgrable ou d une fonction numrique. Comptes Rendus de L Acadmie des Sances 222, 847 849. TIERNY, J., 
VANDEBORRE, J.-P., AND DAOUDI, M. 2006. 3d Mesh Skeleton Extraction Using Topological and Geometrical 
Analyses. In 14th Paci.c Conference on Computer Graphics and Applications (Paci.c Graphics 2006), 85 
94. Copyright is held by the author / owner(s). SIGGRAPH 2010, Los Angeles, California, July 25 29, 
2010. ISBN 978-1-4503-0210-4/10/0007  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1836922</article_id>
		<sort_key>770</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>71</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>8</seq_no>
		<title><![CDATA[Marker-less object recognition for surface computing]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836845.1836922</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1836922</url>
		<abstract>
			<par><![CDATA[<p>In surface computing, one of the most important requirements is tracking an object placed on the surface and manipulating information related to that object. To recognize objects, the most popular technique is marker tracking using techniques such as RFID, tag-like TarckMate[Kumpf 2009] and so on. The issues with marker tracking are the effort required to paste the tag and the existence of objects that are difficult to mark with a tag. To recognize objects without tags, feature point tracking on the image plane is one of the most effective ways in the area of the computer vision[Lowe 2004]. Unfortunately it is difficult to extract features from images taken through the frosted glass that is often used in surface computing. In addition, one cannot extract the feature points from objects without strong texture. In this paper, we present a marker-less object recognition system using multi channel silhouettes and quantized polar coordinates.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.4.8</cat_node>
				<descriptor>Object recognition</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245.10010251</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems->Object recognition</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2264414</person_id>
				<author_profile_id><![CDATA[81351600223]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Shiro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ozawa]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[NTT Comware Corporation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264415</person_id>
				<author_profile_id><![CDATA[81319487568]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Takao]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Abe]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[NTT Comware Corporation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264416</person_id>
				<author_profile_id><![CDATA[81319498567]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Noriyuki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Naruto]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[NTT Comware Corporation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264417</person_id>
				<author_profile_id><![CDATA[81466645573]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Toshihiro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nakae]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[NTT Comware Corporation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264418</person_id>
				<author_profile_id><![CDATA[81466640460]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Makoto]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nakamura]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[NTT Comware Corporation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264419</person_id>
				<author_profile_id><![CDATA[81466640540]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Naoya]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Miyashita]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[NTT Comware Corporation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264420</person_id>
				<author_profile_id><![CDATA[81100281714]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>7</seq_no>
				<first_name><![CDATA[Mitsunori]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hirano]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[NTT Comware Corporation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264421</person_id>
				<author_profile_id><![CDATA[81319502531]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>8</seq_no>
				<first_name><![CDATA[Kazuhiko]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tanaka]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[NTT Comware Corporation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[
Kumpf, A. 2009. Trackmate: Large-scale accessibility of tangible user interfaces. <i>Thesis (M.S.)--Massachusetts Institute of Technology.</i>
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>996342</ref_obj_id>
				<ref_obj_pid>993451</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[
Lowe, D. G. 2004. Distinctive image features from scale-invariant keypoints. <i>Int. J. Comput. Vision 60</i>, 2, 91--110.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1836923</article_id>
		<sort_key>780</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>72</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>9</seq_no>
		<title><![CDATA[Parallelization of the x264 encoder using OpenCL]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836845.1836923</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1836923</url>
		<abstract>
			<par><![CDATA[<p>With the introduction of H.264, the complexity on video encoders has increased dramatically. As hardware based encoding solutions profit from the strict sequential design and already feature real time capabilities for high definition material, software solutions lack most of the encoding performance. More precisely, the performance of software encoders is limited due to the computation power of encoding system as well as the high level of codec-intern dependencies. As a consequence, software encoders supporting high definition needs are very rare.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.4.2</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010395</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image compression</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2264422</person_id>
				<author_profile_id><![CDATA[81351607698]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Erich]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Marth]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Mannheim]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264423</person_id>
				<author_profile_id><![CDATA[81310487368]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Guillermo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Marcus]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Heidelberg]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[
Chen, W.-N., and Hang, H.-M. 2008. H.264/avc motion estimation implementation on compute unified device architecture (cuda). Tech. rep., National Chiao-Tung University.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[
Cross, J., 2008. GPU Accelerated Video Transcoding. Online Article, December. {online} http://www.extremetech.com/article2/0, 2845, 2337057, 00.asp.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1652960</ref_obj_id>
				<ref_obj_pid>1652959</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[
Schwalb, M., Ewerth, R., and Freisleben, B. 2009. Fast motion estimation on graphics hardware for h.264 video encoding. <i>Trans. Multi. 11</i>, 1, 1--10.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[
Shimpi, A. A., 2008. Badaboom: A Full Test of Elemental's GPU Accelerated H.264 Transcoder. Online Article, August. {online} http://www.anandtech.com/show/2586.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Parallelization of the x264 encoder using OpenCL Erich Marth, University of Mannheim, erich.marth@googlemail.com 
Guillermo Marcus, University of Heidelberg, guillermo.marcus@ziti.uni-heidelberg.de Sources Online at: 
http://li5.ziti.uni-heidelberg.de/x264gpu  Figure 1: OpenCL Powered Modules Of the x.264 Encoding Pipeline 
1 Introduction With the introduction of H.264, the complexity on video encoders has increased dramatically. 
As hardware based encoding solutions pro.t from the strict sequential design and already feature real 
time capabilities for high de.nition material, software solutions lack most of the encoding performance. 
More precisely, the perfor­mance of software encoders is limited due to the computation power of encoding 
system as well as the high level of codec-intern de­pendencies. As a consequence, software encoders supporting 
high de.nition needs are very rare. The increasing computation power of massive parallel architectures 
such as modern graphics devices can be used to speed-up the en­coding of H.264 video material. Compared 
to plain hardware so­lutions, graphics device powered encoders have the advantage of much lower initial 
costs and at the same time offer the .exibility of boosting the performance with future device upgrades. 
In addition, computers of today already include high performance graphics de­vices, which improve encoding 
times with nearly zero extra costs. While other stand alone GPU accelerated encoding solutions exist 
for H.264, this work shows the .rst working parallelization of the open source H.264 encoder x264 using 
OpenCL. 2 Parallelization using OpenCL In the beginning, the parallelization was targeting a straight 
for­ward OpenCL based motion estimation without the actual integra­tion into the encoding process. One 
straight forward approach was based upon the sub-optimal Three Step Search (TSS) algorithm. The implemented 
Assisted Three Step Search introduced additional assistant points for more concurrency. In addition, 
a second algo­rithm was implemented, derived from the computationally intensive Exhaustive Search (ES) 
 Full Search algorithm. The Exhaustive Search Derivation (ESD) differs in using a reduced set of candi­dates 
 only a fourth of the original set examining even positioned translations only. After .nishing the motion 
estimation, the OpenCL powered com­putation was integrated into the encoding .ow of the x264 en­coder 
by a plain serial design. In favor of higher encoding speeds, better device utilization as well as better 
adaption to the encoder architecture, the serial design was later replaced by a more au­tonomous OpenCL 
working thread approach. The new working thread pipeline was optimized by using principles from the RISC 
architecture. More precisely, the estimation and selection modules were stripped down to a single process, 
moving the extracted func­tionality to discrete modules. In a .nal step, the sub-sequential Mo­tion Estimation, 
Transformation and Quantization processes were ported to OpenCL and merged into the pipeline as well. 
While the Transformation was applied on blocks with 4x4 size con­forming to the H.264 speci.cation, the 
.nal Quantization process was implemented equally to the variant used inside the original x264 encoder. 
Compared to the H.264 speci.cation, the x264 en­coder merges the element-wise multiplication of the DCT 
with the Quantization step using an LUT based approach. 3 Results Considering the fact that only a fraction 
of the motion estima­tion capabilities have been ported to OpenCL, the OpenCL pow­ered encoding is up 
to 55% faster than the original Full Search based encoding of the unmodi.ed x264. While other GPU solu­tions 
claim up to 20x speedup, independent tests against unmodi­.ed x264 shows similar gains as our implementation 
for FullHD. Furthermore, the current work is the .rst open-source, working in­tegration into the x264 
encoder that enables it to pro.t from the computing power of high performance graphics devices. References 
CHEN, W.-N., AND HANG, H.-M. 2008. H.264/avc motion estimation implementation on compute uni.ed device 
architecture (cuda). Tech. rep., National Chiao-Tung University. CROSS, J., 2008. GPU Accelerated Video 
Transcoding. Online Article, De­cember. [online] http://www.extremetech.com/article2/ 0,2845,2337057,00.asp. 
SCHWALB, M., EWERTH, R., AND FREISLEBEN, B. 2009. Fast motion estimation on graphics hardware for h.264 
video encoding. Trans. Multi. 11, 1, 1 10. SHIMPI, A. A., 2008. Badaboom: A Full Test of Elemental s 
GPU Ac­celerated H.264 Transcoder. Online Article, August. [online] http: //www.anandtech.com/show/2586. 
Copyright is held by the author / owner(s). SIGGRAPH 2010, Los Angeles, California, July 25 29, 2010. 
ISBN 978-1-4503-0210-4/10/0007  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1836924</article_id>
		<sort_key>790</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>73</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>10</seq_no>
		<title><![CDATA[Practical 3D decoration on flat media with anisotropic reflection]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836845.1836924</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1836924</url>
		<abstract>
			<par><![CDATA[<p>We have proposed a method called Bump Mapping onto Real Objects (BMRO)[1] for displaying the appearance of curved surface on flat media. The method converts normal vectors of modeled curved surface into directions of grooves by which anisotropic reflection occurs for displaying a curved surface. Although curved surfaces can appear on media by BMRO, it is still insufficient for practical use because the streamlines used for a pattern of grooves are often placed too closely or too sparsely to one another due to the vector plot employed for generating them. The simplest solution to avoid the non-uniformity is to divide the entire region into regular square or hexagonal cells and to fill each cell with parallel lines in a given direction instead of tracing the direction field strictly with streamlines, but this improvement causes aliasing to noticeably appear at the edges and ridges of the original model. In this article, we propose an improvement on generating cells that reduces aliasing for BMRO and makes it practical for industrial applications.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2264424</person_id>
				<author_profile_id><![CDATA[81466643793]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Eriko]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kimura]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Dai Nippon Printing Co., Ltd.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264425</person_id>
				<author_profile_id><![CDATA[81100533332]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Naoki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kawai]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Dai Nippon Printing Co., Ltd.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264426</person_id>
				<author_profile_id><![CDATA[81100338478]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Kazunori]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Miyata]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Japan Advanced Institute of Science and Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1187124</ref_obj_id>
				<ref_obj_pid>1187112</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[
Kawai, N, Bump Mapping onto Real Objects. <i>ACM SIGGRAPH 2005 Sketches.</i>
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[
Noda, T et al, Mosaic Image Generation using 3D Models. <i>ASIAGRAPH 2009 PROCEEDINGS pp.110--115.</i>
]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Practical 3D Decoration on Flat Media with Anisotropic Reflection Eriko Kimura1 Naoki Kawai1 Kazunori 
Miyata2 1Dai Nippon Printing Co., Ltd. 2 Japan Advanced Institute of Science and Technology 1 Introduction 
We have proposed a method called Bump Mapping onto Real Objects (BMRO)[1] for displaying the appearance 
of curved surface on flat media. The method converts normal vectors of modeled curved surface into directions 
of grooves by which anisotropic reflection occurs for displaying a curved surface. Although curved surfaces 
can appear on media by BMRO, it is still insufficient for practical use because the streamlines used 
for a pattern of grooves are often placed too closely or too sparsely to one another due to the vector 
plot employed for generating them. The simplest solution to avoid the non-uniformity is to divide the 
entire region into regular square or hexagonal cells and to fill each cell with parallel lines in a given 
direction instead of tracing the direction field strictly with streamlines, but this improvement causes 
aliasing to noticeably appear at the edges and ridges of the original model. In this article, we propose 
an improvement on generating cells that reduces aliasing for BMRO and makes it practical for industrial 
applications. 2 Method Arranging cells in a way that adapts to edges and ridges (feature lines) could 
reduce aliasing around feature lines for BMRO. Noda et al discussed a similar problem on generating mosaic 
images from 3D models, and proposed a method for arranging square tiles adaptively to the feature lines 
of 3D objects [2]. The method consists of the following three steps. First, feature lines are extracted 
from the surface of 3D objects based on the spatial variance in curvature and depth. Then, the entire 
region is divided into some belts with a fixed width depending on the distance from the nearest feature 
line. Finally, square tiles are arranged along the centerlines of all belts at a fixed interval. Figure 
1 shows extracted feature lines and belts, and Figure 2(a) shows a part of the final mosaic image obtained 
by steps mentioned above. Although feature lines are preserved and the aliasing on BMRO is reduced by 
employing the arranged squares as cells as shown in Figure 2(b), gaps between the adjacent cells cause 
visual artifacts due to vacancy of grooves. It is necessary to cover the entire region with cells and 
with no gaps so as to eliminate artifacts. However it is impossible to avoid gaps by just inlaying congruent 
squares. The advantage of Noda s method is that the belts preserve the feature lines. On the one hand, 
Voronoi division divides an arbitrary region into cells with no gaps. Then we combine the advantages 
of Noda s belts and Voronoi division to both preserve feature lines and avoid gaps. Our new method starts 
with Noda s belts as the first stage of a two-pass division. Then we apply Voronoi division on each of 
the belts independently. We put Voronoi seeds on the centerline of each belt with a certain interval 
then apply Voronoi division within the target belt. Figure 2(c) shows a result of the new method in which 
cells cover the entire region without gaps and feature lines are preserved. All the cells appear as a 
similar size and the outcome is sufficient for reproduction processes such as embossing and foil stamping. 
The average area of these adaptive cells s that satisfies the ideal condition of the specific post process 
can be almost ensured by setting s as both width of belts and seed interval. Figure 1: Noda s (a) feature 
lines and (b) belts Figure 2: (a) Noda s mosaic, (b) BMRO with Noda s tiles, (c) our adaptive cells 
 3 Results We made BMROs with regular hexagonal cells and with adaptive cells. We gave 1.0 square millimeter 
as the average area of cells and filled each cell with parallel lines 80 micrometers wide at intervals 
of 120 micrometers. After substantiating parallel lines onto a brass mould with approximately 30 micrometers 
in depth by etching, we embossed grooves onto pieces of paper by foil stamping them with attaching aluminum 
foil. Figure 3 shows two BMROs and their magnified images with hexagonal cells (a, b) and adaptive cells 
(c, d). Aliasing around feature lines is reduced for BMRO when using adaptive cells comparing with regular 
cells, and our proposed method makes three-dimensional decoration onto the surface of printed matter 
practical. Figure 3: (a) BMRO with hexagonal cells, (b) magnified image of (a), (c) BMRO with adaptive 
cells, (d) magnified image of (c)  References [1] Kawai, N, Bump Mapping onto Real Objects.ACM SIGGRAPH 
2005 Sketches. [2] Noda, T et al, Mosaic Image Generation using 3D Models. ASIAGRAPH 2009 PROCEEDINGS 
pp.110-115. Copyright is held by the author / owner(s). SIGGRAPH 2010, Los Angeles, California, July 
25 29, 2010. ISBN 978-1-4503-0210-4/10/0007  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1836925</article_id>
		<sort_key>800</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>74</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>11</seq_no>
		<title><![CDATA[Quality-preserving image downsizing]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836845.1836925</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1836925</url>
		<abstract>
			<par><![CDATA[<p>The image quality of a digital viewfinder is considerably lower than that of a through-the-lens optical system. While the sensor may be capable of capturing 10 or 20 megapixels, the screen of the viewfinder is typically constrained to resolutions under 1 megapixel. The limited resolution makes it impossible to discern all the small details of the captured image. Small blurs and noise that are present in the full-size image can render the image unusable for certain tasks, yet these artifacts may be too small to be discernible in the downsampled version shown on the camera viewfinder.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.4.2</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010395</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image compression</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2264427</person_id>
				<author_profile_id><![CDATA[81100626517]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Matthew]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Trentacoste]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of British Columbia]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264428</person_id>
				<author_profile_id><![CDATA[81100175469]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Rafal]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Mantiuk]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of British Columbia]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264429</person_id>
				<author_profile_id><![CDATA[81100644737]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Wolfgang]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Heidrich]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of British Columbia]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[
Bae, S., and Durand, F. 2007. Defocus magnification. <i>Computer Graphics Forum 26</i>, 3, 571--579.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1737470</ref_obj_id>
				<ref_obj_pid>1737463</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[
Samadani, R., Mauer, T. A., Berfanger, D. M., and Clark, J. H. 2010. Image thumbnails that represent blur and noise. <i>IEEE Transactions on Image Processing 19</i>, 2, 363--373.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Quality-Preserving Image Downsizing Matthew Trentacoste Rafal Mantiuk Wolfgang Heidrich University of 
British Columbia 1 Introduction The image quality of a digital view.nder is considerably lower than 
that of a through-the-lens optical system. While the sen­sor may be capable of capturing 10 or 20 megapixels, 
the screen of the view.nder is typically constrained to resolutions under 1 megapixel. The limited resolution 
makes it impossible to discern all the small details of the captured image. Small blurs and noise that 
are present in the full-size image can render the image unusable for certain tasks, yet these artifacts 
may be too small to be discernible in the downsampled version shown on the camera view.nder. We preserve 
spatial detail such as blur and noise while creating the thumbnail image for presentation in a digital 
view.nder. We present an ef.cient method for enhancing the artifacts that may be present in both still 
images and live preview video streams, making them more identi.able, and thus decreasing the chance of 
capturing low-quality images. We analyze the image to detect artifacts present but too small to be visible 
on a digital view.nder, and modify the downsampled thumbnail image such that they are large enough to 
be discernible, as seen in Figure 1. The result more accurately re­.ects the original image, allowing 
faster and more accurate assess­ments of quality in the .eld or when browsing thumbnails. Our blur enhancement 
produces an image with out-of-focus re­gions further blurred like Bae and Durand [2007], but it produces 
a lower resolution approximation at signi.cantly reduced computa­tional cost. Similar to our work, Samadani 
et al. [2010] developed a method for amplifying speci.c artifacts present in full-size images to be visible 
in thumbnail images. However, their method cannot accurately detect motion blur or noise in textured 
areas. Figure 1: A comparison of our quality-preserving downsampling algorithm (right) with a regular 
thumbnail (left). The full resolution original image is shown in the cropped inserts below. Note that 
the cat s face is the only part of the image that is in focus (see detail regions below), a feature that 
is preserved by our algorithm. 2 Our approach In order to make the blur and noise artifacts more apparent 
in a downsampled thumbnail image, we .rst must analyze the full-size image to determine how much of each 
attribute is present. Given an estimate of the signi.cance of both blur and noise, we amplify these characteristics 
so they remain visible in the generated thumb­nail. This process is particularly important when the strength 
of an artifact is large enough to be of visual signi.cance, but small enough to be invisible in the thumbnail. 
The outline of our algorithm is as follows: 1) We identify the strength of both blur and noise present 
in the full-size image. 2) Based on our blur estimate, we determine how much blur needs to be added to 
each region of the thumbnail. 3) We downsample the image, and add the required amount of blur. 4) For 
each image re­gion, we model how much of the original noise is lost due to both the downsampling operation 
and the subsequent blur enhancement. 5) Finally, we re-introduce the required amount of noise for each 
pixel. Representations of the intermediate steps can be seen in Fig­ure 2. Figure 2: Upper left: original 
image. Upper right: estimated blur. Lower left: false color noise map. Lower right: false color blur­adjusted 
noise map. In cases like defocus and motion blur, the amount of blur typically varies across the image. 
In order to retain the detail of in-focus re­gions while further removing detail in out-of-focus regions, 
our blur estimation routine determines a spatially-variant estimate of blur across an image. Similarly, 
our noise estimation reliably estimates the noise present at different pixel intensities, and compensates 
for the differing amount of noise reduction resulting from the increased blur of the thumbnail. The end 
product is a computationally effective means of preserv­ing small-scale image artifacts in downsampled 
images that ensures they are visible on a digital view.nder or when browsing image thumbnails. References 
BAE, S., AND DURAND, F. 2007. Defocus magni.cation. Com­puter Graphics Forum 26, 3, 571 579. SAMADANI, 
R., MAUER, T. A., BERFANGER, D. M., AND CLARK, J. H. 2010. Image thumbnails that represent blur and noise. 
IEEE Transactions on Image Processing 19, 2, 363 373. Copyright is held by the author / owner(s). SIGGRAPH 
2010, Los Angeles, California, July 25 29, 2010. ISBN 978-1-4503-0210-4/10/0007  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1836926</article_id>
		<sort_key>810</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>75</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>12</seq_no>
		<title><![CDATA[Robust movement detection based on a new similarity index for HDR imaging]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836845.1836926</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1836926</url>
		<abstract>
			<par><![CDATA[<p>It is known that a high dynamic range (HDR) image can be produced by sequentially capturing a set of low dynamic range (LDR) images with different exposure times [Debevec and Malik 1997]. However, ghosting artifacts could be produced via this method when there are moving objects in a scene. In this poster, a similarity index is first introduced for such LDR images by using intensity mapping functions (IMFs) among them. The index is then applied to detect moving objects such that ghosting artifacts are removed from the eventual HDR image. The details are given as below.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.4.7</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.4.8</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010225.10010227</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision tasks->Scene understanding</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2264430</person_id>
				<author_profile_id><![CDATA[81423596048]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Zhengguo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Li]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Institute for Infocomm Research, Singapore]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264431</person_id>
				<author_profile_id><![CDATA[81100244820]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Susanto]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Rahardja]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Institute for Infocomm Research, Singapore]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264432</person_id>
				<author_profile_id><![CDATA[81323497843]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Shiqian]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Institute for Infocomm Research, Singapore]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264433</person_id>
				<author_profile_id><![CDATA[81474692949]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Zijian]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Zhu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Institute for Infocomm Research, Singapore]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264434</person_id>
				<author_profile_id><![CDATA[81100418227]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Shoulie]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Xie]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Institute for Infocomm Research, Singapore]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>258884</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[
Debevec, P. E. and Malik, J. 1997. Rendering high dynamic range radiance maps from photographs. In <i>Proceedings of SIGGRAPH 1997</i>, 369--378.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[
FDRTOOLS, PHOTOMATIX, AND QTPFSGUI 2009. http://fdrtools.com/front_e.php, http://www.hdrsoft.com/, http://qtpfsgui.sourceforge.net/.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>950762</ref_obj_id>
				<ref_obj_pid>950629</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[
Grossberg M. and Nayar S. 2003. Determining the camera response from images: what is knowable?. <i>IEEE Trans. on Image Processing 25</i>, 11 (Nov.), 1455--1467.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Robust Movement Detection Based on a New Similarity Index for HDR Imaging Zhengguo Li, Susanto Rahardja, 
Shiqian Wu, Zijian Zhu and Shoulie Xie* Signal Processing Department, Institute for Infocomm Research, 
1 Fusionopolis Way, Singapore 138632  Figure 1: The HDR images by (a) FDRTOOLS. (b) PHOTOMATIX. (c) 
QTPFSGUI. (d) Ours with Debevec and Malik s scheme. It is known that a high dynamic range (HDR) image 
can be pro-where the value of .k,l(i, j) is ( 15 - max{E(Zk,l(i, j)), 16 duced by sequentially capturing 
a set of low dynamic range (LDR) E(ZZk,l(i, j))}e(k, Ref(k))), Ref(k) corresponds to the expo­images 
with different exposure times [Debevec and Malik 1997]. sure time of the reference image of image k, 
e(k, Ref(k)) is However, ghosting artifacts could be produced via this method when there are moving objects 
in a scene. In this poster, a similar- max{.tk, .tRef(k)}/ min{.tk, .tRef(k)}, and the scale fac­ity 
index is .rst introduced for such LDR images by using intensity tor E(z) is de.ned as mapping functions 
(IMFs) among them. The index is then applied { to detect moving objects such that ghosting artifacts 
are removed )16 12z )( z (1 - 17 ; if z> 127 128 255 )16 . (3) 51 from the eventual HDR image. The details 
are given as below. E(z)= 12z )(50- 10z (1 - 16 255 otherwise ; 1) A New Similarity Index Consider two 
LDR images Z1 and Z2. .1,2,l(z) and .2,1,l(z) are The values of . are computed by using k,Ref(k),l and 
.Ref(k),k,l the IMFs from the lth color channel of image Z1 to that of im­the accumulated histograms 
of the kth image and its reference im­age Z2 and vice verse, respectively. A pixel level similarity index, 
age [Grossberg and Nayar 2003]. All LDR images are processed in Sl(Z1,l(i, j),Z2,l(i, j)), is de.ned 
as the order of (k0 - 1), ···,1, k0, (k0 + 1), ···, n0. Since the correla­tion between two successive 
images is the strongest, the reference image is updated after checking all pixels in the current image. 
All Sl(Z1,l,Z2,l)= .. . 2.1,2,l(Z1,l)Z2,l+1 ; if Z1,l is more reliable .2 +1 (Z1,l)+Z22 ,l1,2,l valid 
pixels are adopted to replace their co-located pixels in the ref­ erence image. The IMFs are used to 
synthesize pixels to replace . 2Z1,l.2,1,l(Z2,l)+1 ; otherwise Z2 +.2 (Z2,l)+1 1,l 2,1,l other pixels 
in the reference image. The updated reference image is (1) applied to classify all pixels of the subsequent 
image. The function of IMFs .1,2,l(z) and .2,1,l(z) is to improve the ro­bustness of the proposed index 
with respect to scale changes be-In the remaining part of this poster, we shall verify the proposed tween 
Z1,l(i, j) and Z2,l(i, j). 2) An IMF Based Robust Movement Detection movement detection scheme by combining 
it with the scheme in [Debevec and Malik 1997] to form a framework for the synthe­ sis of HDR images. 
This framework is suitable for both static Let n0 be the total number of LDR images. A middle image, 
Zk0 , and dynamic scenes. To illustrate the ef.ciency of the combined is selected as a basis for the 
movement detection. All pixels in Zk0 framework, we compare it with three commercial softwares [FDR­are 
marked as valid. Pixel Zk(i, j) in the kth (1 = k = n0,k = TOOLS, PHOTOMATIX, and QTPFSGUI 2009] by testing 
an im­k0) image is marked as valid if the similarities between all color age sequence that is composed 
of 11 images with waiving leafs. It channels of Zk(i, j) and those of its co-located pixel ZZk(i, j) 
in the reference image are high, i.e., is shown in Fig. 1 that ghosting artifacts, due to moving leafs, 
are not removed by using these commercial softwares, especially by Sl(Zk,l(i, j), ZZk,l(i, j)) > Thrk,l(i, 
j) 6= 2.k,l(i, j) 1 + .2 k,l(i, j) , (2) the QTPFSGUI. However, they are removed by our method. References 
*{ezgli, rsusanto, shiqian, zhuzj, slxie}@i2r.a-star.edu.sg DEBEVEC, P. E. AND MALIK, J. 1997. Rendering 
high dynamic range radiance maps from photographs. In Proceedings of SIG-GRAPH 1997, 369 378. FDRTOOLS, 
PHOTOMATIX, AND QTPFSGUI 2009. http://fdrtools.com/front e.php, http://www.hdrsoft.com/, http://qtpfsgui.sourceforge.net/. 
GROSSBERG M. AND NAYAR S. 2003. Determining the camera re­sponse from images: what is knowable?. IEEE 
Trans. on Image Processing 25, 11 (Nov.), 1455 1467. Copyright is held by the author / owner(s). SIGGRAPH 
2010, Los Angeles, California, July 25 29, 2010. ISBN 978-1-4503-0210-4/10/0007 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1836927</article_id>
		<sort_key>820</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>76</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>13</seq_no>
		<title><![CDATA[Simple gamma correction for fringe projection profilometry system]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836845.1836927</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1836927</url>
		<abstract>
			<par><![CDATA[<p>Fringe projection profilometry (FPP) is one of the most commonly used non-contact methods for retrieving the three-dimensional (3D) shape information of objects. In reality, the nonlinearity mostly caused by the gamma effect of digital otpic system, includes both projector and camera, gives inevitable intensity changes, which dramatically reduce the measurement accuracy. In this poster, a robust and simple scheme to eliminate the intensity nonlinearity induced by gamma effect. Firstly, by using phase shifting techniques, the gamma value involved in the measurement system can be detected accurately. Then, a gamma encoding process is applied to the system for future actual 3D shape measurements. With the proposed technique, high accuracy of measurement can be achieved with the traditional three-step phase-shifting algorithm.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.4.3</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010224</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2264435</person_id>
				<author_profile_id><![CDATA[81466647101]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Thang]]></first_name>
				<middle_name><![CDATA[M.]]></middle_name>
				<last_name><![CDATA[Hoang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The Catholic University of America]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>2320179</ref_obj_id>
				<ref_obj_pid>2319001</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[H. Farid, "Blind inverse gamma correction," IEEE Transaction on Image Processing. &#60;b&#62;10&#60;/b&#62;, 1428 (2001).]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Simple Gamma Correction for Fringe Projection Pro.lometry System Thang M. Hoang* The Catholic University 
of America  Figure 1: From left to right:a captured shell model,shape measurement results obtained by 
the 3-, 7-step phase-shifting schemes without gamma correction, and the 3-step phase-shifting scheme 
with proposed gamma correction method 1 Introduction Fringe projection pro.lometry (FPP) is one of the 
most commonly used non-contact methods for retrieving the three-dimensional (3D) shape information of 
objects. In reality, the nonlinearity mostly caused by the gamma effect of digital otpic system, includes 
both projector and camera, gives inevitable intensity changes, which dra­matically reduce the measurement 
accuracy. In this poster, a robust and simple scheme to eliminate the intensity nonlinearity induced 
by gamma effect. Firstly, by using phase shifting techniques, the gamma value involved in the measurement 
system can be detected accurately. Then, a gamma encoding process is applied to the sys­tem for future 
actual 3D shape measurements. With the proposed technique, high accuracy of measurement can be achieved 
with the traditional three-step phase-shifting algorithm. 2 Gamma Correction In practice, the nonlinear 
luminance distortion in the FPP system brings high-order harmonics to the actual fringe interferograms. 
Hence, the intensity of the captured fringe image can be mathe­matically expressed as follows: p p I(x, 
y)= I0(x, y)+ Ij (x, y) cos {j [f(x, y)+ d]} (1) j=1 where (x; y) denotes an any point in the image, 
I0 is the mean intensity, Ij is the modulation amplitude, f is the fringe phase, d is the phase-shift 
amount, and p is the highest signi.cant harmonic order of the captured fringes. Using the least square 
approach, the phase distribution in Equation (1) can be retrieved correctly: - p+2 -1 - i=1 sin(di)Ii 
f = tan -p+2 (2) i=1 cos(di)Ii The equation indicates that (p + 2)-step uniform phase-shifting scheme 
can be applied to retrieve phase accurately from fringe pat­terns with nonlinear harmonics up to the 
p-th order. The traditional phase-shifting algorithm with large step is very sim­ple and effective for 
extracting correct phase from real FPP images. In order to have high speed measurement, however, taking 
more than three phase-shifted frames is usually unpreferable.To deal with this issue, a novel gamma correction 
scheme is described as fol­lows. *e-mail:34hoang@cardinalmail.cua.edu In theory, the gamma effect of 
the projector and camera system can be described as: I = I0 . / (3) where I is the caputured intensity, 
I0 is the intensity of the ideal / sinusoidal pattern, and .is the gamma value of the entire system including 
projector and camera. / To detect .in equation (3) for the three-step phase shifting algo­rithm, the 
following equation can be used: . . ./ ./ . -I2 sin( p 3 )+ I3 sin( 23 p ). tan -1 - fc =0 (4) ./ ./ 
./ .I1 + I2 cos( p 3 )+ I3 cos( 23 p ) . where fc is the correct phase value obtained in Equation (2), 
which in practice is determined by using the conventional large step phase­shifting algorithm presented 
previously. It is noted that the previous equation is for a single pixel only; to solve for . for the 
entire image to ensure high accuracy, a least-square approach is highly recommended. 3 Results To verify 
the validity of the proposed approach, an experiment of measuring the 3D shape of a sea shell is conducted. 
It is con.rmed that the phase error of proposed method is only around 0.01 radian, and the encoded gamma 
is calculated with errors averaging only 0.1 %, which is signi.cantly better than previous method s result, 
7.5 % [Farid et al.2001]. Using this technique, accuracy with fast three­stepping phase shifting algorithm 
can be yielded up to 1/10000 . The above .gure demonstrates 3D imaging reconstruction from three different 
schemes. It shows that the three-step phase-shifting algorithm with proposed gamma encoding method can 
achieve ac­curacy even higher than the one provided by the larger-step phase­shifting algorithm. The 
experiments thus demonstrate when high speed is desired, the proposed gamma encoding scheme can be ap­plied 
with the conventional three-step phase-shifting algorithm to achieve high measurement accuracy. References 
H. Farid, Blind inverse gamma correction, IEEE Transaction on Image Processing. 10, 1428 (2001). Copyright 
is held by the author / owner(s). SIGGRAPH 2010, Los Angeles, California, July 25 29, 2010. ISBN 978-1-4503-0210-4/10/0007 
  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1836928</article_id>
		<sort_key>830</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>77</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>14</seq_no>
		<title><![CDATA[Temporally coherent video matting]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836845.1836928</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1836928</url>
		<abstract>
			<par><![CDATA[<p>Existing video matting approaches determine the alpha matte sequence frame-by-frame, which lead to flickering near the boundary of the foreground region. We reduce this effect by considering video data as a spatio-temporal cube, and extending a robust matting algorithm to a 3D solver. Our results demonstrate consistent and visually pleasing alpha mattes, and tend to preserve temporal coherence better than previous techniques.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.4.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010383</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Image processing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2264436</person_id>
				<author_profile_id><![CDATA[81448593509]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Sun-Young]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lee]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Yonsei University, Korea]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264437</person_id>
				<author_profile_id><![CDATA[81331508005]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Jong-Chul]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yoon]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Yonsei University, Korea]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264438</person_id>
				<author_profile_id><![CDATA[81409592301]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[In-Kwon]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lee]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Yonsei University, Korea]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[
Wang, J., and Cohen, M. F. Optimized color sampling for robust matting. In <i>IEEE Computer Vision and Pattern Recognition 2007.</i>
]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Temporally Coherent Video Matting Sun-Young Lee* Jong-ChulYoon In-Kwon Lee Dept.of Computer Science,Yonsei 
University,Korea Figure 1: Video matting results (a total of 80 frames, of whichframes 20, 40, 60 and 
80 are shown): The upper row of eachsequence is the original video; the middle row is the alpha matte 
sequence, and the lower row is a sequence composited with a new background image. 1 Introduction Existing 
video matting approaches determine the alpha matte se­quence frame-by-frame, which lead to .ickering 
near the bound­ary of the foreground region. We reduce this effect by considering video data as a spatio-temporal 
cube, and extending a robust mat­ting algorithm to a 3D solver. Our results demonstrate consistent and 
visually pleasing alpha mattes, and tend to preserve temporal coherence better than previous techniques. 
 2 Description We extend an existing robust matting algorithm [Wang and Cohen ]from 2D to a 3D lattice 
by considering the time axis as a third spatial dimension. We will assume that the trimap necessary for 
the video to be matted has already been generated. Our system can utilize anymethod of trimap generation 
or re.nement. An accurate and temporally coherent series of trimaps will naturally improve the overall 
performance of the algorithm. In matting the current frame, we consider three consecutive frames, the 
previous, current, and next. We select several samples from each frame (20 samples per frame in all our 
examples). The con.dence and alpha values for each pair of foreground and background samples are estimated 
using the technique described for image matting, and the three pairs withthehighest con.dencevaluesare 
selected.Wealsoneedtoex­tend the random-walk algorithm to three dimensions.We therefore construct a 3D 
graph in (x, y, t) space, and use a 3 × 3 × 3 kernel to compute the edge weights Wij . Instead of the 
cubic 3 ×3 ×3 kernel, we can use an anisotropicker­nel which takes account of the movement of the foreground 
object. Using an optical .ow vector .eld, we can obtain the .ow distance d, which is the vector difference 
between the optical .ow vector and the edge vector of the node. The vector d measures how for the video 
object moves between the current frame and the next or previous frame.Wecanthen distortthekernelininverse 
proportion to the magnitude of d. An anisotropickernel produces more con­sistent results in the temporal 
axis. So Equation may be rewritten *e-mail: shepherd@cs.yonsei.ac.kr e-mail: media19@cs.yonsei.ac.kr 
e-mail: iklee@yonsei.ac.kr as: (()-1 ) . m 1 . Wij= 1+(Ci -µk) k + I(Cj -µk), |d||wk||wk| k|(i,j) wk 
(1) where wk is thekernel that contains pixels i and j, and k iterates over thosekernels. The terms µk 
and k are respectively the mean andvarianceof the colorsin eachkernel, and |wk| is the number of pixelsin 
thekernel wk.We typically set . to be 10-5.Parameter m controlsthe in.uence of the .ow distance. We use 
values of m in the range [0.1, 2.5].Ifa video hasafast-moving foreground object, m is larges and the 
alpha values are more accurate, since |d| also becomes large. 3 Result and Future Work Amatting resultis 
shownin Figure1as sequencesof images sam­pled every few frames. (Please see also the results in the supple­mentary 
videos.) Our results demonstrate consistent and visually pleasing alpha mattes, and tend to preservetemporal 
coherence bet­ter than frame-by-frame techniques. Our system could be further improved by employing an 
advanced color model and sampling method which were specialized to the time axis. In addition, when the 
user edits selected frames, it should be possible to generate new trimaps and mattes interactively, by 
taking advantage of the locality of the user editing procedure. We would like to develop an integrated 
tool with a user interface, and to construct it with the help of a users. References WANG,J., AND COHEN,M.F. 
Optimized color sampling for ro­bust matting. InIEEE ComputerVision andPattern Recognition 2007.  Acknowledgements 
Thisworkwas supportedbytheITR&#38;D programof MKE/MCST/IITA. [2008-F-031­01, Development of Computational 
PhotographyTechnologies for Image andVideo Contents] Copyright is held by the author / owner(s). SIGGRAPH 
2010, Los Angeles, California, July 25 29, 2010. ISBN 978-1-4503-0210-4/10/0007  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1836929</article_id>
		<sort_key>840</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>78</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>15</seq_no>
		<title><![CDATA[Virtual face sculpting]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836845.1836929</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1836929</url>
		<abstract>
			<par><![CDATA[<p>This paper introduces the new concept of <i>virtual face sculpting.</i> Given the images of a human face and a statue face (cf Fig 1-a and b), the goal of this application is to sculpt a virtual statue (cf Fig 1-c) as if the human face was sculpted on the statue. This problem is complicated and must face some important difficulties. For example, the virtual sculpture must verify the color and texture consistency of the original statue. Moreover, the structure of the human face must also not be modified, otherwise the person will not be recognizable.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Virtual reality</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010866</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Virtual reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2264439</person_id>
				<author_profile_id><![CDATA[81384612822]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Jean-Charles]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bazin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[RCV Lab, KAIST, Daejeon, South Korea]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264440</person_id>
				<author_profile_id><![CDATA[81466646878]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Soonkee]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Chung]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[RCV Lab, KAIST, Daejeon, South Korea]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264441</person_id>
				<author_profile_id><![CDATA[81453659764]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Roger]]></first_name>
				<middle_name><![CDATA[Blanco]]></middle_name>
				<last_name><![CDATA[Ribera]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Visual Media Lab, KAIST, Daejeon, South Korea]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264442</person_id>
				<author_profile_id><![CDATA[81542054856]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Quang]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Pham]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[RCV Lab, KAIST, Daejeon, South Korea]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264443</person_id>
				<author_profile_id><![CDATA[81410591820]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Inso]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kweon]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[RCV Lab, KAIST, Daejeon, South Korea]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>52121</ref_obj_id>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Pearl, J. 1988. <i>Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference.</i> Morgan Kaufmann.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>882269</ref_obj_id>
				<ref_obj_pid>1201775</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Perez, P., Gangnet, M., and Blake, A. 2003. Poisson image editing. <i>ACM Transactions on Graphics (SIGGRAPH'03) 22</i>, 3, 313--318.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1073274</ref_obj_id>
				<ref_obj_pid>1186822</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Sun, J., Yuan, L., Jia, J., and Shum, H.-Y. 2005. Image completion with structure propagation. <i>ACM Transactions on Graphics (SIGGRAPH'05) 24</i>, 3, 861--868.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Figure 1: Results of our virtual face sculpting algorithm. (a) and (d): input human face. (b) and (e): 
input statue face. (c) and (f): virtual statue face obtained by the proposed algorithm. 1 Introduction 
This paper introduces the new concept of virtual face sculpting. Given the images of a human face and 
a statue face (cf Fig 1-a and b), the goal of this application is to sculpt a virtual statue (cf Fig 
1­c) as if the human face was sculpted on the statue. This problem is complicated and must face some 
important dif.culties. For exam­ple, the virtual sculpture must verify the color and texture consis­tency 
of the original statue. Moreover, the structure of the human face must also not be modi.ed, otherwise 
the person will not be recognizable. To the best of our knowledge, this paper is the .rst attempt for 
this face sculpture. Nevertheless some existing works could be applied for this task, while it is usually 
not their primary goal. However these methods either require 3D information of the faces or cannot handle 
color and texture consistency (e.g. [Perez et al. 2003] or warping). In this paper, we explain our approach 
to solve the challenging dif­.culties of this application and present the results. 2 Our approach The 
key dif.culty of this application is the fact that two aspects compete with each other. First, the output 
statue must have coherent color and texture with the original statue, which we refer as the Color and 
Texture Consistency Problem (CTCP). But, secondly, the output statue must also look like the original 
human face. Our approach consists in extracting some patches (i.e. rectangular parts) from the input 
statue face and combining them in an appro­priate con.guration. This combination of patches is performed 
with respect to two constraints: their con.guration must (1) respect the structure of the given human 
face and (2) minimize the patching effect . We mathematically de.ned the problem as an energy mini­mization 
composed of 2 competitive terms: a similarity term (struc­ture constraint) and a coherence term (overlapping 
constraint). The similarity term compares two corresponding patches (one from the input statue and one 
from the human face) by computing the sum of squared differences (SSD) in the gradient domain. This term 
per­mits to build the correspondence of patches between the statue and human faces: for example, .nding 
the left eye corner in the statue and human images. Globally, it is used to reconstruct the human face 
structure from the statue face patches. The coherence term compares two overlapping patches in the output 
statue by the SSD in RGB colorspace. It is used to .nd patches that overlap correctly in order to get 
a smooth image and avoid the patching effect . We consider this energy minimization as a graph labeling 
problem, inspired from [Sun et al. 2005]. This labeling problem is a dif.cult task due to the high number 
of labels and the graph connectivity. We applied Belief Propagation (BP) [Pearl 1988] to .nd the labels 
(i.e. the patch combination) and also a warping step to drastically decrease the number of candidate 
patches (i.e. the search space) and impose some apriori geometric information about the faces. This warping 
needs some point correspondences that can be obtained by automatic facial feature extraction or manually 
selected. The proposed method can run automatically and takes about 1 to 20 minutes depending on the 
image size and the number of iterations, in non-optimized Matlab code. Results obtained by the proposed 
method are shown in Figures 1 and 2. We have been able to re­construct some features that cannot be created 
by existing methods, such as the cheek wrinkle in Fig 1-c and the hair streak in Fig 1-f. Acknowledgments 
to the Flickr user Kuwait-Ra ed Qutena for Fig 1-a. More results are available on the authors website. 
Figure 2: Extra results obtained by our virtual face sculpting method. Same legend than Figure 1. References 
PEARL, J. 1988. Probabilistic Reasoning in Intelligent Systems : Networks of Plausible Inference. Morgan 
Kaufmann. PEREZ, P., GANGNET, M., AND BLAKE, A. 2003. Poisson image editing. ACM Transactions on Graphics 
(SIGGRAPH 03) 22, 3, 313 318. SUN, J., YUAN, L., JIA, J., AND SHUM, H.-Y. 2005. Image completion with 
structure propagation. ACM Transactions on Graphics (SIGGRAPH 05) 24, 3, 861 868. Copyright is held by 
the author / owner(s). SIGGRAPH 2010, Los Angeles, California, July 25 29, 2010. ISBN 978-1-4503-0210-4/10/0007 
 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	</section>
	<section>
		<section_id>1836930</section_id>
		<sort_key>850</sort_key>
		<section_seq_no>7</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Interaction]]></section_title>
		<section_page_from>79</section_page_from>
	<article_rec>
		<article_id>1836931</article_id>
		<sort_key>860</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>79</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[A new "multiplex content" displaying system compatible with current 3D projection technology]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836845.1836931</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1836931</url>
		<abstract>
			<par><![CDATA[<p>We have enabled the superimposition of multiplexed images on the same screen at the same time with tangible and stable equipment. Our multiplex images can be seen by wearing special configured polarized glasses, and the image projection method is designed to be based on current 3D stereoscopic technology, which is now prevalent and making rapid progress, thus high compatibility with current contents industries is retained. Therefore our system enables the wide range of applications with new expressions and can easily be put into production.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.4.0</cat_node>
				<descriptor>Image displays</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2264444</person_id>
				<author_profile_id><![CDATA[81466642855]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Koki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nagano]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tokyo Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264445</person_id>
				<author_profile_id><![CDATA[81466648620]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Takeru]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Utsugi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tokyo Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264446</person_id>
				<author_profile_id><![CDATA[81466645279]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Mika]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hirano]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tokyo Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264447</person_id>
				<author_profile_id><![CDATA[81100204487]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Takeo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hamada]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tokyo Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264448</person_id>
				<author_profile_id><![CDATA[81319501381]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Akihiko]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Shirai]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[CSWC, Tokyo Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264449</person_id>
				<author_profile_id><![CDATA[81100441141]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Masayuki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nakajima]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tokyo Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[
Shirai, A., et al. {information display}. <i>Japan patent application, filed No. 2010-088213</i> (6th April 2010), (IPC:G06F 21/20).
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[
Takeo Hamada, Koki Nagano, T. U. A. S. Scritter: A multiplexed image system for a public screen. <i>Proceedings of Virtual Reality International Conference (VRIC) Laval Virtual ReVolution 2010</i> (April 2010), pp. 321--323.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Figure 1: Multiplexed subtitles can be .ltered in realtime (left), system con.gure (middile), contents 
example: Naked/Clothed Maja(right) Abstract We have enabled the superimposition of multiplexed images 
on the same screen at the same time with tangible and stable equipment. Our multiplex images can be seen 
by wearing special con.gured polarized glasses, and the image projection method is designed to be based 
on current 3D stereoscopic technology, which is now prevalent and making rapid progress, thus high compatibility 
with current contents industries is retained. Therefore our system en­ables the wide range of applications 
with new expressions and can easily be put into production. 1 Motivation: Giving expansive variations 
for current 3D viewing technology This article suggests a new method for contents creation and broad­ens 
horizons of current 3D technologies. Presently, the contents of 3D digital cinemas are rapidly improving 
as many more theaters start installing 3D digital projectors. These stereo graphics, that are realized 
with active or passive glasses, enhance the expression of digital content and help prevent illegal copies 
of cinemas. Conse­quently, we utilize the infrastructures of 3D graphic projection as a means of contents 
productions and suggest new contents, multi­plexed contents . 2 Technology: retained compatibility with 
current 3D stereo display system We have realized the multiplex with no special modi.cation to current 
major stereoscopic projection methods in order to expand new expressions of contents with the compatibility 
and sustainable growth. In the passive stereo system, we have con.gured two dif­ferent image channels 
by using polarized DMD projectors (Christie DS+300W / ProjectionDesign F22SX+), instead of stereoscopic 
images of L and R, and two pairs of special con.gured glasses polarized differently between the pairs, 
but identical between the right and left eye. The synchronized source channels have been generated by 
network distributed contents players. Especially in the active stereo system, the glasses are electrically-activated 
time­ e-mail:scritter@shirai.la, shirai@mail.com division. The system is more complicated but it can 
realize more channels than that of passive by assigning channels to each viewer. As the passive stereo 
prototype that is connected with two PCs by our contents player program, we screened two different contents, 
The Naked Maja with Japanese subtitles and The Clothed Maja with French subtitles, and prepared two types 
of glasses so that visitors can enjoy the double contents by changing the glasses. We could con.rm that 
people from wide range of races and cultures can enjoy the different contents without explanation just 
by selecting the glasses at Laval Virtual ReVolution 2010. 3 Applications: Tangible Contents Selectors 
Already realized applications are as follows: (1) multi subtitles in cinema; (2)multi cultural commentary 
for arts in multi subtitles; (3) multi player drawing application. In connection with (2), we en­abled 
not only multi subtitles but also the channels change depend­ing upon user s age and comprehensive ability. 
Moreover,we cre­ated the magni.er shaped glasses and realized the seamless change of channels. As the 
specialized feature of this system, we can rec­ognize the value in the idea that users can easily choose 
information by the glasses, not but by the system. Also other possible applica­tions resonated with people 
are as follows: (4) multiplex of both video game and visual contents on a domestic screen ; (5) shar­ing 
messages with video from SNS like Twitter on public screen ; (6) information display on digital signage 
depending on types of glasses .lter and one s attributes ; (7) educational contents ; (8) game development 
; (9) combination with Augmented Reality tech­niques. We hope our system will be applied for content 
platforms like video games and 3D projectors standards and it will enrich new expressions of digital 
contents.  References SHIRAI, A., ET AL. [information display]. Japan patent applica­tion, .led No. 
2010-088213 (6th April 2010), (IPC:G06F 21/20). TAKEO HAMADA, KOKI NAGANO, T. U. A. S. Scritter: A multi­plexed 
image system for a public screen. Proceedings of Virtual Reality International Conference (VRIC) Laval 
Virtual ReVolu­tion 2010 (April 2010), pp. 321 323. Copyright is held by the author / owner(s). SIGGRAPH 
2010, Los Angeles, California, July 25 29, 2010. ISBN 978-1-4503-0210-4/10/0007 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1836932</article_id>
		<sort_key>870</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>80</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[A web system for creating and sharing 3D auditory contents]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836845.1836932</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1836932</url>
		<abstract>
			<par><![CDATA[<p>Recently, 3D movies such as "Avatar" have been popular because they can provide hyper reality. Most of these movies require special facility such as IMAX 3D. Therefore it has been difficult to introduce these movies to home environment. However, 3D TV for individual use has already been developed and is expected to become popular in a few years. In such situation, the demand of 3D contents for those systems will be higher and higher. However it is very difficult to create such contents because it requires exclusive tool, high technique and much cost.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>H.3.4</cat_node>
				<descriptor>World Wide Web (WWW)</descriptor>
				<type>P</type>
			</other_category>
			<other_category>
				<cat_node>H.5.5</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10002951.10003317.10003371.10003386.10003390</concept_id>
				<concept_desc>CCS->Information systems->Information retrieval->Specialized information retrieval->Multimedia and multimodal retrieval->Music retrieval</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010469.10010475</concept_id>
				<concept_desc>CCS->Applied computing->Arts and humanities->Sound and music computing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002951.10003317</concept_id>
				<concept_desc>CCS->Information systems->Information retrieval</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002951.10003152</concept_id>
				<concept_desc>CCS->Information systems->Information storage systems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2264450</person_id>
				<author_profile_id><![CDATA[81442611667]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Seiya]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Matsuda]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Graduate School of Engineering Kanazawa Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264451</person_id>
				<author_profile_id><![CDATA[81442605405]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Tomohito]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yamamoto]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[College of Information Science and Human Communication Kanazawa Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[
Hughes, D. E. 2009. Integrating and delivering sound using motion capture and multi-tiered speaker placement. In <i>Virtual and Mixed Reality</i>, 179--185.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[
Takahashi, K., Ikeda, S., and Yamamoto, T. 2009. Light aural display using network connected multiple computers. In <i>HCI International 2009 - Posters</i>, Springer, 401--405.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1276485</ref_obj_id>
				<ref_obj_pid>1275808</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[
van den Hengel, A., Dick, A., Thorm&#228;hlen, T., Ward, B., and Torr, P. H. S. 2007. Videotrace: rapid interactive scene modelling from video. In <i>SIGGRAPH '07: ACM SIGGRAPH 2007 papers</i>, ACM, New York, NY, USA, 86.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 A web system for creating and sharing 3D auditory contents Seiya Matsuda* Graduate School of Engineering 
Kanazawa Institute of Technology Tomohito Yamamoto College of Information Science and Human Communication 
Kanazawa Institute of Technology Figure 1: (a)System Overview, (b)Design System, (c)Download System 
 1 Introduction Recently, 3D movies such as Avatar have been popular because they can provide hyper reality. 
Most of these movies require spe­cial facility such as IMAX 3D. Therefore it has been dif.cult to introduce 
these movies to home environment. However, 3D TV for individual use has already been developed and is 
expected to be­come popular in a few years. In such situation, the demand of 3D contents for those systems 
will be higher and higher. However it is very dif.cult to create such contents because it requires exclusive 
tool, high technique and much cost. To solve these problems, some systems for creating 3D contents have 
been proposed. For example, van den Hengel et al. have devel­oped the system that can generate a 3D model 
by tracing a material shape on video images[van den Hengel et al. 2007]. For creating 3D auditory contents, 
the system that can generate contents for sur­round audio system by manipulating virtual sound object 
with real hand has been developed[Hughes 2009]. However these systems still require special tools and 
techniques. Therefore, in this research, we focus on 3D auditory content at .rst, and develop a web system 
that realizes to create and share 3D con­tents easily. In this system, user can design virtual sound 
space with an operation like paint software on a web browser and share them on the internet. Moreover 
user can enjoy the created contents with our 3D auditory display[Takahashi et al. 2009] that is composed 
of mobile devices (iPhone, iPod touch and PC), and that realize to place speakers in any 3D positions 
and numbers at low cost. 2 Overview of our web system Our system is composed of Design System for designing 
vir­tual sound space, Download System to download designed sound space, and Auditory Display that we 
have already developed. Fig.1(a) shows system overview. The system is divided into a sys­tem on the internet 
and a system in local environment. The design system (Fig.1(b)) has been developed by Ajax technol­ogy 
and GUI of the system has an appearance of paint soft. There­fore, users can design sound space as if 
drawing a picture with­ *e-mail:z6900706@venus.kanazawa-it.ac.jp e-mail:tyama@neptune.kanazawa-it.ac.jp 
out special knowledge. To design virtual sound space, .rstly, users enter some information to the input 
form and create new project. The canvas window (Fig.1(b)-1) is displayed after creating a new project. 
To create a new sound object, users open the input form, and enter Object Name , Sound Source File , 
Position of Sound Source and other options. If users want to move sound object, users just only drag 
and drop the sound object on the window. In case of setting animation, users set a time line bar, base 
point and moving path on the same canvas window. By repeating these proce­dures, users can design virtual 
sound space. After designing, users save it and publish them on the internet. Fig.1(c) shows the download 
system for downloading and sharing 3D auditory contents. This system enables to download contents by 
searching for information of contents ( Creator s Name , Cre­ator s Comment , User s Comment for Evaluation 
, Tags , etc). Users search the contents with these keywords, and save the packed data into local auditory 
display. After downloading the contents, user enjoy them by our auditory system that was placed in favorite 
position and numbers of speakers. 3 Future Works In future work, we will develop the system that can 
design and share not only sound space but also visual space by using multimodal device that integrates 
head-mount-display and our auditory system. With these systems, it will be possible to create and share 
audio­visual 3D contents on the internet. References HUGHES, D. E. 2009. Integrating and delivering 
sound using motion capture and multi-tiered speaker placement. In Virtual and Mixed Reality, 179 185. 
TAKAHASHI, K., IKEDA, S., AND YAMAMOTO, T. 2009. Light aural display using network connected multiple 
computers. In HCI International 2009 -Posters, Springer, 401 405. VAN DEN HENGEL, A., DICK, A., THORM 
¨ AHLEN, T., WARD, B., AND TORR, P. H. S. 2007. Videotrace: rapid interactive scene modelling from video. 
In SIGGRAPH 07: ACM SIGGRAPH 2007 papers, ACM, New York, NY, USA, 86. Copyright is held by the author 
/ owner(s). SIGGRAPH 2010, Los Angeles, California, July 25 29, 2010. ISBN 978-1-4503-0210-4/10/0007 
  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1836933</article_id>
		<sort_key>880</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>81</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Colorful Touch Palette]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836845.1836933</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1836933</url>
		<abstract>
			<par><![CDATA[<p>Previously, pictures were painted using tools such as crayons or even by hand. Surfaces such as canvases or walls, provided the tactile sensations of the drawing surface while painting. However, this tactile experience has got lost because of advances in computer graphics software. Besides, a conventional multi-touch interface [1] can not provide tactile sensation. We propose a novel interactive painting interface called "<i>Colorful Touch Palette</i>" that may help us to rediscover our creativity. The user can touch the canvas having the electrode, select or blend tactile textures of their choice, draw a line, and experience the tactile sensations of painting as shown in Figure 1. Various tactile textures can be created by blending textures as paints. This interface can be used to design complex spatial tactile patterns for haptic-friendly products. Moreover, this system can be potentially used to create novel tactile paintings.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Color, shading, shadowing, and texture</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010384</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Texturing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2264452</person_id>
				<author_profile_id><![CDATA[81466647546]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Yuki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hirobe]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264453</person_id>
				<author_profile_id><![CDATA[81331496816]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Shinobu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kuroki]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264454</person_id>
				<author_profile_id><![CDATA[81331503619]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Katsunari]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sato]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264455</person_id>
				<author_profile_id><![CDATA[81421594598]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Takumi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yoshida]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264456</person_id>
				<author_profile_id><![CDATA[81331499667]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Kouta]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Minamizawa]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264457</person_id>
				<author_profile_id><![CDATA[81365592882]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Susumu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tachi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1179159</ref_obj_id>
				<ref_obj_pid>1179133</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Han, J. Y. 2006. Multi-touch interaction wall. ACM SIGGRAPH Emerging Technology.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>962721</ref_obj_id>
				<ref_obj_pid>962710</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Kajimoto, H., Inami, M., Kawakami, N., AND Tachi, S. 2004 SmartTouch: Electric skin to touch the untouchable. IEEE CG&amp;A 24:36--43]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>835832</ref_obj_id>
				<ref_obj_pid>580521</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Kajimoto, H., Kawakami, N., Maeda, T., AND Tachi, S. 2001 Electrocutaneous Display as an Interface to a Virtual Tactile World, IEEE Virtual Reality Conf., Yokohama, Japan.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Colorful Touch Palette Yuki Hirobe* Shinobu Kuroki* Katsunari Sato* Takumi Yoshida* Kouta Minamizawa 
 Susumu Tachi *The University of Tokyo Keio University Figure 1: Concept drawing Figure 2: Tactual 
Cap 1. Introduction Previously, pictures were painted using tools such as crayons or even by hand. Surfaces 
such as canvases or walls, provided the tactile sensations of the drawing surface while painting. However, 
this tactile experience has got lost because of advances in computer graphics software. Besides, a conventional 
multi-touch interface [1] can not provide tactile sensation. We propose a novel interactive painting 
interface called Colorful Touch Palette that may help us to rediscover our creativity. The user can touch 
the canvas having the electrode, select or blend tactile textures of their choice, draw a line, and experience 
the tactile sensations of painting as shown in Figure 1. Various tactile textures can be created by blending 
textures as paints. This interface can be used to design complex spatial tactile patterns for haptic-friendly 
products. Moreover, this system can be potentially used to create novel tactile paintings. 2. Technical 
Innovations To realize the concept of this interface, following features are required. (1) Providing 
various types of tactile sensation that the user can recognize comfortably. (2) Creating new tactile 
textures by blending original textures. (3) Providing tactile feedback according to the motion and posture 
of the finger.  With regarding to feature (1), the previous electro-tactile stimulation system [2] could 
provide only uniform rough textures and could not provide grating convex patterns with a resolution higher 
than the electrode interval. We improved the technique of electro-tactile stimulation of spatial patterns, 
and we archived various patterns of tactile sensations. We provided various degrees of roughness by changing 
the intensity and controlling the distribution and variances of each electrode. We also virtually increased 
the spatial resolution by changing the stimulus points faster than the fingertip movements as occasion 
demanded, instead of synchronizing both movements. We designed the blending method of tactile textures 
for realizing feature (2). It is known that each polar stimulus can produce different sensations: sense 
of vibration by anodic stimulus and sense of pressure by cathodic stimulus [3]. To calculate the stimuli 
of the blended tactile textures, we defined a pressure model Mp(x,y) and a vibration model Mv(x,y) for 
each tactile texture and * e-mail: {yuki_hirobe,shinobu_kuroki,katsunari_sato, takumi_yoshida}@ipc.i.u-tokyo.ac.jp 
 e-mail: {kouta, tachi}@tachilab.org Figure 3: Touch Palette Figure 4: Texture Canvas (a) 2D canvas (b) 
3D canvas described a texture as T(Mp,Mv). When the user blends two different tactile textures (T1(Mp1, 
Mv1) and T2(Mp2,Mv2)), the blended texture (Tb(Mpb, Mvb)) can be calculated as follows: Mpb(x, y) = Mp1(x, 
y).Mp2(x, y) Mvb(x, y) = Mpb(x, y).(Mv1(x, y).Mv2(x, y)) For feature (3), we implemented temporal patterns 
of electro-tactile feedback. In previous works, the special distribution of electrode intensities in 
the finger pad was not controlled according to the human motion. Here, we measured the movement and the 
contact state of the finger, and used the data to provide tactile feedback according to the velocity 
and pressure of the finger. 3. System Configurations This system is composed of a Tactual Cap , Touch 
palette and Texture Canvas . Tactual Cap : a cap shaped device, as shown in Figure 2, that consists of 
a high-density electrode matrix for providing tactile feedback, a pressure sensor for estimating the 
contact state of the fingertip, and tracking markers for detecting the fingertip posture. Touch Palette 
: a palette shaped display that consists of a touch panel. Several types of visuo-tactile textures called 
tactile colors are arranged as shown in Figure 3. Texture Canvas : we propose two types of canvases 
a 2D canvas and a 3D canvas, as shown Figure 4. The 2D canvas consists of a touch panel monitor mounted 
on an easel. It is used for painting a tactile picture. The 3D canvas consists of a solid screen, a projector, 
and motion tracking cameras. It is used for surface prototyping. The user blendes the tactile colors 
together on the Touch Palette using a finger with the Tactual Cap, which creates various tactile colors. 
Then the user touches and draws a visuo-tactile painting on the Texture Canvas at will with tactile feedback. 
The user can appreciate the visuo-tactile picture he/she painted on the basis of the tactile feedback. 
The painted tactile pictures are archived, and other users can enjoy them with stimulating tactile sensations. 
 References [1] Han, J.Y. 2006. Multi-touch interaction wall. ACM SIGGRAPH Emerging Technology. [2] Kajimoto, 
H., Inami, M., Kawakami, N., AND Tachi, S. 2004 SmartTouch: Electric skin to touch the untouchable. IEEE 
CG&#38;A 24:36-43 [3] Kajimoto, H., Kawakami, N., Maeda, T., AND Tachi, S. 2001 Electrocutaneous Display 
as an Interface to a Virtual Tactile World, IEEE Virtual Reality Conf., Yokohama,Japan . Copyright is 
held by the author / owner(s). SIGGRAPH 2010, Los Angeles, California, July 25 29, 2010. ISBN 978-1-4503-0210-4/10/0007 
 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1836934</article_id>
		<sort_key>890</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>82</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[Force reflecting porous media with dynamic elasticity change]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836845.1836934</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1836934</url>
		<abstract>
			<par><![CDATA[<p>Liquid absorption affects the behavior of objects. Rain absorbed in the barrage can weaken its structure and cause the dam failure. A wet sponge ball bounces differently from a dry one. Porous media is a material that has internal pore space and is able to absorb liquid (e.g. a sponge or soil). Liquid absorption changes not only geometrical properties, e.g. volume, but also mechanical properties, e.g. elasticity. The aim of this study is to physically model the structural change of a porous media due to liquid absorption. Previous studies have focused on liquid flow inside the media[Lenaerts et al. 2008]. In contrast, this paper proposes a porous model that is able to simulate elastic change in a real sponge.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>H.5.2</cat_node>
				<descriptor>Haptic I/O</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10011752</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Haptic devices</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2264458</person_id>
				<author_profile_id><![CDATA[81100594294]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Yoshihiro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kuroda]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Osaka University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264459</person_id>
				<author_profile_id><![CDATA[81458650294]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Hirotoshi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ashida]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Osaka University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264460</person_id>
				<author_profile_id><![CDATA[81317494648]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Masataka]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Imura]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Osaka University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264461</person_id>
				<author_profile_id><![CDATA[81544909056]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Yoshiyuki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kagiyama]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Osaka University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264462</person_id>
				<author_profile_id><![CDATA[81100518076]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Osamu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Oshiro]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Osaka University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1360648</ref_obj_id>
				<ref_obj_pid>1360612</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Lenaerts, T., Adams, B., and Dutr&#233;, P. <i>2008. Porous flow in particle-based fluid simulations.</i> ACM Transactions on Graphics 27, <i>491--498</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Terzaghi, K. <i>1936. The shearing resistance of saturated soils and the angle between the planes of shear.</i> First international conference on soil Mechanics, <i>Vol.1, 54--59.</i>]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 ForceRe.ectingPorousMedia withDynamicElasticityChange YoshihiroKuroda HirotoshiAshida MasatakaImura 
YoshiyukiKagiyama OsamuOshiro OsakaUniversity 1 Introduction Liquidabsorptionaffects thebehaviorof objects.Rainabsorbed 
in the barrage can weaken its structure and cause the dam failure. A wet sponge ball bounces differently 
from a dry one. Porous media isamaterialthathasinternalporespaceandisable toabsorbliquid (e.g. aspongeorsoil).Liquidabsorptionchangesnotonlygeomet­rical 
properties, e.g. volume, but also mechanical properties, e.g. elasticity. The aim of this study is to 
physically model the struc­tural change of a porous media due to liquid absorption. Previous studieshavefocused 
on liquid.owinsidethemedia[Lenaertset al. 2008]. In contrast, this paper proposes a porous model that 
is able to simulate elastic change in a real sponge. 2 Simulating elasticchangeofporousmedia As shown 
in Fig. 1, the particle system represents porous media and allowselasticrepresentationwith thereal-timedynamicchange 
of mechanical properties. Porosity fi refers to volume fraction of pore space within the volume of particle 
i. Saturation Si refers to volumefractionof liquidintheporevolume. Figure 1: Simulation framework forporousmedia 
 The principle of effective stress divides the total stress si into ef­fective stress si ' andporepressure 
ui [Terzaghi1936]. si ' = si - ui . (1) Porepressureisthepressureofthe liquidintheporespace.Theef­fectivestress,whichis 
thestressacting on thesolid,decreaseswhen the pore pressure increases due to liquid absorption.The proposed 
porousmodel considersnon-linearitybetween theporepressure ui and the saturation Si to represent the rapid 
decrease of elasticity in thebeginning of liquidabsorption.(k*: coef.cients) ui = k1 Si + k2 Si 2 + k3 
Si 3 (2) A decrease in effective stress si ' increases theporespace(porosity fi ).Inmaterialphysics, 
theelasticchangeismodeledas thechange of theporosityasfollows: Ei (t)= Ei (0)(1 - ßfi (t)) (3) where 
Ei 0 ,Eit aretheYoung smodulusatinitial stateand at time t, respectively, and ß is a constant.  Figure 
2: Simulation results(liquid absorption: 0ml and0.8ml) , Young s modulus (kPa) 350 300 250 200 150 
100 50 0 Amount of water (ml) Figure 3: The measured and estimated elastic change Fig. 2 shows the deformation 
of the porous model by uni.ed ap­plied forces for several conditions of liquid absorption. The model 
was deformed to a larger extent when the liquid absorption in­creased.Fig.3 shows themeasuredYoung smodulusviathecom­pression 
test of a real sponge and the estimated Young s modulus via linearand non-linearporepressuremodels.Theresultscon.rm 
the proposed model s ability to simulate the elastic change of the real sponge. Acknowledgments This 
study was partly supported by the Global COE Program in silico medicine atOsakaUniversity,. References 
LENAERTS, T., ADAMS, B., AND DUTRE´, P. 2008. Porous .ow in particle-based.uid simulations. ACMTransactions 
onGraphics 27,491 498. TERZAGHI, K. 1936. The shearing resistance of saturated soils and the angle between 
the planes of shear. First international conference on soilMechanics,Vol.1,54 59.  Copyright is held 
by the author / owner(s). SIGGRAPH 2010, Los Angeles, California, July 25 29, 2010. ISBN 978-1-4503-0210-4/10/0007 
 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1836935</article_id>
		<sort_key>900</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>83</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>5</seq_no>
		<title><![CDATA[Gesture controlled interactive whiteboard based on SVM and fuzzy logic]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836845.1836935</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1836935</url>
		<abstract>
			<par><![CDATA[<p>Nowadays, one of the main focuses of the Human-computer interaction area is controlling computers by gestures. Various gesture types provide means of controlling user interfaces and applications. However, most of them involve the front-facing camera and the user's gestures are recognized often from the static background. In addition, colorful gloves, gloves with motion sensors or infrared diodes are often used for this purpose.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>H.5.2</cat_node>
				<descriptor>Haptic I/O</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>H.5.3</cat_node>
				<descriptor>Collaborative computing</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003120.10003130</concept_id>
				<concept_desc>CCS->Human-centered computing->Collaborative and social computing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10011752</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Haptic devices</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2264463</person_id>
				<author_profile_id><![CDATA[81466644077]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Michal]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lech]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Gdansk University of Technology in Poland]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264464</person_id>
				<author_profile_id><![CDATA[81100454798]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Bozena]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kostek]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Gdansk University of Technology in Poland]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Kelly, W., Painter, J. 1996, Hypertrapezoidal Fuzzy Membership Functions, <i>Fifth IEEE International Conference on Fuzzy Systems</i>, New Orleans, pp. 1279--1284.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Lech, M., Kostek, B., Czyzewski, A., Odya, P. 2009, Gesture Recognition Framework for Multimedia Content Viewer Controlling, <i>IEEE SPA 2009</i>, Pozna&#324;, Pl.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Gesture Controlled Interactive Whiteboard Based on SVM and Fuzzy Logic Michal Lech, Bozena Kostek Gdansk 
University of Technology in Poland*  (a) (b) (c) Figure 1: The picture displayed by multimedia projector 
(a), the corresponding frame from the camera with the perspective corrected (b) and the result of image 
subtracting after applying image processing methods (c) 1 Introduction Nowadays, one of the main focuses 
of the Human-computer interaction area is controlling computers by gestures. Various gesture types provide 
means of controlling user interfaces and applications. However, most of them involve the front-facing 
camera and the user s gestures are recognized often from the static background. In addition, colorful 
gloves, gloves with motion sensors or infrared diodes are often used for this purpose. The approach presented 
herein utilizes a camera placed behind the user, coupled with a multimedia projector. Gestures are recognized 
on the variable background. No special manipulators or infrared lights are necessary. To present the 
possibilities of such a use the Interactive Whiteboard application was developed. Besides the basic functionalities 
of writing, deleting the content, etc. the user can load images of various types and draw on them. Recognition 
of gestures enabling the user to rotate, zoom or to browse the images is provided by the system. Each 
gesture can be performed in very close proximity to the whiteboard (e.g. writing) or from a distance 
(e.g. for image handling). Since multimedia projectors are often a standard equipment of classrooms, 
the whiteboard presented may be a cheaper solution in comparison with existing interactive whiteboards 
based on projectors, special frames with sensors and electronic pens. 2 Our Approach The solution is 
based on subtracting the projected image (a) and the image extracted from the video stream (b), and recognizing 
gestures in the further processed, output (c). During the gesture recognition process SVM classifiers 
and fuzzy logic are employed [Lech et al. 2009]. First, the effective area of an image grabbed from the 
camera mounted on the ceiling projector is determined. This area, treated as a view of the image displayed 
by the projector, is determined by the user who points out positions of the image corners in the frame. 
Based on these positions, scaling of the projected image is performed to ensure identical dimensions 
with the image grabbed from the camera. Then, the perspective correction is performed. The next step 
consists in color calibration which reduces the impact of light conditions and distortion introduced 
by the camera lens, such as vignetting. During the calibration process five colored images (red, green, 
blue, white and black) are displayed. Tables of discrete constant values used in the later image processing 
are created as a result of subtraction of each displayed image and respective camera frame. Next, the 
processed frame obtained from the camera is subtracted from the projected image. As default, RGB subtracting 
is performed using only the green component. An appropriate value retrieved from the color calibration 
table is added to each output pixel. The result is binarized and subjected to separable median filtering 
with the square mask of size equal to 9x9 pixels. The obtained image is divided into a few vertical parts 
depending on the number of hands used for controlling (e.g. 4 for 2 persons). For each part SVM classifiers 
are used to match the palm shapes with masks describing the palm gestures. The velocity vectors are created 
based on hand positions between each two adjacent frames from the video stream, both separately in each 
part and within adjacent parts when no hand is found in a particular frame part. Magnitudes of the velocity 
vectors are determined by four triangular fuzzy membership functions. Directions of movements are determined 
by hypertrapezoidal fuzzy membership functions [Kelly and Painter 1996]. Employing fuzzy logic rules 
for the results obtained enable to recognize dynamic hand gestures (e.g. moving both hands farther apart). 
Each recognized gesture is interpreted according to the associated system event. Acknowledgments Research 
funded within the project No. POIG.01.03.01-22­017/08, entitled "Elaboration of a series of multimodal 
interfaces and their implementation to educational, medical, security and industrial applications". The 
project is subsidized by the European regional development fund and by the Polish State budget. References 
KELLY, W., PAINTER, J. 1996, Hypertrapezoidal Fuzzy Membership Functions, Fifth IEEE International Conference 
on Fuzzy Systems, New Orleans, pp. 1279-1284. LECH, M., KOSTEK, B., CZYZEWSKI, A., ODYA, P. 2009, Gesture 
Recognition Framework for Multimedia Content Viewer Controlling, IEEE SPA 2009, Poznan, Pl. * e-mail: 
{mlech, bozenka}@sound.eti.pg.gda.pl Copyright is held by the author / owner(s). SIGGRAPH 2010, Los Angeles, 
California, July 25 29, 2010. ISBN 978-1-4503-0210-4/10/0007  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1836936</article_id>
		<sort_key>910</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>84</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>6</seq_no>
		<title><![CDATA[GRENDL]]></title>
		<subtitle><![CDATA[grid enabled distribution and control for Laptop Orchestras]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836845.1836936</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1836936</url>
		<abstract>
			<par><![CDATA[<p>Laptop Orchestras (LOs) have recently become a very popular mode of musical expression. They engage groups of performers to use ordinary laptop computers as instruments and sound sources in the performance of specially created music software. By using an orchestral metaphor, LOs provide an engaging and challenging environment to experiment with human-computer interaction, network and machine latency, and sound/signal processing. While the LOs at Princeton and Stanford are perhaps the best known, LOs have now been established at many universities in the US and UK, and as private ensembles around the world.</p> <p>Perhaps the biggest challenge for LOs is the distribution, management and control of software across heterogeneous collections of networked computers. Software must be stored and distributed from a central repository, but launched on individual laptops immediately before performance. Each "composition" consists of unique combinations of software, user interfaces, and physical devices.</p> <p>Moreover, performers in a Laptop Orchestra can have a complex array of application layers to manage and launch before the start of a specific piece's performance. For example, one work written for our LO requires a bluetooth middleware application which reads gestures from a Wii-mote and converts them into OpenSoundControl [Wright and Freed 1997] messages to be forwarded to a custom Max application, all of which must be launched and configured before any performance. Combine this with the rapid turnaround from one composition to the next during a concert performance, and the problem of preparing members of the laptop orchestra with the appropriate tools for each piece becomes daunting.</p> <p>The GRENDL project leverages proven grid computing frameworks and approaches the Laptop Orchestra as a distributed computing platform for interactive computer music. This allows us to readily distribute software to each laptop in the orchestra depending on the laptop's internal configuration, its role in the composition, and the player assigned to that computer. Using the SAGA framework [Goodale et al.], GRENDL is able to run pre-distribution scripts on a master computer, distribute software to client computers, launch post-distribution scripts on the master computer and launch application scripts on client computers that in turn manage application environments for each composition.</p> <p>SAGA, the Simple API for Grid Applications, is a distributed computing middleware used to distribute, manage and process grid-based applications, typically for scientific research problems in such diverse fields as numerical relativity, computational fluid dynamics and materials science. Its functionality and stability are well regarded within the computational science community and SAGA has become a standard API for grid computing.</p> <p>Our initial experiments have demonstrated that SAGA can be used successfully in a concert environment. The Laptop Orchestra of Louisiana (LOLs) debut concert on April 14, 2010 used a prototype version of GRENDL to manage two of the seven works performed, and GRENDL worked flawlessly.</p> <p>GRENDL proposes to go further than just applying SAGA to the LO environment. We will use tangible and physical objects [Ullmer et al. 2008] to represent individuals, resources, roles and compositions such that GRENDL knows how to distribute software appropriate to the LO's environment, the individuals performing and their role in the composition. By using RFID-embeded objects [Ullmer et al. 2010], master and client computers determine who is at which computers, and what is being performed. Just as a music librarian knows where to place parts for each composition on which music stands, GRENDL will know where to send software and how to launch that software for each composition, laptop and performer.</p> <p>Extending SAGA to work with tangibles and in novel runtime environments, will require extensions to SAGA -- support for new interfaces and instruments -- as well as require some performance engineering in order for commands to be processed with lower latency than "traditional" distributed systems are designed to tolerate. The trans-disciplinary nature of GRENDL provides potential to shed new light on existing challenges in computational and computer science. The LO setting presents a unique perspective from which to investigate topics such as time-sensitive and dynamic job scheduling, latency-bound interaction, and effective user interfaces for grid computing environments. Some of the first iteration interaction technologies have been developed for distributed computational science applications, and some of what is learned through GRENDL will likely be applicable in that area.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[grid computing]]></kw>
			<kw><![CDATA[laptop orchestra]]></kw>
			<kw><![CDATA[music]]></kw>
			<kw><![CDATA[tangible interaction]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>C.2.4</cat_node>
				<descriptor>Distributed applications</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>J.5</cat_node>
				<descriptor>Performing arts (e.g., dance, music)</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010469.10010471</concept_id>
				<concept_desc>CCS->Applied computing->Arts and humanities->Performing arts</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10011007.10010940.10010971.10011120.10011680</concept_id>
				<concept_desc>CCS->Software and its engineering->Software organization and properties->Software system structures->Distributed systems organizing principles->Organizing principles for web applications</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2264465</person_id>
				<author_profile_id><![CDATA[81100558043]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Stephen]]></first_name>
				<middle_name><![CDATA[David]]></middle_name>
				<last_name><![CDATA[Beck]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Louisiana State University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264466</person_id>
				<author_profile_id><![CDATA[81421592256]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Shantenu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Jha]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Louisiana State University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264467</person_id>
				<author_profile_id><![CDATA[81100226633]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Brygg]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ullmer]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Louisiana State University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264468</person_id>
				<author_profile_id><![CDATA[81453641206]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Chris]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Branton]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Louisiana State University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264469</person_id>
				<author_profile_id><![CDATA[81466641600]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Sharath]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Maddineni]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Louisiana State University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Goodale, T., Jha, S., Kaiser, H., Kielmann, T., Kleijer, P., Merzky, A., Shalf, J., and Smith, C. A Simple API for Grid Applications (SAGA). OGF Document Series 90, http://www.ogf.org/documents/GFD.90.pdf.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1347436</ref_obj_id>
				<ref_obj_pid>1347390</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Ullmer, B., Sankaran, R., Jandhyala, S., Tregre, B., Toole, C., Kallakuri, K., Laan, C., Hess, M., Harhad, F., Wiggins, U., et al. 2008. Tangible menus and interaction trays: core tangibles for common physical/digital activities. In <i>Proceedings of the 2nd international conference on Tangible and embedded interaction</i>, ACM, 209--212.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1709904</ref_obj_id>
				<ref_obj_pid>1709886</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Ullmer, B., Dever, Z., Sankaran, R., Toole Jr, C., Freeman, C., Cassady, B., Wiley, C., Diabi, M., Wallace Jr, A., DeLatin, M., et al. 2010. Cartouche: conventions for tangibles bridging diverse interactive systems. In <i>Proceedings of the fourth international conference on Tangible, embedded, and embodied interaction</i>, ACM, 93--100.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Wright, M., and Freed, A. 1997. Open sound control: A new protocol for communicating with sound synthesizers. In <i>Proceedings of the 1997 International Computer Music Conference</i>, 101--104.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 GRENDL: GRid ENabled Distribution and control for Laptop orchestras Stephen David Beck* , Shantenu 
Jha, Brygg Ullmer, Chris Branton, Sharath Maddineni Center for Computation &#38; Technology Louisiana 
State University Abstract Laptop Orchestras (LOs) have recently become a very popular mode of musical 
expression. They engage groups of performers to use ordinary laptop computers as instruments and sound 
sources in the performance of specially created music software. By using an orchestral metaphor, LOs 
provide an engaging and challenging environment to experiment with human-computer interaction, net­work 
and machine latency, and sound/signal processing. While the LOs at Princeton and Stanford are perhaps 
the best known, LOs have now been established at many universities in the US and UK, and as private ensembles 
around the world. Perhaps the biggest challenge for LOs is the distribution, manage­ment and control 
of software across heterogeneous collections of networked computers. Software must be stored and distributed 
from a central repository, but launched on individual laptops imme­diately before performance. Each composition 
consists of unique combinations of software, user interfaces, and physical devices. Moreover, performers 
in a Laptop Orchestra can have a complex array of application layers to manage and launch before the 
start of a speci.c piece s performance. For example, one work written for our LO requires a bluetooth 
middleware application which reads gestures from a Wii-mote and converts them into OpenSoundCon­trol 
[Wright and Freed 1997] messages to be forwarded to a custom Max application, all of which must be launched 
and con.gured be­fore any performance. Combine this with the rapid turnaround from one composition to 
the next during a concert performance, and the problem of preparing members of the laptop orchestra with 
the ap­propriate tools for each piece becomes daunting. The GRENDL project leverages proven grid computing 
frame­works and approaches the Laptop Orchestra as a distributed com­puting platform for interactive 
computer music. This allows us to readily distribute software to each laptop in the orchestra depending 
on the laptop s internal con.guration, its role in the composition, and the player assigned to that computer. 
Using the SAGA frame­work [Goodale et al. ], GRENDL is able to run pre-distribution scripts on a master 
computer, distribute software to client com­puters, launch post-distribution scripts on the master computer 
and launch application scripts on client computers that in turn manage application environments for each 
composition. SAGA, the Simple API for Grid Applications, is a distributed com­puting middleware used 
to distribute, manage and process grid­based applications, typically for scienti.c research problems 
in such diverse .elds as numerical relativity, computational .uid dy­namics and materials science. Its 
functionality and stability are well regarded within the computational science community and SAGA has 
become a standard API for grid computing. Our initial experiments have demonstrated that SAGA can be 
used successfully in a concert environment. The Laptop Orchestra of Louisiana (LOLs) debut concert on 
April 14, 2010 used a prototype version of GRENDL to manage two of the seven works performed, and GRENDL 
worked .awlessly. GRENDL proposes to go further than just applying SAGA to the *e-mail: sdbeck@lsu.edu 
LO environment. We will use tangible and physical objects [Ullmer et al. 2008] to represent individuals, 
resources, roles and composi­tions such that GRENDL knows how to distribute software appro­priate to 
the LO s environment, the individuals performing and their role in the composition. By using RFID-embeded 
objects [Ullmer et al. 2010], master and client computers determine who is at which computers, and what 
is being performed. Just as a music librarian knows where to place parts for each composition on which 
music stands, GRENDL will know where to send software and how to launch that software for each composition, 
laptop and performer. Extending SAGA to work with tangibles and in novel runtime en­vironments, will 
require extensions to SAGA support for new interfaces and instruments as well as require some performance 
engineering in order for commands to be processed with lower la­tency than traditional distributed systems 
are designed to toler­ate. The trans-disciplinary nature of GRENDL provides potential to shed new light 
on existing challenges in computational and com­puter science. The LO setting presents a unique perspective 
from which to investigate topics such as time-sensitive and dynamic job scheduling, latency-bound interaction, 
and effective user interfaces for grid computing environments. Some of the .rst iteration inter­action 
technologies have been developed for distributed computa­tional science applications, and some of what 
is learned through GRENDL will likely be applicable in that area. CR Categories: C.2.4 [Computer-Communication 
Networks]: Distributed Systems Distributed Applications; J.5 [Computer Ap­plications]: Arts and Humanities 
Performing arts Keywords: grid computing, laptop orchestra, music, tangible in­teraction References 
GOODALE, T., JHA, S., KAISER, H., KIELMANN, T., KLEI-JER, P., MERZKY, A., SHALF, J., AND SMITH, C. A 
Simple API for Grid Applications (SAGA). OGF Document Series 90, http://www.ogf.org/documents/GFD.90.pdf. 
ULLMER, B., SANKARAN, R., JANDHYALA, S., TREGRE, B., TOOLE, C., KALLAKURI, K., LAAN, C., HESS, M., HARHAD, 
F., WIGGINS, U., ET AL. 2008. Tangible menus and interaction trays: core tangibles for common physical/digital 
activities. In Proceedings of the 2nd international conference on Tangible and embedded interaction, 
ACM, 209 212. ULLMER, B., DEVER, Z., SANKARAN, R., TOOLE JR, C., FREEMAN, C., CASSADY, B., WILEY, C., 
DIABI, M., WAL-LACE JR, A., DELATIN, M., ET AL. 2010. Cartouche: con­ventions for tangibles bridging 
diverse interactive systems. In Proceedings of the fourth international conference on Tangible, embedded, 
and embodied interaction, ACM, 93 100. WRIGHT, M., AND FREED, A. 1997. Open sound control: A new protocol 
for communicating with sound synthesizers. In Pro­ceedings of the 1997 International Computer Music Conference, 
101 104. Copyright is held by the author / owner(s). SIGGRAPH 2010, Los Angeles, California, July 25 
 29, 2010. ISBN 978-1-4503-0210-4/10/0007  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1836937</article_id>
		<sort_key>920</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>85</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>7</seq_no>
		<title><![CDATA[Guitar-leading band]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836845.1836937</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1836937</url>
		<abstract>
			<par><![CDATA[<p>Interfacing with the guitar using the audio signal is one of the oldest problems in Computer Music, and advances in the area were astonishing. In our days it is possible to simulate a huge range of amplifiers, apply many filter effects and evaluate the pitch of a plucked string robustly, to mention a few useful applications.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>H.5.2</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>H.5.5</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010469.10010475</concept_id>
				<concept_desc>CCS->Applied computing->Arts and humanities->Sound and music computing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002951.10003317.10003371.10003386.10003390</concept_id>
				<concept_desc>CCS->Information systems->Information retrieval->Specialized information retrieval->Multimedia and multimodal retrieval->Music retrieval</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2264470</person_id>
				<author_profile_id><![CDATA[81466640832]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[M.]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Cicconet]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Visgraf/IMPA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264471</person_id>
				<author_profile_id><![CDATA[81442611891]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[L.]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Velho]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Visgraf/IMPA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264472</person_id>
				<author_profile_id><![CDATA[81388598371]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[P.]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Carvalho]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Visgraf/IMPA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264473</person_id>
				<author_profile_id><![CDATA[81466647112]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[G.]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Cabral]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[D'Accord Music Software]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Guitar-Leading Band M. Cicconet L. Velho P. Carvalho G. Cabral Visgraf/IMPA Visgraf/IMPA Visgraf/IMPA 
D Accord Music Software (1) (2) (3) (4) Phalange rods (1), guitar .ducial (2), bad (3) and good (4) 
arrangement of notes. 1 Motivation Interfacing with the guitar using the audio signal is one of the old­est 
problems in Computer Music, and advances in the area were astonishing. In our days it is possible to 
simulate a huge range of ampli.ers, apply many .lter effects and evaluate the pitch of a plucked string 
robustly, to mention a few useful applications. However, there are problems very hard to solve using 
audio, like recognizing the chord being played when the musician is not down­or up-stroking all strings 
at once, but picking them one at a time. In this work we explore the visual interface of the guitar, 
a subject that only in recent years has received the proper attention. Three as­pects are treated. The 
.rst has just been mentioned: the problem of chord recognition when not all notes of the chord are played. 
The second relates to the implementation of an automatic composition algorithm inspired on the bi-dimensional 
nature of the representa­tion of the diatonic scale in the guitar fretboard. Finally, the knowl­edge 
about the current chord, or simply the rough position of the hand in the guitar fretboard, allows controlling 
some parameters of the automatic composition algorithm, what becomes specially in­teresting in live performance. 
 2 Chord Recognition and Hand Location A Supervised Machine Learning algorithm is used to learn the pat­terns 
of the rough positions of the .ngertips (in guitar-fretboard coordinates) corresponding to the chords 
we want to recognize. To determine the approximate position of the .ngertips in the scene, retro-re.exive 
rods are attached to the back of the .nger middle phalanges, by means of elastic ribbons (Figure 1). 
To locate the guitar, four circular retro-re.exive .ducials are attached to it (Fig­ure 2). Then, using 
infrared light and camera, .ducials and rods can be isolated, and a projective transformation is used 
to estimate the north-most extreme of the rods in guitar fretboard coordinates. The center of mass of 
the .nger points represents the rough position of the hand in the same coordinate system. 3 Automatic 
Composition Let us say we want to compose a melody using the diatonic scale, in the key of G. Figure 
3 shows almost all the notes of such a scale be­tween the .rst and the 19th fret of the guitar. Such 
a representation is not algorithmically friendly, due to the absence of a clear pat­tern. Fortunately 
we can rearrange the notes of the scale as shown in Figure 4. This way, it becomes easy to write routines 
to build a sequence of notes, i.e., a melodic line. We can, for example, imple­ment two independent markovian 
processes, one for the rows and the the other for the columns of the matrix of points. This kind of interface 
is more adequate for simulating guitar im­provisation, since the availability of a musical note near 
the current region of improvisation is also important, besides the note itself. The availability of a 
note changes depending on the dimension of the instrument interface, so a bi-dimensional method .ts better 
in the case of the guitar. The sequence of rhythmic patterns may also be controlled by a markovian process. 
Every time a new beat is about to begin, the system decides if one whole, two halfs, three thirds of 
four quarter notes should be played along it. Or even if no note should be played at all. Only then the 
melodic line is built. This time the information of the chord currently being played is relevant, because 
melody and harmony must combine. As an example, the algorithm may check if the .rst note of the sequence 
sampled for the next beat is the same (regardless the octave) of the current chord s root note. 4 Guitar-Led 
Piece It should be clear by now how the above mentioned bi­dimensional automatic composition algorithm 
and visual chord­recognition/hand-detection methods can be combined. Some pa­rameters of the former are 
controlled by the information provided by the latter. As a proof of concept we have composed a music 
piece in which those methods are explored. It is organized in cycles, bars and beats: four beats per 
bar and four bars per cycle. We have used four musical instruments: guitar, string ensemble, drums and 
piano, of which just the former is a real instrument. Most of the time the string ensemble follows exactly 
the chord that is being captured by the computer vision system, but in some cycles it can also perform 
a chord sequence memorized in previous cycles. After the drum loop is triggered by a keyboard command, 
the pre-programmed loops will run for a certain number of cycles, up to the end of the piece. Every time 
a new beat is about to begin, the Markov-process based sequences are sampled and resampled until the 
melody conditions are satis.ed or the maximum number of trials is reached. Eventu­ally the system turns 
the Air Guitar module on, so the location of the hand controls the region to where the sequence of notes 
has to converge. Although the improvisation is guitar-based, other sounds can be used. We chose the piano. 
Results are very encouraging. We see many areas that could bene.t from the proper exploitation of the 
visual aspect of the guitar inter­face, like teaching, music games and live performance, to cite a few. 
As future work we plan to eliminate the need of using .nger rods and to improve the restrictions of the 
guitar improvisation method, observing the human-hand joint system. More about this project can be found 
at: www.impa.br/~cicconet/thesis/guitar leading band. Copyright is held by the author / owner(s). SIGGRAPH 
2010, Los Angeles, California, July 25 29, 2010. ISBN 978-1-4503-0210-4/10/0007  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1836938</article_id>
		<sort_key>930</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>86</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>8</seq_no>
		<title><![CDATA[How people tend to organize sensory information into unified wholes in haptic phone?]]></title>
		<subtitle><![CDATA[focusing on cross modality interaction]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836845.1836938</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1836938</url>
		<abstract>
			<par><![CDATA[<p>Considerations of interface design have been limited to the senses of sight and hearing. However, as the sense of touch, such as haptics, began to be applied to equipment, new interaction has emerged. Due to the integrated nature of people (Goldstein, 2002), it is important for a new system that added tactile stimuli to correctly analyze and understand users' experiences. This study analyzes integrated cross modality user experiences from devices providing information on the senses of sight, hearing, and touch.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>H.5.2</cat_node>
				<descriptor>Haptic I/O</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10011752</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Haptic devices</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Human Factors</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2264474</person_id>
				<author_profile_id><![CDATA[81466645601]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Ji-Hye]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[An]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Hong-Ik University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264475</person_id>
				<author_profile_id><![CDATA[81466642530]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Su-Jin]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lee]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Yonsei University, Korea]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[
Goldstein, E. B. 2007. <i>Sensation &amp; Perception, 7</i>th edition. Belmont, CA: Wadsworth.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[
Strauss, A. &amp; Corbin, J. 1990. <i>Basics of Qualitative Research: Grounded Theory, Procedures, and Techniques.</i> Sage Publications, Newbury Park, CA.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 How People Tend to Organize Sensory Information into Unified Wholes in Haptic Phone? - Focusing on Cross 
Modality Interaction Ji-Hye An l Visual Communication Design. Hong-Ik University * Su-Jin Lee l HCI Lab 
@ Yonsei University, Korea ** Contextual Conditions Inter-relative Situation 1. Introduction Considerations 
of interface design have been limited to the senses of sight and hearing. However, as the senseoftouch,suchashaptics, 
began to be applied to equipment, new interaction has emerged. Due to the integrated nature of people(Goldstein, 
2002), it is important for a new system that added tactile stimuli to correctly analyze andunderstand 
users experiences. This study analyzes integrated cross modality user experiences from devices providing 
information on the senses of sight, hearing, and touch. 2. Methodoogy The research procedure is basedon 
agroundedtheory(Strauss&#38;A. Corbin,1990).The grounded theory is a process of inductive research and 
study that establishes a theory through collection of related data and analysis.To recruit participants 
fitting for the objective of the study, the research was conducted by selecting those who have used devices 
for over six months.The average age of participants was 28.2, while 39% of themwerefemales and 41%were 
males.Theinterview time was from 1.5 hour to 2 hours, including a break time. 3. Analysis  At Open 
coding stage,users experienceswereclassified bypurchase chances or interactions with devices at usual 
times with situational elements and sensory information before being conceptualized. The second stage 
is the axial coding and categories could be drawn by stages of interaction as seen from <Figure 2>. Classes 
of derived phenomenon from the axialcoding stagewere categorizedbasedon configuration ways between sensory 
information. Cross modality can be classified into transformation phenomenon, replacement phenomenon,and 
closure phenomenon.It arranges users experiences in the orderofinter-relative situations,contextual conditions,arbitrating 
situations, phenomenon, action/interaction, and results. 1.Transformation phenomenon is one of the cross 
modality experiences that provides information on more than two senses, and users divide sensory information 
into main and sub senses, so that they recognize main sensory information strengthened by sub -sensory 
information. At this time, in order to strengthen specific sensory information, ambiguous information 
is transformed to main information while unrelated information is excluded or reduced in the process 
of integration. Copyright is held by the author / owner(s). SIGGRAPH 2010, Los Angeles, California, 
July 25 29, 2010. ISBN 978-1-4503-0210-4/10/0007  The configuration way of sensory information that 
influences users cross modality experiences Transformation, Closure and Replacement phenomenons Characteristics 
of sensory information Results Understanding about the relationship between sensory information Focusing 
on emotional understanding of design elements of sensory information Designed using a combination of 
sensory information [ Figure 2. Procedure of analysis for In-depth interviews ] 2.Closure phenomenon 
alsoreferstotheoneofthecrossmodality experiences that makes users remember past experience through sensory 
information of sight,hearing,and touch.Atthismoment, the impact of past experience is stronger, as the 
suggested sensory information is more specific and vivid.Moreover, changes ofsensory information should 
be occurred within the expectable level of users. 3. Replacement phenomenon means one of the cross modality 
experiences that provides information on more than two senses. When certain sensory information is shielded 
, the other sensory information replaced the shield information and was recognized by a user. This occurs 
more easily when the level of similarity between shielded sensory information and replacement sensory 
information is high and when the kinds of replacement sensory information are diverse.  4. Conclusion 
Since people embrace information easily without extra cognitive process, usersmightconsidercrossmodality 
asasimplephenomenon, but actually cross modality is not simple and does not occur in the same way. From 
the study, it was determined that cross modality phnomenon strongly influences the emotional level of 
users, due to the significant increase of ascross modality phenoenon of information devices providingthetactilesenseand 
the activeinteraction between private devices and users.This is because tactile information is actively 
introduced as the touch function of recent information devices strengthens. In particular, in mobile 
phones, the characteristics that users carry all the time in private fit well with the characteistics 
of tactile sense, which is sensitive and privatized sensory information. In transformation phenomenon, 
audio and visual senses intervene as main stimuli while the tactile sense intervenes as sub -stimuli 
amplifying themainstimuli.In careofreplacement phenomenon, we can know that tactile sense is utilized 
as a substitution of audio and visual senses. For closure phenomenon also, it is tactile phenomenon that 
most directly and vividly recalls past experience. References Goldstein, E.B. 2007. Sensation &#38; 
Perception, 7th edition. Belmont, CA:Wadsworth. Strauss,A. &#38; Corbin, J. 1990. Basics of Qualitative 
Research: Grounded Theory, Procedures, and Techniques. Sage Publications, Newbury Park,CA. Acknowledgements 
Immersion Corporation and 9FruitsMedia generously supported this research. We would like to appreciate 
Seungji Yoo and Kahyun Huh for their supports.   E- mail : * jihye2723@naver.com|** Corresponding 
Author : thecolor@yonsei.ac.kr 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1836939</article_id>
		<sort_key>940</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>87</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>9</seq_no>
		<title><![CDATA[Image retrieval using collaborative filtering and visual navigation]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836845.1836939</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1836939</url>
		<abstract>
			<par><![CDATA[<p>Internet image search systems mostly use words from the context of the web page containing the image as keywords. The performance of these search systems is rather poor, as the search systems neither know the intention of the searching user nor the semantic relationships of these images. Content-based image retrieval (CBIR) systems rely on the assumption that similar images share similar visual features. Despite intense research efforts, the results of CBIR systems have not reached the performance of text based search engines. The main problem of CBIR systems is the semantic gap between the content that can be described with low-level visual features and the description of image content that humans use with high-level semantic concepts. Some image retrieval systems have combined the keyword and the content-based visual search approach. However with this approach many images may be found that semantically do not match. In addition semantically similar images that visually look different cannot be found at all.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[CBIR]]></kw>
			<kw><![CDATA[collaborative techniques]]></kw>
			<kw><![CDATA[semantic modeling]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.4.3</cat_node>
				<descriptor>Filtering</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>H.3.3</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010224</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002951.10003317</concept_id>
				<concept_desc>CCS->Information systems->Information retrieval</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2264476</person_id>
				<author_profile_id><![CDATA[81100282063]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Kai]]></first_name>
				<middle_name><![CDATA[Uwe]]></middle_name>
				<last_name><![CDATA[Barthel]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Applied Sciences, Berlin]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264477</person_id>
				<author_profile_id><![CDATA[81466644625]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Sebastian]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[M&#252;ller]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Applied Sciences, Berlin]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264478</person_id>
				<author_profile_id><![CDATA[81466648577]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[David]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Backstein]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Applied Sciences, Berlin]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264479</person_id>
				<author_profile_id><![CDATA[81323494132]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Dirk]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Neumann]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Applied Sciences, Berlin]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264480</person_id>
				<author_profile_id><![CDATA[81466644898]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Klaus]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Jung]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Applied Sciences, Berlin]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Image Retrieval using Collaborative Filtering and Visual Navigation Kai Uwe Barthel, Sebastian M¨uller, 
David Backstein, Dirk Neumann, Klaus Jung * HTW, University of Applied Sciences, Berlin  Figure 1: (a) 
A visually sorted display of 355 Flickr sun.ower images. Due to the visually sorted display particular 
images can easily be found. The user can select candidate images to re.ne the search. (b) These selected 
images are used to retrieve visually similar images and to model the semantic relationship between the 
images. Every time a user selects two or more candidate images a weighted link between these images is 
updated to build a network of image relationships. (c) This network of semantic image relationships can 
be navigated visually. Keywords: CBIR, collaborative techniques, semantic modeling 1 Introduction Internet 
image search systems mostly use words from the context of the web page containing the image as keywords. 
The performance of these search systems is rather poor, as the search systems neither know the intention 
of the searching user nor the semantic relation­ships of these images. Content-based image retrieval 
(CBIR) sys­tems rely on the assumption that similar images share similar visual features. Despite intense 
research efforts, the results of CBIR sys­tems have not reached the performance of text based search 
engines. The main problem of CBIR systems is the semantic gap between the content that can be described 
with low-level visual features and the description of image content that humans use with high-level se­mantic 
concepts. Some image retrieval systems have combined the keyword and the content-based visual search 
approach. However with this approach many images may be found that semantically do not match. In addition 
semantically similar images that visually look different cannot be found at all. Collaborative .ltering 
uses the known preferences of a group of users to make recommendations of the unknown preferences for 
other users. Recommender systems using these methods have been used very successfully to suggest similar 
music, books or prod­ucts. Up to now collaborative .ltering has not been used for image retrieval. On 
the one hand this is due to the fact that usually the number of images to be searched is extremely high. 
On the other hand existing image search systems do not allow to learn relations between different images 
from the users interaction. In this work we propose a new image search system using key­words, low-level 
visual features, and collaborative .ltering tech­niques to generate a network of semantic inter-image 
relationships. Unlike other approaches our new system does not try to learn the *e-mail: barthel@htw-berlin.de 
degree of con.dence between images and associated keywords. We rather propose to model the degree of 
similarity between images by building up a network of linked images. The weights of the inter­image links 
are learned from the users interaction with the search system only. 2 Our Approach Conventional image 
search systems typically display 20 -50 result images at a time. In a .rst step our image search system 
retrieves a much larger set of result images. This set will be displayed visually sorted, allowing to 
inspect up to 1000 images simultaneously. Due to this large number of images it is most likely that the 
user will .nd and select several candidate images that are close to his desired search result. In a next 
step this candidate set is used to retrieve further visually similar images. But even more important 
is the fact that these candidate images that have been selected together can be used to model the inter-image 
relationships. Each time two or more images have been selected together their link weights will be increased. 
By collecting candidate sets from many searches we successively build a semantic network of weighted 
links of image relationships. This network of semantic image relationships can be used to im­prove the 
image search results by retrieving images that are linked to the query images. Opposed to existing systems 
our new image search system can retrieve semantically similar images that do not share the same keyword 
or the same visual appearance. In addi­tion the search results will get better every time a new search 
is performed. For images that are linked to a suf.ciently large number of other images a visual navigation 
mode may be enabled. In this mode all connected images are displayed using a force-based graph layout 
scheme. This visual navigation mode enables the user to navigate through large image collections in a 
totally new way just by follow­ing the semantic links -independent of any language or keywords. Copyright 
is held by the author / owner(s). SIGGRAPH 2010, Los Angeles, California, July 25 29, 2010. ISBN 978-1-4503-0210-4/10/0007 
  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1836940</article_id>
		<sort_key>950</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>88</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>10</seq_no>
		<title><![CDATA[Integrated space]]></title>
		<subtitle><![CDATA[authoring in an immersive environment with 3D body tracking]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836845.1836940</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1836940</url>
		<abstract>
			<par><![CDATA[<p>Our research explores the use of real-time computer vision techniques and a pair of standard computer cameras to provide 3D human body awareness in an inexpensive, immersive environment system, Fig. 1. The goal is to enhance the user experience of immersion in a virtual scene that is displayed by a 3D screen. We combine stereo vision and stereo projection to allow for both the user and the virtual scene to become aware of each others 3D presence as part of a single, integrated 3D space.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.4.8</cat_node>
				<descriptor>Tracking</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245.10010253</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems->Tracking</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010225.10010227</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision tasks->Scene understanding</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2264481</person_id>
				<author_profile_id><![CDATA[81100132667]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Paulo]]></first_name>
				<middle_name><![CDATA[F. U.]]></middle_name>
				<last_name><![CDATA[Gotardo]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The Ohio State University, Columbus (OH)]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264482</person_id>
				<author_profile_id><![CDATA[81335496510]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Alan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Price]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The Ohio State University, Columbus (OH)]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Integrated Space: Authoring in an Immersive Environment with 3D Body Tracking Paulo F.U. Gotardo, Alan 
Price* The Advanced Computing Center for the Arts and Design The Ohio State University, Columbus (OH) 
 1 Introduction Our research explores the use of real-time computer vision tech­niques and a pair of 
standard computer cameras to provide 3D human body awareness in an inexpensive, immersive environment 
system, Fig. 1. The goal is to enhance the user experience of immer­sion in a virtual scene that is displayed 
by a 3D screen. We combine stereo vision and stereo projection to allow for both the user and the virtual 
scene to become aware of each others 3D presence as part of a single, integrated 3D space. We focus on 
enabling authoring applications based on the direct manipulation of virtual objects, with users interacting 
from a .rst­person perspective. This emphasis contrasts with the avatar-based, mostly reactive focus 
often employed in the design of computer game interfaces. We note that our work is part of an effort 
to develop low-cost solutions that provide interaction designers with means to prototype ideas easily 
and in anticipation of future re­leases of similar technology in the mainstream of HCI applications. 
Here, we present a prototype system that demonstrates the interac­tion paradigm above. No markers or 
other special user-born equipment is required. The user s presence in front of the projection screen 
is automatically de­tected, Fig. 1(a), and head tracking dynamically updates the camera frustum, adjusting 
the 3D output. This capability signi.cantly en­hances the experience of immersion as it allows for 3D 
objects to be seen from different points of view as the user moves sideways or closer/farther from the 
screen. Hand detection enables the user to directly interact with virtual objects by touching and moving 
them in the scene. Rapid hand gestures allow captured objects to be re­leased. In the metro scene in 
Fig. 1(b), the user is presented with a scale model of a city and can reorganize its layout by grabbing 
and re­locating buildings. The 3D location of the user s hand is indicated by a small green sphere that 
serves as an optional hand avatar. The user can trigger different actions by touching a simple heads-up­display 
(HUD) controller that virtually .oats on the right of the 3D space. These actions are: grabbing buildings 
with a white, spherical object that extends the user s reach farther into the scene; drawing or sculpting 
3D curves that turn into 3D scene objects; and .ying (moving) to a different location of the city. Simulation 
of rigid body dynamics enhances the user interaction experience in this new 3D space. While we do not 
present a new stereo vision algorithm or a novel 3D projection device, we propose a simple and effective 
two-way visual interface unencumbered by controllers, tracking markers, or other user-borne devices. 
Only the use of polarized glasses is re­quired for the perception of the 3D output. Our system does not 
recognize .ne details such as individual .ngers due to limitations in the current technology of stereo 
imaging. Even state-of-the-art stereo algorithms are still incapable of providing depth estimates in 
real-time, for a small 0.08 megapixel (320 × 240 pixels) image pair. However, our interactive system 
is an engineering solution and creative interaction design that provides a quality real-time user ex­perience 
balanced against computation limitations. *e-mail: {pgotardo,aprice}@accad.osu.edu Figure 1: Integrated 
space: (a) one of the two input images and the computed depthmap showing all detected body parts; (b) 
the 3D projection screen, stereo cameras, and infrared illuminators. 2 Technical Approach Our 3D projection 
system, Fig. 1(b), was built in-house following a known procedure. The large-scale rear screen projection 
of a vir­tual scene provides depth perception for users wearing 3D glasses. Centered atop the projection 
screen is a pair of cameras with wide­angle lenses. Two infrared illuminators, on the top left and right 
corners of the screen, allow for stereo imaging to be done with low visible light, as required for optimal 
visualization of the 3D output. We use the fast stereo imaging routines of the computer vision library 
OpenCV (http://opencv.willowgarage.com) and compute sparse depthmaps at a coarse image resolution. This 
is done at 30 fps on a quad-core desktop processor. We also developed our own software that smoothly 
extrapolates depth estimates to nearby pixels with unknown depth, originated by low-contrast image re­gions. 
Our computer vision routines also perform body and head detection with a random sampling algorithm that 
generates and val­idates hypothetical, whole-body cylindrical models. A similar sub­sequent procedure 
looks for arms and hands using an articulated arm model. Our software can tolerate pixels with poor or 
unknown depth estimates and successfully analyze rough depthmaps com­puted with fast but less accurate 
stereo algorithms. A possible extension enabled by our current system is to use ges­ture recognition 
by matching the 3D curves, originated by the time­trajectory of a users hand, against prede.ned curves. 
This func­tionality can be used to complement or replace our HUD virtual controller used to trigger different 
actions. Our system provides a visual experience of immersion that is sim­ilar to expensive GeoWallTMsystems. 
However, this capability is achieved without the need for the user to learn how to handle com­plex trackers 
or control devices, allowing for passers-by to imme­diately begin interaction with applications. Our 
video-based track­ing system allows for multiple users with simultaneous points of interaction. Our system 
comprises a combination of inexpensive, off-the-shelf hardware and software that is reproducible, recon­.gurable, 
and expandable to accommodate new technological ad­vances as they become readily available. Copyright 
is held by the author / owner(s). SIGGRAPH 2010, Los Angeles, California, July 25 29, 2010. ISBN 978-1-4503-0210-4/10/0007 
  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1836941</article_id>
		<sort_key>960</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>89</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>11</seq_no>
		<title><![CDATA[LLP+]]></title>
		<subtitle><![CDATA[multi-touch sensing using cross plane infrared laser light for interactive based displays]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836845.1836941</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1836941</url>
		<abstract>
			<par><![CDATA[<p>Multi-touch sensing exists in a number of applications and is presently used in personal computing devices (i.e. laptops and desktop computers), mobile touch screens, kiosks, Interactive wall displays (i.e. subway station map), ATMS, and any display requiring an interactive platform. Current multi-touch sensing methods use capacitive and or resistive based touchscreens both which are expensive and difficult to make. Infrared based touchscreens is being studied as an alternative method that is effective and low-cost solution of producing equal results particularly with large interactive displays.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.4.0</cat_node>
				<descriptor>Image displays</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>H.5.2</cat_node>
				<descriptor>Haptic I/O</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10011752</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Haptic devices</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2264483</person_id>
				<author_profile_id><![CDATA[81466645024]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Jae-Hee]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Park]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Yonsei University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264484</person_id>
				<author_profile_id><![CDATA[81100050988]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Tackdon]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Han]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Yonsei University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1095054</ref_obj_id>
				<ref_obj_pid>1095034</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[
Han, J. Y. 2005. Low-cost multi-touch sensing through frustrated total internal reflection. In <i>UIST '05: Proceedings of the 18th annual ACM symposium on User interface software and technology</i>, ACM, New York, NY, USA, 115--118.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[
NUI, 2009. Nui group community, community core vision (ccv) software. http://ccv.nuigroup.com/.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 LLP+: Multi-Touch Sensing Using Cross Plane Infrared Laser Light for Interactive Based Displays Jae-Hee 
Park* Tackdon Han Media System Labs, Yonsei University Figure 1: (a) LLP+ showing touch input detection 
without occlusion. (b) An occluding object (hazard) in front of touch input (.nger). (c) Shows two planes 
of infrared light . The X-Laser Plane detects objects without occlusion while the Y-Laser Plane detects 
touch input points with object hazard. 1 Introduction Multi-touch sensing exists in a number of applications 
and is presently used in personal computing devices (i.e. laptops and desktop computers), mobile touch 
screens, kiosks, Interactive wall displays (i.e. subway station map), ATMS, and any display requir­ing 
an interactive platform. Current multi-touch sensing methods use capacitive and or resistive based touchscreens 
both which are expensive and dif.cult to make. Infrared based touchscreens is be­ing studied as an alternative 
method that is effective and low-cost solution of producing equal results particularly with large interac­tive 
displays. Previous research using infrared based touch screens is not new and have been studied using 
several implementations most notably Jef­ferson Hans FTIR [Han 2005] implementation. However, each im­plementation 
has several limitations and a trade-off decision must be made when considering which technology to use. 
FTIR does not suffer from occlusion, but cannot detect objects and is well suited for detecting .ngers 
and other point-based touch inputs. Mi­crosoft s Surface uses a diffuse based infrared light source but 
does not do well with high ambient light. Another technology uses an in­frared light plane on top of 
the interaction surface but suffers from occlusion when objects are placed in front of the touch input. 
Our approach combines two technologies together to effectively eliminate limitations placed on the touch 
screen when only one im­plementation is used and reduces the complexity of design and cost. 2 Our Approach 
Laser Light Plane Cross (LLP+) creates an occlusion free zone for interaction on display surfaces. It 
also increases the ability to de­tect multiple touch inputs other than .ngers such as edges for object 
recognition. Since our design allows for the detection of edges, it offers the possibility of affordances 
to recognizing objects such as *e-mail: james.park@msl.yonsei.ac.kr hantack@colorzip.com  Figure 2: 
Results showing touch inputs (.ngers) detected with four occluding objects. mobile phones, wine bottles, 
cameras, and other large objects. Our design does not require a complaint surface to enhance touch ex­perience. 
Unlike previous methods using infrared lasers our design does not suffer from occlusion while handling 
normal touch input detection without hazards (Figure1a). It is an occlusion free sur­face and can recognize 
a touch input even when hazards block the line of sight of the laser module (as seen in .gure 1b). The 
dual plane created by the lasers (as described in Figure 1c) is needed to handle obstructing objects. 
We tested our method on small piece of acrylic and used a four-block object hazard scheme to impede the 
touch input source. We used the Community Core Vision (CCV) software [NUI 2009] to track the points and 
detect edges created by the hazard (as seen in Figure 2). References HAN, J. Y. 2005. Low-cost multi-touch 
sensing through frustrated total internal re.ection. In UIST 05: Proceedings of the 18th annual ACM symposium 
on User interface software and tech­nology, ACM, New York, NY, USA, 115 118. NUI, 2009. Nui group community, 
community core vision (ccv) software. http://ccv.nuigroup.com/. Copyright is held by the author / owner(s). 
SIGGRAPH 2010, Los Angeles, California, July 25 29, 2010. ISBN 978-1-4503-0210-4/10/0007  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1836942</article_id>
		<sort_key>970</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>90</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>12</seq_no>
		<title><![CDATA[Long-term memory retention and recall of collected personal memories]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836845.1836942</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1836942</url>
		<abstract>
			<par><![CDATA[<p>To remember important information, we often take pictures and arrange them into collections. Photos can also be gathered and organized via personal lifelogs and social media websites which may include contextual metadata such as location, participants, rating, and even emotional tags. However, memories and connections between places, events, and people can be difficult to recollect. Memory recall in our brain can depend on several factors: emotional level, context variability, loss of information during encoding, etc. As time passes, memories are gradually forgotten or become altered, e.g. due to collision with newly encoded information [Yi Chen 2010].</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>J.4</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010455</concept_id>
				<concept_desc>CCS->Applied computing->Law, social and behavioral sciences</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2264485</person_id>
				<author_profile_id><![CDATA[81322495368]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Norbert]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gy&#337;rb&#237;r&#243;]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Aizu]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264486</person_id>
				<author_profile_id><![CDATA[81363598819]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Henry]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Larkin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Aizu]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264487</person_id>
				<author_profile_id><![CDATA[81406592137]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Michael]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Cohen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Aizu]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[
Kornell, N. 2009. Optimising Learning Using Flashcards: Spacing Is More Effective Than Cramming. <i>Applied Cognitive Psychology 1317</i>, January, 1297--1317.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1785479</ref_obj_id>
				<ref_obj_pid>1785455</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[
Yi Chen, G. J. J. 2010. Augmenting human memory using personal lifelogs. In <i>Proc. First Augmented Human Int. Conf.</i>
]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Long-term Memory Retention and Recall of Collected Personal Memories Norbert Gy.orb´ir´o *, Henry Larkin 
, and Michael Cohen University of Aizu  Figure 1: Left side: the automatically selected photo .rst turned 
into a memory cue by blurring the subject s face. Right side: the blurred image is presented to the user 
to facilitate active review by guessing who is in the picture before revealing the original photo. 1 
Introduction To remember important information, we often take pictures and arrange them into collections. 
Photos can also be gathered and organized via personal lifelogs and social media websites which may include 
contextual metadata such as location, participants, rat­ing, and even emotional tags. However, memories 
and connections between places, events, and people can be dif.cult to recollect. Memory recall in our 
brain can depend on several factors: emo­tional level, context variability, loss of information during 
encod­ing, etc. As time passes, memories are gradually forgotten or be­come altered, e.g. due to collision 
with newly encoded information [Yi Chen 2010]. Spaced repetition is a learning technique that reviews 
learning ma­terial in increasing intervals of time. It has been shown to en­hance retention rate of information 
over a longer span of time (see e.g. [Kornell 2009]). As such, it can be used to potentially boost long-term 
memory retention of collected personal memories. 2 Our Approach A novel photo view and review personal 
information system is pro­posed to increase retention and recall of collected memories by uti­lizing 
the spacing effect in learning and presenting a context-aware selection of photos as the learning material. 
The primary aim is to improve memory connections by automatically creating visual photo journeys around 
the following themes: Events. Events can consist of multiple sub-events and multi­ple participants or 
be around the same location. For example, a trip to Thailand may be comprised of several meals, day trips 
to the beach and forest, and with several people involved.  People. Our feelings regarding people in 
our lives are often drawn from shared experiences. By linking an image series for a person with experiences 
over varying periods of time and varying places, we can increase the connectivity associated with that 
person in our mind.  Each visual journey contains a collection of pictures, in which pho­tos are selected 
in connection to the chosen event or person. The *e-mail: ngyorbiro@acm.org e-mail: research@logic.nu 
e-mail: mcohen@u-aizu.ac.jp available metadata is utilized to discover relations between pho­tos and 
vary the composition of journeys to increase the number of memory cues, thus stimulating retention and 
recall of personal memories. Review times are scheduled for visual journeys. For ef­fective learning, 
photos are turned into memory cues by blurring or hiding parts of the image. Transformations are done 
automatically, e.g. by relying on the automatic face detection feature common in photo album applications 
and blurring the corresponding area. This prompts the user to actively think before the original, unmodi.ed 
photo is displayed. In general when using a spaced repetition sys­tem, the success of recall from retrieval 
cues is evaluated through user rating and used for scheduling subsequent review times. This would mean 
that the user devotes some time of the day to actively review scheduled parts of visual journeys. While 
this may be ac­ceptable for a group of users, we are also considering a passive, background review mode 
where user evaluation is optional. The system would assign ratings based on the user s physiological 
sig­nals, such as heart rate, and utilizing gaze tracking to con.rm the user s target of attention. Our 
system hopes to invoke a curiosity effect, through which a user will naturally be curious as to what 
picture will be next, forcing them to relate what they have seen with other memories. Complementary to 
still photos, audio clips, music, and video media may also be used as memory stimuli. The system could 
be applied to mobile phones, laptops &#38; desktop computers, digital picture frames, and TV gaming and 
media sys­tems, including online social media. Such devices and services can then be linked to a single 
database via a computer network. 3 Further possible applications Memory triggering also has other functions 
in daily life. Due to the emotion-arousing ability of memories, it also becomes possible to trigger moods, 
e.g. to attain a positive effect from certain triggers such as inspirational photos capturing moments 
when one may have performed his or her personal best. References KORNELL, N. 2009. Optimising Learning 
Using Flashcards: Spac­ing Is More Effective Than Cramming. Applied Cognitive Psy­chology 1317, January, 
1297 1317. YI CHEN, G. J. J. 2010. Augmenting human memory using per­sonal lifelogs. In Proc. First Augmented 
Human Int. Conf. Copyright is held by the author / owner(s). SIGGRAPH 2010, Los Angeles, California, 
July 25 29, 2010. ISBN 978-1-4503-0210-4/10/0007  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1836943</article_id>
		<sort_key>980</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>91</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>13</seq_no>
		<title><![CDATA[MusicSpace]]></title>
		<subtitle><![CDATA[you "play" the music]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836845.1836943</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1836943</url>
		<abstract>
			<par><![CDATA[<p>Hearing is one of human's five senses. In our daily life, we usually guess where we are and the surrounding conditions not only by the visual feedbacks of the surrounded scene, but also by environmental sounds. For example, subway stations usually hint people the door closing by an urgent sound. In Taiwan, the garbage trucks usually broadcast one special song, and people can judge whether the car is coming. Similarly, we usually can recognize our familiar people only by hearing the sounds they generated without actually seeing them. For example, John usually bats basketball while entering the room. Hence, before he enters the room, the familiar sounds is heard, and can be recognized. Moreover, through the sense of hearing, people can only use their peripheral attention to quickly know where they are and what happens.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>H.5.5</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010469.10010475</concept_id>
				<concept_desc>CCS->Applied computing->Arts and humanities->Sound and music computing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002951.10003317.10003371.10003386.10003390</concept_id>
				<concept_desc>CCS->Information systems->Information retrieval->Specialized information retrieval->Multimedia and multimodal retrieval->Music retrieval</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2264488</person_id>
				<author_profile_id><![CDATA[81466646300]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Chun-Yu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tsai]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[National Taiwan University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264489</person_id>
				<author_profile_id><![CDATA[81448597922]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Hung-Jung]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[National Taiwan University of Technology Science]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264490</person_id>
				<author_profile_id><![CDATA[81466646125]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Tzu-Hao]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kuo]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[National Taiwan University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264491</person_id>
				<author_profile_id><![CDATA[81416600159]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Kai-Yin]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Cheng]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[National Taiwan University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264492</person_id>
				<author_profile_id><![CDATA[81466643167]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[I-Chao]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Shen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[National Taiwan University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264493</person_id>
				<author_profile_id><![CDATA[81100108039]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Bing-Yu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Chen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[National Taiwan University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264494</person_id>
				<author_profile_id><![CDATA[81100043933]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>7</seq_no>
				<first_name><![CDATA[Rung-Huei]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Liang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[National Taiwan University of Technology Science]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1085981</ref_obj_id>
				<ref_obj_pid>1085939</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Pellarin, L., B&#246;ttcher, N., Olsen, J. M., Gregersen, O., Sarafin, S., and Guglielmi, M. 2005. Connecting strangers at a train station. In <i>Proceedings of 2005 Conference on New Interfaces for Musical Expression</i>, 152--155.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1378042</ref_obj_id>
				<ref_obj_pid>1377999</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Stahl, C. 2007. The roaring navigator: a group guide for the zoo with shared auditory landmark display. In <i>Proceedings of 2007 International Conference on Human Computer Interaction with Mobile Devices and Services</i>, 383--386.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1057038</ref_obj_id>
				<ref_obj_pid>1056808</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Warren, N., Jones, M., Jones, S., and Bainbridge, D. 2005. Navigation via continuously adapted music. In <i>ACM CHI 2005 Extended Abstracts</i>, 1849--1852.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Chun-Yu Tsai* * Figure 1: (a) Meet friends through Meeting Mode. (b) Find destination through Navigation 
Mode. 1 Introduction Hearing is one of human s .ve senses. In our daily life, we usually guess where 
we are and the surrounding conditions not only by the visual feedbacks of the surrounded scene, but also 
by environmental sounds. For example, subway stations usually hint people the door closing by an urgent 
sound. In Taiwan, the garbage trucks usually broadcast one special song, and people can judge whether 
the car is coming. Similarly, we usually can recognize our familiar people only by hearing the sounds 
they generated without actually seeing them. For example, John usually bats basketball while entering 
the room. Hence, before he enters the room, the familiar sounds is heard, and can be recognized. Moreover, 
through the sense of hearing, people can only use their peripheral attention to quickly know where they 
are and what happens. Music and physical space are seldom associated together. Only few work combined 
them. The G-series music player1 takes the earth as the disk and the satellite as the needle. The most 
useful way is to combine the music with navigation. Warren et al. [2005] proposed Ontrack which is a 
system that navigating by music. Christoph [2007] proposed the roaring navigator which is a guiding system 
used in the zoo. Music can also be used socially. Pellarin et al. [2005] performed the project with their 
virtual instrument in­stalled at a train station. While people walked into the prede.ned areas, the sounds 
ware generated, and the social communication might be built between strangers. 2 Implementation The 
core design concept of MusicSpace is that there are full of mu­sic notes spreaded out in the space, and 
every object in the space can be represented as one music. In our design, we do not only encode music 
onto every landmark, but also encode music onto ev­ery people. Therefore, MusicSpace can not only help 
people to learn the environment, but also can let people use it as a social plat­form. We implemented 
our system based on the prevalent mobile technologies by retrieving the GPS information. We designed 
our *e-mail:{apfelpuff,kakukogou,keynes,jdily}@cmlab.csie.ntu.edu.tw e-mail:{liang,m9710303}@mail.ntust.edu.tw 
e-mail:robin@ntu.edu.tw 1http://www.g-turns.com/ system into two levels, which are people-to-people 
and people-to­environment interactions. For people-to-people interaction, we encode each individual as 
one featured song, and design two modes for this application, which are friend and social modes. The 
friend mode allows users recognizing their friends while they are nearby. The social mode allows them 
to meet a new friend in the space by checking the playlist which are composed of nearby strangers. For 
people-to-environment interac­tion, we encode each landmark or area as one featured song, and design 
two modes, which are environment and navigation modes. The environment mode helps the users to recognize 
where they are and the navigation mode is used for .nding speci.c locations. However, the key is what 
music the system should play, because there might be several landmarks nearby and moreover, landmarks 
might be in an area which also has its own represented music. Therefore, if there are several landmarks 
nearby, we play the sound of the nearest landmark louder. Besides, we also play the sounds of nearby 
landmarks by a modi.ed Round Robin algorithm. Accord­ing to the hierarchical songs, which means that 
broader area con­tains several small areas, we designed the playing algorithm accord­ing to user s moving 
speed. If the user moves with high speed, they should take an overview of the environment. Therefore, 
the system will play the broader area s representative song rather than the small areas songs. However, 
by considering the most famous landmarks in the city, the weighted algorithm also takes the assigned 
degree­of-interest scores into account. References PELLARIN, L., B¨ N., OLSEN, OTTCHER, J. M., GREGERSEN, 
O., SARAFIN, S., AND GUGLIELMI, M. 2005. Connecting strangers at a train station. In Proceedings of 2005 
Conference on New Interfaces for Musical Expression, 152 155. STAHL, C. 2007. The roaring navigator: 
a group guide for the zoo with shared auditory landmark display. In Proceedings of 2007 International 
Conference on Human Computer Interaction with Mobile Devices and Services, 383 386. WARREN, N., JONES, 
M., JONES, S., AND BAINBRIDGE, D. 2005. Navigation via continuously adapted music. In ACM CHI 2005 Extended 
Abstracts, 1849 1852. Copyright is held by the author / owner(s). SIGGRAPH 2010, Los Angeles, California, 
July 25 29, 2010. ISBN 978-1-4503-0210-4/10/0007  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1836944</article_id>
		<sort_key>990</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>92</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>14</seq_no>
		<title><![CDATA[Narrative image composition using objective and subjective tagging]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836845.1836944</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1836944</url>
		<abstract>
			<par><![CDATA[<p>With recent advancements in digital photography, data storage and network technologies, publishing and sharing of digital images in Internet has been drastically increased. High popularity and growth of internet image libraries such as Flickr and Picasa are good examples for these trends. In order to enable easy browsing and searching, online storages store meta-data in the form of keywords to describe images. Meta-data could be a general description of the image, a specific tag or an annotation to describe spatial information within the image. In order to ease and improve the efficiency of tagging process, image processing and analysis algorithms has been combined to manual tagging systems [Yang et al. 2009]. Furthermore, meta-data can be used to create interesting presentations of images. Specially in exhibition displays, automated slide shows and digital photo frames, meta data are used to group related images and present them under different categories. However, presentations generated by most of the existing systems are limited to sequential displaying of individual images as correlated groups. In this paper we present a system that composite digital images in a narrative fashion utilizing objective and subjective tagging information. Proposed system can extract a region from one image and composite it into another image according to available meta-data. As a proof of concept we created a right to left continuous image panning application using Adobe Flash which combines different images according to their similarities and composite them together to create a narrative image presentation.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.2.4</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010187</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Knowledge representation and reasoning</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10003790</concept_id>
				<concept_desc>CCS->Theory of computation->Logic</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2264495</person_id>
				<author_profile_id><![CDATA[81456616335]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Anusha]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Withana]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264496</person_id>
				<author_profile_id><![CDATA[81466647610]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Rika]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Matsui]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264497</person_id>
				<author_profile_id><![CDATA[81100344143]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Maki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sugimoto]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264498</person_id>
				<author_profile_id><![CDATA[81335491293]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Kentaro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Harada]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Olympus Corporation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264499</person_id>
				<author_profile_id><![CDATA[81335492043]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Masa]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Inakage]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1406354</ref_obj_id>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Segaran, T. 2007. <i>Programming Collective Intelligence.</i> O'Reilly Media.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1699333</ref_obj_id>
				<ref_obj_pid>1698924</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Yang, K., Wang, M., and Zhang, H.-J. 2009. Active tagging for image indexing. In <i>Proceedings of the IEEE International Conference on Multimedia and Expo, ICME</i>, IEEE, 1620--1623.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Narrative Image Composition Using Objective and Subjective Tagging Anusha Withana *, Rika Matsui , Maki 
Sugimoto , Kentaro Harada , Masa Inakage Keio University, Japan Olympus Corporation Figure 1: Shows 
tag information and .nal composite image according to tags. (a) image description: desert mountain; objective 
tag (yellow): car; subjective tag (white): people; (b) image description: African desert; objective tag 
(yellow): grass; subjective tag (white): people; (c) image description: car; (d) image description: African 
American women; (e) image description: grass; (f) image description: group of people; (g) Part of the 
.nal composite image, matched and composite into a single image according to the meta-data 1 Introduction 
With recent advancements in digital photography, data storage and network technologies, publishing and 
sharing of digital images in Internet has been drastically increased. High popularity and growth of internet 
image libraries such as Flickr and Picasa are good exam­ples for these trends. In order to enable easy 
browsing and search­ing, online storages store meta-data in the form of keywords to describe images. 
Meta-data could be a general description of the image, a speci.c tag or an annotation to describe spatial 
informa­tion within the image. In order to ease and improve the ef.ciency of tagging process, image processing 
and analysis algorithms has been combined to manual tagging systems [Yang et al. 2009]. Fur­thermore, 
meta-data can be used to create interesting presentations of images. Specially in exhibition displays, 
automated slide shows and digital photo frames, meta data are used to group related im­ages and present 
them under different categories. However, pre­sentations generated by most of the existing systems are 
limited to sequential displaying of individual images as correlated groups. In this paper we present 
a system that composite digital images in a narrative fashion utilizing objective and subjective tagging 
infor­mation. Proposed system can extract a region from one image and composite it into another image 
according to available meta-data. As a proof of concept we created a right to left continuous image panning 
application using Adobe Flash which combines different images according to their similarities and composite 
them together to create a narrative image presentation. 2 Our Approach Most of the existing image annotation 
systems are limited to objec­tive tagging. For an example, users will tag or annotate contents of the 
image such as a visible object or an animal which exists in the target image. These information are very 
useful to extract infor­mation about the image, its contents and correlate them with other images with 
similar contents. However, they provide little help to composite an external image object between the 
existing content of the image. Our goal is to extract image features such as humans, animals or objects 
from one image and composite them into a sin­gle narrative image sequence. In order to achieve this goal, 
we use subjective tagging of images. *e-mail:anusha@kmd.keio.ac.jp In subjective tagging, users are allowed 
to tag contents on the im­ages which are probable to exist but currently not available. For an example, 
let us say an image has a scenery of sea and a user can mark a polygon region on the sea and tag it as 
a boat even though it does not actually exist on the image. It is similar to a container in the image 
which is suitable to hold a boat. Subjective and objective tags are separately stored in a relational 
database. System uses a keyword count based Pearson correlation analysis algorithm to generate correlation 
indexes between each keywords and tags [Segaran 2007]. Two types of correlations are created between 
tagged contents. First is relationship between ob­jective tags and subjective tags using keyword counts. 
Secondly, intra-image spatial correlation index to derive content types which are always closely located 
with each other in an image. For an example, a tag to describe sea can be highly correlated to a tag 
describing a ship because these two tags can coexist in multiple images. In our image panning application, 
initial image to display is pre­selected from a repository. Relationship index between subjective and 
objective tags are used to .nd a possible content to .ll in a region marked as a subjective tag on the 
selected image. Once a probable matches are found, feature marked in the objective tag is extracted by 
cropping out rest of the image and composite it on to the target image. Furthermore, intra-image spatial 
correlation index is used to composite possible other contents that could coexist in the same target 
image. Once the intra-image composition is .nished, system calculates a matching image to attached to 
the right of the target image to make a continuous sequential panning of images. If possible, objective 
tags are used to composite a bridging feature be­tween adjacent images. This process continues till system 
.nishes displaying all available images. References SEGARAN, T. 2007. Programming Collective Intelligence. 
O Reilly Media. YANG, K., WANG, M., AND ZHANG, H.-J. 2009. Active tagging for image indexing. In Proceedings 
of the IEEE International Conference on Multimedia and Expo, ICME, IEEE, 1620 1623. Copyright is held 
by the author / owner(s). SIGGRAPH 2010, Los Angeles, California, July 25 29, 2010. ISBN 978-1-4503-0210-4/10/0007 
  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1836945</article_id>
		<sort_key>1000</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>93</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>15</seq_no>
		<title><![CDATA[O-Link]]></title>
		<subtitle><![CDATA[augmented object system for intergenerational communication]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836845.1836945</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1836945</url>
		<abstract>
			<par><![CDATA[<p>We propose O-Link, a system that allows us to convey our experience by binding digital videos to a real object in order to facilitate intergenerational communications. We focus on two factors in designing our interface to build a closer relationship between grandparents and grandchildren.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>H.5.1</cat_node>
				<descriptor>Artificial, augmented, and virtual realities</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010392</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Mixed / augmented reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010866</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Virtual reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010866</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Virtual reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2264500</person_id>
				<author_profile_id><![CDATA[81466645573]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Toshihiro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nakae]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[NTT Comware Corporation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264501</person_id>
				<author_profile_id><![CDATA[81351600223]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Shiro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ozawa]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[NTT Comware Corporation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264502</person_id>
				<author_profile_id><![CDATA[81466640540]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Naoya]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Miyashita]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[NTT Comware Corporation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1394472</ref_obj_id>
				<ref_obj_pid>1394445</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Nunes, M., Greenberg, S., and Neustaedter, C. 2008. Sharing digital photographs in the home through physical mementos, souvenirs, and keepsakes. In <i>DIS '08: Proceedings of the 7th ACM conference on Designing interactive systems</i>, ACM, New York, NY, USA, 250--260.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 O-Link: Augmented Object System for Intergenerational Communication Toshihiro NAKAE Shiro OZAWA Naoya 
MIYASHITA § NTT COMWARE CORPORATION NTT COMWARE CORPORATION NTT COMWARE CORPORATION  Figure 1: A real 
origami conveys grandchild s experience to grandparents 1 Introduction We propose O-Link, a system that 
allows us to convey our experi­ence by binding digital videos to a real object in order to facilitate 
intergenerational communications. We focus on two factors in de­signing our interface to build a closer 
relationship between grand­parents and grandchildren. First, a generation gap related to acquired skills 
still exists between digital citizens and most elderly people. Even if grandparents want to watch videos 
of their grandchildren online, it is often so dif.cult for them to manipulate video sharing systems that 
they tend to hesi­tate from playing the videos. We believe tangible user interface can facilitate elderly 
people s access to digital information. Second, handicraft activities play an important role in children 
s daily life. Physical handicrafts made by grandchildren, such as origami, sewing, and drawing give grandparents 
a window into the kids growth which makes the grandparents happy. Although children s works have such 
a powerful impact, the quality of work varies from child to child. Most children subconsciously have 
a de­sire to be praised for not only what they have made but how they made it. 2 Our Approach Figure 
1 depicts the overview of a typical usage scenario of O-Link. A grandchild makes a physical craft, say 
origami, to give to her grandparents while recording what she is doing. O-Link has a video editing application 
that allows users to link a physical ob­ject and the video clips that were recorded and stored in the 
system. After the process of linking, she sends the origami to her grandpar­ents by postal mail. Since 
the data can be shared over the network, if the origami is put down on the box in grandparents house, 
the related video clips appear on the screen automatically. As shown in Figure 2, the shape of an object 
is captured by the box-shaped device equipped with a camera inside. The object itself act as its tag. 
To organize and retrieve digital contents, we used physical objects directly related to the data. Physical 
objects have so many affor­dances that even the people with low computer literacy can easily access corresponding 
data. e-mail: nakae@nttcom.com e-mail: ozawa.shiro@nttcom.co.jp §e-mail: miyashita.naoya@nttcom.co.jp 
  Figure 2: Left: O-Link System s feature to link an object and the related video clips. Right: Play 
back video clips from an object Attaching tags like RFID or QR code is the most common technique for 
object recognition [Nunes et al. 2008]. While such techniques have the capability to detect objects exactly, 
it can be invasive and cumbersome when used in daily life. Non-tagging-based O-Link enables children 
to present a handmade gift as it is. By just laying origami on the box, it starts telling a child s story, 
for example, what she was trying to make or how she got involved in its activity. 3 Discussion and Future 
Work We developed the .rst prototype of O-Link to embody our concept. Most of the people using this system 
gave us positive and infor­mative comments like Even a clumsy shaped origami that a three­year-old child 
made can change into your treasure. For the second prototype, we plan to implement a feedback feature 
from grandpar­ents to grandchildren. We also plan to quantitatively evaluate the effect of this system 
on the intergenerational communication style for the next step. References NUNES, M., GREENBERG, S., 
AND NEUSTAEDTER, C. 2008. Sharing digital photographs in the home through physical me­mentos, souvenirs, 
and keepsakes. In DIS 08: Proceedings of the 7th ACM conference on Designing interactive systems, ACM, 
New York, NY, USA, 250 260. Copyright is held by the author / owner(s). SIGGRAPH 2010, Los Angeles, California, 
July 25 29, 2010. ISBN 978-1-4503-0210-4/10/0007  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1836946</article_id>
		<sort_key>1010</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>94</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>16</seq_no>
		<title><![CDATA[Perception of perspective distortions of man-made virtual objects]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836845.1836946</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1836946</url>
		<abstract>
			<par><![CDATA[<p>In computer graphics one is often concerned with representing 3D objects on 2D displays, which provide often only a limited <i>display field of view</i> (DFOV) to the observer. Usually, planar geometric projections, in particular linear <i>perspective projections</i>, are applied, which make use of a straightforward mapping of graphical entities in a 3D view <i>frustum</i> to a 2D image plane. Corresponding to the DFOV introduced for computer screens, the aperture angle of the virtual camera is often denoted as <i>geometric field of view</i> (GFOV) [Kjelldahl and Prime 1995]. Projections of virtual objects on a computer screen are affected by the interplay between the GFOV that is used to render the scene, and the DFOV (see Figure 1). In this context, only little research has been conducted to identify perspective projections that appear realistic to users. Instead, graphics designers and developers often choose GFOVs that vary significantly from the DFOV [Steinicke et al. 2009].</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Virtual reality</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010866</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Virtual reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2264503</person_id>
				<author_profile_id><![CDATA[81313483772]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Frank]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Steinicke]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of M&#252;nster]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264504</person_id>
				<author_profile_id><![CDATA[81317491340]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Gerd]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bruder]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of M&#252;nster]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264505</person_id>
				<author_profile_id><![CDATA[81100153883]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Scott]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kuhl]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Michigan Technological University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Kjelldahl, L., and Prime, M. 1995. A study on how depth perception is affected by different presentation methods of 3D objects on a 2D display. <i>Computers &amp; Graphics 19</i>, 2, 199--202.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1643940</ref_obj_id>
				<ref_obj_pid>1643928</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Steinicke, F., Bruder, G., Kuhl, S., Willemsen, P., Lappe, M., and Hinrichs, K. 2009. Judgment of natural perspective projections in head-mounted display environments. In <i>Proceedings of ACM Symposium on Virtual Reality Software and Technology (VRST)</i>, 35--42.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Perception of Perspective Distortions of Man-Made Virtual Objects Frank Steinicke and Gerd Bruder Scott 
Kuhl Visualization and Computer Graphics Department of Computer Science University of M¨unster Michigan 
Technological University (a) GFOV=10. (b) GFOV=20. (c) GFOV=30. (d) GFOV=40. (e) GFOV=50. (f) GFOV=60. 
Figure 1: Perspective distortions of a Utah teapot rendered with different geometric .elds of view and 
adapted viewpoints. 1 Introduction and Background In computer graphics one is often concerned with representing 
3D objects on 2D displays, which provide often only a limited dis­play .eld of view (DFOV) to the observer. 
Usually, planar geo­metric projections, in particular linear perspective projections, are applied, which 
make use of a straightforward mapping of graphical entities in a 3D view frustum to a 2D image plane. 
Corresponding to the DFOV introduced for computer screens, the aperture angle of the virtual camera is 
often denoted as geometric .eld of view (GFOV) [Kjelldahl and Prime 1995]. Projections of virtual ob­jects 
on a computer screen are affected by the interplay between the GFOV that is used to render the scene, 
and the DFOV (see Fig­ure 1). In this context, only little research has been conducted to identify perspective 
projections that appear realistic to users. In­stead, graphics designers and developers often choose 
GFOVs that vary signi.cantly from the DFOV [Steinicke et al. 2009]. In this work we take some .rst steps 
to analyze the user s ability to detect perspective distortions of man-made virtual objects dis­played 
on a computer screen. We describe a psychophysical exper­iment, which reveals how computer graphics projections 
have to be adjusted such that users perceive a realistic view to a virtual object. 2 Psychophysical 
Experiment 2.1 Procedure For the experiment we recruited 20 (age 23-32, Ø : 26.1) experts in the domain 
of computer graphics, architectural design, 3D mod­eling, and CAD, each with at least 4 years professional 
experience. Subjects were positioned in front of two screens (from which one was dissembled) with their 
head .xed by a chin-rest, resulting in a DFOV of 26.. We arranged a physical Utah teapot inside the frame 
of the dissembled right screen, whereas on the left screen we dis­played sequentially two images of a 
corresponding virtual replica of the physical teapot that we rendered with different GFOVs. In each trial 
the subject s task was to decide, which of the two rendered teapots was a more realistic model of the 
real teapot (based on a two-alternative force choice task). We tested gains gF between 0.2 and 1.8 in 
steps of 0.2, which were applied to the GF OV = 26. . 2.2 Results Figure 2 shows the pooled results 
together with the standard error over all subjects for the discrimination task. The x-axis shows the 
applied gain gF , the y-axis shows the probability that subjects es­timate the virtual teapot that was 
rendered with a larger GFOV as a more realistic model of the physical teapot. From the .tted sig­moid 
function we determined an insigni.cant bias for the point of subjective equality (PSE =0.9891). The PSE 
shows that if the GFOV matches the DFOV, subjects perceive physical and virtual teapot as identical objects 
in terms of metric properties and propor­tions. We measured lower and upper (75%) detection thresholds 
at gains gF =0.4 and gF =1.6.  Figure 2: Pooled results of the discrimination between physical and virtual 
teapots.  3 Conclusion The results of our experiment suggest that perspective projections of virtual 
objects are perceived as most realistic by subjects when the GFOV matches the DFOV. In addition, when 
the GFOV used for perspective rendering varies less than ±60%, subjects cannot detect reliably a perspective 
distortion of a rendered 3D object. References KJELLDAHL, L., AND PRIME, M. 1995. A study on how depth 
percep­tion is affected by different presentation methods of 3D objects on a 2D display. Computers &#38; 
Graphics 19, 2, 199 202. STEINICKE, F., BRUDER, G., KUHL, S., WILLEMSEN, P., LAPPE, M., AND HINRICHS, 
K. 2009. Judgment of natural perspective projections in head-mounted display environments. In Proceedings 
of ACM Symposium on Virtual Reality Software and Technology (VRST), 35 42. Copyright is held by the author 
/ owner(s). SIGGRAPH 2010, Los Angeles, California, July 25 29, 2010. ISBN 978-1-4503-0210-4/10/0007 
  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1836947</article_id>
		<sort_key>1020</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>95</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>17</seq_no>
		<title><![CDATA[Playing the QWERTY keyboard]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836845.1836947</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1836947</url>
		<abstract>
			<par><![CDATA[<p>Computer hardware and music softwares have evolved to such a level that, in our days, it is possible to compose high quality music using only a simple laptop equipped with the proper applications.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>H.5.5</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010469.10010475</concept_id>
				<concept_desc>CCS->Applied computing->Arts and humanities->Sound and music computing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002951.10003317.10003371.10003386.10003390</concept_id>
				<concept_desc>CCS->Information systems->Information retrieval->Specialized information retrieval->Multimedia and multimodal retrieval->Music retrieval</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2264506</person_id>
				<author_profile_id><![CDATA[81466640832]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Marcelo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Cicconet]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Vision and Graphics Laboratory, IMPA, Rio de Janeiro]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264507</person_id>
				<author_profile_id><![CDATA[81388598371]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Paulo]]></first_name>
				<middle_name><![CDATA[Cezar]]></middle_name>
				<last_name><![CDATA[Carvalho]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Vision and Graphics Laboratory, IMPA, Rio de Janeiro]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[
Cicconet, M., Franco, T., and Carvalho, P. C. 2010. Plane tessellation with musical scale tiles and bidimensional automatic composition. In <i>International Computer Music Conference.</i>
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1279771</ref_obj_id>
				<ref_obj_pid>1279740</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[
Fiebrink, R., Wang, G., and Cook, P. 2007. Don't forget your laptop: Using native input capabilities for expressive music control. In <i>New Interfaces for Musical Expression.</i>
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1816929</ref_obj_id>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[
Hewitt, M. 2008. <i>Music Theory for Computer Musicians.</i> Course Technology / Cengage Learning.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Playing the QWERTY Keyboard Marcelo Cicconet Paulo Cezar Carvalho Vision and Graphics Laboratory, IMPA, 
Rio de Janeiro  (a) (b) (c) Figure 1: Chromatic-scale (a), heptatonic-scale (b) and pentatonic-scale 
(c) keyboards. Computer hardware and music softwares have evolved to such a level that, in our days, 
it is possible to compose high quality music using only a simple laptop equipped with the proper applications. 
However, the interface for entering musical notes in softwares run­ning on laptops has not evolved. If 
music writers and performers want a reasonable way to interact with musical applications, they usually 
have to make use of an external MIDI Keyboard, i.e., a piano-like interface (Figure 2(a)), what can be 
a problem when mo­bility is a concern. This work has two purposes. First, to introduce a simple mapping 
between the keys of the QWERTY keyboard and the notes of pen­tatonic and heptatonic musical scales, thus 
presenting an alternative to the external musical interfaces. Second, to urge computer man­ufacturers 
to implement the necessary properties for the QWERTY keyboard to be fully used as a musical input device, 
especially re­garding polyphony and key-down velocity. Our keys-to-notes mapping is an extension of the 
idea called fret­based pitch selection, presented in [Fiebrink et al. 2007]. Simply put, their mapping 
consists in observing the distribution of the chro­matic scale notes in a fretted musical instrument 
tunned in fourths, as shown in Figure 2. In other words, for each octave there is a cor­responding 12-notes 
tile (Figure 3(a)), and the mapping is obtained from the tessellation of the plane using those tiles, 
as depicted in Figure 1(a). The solid lines seen in Figure 1(a) indicate the bound­aries between bands 
of tiles whose notes are taken from the same octave. Dashed lines indicate boundaries between tiles. 
 (a) (b) Figure 2: Chromatic scale as seen in the piano interface (a) and in fretted musical instruments 
tuned in fourths (b). In our previous work [Cicconet et al. 2010], we have extended the idea of tessellating 
the plane with musical-scale tiles to scales other than 12-note chromatic, namely, the Blues Major, Blues 
Minor, general heptatonic (7 notes) and general pentatonic (5 notes) scales. Fortunatelly the disposition 
of keys in the QWERTY keyboard is compatible with the tessellations corresponding to the heptatonic and 
pentatonic scales. Moreover, most of the modern songs (at least regarding Western music) are written 
using scales of 5 or 7 notes [Hewitt 2008]. The tiles for the heptatonic and pentatonic scales are shown 
in Fig­ure 3 (b) and (c), and the mappings obtained from the corresponding tessellations are depicted 
in Figure 1 (b) and (c), respectively. (a) (b) (c) Figure 3: Chromatic (a), heptatonic (b) and pentatonic 
(c) tiles. It is worth mentioning that what is shown in Figure 1 are just three of many realizations 
of mappings between keys and notes which are based on the tessellations just described. In Figure 1 the 
Z key is being used as pivot, but, obviously, this is not mandatory. Any other key of the 4 ×10 grid 
could be used as well. Besides, the note and the octave corresponding to the pivot key are also variable. 
In our implementation of the discussed mappings, some hardware­related limitations were veri.ed. First, 
the used keyboard was un­aware of key-down velocity, a feature that imposes some limits on the performance 
expressiveness. Second, polyphony is not the same over the keyboard, and there are combinations of three 
keys that cannot be played simultaneously. Other performance limitation concerns the absence of a pitch 
wheel, very common on MIDI keyboards. But this issue could be circumvented by using the mouse or the 
trackpad. For a pitch shift of one or two semi-tons, up or down the chromatic scale, modi.er keys (ctrl, 
alt, etc) could be applied. This would be especially use­ful to reach that particular note which is out 
of the chosen scale, but that the composer does not renounce to make use of. References CICCONET, M., 
FRANCO, T., AND CARVALHO, P. C. 2010. Plane tessellation with musical scale tiles and bidimensional automatic 
composition. In International Computer Music Conference. FIEBRINK, R., WANG, G., AND COOK, P. 2007. Don 
t forget your laptop: Using native input capabilities for expressive music control. In New Interfaces 
for Musical Expression. HEWITT, M. 2008. Music Theory for Computer Musicians. Course Technology / Cengage 
Learning. Copyright is held by the author / owner(s). SIGGRAPH 2010, Los Angeles, California, July 25 
 29, 2010. ISBN 978-1-4503-0210-4/10/0007  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1836948</article_id>
		<sort_key>1030</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>96</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>18</seq_no>
		<title><![CDATA[Shadow WIM]]></title>
		<subtitle><![CDATA[a multi-touch, dynamic world-in-miniature interface for exploring biomedical data]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836845.1836948</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1836948</url>
		<abstract>
			<par><![CDATA[<p>Advances in high-performance (supercomputer) simulations are revolutionizing biomedical research. Figure 1 shows a visualiazation of data from a cutting-edge computational fluid dynamics (CFD) simulation of blood flow through a replacement heart valve. Our collaborators in medical device design hope to use these data as part of a new approach to redesigning the valve hinging mechanism, ultimately improving the longevity of these devices. Biomedical engineers face significant challenges in exploring and understanding these data.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>J.3</cat_node>
				<descriptor>Biology and genetics</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>H.5.2</cat_node>
				<descriptor>Haptic I/O</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010444.10010087</concept_id>
				<concept_desc>CCS->Applied computing->Life and medical sciences->Computational biology</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010444.10010095</concept_id>
				<concept_desc>CCS->Applied computing->Life and medical sciences->Systems biology</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10011752</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Haptic devices</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010444.10010935</concept_id>
				<concept_desc>CCS->Applied computing->Life and medical sciences->Genetics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2264508</person_id>
				<author_profile_id><![CDATA[81453654512]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Dane]]></first_name>
				<middle_name><![CDATA[M.]]></middle_name>
				<last_name><![CDATA[Coffey]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Minnesota]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264509</person_id>
				<author_profile_id><![CDATA[81100317632]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Daniel]]></first_name>
				<middle_name><![CDATA[F.]]></middle_name>
				<last_name><![CDATA[Keefe]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Minnesota]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1130524</ref_obj_id>
				<ref_obj_pid>1134820</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Wingrave, C. A., Haciahmetoglu, Y., and Bowman, D. A. 2006. Overcoming world in miniature limitations by a scaled and scrolling wim. <i>Proceedings of the IEEE conference on Virtual Reality</i>, 116.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Shadow WIM: A Multi-Touch, Dynamic World-In-Miniature Interface for Exploring Biomedical Data Figure 
1: Exploring a time-varying visualization of simulated blood .ow through a heart valve model. (a) The 
3D anatomical model. (b) The shadow shows an orientation of the WIM that the user wants to change. (c) 
Using seamless multitouch gestures, the user reorients the WIM, changing the shadow. (d) With the WIM 
set, the user de.nes a .lm plane, overlayed in blue, between the .ngers. 1 Introduction Advances in high-performance 
(supercomputer) simulations are revolutionizing biomedical research. Figure 1 shows a visuali­azation 
of data from a cutting-edge computational .uid dynamics (CFD) simulation of blood .ow through a replacement 
heart valve. Our collaborators in medical device design hope to use these data as part of a new approach 
to redesigning the valve hinging mechanism, ultimately improving the longevity of these devices. Biomedical 
engineers face signi.cant challenges in exploring and understand­ing these data. This work addresses 
the speci.c challenge of navigating through virtual reality visualizations that include complex anatomic 
geome­tries, such as the MRI-reconstructed geometry shown in Figure 1. Although navigation in virtual 
environments has been studied ex­tensively, emerging datasets present new challenges. Our approach builds 
upon well known World In Miniature (WIM) techniques, e.g. [Wingrave et al. 2006]. The contributions of 
our work are ex­tending scalable and scrollable WIMs [Wingrave et al. 2006] to ap­ply to virtual environments 
that do not include a ground plane or a well-established default viewing orientation, developing a multi­touch 
interface to control the Shadow WIM that we introduce, demonstrating this interface in a biomedical engineering 
applica­tion, and reporting on lessons learned. 2 Shadow WIM Our system consists of a dual surface, 
multi-touch, virtual reality environment. The .rst surface is a vertical stereo screen, the second a 
horizontal multi-touch tablee as seen in Figure 1. In contrast to a typical WIM, which is a .at map, 
the Shadow WIM has 2 parts: a shadow projection and the .oating 3D geometry that casts it. The user interacts 
directly with the shadow, while the 3D geometry is useful, and only displayed, during reorienting operations. 
A WIM reorientation mode is used to change the orientation of both parts of the WIM (3D and shadow) simultaneously. 
The view de.nition mode is used de.ne the real world view. Multi-touch gestures are used to seamlessly 
transition between the two modes. WIM Reorientation: To reorient the WIM to create a useful inter­ *{coffey, 
keefe}@cs.umn.edu active shadow, a single point of contact is used for .lmplane trans­lation and two 
for scaling and rotating. Then, in an extension to typical direct manipulation interfaces, the virtual 
environment can be tilted or rolled by moving two points of contact together in the y or x direction 
respectively, effectively changing the up direction of the environment. The reorientation process can 
be seen in Figure 1 (b) and (c). View De.nition: New global 3D views for the vertical display are created 
by de.ning a new .lm plane using the Shadow WIM. Four points of contact (two .ngers from each hand) are 
used to de.ne the width of the .lm plane by touching directly on the shadow. The resulting plane is mapped 
directly to the physical plane of the ver­tical display. The height of the .lm plane is initialized to 
the center of the anatomical model and can be adjusted by moving a thumb of either hand in a vertical 
motion. 3 Design Lessons and Future Work In implementing the Shadow WIM, we faced two interesting issues. 
First, we found that reorienting the WIM relative to a .lm plane .xed on the table surface was too confusing 
for users because both the WIM and the global 3D view changed at the same time. By moving the .lm plane 
along with the WIM when it is reoriented, we avoid this. Second, we found that the 3D part of the WIM 
was distracting and dif.cult to view when the WIM was scaled to large sizes because it was positioned 
very close to the user s eyes. By scaling down and slightly offseting the 3D part of the WIM, creat­ing 
a slight mismatch between the two parts, we allow for meaning­ful reorienting when the WIM is at a very 
large size. In the future, we plan to study the perceptual issues raised by this mismatch, to see if 
it effects the interaction. References WINGRAVE, C. A., HACIAHMETOGLU, Y., AND BOWMAN, D. A. 2006. Overcoming 
world in miniature limitations by a scaled and scrolling wim. Proceedings of the IEEE conference on Virtual 
Reality, 116. Copyright is held by the author / owner(s). SIGGRAPH 2010, Los Angeles, California, July 
25 29, 2010. ISBN 978-1-4503-0210-4/10/0007  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1836949</article_id>
		<sort_key>1040</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>97</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>19</seq_no>
		<title><![CDATA[TapShot]]></title>
		<subtitle><![CDATA[screenshot snippets as GUI shortcuts]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836845.1836949</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1836949</url>
		<abstract>
			<par><![CDATA[<p>During extended sessions with a graphical user interface (GUI), users often apply a small set of commands with high frequency. A majority of direct manipulation tasks on a GUI are carried out using the mouse, particularly when keyboard shortcuts are not provided or the user is not familiar with them. Thus, to invoke a certain command, the user is required to aim the mouse pointer at a given onscreen widget and click with the mouse. If the overall task requires a user to click on the same widget repeatedly as part of a sequence of different interleaved micro-tasks, the overall performance suffers, as each point-and-click action requires a considerable amount of time for correctly aiming at the respective control.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>H.5.2</cat_node>
				<descriptor>Graphical user interfaces (GUI)</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010865</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Graphical user interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2264510</person_id>
				<author_profile_id><![CDATA[81453651981]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Kristian]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gohlke]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Applied Sciences]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264511</person_id>
				<author_profile_id><![CDATA[81416608787]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Michael]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hlatky]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Applied Sciences]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264512</person_id>
				<author_profile_id><![CDATA[81408599742]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[J&#246;rn]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Loviscach]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Applied Sciences]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1570465</ref_obj_id>
				<ref_obj_pid>1570433</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Besacier, G., and Vernier, F. 2009. Toward user interface virtualization: legacy applications and innovative interaction systems. In <i>Proc. of EICS '09</i>, 157--166.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>741332</ref_obj_id>
				<ref_obj_pid>647987</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Myers, B. A., Peck, C. H., Nichols, J., Kong, D., and Miller, R. 2001. Interacting at a distance using semantic snarfing. In <i>Proc. of UbiComp '01</i>, 305--314.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1622213</ref_obj_id>
				<ref_obj_pid>1622176</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Yeh, T., Chang, T.-H., and Miller, R. C. 2009. Sikuli: using GUI screenshots for search and automation. In <i>Proc. of UIST '09</i>, 183--192.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 TapShot: Screenshot Snippets as GUI Shortcuts Kristian Gohlke* , Michael Hlatky Hochschule Bremen (University 
of Applied Sciences) 1 Introduction During extended sessions with a graphical user interface (GUI), 
users often apply a small set of commands with high frequency. A majority of direct manipulation tasks 
on a GUI are carried out using the mouse, particularly when keyboard shortcuts are not provided or the 
user is not familiar with them. Thus, to invoke a certain com­mand, the user is required to aim the mouse 
pointer at a given on­screen widget and click with the mouse. If the overall task requires a user to 
click on the same widget repeatedly as part of a sequence of different interleaved micro-tasks, the overall 
performance suf­fers, as each point-and-click action requires a considerable amount of time for correctly 
aiming at the respective control. This work demonstrates a system that enables quick and transpar­ent 
access of frequently used functionality in virtually any GUI­enabled legacy application. For this purpose, 
we employ the capabilities of readily available touch-sensitive devices such as the iPhone or similar 
multitouch controllers to replace repetitive point-and-click mouse interactions. The use of mouse interaction 
still prevails for precise direct manipulation and for triggering more rarely used commands on a GUI. 
Carrying over the idea of us­ing GUI screenshots for scripting automated actions [Yeh et al. 2009], we 
use small screenshots as pushbuttons on the touch de­vice. Controlling software via dynamically labelled 
keys is also in the spirit of the steeply priced Optimus Maximum computer key­board (http://www.artlebedev.com/everything/optimus/). 
 2 System Design The TapShot system allows quickly assigning functionality from a GUI to a touch screen 
device through custom visual shortcuts. Each shortcut is represented by a small icon on the touch screen. 
Initially, each icon s place is blank. When the user interacts with any applica­tion through the mouse 
on the main computer display, shortcuts can be created. This is achieved by pressing any of the icons 
while si­multaneously holding down an additional snip button on the touch display. This invokes a learn 
mode, which is further indicated by an icon-sized translucent square underneath the mouse pointer on 
the main screen. The size of the square can be adjusted by spinning the scroll wheel. Once the user clicks 
a mouse button, the system grabs a screen­shot of the square area and presents the captured snippet on 
the touch display, scaled to .t into the grid of icons, see Figure 1. Now, touching the icon initiates 
a mouse click of the previously entered type (left/middle/right, double) at the original screen location, 
thus providing a shortcut to the underlying command. This approach does not require any additional information 
from GUI widgets or the operating system. Thus, it works with any appli­cation that reacts to mouse input. 
As a fast and transparent way of mapping shortcuts to an input device placed conveniently at the .n­gertips 
of the user, the system can reduce strain and supports a more ef.cient interaction when carrying out 
tasks that require repetitive clicking on the same UI widget. The current version of TapShot employs 
a client-server architecture that enables its use on virtually any networked touch-screen device. *e-mail: 
kgohlke@acm.org e-mail: joern.loviscach@fh-bielefeld.de J¨orn Loviscach Fachhochschule Bielefeld (University 
of Applied Sciences)  Figure 1: The TapShot icons collected by the user (left) can be used to invoke 
commands on the main screen (right).  3 Outlook Ongoing work aims at providing a more robust mapping 
between the selected locations of action and the current state of the GUI of an unmodi.ed legacy application, 
similar to Besacier et al. [2009] or Myers et al. [2001]. An initial step will be to store location data 
relative to the coordinates of the window that was active at the time of grabbing screenshots and reposition 
the mouse click event from the touch screen in case the underlying window s position has changed since 
the assignment. As an alternative, image process­ing could be applied to track UI widgets on a completely 
graphical basis, following the approach of Yeh at al. [2009]. The TapShot sys­tem could provide a way 
to quickly record and trigger sequences of click events or keystrokes on the .y, similar to the macro 
function­ality already present in many applications with the added bene.t of a central and more transparent 
interface. To control actions that are not directly accessible from a GUI, the TapShot system could further 
be used to .re MIDI, OSC or TUIO data events. References BESACIER, G., AND VERNIER, F. 2009. Toward 
user interface vir­tualization: legacy applications and innovative interaction sys­tems. In Proc. of 
EICS 09, 157 166. MYERS, B. A., PECK, C. H., NICHOLS, J., KONG, D., AND MILLER, R. 2001. Interacting 
at a distance using semantic snar.ng. In Proc. of UbiComp 01, 305 314. YEH, T., CHANG, T.-H., AND MILLER, 
R. C. 2009. Sikuli: using GUI screenshots for search and automation. In Proc. of UIST 09, 183 192. Copyright 
is held by the author / owner(s). SIGGRAPH 2010, Los Angeles, California, July 25 29, 2010. ISBN 978-1-4503-0210-4/10/0007 
  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1836950</article_id>
		<sort_key>1050</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>98</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>20</seq_no>
		<title><![CDATA[The visible electricity device]]></title>
		<subtitle><![CDATA[visible breadboard]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836845.1836950</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1836950</url>
		<abstract>
			<par><![CDATA[<p>Visible Breadboard is the Breadboard like interface which shows voltages of each and every hole by full color LED and enable us to make wiring by tracing with finger tips. Users could insert electrical material into the holes and make a circuit on this device. Users could understand what is happening in the circuit and correct the connections with finger tracing.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>H.5.2</cat_node>
				<descriptor>Graphical user interfaces (GUI)</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010865</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Graphical user interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2264513</person_id>
				<author_profile_id><![CDATA[81547612056]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Yoichi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ochiai]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Tsukuba]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Suzuki, H., and Kato, H. 1993 "AlgoBlock: a Tangible Programming Language, a Tool for Collaborative Learning." <i>In Proceedings of 4th European Logo Conference, Aug. 1993, Athens Greece</i>, pp. 297--303.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>258715</ref_obj_id>
				<ref_obj_pid>258549</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Ishii, H. and Ullmer, B. 1997 Tangible bits: towards seamless interfaces between people, bits and atoms. In <i>Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (Atlanta, Georgia, United States, March 22--27, 1997). S. Pemberton, Ed. CHI '97. ACM, New York, NY</i>, pp. 234--241.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 The Visible Electricity Device: Visible Breadboard - touchable, visible, easy controllable; the making 
of an entirely new kind and type of breadboard - Yoichi Ochiai* University of Tsukuba, College of Information 
Science, Department of Media Arts,Science and Technology   (a) Figure 1:(a) Overview of the Device, 
(b) Top Board. Abstract Visible Breadboard is the Breadboard like interface which shows voltages of 
each and every hole by full color LED and enable us to make wiring by tracing with .nger tips. Users 
could insert electrical material into the holes and make a circuit on this device. Users could understand 
what is happening in the circuit and correct the connections with .nger tracing. 1 Introduction Many 
devices for prototyping has been developed such as the "Algoblock"(work of Suzuki and Kato[1]). In prototyping 
the electrical circuits, people use a tool called breadboard. It has many holes and user can make electrical 
circuit on it. In this paper, I would like to introduce the breadboard like interface for making the 
electrical circuit.Today the electrical circuits are generally made by various tool kits with the help 
of computing. These tool kits such as the Arduino can easily be connected to sensors or actuators with 
simple coding. This is all good and handy. However, there is a problem with this method. That is we cannot 
see what is happening inside the circuits.In order to solve this problem I have worked on a new device 
 (a) (b) (c) (d)   Figure 2: Four Basic Functions: (a)Finger Detecting Function, (b)Wire Connecting 
Function, (c)Voltage Mesuring Function, (d) LED Displaying Function. *e-mail: yoichi.ochiai@me.com (b) 
 which I call the "Visible Breadboard". I would like to explain some of the salient points of this new 
device. The Visible Breadboard shows voltages of each and every hole by LED, thus enabling us to see 
speci.cally any mis-wiring or malfunction in the circuits. Another convenient aspect of this device is 
".nger tip wiring". If you want to make a certain wiring, all you have to do is to trace it on the board 
with your .nger tips. This way makes it easier for you to change the circuit if necessary at an instance. 
Additionally it gives a good educational lesson as to how the electrical circuits work. 2 Device's Functions 
The Visible Breadboard offers four basic functions. Firstly, .nger detecting function(Figure2(a)). This 
can be done by touching the top part of the board to make the change of capacitance possible to detect 
where the .nger touches. Secondly, the wire connecting function(Figure2(b)). There are dozens of SSRs(Solid 
State Relays) embedded at the bottom of the board. The system selects a proper SSRs and open its way 
for the wire connection thus making a circuit.Thirdly, Voltage Mesuring function(Figure2(c)). It mesures 
the voltage of each and every hole of Top board. The system uses SSR to change the connection of the 
holes to AD converter. Fourthly, LED Display Function(Figure2(d)). It shows the voltage of LED colors 
and brightness. It is like a thermometer, thereby you can see which hole's voltage is high or low.The 
Visible Breadboard is user friendly interface for making circuit and understanding basics of electronics. 
It is simple, touchable, visible to the details and easy to handle. I believe that this device can contribute 
and bring in newperspective to the .elds of HCI and engineering of programability for hardwares. References 
[1] Suzuki, H., and Kato, H. 1993 AlgoBlock: a Tangible Programming Language, a Tool for Collaborative 
Learning. In Proceedings of 4th European Logo Conference, Aug. 1993, Athens Greece, pp. 297-303. [2] 
Ishii, H. and Ullmer, B. 1997 Tangible bits: towards seamless interfaces between people, bits and atoms. 
In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (Atlanta, Georgia, United 
States, March 22 -27, 1997). S. Pemberton, Ed. CHI '97. ACM, New York, NY, pp. 234-241. Copyright is 
held by the author / owner(s). SIGGRAPH 2010, Los Angeles, California, July 25 29, 2010. ISBN 978-1-4503-0210-4/10/0007 
  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1836951</article_id>
		<sort_key>1060</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>99</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>21</seq_no>
		<title><![CDATA[Thermal design display device to use the thermal tactile illusions]]></title>
		<subtitle><![CDATA["Thermo-Paradox"]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836845.1836951</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1836951</url>
		<abstract>
			<par><![CDATA[<p>"Thermo-Paradox" is a thermal design display device to use the thermal tactile Physiological illusions that can interactively present patterns of warm and cool temperatures. The technological success of a compact 80-pixel, 9-inch thermal display allows text information to be conveyed by temperature, which has never before been achieved, and the device compactness increases the degree of freedom in presentation methods. We propose this unprecedented tactile expression as a device that can display thermal images that interactively match a visual image, using the tactile Paradoxical sensation produced by the ability to control the temperature of each pixel. (Fig. 1)</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.4.0</cat_node>
				<descriptor>Image displays</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2264514</person_id>
				<author_profile_id><![CDATA[81100274861]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Kumiko]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kushiyama]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tokyo Metropolitan University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264515</person_id>
				<author_profile_id><![CDATA[81319488008]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Tetsuaki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Baba]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tokyo Metropolitan University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264516</person_id>
				<author_profile_id><![CDATA[81472650104]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Kouki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Doi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[National Institute of Special Needs Education]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264517</person_id>
				<author_profile_id><![CDATA[81319500609]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Shinji]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sasada]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Japan Electronics Collage]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1179039</ref_obj_id>
				<ref_obj_pid>1178977</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[K. Kushiyama, S. Sasada, Thermoesthesia ACM SIGGRAPH 2006 Art Gallery, Sketches]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1599328</ref_obj_id>
				<ref_obj_pid>1599301</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Takada, Higurashi, Suzuki, Ohta, Baba, Kushiyama Thermo-Pict SIGGRAPH 2009 Poster]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Zotterman (1953) <i>Ann. Rev. Physiol</i>, 15 357--372]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Thermal Design Display device to use the thermal tactile illusions Thermo-Paradox Kumiko KUSHIYAMA(1) 
, Tetsuaki Baba(1) Kouki Doi (2) Shinji SASADA(3) 1)Tokyo Metropolitan University 2)National Institute 
of Special Needs Education 3)Japan Electronics Collage 1. Introduction Thermo-Paradox is a thermal design 
display device to use the thermal tactile Physiological illusions that can interactively present patterns 
of warm and cool temperatures. The technological success of a compact 80-pixel, 9-inch thermal display 
allows text information to be conveyed by temperature, which has never before been achieved, and the 
device compactness increases the degree of freedom in presentation methods. We propose this unprecedented 
tactile expression as a device that can display thermal images that interactively match a visual image, 
using the tactile Paradoxical sensation produced by the ability to control the temperature of each pixel. 
(Fig. 1) Fig 1. Thermo-Paradoxical sensation  2. System configuration 2.1 Outline A 130mm x 170mm display 
that comprises a total of 80 Peltier elements in a 8 x 10 configuration occupying a 15 mm x15 mm mounted 
on a touch panel detects the position and time of touches on the screen, and the image and temperature 
are controlled simultaneously in accordance with the touches. The Peltier elements allows switching between 
heating and cooling of the surface. The devices are programmed so that they can be controlled individually 
for heating and cooling. (Fig. 2) By placing a projector on the upper part, an image can be projected 
onto the thermal display. The event module sends position data from the touch-screen to the image generator 
in real time. Fig.2 System outline  Copyright is held by the author / owner(s). SIGGRAPH 2010, Los 
Angeles, California, July 25 29, 2010. ISBN 978-1-4503-0210-4/10/0007  2.2 Image generation The image 
generator uses the data sent from the event module to generate a 3-D image in real time with OpenGL. 
What kind of sensation results when the middle finger senses cold and the index finger senses warmth? 
The image and thermal display are controlled simultaneously to create sensations of moist and dry, bumps 
and dents on the surface and other such tactile effects in high-speed real time using vision and the 
thermo-tactile illusions produced by touching the warm and cool stripes. (Fig. 3) Fig.3 Image generation 
, Thermo Camera Image The sense of cold is caused by the high temperature stimulation of 45_ C or more. 
This is called paradoxical sensation. (Fig.4) Fig.4 Specialized Thermoreceptor [3]  3. Conclusion 
This work has yielded a compact thermal sensation display technology that can used to create tactile 
sensations for a new kind of tactile communication that can be used in daily life. This product is expected 
to have a stimulating effect in a variety of fields, and present new creative opportunities. References 
 [1]K.Kushiyama,S.SASADA ,.Thermoesthesia.ACM SIGGRAPH 2006 Art Gallery , Sketches [2]Takada,Higurashi,Suzuki,Ohta,Baba,Kushiyama 
Thermo-Pict.SIGGRAPH 2009 Poster [3]Zotterman (1953) Ann.Rev.Physiol,15 357-372  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1836952</article_id>
		<sort_key>1070</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>100</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>22</seq_no>
		<title><![CDATA[Using mediator objects to easily and robustly teach visual objects to a robot]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836845.1836952</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1836952</url>
		<abstract>
			<par><![CDATA[<p>Social robots are drawing an increasing interest both in scientific and economic communities and one of the main issues is the need to provide these robots with the ability to interact easily and naturally with humans. We believe that the interaction issues may have a very strong impact on the whole system and should be given more attention. Current research however focus mainly on the the visual perception and/or machine learning issues (see for example Steels and Kaplan [1]). We think that by focusing on the users and on the interface we can help them provide the learning system with very high quality learning examples.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.2.9</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010520.10010553.10010554</concept_id>
				<concept_desc>CCS->Computer systems organization->Embedded and cyber-physical systems->Robotics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010213.10010204</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Control methods->Robotic planning</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010199.10010204</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Planning and scheduling->Robotic planning</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2264518</person_id>
				<author_profile_id><![CDATA[81456637844]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Pierre]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Rouanet]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264519</person_id>
				<author_profile_id><![CDATA[81100662126]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Pierre-Yves]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Oudeyer]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264520</person_id>
				<author_profile_id><![CDATA[81100465124]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[David]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Filliat]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[L. Steels and F. Kaplan, "Aibo's first words: The social learning of language and meaning," <i>Evolution of Communication</i>, vol. 4, no. 1, pp. 3--32, 2000. {Online}. Available: http://www3.isrl.uiuc.edu/junwang4/langev/localcopy/pdf/steels02aiboFirst.pdf]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>754103</ref_obj_id>
				<ref_obj_pid>648287</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[F. L&#246;mker and G. Sagerer, "A multimodal system for object learning," in <i>Proceedings of the 24th DAGM Symposium on Pattern Recognition.</i> London, UK: Springer-Verlag, 2002, pp. 490--497.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[P. Rouanet, P.-Y. Oudeyer, and D. Filliat, "An integrated system for teaching new visually grounded words to a robot for non-expert users using a mobile device," in <i>Proceedings of the Humanoids 2009 Conference</i>, 2009.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Using mediator objects to easily and robustly teach visual objects to a robot. Pierre Rouanet, Pierre-Yves 
Oudeyer and David Filliat Social robots are drawing an increasing interest both in scienti.c and economic 
communities and one of the main issues is the need to provide these robots with the ability to interact 
easily and naturally with humans. We believe that the interaction issues may have a very strong impact 
on the whole system and should be given more attention. Current research however focus mainly on the 
the visual perception and/or machine learning issues (see for example Steels and Kaplan [1]). We think 
that by focusing on the users and on the interface we can help them provide the learning system with 
very high quality learning examples. In particular, we think that we should focus on the following questions: 
Yet, we think that by focusing on the users and on the interface we can help them to provide the learning 
system with really good quality learning examples. In particular, we think that we should focus on the 
following questions: Attention drawing: How can a human smoothly and intuitively draw the robot s attention 
toward the interaction? Pointing and Joint attention: How can a human robustly designate an object to 
the robot? How can a human under­stand what the robot is paying attention to? One can try to address 
these challenges by transposing the human-like interactions, such as gaze tracking or pointing gestures. 
However, most social robots have a visual apparatus and in particular a small .eld of view which makes 
this kind of interaction non-robust and very restrictive in real environments. Other researchers directly 
wave objects in front of the camera of the robot and so can achieve a motion-based joint attention [2]. 
Although this approach is interesting it can only work with light and movable objects and therefore could 
be tiring or even impossible for the elderly or the disabled. We are here proposing to use small devices 
as mediator objects between the human and the robot. We already presented an iPhone based interface [3] 
and we are here presenting a Wiimote and laser pointer based interface. This interface allows users to 
drive the robot and to draw its attention toward a speci.c object in order to name it. The laser spot 
is automatically tracked by the robot and a laser sound is played as a visual feedback allowing users 
to know whereas the laser spot and so the designated object was inside the robot s .eld of view. It is 
a crucial help as non-expert humans have dif.culties to correctly estimate the robot s capacities. To 
name an object, users have to .rst encircle it. On top of being a simple gesture to select an object, 
it is also providing a rough visual segmentation which is otherwise still an ill-de.ned problem in an 
unconstrained environment. Copyright is held by the author / owner(s). SIGGRAPH 2010, Los Angeles, California, 
July 25 29, 2010. ISBN 978-1-4503-0210-4/10/0007 We are also presenting an evaluation of the laser 
interface and its comparison with other interfaces and especially the iPhone interface mentioned above. 
We design an experiment where we ask participants to teach the robot names for .ve different objects. 
Participants used three different interfaces: the .rst one was very simple and did not provide any feedback 
of what the robot sees, the second was the laser interface and the third was the iPhone interface. We 
.rst study the overall quality of the learning examples among these different interfaces. We noticed 
that the object was en­tirely visible on only 25% of the learning examples gathered without any feedback 
and even entirely absent in 37% of these examples. While the object was entirely visible in more than 
90% of the learning examples collected with an interface providing a feedback (laser or iPhone). We also 
used the gathered learning examples to train a learning system based on the bag of visual words and evaluate 
its performance in generalization on an of.ine database. We showed that, while the laser interface allows 
the user to provide high quality examples, encircling with the laser is not as effective as encircling 
on the screen of the iPhone. Indeed, the projection of the laser spot on the camera plane often results 
in cutting the encircled object as shown in the pictures above.  REFERENCES [1] L. Steels and F. Kaplan, 
Aibo s .rst words: The social learning of language and meaning, Evolution of Communication, vol. 4, no. 
1, pp. 3 32, 2000. [Online]. Available: http://www3.isrl.uiuc.edu/ jun­wang4/langev/localcopy/pdf/steels02aiboFirst.pdf 
[2] F. L¨omker and G. Sagerer, A multimodal system for object learning, in Proceedings of the 24th DAGM 
Symposium on Pattern Recognition. London, UK: Springer-Verlag, 2002, pp. 490 497. [3] P. Rouanet, P.-Y. 
Oudeyer, and D. Filliat, An integrated system for teaching new visually grounded words to a robot for 
non-expert users using a mobile device, in Proceedings of the Humanoids 2009 Conference, 2009. 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1836953</article_id>
		<sort_key>1080</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>101</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>23</seq_no>
		<title><![CDATA[Using innovative ehealth interventions in a local health care context]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836845.1836953</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1836953</url>
		<abstract>
			<par><![CDATA[<p>Information and communication technologies (ITC) offer innovative ways to improve health services and systems. Integration of eHealth into the daily life of rural health-care workers is fast becoming a reality in developing countries. Computermediated communication systems can be used to bridge the gap between doctors in underserved regions with local shortages of medical expertise and medical specialists. eHealth for primary health-care includes applications that directly support disease prevention, patient diagnosis and patient management and care.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>J.3</cat_node>
				<descriptor>Medical information systems</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>H.3.4</cat_node>
				<descriptor>World Wide Web (WWW)</descriptor>
				<type>P</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010444.10010447</concept_id>
				<concept_desc>CCS->Applied computing->Life and medical sciences->Health care information systems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002951.10003317</concept_id>
				<concept_desc>CCS->Information systems->Information retrieval</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002951.10003152</concept_id>
				<concept_desc>CCS->Information systems->Information storage systems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2264521</person_id>
				<author_profile_id><![CDATA[81466647357]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Patricia]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Codyre]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Universit&#228;tsKlinikum Heidelberg, Heidelberg, Germany]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Nosek, B. A., Banaji, M., and Greenwald, A. G. (2002). Harvesting implicit group attitudes and beliefs from a demonstration web site. Group dynamics, 6(1):101--115.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>768937</ref_obj_id>
				<ref_obj_pid>767766</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Orlikowski, W. J. (2000). Using technology and constituting structures: a practice lens for studying technology of organizations. Organizational science, 11(4):404--428.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Popper, K. (1972). Objective Knowledge: An Evolution Approach. Oxford University Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Using innovative ehealth interventions in a local health care context. Patricia Codyre UniversitätsKlinikum 
Heidelberg: Institut für Public Health, Heidelberg, Germany  Village clusters in CRSN, Nouna, Burkina 
Faso. The four clusters are; Nouna town, north-eastern, south-eastern, and south-western village clusters 
are marked using circular boundaries. 1 Introduction Information and communication technologies (ITC) 
offer innovative ways to improve health services and systems. Integration of eHealth into the daily life 
of rural health-care workers is fast becoming a reality in developing countries. Computer­mediated communication 
systems can be used to bridge the gap between doctors in underserved regions with local shortages of 
medical expertise and medical specialists. eHealth for primary health-care includes applications that 
directly support disease prevention, patient diagnosis and patient management and care. This study explores 
the use of innovative IT solutions to provide the social, institutional and infrastructural context for 
sustained, self­organizing growth of a distributed medical community in Nouna, Burkina Faso. Africa. 
The area covers 1.775 km2 with 76 000 inhabitants in a rural and semi-urban environment. 2. GOAL The 
goal of this research is: To study human-computer interaction (HCI) within this rural medical community 
and examine a solution to ensure the establishment of a viable infrastructure in the community.  To 
initiate a comparative usability evaluation to determine which dimensions and questionnaire items are 
most critical.  To produce a multilevel analysis of sociability, usability, and health community dynamics 
in the community setting.  3. Conclusion and future work The primary emphasis is on results of broad 
application in a local context. As primary health care is the main target of this study, the success 
of this intervention is predicated on a high level of community involvement to solve key challenges through: 
 supporting policy and practice in health systems and technical programmes.  providing technical assistance 
for governance, monitoring and improvement of e-Health services within the community.  providing theoretical 
context for studies addressing the role of technological design and communication culture.  providing 
theoretical context for studies, dealing with large data sets within rural districts and charting the 
use of IT infrastructure in an African village.   References NOSEK, B. A., BANAJI, M., AND GREENWALD, 
A. G. (2002).Harvesting implicit group attitudes and beliefs from a demonstration web site. Group dynamics,6(1):101 
115. ORLIKOWSKI, W. J. (2000). Using technology and constituting structures: a practice lens for studying 
technology of organizations . Organizational science, 11(4):404 428. POPPER, K. (1972). Objective Knowledge: 
An Evolution Approach. Oxford University Press. Copyright is held by the author / owner(s). SIGGRAPH 
2010, Los Angeles, California, July 25 29, 2010. ISBN 978-1-4503-0210-4/10/0007  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	</section>
	<section>
		<section_id>1836954</section_id>
		<sort_key>1090</sort_key>
		<section_seq_no>8</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Modeling]]></section_title>
		<section_page_from>102</section_page_from>
	<article_rec>
		<article_id>1836955</article_id>
		<sort_key>1100</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>102</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[3D human face identity transfer using deformation gradient]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836845.1836955</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1836955</url>
		<abstract>
			<par><![CDATA[<p>Modeling a 3D face toward a specific person is a tedious and painstaking task even for skilled artists. Crafting a 3D cartoon-style or a 3D fiction-creature face to reflect a specific person likeness is even more challenging. For example, creating an ogre that keeps the actor/actress likeness or constructing a 3D avatar that reflects the person identity is an intensive process that involves high artistic skills to convey the human identity on the monster geometries. This work presents an automatic 3D face modeling system that transfers a target 3D human face identity (likeness) to any 3D character faces. This system can be used for a broad variety of a 3D face modeling such as an early stage 3D character face design or a individualized 3D avatars creation.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.4.8</cat_node>
				<descriptor>Object recognition</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245.10010251</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems->Object recognition</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2264522</person_id>
				<author_profile_id><![CDATA[81351609318]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Tanasai]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sucontphunt]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Southern California]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264523</person_id>
				<author_profile_id><![CDATA[81100180219]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Zhigang]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Deng]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Houston]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264524</person_id>
				<author_profile_id><![CDATA[81100662479]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Ulrich]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Neumann]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Southern California]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>311556</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[V. Blanz and T. Vetter. A morphable model for the synthesis of 3D faces. In <i>Proc. of ACM SIGGRAPH '99</i>, pages 187--194, 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1015736</ref_obj_id>
				<ref_obj_pid>1015706</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[R. W. Sumner and J. Popovi&#263;. Deformation transfer for triangle meshes. <i>ACM Trans. Graph.</i>, 23(3):399--405, 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 3D Human Face Identity Transfer Using Deformation Gradient TanasaiSucontphunt* ZhigangDeng UlrichNeumann 
University ofSouthernCalifornia University ofHouston University ofSouthernCalifornia 1 INTRODUCTION 
Modeling a 3D face toward a speci.c person is a tedious and painstakingtask evenforskilled artists.Crafting 
a3D cartoon-style or a 3D .ction-creature face to re.ect a speci.c person likeness is even more challenging. 
For example, creating an ogre that keeps the actor/actress likeness or constructing a 3D avatar that 
re.ects theperson identity isan intensiveprocess that involveshigh artistic skills toconveythehuman identityon 
themonstergeometries.This workpresentsanautomatic3Dfacemodeling systemthat transfers a target3Dhumanfaceidentity(likeness)toany3D 
characterfaces. This system can be used for a broad variety of a 3D face modeling such as an early stage 
3D character face design or a individualized 3D avatars creation. Inthispaper,forsimplicity,wedemonstrateour 
taskby crafting a 3D monster face toward a speci.c human face likeness. Here, givena3D monstermodel tobe 
transformed and a3Dhumanface model to be transferred, our system automatically transfers the hu­man likenesstothemonsterfaceusing 
adeformationgradientbased transferring technique. Figure 1: 3D humanface identity is transferred to 
a 3D monster face automatically by three main steps shown as the arrows. Step-1, the input3Dmonsterfaceis 
morphedto a3Dhumanform ofthe monster (human-monster)bythe3D morphable model approach. Step-2, the monster-average 
face is constructed by the deformation gradient of thehuman-monster andthehuman-averageface. Thisprocesshelps 
remove the original monster identity. Step-3, the .nal result of the 3D monsterfaceisgeneratedfromthe 
monster-averagefaceby the deformationgradientbetween thehuman-averageface and theinput 3Dhuman face. 
The faces in the boxes are required inputs. * e-mail: sucontph@usc.edu e-mail: zdeng@cs.uh.edu e-mail: 
uneumann@graphics.usc.edu 2 METHOD The3Didentity transferframeworkismainlycomposedoftwopro­cedures: an 
identity capturing and an identity transferring. Identity capturing: in order to capture a target human 
identity, we follow a caricature modeling practice. In the caricature mod­eling, the caricature is crafted 
mainly by exaggerating the facial geometries that apparently differ from the average facial geome­tries. 
The more geometric difference from the average face, the more identity theperson isrevealed in thecaricature.Accordingly, 
the difference from the human-average face can represent the hu­man identity. This difference is the 
main context to transfer from the targethumanface tothe target monsterface. Identity transferring: tocorrectlytransfertheminutedetailsof 
thegeometricdifference tothe3D monstermodel,weemploy ade­formation transferframework[2]. In this technique, 
thegeometric difference is represented by a deformation gradient. The deforma­tion gradient is a non-translation 
af.ne transformation matrix that transforms the edges and normal vector of each triangle between two 
poses. Eq. 1 shows the general equation of the deformation transfer. argmin Tx -S (1) x where x is the 
vector containingthe resultant target vertices, T is a linearoperatorformingdeformationgradientforthetarget 
model, and S is the deformation gradient between two poses of the source model. In our approach, the 
target model is the 3D monster face and thesourcemodel is the3Dhumanface.Inshort, thedeforma­tiongradientbetweenthe 
target3Dhumanfaceandthe3Dhuman­averagefaceisthehumanidentity tobe transferred tothe3D mon­ster face. 
To analytically transfer the geometries between human and monster, the human subspace and the monster 
subspace are constructed to arrange the models rationally. Essentially, the hu­man with the similar identity 
as the monster are mapped together as a one-to-one mapping function between subspaces as shown in Fig. 
1. However, if we transfer the human identity to the monster face directly, the resultant monster face 
will be mixed with both the transferredidentityandtheoriginal monsterfaceidentity.Con­sequently, to neutralize 
the monster identity, the monster-average face isnecessarilyconstructed.rst.Since themonster-averageface 
does not exist in our dataset, the monster-average face requires to be synthesized from the available 
monster face. To do so, we .rst map the input monster face in the monster subspace to the human face 
in thehumansubspacebya3D morphablemodeltechnique[1] forming ahuman-monsterface.In thisprocess, the3D 
monsterge­ometry is iterativelyprojected to thespacespannedbyhumanface eigen-vectors to determine the 
closest coef.cients to re-construct thehuman-monsterface.After that, thedeformation transfer isused to 
transfer the difference between the human-monster face and the human-average face to the input 3D monster 
face to synthesize the monster-average face. Finally, with the monster-average face, the human identity 
is transferred to themonster-averageface togener­atethe.nal target monsterface. REFERENCES [1] V. Blanz 
and T. Vetter. A morphable model for the synthesis of 3D faces. In Proc. of ACM SIGGRAPH 99,pages187 
194,1999. [2] R.W.SumnerandJ.Popovi´c.Deformationtransferfortriangle meshes. ACM Trans. Graph.,23(3):399 
405,2004. Copyright is held by the author / owner(s). SIGGRAPH 2010, Los Angeles, California, July 25 
 29, 2010. ISBN 978-1-4503-0210-4/10/0007  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1836956</article_id>
		<sort_key>1110</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>103</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[A scripting language for Digital Content Creation applications]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836845.1836956</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1836956</url>
		<abstract>
			<par><![CDATA[<p>Digital Content Creation (DCC) Applications (e.g. Blender, Autodesk 3ds Max) have long been used for the creation and editing of digital content. Due to current advancement in the field, the need for controlled automated work forced these applications to add support for small programming languages that gave power to artists without diving into many details. With time these languages developed into more mature languages and were used for more complex tasks (driving physics simulations, controlling particle systems, or even game engines). For long, these languages have been interpreted, embedded within the applications, lagging the UIs or incomparable with real programming languages (regarding Completeness, Expressiveness, Extensibility and Abstractions). Two approaches were used to implement those languages. Either build them from scratch (like MaxScript), or use an existing popular language and write a set of extensions to it and embed it (like Blender and Python). In practice, both those solutions suffer, the first method produces languages lacking being real, competitive languages and generally very inefficient, the second method has problems arising from not being dedicated in first place for that kind of applications so, they lack expressiveness facilities (like dedicated constructs) that support that particular domain, also it's very hard to optimize these languages for specific DCC situations.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.6</cat_node>
				<descriptor>Languages</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003120.10003121.10003128</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction techniques</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Languages</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2264525</person_id>
				<author_profile_id><![CDATA[81466643823]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Mohammed]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yousef]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Assiut University, Assiut, Egypt]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264526</person_id>
				<author_profile_id><![CDATA[81537602656]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Ahmed]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hashem]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Assiut University, Assiut, Egypt]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264527</person_id>
				<author_profile_id><![CDATA[81466646091]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Hassan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Saad]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Assiut University, Assiut, Egypt]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264528</person_id>
				<author_profile_id><![CDATA[81466644774]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Amr]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gamal]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Assiut University, Assiut, Egypt]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264529</person_id>
				<author_profile_id><![CDATA[81466647839]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Osama]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Galal]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Assiut University, Assiut, Egypt]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264530</person_id>
				<author_profile_id><![CDATA[81543874456]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Khaled]]></first_name>
				<middle_name><![CDATA[F.]]></middle_name>
				<last_name><![CDATA[Hussain]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Assiut University, Assiut, Egypt]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[
Kettner, L., 2010. 3d polyhedral surfaces. CGAL Editorial Board - 3.6 editions.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1236265</ref_obj_id>
				<ref_obj_pid>1236246</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[
Ming Lien, J., and Amato, N. M. 2007. Approximate convex decomposition of polyhedra. In <i>Proceedings of the ACM Symposium on Solid and Physical Modeling</i>, ACM, 121--131.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>213603</ref_obj_id>
				<ref_obj_pid>213593</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[
Parr, T. J., and Quong, R. W. 1994. Antlr: A predicated-ll(k) parser generator. <i>Software Practice and Experience 25</i>, 789--810.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 A Scripting language for Digital Content Creation Applications MohammedYousef,AhmedHashem,HassanSaad,AmrGamal,OsamaGalal 
* , andKhaledF.Hussain FacultyofComputers andInformation,AssiutUniversity,Assiut,Egypt Figure 1: from 
left to right, (a) a sphere with bevel on faces of 5 sides then Catmull-Clark subdivision (b) a spring 
with bend on Y axis (c) a torus with taper on Y axis (d) Cloth(red) covering a torus(green) after tapering 
it during a physics simulation. 1 Introduction Digital Content Creation (DCC) Applications (e.g. Blender, 
Au­todesk 3ds Max) have long been used for the creation and edit­ing of digital content. Due to current 
advancement in the .eld, the need for controlled automated work forced these applications to add support 
for small programming languages that gave power to artists without diving into many details. With time 
these lan­guages developed into more mature languages and were used for more complex tasks(drivingphysics 
simulations, controllingpar­ticle systems, or even game engines). For long, these languages have been 
interpreted, embedded within the applications, lagging theUIs orincomparable with realprogramming languages(regard­ingCompleteness,Expressiveness,ExtensibilityandAbstractions). 
Two approaches were used to implement those languages. Either build themfrom scratch(likeMaxScript), 
oruse an existingpopu­larlanguage and write a set of extensions toit and embedit(like Blender and Python). 
In practice, both those solutions suffer, the .rst method produces languages lacking being real, competitive 
languages and generally very inef.cient, the second method has problemsarisingfromnotbeingdedicatedin 
.rstplaceforthatkind of applications so,theylack expressivenessfacilities(likededicated constructs) that 
support that particular domain, also it s very hard to optimize theselanguages for speci.cDCC situations. 
In this paper, we present a system that addresses those problems. A high level scripting language (Zlang) 
and a DCC Engine, the language can be interpreted, compiled, extended in C/C++ and has a number of constructs 
and optimizations dedicated to DCC do­main. The engine provides geometric primitives, mesh modi.ers, 
key-framed animation andPhysicsSimulations(Rigid/SoftBody, Cloth and Fluid Simulations), the engine also 
is designed and im­plemented as alibrary soit canbe used alone and embedded. 2 TECHNICAL APPROACH In 
the DCC engine, we store meshes in a half-edge data structure. Two classes of algorithms for constructing 
primitives are used, ei­ther sculpturing (using Euler operations) or connecting generated point sets 
into faces. Mesh modi.ers operate on half-edges to achieve required effect; they either operateperface(likeExtrude 
andOutline) oronthewholemesh(likeTwistandBend). Every object has a stack of modi.ers that are applied 
to it in order. To produce animation,key-frame values aregiventomodi.erproper­ties, these values are 
theninterpolated so that eachproperty of each * e-mail: {mohamed.mahdi, ahmed.ali, hassan.rezg, amr.abdelha.z, 
us­ama.mohamed}@compit.au.edu.eg e-mail: khaled.hussain2000@gmail.com modi.er has a value at a given 
frame, applying modi.ers on copy of meshes at each frame produces the animation. We use a scene graphdata 
structurefor managing the scene. We provide algorithms for representing our objects accurately as convex 
shapes in physics simulations and use ACD [ming Lien and Amato 2007] for approximating general shapes 
to convexes. A unique feature we provide is deducing convex decomposition of our concave primitives depending 
on their half-edge structure; this allows ef.cient and accurate representation of complexly modi.ed primitives 
in physics simulations (e.g. a tube that is bended and twisted toform apipe, or as usedinFigure1(d)). 
Our scripting language is dynamic, memory managed and object oriented(hybridparadigm). Aninterpreterisprovidedforthelan­guage 
withaninterestthatZlang scriptsbe as standalone aspossible and crossplatform. Theinterpreterfocuses alsoin 
optimizingDCC objects creation and array creation and manipulaion. 3 IMPLEMENTATION AND FUTURE WORK 
In choosing tools for the system we focused on high performance open source or free, cross platform tools 
with large communities. We used CGAL Polyhedron [Kettner 2010] for storing meshes. GSL is used for interpolating 
values for key-framed animation us­ing cubic splines. Eigen libray is used for ef.cient matrix manip­ulation. 
OpenSceneGraph is used for managing the scene graph and other effects(e.g. lights). WeusedPhysX as aphysics 
engine (we preferred it over Bullet due to its current GPU acceleration). ANTLR[ParrandQuong1994] isused 
as aparserandlexergen­erator, in interpretation; the syntax tree is optimized, traversed and executed. 
For thefuture we are now working on two sides. First we areimplementingparts of our systeminOpenCL(especiallyglobal 
modi.ers). Second we are developing a Compiler for Zlang using LLVMas abackend as we are aiming at standaloneinteractive 
sim­ulations. References KETTNER, L., 2010. 3d polyhedral surfaces. CGAL Editorial Board -3.6 editions. 
MING LIEN, J., AND AMATO, N. M. 2007. Approximate convex decomposition of polyhedra. In Proceedings of 
the ACM Sym­posium on Solid and Physical Modeling,ACM,121 131. PARR,T.J., AND QUONG,R.W. 1994. Antlr:Apredicated-ll(k) 
parser generator. Software Practice and Experience 25, 789 810. Copyright is held by the author / owner(s). 
SIGGRAPH 2010, Los Angeles, California, July 25 29, 2010. ISBN 978-1-4503-0210-4/10/0007  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1836957</article_id>
		<sort_key>1120</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>104</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Automated 3D mesh segmentation using 2D footprints]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836845.1836957</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1836957</url>
		<abstract>
			<par><![CDATA[<p>Segmenting 3D meshes into distinct components is vital and necessary for more efficient processing and usability. The smaller segments are usually easier to process and can be associated with with semantics or geometric features. This can be used in 3D parametrization, 3D database creation, animation, deformation transfer and many other 3D graphics applications. However, automating such a process is challenging due to the variety and complexity of the input.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.4.6</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245.10010248</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems->Video segmentation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245.10010247</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems->Image segmentation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2264531</person_id>
				<author_profile_id><![CDATA[81466647216]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Wael]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Abdelrahman]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Deakin University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264532</person_id>
				<author_profile_id><![CDATA[81466647424]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Sara]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Farag]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Deakin University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[
Dupenois, M., and Galton, A., 2009. Assigning footprints to dot sets: An analytical survey.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1648425</ref_obj_id>
				<ref_obj_pid>1099154</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[
Lee, Y., Lee, S., Shamir, A., Cohen-Or, D., and Seidel, H.-P. 2005. Mesh scissoring with minima rule and part salience. <i>Comput. Aided Geom. Des. 22</i>, 5, 444--465.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[
Shamir, A. 2008. A survey on mesh segmentation techniques. <i>Computer Graphic Forum 27</i>, 6, 1539--1556.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Automated 3D Mesh Segmentation Using 2D Footprints Wael Abdelrahman and Sara Farag* Center of Intelligent 
Systems Research (CISR) Deakin University  (a) (b) (c) Figure 1: (a) The 2D footprint of a dog 3D mesh. 
(b) The vector representation of the 2D footprint. (c) The resulting cut (segmentation) lines in 3D. 
These lines connects the points of concave discontinuity and their antipodals suggested by the proposed 
algorithm. 1 Introduction Segmenting 3D meshes into distinct components is vital and nec­essary for more 
ef.cient processing and usability. The smaller segments are usually easier to process and can be associated 
with with semantics or geometric features. This can be used in 3D parametrization, 3D database creation, 
animation, deformation transfer and many other 3D graphics applications. However, au­tomating such a 
process is challenging due to the variety and com­plexity of the input. To have an automated and usable 
algorithm, the number of param­eters needs to be small or zero. An algorithm can have tuning pa­rameters 
that for example change the output according to available resources (time and power) but should try to 
minimize or avoid knowledge based parameters that are dependent on the input such as the number of required 
segments. The closest theory and seg­mentation criteria to the human mind is the minima rule from the 
cognitive theory. This rule suggests that a human cuts 3D objects whenever a concave discontinuity is 
found. To satisfy this theory the 3D mesh needs to be searched for the discontinuities. Then, a closed 
contour (cut line) around the discontinuity needs to be cre­ated. Lee et al. [Lee et al. 2005] used a 
similar technique but re­quired four parameters supplied to four functions (distance, normal, centricity 
and feature) to form the contours. We propose a novel algorithm that satis.es the cognition theory but 
requires no object dependent parameters and only tuning parame­ters that can be .xed for all inputs. 
The algorithm searches for the cut lines end points and tries to create plausible closed contours.  
2 Our Approach The problem of 3D segmentation is de.ned mathematically on a boundary mesh M as generating 
a set of sub-segments S, based on a certain criterion C. [Shamir 2008]. There exist a class of im­plicit 
algorithms that produce the boundaries between the segments instead of the sub segments. The proposed 
algorithm uses 2D foot­prints of multiple 3D object poses to identify two opposite vertices that can 
be used to generate a closed contour. The algorithm rotates a 3D boundary mesh around the three carte­ 
*{wmam, sfara@deakin.edu.au} sian axes with a rate of .. The smaller the . value the greater the number 
of 2D poses and accuracy of the antipodals. For each pose the mesh is projected on the x - y plane to 
form a set of 2D points. To get a vector format of the 2D projection, we need to have a set of lines 
that approximate its curvature. This requires the knowledge of the bounding curves of the 2D points set 
or their footprints. The footprint consist of the set of points that are de.ning the bound­ary. A recent 
survey by [Dupenois and Galton 2009] discusses var­ious techniques of obtaining the 2D footprints. This 
footprint is then linearized to form a set of lines. Lines formation is computed by checking the angle 
f between the pair of vectors V1,2 and V2,3 formed by the three points P1,P2 and P3 is less than fthreshold. 
The fthreshold is a tuning parameter and determines the number of the points of discontinuity to consider. 
This is general for any input 3d mesh. The output of the vectorizing process is a set of n lines {l1,l2, 
..., ln}. Each line l approximates a set of the footprints points and a set of normals {N1,N2, ..., Nm} 
is de.ned where m is the points count. Using this vector format, we can get the antipodal of the discontinuity 
vertices in 3D. The .nal step is to de.ne a con­tour between each pair of points. This is accomplished 
by de.ning a plane that contains the two vertices and another very close point determined based on the 
pose. The adjacent vertices between the two points are selected based on their angle with the plane. 
The preliminary results for a 3D dog mesh is shown in Figure1. The cut lines still need a post processing 
to produce smoother borders and remove redundant vertices. Nevertheless, the algorithm is fully autonomous 
and no object speci.c parameters are required. Future work will also include testing different categories 
of 3D objects and benching the algorithm against recent work. References DUPENOIS, M., AND GALTON, A., 
2009. Assigning footprints to dot sets: An analytical survey. LEE, Y., LEE, S., SHAMIR, A., COHEN-OR, 
D., AND SEIDEL, H.-P. 2005. Mesh scissoring with minima rule and part salience. Comput. Aided Geom. Des. 
22, 5, 444 465. SHAMIR, A. 2008. A survey on mesh segmentation techniques. Computer Graphics Forum 27, 
6, 1539 1556. Copyright is held by the author / owner(s). SIGGRAPH 2010, Los Angeles, California, July 
25 29, 2010. ISBN 978-1-4503-0210-4/10/0007  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1836958</article_id>
		<sort_key>1130</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>105</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[Automatic muscle generation for physically-based facial animation]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836845.1836958</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1836958</url>
		<abstract>
			<par><![CDATA[<p>Physically-based facial animation (FA) techniques are notoriously difficult to create, reuse, and art-direct. We address these shortcomings by proposing a rig-builder that automatically generates bony and soft-tissue substructures for any given head model. In an earlier work, [Aina 2009] presented a method for fitting a generic skull to any given head model as a first step toward automated rig-building. Here, we outline work done since, and give an overview of a method for creating muscles of facial expression (mimic muscles), and other soft-tissues in the gap between a given head model and a fitted generic skull.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Animation</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2264533</person_id>
				<author_profile_id><![CDATA[81421593448]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Olusola]]></first_name>
				<middle_name><![CDATA[O.]]></middle_name>
				<last_name><![CDATA[Aina]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[NCCA, Bournemouth University, UK]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264534</person_id>
				<author_profile_id><![CDATA[81339492799]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Jian]]></first_name>
				<middle_name><![CDATA[Jun]]></middle_name>
				<last_name><![CDATA[Zhang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[NCCA, Bournemouth University, UK]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1536173</ref_obj_id>
				<ref_obj_pid>1536170</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Aina, O. O. 2009. Generating anatomical substructures for physically-based facial animation. part 1: A methodology for skull fitting. <i>Vis. Comput. 25</i>, 5--7.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Larrabee, W. F., Makielski, K. H., and Sykes, J. 1997. Surgical anatomy for endoscopic facial surgery. In <i>Endoscopic Facial Plastic Surgey</i>, K. Gregory S, Ed. 3--33.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1185664</ref_obj_id>
				<ref_obj_pid>1185657</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Polthier, K., and Schmies, M. 2006. Straightest geodesics on polyhedral surfaces. In <i>SIGGRAPH '06</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Automatic Muscle Generation for Physically-Based Facial Animation  Figure 1: L-R: (1) generic skull 
plus muscle attachment regions (red), temporalis and masseter (brown), and aesthetic region bound­aries 
(blue) (2.) typical head model (3.) generic skull .tted to head model (4.) SMAS-plane (blue) and mutual 
tangents and convex hulls (both red) of muscle attachment regions (black) (5.) muscle .bres (red) as 
boundary-value straightest geodesics. Introduction Physically-based facial animation (FA) techniques 
are notori­ously dif.cult to create, reuse, and art-direct. We address these shortcomings by proposing 
a rig-builder that automatically gen­erates bony and soft-tissue substructures for any given head model. 
In an earlier work, [Aina 2009] presented a method for .tting a generic skull to any given head model 
as a .rst step to­ward automated rig-building. Here, we outline work done since, and give an overview 
of a method for creating muscles of facial expression (mimic muscles), and other soft-tissues in the 
gap between a given head model and a .tted generic skull. Relevant human facial anatomy 1. The subcutaneous 
tissues of the human face are uniformly divided into a super.cial and deep portion by the Super­.cial 
Muscular Aponeurotic System (SMAS) [Larrabee et al. 1997]. The SMAS sends .brous extensions to the dermis, 
envelopes the mimic muscles, and acts as a dis­tributor of force for the muscles. The SMAS overlays the 
masseter and temporalis muscles, and the temporal fascia. 2. The mimic muscle system consists of about 
40 muscles. Some variation in the number, size, symmetry, length and shape of mimic muscles is frequently 
observed between and within individuals. Mimic muscles are thin, sheet-like bundles of oriented, contractile 
.bers, that are attached to the skull at one end, and to the dermis at the other. 3. The human face 
can be divided into facial aesthetic units. Within each unit, the skin has fairly uniform histology, 
color, thickness etc. [Larrabee et al. 1997].  Method SMAS construction The SMAS represents a plane 
on which mimic muscles lie, and is constructed as a variational implicit surface (VIS). In order to construct 
this surface, the masseter and temporalis muscles, and the temporalis fascia geometries are added to 
the generic skull. The boundaries of the facial aes­thetic units are subsequently de.ned on the generic 
skull and the masseter, and used to select groups of vertices on the forehead to act as point constraints 
for creating for the VIS. The outer­most facial aesthetic unit borders also act as point constraints, 
and de.ne the extent to which the VIS is triangulated. All point constraints are offset from the generic 
skull. *oaina@bournemouth.ac.uk Copyright is held by the author / owner(s). SIGGRAPH 2010, Los Angeles, 
California, July 25 29, 2010. ISBN 978-1-4503-0210-4/10/0007 Muscle construction Each mimic muscle 
is modeled as a sheet of .bers stretching from an origin on the skull to an insertion beneath the SMAS. 
Accordingly, the generic skull is given UV­information in order to map a multi-layer texture de.ning 
the anatomically established, or artistically-decided, origins and in­sertions for real or imaginary 
muscles. The border of each mus­cle attachment region is converted to 3d using a reverse tex­ture mapping 
process. These additions to the generic skull are one-time operations, applicable to any head model, 
but can be tweaked if required. The lateral extents of each muscle are constructed as the mutual geodesic 
tangents connecting its origin and insertion regions. This is done by .rst constructing the geodesic 
convex hull of each attachment region, and then their joint convex hull, using a newly developed version 
of the divide and conquer algorithm for discrete manifolds. Any number of muscle .bers can sub­sequently 
be constructed as boundary-value straightest geodesic interpolated between a pair of mutual tangents. 
[Polthier and Schmies 2006] originally de.ned a straightest geodesic as the solution to an initial value 
problem. We however, have developed a method for constructing straightest geodesics between pairs of 
points using a euclidian path heuristic. As a fallback, we also provide a technique for approximating 
the straightest geodesic in the neighborhood of hyperbolic vertices, and term this approximation the 
straightest-possible geodesic. Straightest geodesics are used instead of conventional geodesics for two 
important reasons. First, the straightest property of the former best describes the orientation of muscle 
.bers. (It makes sense to construct muscle .bers linearly because they con­tract linearly.) Second, conventional 
geodesics do not guaran­tee the straightness property, and also perform extensive sur­face exploration, 
the cost of which would be prohibitive for the multiple-source multiple-destination path computations 
required by the problem at hand. For example, constructing the 5 muscles shown above in Figure 1 requires 
1100 geodesic computations. References AINA, O. O. 2009. Generating anatomical substructures for physically-based 
facial animation. part 1: A methodology for skull .tting. Vis. Comput. 25, 5-7. LARRABEE, W. F., MAKIELSKI, 
K. H., AND SYKES, J. 1997. Surgical anatomy for endoscopic facial surgery. In Endo­scopic Facial Plastic 
Surgey, K. Gregory S, Ed. 3 33. POLTHIER, K., AND SCHMIES, M. 2006. Straightest geodesics on polyhedral 
surfaces. In SIGGRAPH 06. 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1836959</article_id>
		<sort_key>1140</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>106</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>5</seq_no>
		<title><![CDATA[Cyclic twill-woven objects]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836845.1836959</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1836959</url>
		<abstract>
			<par><![CDATA[<p>Any arbitrary twist of the edges of an extended graph rotation system induces a cyclic weaving on the corresponding surface [Akleman et al. 2009]. This recent theoretical result allows us to study generalized versions of textile weaving structures as cyclic weaving structures on arbitrary surfaces. In this work, we extend the study to twill weaving, which is used in fabrics such as denim or gabardine. Biaxial twill is a textile weave in which the weft (filling) threads pass over and under two consecutive warp threads and each row is obtained from the row above it by a shift of 1 unit to the right or to the left. The shift operation creates the characteristic diagonal pattern that makes the twill fabric visually appealing.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061.10010063</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures->Computational geometry</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2264535</person_id>
				<author_profile_id><![CDATA[81100124987]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Ergun]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Akleman]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Texas A&M University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264536</person_id>
				<author_profile_id><![CDATA[81342491147]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Jianer]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Chen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Texas A&M University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264537</person_id>
				<author_profile_id><![CDATA[81448598429]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Yen-Lin]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Chen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Texas A&M University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264538</person_id>
				<author_profile_id><![CDATA[81100014535]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Qing]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Xing]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Texas A&M University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1531384</ref_obj_id>
				<ref_obj_pid>1531326</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Akleman, E., Chen, J., Xing, Q., and Gross, J. 2009. Cyclic plain-weaving with extended graph rotation systems. <i>ACM Transactions on Graphics; Proceedings of SIGGRAPH'2009</i>, 78.1--78.8.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Cyclic Twill-Woven Objects Ergun Akleman Jianer Chen Yen-Lin Chen Qing Xing Visualization Dept. Computer 
Science Dept. Computer Science Dept. Architecture Dept. Texas A&#38;M University Texas A&#38;M University 
Texas A&#38;M University Texas A&#38;M University  (a) Venus (b) Bunny (c) Cubical (d) Geodesic Dome 
Figure 1. Examples of cyclic twill weaving on polygonal mesh surfaces constructed with the voting algorithm. 
Any arbitrary twist of the edges of an extended graph rotation sys­tem induces a cyclic weaving on the 
corresponding surface [Akle­man et al. 2009]. This recent theoretical result allows us to study generalized 
versions of textile weaving structures as cyclic weaving structures on arbitrary surfaces. In this work, 
we extend the study to twill weaving, which is used in fabrics such as denim or gabardine. Biaxial twill 
is a textile weave in which the weft (.lling) threads pass over and under two consecutive warp threads 
and each row is obtained from the row above it by a shift of 1 unit to the right or to the left. The 
shift operation creates the characteristic diagonal pattern that makes the twill fabric visually appealing. 
In this work, we introduce the de.nition of a twill as a cyclic weav­ing structure on general surfaces. 
Based on this de.nition, we prove that three mesh conditions are necessary and suf.cient to obtain twill 
weaving from a given mesh. We show that many arbitrary meshes do not satisfy these three conditions. 
It is, therefore, not possible to obtain exact twill for many meshes. On the other hand, for mostly (4, 
4) meshes, i.e. meshes with large areas of quadri­laterals with 4-valent vertices, it is possible to 
obtain a reasonably good result of twill in most places. Based on this intuition, we have developed a 
voting algorithm that guarantees to satisfy most of the twill conditions that allow to demonstrate diagonal 
patterns every­where as shown in examples in Figure 1.(a), (b) and (c). The voting algorithm also creates 
exact twill if the mesh is twillable. The subdivided meshes are good candidates for twillable meshes 
since subdivisions can make the number of crossings in each cycle divisible by 4 and they can populate 
a mesh with regular regions. Triaxial twill is created from meshes that are populated with (3, 6) regions 
(i.e triangles with 6-valent vertices). Such meshes can be obtained by triangular subdivision schemes, 
such as midpoint sub­division. We prove that every mesh obtained by a midpoint subdi­vision is twillable. 
Patterns created by triaxial twill are visually interesting and remind some of the M. C. Escher s tilings 
as shown in Figure 1.(d). How­ever, triaxial twill does not demonstrate the characteristic diagonal pattern 
of biaxial twill, which can only be obtained from meshes populated by (4, 4) regions. Any mesh can be 
populated by (4, 4) regions by iteratively applying a quad-remeshing scheme such as Catmull-Clark or 
Doo-Sabin sub­divisions. We have investigated the meshes that can be converted to or can continue to 
be twillable after applications of quad-remeshing schemes. We call these meshes descendent twillable 
meshes. We have identi.ed a set of conditions to obtain descendent twill­able meshes with quad-remeshing 
schemes as shown in Figure 2. We prove that threads of such biaxial twill woven objects can al­ways be 
classi.ed as warp and weft and can be two-colored. We have also developed a coloring algorithm to paint 
threads of woven object with minimum number of colors. All images in this work are created using this 
minimum coloring algorithm. Figure 2: Three biaxial twill weaving objects obtained by the de­scendants 
of the same descendent-twillable mesh. References AKLEMAN, E., CHEN, J., XING, Q., AND GROSS, J. 2009. 
Cyclic plain-weaving with extended graph rotation systems. ACM Transactions on Graphics; Proceedings 
of SIGGRAPH 2009, 78.1 78.8. Copyright is held by the author / owner(s). SIGGRAPH 2010, Los Angeles, 
California, July 25 29, 2010. ISBN 978-1-4503-0210-4/10/0007  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1836960</article_id>
		<sort_key>1150</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>107</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>6</seq_no>
		<title><![CDATA[Darwin's Lake]]></title>
		<subtitle><![CDATA[sketch-based creature creation system enables users to collaborate with contents designers]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836845.1836960</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1836960</url>
		<abstract>
			<par><![CDATA[<p>In this study, we propose a novel system to create vivid animated 3d creature model from user's freeform 2d stroke. The most famous technique that construct 3d model from user's 2d sketch is Teddy system[Igarashi et al. 1999]. Past teddy like system create 3D mesh from user sketched 2d-shape, only what is needed is the stroke. These approaches allow a lot of freedom of creativity to us. However, there are also the cases we need to build the identification or determine the character of contents such like most games. In these cases, although it proper to design or control the contents in advance by the developers, the past methods are too much freely to control their contents. To address this, our approach enables end-users to collaborate with product developer's design without decreasing the freedom of creativity. It creates 3d animated creature from the combination of user's freeform stroke and the primitive models that are created by contents' designer beforehand. The generated creature depend on the user's inspiration, so obtained variations are infinte, but don't lack of the identification of the contents which is designed by the creators. Furthermore, our approach makes us to animate the generated model as if it is alive much easier than past methods.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>H.5.3</cat_node>
				<descriptor>Collaborative computing</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003130</concept_id>
				<concept_desc>CCS->Human-centered computing->Collaborative and social computing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2264539</person_id>
				<author_profile_id><![CDATA[81447597329]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Kazuhiko]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yamamoto]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Kyushu University, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264540</person_id>
				<author_profile_id><![CDATA[81442599677]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Toki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Takeda]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Kyushu University, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264541</person_id>
				<author_profile_id><![CDATA[81319487558]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Ryoichi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ando]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Kyushu University, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264542</person_id>
				<author_profile_id><![CDATA[81466646663]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Syota]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kawano]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Kyushu University, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>311602</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[<b>Takeo Igarashi, Satoshi Matsuoka, and Hidehiko Tanaka. 1999</b>. Teddy: A Sketching Interface for 3D Freeform Design, ACM SIGGRAPH, 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Darwin s Lake : Sketch-Based Creature Creation System Enables Users to Collaborate with Contents Designers. 
Kazuhiko Yamamoto, Toki Takeda, Ryoichi Ando, Syota Kawano Graduate School of Design, Kyushu University, 
Japan 1 Introduction In this study, we propose a novel system to create vivid animated 3d creature model 
from user s freeform 2d stroke. The most famous technique that construct 3d model from user s 2d sketch 
is Teddy system[Igarashi et al. 1999]. Past teddy like system create 3D mesh from user sketched 2d-shape, 
only what is needed is the stroke. These approaches allow a lot of freedom of creativity to us. How­ever, 
there are also the cases we need to build the identi.cation or determine the character of contents such 
like most games. In these cases, although it proper to design or control the contents in advance by the 
developers, the past methods are too much freely to control their contents. To address this, our approach 
enables end-users to collaborate with product developer s design without decreasing the freedom of creativity. 
It creates 3d animated creature from the com­bination of user s freeform stroke and the primitive models 
that are created by contents designer beforehand. The generated creature depend on the user s inspiration, 
so obtained variations are in.nte, but don t lack of the identi.cation of the contents which is designed 
by the creators. Furthermore, our approach makes us to animate the generated model as if it is alive 
much easier than past methods. 2 Model Construction Our algorithm creates a new creature-like 3d model 
from the com­bination of the user sketched stroke and model parts(means primi­tives, which represent 
heads, bodies, tails, .ns...etc) which is cre­ated by designers in advance. We .rst create a closed planar 
poly­gon by connecting the start-point and end-point of user de.ned stroke(Figure.1-a), and extract the 
spine and control skeletons from the polygon. These control skeletons are used for animation. Next, we 
construct box-shaped model deformation spaces from the skele­tons. Finally, we apply preconstructed model 
parts, which are cre­ated by designers of the contents, into the model deformation space by performing 
Free Form Deformation(FFD). When user s 2d stroke is given, the system then performs con­strained Delaulay 
triangulation of the polygon(Figure.1-b). The edges of user de.ned initial polygon are called external 
edges, while edges added in the triangulation are called internal edges. We then divide the triangles 
into three categories: triangle with two external edges (terminal triangle), triangle with one external 
edge (sleeve triangle), and triangles without external edges (junction tri­angle). This prodecure is 
same as teddy system. In the case of ter­minal triangles and sleeve triangles, we de.ne rib lines(red 
lines in Figure.1-c) which is obtained by connecting the midpoint of ex­ternal edge and the vertex with 
opposite side of each triangle. The midpoint of rib lines becomes the control skeletons(Figure.1-d). 
In the case of junction triangles, control skeletons are de.ned as the midpoints of each line segment 
of the triangle and the grav­ity point. By connecting the adjacent control skeletons, the spine of the 
polygon is obtained. Next, we elevate rib lines propo­tional to their length, and construct 3d-trapezoids 
that is called Control P lane as surrounding the control skeletons(Figure.2). These control planes are 
related to each control skeleton, and move accorrding to the skeletons. Finally, we connect each vertices 
of adjacent control planes, and construct box-shaped model defor­mation space(Figure.2). The spaces are 
divided into three cate­gories:terminal space which correspond to head or tail of creature, sleeve space 
which correspond to body, junction space which is constructed from junction triangle. We adjust model 
parts(The de­signer created primitives), and .t into each space using Free Form Deformation technique(Figure.1-f). 
 3 Animation To animate the created creature, we move the positions of control skeletons. Accompany 
with the motions of skeletons, the plane Fig.1 Construction of 3D creature. Fig.2 Three kinds of Model 
Deformation Space related to the skeleton move and rotate, and model deformation spaces are deformed. 
Using these spaces, we can animate the model parts by performing FFD again. Each position of skeletons 
P k are updated by blending rigid body motion and physical motion. kk-1kk P = bRPrel + (1 - b)S(P ) (1) 
 where P k denotes the relative position with the parent skeleton at rel the initial state. The .rst 
term of right hand side of Eq.(1) represents rigid body motion, and the second term represents physical 
motion obtained by calcurating the forces S(P k) genatrated by simulating the creature s tissue. For 
physical forces, we know the third order nonlinear spring and damper system is well performed experimen­taly. 
The Parameter b denotes the blending weight which also rep­resents the rigidity of the creature s tissue, 
and Rk-1 is the rotation matrix of the parent skeleton. Additionally, if the model parts have an animation 
data, we can apply both motion obtained by Eq.(1) and the animation data by blending the positions of 
the vertices. 4 Results and Conclusion Fig.3 Vivid animated creatures. Our prototype system Darwin 
s Lake explore in.nte variation of 3d creature from user s sketch, but don t lack of the identi.cation 
as a content at all. What users should do is drawing a 2d closed shape, and selecting a model parts, 
and then 3d creature is genareted easily. The genarated creatures can be animated as if it is alive in 
this system. Additionally our algorithm can animate more than 500 creatures in real time using low spec 
PC using CPU only. References Takeo Igarashi, Satoshi Matsuoka, and Hidehiko Tanaka. 1999. Teddy: A Sketching 
Interface for 3D Freeform Design, ACM SIG­GRAPH,1999. Copyright is held by the author / owner(s). SIGGRAPH 
2010, Los Angeles, California, July 25 29, 2010. ISBN 978-1-4503-0210-4/10/0007  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1836961</article_id>
		<sort_key>1160</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>108</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>7</seq_no>
		<title><![CDATA[FASTCD]]></title>
		<subtitle><![CDATA[fracturing-aware stable collision detection]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836845.1836961</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1836961</url>
		<abstract>
			<par><![CDATA[<p>Simulating complex phenomena such as fracture requires collision detection (CD) methods to avoid any inter-collisions among deforming models and self-collisions (i.e. intra-collisions) within each deforming model. CD is typically the main computational bottleneck of simulating such complex phenomena.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2264543</person_id>
				<author_profile_id><![CDATA[81442594475]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Jae-Pil]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Heo]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[KAIST]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264544</person_id>
				<author_profile_id><![CDATA[81442593431]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Duksu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kim]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[KAIST]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264545</person_id>
				<author_profile_id><![CDATA[81100554314]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Joon-Kyung]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Seong]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[KAIST]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264546</person_id>
				<author_profile_id><![CDATA[81329489309]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Jeong-Mo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hong]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Dongguk University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264547</person_id>
				<author_profile_id><![CDATA[81384613762]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Min]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Zhejiang University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264548</person_id>
				<author_profile_id><![CDATA[81100019061]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Sung-Eui]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yoon]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[KAIST]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1364908</ref_obj_id>
				<ref_obj_pid>1364901</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Tang, M., Curtis, S., Yoon, S.-E., and Manocha, D. 2008. Interactive continuous collision detection between deformable models using connectivity-based culling. <i>ACM Symp. on Solid and Physical Modeling</i>, 25--36.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Teschner, M., Heidelberger, B., Muller, M., Pomeranets, D., and Gross, M. 2003. Optimized spatial hashing for collision detection of deformable objects. In <i>Proc. of Vision, Modeling and Visualization</i>, 47--54.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Volino, P., and Thalmann, N. M. 1994. Efficient self-collision detection on smoothly discretized surface animations using geometrical shape regularity. <i>Computer Graphics Forum (EuroGraphics Proc.) 13</i>, 3, 155--166.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 FASTCD: Fracturing-Aware Stable Collision Detection Jae-Pil Heo1, Duksu Kim1, Joon-Kyung Seong1, Jeong-Mo 
Hong2, MinTang3, and Sung-EuiYoon1 1 KAIST 2 Dongguk University 3 Zhejiang University (a) Exploding 
Dragon Benchmark (b) Breaking-Wall Benchmark Figure 1: These .gures show two complex and large-scale 
fracturing benchmarks that have topological changes. (a) Three frames of a breaking dragon benchmark 
that consists of 252Ktriangles throughout the simulation. (b) Three frames ofa breaking-wall benchmark 
that starts with 42Ktriangles and ends with 140Ktriangles. Our method spends 252 ms and 97 ms for discrete 
collision detection including self-collision detections of these dragon and wall benchmarks respectively 
by using a single CPU-thread. Moreover, we show a more stable performance by achieving up to two orders 
of magnitude performance improvement at fracturing events where deforming meshes change their topologies. 
1 Introduction Simulating complex phenomena such as fracture requires colli­sion detection (CD) methods 
to avoid anyinter-collisions among de­forming models and self-collisions (i.e. intra-collisions) within 
each deforming model. CD is typically the main computational bottleneck of simulating such complex phenomena. 
CD methods are commonly accelerated by using bounding vol­ume hierarchies (BVHs) constructed from deforming 
models. BVHs of deforming meshes should be updated as deforming meshes change their geometry and topology. 
At fracturing events, the geometry and topology undergo more drastically changes. Therefore, BVHs at 
fracturing events become to have lower culling ef.ciencies, de­grading the performance of CD more signi.cantly. 
As a result, users may experience noticeable performance degradations at such frac­turing events. Therefore, 
large-scale fracturing simulations have not been widely employedinvarious interactive applications, becauseof 
their unstable performances, which are critical problems for interac­tive applications.  2 Our Method 
In this work, we propose a Fracturing-Aware STable CD (FASTCD) method for complex and large-scale fracturing 
models that have geometric and topological changes, in order to achieve a stable and fast performance 
for CD including self-collision detec­tion. Our FASTCD method relies on three main contributions: 1) 
a novel culling method to improve the performance of self-collision detection,2)a selective BVH restructuring 
method based ona novel cost metric, and3)fast BVH construction method. We present a novel culling method,Dual-Cone 
method, for self­collision detection. Our dual-cone method is inspired by two suf­.cient conditions for 
self-colliding surface proposedbyVolino and Thalmann [Volino and Thalmann 1994]. The time complexity 
for checking these conditions was O(n 2) at worst case, where n is the numberof trianglesofgivensurface.Weusetwo 
conesto reducethe overhead to examine whether the surface can have self-collisions or not. Our dual-cone 
method has O(1) time complexity and is ef.cient even when deforming models havedrastic geometric and 
topological changes. At fracturing events, the geometry and topology undergo drasti­cally changes. Therefore, 
culling ef.ciencies of BVHs of fractur­ing models can be signi.cantly lowered. In order to maintain high­quality 
BVHs, we restructure sub-BVHs which have low culling ef­.ciency. In order to detect such sub-BVH, we 
de.ne a novel cost metric that measuresexpected numberofBVoverlap tests performed recursively identify 
self-collisions and inter-collisions.We also con­sider the potential bene.t and the potential performance 
loss due to restructuring.Wedo restructuring only when the bene.tisexpected to be larger than the overhead. 
Even though we perform selective restructuring for fracturing models, it may take a huge amount of time 
to reconstruct sub-BVHs  Figure 2: Left.gureshowsframerategraphsof continuousCDfortheexplodingdragon 
benchmark withTang et al. s method(T-CCD), and our method(Ours). Right .gure showsframerategraphsof discreteCD 
for thebreaking-wall benchmark withTeschner et al. s method(S-Hash), and our method(Ours). for complexfracturing 
models as shown in Fig. 1. This can cause the overall CD method to be unstable, because the performance 
degrada­tions from the reconstruction can be very high. In order to overcome this problem, we propose 
a fast BVH construction method based on grid and hashing, instead of heavily relying on expensive sorting 
techniques, which are commonly used in most O(nlogn) construc­tion methods. 3 Result As can be seen 
in the left graph of Fig. 2, our method(Ours) shows a more stable performance even when deforming models 
change their topology. On the other hand, T-CCD [Tang et al. 2008], one of the-state-of-the-art techniques 
for deforming models, shows drastic performance degradations at such cases. More speci.cally, our method 
improves the continuousCD performancebyafactorof 260 times over T-CCD method at frames where deforming 
models change their topologies. The more graceful performance degradation of our method is due to our 
selective restructuring method that also uses ourfast BVH construction method. Wealso compare the discrete 
CD performance of our method with that of S-Hash [Teschner et al. 2003], one of most widely used tech­niques 
for volumetric fracturing simulations. Our method(Ours) runs 20 times faster in the breaking-wall models 
(right graph of Fig. 2). The inferior performance of S-Hash is mainly because many parts of fracturing 
models come in a close proximity, causing many grid cells to have multiple triangles. References TANG,M.,CURTIS,S.,YOON,S.-E., 
AND MANOCHA,D. 2008. Interactive continu­ ous collision detection between deformable models using connectivity-based 
culling. ACM Symp. on Solid and Physical Modeling, 25 36. TESCHNER,M.,HEIDELBERGER,B.,MULLER,M.,POMERANETS,D., 
ANDGROSS, M. 2003. Optimized spatial hashing for collision detection of deformable objects. In Proc.ofVision, 
Modeling andVisualization, 47 54. VOLINO, P., AND THALMANN, N. M. 1994. Ef.cient self-collision detection 
on smoothly discretized surface animations using geometrical shape regularity. Com­puterGraphicsForum 
(EuroGraphicsProc.)13, 3, 155 166. Copyright is held by the author / owner(s). SIGGRAPH 2010, Los Angeles, 
California, July 25 29, 2010. ISBN 978-1-4503-0210-4/10/0007  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1836962</article_id>
		<sort_key>1170</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>109</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>8</seq_no>
		<title><![CDATA[Faster accurate reflections throught quadric mirrors]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836845.1836962</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1836962</url>
		<abstract>
			<par><![CDATA[<p>Reflectors attract the attention of people since they reflect discontinuous images of the world and often provide unexpected information of a non-direct field of view. This is why reflections still have a lot of research attention in rendering of images in computer graphics, computer vision and optics, amongst other fields.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.6</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2264549</person_id>
				<author_profile_id><![CDATA[81100240208]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Nuno]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Goncalves]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Coimbra]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264550</person_id>
				<author_profile_id><![CDATA[81466642640]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Ana]]></first_name>
				<middle_name><![CDATA[Catarina]]></middle_name>
				<last_name><![CDATA[Nogueira]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Coimbra]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>2383934</ref_obj_id>
				<ref_obj_pid>2383894</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[
Estalella, P., Martin, I., Drettakis, G., and Tost, D. 2006. A gpu-driven algorithm for accurate interactive reflections on curved objects. In <i>Eurographics Symposium on Rendering</i>.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[
Goncalves, N. 2010. On the reflection point where light reflects to a known destination in quadric surfaces. <i>Optics Letters 35</i>, 2 (Jan).
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[
Roger, D., and Holzschuch, N. 2006. Accurate specular reflections in real-time. In <i>Eurographics</i>, vol. 25.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[
Szirmay-Kalos, L., Umenhoffer, T., Patow, G., Szcsi, L., and Sbert, M. 2009. Specular effects on the gpu: State of the art. In <i>Computer Graphics Forum</i>.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Faster accurate re.ections throught quadric mirrors Nuno Goncalves* Ana Catarina Nogueira Institute 
of Systems and Robotics -University of Coimbra  Figure 1: The light ray re.ection by a quadratic surface 
(sphere in this example). 1 Introduction Re.ectors attract the attention of people since they re.ect 
discontin­uous images of the world and often provide unexpected information of a non-direct .eld of view. 
This is why re.ections still have a lot of research attention in rendering of images in computer graphics, 
computer vision and optics, amongst other .elds. In a generic combination of camera (image plane) and 
re.ectors there isn t any expression that gives us the point on the re.ector surface where the light 
ray is re.ected on the image direction (this expression, when it exists, is called projection model). 
The non existence of this projection model limits the rendering process to use back tracing with the 
well-known advantages and drawbacks. The use of forward methods to project points (very popular for the 
use with global illumination and other rendering techniques) is achieved by either using Snell Law or 
Fermat Principle (which give accurate results) or by introducing approximations on the re.ec­tions [Szirmay-Kalos 
et al. 2009; Estalella et al. 2006; Roger and Holzschuch 2006]. The necessary trade-off between performance 
and accuracy can severely affect the quality of the resulting images. Goncalves [Goncalves 2010] recently 
proved that when the re.ec­tor is a second order quadratic surface (includes spheres, ellipsoids, paraboloids 
and hyperboloids) the projection model that relates a 3D world point with its corresponding image point 
(knowing the re.ection point) can be searched for in a unidimensional curve on the re.ector surface. 
This result was proved to be a key idea to im­prove the performance of forward projection of world point 
to the image. Particularly it has been proved that the proposed method can compute the re.ection point 
with a one-order of magnitude quicker method than those that usually use Fermat Principle or Snell Law. 
 2 Our approach Suppose a 3D world point located at the source S and a viewing eye at the target T as 
showed in right part of .gure 1. If the re.ector surface is a quadratic equation expressed by the matrix 
Q, then it has been proved by Goncalves [Goncalves 2010] that the re.ection point belongs to a curve 
on the re.ector surface given by the inter­section of quadric Q and quadric A = f (Q, S, T). The curve 
is *e-mail:nunogon@isr.uc.pt anacatnog@isr.uc.pt then searched for the point where incident and re.ected 
angles are equal (Snell Law) or for the point that makes the light travel the fastest path (Fermat Principle). 
In practice it means that the re.ection point on the surface (essential for rendering with re.ections), 
is searched for in a unidimensional curve instead of being searched for in a multidimensional space. 
Our experimental tests show that when compared with the classical approaches using Snell Law or Fermat 
Principle, our method is one order of magnitude quicker, while maintaining the accuracy. It is also well-known 
that non linear optimization algorithms in one di­mension converge much quicker than in a multi dimensional 
space. Figure 1 shows an example of a rendering using our re.ection al­gorithm with highly accurate and 
realistic re.ection information. For future directions we want to compare our algorithm with the mesh-based 
methods that usually obtain better performances with low accuracy and sometimes introducing undesirable 
artifacts. For that, we want to .rst adapt our approach to use a graphic processing unit (GPU) and to 
extend the re.ectors to other surfaces that can be modeled or approximated by quadratic equations. Acknowledgements 
The authors gratefully acknowledge the support of the Portuguese Foundation for the Science and Technology 
with the project PTDC/EIA-CCO/109120/2008. References ESTALELLA, P., MARTIN, I., DRETTAKIS, G., AND 
TOST, D. 2006. A gpu-driven algorithm for accurate interactive re.ections on curved objects. In Eurographics 
Symposium on Rendering. GONCALVES, N. 2010. On the re.ection point where light re.ects to a known destination 
in quadric surfaces. Optics Letters 35,2 (Jan). ROGER, D., AND HOLZSCHUCH, N. 2006. Accurate specular 
re.ections in real-time. In Eurographics, vol. 25. SZIRMAY-KALOS, L., UMENHOFFER, T., PATOW, G., SZCSI, 
L., AND SBERT, M. 2009. Specular effects on the gpu: State of the art. In Computer Graphics Forum. Copyright 
is held by the author / owner(s). SIGGRAPH 2010, Los Angeles, California, July 25 29, 2010. ISBN 978-1-4503-0210-4/10/0007 
 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<article_sponsors>
			<funding_agency>Portuguese Foundation for the Science and Technology</funding_agency>
			<grant_numbers>
				<grant_number>PTDC/EIA-CCO/109120/2008</grant_number>
			</grant_numbers>
		</article_sponsors>
	</article_rec>
	<article_rec>
		<article_id>1836963</article_id>
		<sort_key>1180</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>110</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>9</seq_no>
		<title><![CDATA[Motion texture animation of water surface]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836845.1836963</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1836963</url>
		<abstract>
			<par><![CDATA[<p>We present a new method for making wave animation from still water image. In our method, users can control the behavior of wave in the water surface intuitively and interactively. After we simulate the wave using a Spectral Method [Tessendorf 1999], we have the water surface corresponding to the projection system of static images. Previous works for animating water surface. Chuang <i>et al</i> [Chuang and Goldman 2005] proposed a method for generating an animating of picture using displacement mapping and warping, however, those methods are only effective for gentle and calm water surfaces. Contrarily, our method is adaptively used for large scale waves of water height field.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Animation</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2264551</person_id>
				<author_profile_id><![CDATA[81545962956]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Yasuyuki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tomita]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Kyushu University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264552</person_id>
				<author_profile_id><![CDATA[81331505544]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Reiji]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tsuruno]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Kyushu University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1073273</ref_obj_id>
				<ref_obj_pid>1186822</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Chuang, Y.-Y., and Goldman, D. B. 2005. Animating Pictures with Stochastic Motion Textures. In <i>Proceedings of ACM SIGGRAPH 25</i>, 3 (July), 853--860.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Tessendorf, J. 1999. Simulating Ocean Water. <i>SIGGRAPH Simulating Nature Realistic and Interactive Techniques Course Notes</i>, 47.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Motion Texture Animation of Water Surface Yasuyuki Tomita * Reiji Tsuruno Kyushu University Kyushu University 
School of Design Faculty of Design 1 Introduction We present a new method for making wave animation 
from still wa­ter image. In our method, users can control the behavior of wave in the water surface intuitively 
and interactively. After we simulate the wave using a Spectral Method [Tessendorf 1999], we have the 
wa­ter surface corresponding to the projection system of static images. Previous works for animating 
water surface. Chuang et al [Chuang and Goldman 2005] proposed a method for generating an animat­ing 
of picture using displacement mapping and warping, however, those methods are only effective for gentle 
and calm water surfaces. Contrarily, our method is adaptively used for large scale waves of water height 
.eld. 2 Our Method Figure 1:Height Field Figure 2: 3D Water Surface Our method has two major steps 
to create the dynamic wave ani­mation for static images. At the .rst step, we simulate the plausible 
motion of water surface and wave in real time by Spectral Method. At the second step, we match the parameters 
of water surface and static images of water surfaces. 2.1 Simulating Water Surface We use Spectral Method 
[Tessendorf 1999] to simulate the water surfaces and waves. The spectrum .lter Ph (k) which is based 
on a statistical models of wind speed V and direction w , is given as formula (1). exp(-1/(kL)2 ) Ph 
(k)= A|k · w |2 (1) k4 where L = Vg 2 , k, A are spatial frequencies constant coef.cients. By using this 
.lter and formula (2), we can generate height .eld of water surfaces as seen from Figure 1. 1 h 0 (k)= 
v (.r + i.i ) Ph (k) (2) 2 where .r and .i are gaussian ramdom numbers, respectively. From the obtained 
height .eld, we can create 3D waving water surfaces illustrated in Figure 2. We also give wind parameters 
by using mouse operations which determines the behavior of waves. This *e-mail:dusk@verygood.aid.design.kyushu-u.ac.jp 
e-mail:tsuruno@design.kyushu-u.ac.jp allows users to specify the direction and speed of water surfaces 
intuitively and interactively. 2.2 Matching to Images The water surface model is projected to target 
picture s geometry. First, water area is manually cropped by user. Next, we composite generated 3D water 
surface meshes into the image. Texture map­ping method is used to produce plausible rendering of surfaces. 
 3 Result Figure 3:Target Image Figure 4: Result Image Figure 4 illustrates an obtained image by using 
our method. This choppy lake image(Figure 4) was generated from a calm lake im­age( Figure 3). It is 
also possible to control a state of water surface in a still image with wind speed by mouse drag distance 
and wind direction as the user inputs direction of wind in real time. We have used Intel(R) Core(TM) 
2 Quad CPU 2.66GHz, 2GB memory as spec of PC. This example was animated with 10 frames per second. 4 
Conclusion We proposed a method for generating waving water surface anima­tion for still images. First, 
we modeled 3D water surfaces using Spectrum Method. Second, we .tted 3D water surfaces into tar­get still 
image by projection geometry. Finally, we added 3D water surface images onto target image and succeeded 
to animate water surfaces in the still image. Our method can be used to generate dy­namic wave animations 
from a still image. In the future work, we are planning to design automatic algorithm which projects 
geome­try to render optical realistic scenes. References CHUANG, Y.-Y., AND GOLDMAN, D. B. 2005. Animating 
Pic­tures with Stochastic Motion Textures. In Proceedings of ACM SIGGRAPH 25, 3 (July), 853 860. TESSENDORF, 
J. 1999. Simulating Ocean Water. SIGGRAPH Simulating Nature Realistic and Interactive Techniques Course 
Notes, 47. Copyright is held by the author / owner(s). SIGGRAPH 2010, Los Angeles, California, July 25 
 29, 2010. ISBN 978-1-4503-0210-4/10/0007  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1836964</article_id>
		<sort_key>1190</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>111</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>10</seq_no>
		<title><![CDATA[Physical modeling of heterogeneous embedded deformable object deformation]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836845.1836964</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1836964</url>
		<abstract>
			<par><![CDATA[<p>Simulation of the interactions with deformable models is important in many applications such as medical training and tissue engineering. To physically model the 3D object, both the inner and outer segments need to be considered. This implies dealing with different materials and hence different deformation behavior. Thus, a physically-based simulation needs to augment the behavior of embedded materials when the materials are in direct physical contact, and produce a plausible net result in both visual and haptic cues.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.6.5</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341.10010342</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Model development and analysis</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061.10010063</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures->Computational geometry</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2264553</person_id>
				<author_profile_id><![CDATA[81466647424]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Sara]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Farag]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Deakin University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264554</person_id>
				<author_profile_id><![CDATA[81466647216]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Wael]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Abdelrahman]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Deakin University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>522098</ref_obj_id>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Mitchell, M. 1998. <i>An Introduction to Genetic Algorithms</i>. MIT.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1531358</ref_obj_id>
				<ref_obj_pid>1576246</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Nesme, M., Kry, P. G., Je&#345;&#225;bkov&#225;, L., and Faure, F. 2009. Preserving topology and elasticity for embedded deformable models. In <i>ACM Transactions on Graphics (Proc. of SIGGRAPH)</i>, ACM. to appear.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Physical Modeling of Heterogeneous Embedded Deformable Object Deformation Sara Farag and Wael Abdelrahman 
* Center of Intelligent Systems Research (CISR) Deakin University  Figure 1: (Right) A 1D heterogeneous 
bar deformed under external forces (shown as red arrows) and .xed from one side (green .xtures at the 
left end). The material distribution affects its behavior and optimised values are required of the shape 
(interpolation) functions. (Left) Using a genetic algorithm to estimate the shape functions. 1 Introduction 
Simulation of the interactions with deformable models is important in many applications such as medical 
training and tissue engineer­ing. To physically model the 3D object, both the inner and outer segments 
need to be considered. This implies dealing with differ­ent materials and hence different deformation 
behavior. Thus, a physically-based simulation needs to augment the behavior of em­bedded materials when 
the materials are in direct physical contact, and produce a plausible net result in both visual and haptic 
cues. A straightforward solution is to model the object as a mass spring system (MSS) and change stiffness 
in different material regions. Although this is a direct and fast approach, it is not usually conver­gent 
or matches the constitutional laws. Other approaches use .nite element methods (FEM) to simulate the 
interactions. For static sim­ulation the governing equation is: Kabu1 b = F a (1) where the stiffness 
matrix (for the 1D case) is given by: lL 2µ(1 - .) .Na(x1) .Nb(x1) Kab = A dx1 (2) 1 - 2. .x1 .x1 0 Here 
µ is the shear modulus, . is the Poisson s ratio, A is the cross section, L is the 1D bar length, u1 
b is the displacement vector, F a is the external force vector, and Na and Nb are the shape functions. 
The shape functions are used in the interpolation of displacements along the object elements. In classic 
FEM, linear or quadratic shape functions are used for the 1D case where -1 = . = 1 Eq. (3). However, 
they do not re.ect the material distribution of the object because they are static and designed independently 
to be generic [Nesme et al. 2009]. N1(.)=0.5(1 - .) N2(.)=0.5(1 + .) (3) We propose a shape function 
estimation techniques using genetic algorithms. The technique uses empirical data sets as a .tness func­tion 
to judge the accuracy of the solution. *{sfara, wmam@deakin.edu.au}  2 Our Approach In order to use 
an optimisation technique such as genetic algo­rithms [Mitchell 1998], we need to have an individual 
represen­tation (chromosome) and a .tness function to evaluate the perfor­mance of the individuals. The 
algorithm also has tuning parameters which are the mutation and the crossover functions. For the esti­mation 
of the shape function, we represented the individual as a vector with length equal to the number of different 
materials. The .tness function used is a critical part of the system as it needs to be matching realistic 
behavior and, quick to calculate as well. The parametric methods such as the generic functions used in 
FEM are not computationally ef.cient [Nesme et al. 2009]. Thus, we pro­pose using a data-driven approach 
with collected data sets using a robot arm and force sensor. The .tness function is de.ned as the average 
Euclidean distance between the displacement vector values that an individual scores against a force vector 
with and the pre-computed data set of dis­placements by the robot arm. Thus, mathematically speaking 
for an individual represented as [f1,f2,...,fn] we de.ne its .tness as: "m(,"n(ui - ui )2/n) 11 ind target 
fit(ind)= (4) m Where m is the number of external applied forces, n is the length of chromosome vector, 
uind is the displacement vector generated from using fS which are the shape functions derivatives in 
Eqs. (1) and (2) and utarget is the displacement vector generated empiri­cally by the robot arm. For 
the 1D case the optimised model behavior is more realistic than the generic shape functions and no need 
to enforce constraints as in [Nesme et al. 2009]. References MITCHELL, M. 1998. An Introduction to Genetic 
Algorithms. MIT. R´ NESME, M., KRY, P. G., JE.ABKOV A´, L., AND FAURE, F. 2009. Preserving topology 
and elasticity for embedded de­formable models. In ACM Transactions on Graphics (Proc. of SIGGRAPH), 
ACM. to appear. Copyright is held by the author / owner(s). SIGGRAPH 2010, Los Angeles, California, July 
25 29, 2010. ISBN 978-1-4503-0210-4/10/0007  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1836965</article_id>
		<sort_key>1200</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>112</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>11</seq_no>
		<title><![CDATA[Rapid surface and volume mesh generation from depth-augmented visual hulls]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836845.1836965</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1836965</url>
		<abstract>
			<par><![CDATA[<p>Reconstructing scanned geometry is an important operation in geometry processing. Volumetric algorithms reconstruct the object volume by transforming range images into global coordinates and using scanline algorithms to build a scalar field that can be isocontoured to obtain the surface [Curless and Levoy 1996].</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061.10010063</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures->Computational geometry</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2264555</person_id>
				<author_profile_id><![CDATA[81466643516]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[James]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gregson]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of British Columbia]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264556</person_id>
				<author_profile_id><![CDATA[81547595956]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Zheng]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of British Columbia]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>237269</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Curless, B., and Levoy, M. 1996. A volumetric method for building complex models from range images. In <i>SIGGRAPH '96: Proceedings of the 23rd annual conference on Computer graphics and interactive techniques</i>, ACM, New York, NY, USA, 303--312.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1276448</ref_obj_id>
				<ref_obj_pid>1276377</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Labelle, F., and Shewchuk, J. R. 2007. Isosurface stuffing: fast tetrahedral meshes with good dihedral angles. <i>ACM Trans. Graph. 26</i>, 3, 57.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>37422</ref_obj_id>
				<ref_obj_pid>37402</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Lorensen, W. E., and Cline, H. E. 1987. Marching cubes: A high resolution 3d surface construction algorithm. <i>SIGGRAPH Comput. Graph. 21</i>, 4, 163--169.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>807402</ref_obj_id>
				<ref_obj_pid>800248</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Williams, L. 1978. Casting curved shadows on curved surfaces. In <i>In Computer Graphics (SIGGRAPH 78 Proceedings</i>, 270--274.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Rapid Surface and Volume Mesh Generation from Depth-Augmented Visual Hulls James Gregson * Zheng Wang 
University of British Columbia University of British Columbia  Figure 1: Left to right: Input scan data. 
Single-scan depth map. All-scans silhouette map. Marching cubes reconstruction using Depth-Augmented 
Visual Hull. 1 Introduction Reconstructing scanned geometry is an important operation in ge­ometry processing. 
Volumetric algorithms reconstruct the object volume by transforming range images into global coordinates 
and using scanline algorithms to build a scalar .eld that can be isocon­toured to obtain the surface 
[Curless and Levoy 1996]. We take a slightly different volumetric approach by generating depth-maps for 
each range image. These divide the scanned frus­tum into an inside and outside half-space. Inside/outside 
tests may then be performed on the depth maps in image space using shadow­mapping techniques [Williams 
1978]. To accommodate imperfec­tions in the depths maps we use additional silhouette maps to help resolve 
ambiguously classi.ed points. We call the combination Depth-Augmented Visual Hulls. 2 Our Approach We 
assume as input a set of range scans consisting of two­dimensional arrays of points with implicit connectivity 
(quadrilat­erals in our case) along with the per-scan transformations required to align the scans. We 
cull poor quadrilaterals in a preprocess­ing step based on ratios of edge lengths and quadrilateral areas. 
This helps remove quadrilaterals from areas that were scanned at a glancing angle, areas with specular 
re.ections and areas where self-occlusions occurred. We then build a set of depth maps one per scan of 
the scanned geometry by rendering it from the point of view of the scanner head. Points within the scanned 
frustum can then be classi.ed as inside or outside by transformation into the scanner coordinate frame 
and comparison of the point depth value with the stored depth value. If the depth-maps were a perfect 
representation of the underlying geometry, only points classi.ed as inside for all scans would be inside 
the object. Any meshing algorithm that can function with only a boolean predicate could then be used 
to generate a surface [Lorensen and Cline 1987] or volume [Labelle and Shewchuk 2007] mesh. Invariably 
the scans are not perfect. Furthermore the culling process removes portions of the depth map. This causes 
points projecting near the object silhouette to be mis-classi.ed as outside. To ac­commodate this we 
augment the depth-maps with silhouette maps. *e-mail: jgregson@cs.ubc.ca e-mail: zhwang@cs.ubc.ca Each 
scan is then capable of classifying a point as strictly inside, strictly outside, or ambiguously positioned. 
This last classi.cation is used for points within the silhouette, but with depth-map values beyond the 
object extents. The binary inside/outside predicate for all scans must then be modi.ed to only classify 
a point as inside the object if it is an inside point in at least one scan, and not an out­side point 
in any other. This essentially uses the depth information where available, and falls back to a visual 
hull for scans where data is incomplete. The resulting algorithm is straightforward. Every candidate 
point is classi.ed as inside or outside using the modi.ed boolean predi­cate. Cells whose corners are 
entirely inside may be written to disk, while those with a mixture of inside and outside vertices have 
their surface-crossing edges added to a list for further processing. The surface intersection for those 
edges are then determined by binary search using the binary predicate. The resulting edge-intersections 
can then be stored for use in marching cubes or isosurface stuf.ng to generate a water-tight mesh. Although 
our preliminary implementation is serial, the algorithm was designed to be implemented in parallel on 
the GPU. All opera­tions are GPU and cache-friendly. Even so, our unoptimized imple­mentation is fast; 
the 109,000 triangle reconstruction of the Bunny took 14 seconds on a mid-range laptop. Due to simple 
operation and coalesced memory access we expect a GPU version currently in development to be signi.cantly 
faster. References CURLESS, B., AND LEVOY, M. 1996. A volumetric method for building complex models 
from range images. In SIGGRAPH 96: Proceedings of the 23rd annual conference on Computer graphics and 
interactive techniques, ACM, New York, NY, USA, 303 312. LABELLE, F., AND SHEWCHUK, J. R. 2007. Isosurface 
stuf.ng: fast tetrahedral meshes with good dihedral angles. ACM Trans. Graph. 26, 3, 57. LORENSEN, W. 
E., AND CLINE, H. E. 1987. Marching cubes: A high resolution 3d surface construction algorithm. SIGGRAPH 
Comput. Graph. 21, 4, 163 169. WILLIAMS, L. 1978. Casting curved shadows on curved sur­faces. In In Computer 
Graphics (SIGGRAPH 78 Proceedings, 270 274. Copyright is held by the author / owner(s). SIGGRAPH 2010, 
Los Angeles, California, July 25 29, 2010. ISBN 978-1-4503-0210-4/10/0007  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1836966</article_id>
		<sort_key>1210</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>113</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>12</seq_no>
		<title><![CDATA[Using interactive evolution to discover camouflage patterns]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836845.1836966</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1836966</url>
		<abstract>
			<par><![CDATA[<p>This poster describes an abstract computation model of the evolution of camouflage in nature. Evolution is represented by <i>genetic programming.</i> Camouflage patterns are represented by <i>procedural texture synthesis.</i> A 2D <i>environment</i> is represented by a supplied photo. A <i>predator</i> is represented by a human's visual perception, interacting through a graphical user interface.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.4.8</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010225.10010227</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision tasks->Scene understanding</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2264557</person_id>
				<author_profile_id><![CDATA[81100469355]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Craig]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Reynolds]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Sony Computer Entertainment]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[
Reynolds, C. 2010. Interactive Evolution of Camouflage. To appear in the proceedings of the <i>12th International Conference on the Synthesis and Simulation of Living Systems</i> (ALife XII), August 2010. URL=http://www.red3d.com/cwr/iec/
]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Using Interactive Evolution to Discover Camouflage Patterns Craig Reynolds Sony Computer Entertainment, 
US R&#38;D  Figure 1: camouflaged circular prey overlaid on the background image for which they were 
evolved (a) tree bark, (b) twisty wire, (c) flowers and leaves, (d) serpentine, (e) lentils 1 Introduction 
This poster describes an abstract computation model of the evolu­tion of camouflage in nature. Evolution 
is represented by genetic programming. Camouflage patterns are represented by procedural texture synthesis. 
A 2D environment is represented by a supplied photo. A predator is represented by a human s visual perception, 
interacting through a graphical user interface. In the natural world, many predators hunt using vision. 
Prey that are harder to see have a survival advantage. Over time this can lead to the type of camouflage 
known as cryptic coloration. As prey become more cryptic, predator vision must improve to detect the 
prey. That coevolutionary system leads to well-camouflaged prey and sharp-eyed predators. This work is 
a first step toward simulating that natural system. It defers the difficulty of building a simulated 
predator by taking a hybrid approach, using a human in the loop to play the role of predator along with 
texture synthesis and evolutionary computa­tion. This poster will describe and illustrate the procedures 
used, show examples of evolved camouflage and discuss future work. 2 Experimental Procedure Natural 
morphogenesis is represented by a C++ library for texture synthesis (purely procedural, not example-based). 
This was con­nected to the genetic programming facility of Open BEAGLE, an open source library for evolutionary 
computation. This combina­tion could be used to evolve textures with a traditional objective fitness 
function. However in these experiments, the fitness meas­ure is subjective, based on human perception 
and judgement. In each round of the camouflage game a cohort of 10 camou­flaged prey are displayed to 
the user overlaid on a given back­ground image. The user searches for the most conspicuous prey and clicks 
to eat it, causing it to be removed from the display and from the evolutionary population. This is repeated 
until 5 prey remain which are allowed to survive. 5 new camouflaged prey are bred from parents in the 
surviving population. In this way the effectiveness of each prey s camouflage is judged relative to the 
others in its randomly selected cohort. Over time this process removes the most conspicuous prey from 
the population, allowing it to converge on and refine more cryptic textures. 3 DIscussion and Future 
Work Not all runs produce good results, although many do and some are strikingly effective. This interactive 
procedure is time consuming and mind numbing but served its purpose to prototype camouflage evolution 
and demonstrate its feasibility. In these runs the user makes about 5000 mouse clicks over several hours. 
Future work will include larger hybrid systems using distributed human com­putation, using crowd sourcing 
or games with a purpose. The eventual goal of this work is to close the loop and model the coevolutionary 
dynamics of camouflage and predator vision. The hope is to create an artificial predator using techniques 
from machine learning and machine vision. This would allow running much larger simulations. More importantly 
it would provide a complete computational model of the natural system that may be useful to biologists 
for experiments and teaching. Other applica­tions of evolutionary texture synthesis will also be investigated. 
 References REYNOLDS, C. 2010. Interactive Evolution of Camouflage. To appear in the proceedings of 
the 12th International Conference on the Synthesis and Simulation of Living Systems (ALife XII), August 
2010. URL=http://www.red3d.com/cwr/iec/  Figure 2: progression of camouflage patterns during a run with 
the granite environment Copyright is held by the author / owner(s). SIGGRAPH 2010, Los Angeles, California, 
July 25 29, 2010. ISBN 978-1-4503-0210-4/10/0007  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1836967</article_id>
		<sort_key>1220</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>114</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>13</seq_no>
		<title><![CDATA[Voronoi diagram computation for protein molecules using graphics hardware]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836845.1836967</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1836967</url>
		<abstract>
			<par><![CDATA[<p>We present an interactive algorithm to compute Voronoi diagrams for protein molecules. In the research area of biochemistry, a molecule is generally represented as a set of 3D spheres with various radii. In this paper, we propose a method to compute Voronoi diagrams for a set of spheres in the 3D discrete domain. We achieved interactive construction of Voronoi diagrams through our adaptive subdivision scheme and massively parallel processing supported by current graphics hardware.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>J.3</cat_node>
				<descriptor>Biology and genetics</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.1</cat_node>
				<descriptor>Graphics processors</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010444.10010087</concept_id>
				<concept_desc>CCS->Applied computing->Life and medical sciences->Computational biology</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010444.10010095</concept_id>
				<concept_desc>CCS->Applied computing->Life and medical sciences->Systems biology</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010389</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Graphics processors</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010444.10010935</concept_id>
				<concept_desc>CCS->Applied computing->Life and medical sciences->Genetics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2264558</person_id>
				<author_profile_id><![CDATA[81100660875]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Ku-Jin]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kim]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Kyungpook National University, Republic of Korea]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264559</person_id>
				<author_profile_id><![CDATA[81466644709]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Jung-Eun]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lee]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Kyungpook National University, Republic of Korea]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264560</person_id>
				<author_profile_id><![CDATA[81350582599]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Nakhoon]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Baek]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Kyungpook National University, Republic of Korea]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Voronoi Diagram Computation for Protein Molecules Using Graphics Hardware Ku-Jin Kim* Jung-Eun Lee 
Nakhoon Baek Kyungpook National University, RepublicofKorea  Figure 1: (a) snapshots from our CUDA-based 
implementations (b) a clipped plane view 1 Introduction We present an interactive algorithm to computeVoronoi 
diagrams for protein molecules. In the research area of biochemistry, a molecule is generally represented 
as a set of 3D spheres with vari­ous radii.Inthis paper,we proposea methodto computeVoronoi diagrams 
for a set of spheres in the 3D discrete domain. We achieved interactive construction ofVoronoi diagrams 
through our adaptivesubdivision scheme and massively parallel processing sup­ported by current graphics 
hardware. 2 GPU Based Voronoi Diagram Computing WegettheVoronoi diagramsforthegivensphere sites. Each 
sphere site si is described as its center point ci =(xi,yi,zi) and its radius ri. Let S = {si|1 = i = 
n} be a set of sphere sites in the 3D space. From a query point q =(x, y, z), the Euclidean distance 
to a sphere site si is calculated as dist(q,si)= dq - cid- ri. The Voronoi region of a sphere sitesp 
can be formed as : reg(sp)= {p|dist(p,sp) = dist(p,si), for all si . S, si = sp}. When dist(q,sp)= dist(q,sq), 
the point q belongs to both of reg(sp) and reg(sq), to .nally construct the boundary of those regions. 
We are focusing on the discrete domain. Each rectangular voxels in the 3D space is marked to belong to 
the speci.c region or to the boundary area,to .nally displaythe3DVoronoi diagram.We startfromthe distancebetweenthe 
centerpointqofthevoxeland the nearest sphere site sp. Though q . reg(sp), we also need to decide whether 
all the points in the voxel belong to reg(sp) or not. We present a region decision algorithm for each 
voxel as follows: Kernel Program for each voxel, with its center q and half of the diagonal length d 
step 1. calculate dist(q, si) from the voxel center point q, for all sphere sites si. step 2. get dmin 
= min(dist(q, si)) and its corresponding nearest sphere site smin. step 3. count the number n of the 
sphere sites whose dist(q, si) is less than dmin + 2d. step 4. if n = 1, the voxel belongs to reg(smin). 
otherwise, it may belong to the boundary of the Voronoid regions with respect to the corresponding sphere 
sites. *e-mail: kujinkim@yahoo.com e-mail:highshia@nate.com (corresponding author) e-mail:oceancru@gmail.com 
The above algorithm should be executed for all voxels, and thus its CPU-based sequential execution may 
require a considerable amountof time.We achievedits interactiveexecution withCUDA­based massively parallel 
implementation. Voxel-related data are stored into the CUDA texture memories and the above voxel­speci.c 
algorithmis realizedasaCUDAkernel programto .nally be executed as CUDA threads. Overall CUDA framework 
with adaptive re.nement can be represented as follows: procedure framework do step1. subdivide the space 
into 8 × 8 × 8 voxels, which is the current CUDAhardware limit. step 2. invoke kernel program for each 
voxel step 3. .nd any voxels belonged to boundary areas step 4. if any, process framework recursively 
for those voxels. until user-speci.ed accuracylimit is achieved. 3 Experimental Results AsshowninTable1,we 
achievedat least15 timesandatmost129 timesfaster performancein comparisonwith CPUbased sequential implementations. 
Our CUDA-based implementation can be used in an interactive manner, while CPU-based ones not. subdivision 
execution time (msec) Speed ups (times) CPU-based CUDA-based 32 × 32 × 32 750 48 15.63 64 × 64 × 64 6,016 
69 87.19 128 × 128 × 128 47,750 404 118.19 256 × 256 × 256 381,625 2,939 129.85 CPU: Intel Core2 Duo 
E8400, 3GHz CPU with 3GB RAM GPU: nVIDIA GeForce GTX 285 with 1GBVideo RAM with 1468 sphere sites from 
a protein molecule description .le. Table 1: Execution times for our implementations  4 Conclusions 
In this paper, we presented an interactive-time algorithm to com­puteVoronoi diagrams for protein molecules. 
With respect to the user-speci.ed accuracylimits, our algorithm shows at least 15 times to at most 129 
times better performance in comparison to the single­core CPU-based implementations. Basedon ourwork,weexpectto 
develop interactive-time algorithms to compute the volume of the protein and molecular surfaces as further 
researches. Copyright is held by the author / owner(s). SIGGRAPH 2010, Los Angeles, California, July 
25 29, 2010. ISBN 978-1-4503-0210-4/10/0007  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	</section>
	<section>
		<section_id>1836968</section_id>
		<sort_key>1230</sort_key>
		<section_seq_no>9</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Rendering]]></section_title>
		<section_page_from>115</section_page_from>
	<article_rec>
		<article_id>1836969</article_id>
		<sort_key>1240</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>115</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[A full HDR pipeline from acquisition to projection]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836845.1836969</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1836969</url>
		<abstract>
			<par><![CDATA[<p>In the real world, the ratio between full brightness of the sun and complete darkness is in the range of 2.000.000.000:1. However today's projection display technology is limited to contrast ratios of approximately 10.000:1. This hinders a convincing simulation and presentation of lighting effects in professional markets such as car styling, architecture and industrial design. At the same time, High Dynamic Range Imaging (HDRI) has been developed as a new field of research resulting in breakthroughs in image based lighting. What is missing today are interactive visualisation systems that fully support HDR material and light information from the acquisition stage right through the processing stage to the display stage. Current software systems do exist to simulate the effect of light sources in virtual scenes. However, they require specialist training, they are complex to use, they cannot operate in real-time, often requiring modification and recalibration. Current systems also do not support HDRI. This means that not only do they lack the ability to simulate real lighting conditions, e.g. the position and intensity of the sun, cloudcover, but also the behaviour of materials in various light conditions.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.4.1</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2264561</person_id>
				<author_profile_id><![CDATA[81405593761]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Pedro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Santos]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Fraunhofer-IGD]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264562</person_id>
				<author_profile_id><![CDATA[81421599523]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Thomas]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gierlinger]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Fraunhofer-IGD]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264563</person_id>
				<author_profile_id><![CDATA[81314489502]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Rafael]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Huff]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Fraunhofer-IGD]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264564</person_id>
				<author_profile_id><![CDATA[81464653620]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Martin]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ritz]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Fraunhofer-IGD]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264565</person_id>
				<author_profile_id><![CDATA[81100152008]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Andr&#233;]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Stork]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[TU-Darmstadt]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1666812</ref_obj_id>
				<ref_obj_pid>1666778</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[
Santos, P., Schmedt, H., Hohmann, S., and Stork, A. 2009. The hybrid outdoor tracking extension for the daylight blocker display. In <i>SIGGRAPH ASIA '09: ACM SIGGRAPH ASIA 2009 Posters</i>, ACM, New York, NY, USA, 1--1.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[
Vandenberghe, P. Maximus, B. 2008. Challenges and technologies for multi-channel projection systems. <i>SID Int. Symp. Digest Tech</i> 1, 167--170.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 A full HDR pipeline from acquisition to projection Pedro Santos* Thomas Gierlinger Rafael Huff Martin 
Ritz§ e Stork¶ Andr´Fraunhofer-IGD Fraunhofer-IGD Fraunhofer-IGD Fraunhofer-IGD TU-Darmstadt 1 Introduction 
In the real world, the ratio between full brightness of the sun and complete darkness is in the range 
of 2.000.000.000:1. However today s projection display technology is limited to contrast ratios of approximately 
10.000:1. This hinders a convincing simulation and presentation of lighting effects in professional markets 
such as car styling, architecture and industrial design. At the same time, High Dynamic Range Imaging 
(HDRI) has been developed as a new .eld of research resulting in breakthroughs in image based lighting. 
What is missing today are interactive visualisation systems that fully support HDR material and light 
information from the acquisition stage right through the processing stage to the display stage. Current 
software systems do exist to simulate the effect of light sources in virtual scenes. However, they require 
specialist training, they are complex to use, they cannot operate in real-time, often requiring modi.cation 
and recalibration. Current systems also do not support HDRI. This means that not only do they lack the 
ability to simulate real lighting conditions, e.g. the position and intensity of the sun, cloudcover, 
but also the behaviour of materials in various light conditions. Figure 1: Hybrid PRT+Raytracing, Spheron 
HDR acquisition, Barco HDR projector In this publication we present one of the .rst full HDR visual­ization 
systems (see Figure 1) starting with HDR material and light acquisition, providing a HDR light simulation 
and rendering pipeline and .nally displaying maximum .delity image quality with color gamut enhanced 
HDR projection technology to bring the total dynamic range to over 5.000.000:1. We demonstrate these 
capabilities in the .elds of car design and architecture. Figure 2: PRT only; Raytracing only; Hybrid 
PRT+Raytracing *e-mail: pedro.santos@igd.fraunhofer.de e-mail:thomas.gierlinger@igd.fraunhofer.de e-mail:rafael.huff@igd.fraunhofer.de 
§e-mail:martin.ritz@igd.fraunhofer.de ¶e-mail:andre.stork@igd.fraunhofer.de  2 Exposition The aim in 
HDR acquisition is to further develop a HDR sensor that is used to measure BRDFs (material properties) 
and light .elds (environment lights / light probes) that are used for physically based image synthesis. 
A HDR sensor and its dynamic range (DR) capa­bilities have been re-evaluated in the context of an experimental 
HDR goniometer and a full spherical HDR camera (in full produc­tion) in order to identify all opportunities 
to improve on the DR­performance of the two devices. First measurements of BRDFs and light .elds have 
been performed and an interchange .leformat for the BRDF data has been de.ned. The rendering work performed 
is concerned with physically based image synthesis utilizing measured material and light data. The goal 
is to generate realistic images for design review with a quality close to current off-line rendering 
systems but at higher speeds. To achieve this we developed the concept of Hybrid Rendering to fuse diffuse 
global illumination results generated using Precomputed Radiance Transfer (PRT) and specular effects 
generated by Ray Tracing (RT). Support for dynamic objects and local light sources is achieved by further 
development of the Precomputed Shadow Fields algorithm. Initial CPU-based implementations of all relevant 
algorithms are available. The concept of Hybrid Rendering using PRT+RT has been veri.ed (see Figure 2). 
The method to produce 16-bit output for the HDR projector has been validated. The HDR projection technologies 
are concerned with increasing the dynamic range, accuracy and color gamut of existing projec­tors. The 
expected increase in dynamic range to 5.000.000:1 using a combination of dimming systems has been validated 
[Vanden­berghe 2008]. To enable communication of HDR image data from the image generator (rendering PC) 
to the projector, 16 bit gamma encoded input and the 23 bit equivalent processing path have been integrated 
into a projector. The (relative) accuracy of the proposed external spectrometer based color maintenance 
system was veri.ed. For white point a maintenance accuracy of better than 0.005 in CIE 1931 x,y can be 
expected. In addition a 30% color gamut expansion over EBU is achievable on a projector at the expense 
of max. 15% of the light output. 3 Conclusion This publication extends previously published work [Santos 
et al. 2009] by implementing one of the .rst full HDR pipelines from acquisition to projection for photo-realistic 
rendering. This work was funded in part by European research grant MAXIMUS FP7-ICT-1-217039. References 
SANTOS, P., SCHMEDT, H., HOHMANN, S., AND STORK, A. 2009. The hybrid outdoor tracking extension for the 
daylight blocker display. In SIGGRAPH ASIA 09: ACM SIGGRAPH ASIA 2009 Posters, ACM, New York, NY, USA, 
1 1. VANDENBERGHE, P. MAXIMUS, B. 2008. Challenges and tech­nologies for multi-channel projection systems. 
SID Int. Symp. Digest Tech 1, 167 170. Copyright is held by the author / owner(s). SIGGRAPH 2010, Los 
Angeles, California, July 25 29, 2010. ISBN 978-1-4503-0210-4/10/0007  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<article_sponsors>
			<funding_agency>European research grant</funding_agency>
			<grant_numbers>
				<grant_number>MAXIMUS FP7-ICT-1-217039</grant_number>
			</grant_numbers>
		</article_sponsors>
	</article_rec>
	<article_rec>
		<article_id>1836970</article_id>
		<sort_key>1250</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>116</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[A physical rendering model for human teeth]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836845.1836970</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1836970</url>
		<abstract>
			<par><![CDATA[<p>This paper introduces a layered model for rendering human teeth, to be used in photorealistic rendering of humans for games and animations. While the lighting responses of teeth have been studied in the dental industry ([Joiner 2004], [Brodbelt et al 1981], [Zijp and ten Bosch 1993]) for the production of realistic-looking dentures, to our knowledge this is the first study of its type in computer graphics for the production of realistic renderings. Human teeth exhibit translucency and are characterized by complex light interaction. From a rendering perspective, we make use of a bank of sliders (Figure 2) to vary optical properties at runtime and achieve desired rendering effects, including aging effects as shown in Figure 1. We also make use of hand drawn distribution maps for different layers rather than texture maps to achieve diffuse coloring.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.6.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010341</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2264566</person_id>
				<author_profile_id><![CDATA[81466642600]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Sudarshanram]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Shetty]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Oregon State University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264567</person_id>
				<author_profile_id><![CDATA[81324487523]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Mike]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bailey]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Oregon State University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Brodbelt et al 1981 R. H. W. Brodbelt, W. J. O'Brien, P. L. Fan, J. G. Frazer-Dib and R. Yu, "Translucency of Human Dental Enamel", Journal of Dental Research (1981).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Dreon et al 2007 Eugene Dreon, David Luebke, and Eric Enderton, "Efficient Rendering of Human Skin", <i>Eurographics Symposium on Rendering</i> (2007)]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Joiner 2004 Andrew Joiner, "Tooth colour: a review of the literature", <i>Journal of Dentistry</i>, (2004) Unilever Oral Care, UK]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Zijp and ten Bosch 1993 J. R. Zijp and J. J. ten Bosch, "Theoretical model for the scattering of light by dentin and comparison with measurements", <i>Applied Optics</i>, Vol. 32, 411--415, (1993)]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 A Physical Rendering Model for Human Teeth Sudarshanram Shetty Mike Bailey Oregon State University 
(a) (b) (c) (d) Figures 1: (a)-(d) show various shades exhibited by a central incisor for different optical 
properties. In younger people, teeth tend to be whiter (a): High scattering, Low absorption. With age, 
teeth tend to be yellowish due to wear and tear of enamel, exposing more dentin, and thus reducing scattering 
(d): Low scattering, High Absorption. Our model simulates these conditions. 1 Abstract This paper introduces 
a layered model for rendering human teeth, to be used in photorealistic rendering of humans for games 
and animations. While the lighting responses of teeth have been studied in the dental industry ([Joiner 
2004], [Brodbelt et al 1981], [Zijp and ten Bosch 1993]) for the production of realistic­looking dentures, 
to our knowledge this is the first study of its type in computer graphics for the production of realistic 
renderings. Human teeth exhibit translucency and are characterized by complex light interaction. From 
a rendering perspective, we make use of a bank of sliders (Figure 2) to vary optical properties at runtime 
and achieve desired rendering effects, including aging effects as shown in Figure 1. We also make use 
of hand drawn distribution maps for different layers rather than texture maps to achieve diffuse coloring. 
 2 Description Tooth enamel is predominantly made of a hydroxyapatite-like crystalline material in addition 
to other organic materials. Dentin is composed of tubules which extend from the dentin enamel junction 
(DEJ) to the pulp. Dentin concentration is less in DEJ also density of enamel decreases from the surface 
as we move inwards. Our model is composed of two layers: The outer enamel (with a thin film of saliva 
on top) and inner dentin. We neglect the pulp layer and consider only crown section for our simulation. 
We use a multipole diffusion model to approximate diffuse color contribution and a Kelemen/Szirmay- Kalos 
model to obtain specular contribution for a fixed roughness and specular intensity factors. We make use 
of a distribution map which can be characterized as a grey scale variation of density of layers. Diffuse 
color is computed by convolution of reflectance profiles with the distribution maps along with the total 
irradiance available for the diffusion process. Sliders (Figure 2) are used to vary optical properties 
of enamel and dentin such as scattering, absorption and asymmetry factors over red, green and blue wavelengths. 
We then combined this with sub-surface scattering (such has been recently used in skin rendering [Dreon 
et al 2007]) and Rayleigh Scattering (which accounts for a slight blue-ish hue on some teeth). Figure 
2: User interface to vary optical properties  3 References BRODBELT ET AL 1981 R.H.W. Brodbelt, W.J. 
O'Brien, P.L. Fan, J.G. Frazer-Dib and R. Yu, Translucency of Human Dental Enamel , Journal of Dental 
Research (1981). DREON ET AL 2007 Eugene Dreon, David Luebke, and Eric Enderton, Efficient Rendering 
of Human Skin , Eurographics Symposium on Rendering (2007) JOINER 2004 Andrew Joiner, Tooth colour: a 
review of the literature , Journal of Dentistry, (2004) Unilever Oral Care, UK ZIJP AND TEN BOSCH 1993 
J.R. Zijp and J.J. ten Bosch, Theoretical model for the scattering of light by dentin and comparison 
with measurements , Applied Optics, Vol. 32, 411­415, (1993) Copyright is held by the author / owner(s). 
SIGGRAPH 2010, Los Angeles, California, July 25 29, 2010. ISBN 978-1-4503-0210-4/10/0007 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1836971</article_id>
		<sort_key>1260</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>117</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Approximate ambient occlusion for dynamic scenes using the GPU]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836845.1836971</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1836971</url>
		<abstract>
			<par><![CDATA[<p>Ambient occlusion has been tackled in many different ways to inculcate realism into renderings. Ambient occlusion is a crude approximation to global illumination. But performing a full global illumination in real-time has turned out to be computationally expensive. Combined with local rendering models, ambient occlusion can produce renderings which have increased realism.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.1</cat_node>
				<descriptor>Graphics processors</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010389</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Graphics processors</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2264568</person_id>
				<author_profile_id><![CDATA[81466642099]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Shailen]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Agrawal]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[UBC]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264569</person_id>
				<author_profile_id><![CDATA[81482642305]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Subodh]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kumar]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[IIT Delhi]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1409079</ref_obj_id>
				<ref_obj_pid>1457515</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Zhou, K., Hou, Q., Wang, R., and Guo, B. 2008. Real-time kd-tree construction on graphics hardware. In <i>SIGGRAPH Asia '08: ACM SIGGRAPH Asia 2008 papers</i>, ACM, 1--11.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Approximate Ambient Occlusion for Dynamic Scenes using the GPU Shailen Agrawal* Subodh Kumar UBC IIT 
Delhi  Figure 1: (a) Nearest Neighbor query for a single point on hand (b) All neighboring triangles 
are returned, a part of which is the tip of the thumb. All returned neighbors cause occlusion on the 
query point. Returned triangles are projected onto the view plane at the occlusion vertex (c) Use a ratio 
of green and red areas to determine occlusion caused at the query point (d), (e) Some of the occlusion 
values can be reused between two frames in a sequence if their neighborhood remains the same. For example 
the local neighborhood of the shown query points in blue doesn t change due to the head turn, so we must 
avoid reprojecting all the neighbors and recomputing occlusion for such. 1 Introduction Ambient occlusion 
has been tackled in many different ways to in­culcate realism into renderings. Ambient occlusion is a 
crude ap­proximation to global illumination. But performing a full global illumination in real-time has 
turned out to be computationally ex­pensive. Combined with local rendering models, ambient occlusion 
can produce renderings which have increased realism. In this work an improvement over existing occlusion 
computation techniques is proposed for use in rendering shadows for dynamic scenes. Using modern GPU 
hardware, ambient occlusion in dy­namic scenes can be rendered in real-time. Clever updation and pre-processing 
techniques are required to accomplish this feat. The main contributions of this technique are towards 
the development of an approximate occlusion computation via projections onto the view plane at the query 
vertex and a scheme which utilizes coher­ence in both spatial and temporal domain to reuse already computed 
occlusion values.  2 Our Approach Occlusion is computed at each query point in the scene by deter­mining 
the local neighborhood of the query point. The local neigh­borhood search is accelerated using a kd-tree 
data structure which is built on the GPU in real time using the technique mentioned in [Zhou et al. 2008]. 
Since the scene is dynamic and we want to maintain the kd-tree structure at each frame, it is essential 
that we can construct the kd-tree quickly. Once all the triangles in the local neighborhood are determined 
for a query point, we project them onto the query point and determine the occlusion. Using an approach 
like this, an occlusion computa­tion will have to be performed for all query points for every frame. 
We develop a scheme which utilizes spatial and temporal coherence for reusing already computed occlusion 
values. This results in re­duction of large number of new occlusion computations for query points which 
can take advantage of coherence in space or time. The query points can be per-vertex in which case the 
occlusion values are passed onto a vertex shader for modulating local lighting. *e-mail: shailen@cs.ubc.ca 
e-mail: subodh@cse.iitd.ac.in Firstly we can use coherence in space. If the local neighborhood for two 
query points close-by is similar then the same occlusion value can be used for them instead of recomputing 
it for every such query point. A neighborhood matching scheme is being developed where fast neighborhood 
matches can be performed on the GPU. This uti­lizes coherence in the recipient side. Only the local neighborhood 
is used, so the model could change at a distance without changing the occlusion at the query point. While 
projecting details we use a simpli.ed version of the scene which utilizes coherence in occluder side. 
Secondly we develop a scheme for utilizing coherence in time. As shown in Figure 1 (d) and (e), there 
are points for which the lo­cal neighborhood remains same across two frames in time. Hence for points 
like these we can reuse the occlusion values instead of recomputing it for each frame.  Figure 2: Current 
state of rendering from the system References ZHOU, K., HOU, Q., WANG, R., AND GUO, B. 2008. Real-time 
kd-tree construction on graphics hardware. In SIGGRAPH Asia 08: ACM SIGGRAPH Asia 2008 papers, ACM, 1 
11. Copyright is held by the author / owner(s). SIGGRAPH 2010, Los Angeles, California, July 25 29, 
2010. ISBN 978-1-4503-0210-4/10/0007  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1836972</article_id>
		<sort_key>1270</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>118</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[Artistic sketching with a painterly rendering algorithm]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836845.1836972</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1836972</url>
		<abstract>
			<par><![CDATA[<p>Hand-drawn sketches often depict geometry, colour and texture using loose and roughly drawn lines. Many automated sketching algorithms focus on accurately depicting salient details using pen-and-ink drawings. The approach of Hertzmann et. al. [2000] sketches the contours and silhouettes of 3D meshes, while the interactive algorithm of Kalnins et. al. [2002] renders decorative lines with artistic brushes and suggestions. Other 2D algorithms render Sobel and Canny edges with artistic brushes [Orzan et al. 2007].</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[sketching]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.4</cat_node>
				<descriptor>Paint systems</descriptor>
				<type>S</type>
			</primary_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2264570</person_id>
				<author_profile_id><![CDATA[81466647327]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Lesley]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Northam]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Waterloo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264571</person_id>
				<author_profile_id><![CDATA[81466647282]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Joe]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Istead]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Waterloo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264572</person_id>
				<author_profile_id><![CDATA[81100155157]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Craig]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kaplan]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Waterloo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>345074</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Hertzmann, A., and Zorin, D. 2000. Illustrating smooth surfaces. In <i>proceedings of SIGGRAPH 2000</i>, ACM Press/Addison-Wesley Publishing Co., New York, NY, USA, 517--526.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>280951</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Hertzmann, A. 1998. Painterly rendering with curved brush strokes of multiple sizes. In <i>proceedings of SIGGRAPH 1998</i>, ACM, New York, NY, USA, 453--460.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>566648</ref_obj_id>
				<ref_obj_pid>566570</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Kalnins, R. D., Markosian, L., Meier, B. J., Kowalski, M. A., Lee, J. C., Davidson, P. L., Webb, M., Hughes, J. F., and Finkelstein, A. 2002. WYSIWYG NPR: drawing strokes directly on 3d models. In <i>proceedings of SIGGRAPH 2002</i>, ACM, New York, NY, USA, 755--762.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1274888</ref_obj_id>
				<ref_obj_pid>1274871</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Orzan, A., Bousseau, A., Barla, P., and Thollot, J. 2007. Structure-preserving manipulation of photographs. In <i>proceedings of NPAR 2007</i>, ACM, New York, NY, USA, 103--110.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Artistic Sketching with a Painterly Rendering Algorithm LesleyNortham* ,JoeIstead, andCraigKaplan University 
ofWaterloo Figure 1: Painting of a rose (left), and its corresponding sketch (right). CR Categories: 
I.3.4[Computer Graphics]: Graphics Utilities PaintSystems Keywords: sketching 1 Introduction Hand-drawnsketchesoftendepictgeometry,colourand 
textureus­inglooseand roughlydrawn lines.Many automated sketching algo­rithms focus on accurately depicting 
salient details using pen-and­inkdrawings. Theapproach ofHertzmannet. al.[2000] sketches the contours 
and silhouettes of 3D meshes, while the interactive algorithm ofKalnins et. al.[2002] rendersdecorative 
lines with artisticbrushes and suggestions. Other2D algorithms renderSobel andCanny edges with artisticbrushes[Orzan 
et al.2007]. Sketchesproducedby theseapproachesoftenappearrigidbecause theypreciselydepictshapeat theexpenseof 
artisticstylization.In this article we present a sketching algorithm that creates relaxed, looseand.owing 
sketchesfrom2Dimagesby selectivelyrendering portions of the vector .eld. Our method reproduces contours, 
sil­houettesandtexturesfrom thesource imagewhilefavouringartistic renditionand stylistic .exibilityoveraccuratedepiction. 
 2 Approach To create a sketch, we use Hertzmann s painterly rendering algo­rithm togeneratelayersofbrush 
strokesthatfollowthevector.eld of theGaussian-blurred input image.We thendrawone-pixel wide curves along 
each stroke. This layer-based approach progressively re.nes salient regions of the input image, emphasizing 
contours, silhouettes and textures in * e-mail: lanortha@uwaterloo.ca the output sketch. The re.ning 
process concentrates sketch lines along edges,producing anoverdrawnappearancecommon tomany sketching 
styles. Hertzmann s painterly rendering algorithm endows our sketching method with stylistic control 
and .exibility in several ways. Alter­ations to thevector .eldandpainterlyrenderingparameters(e.g. strokelength,brush 
width, tolerance)providecontrol overthe.ow and placement of sketch lines. Filtering out certain brush 
stroke layers in the painting emphasizes artistic properties in the sketch. Also, rendering the sketch 
lines with artistic brushes simulates a varietyof media(e.g. colouredpencils,charcoal,crayon). References 
HERTZMANN, A., AND ZORIN, D. 2000. Illustrating smooth sur­faces.Inproceedings of SIGGRAPH 2000,ACMPress/Addison­WesleyPublishingCo.,NewYork,NY,USA,517 
526. HERTZMANN, A. 1998. Painterly rendering with curved brush strokes of multiple sizes. In proceedings 
of SIGGRAPH 1998, ACM,NewYork,NY,USA,453 460. KALNINS, R. D., MARKOSIAN, L., MEIER, B. J., KOWALSKI, 
M. A., LEE, J. C., DAVIDSON, P. L., WEBB, M., HUGHES, J. F., AND FINKELSTEIN, A. 2002. WYSIWYGNPR:drawing 
strokes directly on 3d models. In proceedings of SIGGRAPH 2002,ACM,NewYork,NY,USA,755 762. ORZAN, A., 
BOUSSEAU, A., BARLA, P., AND THOLLOT,J. 2007. Structure-preserving manipulation of photographs. In proceed­ings 
of NPAR 2007,ACM,NewYork,NY,USA,103 110. Copyright is held by the author / owner(s). SIGGRAPH 2010, Los 
Angeles, California, July 25 29, 2010. ISBN 978-1-4503-0210-4/10/0007  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1836973</article_id>
		<sort_key>1280</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>119</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>5</seq_no>
		<title><![CDATA[Combining spherical harmonics and point-source illumination for efficient image-based relighting]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836845.1836973</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1836973</url>
		<abstract>
			<par><![CDATA[<p>Traditional image-based relighting technique requires capturing a dense set of lighting directions surrounding the object and uses the linearity of light transport property together with the illumination data of the target environment to relight an object [Debevec et al. 2000]. However, this can be a very data intensive process because such datasets typically involve photographing hundreds of lighting directions. It is also difficult to modify or edit the data in post-production environments because the data is high dimensional. Adjustment has to be made in several dimensions in order to add artistic effects to the result. Difficulty in acquisition process is also one of the main problems. The capturing process typically lasts long enough to only be suitable for static objects. In this poster, we present a relighting technique which greatly reduces the number of images required for relighting, and still generate realistic results. We combine spherical harmonics with point lights to achieve efficient image based relighting. Spherical harmonics can efficiently capture smooth low frequency illumination [Ramamoorthi and Hanrahan 2001] while point lights capture high frequency directional illumination. Combining both techniques, we create relighting results which have both low and high frequency illumination data. This technique also benefits the acquisition process by reducing the number of required photographs which results in shorter capture time. In addition, fewer dimensions of the data can potentially simplify modification or editing of reflectance data.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2264573</person_id>
				<author_profile_id><![CDATA[81442617187]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Borom]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tunwattanapong]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Southern California]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264574</person_id>
				<author_profile_id><![CDATA[81385599077]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Abhijeet]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ghosh]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[USC Institute for Creative Technologies]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264575</person_id>
				<author_profile_id><![CDATA[81100086933]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Paul]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Debevec]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[USC Institute for Creative Technologies]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>344855</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Debevec, P., Hawkins, T., Tchou, C., Duiker, H.-P., Sarokin, W., and Sagar, M. 2000. Acquiring the reflectance field of a human face. In <i>SIGGRAPH '00: Proceedings of the 27th annual conference on Computer graphics and interactive techniques</i>, ACM Press/Addison-Wesley Publishing Co., New York, NY, USA, 145--156.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383317</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Ramamoorthi, R., and Hanrahan, P. 2001. An efficient representation for irradiance environment maps. In <i>SIGGRAPH '01: Proceedings of the 28th annual conference on Computer graphics and interactive techniques</i>, ACM, New York, NY, USA, 497--500.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Combining Spherical Harmonics and Point-Source Illumination for Ef.cient Image-Based Relighting Borom 
Tunwattanapong Abhijeet Ghosh Paul Debevec University of Southern California USC Institute for Creative 
Technologies (a) Ground Truth (b) Combination with un-optimized weights (c) Combination with optimized 
weights Figure 1: (a): The relighting image under the Grace cathedral illumination using the full re.ectance 
function data consisting of 253 lighting conditions. (b): An approximation of the ground truth using 
a combination of spherical harmonics and point lights with un-optimized weights. (c): An approximation 
of the ground truth using a combination of spherical harmonics and point lights with optimized weights. 
1 Overview Traditional image-based relighting technique requires capturing a dense set of lighting directions 
surrounding the object and uses the linearity of light transport property together with the illumina­tion 
data of the target environment to relight an object [Debevec et al. 2000]. However, this can be a very 
data intensive process because such datasets typically involve photographing hundreds of lighting directions. 
It is also dif.cult to modify or edit the data in post-production environments because the data is high 
dimen­sional. Adjustment has to be made in several dimensions in order to add artistic effects to the 
result. Dif.culty in acquisition pro­cess is also one of the main problems. The capturing process typi­cally 
lasts long enough to only be suitable for static objects. In this poster, we present a relighting technique 
which greatly reduces the number of images required for relighting, and still generate real­istic results. 
We combine spherical harmonics with point lights to achieve ef.cient image based relighting. Spherical 
harmonics can ef.ciently capture smooth low frequency illumination [Ramamoor­thi and Hanrahan 2001] while 
point lights capture high frequency directional illumination. Combining both techniques, we create re­lighting 
results which have both low and high frequency illumi­nation data. This technique also bene.ts the acquisition 
process by reducing the number of required photographs which results in shorter capture time. In addition, 
fewer dimensions of the data can potentially simplify modi.cation or editing of re.ectance data. 2 Method 
A lighting system such as Light Stage can project spherical har­monic illumination on the object. This 
means that we can directly capture spherical harmonics coef.cients from photographing the object under 
these lighting conditions. For point-source lights, we capture the object while illuminating it with 
just one light at a time to simulate directional illumination. We capture point-source light­ing conditions 
by giving more samples to the upper hemisphere which is usually brighter compared to the lower hemisphere. 
The number of photographs in the capture process has been limited to 20 photographs which is one order 
of magnitude less than the dense data set. In order to reconstruct high frequency data, we gener­ate 
a residual environment map by subtracting spherical harmonic reconstruction of the original environment 
maps from the original maps. The residual map normally contains negative pixels which we clamp to zero 
in order to prevent the possibility of reducing energy from point-source illumination. We then convert 
the result to angular Voronoi cells of point-source light in order to compute the energy of each light. 
The residual maps usually contain data in brighter areas, which are mostly high frequency data, and data 
in some areas that have been under estimated by spherical harmonic reconstruction. We also experiment 
with different combinations of spherical harmonic and point lights to .nd the most suitable setup for 
the 20 photographs budget that we have (see Supplementary Document). The best setup is the 2nd order 
spherical harmonic (9 images) and 11 point-source illumination which captures both high and low frequency 
illumination (see Supplementary Document). 3 Optimizing Combination Weights Direct combination of spherical 
harmonics and point lights achives sub optimal relighting (Fig. 1, b). Hence, we present an approach 
to .nd the optimal combination weights of spherical harmonics and point lights. These weights present 
the contribution of each data to the .nal result. We optimize the combination of weights by reduc­ing 
the difference between a reconstructed map of the original envi­ronment map and the original one. In 
order to obtain the difference, we project the original environment map into PCA (principal com­ponent 
analysis) space of spherical harmonic and point-source data to obtain a feature vector of the map in 
PCA space. Then we cal­culate the distance of that feature vector with the feature vector of the current 
weights. We use a bounded version of FMINSEARCH function in MATLAB to .nd the optimal weights that reduce 
the distance between two feature vectors. Subsequently, we apply the optimized weights to the spherical 
harmonic and point-source data to generate the .nal result (see Fig. 1, c). References DEBEVEC, P., 
HAWKINS, T., TCHOU, C., DUIKER, H.-P., SAROKIN, W., AND SAGAR, M. 2000. Acquiring the re.ectance .eld 
of a human face. In SIGGRAPH 00: Proceedings of the 27th annual conference on Computer graphics and interactive 
techniques, ACM Press/Addison- Wesley Publishing Co., New York, NY, USA, 145 156. RAMAMOORTHI, R., AND 
HANRAHAN, P. 2001. An ef.cient represen­ tation for irradiance environment maps. In SIGGRAPH 01: Proceed­ 
ings of the 28th annual conference on Computer graphics and interactive techniques, ACM, New York, NY, 
USA, 497 500. Copyright is held by the author / owner(s). SIGGRAPH 2010, Los Angeles, California, July 
25 29, 2010. ISBN 978-1-4503-0210-4/10/0007  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1836974</article_id>
		<sort_key>1290</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>120</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>6</seq_no>
		<title><![CDATA[Computational holography]]></title>
		<subtitle><![CDATA[the real 3-D by fast wave-field rendering in ultra high resolution]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836845.1836974</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1836974</url>
		<abstract>
			<par><![CDATA[<p>James Cameron's <i>Avatar</i> pioneered 3-D films in practical meaning. All audiences of the movie were happy even though their eye points were fixed when they were watching the movie. However, there are certain area that fixed eye points are not acceptable. This is why the authors focus on <i>Computational Holography</i> (see Fig. 1).</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2264576</person_id>
				<author_profile_id><![CDATA[81466647536]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Kyoji]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Matsusima]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Kansai University, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264577</person_id>
				<author_profile_id><![CDATA[81466640461]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Masaki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nakamura]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Kansai University, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264578</person_id>
				<author_profile_id><![CDATA[81466641461]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Sumio]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nakahara]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Kansai University, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264579</person_id>
				<author_profile_id><![CDATA[81442593153]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Ichiroh]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kanaya]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Osaka University, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[
K. Matsushima, S. Nakahara: <i>Extremely high-definition full-parallax computer-generated hologram created by the polygon-based method</i>; Applied Optics, Vol. 48, No. 34, pp. 54--63, 2009.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Computational Holography: The real 3-D by fast wave-.eld rendering in ultra high resolution Kyoji Matsusima*, 
Masaki Nakamura*, Sumio Nakahara*, and Ichiroh Kanaya***Kansai University, Japan; **Osaka University, 
Japan  1. Introduction James Cameron s Avatar pioneered 3-D .lms in practical meaning. All audiences 
of the movie were happy even though their eye points were .xed when they were watching the movie. However, 
there are certain area that .xed eye points are not acceptable. This is why the authors focus on Computational 
Holography (see Fig. 1).Holography is a technique that allows thelight scattered from an object to be 
recorded and later reconstructed so that it appears as if the object is in the same position relative 
to the recording medium as it was when recorded [Wikipedia]. One of advantages of holography is that 
it provides full 3-D view, meaning that viewers can see different views of the object from the corresponding 
view angles.The computational holography (computer­generated holography) has long history and is often 
called an ultimate 3-D rendering technology since it produces not only sensation of depth but also lightwave 
from the rendered objectsthemselves. However, we must face dif.culty of the computational holography. 
Synthesizing holography has been quite dif.cult due to its heavy load of computation. To illustrate, 
a stamp-sized synthesized holography requires 4 billion pixels (216 x 216 pixels). There have not been 
any practical technique in computing such ultra high resolution light .eld so far.The authors propose 
a polygon/silhouette­based computational holography that overcomes a wall of that computational complexity 
[1]. This method numerically generates a lightwave of surfaces of arbitrary objects, whose shape is given 
in a set of vertex data of polygonal facets. Occluded areas of the scene are correctly removed bycomputing 
silhouette of the objects. The authors also propose a segmented frame buffer that can handle very large 
wave-.eld that cannot .t in a computer s memory of today. Copyright is held by the author / owner(s). 
SIGGRAPH 2010, Los Angeles, California, July 25 29, 2010. ISBN 978-1-4503-0210-4/10/0007  2. Fast Wave-.eld 
Rendering The authors propose polygon/silhouette-based wave­.eld rendering technique for accurate and 
fast rendering of lightwave. This technique computes wave-.eld propagation from small facets and then 
integrates the all contribution from the facets.The wave-.eld doesn t obey Kajiya srendering equation. 
Instead of conventional ray­tracing, complex amplitude is computed. For reducing complexity of the integral 
and hiding back-faced/occluded areas, the silhouette of the object is .rstly computed and used for culling 
unnecessary .elds.To save working memory of the integral,frame buffer is segmented in reasonable size. 
At this stage contributions of each facets to the selected segment is estimated and non-contributing 
facets are marked as ignorable (see Fig. 2). The computing of lightwave propagation is then done so that 
.nal wave .eld is drawn on the frame buffers. Those stages can run simultaneously on multi processors 
with a shared memory.The authors demonstrate implementationof a large-scale computational holography, 
venus,produced by a laser lithography shown in Fig. 1. 4. Conclusion The authors proposed large-scale 
full-parallax computacional holography and demonstrated venus for its feasibility. The future works include 
importing rendering technique in CGs. Acknowledgement The mesh data for the venus object is provided 
courtesy of INRIA by the AIM@SHAPE Shape Repository. This work was supported by the JSPS.KAKENHI (21500114). 
References Fig. 2: Subdivision of Wave-.eld Computation  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<article_sponsors>
			<funding_agency>JSPS.KAKENHI</funding_agency>
			<grant_numbers>
				<grant_number>21500114</grant_number>
			</grant_numbers>
		</article_sponsors>
	</article_rec>
	<article_rec>
		<article_id>1836975</article_id>
		<sort_key>1300</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>121</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>7</seq_no>
		<title><![CDATA[Controlling the dark side in toon shading]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836845.1836975</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1836975</url>
		<abstract>
			<par><![CDATA[<p>Sharply separating a diffuse surface into a light and dark side often results in unwanted details. Combining normals from the actual surface with the normals from a simplified surface we get better control of the dark side.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.4</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2264580</person_id>
				<author_profile_id><![CDATA[81466647900]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Ole]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gulbrandsen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Controlling the Dark Side in Toon Shading Ole Gulbrandsen* 1 Introduction Sharply separating a diffuse 
surface into a light and dark side often results in unwanted details. Combining normals from the actual 
surface with the normals from a simpli.ed surface we get better control of the dark side. 2 Theory Our 
solution is to create a shell covering the actual geometry (as in .gure 1), and store the shell normals 
in a point cloud. .gure 1: Sampling normals from an outer shell The surface shader samples the point 
cloud and calculates a new normal (1) which is then used as the new shading normal. The weighting between 
the shell normals and the actual surface normal can easily be controlled by storing ß in a texture map. 
 N navg =1 ßnsurface + (1 - ß) nptci(1) N +1 i=1  3 Implementation surface bakeNormals(string Filename=""; 
string CoordSys="") { bake3d( Filename, "", P, n, "coordsystem", CoordSys, "interpolate", 1, ); } surface 
readNormals(string Filename=""; float b=0.5; string CoordSys="") { uniform string category = concat( 
"pointcloud", ":", Filename ); point p_ptc = transform(CoordSys,P); normal n = normalize(N); normal 
Nsample,Navg,n_avg,n_shade=0; float Samples=8; float MaxDist=1; gather(category, Pworld, n, PI/2,Samples, 
"maxdist",MaxDist,"point:normal", Nsample ){ Navg += Nsample; } n_avg = ntransform(CoordSys,"current",Navg); 
n_shade = b*n+(1-b)*n_avg; //pass n_shade on to the toon shader } *e-mail: olegul@hotmail.com Copyright 
is held by the author / owner(s). SIGGRAPH 2010, Los Angeles, California, July 25 29, 2010. ISBN 978-1-4503-0210-4/10/0007 
 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1836976</article_id>
		<sort_key>1310</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>122</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>8</seq_no>
		<title><![CDATA[Curvature depended local illumination approximation of ambient occlusion]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836845.1836976</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1836976</url>
		<abstract>
			<par><![CDATA[<p>This paper discusses an approach for computing the ambient occlusion by curvature depended approximation of occlusion. Ambient occlusion is widely used to improve the realism of fast lighting simulation. The ambient occlusion is defined as follows.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>G.1.2</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003752.10010061.10010063</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures->Computational geometry</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10003809.10003636</concept_id>
				<concept_desc>CCS->Theory of computation->Design and analysis of algorithms->Approximation algorithms analysis</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003714.10003736.10003737</concept_id>
				<concept_desc>CCS->Mathematics of computing->Mathematical analysis->Functional analysis->Approximation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2264581</person_id>
				<author_profile_id><![CDATA[81466646558]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Tomohito]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hattori]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Waseda University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264582</person_id>
				<author_profile_id><![CDATA[81331497177]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Hiroyuki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kubo]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Waseda University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264583</person_id>
				<author_profile_id><![CDATA[81100066959]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Shigeo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Morishima]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Waseda University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Hayden, L. 2002. Production-Ready Global Illumination. <i>Siggraph2002.</i>
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Miguel, S. Real-Time Depth Buffer Based Ambient Occlusion. Game Developer Conference, 2008]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Janne K., Samuli L. 2005. Ambient Occlusion Field. <i>Siggraph 2005</i>
]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Curvature Depended Local Illumination Approximation of Ambient Occlusion Tomohito Hattori Hiroyuki 
Kubo Shigeo Morishima Waseda University 1. Introduction1 This paper discusses an approach for computing 
the ambient occlusion by curvature depended approximation of occlusion. Ambient occlusion is widely used 
to improve the realism of fast lighting simulation. The ambient occlusion is defined as follows. (1)Labocc 
(x) = 1 . V (x,.)cos. d. p O+ V(x, .) is the visibility function in direction . from vertex location 
x. . is the angle between vertex normal and .. .+ is a hemisphere domain of integration defined by the 
normal. While this method enables effective lighting simulation by considering surrounding occlusion, 
previous works [Miguel et al. 2008 : Janne et al. 2005], needs great computation cost, because the integration 
is solved by monte-carlo collision detection. By contrast, we approximate occlusion by curvature to aim 
to reduce the cost. 2. Shape Geometry Approximation Precisely, the previous ambient occlusion needs monte-carlo 
colli­sion detection because it is difficult to predict neighborhood sur­face from target pixel. This 
is a serious problem, and it is not solved in SSAO yet. Then assuming that a function that approx­imate 
geometry shape makes easy to achieve occlusion, we will acquire a function z = f(x,y) .To acquire concrete 
expression of f(x,y), we calculate Taylor series expansion of f(x,y). Considering from quadratic terms 
of the Taylor series, we acquire eq.(2) using maximum principal curvature as .1 and minimum principal 
curva­ture as .2, because, the quadratic terms are equal to curvature. 1 f (x, y) = (.1 x2 +.2 y2 ) (2) 
2 To calculate neighborhood occlusion, eq.(2) is effective function, because, this function is extremely 
equal to neighborhood shape. 3. Approximation of Ambient Occlusion In this section we introduce a method 
to calculate occlusion from eq.(2). In previous techniques, they have to calculate the ratio of occluded 
ray in .+ to acquire occlusion. By contrast, we can ac­quire the ratio of occluded area on .+ analytically, 
because we already acquired neighborhood shape accuracy according to eq.(2). Thereby, we calculate occluded 
area by the eq.(2) surface to ac­quire occlusion. The equation is as follows. 2p. Labocc (.1,.2) = ..r 
2.sin .'d.'d. (3) 00 . A2 . 22 -1± 1+. A = R(.1 cos .+.2 sin .) . .= arccos . A . .. , 1 httr-zodiac@fuji.waseda.jp 
shigeo@waseda.jp (a)Lambert (b)Our Occlusion (c)Our Rendering (d)Previous Technique (e)Previous Rendering 
Figure 1 Occlusion by Our Technique 4. Implementation In this section we present an efficient implementation 
of our curvature depended ambient occlusion. We compute occlusion from eq.2 in advance and make a look 
up table (LUT), because it needs much computation cost to calculate eq.(3) to acquire exact solution. 
We write this LUT into a texture that have principal curvatures as UV-vector, and we apply this texture 
when we render ambient occlusion. Then our result is in Figure 1. We measured the run-time performance 
on a desktop PC with Intel Core 2 Duo 2.66GHz processor NVIDIA GeForce GTX 285 GPU. The rendering speed 
is more than 1000fps. According to our result, we can render ambient occlusion faster and easier than 
past technique with equal quality. 5. Results and Discussions Past ambient occlusion technique needs 
much computation be­cause of global illumination model. For this reason, we proposed a new, simple ambient 
occlusion technique that is approximated into local illumination. Our technique can render ambient occlu­sion 
by 1000fps in real-time. In this work, we pre-compute curvature with CPU before render­ing. So, real-time 
curvature calculation from vertex by GPU is our future work to treat a non-rigid object. References HAYDEN, 
L. 2002. Production-Ready Global Illumination. Siggraph2002. MIGUEL, S. Real-Time Depth Buffer Based 
Ambient Occlusion. Game Developer Conference, 2008 JANNE K., SAMULI L. 2005. Ambient Occlusion Field. 
Siggraph 2005 Copyright is held by the author / owner(s). SIGGRAPH 2010, Los Angeles, California, July 
25 29, 2010. ISBN 978-1-4503-0210-4/10/0007  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1836977</article_id>
		<sort_key>1320</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>123</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>9</seq_no>
		<title><![CDATA[Depth-based Anisotropic Kuwahara Filtering]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836845.1836977</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1836977</url>
		<abstract>
			<par><![CDATA[<p>When artists draw a picture of photorealistic scene in an image, they describe only specific parts that represent characteristic features carefully, but they express the parts about less important region roughly. In study about Non-photorealistic rendering, image abstraction research reflects such artist's character. Thus, methods about image abstraction commonly preserve image features and flatten non-feature area. Recently, Kyprianidis et al. [2009] introduced Anisotropic Kuwahara Filtering (AKF) which generates feature preserved image abstraction using the smoothed structure tensor. However since they used only color information to defining anisotropic ratio, different regions that have similar color are conquered by each other unintentionally. Hence, we propose the depth-based AKF method that considers not only color, but also depth to generate image abstraction where boundary feateures are effectively preserved.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.4.3</cat_node>
				<descriptor>Filtering</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010224</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2264584</person_id>
				<author_profile_id><![CDATA[81330487764]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Jeong-ho]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ahn]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Yonsei University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264585</person_id>
				<author_profile_id><![CDATA[81331508005]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Jong-Chul]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yoon]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Yonsei University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264586</person_id>
				<author_profile_id><![CDATA[81409592301]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[In-Kwon]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lee]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Yonsei University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[
Kyprianidis, J. E., Kang, H., and D&#246;llner, J. 2009. Image and video abstraction by anisotropic kuwahara filtering. <i>Computer Graphics Forum 28</i>, 7, 1955--1963. Special issue on Pacific Graphics 2009.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[
Zhang, G., Jia, J., Wong, T.-T., and Bao, H. 2008. Recovering consistent video depth maps via bundle optimization. In <i>Proceedings of IEEE Computer Vision and Pattern Recognition</i>, 1--8.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Depth-based Anisotropic Kuwahara Filtering Jeong-ho Ahn * Jong-ChulYoon In-Kwon Lee Yonsei University 
Yonsei University Yonsei University Figure 1: Imageabstraction comparsion between anisotropic kuwahara.ltering 
and depth-based anisotropic kuwahara.ltering: (a) original image; (b) depth map; (c) image abstraction 
result using anisotropic kuwahara .ltering; (d) image abstraction result using depth-based anisotropic 
kuwahara .ltering. 1 Introduction When artists draw a picture of photorealistic scene in an image, they 
describe only speci.c parts that represent characteristic fea­tures carefully,but they express the parts 
about less important re­gion roughly. In study about Non-photorealistic rendering, image abstraction 
research re.ects such artist s character. Thus, meth­ods about image abstraction commonly preserve image 
features and .atten non-feature area. Recently, Kyprianidis et al. [2009] introduced AnisotropicKuwahara 
Filtering(AKF) which generates feature preserved image abstraction using the smoothed structure tensor. 
However since they used only color information to de.n­ing anisotropic ratio, different regions that 
have similar color are conquered by each other unintentionally. Hence, we propose the depth-based AKF 
method that considers not only color, but also depth to generate image abstraction where boundary feateures 
are effectively preserved. 2 Description Noticethat detailsofAKF algorithm canbe foundin[Kyprianidis 
et al. 2009].We modi.ed this algorithm to improve the shape con­servation in image abstraction. When 
the color of an image object is almost completely similar to background color, the result of AKF is prone 
to mix up both regions. because only color information of each pixel on an image is considered. To overcome 
this limitation, we consider the depth of an image. First of all, we .nd depth dis­continuity of contents 
from the depth map in an image using the method by [Zhang et al. 2008]. Using this discontinuity energy, 
we sharply designed anisotropickernel to preserve important parts of an image. We also reduce the size 
ofkernel for weak .ltering about feature region. For implementation, we replace 2D matrix S of [Kyprianidis 
et al. 2009] to S ' by using depth discontinuity d E [0, 1] as follows: () (1-d)w1 S ' = 1+w2 ·d 0 S 
(1) 0 (1 - d)w1 *e-mail:macro516@cs.yonsei.ac.kr e-mail:media19@cs.yonsei.ac.kr e-mail:iklee@yonsei.ac.kr 
where S ' is improvedkernel ratioby our method. w1 denotes the kernelscalingfactor. Basedonthedepth discontinuity,itmakesthe 
smallersizeofkerneltoweakenthe .lteringeffect.The parameter w2 isan elliptic slightnessfactor whereitis 
closerto feature s out­line, to preserve image shape boundary. We replace S ' in existing AKF algorithm, 
This novel ratio can effectively preserve the shape features of image contents. 3 Result and Future 
Work Figure1showsa comparisonofimage abstractionusingAKFand our method . Each outputimagein Figure1is 
generatedbyAKF with three times of iterated step. Since AKF does not use depth in­formation, itfails 
capturing depth discontinuity of image contents and that cause the similar colors are merged by increasing 
num­ber of iteration (Figure 1(c)). But our depth based-method shows enhanced boundary preservation, 
as well as emphasis of people s border details (Figure 1(d)). Although we use existing method forming 
an anisotropic .lter basedon2D tensor informationandimprove performancebydepth information, future research 
will be concentrate on making an anisotropic .lter based on 3D tensor map only using depth infor­mation. 
 Acknowledgements This work was supported by the Korea Science and Engineer­ing Foundation(KOSEF) grant 
funded by the Korea govern-ment(MEST) (No. 2010-0000389) References KYPRIANIDIS, J. E., KANG, H., AND 
D¨ OLLNER,J. 2009. Image and video abstraction by anisotropic kuwahara .ltering. Com­puter GraphicsForum 
28,7, 1955 1963. Special issue onPaci.c Graphics 2009. ZHANG,G.,JIA,J.,WONG,T.-T., AND BAO,H. 2008. Recov­ering 
consistent video depth maps viabundle optimization. In Proceedingsof IEEE ComputerVision andPattern Recognition, 
1 8. Copyright is held by the author / owner(s). SIGGRAPH 2010, Los Angeles, California, July 25 29, 
2010. ISBN 978-1-4503-0210-4/10/0007  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<article_sponsors>
			<funding_agency>Korea Science and Engineering Foundation (KOSEF) grant funded by the Korea government (MEST)</funding_agency>
			<grant_numbers>
				<grant_number>2010-0000389</grant_number>
			</grant_numbers>
		</article_sponsors>
	</article_rec>
	<article_rec>
		<article_id>1836978</article_id>
		<sort_key>1330</sort_key>
		<display_label>Article No.</display_label>
		<pages>2</pages>
		<display_no>124</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>10</seq_no>
		<title><![CDATA[Dynamic ambient occlusion from volumetric proxies]]></title>
		<page_from>1</page_from>
		<page_to>2</page_to>
		<doi_number>10.1145/1836845.1836978</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1836978</url>
		<abstract>
			<par><![CDATA[<p>Real-time applications require simple techniques that enable predictable high performance rendering, but illumination methods for GPUs tend either to be complex and fragile on the one hand or very limited and offering poor visual quality on the other. The addition of a plausible global look to an application's lighting impacts users' perceptions of its realism, and on the spectrum of existing approaches, screen space ambient occlusion (SSAO) lies at the simplest and most predictable end. We have extended screen space ambient occlusion by replacing its depth buffer comparisons with a sampling of a volumetric discretization of the scene, while still evaluating it in an image-space post-process and thus retaining its predictable performance. We show improvements in quality over depth buffer based alternatives at a reasonable additional cost.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[ambient occlusion]]></kw>
			<kw><![CDATA[global illumination]]></kw>
			<kw><![CDATA[slicemap]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>K.6.1</cat_node>
				<descriptor>Life cycle</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>K.7.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003456.10003457</concept_id>
				<concept_desc>CCS->Social and professional topics->Professional topics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003456.10003457.10003490.10003491</concept_id>
				<concept_desc>CCS->Social and professional topics->Professional topics->Management of computing and information systems->Project and people management</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2264587</person_id>
				<author_profile_id><![CDATA[81443596212]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Andrew]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Cox]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University College London, United Kingdom]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264588</person_id>
				<author_profile_id><![CDATA[81100016395]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Jan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kautz]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University College London, United Kingdom]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1111424</ref_obj_id>
				<ref_obj_pid>1111411</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[
Eisemann, E., and D&#233;coret, X. 2006. Fast scene voxelization and applications. In <i>ACM SIGGRAPH Symposium on Interactive 3D Graphics and Games</i>, ACM SIGGRAPH, 71--78.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[
Reinbothe, C., Boubekeur, T., and Alexa, M. 2009. Hybrid ambient occlusion. <i>EUROGRAPHICS 2009 Areas Papers</i>, ??--??
]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Dynamic Ambient Occlusion from Volumetric Proxies Andrew Cox*and Jan Kautz University College London, 
United Kingdom  Abstract Real-time applications require simple techniques that enable pre­dictable high 
performance rendering, but illumination methods for GPUs tend either to be complex and fragile on the 
one hand or very limited and offering poor visual quality on the other. The addi­tion of a plausible 
global look to an application s lighting impacts users perceptions of its realism, and on the spectrum 
of existing ap­proaches, screen space ambient occlusion (SSAO) lies at the sim­plest and most predictable 
end. We have extended screen space ambient occlusion by replacing its depth buffer comparisons with a 
sampling of a volumetric discretization of the scene, while still evaluating it in an image-space post-process 
and thus retaining its predictable performance. We show improvements in quality over depth buffer based 
alternatives at a reasonable additional cost. CR Categories: K.6.1 [Management of Computing and Infor­mation 
Systems]: Project and People Management Life Cycle; K.7.m [The Computing Profession]: Miscellaneous Ethics 
Keywords: Slicemap, ambient occlusion, global illumination  1 Introduction Ambient Occlusion (AO) is 
an approximation to global illumina­tion which is not physically-based but gives a look to renderings 
which is anecdotally well-liked at a far lower cost than a true global illumination solution. While not 
a true global method, AO does require information about a local region of the scene around each shading 
point. That makes it a challenge to map to the independent, triangle-at-a-time and fragment-at-a-time 
model that GPUs are built to accelerate. In its classic presentation AO relies on the primitive of tracing 
rays through a full geometric representation of the scene. Modern vari­ants that run on GPUs have captured 
a per-frame implicit surface as a heightmap by rendering it from the current point of view and writing 
nearest-z to a buffer. They then perform texture lookups in *andrew.cox@cs.ucl.ac.uk j.kautz@cs.ucl.ac.uk 
this buffer in the region of a shade point, using a binary in/out test on the implicit surface or the 
distance to intersection with it in place of a scene ray intersection. This division into separate capture 
and lookup stages plays to the forward rendering strengths of GPUs but it is presently limited by the 
use of a single-valued heightmap as scene proxy, with the result that both false positives and false 
neg­atives are reported for occlusions. We will present a method that shares this separation but which 
captures a richer volumetric proxy for the scene and thus allows a closer approximation to the effects 
that the ray-traced progenitors provide. Our main contributions include the following. Firstly, we provide 
a method for employing a volumetric discretization in dynamic am­bient occlusion that is both faster 
and higher-quality than previ­ous attempts. Secondly, we show both that the additional infor­mation in 
our volumetric representation allows us to display effects that SSAO cannot and that we can avoid its 
characteristic artifacts. Thirdly, we demonstrate that that information can be captured at a lower cost 
than previous methods based on depth peeling or render­ing from multiple viewpoints. Finally, we show 
that a strong recent contribution to the theoretical basis of volumetric AO can be uni­.ed with a practical 
data structure that is a truer volumetric proxy for a scene. 2 Ambient Occlusion from a Discrete Volu­metric 
Proxy We use a discrete volumetric data structure known as a Slicemap that was originated by Eisemann 
and D´ecoret [Eisemann and D´ecoret 2006], and which stores a single bit per voxel. Its chief strength 
is that it can be built in a single geometry pass simply by rasterizing on the GPU. We employ a solid 
Slicemap as a proxy for scene geometry in an AO pass, thus allowing the substitution of complex ray/geometry 
traversals with simple texture fetches to evaluate point in/out with respect to a model. For watertight 
models we generate a solid Slicemap where object interiors are .lled. A solid voxelization has the bene.t 
of avoiding the gaps between voxels that af.ict rasterization-based surface voxelizations. Sampling directly 
from the Slicemap has bandwidth implications since with 128 bit texels, each texel load will bring in 
large amounts of data that is irrelevant to the shading of a particular point. Run­ning at 60Hz in a 
10242 window would require between 48 GiB and 96 GiB (50 GiB expected) per second of bandwidth just to 
evalu­ate AO. We considered a number of options for reducing this cost and went with the simplest: we 
generate a linear format with 8 bit texels. The shader to achieve this simply loads a texel, masks out 
Copyright is held by the author / owner(s). SIGGRAPH 2010, Los Angeles, California, July 25 29, 2010. 
ISBN 978-1-4503-0210-4/10/0007 bytes from it, and then routes them to eight render targets. We adopt 
a dense sampling strategy in our AO pass, loading all voxels within Rtan of a shading point s tangent 
sphere center. To make this viable, for each scene we choose a Slicemap resolution that results in Rtan 
equaling four Slicemap units. That gives us 280 voxels, which is enough to allow smooth gradients and 
requires either 52 or 104 texture loads per sample point. This compares favorably to Reinbothe et al. 
[2009] who must do up to 256 texture loads for their preferred parameter set of 8 ray steps in 32 directions. 
 References EISEMANN, E., AND D´ ECORET, X. 2006. Fast scene voxelization and applications. In ACM SIGGRAPH 
Symposium on Interactive 3D Graphics and Games, ACM SIGGRAPH, 71 78. REINBOTHE, C., BOUBEKEUR, T., AND 
ALEXA, M. 2009. Hy­brid ambient occlusion. EUROGRAPHICS 2009 Areas Papers, ?? ??  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1836979</article_id>
		<sort_key>1340</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>125</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>11</seq_no>
		<title><![CDATA[Exploring global illumination for virtual reality]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836845.1836979</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1836979</url>
		<abstract>
			<par><![CDATA[<p>Real time global illumination (GI) is a difficult and steadily researched area. Advances in the field could potentially benefit virtual reality applications by increasing users' sense of presence. In immersive virtual environments (IVE) like CAVEs, applications must support perspective-corrected stereoscopic rendering. Dmitriev et al. [2004] performed GI in a CAVE using Precomputed Radiance Transfer, which requires a static scene. Mortensen et al. [2007] also performed GI in a CAVE using Virtual Light Fields which did not allow moving lights or geometry. We present our attempt to find GI techniques that support dynamic lights and scene geometry in our 6-sided CAVE-like IVE (<i>DRIVE6</i>). We implemented two separate illumination techniques: GPU photon mapping (GPM) and multiresolution splatting for indirect illumination (MSII). Each technique makes trade-offs between image quality and speed, and appropriate use of each depends on the needs of the application. Anecdotal evidence suggests that these techniques increase the sense of presence, warranting formal study. A user study is planned.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>H.5.1</cat_node>
				<descriptor>Artificial, augmented, and virtual realities</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Virtual reality</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010392</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Mixed / augmented reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010866</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Virtual reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010866</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Virtual reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010866</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Virtual reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2264589</person_id>
				<author_profile_id><![CDATA[81466647986]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Roger]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hoang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Desert Research Institute and University of Nevada, Reno]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264590</person_id>
				<author_profile_id><![CDATA[81435610367]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Steve]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Koepnick]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Desert Research Institute and University of Nevada, Reno]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264591</person_id>
				<author_profile_id><![CDATA[81466647260]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Joseph]]></first_name>
				<middle_name><![CDATA[D.]]></middle_name>
				<last_name><![CDATA[Mahsman]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Desert Research Institute and University of Nevada, Reno]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264592</person_id>
				<author_profile_id><![CDATA[81466647325]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Matthew]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sgambati]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Desert Research Institute and University of Nevada, Reno]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264593</person_id>
				<author_profile_id><![CDATA[81466643641]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Cody]]></first_name>
				<middle_name><![CDATA[J.]]></middle_name>
				<last_name><![CDATA[White]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Desert Research Institute and University of Nevada, Reno]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264594</person_id>
				<author_profile_id><![CDATA[81340488486]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Daniel]]></first_name>
				<middle_name><![CDATA[S.]]></middle_name>
				<last_name><![CDATA[Coming]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Desert Research Institute and University of Nevada, Reno]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1077560</ref_obj_id>
				<ref_obj_pid>1077534</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[
Dmitriev, K., Annen, T., Krawczyk, G., Myszkowski, K., and Seidel, H.-P. 2004. A CAVE system for interactive modeling of global illumination in car interior. In <i>Proc. ACM Symp. on VR Software and Technology</i>, ACM, 137--145.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1572783</ref_obj_id>
				<ref_obj_pid>1572769</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[
McGuire, M., and Luebke, D. 2009. Hardware-accelerated global illumination by image space photon mapping. In <i>Proc. 1st ACM Conf. on High Performance Graphics</i>, ACM, 77--89.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1315210</ref_obj_id>
				<ref_obj_pid>1315184</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[
Mortensen, J., Khanna, P., Yu, I., and Slater, M. 2007. Real-time global illumination in the CAVE. In <i>Proc. ACM Symp. on VR Software and Technology</i>, ACM, 145--148.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1507162</ref_obj_id>
				<ref_obj_pid>1507149</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[
Nichols, G., and Wyman, C. 2009. Multiresolution splatting for indirect illumination. In <i>Proc. 2009 Symp. on Interactive 3D Graphics and Games</i>, ACM, 83--90.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[
Nichols, G., Shopf, J., and Wyman, C. 2009. Hierarchical image-space radiosity for interactive global illumination. <i>Computer Graphics Forum 28</i>, 4, 1141--1149.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1531397</ref_obj_id>
				<ref_obj_pid>1576246</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[
Wang, R., Wang, R., Zhou, K., Pan, M., and Bao, H. 2009. An efficient GPU-based approach for interactive global illumination. <i>ACM SIGGRAPH 2009 28</i>, 3, 1--8.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Exploring Global Illumination for Virtual Reality Roger Hoang, Steve Koepnick, Joseph D. Mahsman, Matthew 
Sgambati, Cody J. White, and Daniel S. Coming  Desert Research Institute and University of Nevada, Reno* 
1 Introduction Real time global illumination (GI) is a dif.cult and steadily re­searched area. Advances 
in the .eld could potentially bene.t virtual reality applications by increasing users sense of presence. 
In im­mersive virtual environments (IVE) like CAVEs, applications must support perspective-corrected 
stereoscopic rendering. Dmitriev et al. [2004] performed GI in a CAVE using Precomputed Radiance Transfer, 
which requires a static scene. Mortensen et al. [2007] also performed GI in a CAVE using Virtual Light 
Fields which did not allow moving lights or geometry. We present our attempt to .nd GI techniques that 
support dynamic lights and scene geometry in our 6­sided CAVE-like IVE (DRIVE6). We implemented two separate 
il­lumination techniques: GPU photon mapping (GPM) and multires­olution splatting for indirect illumination 
(MSII). Each technique makes trade-offs between image quality and speed, and appropriate use of each 
depends on the needs of the application. Anecdotal evi­dence suggests that these techniques increase 
the sense of presence, warranting formal study. A user study is planned. 2 Our Methods Image space photon 
mapping [McGuire and Luebke 2009] raster­izes a scene from the light s perspective to generate initial 
photon bounces, traces all further bounces on the CPU, and scatters the ef­fects of the resulting photons 
by rendering them as ellipsoids with a special shader. This technique does not support area lights or 
accurate re.ection or refraction. Another method implements irra­diance caching and adaptive photon mapping 
and gathering on the GPU but suffers from low frame rates [Wang et al. 2009]. Using OptiX (www.nvidia.com/object/optix.html), 
we combine these methods into our GPM technique that supports re.ections, refractions, and area lights. 
After rasterizing the scene and photon ellipsoids, we ray cast on re.ective and refractive surfaces. 
We per­form photon gathering by casting a ray from the hit point along the surface s normal to .nd which 
photon ellipsoids the ray intersects. We also tested MSII [Nichols and Wyman 2009], which creates vir­tual 
point lights where the scene receives direct illumination, and their contributions are splatted into 
a multiresolution buffer, using min-max mipmaps of normals and depth to select regions for higher resolution 
rendering. We ported a publicly available implementa­tion of this technique (www.cs.uiowa.edu/ cwyman) 
including im­provements using a stencil buffer technique [Nichols et al. 2009]. 3 Results and Conclusion 
Each technique was tested on a cluster of 12 nodes, each with 24 GB of RAM, an Intel Xeon 3.2 GHz processor, 
and an Nvidia Quadro FX 5800 connected to a 1920x1080 projector. Rendered at 800x800 and upscaled, GPM 
achieved frame rates of 11.37, 11.29 and 3.84 fps using scenes containing 70k, 100k, and 346k triangles, 
respectively, without re.ective and refractive surfaces. With these enabled, frame rates dropped to 2.15, 
1.78, and 1.33 fps. MSII reached 16.87, 15.82, 14.72, and 11.00 fps using scenes containing 116k, 186k, 
436k, and 782k triangles, respectively. *{skoepnick, matthew.sgambati, dan.coming}@dri.edu {hoang, mahsmanj, 
cjwhite}@cse.unr.edu Figure 1: (Left) GPU Photon Mapping and (Right) Multiresolution Splatting running 
in a six sided CAVE-like environment. MSII achieved higher frame rates than GPM, but the visual quality 
of the latter is signi.cantly higher, since it can support re.ections, refractions, and multiple indirect 
bounces. MSII also proved to be signi.cantly more scalable than GPM, because it is an image-space technique 
and its speed is independent of geometric complexity. We suspect that the poor GPM frame rates may be 
partially explained by OptiX constructing the photon acceleration structure on the CPU rather than the 
GPU, requiring transfers from and to GPU memory. We made some interesting observation while testing these 
methods. The virtual light drawn on the IVE walls would often illuminate the user, which we felt increased 
the feeling of presence. We also noticed that low frame rates were alleviated by the stereo rendering. 
With both eyes open, slow frame rates were less apparent than with one eye closed. We suspect that since 
DRIVE6 uses active stereo, the user s eyes receive twice the update frequency when combined. This work 
was completed as part of a course at UNR, using equip­ment at DRI supported by the U.S. Army s RDECOM-STTC 
under Contract No. N61339-04-C-0072. References DMITRIEV, K., ANNEN, T., KRAWCZYK, G., MYSZKOWSKI, K., 
AND SEIDEL, H.-P. 2004. A CAVE system for interactive mod­eling of global illumination in car interior. 
In Proc. ACM Symp. on VR Software and Technology, ACM, 137 145. MCGUIRE, M., AND LUEBKE, D. 2009. Hardware-accelerated 
global illumination by image space photon mapping. In Proc. 1st ACM Conf. on High Performance Graphics, 
ACM, 77 89. MORTENSEN, J., KHANNA, P., YU, I., AND SLATER, M. 2007. Real-time global illumination in 
the CAVE. In Proc. ACM Symp. on VR Software and Technology, ACM, 145 148. NICHOLS, G., AND WYMAN, C. 
2009. Multiresolution splatting for indirect illumination. In Proc. 2009 Symp. on Interactive 3D Graphics 
and Games, ACM, 83 90. NICHOLS, G., SHOPF, J., AND WYMAN, C. 2009. Hierarchical image-space radiosity 
for interactive global illumination. Com­puter Graphics Forum 28, 4, 1141 1149. WANG, R., WANG, R., ZHOU, 
K., PAN, M., AND BAO, H. 2009. An ef.cient GPU-based approach for interactive global illumi­nation. ACM 
SIGGRAPH 2009 28, 3, 1 8. Copyright is held by the author / owner(s). SIGGRAPH 2010, Los Angeles, California, 
July 25 29, 2010. ISBN 978-1-4503-0210-4/10/0007  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1836980</article_id>
		<sort_key>1350</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>126</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>12</seq_no>
		<title><![CDATA[Fast soft shadow by depth peeling]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836845.1836980</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1836980</url>
		<abstract>
			<par><![CDATA[<p>Soft shadow generation is a challenging problem in realistic rendering. Previous methods using shadow map or shadow volume work well for point light sources but are difficult to be extended to area lights. This paper presents a new method for fast soft shadow generation under dynamic area light sources. Our algorithm encodes the depth distribution of the scene into a coarse depth grid in a preliminary pass from the light point of view. In the second pass, the scene is rendered from the camera viewpoint to capture the frontmost layer. During deferred shading, the area light is sampled and the irradiance of each shaded pixel is accumulated along the ray. Experimental results demonstrate high quality soft shadows with interactive performance for dynamic scenes and lighting.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Color, shading, shadowing, and texture</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010384</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Texturing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2264595</person_id>
				<author_profile_id><![CDATA[81410593094]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Xuehui]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Liu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Chinese Academy of Sciences]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264596</person_id>
				<author_profile_id><![CDATA[81466645847]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Xiaoguang]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hao]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Chinese Academy of Sciences]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264597</person_id>
				<author_profile_id><![CDATA[81440592538]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Mengcheng]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Huang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Chinese Academy of Sciences]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264598</person_id>
				<author_profile_id><![CDATA[81440596288]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Fang]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Liu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Chinese Academy of Sciences]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264599</person_id>
				<author_profile_id><![CDATA[81327492676]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Mingquan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Zhou]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Beijing Normal University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264600</person_id>
				<author_profile_id><![CDATA[81455605618]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Hanqiu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sun]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Chinese University of Hongkong]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264601</person_id>
				<author_profile_id><![CDATA[81100657893]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>7</seq_no>
				<first_name><![CDATA[En-Hua]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Macau and Beijing Normal University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1572779</ref_obj_id>
				<ref_obj_pid>1572769</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Liu, F., Huang, M.-C., Liu, X.-H., and Wu, E.-H. 2009. Efficient depth peeling via bucket sort. In <i>Proceedings of the Conference on High Performance Graphics 2009</i>, ACM, New York, NY, USA, 51--57.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Fast Soft Shadow by Depth Peeling Xuehui Liu * Xiaoguang Hao Mengcheng Huang Fang Liu Mingquan Zhou 
Hanqiu Sun. En-Hua Wu § State Key Lab of Computer Sciences, Institute of Software, Chinese Academy of 
Sciences Beijing Normal University Chinese University of Hongkong. University of Macau§ Figure 1: Soft 
shadows rendered by our algorithm for several scenes. 1 Introduction Soft shadow generation is a challenging 
problem in realistic render­ing. Previous methods using shadow map or shadow volume work well for point 
light sources but are dif.cult to be extended to area lights. This paper presents a new method for fast 
soft shadow gen­eration under dynamic area light sources. Our algorithm encodes the depth distribution 
of the scene into a coarse depth grid in a pre­liminary pass from the light point of view. In the second 
pass, the scene is rendered from the camera viewpoint to capture the front­most layer. During deferred 
shading, the area light is sampled and the irradiance of each shaded pixel is accumulated along the ray. 
Experimental results demonstrate high quality soft shadows with interactive performance for dynamic scenes 
and lighting. 2 Our Approach We assume the objects in the scene to be water-tight and non­intersected 
with each other so that the ray intersects the objects in pairs. The algorithm starts by setting the 
view point to the cen­ter of the area light. The depth range of each pixel is divided into 16 subintervals 
uniformly to form a depth grid. We utilize 8 MRT buffers and initialize them to 0. Each two consecutive 
MRT chan­nels are bind into pairs as described in [Liu et al. 2009]. Within each subinterval, fragments 
on the front faces will update the .rst channel, while those on the back faces will update the second 
chan­nel using MAX blending. After the geometry pass, the depth val­ues of the furthest fragment within 
each subinterval can be restored from the channel pair and negated if it is on a back face. In the sec­ond 
pass, the scene is rendered from the camera viewpoint to obtain the shaded points and re-project them 
to the light space. z > |zi,p| Table 1: Visibility calculation for current subinterval *China Basic 
S&#38;T 973 Research Grant(2009CB320802), National 863 High-Tec Grant(2008AA01Z301), NSFC(60573155)&#38;UM 
Research Grant. zi,p=0 z j,p > 0 Occluded z j,p < 0 Non-occluded z j,p = 0 Non-occluded zj,p > 0 Occluded 
zi,p > 0 z < |zi,p| zj,p < 0 Non-occluded zj,p = 0 Non-occluded z > |zi,p| Occluded zi,p < 0 z < |zi,p| 
Occluded Non-occluded Scene Triangles Light Samples Ray Samples FPS Budda 1.087M 10X10 10 17.2 10X10 
20 8.1 Torus-knot 2.8K 10X10 10 23.1 10X10 20 11.7 Armadillo 213K 10X10 10 16.2 10X10 20 13.0 Bunny with 
stairway 69K 10X10 30 6.7 20X20 30 1.7 Room with furniture 3.5K 10X10 50 2.2 20X20 50 0.5 Table 2: Performance 
of our algorithm on the test scenes. In deferred shading, the area light is sampled and the visibility 
term between each light sample and each shaded point is calculated ac­cording to the depth grid. We march 
along the ray from a shaded point to a light sample at a .xed step. For a ray sample with a depth value 
z, suppose it is in the ithsubinterval on pixel location p in the depth grid, and the captured depth 
value in that grid cell is zi,p. The algorithm searches for the .rst non-zero subinterval j along the 
ray towards the light. If there is no such an interval, the ray is non-occluded. Otherwise, according 
to zi,p and zj,p, it is easy to determine whether the ray has been occluded or not as summa­rized in 
Table 1. The visibility term can be set to be zero if the ray is occluded or one if not. The irradiance 
of each shaded point can be accumulated by the product of the intensity of each light sample with its 
visibility term to generate realistic soft shadows. 3 Results Table 2 shows the performance of our algorithm 
with different num­ber of light and ray samples at 512x512 on a commodity PC of Intel Duo Core 2.4G Hz 
with 3GB memory, and NVIDIA Geforce 280 GTX. The computation for visibility term dominates the algorithm 
running time, which could be accelerated by taking advantage of the coherency between adjacent pixels 
in the future. References LIU, F., HUANG, M.-C., LIU, X.-H., AND WU, E.-H. 2009. Ef.cient depth peeling 
via bucket sort. In Proceedings of the Conference on High Performance Graphics 2009, ACM, New York, NY, 
USA, 51 57. Copyright is held by the author / owner(s). SIGGRAPH 2010, Los Angeles, California, July 
25 29, 2010. ISBN 978-1-4503-0210-4/10/0007  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<article_sponsors>
			<funding_agency>NSFC</funding_agency>
			<grant_numbers>
				<grant_number>60573155</grant_number>
			</grant_numbers>
		</article_sponsors>
		<article_sponsors>
			<funding_agency>National 863 High-Tec Grant</funding_agency>
			<grant_numbers>
				<grant_number>2008AA01Z301</grant_number>
			</grant_numbers>
		</article_sponsors>
		<article_sponsors>
			<funding_agency>China Basic S&#38;T 973 Research Grant</funding_agency>
			<grant_numbers>
				<grant_number>2009CB320802</grant_number>
			</grant_numbers>
		</article_sponsors>
	</article_rec>
	<article_rec>
		<article_id>1836981</article_id>
		<sort_key>1360</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>127</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>13</seq_no>
		<title><![CDATA[Generating and rendering expressive caricatures]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836845.1836981</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1836981</url>
		<abstract>
			<par><![CDATA[<p>Facial caricature drawing exaggerates physical face features for a comical effect, and can create an entertaining, humorous, and cartoon-like description of a person's face. Recently, example-based approaches have been introduced to generate facial sketches. Most of these approaches exaggerate the caricature appearance by altering the overall facial shape based on capturing artists' exaggeration-prototypes. Rare attempts have been made to alter and control the facial expressions of the generated caricatures. Moreover, example-based approaches learn how to generate artistic sketch styles through a training phase with prototypes from artist sketches. One of the limitations to these systems is that they require a lot of manual work with a large number of training prototypes drawn by artists. In addition, the final appearance of the caricature can only be limited to the prototypes used in the training phase.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2264602</person_id>
				<author_profile_id><![CDATA[81430614338]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Mohammad]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Obaid]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Canterbury, Christchurch, New Zealand]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264603</person_id>
				<author_profile_id><![CDATA[81327490351]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Ramakrishnan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Mukundan]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Canterbury, Christchurch, New Zealand]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264604</person_id>
				<author_profile_id><![CDATA[81100499261]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Mark]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Billinghurst]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Canterbury, Christchurch, New Zealand]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1673987</ref_obj_id>
				<ref_obj_pid>1673074</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Obaid, M., Mukundan, R., Billinghurst, M., and Sagar, M. 2009. Facial expression representation using a quadratic deformation model. In <i>International Conference on Computer Graphics, Imaging and Visualization</i>, IEEE Computer Society.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Generating and Rendering Expressive Caricatures Mohammad Obaid* Ramakrishnan Mukundan Mark Billinghurst 
Computer Science and Software Engineering Department, University of Canterbury, Christchurch, New Zealand 
HITLab New Zealand, University of Canterbury, Christchurch, New Zealand  Figure 1: A caricature animation 
sequence for the smile facial expression 1 Introduction Facial caricature drawing exaggerates physical 
face features for a comical effect, and can create an entertaining, humorous, and cartoon-like description 
of a person s face. Recently, example­based approaches have been introduced to generate facial sketches. 
Most of these approaches exaggerate the caricature appearance by altering the overall facial shape based 
on capturing artists exaggeration-prototypes. Rare attempts have been made to al­ter and control the 
facial expressions of the generated caricatures. Moreover, example-based approaches learn how to generate 
artis­tic sketch styles through a training phase with prototypes from artist sketches. One of the limitations 
to these systems is that they require a lot of manual work with a large number of training prototypes 
drawn by artists. In addition, the .nal appearance of the caricature can only be limited to the prototypes 
used in the training phase. In this paper we propose an approach to generate and render facial caricatures 
from an input face image. Our approach allows the user to control the exaggeration level of the facial 
expression imposed on the caricature drawing. It is also capable of producing an expressive facial animation 
of the caricature drawing (as in Figure 1). The main contributions of our approach are: (1) A quadratic 
defor­mation model for the transformation of facial feature lines, which can effectively map any of the 
six expressions to a face image, (2) an interpolation method for manipulating the facial appearance and 
expressivity of the caricature, and (3) an automatic moment-based stroke rendering algorithm to render 
extracted facial features. Our approach produces very expressive artistically rendered caricatures, and 
could lead to future research directions in the .eld of non­photorealistic rendering. Our method could 
be applied in enter­taining standalone applications or caricature animations, or other domains such as 
augmented reality, animated talking agents etc. 2 Our Approach We now brie.y describe the main components 
of our caricature generation algorithm. The approach combines three main elements to generate the facial 
caricature: (1) facial feature extraction algo­rithms, (2) facial expression representations using quadratic 
defor­ *e-mail: mohammad.obaid@hitlabnz.org e-mail:mukund@cosc.canterbury.ac.nz e-mail:mark.billinghurst@hitlabnz.org 
mation models, and (3) a stroke based caricature rendering algo­rithm. The .rst step is to extract features 
from the image that will undergo the deformations. Active Appearance model (AAM) and digital matting 
are used for identifying facial features of interest, such as eyebrows, eyes, nose, lips, ears and hair. 
We call this part, Render­ing Path Extraction, as the extracted path will serve as the painting line 
for the caricature rendering algorithm. Once the features have been extracted, they are transformed using 
facial expression representation described in [Obaid et al. 2009]. In their work, they represent facial 
expressions by capturing the non­linear nature of muscle deformations for each expression to form quadratic 
deformation models of the main six expressions (smile, fear, surprise, sad, disgust, and anger). The 
quadratic deformation models can be applied on any feature line of the caricature by .rst identifying 
the region to which the line belongs, and then applying the corresponding transformation parameters to 
every point on that line. Finally, the appearance of the generated caricature is enhanced by employing 
a stroke based non-photorealistic rendering algorithm. The algorithm starts with a blank image (canvas), 
and then builds a composition of the caricature appearance, by progressively apply­ing strokes along 
the rendering path. In our approach, strokes are represented as rectangular shapes that have the attribute 
features colour, position, orientation, and size. We apply the geometric mo­ment shape descriptors, along 
the rendering path, to determine the stroke attributes. This produces a caricature rendered with a styl­ized 
appearance and expression of the original image. Currently, we are in the process of conducting subjective 
evalua­tions of the generated caricatures to study their appearance, exag­geration, and the resembling 
of the facial expressions. Future work will also focus on integrating our approach into entertainment 
ap­plications, such as online caricature generations, caricature anima­tions, and standalone applications. 
 References OBAID, M., MUKUNDAN, R., BILLINGHURST, M., AND SAGAR, M. 2009. Facial expression representation 
using a quadratic deformation model. In International Conference on Computer Graphics, Imaging and Visualization, 
IEEE Computer Society. Copyright is held by the author / owner(s). SIGGRAPH 2010, Los Angeles, California, 
July 25 29, 2010. ISBN 978-1-4503-0210-4/10/0007  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1836982</article_id>
		<sort_key>1370</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>128</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>14</seq_no>
		<title><![CDATA[GPU ray casting of virtual globes]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836845.1836982</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1836982</url>
		<abstract>
			<par><![CDATA[<p>Accurately rendering an ellipsoid is a fundamental problem for virtual globes in GIS and aerospace applications where the Earth's standard reference surface is non-spherical. The traditional approach of tessellating an ellipsoid into triangles and rendering via rasterization has several drawbacks [Miller and Gaskins 2009]. Geodetic grid tessellations oversample at the poles (2a), which leads to shading artifacts and ineffective culling. Tessellations based on subdividing an inscribed platonic solid lead to problematic triangles crossing the International Date Line and poles (2b).</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.1</cat_node>
				<descriptor>Graphics processors</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010389</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Graphics processors</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2264605</person_id>
				<author_profile_id><![CDATA[81466641240]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Patrick]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Cozzi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Analytical Graphics, Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264606</person_id>
				<author_profile_id><![CDATA[81466647583]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Frank]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Stoner]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Analytical Graphics, Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[
Dick, C., Kr&#252;ger, J., and Westermann, R. 2009. GPU ray-casting for scalable terrain rendering. In <i>Proceedings of Eurographics 2009 - Areas Papers</i>, 43--50.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[
Miller, J. R., and Gaskins, T. 2009. Computations on an Ellipsoid for GIS. <i>Computer-Aided Design and Applications 6</i>, 4, 575--583.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 GPU Ray Casting of Virtual Globes Patrick Cozzi* Frank Stoner Analytical Graphics, Inc.  (a) (b) (c) 
(d) (e) Figure 1: (a) Front face culled bounding box (wireframe). (b) Ray/ellipsoid intersections in 
cyan. (c) Shaded, ray casted globe depth tested against rasterized billboards. (d) Viewport-aligned ellipsoid 
bounding polygon reduces ray misses. (e) Wireframe bounding polygon overlay. 1 Introduction Accurately 
rendering an ellipsoid is a fundamental problem for vir­tual globes in GIS and aerospace applications 
where the Earth s standard reference surface is non-spherical. The traditional ap­proach of tessellating 
an ellipsoid into triangles and rendering via rasterization has several drawbacks [Miller and Gaskins 
2009]. Geodetic grid tessellations oversample at the poles (2a), which leads to shading artifacts and 
ineffective culling. Tessellations based on subdividing an inscribed platonic solid lead to problematic 
triangles crossing the International Date Line and poles (2b). We present a new approach to globe rendering 
based on GPU ray casting. Instead of tessellating the ellipsoid, we treat it naturally as an implicit 
surface. Simple proxy geometry bounding the ellipsoid from the viewer s perspective is rendered in order 
to invoke a frag­ment shader that casts a ray to .nd the ellipsoid s visible surface and shade accordingly. 
Our approach has the traditional advantages of ray casting implicit surfaces: in.nite level of detail, 
trivial memory requirements, and simplicity. Furthermore, our approach reduces ray misses, runs at real-time 
frame rates on commodity GPUs, and easily integrates into existing rasterization-based engines. (a) 
(b) Figure 2: (a) Over tessellation at the poles. (b) Triangles crossing the International Date Line 
(in blue). 2 Our Approach We start by rendering the ellipsoid s bounding box with front face culling 
(.gure 1a). This invokes a fragment shader that casts a ray from the viewer to the fragment s world space 
position, checking for intersection with the ellipsoid. Figure 1b shows intersections in *e-mail: pjcozzi@siggraph.org 
e-mail: fstoner@agi.com cyan and misses in gray. Fragments with ray misses are discarded. Fragments with 
intersections are shaded using the geodetic surface normal, yielding accurate lighting and texturing. 
Finally, the point of intersection is transformed into window coordinates to compute the correct depth 
value. Figure 1c shows a globe rendered using our approach combined with rasterized billboards. Next, 
we introduce two new optimizations based on transforming the ellipsoid into a coordinate space where 
its representation is a sphere. First, this simpli.es the ray/ellipsoid test. Second, ray misses can 
be reduced. Since the proxy geometry used to invoke the fragment shader only needs to bound the ellipsoid 
from the viewer s perspective, a viewport-aligned convex polygon is suf.­cient. We compute such a bounding 
polygon on the CPU in the transformed space (.gure 1d). The bounding polygon is then ren­dered as a triangle 
fan in the original coordinate space. The number of tangent points allows a trade-off between CPU/vertex 
processing and fragment processing. Using rasterization, rendering a globe resulted in rates of 112 fps 
(65,024 triangles) and 134 fps (960 triangles) on a NVIDIA GeForce 8400 GS at 1440x900 resolution. Our 
bounding box ray casting approach resulted in rates of 94 fps for full view and 78 fps for a horizon 
view (.gure 1e). Our ray/ellipsoid intersection and bounding polygon optimizations improve the rate to 
111 and 94 fps, making GPU ray casting competitive with rasterization. Given the increasing memory bandwidth 
bottleneck, we believe that ray casting concise model representations, such as implicit surfaces, will 
have widespread use. In future work, we plan to compute an adaptive bounding polygon on the GPU and handle 
terrain by ray casting height .elds [Dick et al. 2009]. We thank Kevin Ring, Deron Ohlarik, Vince Coppola, 
Joe Kider, Norm Badler, and Eric Haines for their input. We acknowledge Nat­ural Earth (www.naturalearthdata.com) 
for raster data, and Yusuke Kamiyamane (www.pinvoke.com) for icons. References DICK, C., KRUGER¨, J., 
AND WESTERMANN, R. 2009. GPU ray-casting for scalable terrain rendering. In Proceedings of Eu­rographics 
2009 -Areas Papers, 43 50. MILLER, J. R., AND GASKINS, T. 2009. Computations on an Ellipsoid for GIS. 
Computer-Aided Design and Applications 6, 4, 575 583. Copyright is held by the author / owner(s). SIGGRAPH 
2010, Los Angeles, California, July 25 29, 2010. ISBN 978-1-4503-0210-4/10/0007  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1836983</article_id>
		<sort_key>1380</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>129</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>15</seq_no>
		<title><![CDATA[Multi-image interpolation based on graph-cuts and symmetric optical flow]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836845.1836983</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1836983</url>
		<abstract>
			<par><![CDATA[<p>Multi-image interpolation in space and time has recently received considerable attention. Typically, the interpolated image is synthesized by adaptively blending several forward-warped images. Blending itself is a low-pass filtering operation: the interpolated images are prone to blurring and ghosting artifacts as soon as the underlying correspondence fields are imperfect. We address both issues and propose a multi-image interpolation algorithm that avoids blending. Instead, our algorithm decides for each pixel in the synthesized view from which input image to sample. Combined with a symmetrical long-range optical flow formulation for correspondence field estimation, our approach yields crisp interpolated images without ghosting artifacts.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[2D morphing & warping]]></kw>
			<kw><![CDATA[free-viewpoint video]]></kw>
			<kw><![CDATA[image-based rendering]]></kw>
			<kw><![CDATA[video-based rendering]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>G.1.1</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10002950.10003714.10003715.10003722</concept_id>
				<concept_desc>CCS->Mathematics of computing->Mathematical analysis->Numerical analysis->Interpolation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2264607</person_id>
				<author_profile_id><![CDATA[81331498092]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Christian]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Linz]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[TU Braunschweig]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264608</person_id>
				<author_profile_id><![CDATA[81365598480]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Christian]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lipski]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[TU Braunschweig]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264609</person_id>
				<author_profile_id><![CDATA[81100477906]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Marcus]]></first_name>
				<middle_name><![CDATA[A.]]></middle_name>
				<last_name><![CDATA[Magnor]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[TU Braunschweig]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>505473</ref_obj_id>
				<ref_obj_pid>505471</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Boykov, Y., Veksler, O., and Zabih, R. 2001. Fast approximate energy minimization via graph cuts. <i>IEEE Trans. Pattern Anal. Mach. Intell. 23</i>, 11, 1222--1239.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1531348</ref_obj_id>
				<ref_obj_pid>1531326</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Mahajan, D., Huang, F., Matusik, W., Ramamoorthi, R., and Belhumeur, P. 2009. Moving Gradients: A Path-Based Method for Plausible Image Interpolation. <i>ACM Transactions on Graphics 28</i>, 3, 42:1--42:11.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Steinbruecker, F., Pock, T., and Cremers, D. 2009. Large Displacement Optical Flow Computation without Warping. In <i>IEEE International Conference on Computer Vision (ICCV)</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Stich, T., Linz, C., Albuquerque, G., and Magnor, M. 2008. View and Time Interpolation in Image Space. <i>Computer Graphics Forum (Proc. of PG'08) 27</i>, 7, 1781--1787.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Telea, A. 2004. An image inpainting technique based on the fast marching method. <i>Journal of Graphical Tools Vol. 9</i>, No. 1, 25--36.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Multi-image Interpolation based on Graph-cuts and Symmetric Optical Flow Christian Linz, Christian Lipski, 
Marcus A. Magnor* Computer Graphics Lab, TU Braunschweig  Figure 1: (a) Scene rendered by adaptively 
blending four forward-warped images as proposed by Stich et al. The image shows ghosting artifacts (red 
boxes) and appears blurry. (b) Our proposed graph-cut interpolation algorithm. Black areas indicate holes 
that are invisible in all input cameras. (c) The image overlaid with the optimal labeling, each color 
denotes a different source image. (d) Final result, invisible regions are .lled by spatio-temporal image 
inpainting. Abstract Multi-image interpolation in space and time has recently received considerable attention. 
Typically, the interpolated image is syn­thesized by adaptively blending several forward-warped images. 
Blending itself is a low-pass .ltering operation: the interpolated im­ages are prone to blurring and 
ghosting artifacts as soon as the un­derlying correspondence .elds are imperfect. We address both is­sues 
and propose a multi-image interpolation algorithm that avoids blending. Instead, our algorithm decides 
for each pixel in the syn­thesized view from which input image to sample. Combined with a symmetrical 
long-range optical .ow formulation for correspon­dence .eld estimation, our approach yields crisp interpolated 
im­ages without ghosting artifacts. Keywords: image-based rendering, video-based rendering, free­viewpoint 
video, 2D Morphing &#38; Warping 1 Motivation The synthesis of in-between images from different viewpoints 
and/or time instants is experiencing a renaissance. Mahajan et al. recently proposed a high-quality interpolation 
technique that is based on .nding an optimal path for a pixel transitioning from one image to the other 
[Mahajan et al. 2009]. The strength of this ap­proach is that the path framework allows each pixel to 
transition to the other image somewhere along the path, whenever a good corre­spondence is found.Further 
on, each pixel in the interpolated view is sampled from exactly one source image, thus avoiding ghosting 
or blurring artifacts. A major drawback of this approach is that the path idea can only be applied to 
two images; a direct extension to multi-image interpolation is not feasible. Stich et al. recently introduced 
a spatio-temporal image interpo­lation approach that generates novel views from four input im­ages [Stich 
et al. 2008]. This approach is based on adaptively blend­ing four forward-warped images. While this approach 
also delivers high-quality interpolation results, it suffers from ghosting and blur­ring artifacts as 
soon as the underlying correspondence .elds are imperfect. * {linz,lipski,magnor}@cg.cs.tu-bs.de 2 Our 
Approach In our approach, we combine the strengths of both approaches and propose a multi-image interpolation 
algorithm that avoids blend­ing. We proceed as follows: we .rst forward-warp the input images to the 
desired target position; in constrast to Stich et al. [2008], the warp mesh is cut open in disoccluded 
regions. Our algorithm then decides for each pixel in the interpolated image from which forward-warped 
source image to sample best. To this end, we for­mulate image interpolation as a labeling problem and 
solve the re­sulting optimization problem using graph-cuts [Boykov et al. 2001]. Along those cuts, it 
is crucial that the forward-warped source im­ages are in perfect correspondence.In the path framework 
of Ma­hajan et al., perfect correspondence comes for free by symmetry of paths; in our approach we enforce 
symmetric correspondence .elds by adapting the long-range correspondence estimation algorithm by Steinbruecker 
et al. [2009] to ful.ll this goal. In a last step, parts that are invisible in all input images are .lled 
by inpainting the in­terpolated sequence using the approach of Telea [Telea 2004], suit­ably extended 
to three dimensions. Taken together, our approach yields crisp interpolated images for more than two 
input images.  References BOYKOV, Y., VEKSLER, O., AND ZABIH, R. 2001. Fast approx­imate energy minimization 
via graph cuts. IEEE Trans. Pattern Anal. Mach. Intell. 23, 11, 1222 1239. MAHAJAN, D., HUANG, F., MATUSIK, 
W., RAMAMOORTHI, R., AND BELHUMEUR, P. 2009. Moving Gradients: A Path-Based Method for Plausible Image 
Interpolation. ACM Transactions on Graphics 28, 3, 42:1 42:11. STEINBRUECKER, F., POCK, T., AND CREMERS, 
D. 2009. Large Displacement Optical Flow Computation without Warping. In IEEE International Conference 
on Computer Vision (ICCV). STICH, T., LINZ, C., ALBUQUERQUE, G., AND MAGNOR, M. 2008. View and Time Interpolation 
in Image Space. Computer Graphics Forum (Proc. of PG 08) 27, 7, 1781 1787. TELEA, A. 2004. An image inpainting 
technique based on the fast marching method. Journal of Graphical Tools Vol.9, No.1, 25 36. Copyright 
is held by the author / owner(s). SIGGRAPH 2010, Los Angeles, California, July 25 29, 2010. ISBN 978-1-4503-0210-4/10/0007 
  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1836984</article_id>
		<sort_key>1390</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>130</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>16</seq_no>
		<title><![CDATA[Multi-interfaces based refractive rendering]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836845.1836984</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1836984</url>
		<abstract>
			<par><![CDATA[<p>We presented an multi-interfaces image based method to simulate the refraction and related light effects in real time on a normal graphic card. The multi-interfaces based representation of the refractor is obtained by hiring depth peeling ideas. This leads to significantly better results than two interfaces refraction where only the front and back face of the object was captured.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[deformable objects]]></kw>
			<kw><![CDATA[real-time rendering]]></kw>
			<kw><![CDATA[refraction]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2264610</person_id>
				<author_profile_id><![CDATA[81466644248]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Lei]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ma]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Shanghai Jiao Tong University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264611</person_id>
				<author_profile_id><![CDATA[81332536785]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Shuangjiu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Xiao]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Shanghai Jiao Tong University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264612</person_id>
				<author_profile_id><![CDATA[81440605008]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Xubo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Shanghai Jiao Tong University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1598069</ref_obj_id>
				<ref_obj_pid>1597990</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[
Liu, F., Huang, M., Liu, X., and Wu, E. 2009. Single pass depth peeling via CUDA rasterizer. In <i>SIGGRAPH 2009: Talks</i>, ACM, 79.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[
Wyman, C., and Nichols, G. 2009. Adaptive caustic maps using deferred shading. In <i>Computer Graphics Forum</i>, vol. 28, Blackwell Publishing, 309--318.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Multi-interfaces based Refractive Rendering LeiMa, Shuangjiu Xiao andXuboYang * Shanghai JiaoTong University 
 Figure 1: The .rst three images are the comparisons between our rendering result and two interface 
rendering method; the fourth is the ray intersection problem; the last one is our .nal rendering result 
with caustics. Abstract We presented an multi-interfaces image based method to simulate the refraction 
and related light effects in real time on a normal graphic card. The multi-interfaces based representation 
of the re­fractor is obtained by hiring depth peeling ideas. This leads to sig­ni.cantly better results 
than two interfaces refraction where only the front and backfaceof the objectwascaptured. Keywords: Real-time 
Rendering;Refraction;Deformable Objects 1 Introduction and Motivation Real-time simulation of the natural 
phenomenon caused by the light transmit through objects are always a challenge in Computer Graphics. 
Recent work has show refraction and caustic effects can be per­formed on graphics hardware to obtain 
an impressive realistic re­sult through a two interfaces representation [Wyman and Nichols 2009]. The 
two interfaces representation method reasonably ignore the internal information of one refractor. However, 
the for sophis­ticated refractive model which is also very common in real world such like a cup, such 
assumption is not accurate. The other ideas rendering on an accelerating space representation methods 
such like volumetric dataset and K-D tree organized mesh can achieve an more accurate resultbut withaslow 
speed. We focusonhowto represent the refractor with multi-interfaces and implement the ray tracing idea 
on it to obtain an accurate result. 2 Technical Approach The .rst stepof ourworkis inspiredby the depth 
peeling idea.We implemented the similar method with the [Liu et al. 2009]. The idea is simple: during 
the process of rendering the whole model, for each reached pixel we perform a depth comparison with the 
previous arrived pixel and store the depth and normal information in a sorted order. And this step outputs 
a texture array of sorted depths and normals. Then we implement a multi-interfaces based refraction algorithm. 
Formostof casestheray transportfromalowerindex interfacetoa higher index interface. However, due to the 
representation obtained by the depth peeling process, few rays may transport back from *This work is 
supported by National Natural Science Funds of China (No. 60970051) a higher index to a lower index. 
So we implement a look-back mechanism. The pseudocode is showed as follows. Setting with Max binary steps 
and Linear probe div while not reach the highest interface do Current direction T , position P , interface 
N, depth DP ; Depth of N interface DN ; while DN+1 <DP do sample DN and DN+1; if DP <DN then N -- else 
DN+1-DN P = P + T * linear probe div end if end while Binary search with Max binary steps; end while 
 3 Results and Future work The measurements were performed by rendering images with 1024x1024pixels using 
an Intel2Duo E4600(2.40GHz)PC anda PCIexpressGeForece8800GTwith256MBof memoryand with­out much optimization. 
And the same with previous methods, all the models couldbe underdeformation. Currently we areworking 
Table 1: Performance in fps.MIR(multi-interfaces refraction), TIR(two interfaces refraction), MIRC(MIR 
with simple caustics) Model polygons MIR TIR MIRC Cup 7100 60 72 30 Mug 3746 61 65 29 Buddha 50000 45 
53 21 on manyother effects such like total internal re.ection and subsur­face scattering.  References 
LIU, F., HUANG, M., LIU, X., AND WU, E. 2009. Single pass depth peeling via CUDArasterizer. In SIGGRAPH 
2009:Talks, ACM, 79. WYMAN, C., AND NICHOLS, G. 2009. Adaptive caustic maps using deferred shading. In 
Computer GraphicsForum, vol. 28, Blackwell Publishing, 309 318. Copyright is held by the author / owner(s). 
SIGGRAPH 2010, Los Angeles, California, July 25 29, 2010. ISBN 978-1-4503-0210-4/10/0007  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<article_sponsors>
			<funding_agency>National Natural Science Funds of China</funding_agency>
			<grant_numbers>
				<grant_number>60970051</grant_number>
			</grant_numbers>
		</article_sponsors>
	</article_rec>
	<article_rec>
		<article_id>1836985</article_id>
		<sort_key>1400</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>131</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>17</seq_no>
		<title><![CDATA[Non-photorealistic rendering in stereoscopic 3D visualization]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836845.1836985</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1836985</url>
		<abstract>
			<par><![CDATA[<p>Spatial visualization of virtual contents appears to be, with the appearance of stereoscopic displays, the next step for increasing immersion in visual output. This kind of visualization can be used to facilitate the understanding of complex informations, like anatomic structures. One approach currently adopted for this purpose is the use of non-photorealisitic rendering (NPR), like proposed by Tietjen, Isenberg and Preim [2005]. However, this NPR style, which conventionally tries to simulate a 2D illustration, fused with the the stereoscopic 3D visualization can break the 3D perception of the virtual contents. This work aims to study the 3D perception of virtual contents represented using NPR techniques, in order to evaluate the influence of NPR in the 3D perception when used with stereoscopic information visualization. The stereoscopic NPR visualization was applied in VIDA, a system for the study of anatomic structures that enables the stereoscopic visualization and interaction with virtual objects [Tori et al. 2009], in order to proceed the user tests, the figure 1c show the system used in the test.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[information visualization]]></kw>
			<kw><![CDATA[non-photorealistic rendering]]></kw>
			<kw><![CDATA[real-time rendering]]></kw>
			<kw><![CDATA[stereoscopy]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Color, shading, shadowing, and texture</descriptor>
				<type>S</type>
			</primary_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010384</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Texturing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010384</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Texturing</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2264613</person_id>
				<author_profile_id><![CDATA[81414620439]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Daniel]]></first_name>
				<middle_name><![CDATA[M.]]></middle_name>
				<last_name><![CDATA[Tokunaga]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Interlab Escola Polit&#233;cnica da USP]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264614</person_id>
				<author_profile_id><![CDATA[81418594392]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Cl&#233;ber]]></first_name>
				<middle_name><![CDATA[G.]]></middle_name>
				<last_name><![CDATA[Corr&#234;a]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Interlab Escola Polit&#233;cnica da USP]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264615</person_id>
				<author_profile_id><![CDATA[81414599707]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Ricardo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nakamura]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Interlab Escola Polit&#233;cnica da USP]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264616</person_id>
				<author_profile_id><![CDATA[81350602642]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[F&#225;tima]]></first_name>
				<middle_name><![CDATA[L. S.]]></middle_name>
				<last_name><![CDATA[Nunes]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Interlab Escola Polit&#233;cnica da USP]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264617</person_id>
				<author_profile_id><![CDATA[81319502432]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Romero]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tori]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Interlab Escola Polit&#233;cnica da USP]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>2384109</ref_obj_id>
				<ref_obj_pid>2384060</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[
Tietjen, C., Isenberg, T., and Preim, B. 2005. Combining silhouettes, surface, and volume rendering for surgery education and planning. In <i>Planning, in: IEEE/Eurographics Symposium on Visualization</i>, Springer, 303--310.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[
Tori, R., Nunes, F. L. S., Nakamura, R., Bernardes, J. L., Corr&#234;a, C. G., and Tokunaga, D. M. 2009. Design de intera&#231;&#227;o para um atlas virtual de anatomia usando realidade aumentada e gestos. In <i>Interactions South America 09: Anais do Congresso de Design de Intera&#231;&#227;o</i>, IXDA-SP, 205--216.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Non-Photorealistic Rendering in Stereoscopic 3D Visualization Daniel M. Tokunaga* Cl´eber G. Corr ea 
Ricardo Nakamura F´atima L. S. Nunes Romero Tori Interlab Escola Polit´ ecnica da USP (a) .rst NPR style 
(b) second NPR style (c) VIDA system in anaglyph (d) The four styles used in the test Figure 1: Actual 
Results. CR Categories: I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism Color, shading, 
shadowing, and texture; Keywords: non-photorealistic rendering, stereoscopy, information visualization, 
real-time rendering 1 Introduction Spatial visualization of virtual contents appears to be, with the 
ap­pearance of stereoscopic displays, the next step for increasing im­mersion in visual output. This 
kind of visualization can be used to facilitate the understanding of complex informations, like anatomic 
structures. One approach currently adopted for this purpose is the use of non-photorealisitic rendering 
(NPR), like proposed by Tiet­jen, Isenberg and Preim [2005]. However, this NPR style, which conventionally 
tries to simulate a 2D illustration, fused with the the stereoscopic 3D visualization can break the 3D 
perception of the virtual contents. This work aims to study the 3D perception of virtual contents represented 
using NPR techniques, in order to evaluate the in.uence of NPR in the 3D perception when used with stereoscopic 
information visualization. The stereoscopic NPR visu­alization was applied in VIDA, a system for the 
study of anatomic structures that enables the stereoscopic visualization and interaction with virtual 
objects [Tori et al. 2009], in order to proceed the user tests, the .gure 1c show the system used in 
the test. 2 System Prototype and Initial User Tests The 2D illustration-like NPR style was generated 
using diagonal black stripes to represents the light shading effect and applying sil­houettes lines to 
the object. The colors of the object were set to be the material diffuse value or variate according the 
global y posi­tion. To generate these stripes the global position, of each pixel ren­dered, was used 
with GLSL shader. Figure 1a illustrates this non­photorealistic style. Figure 1b illustrates another 
non-photorealistic style created for the user test that increases the object shading ef­fect, repeating 
the same approach that generated Figure 1a two or more times with different stripes thickness values 
and different il­lumination values. Preliminary user tests were done to verify the 3D perception of the 
stylized objects. In this test, users were asked to compare the 3D perception of the different styles 
(the conventional rendering *e-mail: dmtokunaga@acm.org through OpenGL without and with object silhouettes 
and the two NPR styles previously described; all the styles are presented with anaglyph rendering in 
.gure 1d) two by two. After that, the styles were ordered according the users comparison. This test was 
applied to 14 users including students of computer engineering and design college. The results indicate 
that the 3D perception was worse in the two NPR styles than the conventional OpenGL render. Moreover, 
50% of the users noticed a difference of 3D perception between the conventional OpenGL render with and 
without silhouette, and all of these users claimed that the perception was better without silhouette, 
althought they commented they were not distracted by silhouette of the object. 3 Discussion and Conclusion 
The preliminary user tests give us strong evidence that our hypoth­esis, that NPR can break the 3D perception, 
is true. A possible explanation of this fact is that the abstraction of the object surface, created by 
the NPR, decreases the number of visual cues presented to the user to estimate the spatial information. 
However, this hy­pothesis does not match with the results of the comparison between the conventional 
render with and without silhouette. So, another hypothesis is that 3D perception of a rendered object 
is directly proportional to its photorealism. However, this hypothesis can not be validated with the 
current results, since many users reported that there is no difference between the two rendering techniques, 
and all the users did not complain about the silhouette. Future works in­clude tests with other NPR techniques, 
like hatching, that give more clues of the object surface, formal user tests with different stereo­scopic 
techniques, to verify the hypotheses raised in this work and use of the results obtained in this work 
in interactive user interface design. References TIETJEN, C., ISENBERG, T., AND PREIM, B. 2005. Combining 
silhouettes, surface, and volume rendering for surgery education and planning. In Planning, in: IEEE/Eurographics 
Symposium on Visualization, Springer, 303 310. TORI, R., NUNES, F. L. S., NAKAMURA, R., BERNARDES, J. 
L., CORR EA , C. G., AND TOKUNAGA, D. M. 2009. Design de interac¸ao para um atlas virtual de anatomia 
usando realidade au­ mentada e gestos. In Interactions South America 09: Anais do Congresso de Design 
de Interac¸ao, IXDA-SP, 205 216. Copyright is held by the author / owner(s). SIGGRAPH 2010, Los Angeles, 
California, July 25 29, 2010. ISBN 978-1-4503-0210-4/10/0007  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1836986</article_id>
		<sort_key>1410</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>132</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>18</seq_no>
		<title><![CDATA[Photo zoom]]></title>
		<subtitle><![CDATA[high resolution from unordered image collections]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836845.1836986</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1836986</url>
		<abstract>
			<par><![CDATA[<p>We present a system to automatically construct high resolution images from an unordered set of low resolution photos. It consists of an automatic preprocessing step to establish correspondences between any given photos. The user may then choose one image and the algorithm automatically creates a higher resolution result, several octaves larger up to the desired resolution. Our recursive creation scheme allows to transfer specific details at subpixel positions of the original image. It adds plausible details to regions not covered by any of the input images and eases the acquisition for large scale panoramas spanning different resolution levels.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[example-based texture synthesis]]></kw>
			<kw><![CDATA[super-resolution]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.4.1</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2264618</person_id>
				<author_profile_id><![CDATA[81466648313]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Martin]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Eisemann]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[TU Braunschweig]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264619</person_id>
				<author_profile_id><![CDATA[81310501633]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Elmar]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Eisemann]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[T&#233;l&#233;com ParisTech / MPI / Saarland Univ.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264620</person_id>
				<author_profile_id><![CDATA[81100315426]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Hans-Peter]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Seidel]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MPI Informatik]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264621</person_id>
				<author_profile_id><![CDATA[81100477906]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Marcus]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Magnor]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[TU Braunschweig]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1360650</ref_obj_id>
				<ref_obj_pid>1399504</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[
Han, C., Risser, E., Ramamoorthi, R., and Grinspun, E. 2008. Multiscale texture synthesis. <i>Proc. SIGGRAPH '08 27</i>, 3, 1--8.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[
van Ouwerkerk, J. D. 2006. Image super-resolution survey. <i>Image Vision Comput. 24</i>, 10, 1039--1052.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Photo Zoom: High Resolution from Unordered Image Collections MartinEisemann * ElmarEisemann Hans-PeterSeidel 
MarcusMagnor § TUBraunschweig T´ecomParisTech/MPI/SaarlandUniv. TUBraunschweig el´ MPI Informatik 1x 
2x 4x 8x 16x 32x bilinear nn Figure 1: Our technique allows in.nite zooms into images, creating new details 
at each level. Not only from single images, but also from image collections. From left to right: zoom 
into an image with original resolution of 256 × 256 usingPhotoZoom,bilinear upsampling and nearest neighbor 
upsampling of the same region. Abstract Wepresent asystem toautomaticallyconstructhigh resolution im­ages 
from an unordered set of low resolution photos. It consists of an automatic preprocessing step to establish 
correspondences be­tweenanygivenphotos. Theusermay thenchooseone imageand the algorithm automatically 
creates a higher resolution result, sev­eral octaves larger up to the desired resolution. Our recursive 
cre­ationschemeallowsto transferspeci.cdetailsat subpixelpositions of the original image. It adds plausible 
details to regions not cov­ered by any of the input images and eases the acquisition for large scalepanoramasspanningdifferent 
resolution levels. Keywords: example-based texture synthesis, super-resolution 1 Overview Ourgoalis torelyonmultiplephotos 
toaddhigh-resolutiondetails toachosen inputphoto.Insuchaway,ausercan improveaholiday snapshot so that 
it becomes possible to zoom in to take a closer look at interestingpartsof the imagefarbeyond theoriginal 
image resolution. Starting with an unorderedphoto collection of arbitrary images, our system automatically 
arranges them in a dependency graph that describes which photograph contains details of another one. 
The user then chooses any photo and the system seamlessly enhances itwith thefounddetailsupto thedesiredresolution,using 
asynergybetweenexample-basedtexturesynthesis, like[Hanet al. 2008], and super-resolution,[vanOuwerkerk2006]. 
Ourwork addresses thefollowing challenges: Establishment of reliable correspondences between pho­tographs 
inunorderedphotocollections,even ifdirectfeature matching would fail by making use of an automatically 
cre­ateddependencygraph using advancedfeature matching;  Artifactfreeblending of(potentiallyoverlapping) 
imagesat different resolutions, taken with different cameras, different focal length, white balancing 
or color aberrations by a gradi­entdomainblending technique;  Transferanddetailenhancementbyinformationexchangebe­tweenphotoswherenospeci.cdetailsareavailable 
througha  * e-mail: eisemann@cg.tu-bs.de e-mail: eisemann@telecom-paristech.fr e-mail:seidel@mpi-inf.mpg.de 
§e-mail:magnor@cg.tu-bs.de new multiscale texture synthesis algorithm based on discrete optimization(Fig.2); 
 Amajordif.culty is thathumanobserversareverysensitive toarti­facts inrealworld images.Hence,previoussystemsoptedforuser­supported 
solutions, whereas wepresent afully automatic method. We believe that loosening the restrictions of super-resolution 
from equivalence to theoriginalimage,whendownsampled, tosimilarity to the original image, opens up a 
new interesting .eld of research, that has the possibility to overcome limitations of classic super­resolution 
approaches.  Figure 2: Optimization procedure: Color values are optimized by improving coherence of 
neighboring pixels. For each pixel from p s 3 × 3 neighborhood, its 5 × 5 neighborhood is extracted and 
thebestmatchesare found in thecandidate images(N m(p)k ,gray gridson theright).Theneighborhoods from 
theshiftedcenterpixel (N m(p+.)k -., dotted region around red pixels on the right) are then compared 
to p s originalneighborhood(Np)andp is replaced with itsbest match.  References HAN, C., RISSER, E., 
RAMAMOORTHI, R., AND GRINSPUN, E. 2008. Multiscale texture synthesis. Proc.SIGGRAPH 0827,3, 1 8. VAN 
OUWERKERK, J. D. 2006. Image super-resolution survey. ImageVisionComput.24,10,1039 1052. Copyright is 
held by the author / owner(s). SIGGRAPH 2010, Los Angeles, California, July 25 29, 2010. ISBN 978-1-4503-0210-4/10/0007 
 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1836987</article_id>
		<sort_key>1420</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>133</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>19</seq_no>
		<title><![CDATA[Screen-space Percentage-Closer Soft Shadows]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836845.1836987</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1836987</url>
		<abstract>
			<par><![CDATA[<p>We present an algorithm for computing Percentage-Closer Soft Shadows inside a screen-space rendering loop. Our algorithm is faster than traditional soft shadows based on percentage closer filtering, while providing soft shadows of similar visual quality. It combines naturally with a deferred shading pipeline, making it an ideal choice for video games. This algorithm is not only faster, but allows the use of larger shadow maps without dramatically affecting the rendering speed.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Color, shading, shadowing, and texture</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010384</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Texturing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2264622</person_id>
				<author_profile_id><![CDATA[81466647347]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Mahdi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[MohammadBagher]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University College London and INRIA Grenoble - Rh&#244;ne-Alpes and CNRS and Universit&#233; de Grenoble, Laboratoire Jean Kuntzman]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264623</person_id>
				<author_profile_id><![CDATA[81100016395]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Jan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kautz]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University College London]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264624</person_id>
				<author_profile_id><![CDATA[81100061765]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Nicolas]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Holzschuch]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[INRIA Grenoble - Rh&#244;ne-Alpes and CNRS and Universit&#233; de Grenoble, Laboratoire Jean Kuntzman]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264625</person_id>
				<author_profile_id><![CDATA[81100200748]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Cyril]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Soler]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[INRIA Grenoble - Rh&#244;ne-Alpes and CNRS and Universit&#233; de Grenoble, Laboratoire Jean Kuntzman]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1187153</ref_obj_id>
				<ref_obj_pid>1187112</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[
Fernando, R. 2005. Percentage-closer soft shadows. In <i>SIGGRAPH '05: ACM SIGGRAPH 2005 Sketches</i>, ACM, New York, NY, USA, 35.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[
Pham, T. Q., and van Vliet, L. J. 2005. Separable bilateral filtering for fast video preprocessing. <i>Multimedia and Expo, 2005. ICME 2005. IEEE International Conference on</i>.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1572784</ref_obj_id>
				<ref_obj_pid>1572769</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[
Robison, A., and Shirley, P. 2009. Image space gathering. In <i>HPG '09: Proceedings of the Conference on High Performance Graphics 2009</i>, ACM, New York, NY, USA, 91--98.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>939190</ref_obj_id>
				<ref_obj_pid>938978</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[
Tomasi, C., and Manduchi, R. 1998. Bilateral filtering for gray and color images. In <i>ICCV '98: Proceedings of the Sixth International Conference on Computer Vision</i>, IEEE Computer Society, Washington, DC, USA, 839.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Screen-Space Percentage-Closer Soft Shadows Mahdi MohammadBagher § Jan Kautz Nicolas Holzschuch § 
Cyril Soler § University College London INRIA Grenoble -Rh § CNRS and Universit´ one-Alpes e de Grenoble, 
Laboratoire Jean Kuntzman Abstract We present an algorithm for computing Percentage-Closer Soft Shadows 
inside a screen-space rendering loop. Our algorithm is faster than traditional soft shadows based on 
percentage closer .l­tering, while providing soft shadows of similar visual quality. It combines naturally 
with a deferred shading pipeline, making it an ideal choice for video games. This algorithm is not only 
faster, but allows the use of larger shadow maps without dramatically affecting the rendering speed. 
1 Method Our algorithm, SSPCSS performs all the computations in screen space, while PCSS [Fernando 2005] 
performs them on the shadow map, i.e., in light space. Our algorithm performs the following steps: Preparation: 
we render the auxiliary buffers: the scene depth map, the shadow map, the hard shadow map (the set of 
points in screen space that are in shadow) and the projected shadow map (for each point in shadow in 
screen space, it stores the corresponding closest blocker distance from the shadow map).  Blocker search: 
we scan the projected shadow map and, for each pixel in screen space, .nd the average distance of the 
objects blocking the light source (non-blocker points should just be ignored).  (dpoint -znear )Wsearch 
dpoint Wlight = Penumbra estimation: we estimate the penumbra size for each pixel, using the average 
blocker depth computed at the previous step. Wpenumbra.dscreen Wscreen penumbra = deye 1 (dreceiver -dblocker 
) , where dscreen = f ov , Wpenumbra = Wlight . 2tan() dblocker 2 Shadow .ltering: we .lter the hard 
shadow map by a variable-size .lter, whose diameter is the penumbra size com­puted in the previous step. 
 2 Screen-Space Edge-Aware Filtering The .ltering performed for blocker search and shadow .ltering steps 
need to be edge-aware, since we lose information about edges in screen space. We have used cross-bilateral 
.lters [Tomasi and Manduchi 1998] for this purpose, but any other edge-aware .lter could also be used. 
We used a set of pre-computed 1D separable Gaussian weights or a 2D random Poisson disk as the domain 
.lter in the cross-bilateral .lter. The range .lter is a function of the depth difference in camera space 
between two points. We used two different bilateral .lters: Separable Cross-Bilateral Gaussian Filter: 
a highly ef.cient, but less accurate .lter [Pham and van Vliet 2005]. Cross-Bilateral Poisson Filter: 
an accurate but more computation­ally intensive .lter using a random Poisson disk. It is still faster 
than (a) time as a function of screen resolution (b) SSPCSS compared to PCSS (c) SSPCSS (d) PCSS the 
PCSS algorithm using the same Poisson disk on the shadow map test results. Separable Gaussian .ltering 
is much faster than 2D Poisson .lter­ing because a separable Gaussian .lter takes 2(n+1) samples, com­pared 
to n2 samples for a 2D Poisson .lter. 3 Disscussion and Results Our work uses almost the same idea as 
in [Robison and Shirley 2009], which has focused on image space gathering by ray-tracing, but we have 
focused on rasterization on GPU to acheive higher frame rates for real-time rendering applications such 
as games. The computation time for our Screen-Space PCSS algorithm varies linearly with the total number 
of pixels on the screen. The compu­tation time is almost independent from the shadow map resolution. 
The cross-bilateral separable Gaussian .lter is much faster than the cross-bilateral Poisson .lter. More 
importantly, its performances are almost independent from the .lter radius.  References FERNANDO, R. 
2005. Percentage-closer soft shadows. In SIGGRAPH 05: ACM SIGGRAPH 2005 Sketches, ACM, New York, NY, 
USA, 35. PHAM, T. Q., AND VAN VLIET, L. J. 2005. Separable bilateral .ltering for fast video preprocessing. 
Multimedia and Expo, 2005. ICME 2005. IEEE International Conference on. ROBISON, A., AND SHIRLEY, P. 
2009. Image space gathering. In HPG 09: Proceed­ings of the Conference on High Performance Graphics 2009, 
ACM, New York, NY, USA, 91 98. TOMASI, C., AND MANDUCHI, R. 1998. Bilateral .ltering for gray and color 
im­ages. In ICCV 98: Proceedings of the Sixth International Conference on Computer Vision, IEEE Computer 
Society, Washington, DC, USA, 839. Copyright is held by the author / owner(s). SIGGRAPH 2010, Los Angeles, 
California, July 25 29, 2010. ISBN 978-1-4503-0210-4/10/0007  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1836988</article_id>
		<sort_key>1430</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>134</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>20</seq_no>
		<title><![CDATA[WBSDF for simulating wave effects of light and audio]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836845.1836988</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1836988</url>
		<abstract>
			<par><![CDATA[<p>Diffraction is a common phenomenon in nature when dealing with small scale occluders. It can be observed on biological surfaces, such as feathers and butterfly wings, and man--made objects like rainbow holograms. In acoustics, the effect of diffraction is even more significant due to the much longer wavelength of sound waves. In order to simulate effects such as interference and diffraction within a ray--based framework, the phase of light or sound waves needs to be integrated.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.6.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061.10010063</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures->Computational geometry</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2264626</person_id>
				<author_profile_id><![CDATA[81365598648]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Tom]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Cuypers]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Hasselt University - tUL - IBBT]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264627</person_id>
				<author_profile_id><![CDATA[81548005481]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Se]]></first_name>
				<middle_name><![CDATA[Baek]]></middle_name>
				<last_name><![CDATA[Oh]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MIT]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264628</person_id>
				<author_profile_id><![CDATA[81335491115]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Tom]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Haber]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Hasselt University - tUL - IBBT]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264629</person_id>
				<author_profile_id><![CDATA[81100093388]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Philippe]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bekaert]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Hasselt University - tUL - IBBT]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264630</person_id>
				<author_profile_id><![CDATA[81100022847]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Ramesh]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Raskar]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MIT MediaLab]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[
Oh, S. B., Kashyap, S., Garg, R., Chandran, S., and Raskar, R. 2010. Rendering wave effects with augmented light fields. <i>EuroGraphics</i>.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>311546</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[
Stam, J. 1999. Diffraction shaders. In <i>Proceedings of SIGGRAPH 1999</i>, ACM Press / ACM SIGGRAPH, Computer Graphics Proceedings, Annual Conference Series.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[
Zhang, Z., and Levoy, M. 2009. Wigner distributions and how they relate to the light field. In <i>IEEE Internatinoal Conference on Computational Photography</i>.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 WBSDF for Simulating Wave Effects of Light and Audio Tom Cuypers1 Se Baek Oh2 Tom Haber1 Philippe Bekaert1 
Ramesh Raskar3 1 Hasselt University -tUL -IBBT 2 Mechanical Engineering 3 Camera Culture Group Expertise 
Centre for Digital Media  Figure 1: We propose the Wave BSDF (WBSDF) for rendering wave phenomena in 
light and audio. The WBSDF behaves as a lo­cal scattering function, while simulating interference globally, 
and allows for easy integration into traditional ray based methods. 1 Introduction Diffraction is a common 
phenomenon in nature when dealing with small scale occluders. It can be observed on biological surfaces, 
such as feathers and butter.y wings, and man made objects like rainbow holograms. In acoustics, the effect 
of diffraction is even more signi.cant due to the much longer wavelength of sound waves. In order to 
simulate effects such as interference and diffrac­tion within a ray based framework, the phase of light 
or sound waves needs to be integrated. We introduce a novel method for creating Bidirectional Scattering 
Distribution Functions (BSDFs) that ef.ciently simulate diffraction and interference in light and sound 
in ray based frameworks. The re.ected or scattered radiance of a ray indirectly and independently encodes 
the mutual phase information among the rays, conveniently allowing for interference after multiple bounces. 
Previous work has described how to add local diffraction and interference effects to the Bidirectional 
Re.ection Distribution Function (BRDF) [Stam 1999]. However, a traveling ray does not carry phase information 
and therefore is unable to interfere in a later stage. Our BSDFs, derived from the Wigner Distribution 
Function (WDF) in wave op­tics, abstract away the complexity of phase calculations. Traditional ray based 
renderers, without modi.cations, can directly use these WBSDFs.  2 Wave Based BSDF In optics, parallel 
waves are often described as a function of spatial frequency u and position x. Any wavefront can be subdivided 
in a series of parallel waves, each with its own spatial frequency. In the paraxial zone, where the observation 
is near the optical axis, this spatial frequency is related to the wavelength and the direction of rays, 
which is normal to the wavefront, as u = (sin .)/.. When a wave is transmitted or re.ected it is transformed 
due to its mi­crostructure. As pointed out in previous research [Zhang and Levoy 2009; Oh et al. 2010], 
the light .eld L in position angle space exhibits simi­lar properties as the Wigner Distribution Function 
(WDF) represen­tation R in spatial frequency space: L(x, .) ~ R (x, (sin.)/.). MIT MIT MediaLab This 
transformation function for incoming and outgoing wavefronts in this space is 4D under the assumption 
of a thin element, which is calculated using the well studied WDF: ''x*x-i2px1' Wt(x, u)=tx + tx - e 
udx, (1) 22 where t(x) is a complex function describing amplitude and phase variation of the wave due 
to the surface structure and * denotes complex conjugate. Note that Wt(x, u) may contain positive as 
well as negative real valued coef.cients. However, after integration of all incoming rays, for example 
in a camera aperture, its value intensity always becomes non negative. We can therefore write the light 
and audio transport for wave ren­dering for thin microstructures as: Ro(x, uo)= Re(x, uo)+Wt(x, -uo 
-ui)Ri(x, ui)dui. (2) Where Ro and Ri are the incoming and outgoing light or sound and the BRDF is calculated 
using the WDF. Re is the light or sound emitted by the object. This simple method precomputes the BSDF 
for the entire microstructure, which can be used independently for each patch. Hence, the WBSDF allows 
us to use traditional ray based rendering pipelines to synthesize wave effects of light and sound. We 
can derive the light or audio transport for coherent as well as incoherent sources. Together with optimizations, 
such as impor­tance sampling and lookup tables, we can ef.ciently simulate wave phenomena in the near 
and far .eld.  3 Results We implemented this as a new material plugin for PBRT to generate diffractive 
materials. In order to make the renderer work with this new material, we only disable the security check 
for non negative radiance. No further changes were made to the framework. Using photon mapping as a global 
illumination technique, the rendering took 1700s to render on a 3GHz Core. Additionally, we used a ray 
tracer for audio rendering without phase tracking. We simulated an interesting effect where a closing 
sliding door can increase the audio intensity. These results are shown in .gure 1. References OH, S. 
B., KASHYAP, S., GARG, R., CHANDRAN, S., AND RASKAR, R. 2010. Rendering wave effects with augmented light 
.elds. EuroGraphics. STAM, J. 1999. Diffraction shaders. In Proceedings of SIGGRAPH 1999, ACM Press / 
ACM SIGGRAPH, Computer Graphics Pro­ceedings, Annual Conference Series. ZHANG, Z., AND LEVOY, M. 2009. 
Wigner distributions and how they relate to the light .eld. In IEEE Internatinoal Conference on Computational 
Photography. Copyright is held by the author / owner(s). SIGGRAPH 2010, Los Angeles, California, July 
25 29, 2010. ISBN 978-1-4503-0210-4/10/0007  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1836989</article_id>
		<sort_key>1440</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>135</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>21</seq_no>
		<title><![CDATA[WebGLU development library for WebGL]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836845.1836989</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1836989</url>
		<abstract>
			<par><![CDATA[<p>For more than a decade, rich 3D content on the web has only been available via external, often proprietary, browser plugins. However, a new standard has emerged to change this. WebGL, currently under development by the Khronos Group, is a standard specification for javascript bindings to OpenGL [Khronos 2009]. In September 2009 WebGL support made its way to development builds of Firefox 3.7. Since this time, WebGL has gained greater traction and visibility within developer communities. Although impressive demonstrations of WebGL are available [Vukicevic 2009], we believed that the creation of a development library would help kickstart interest in the creation of new applications.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>H.3.4</cat_node>
				<descriptor>World Wide Web (WWW)</descriptor>
				<type>P</type>
			</other_category>
			<other_category>
				<cat_node>I.3.4</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002951.10003317</concept_id>
				<concept_desc>CCS->Information systems->Information retrieval</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002951.10003152</concept_id>
				<concept_desc>CCS->Information systems->Information storage systems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2264631</person_id>
				<author_profile_id><![CDATA[81466647830]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Benjamin]]></first_name>
				<middle_name><![CDATA[P.]]></middle_name>
				<last_name><![CDATA[DeLillo]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Rochester Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[
Khronos, 2009. Khronos details webgl initiative to bring hardware-accelerated 3d graphics to the internet. http://www.khronos.org/news/press/releases/khronos-webgl-initiative-hardware-accelerated-3d-graphics-internet/ August.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[
Vukicevic, V., 2009. Webgl samples/demos and other bits. http://blog.vlad1.com/2009/09/21/webgl-samples/, September.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 WebGLU Development Library for WebGL Figure 1: Dynamic Shader Editing 1 Introduction For more than 
a decade, rich 3D content on the web has only been available via external, often proprietary, browser 
plugins. However, a new standard has emerged to change this. WebGL, currently un­der development by the 
Khronos Group, is a standard speci.cation for javascript bindings to OpenGL [Khronos 2009]. In Septem­ber 
2009 WebGL support made its way to development builds of Firefox 3.7. Since this time, WebGL has gained 
greater traction and visibility within developer communities. Although impressive demonstrations of WebGL 
are available [Vukicevic 2009], we be­lieved that the creation of a development library would help kick­start 
interest in the creation of new applications. In this paper, we present WebGLU, a development library 
which mitigates the sometimes esoteric complexity of real-time 3D by providing a clean but powerful API. 
WebGLU s design goals of separating rendering concerns from the overall design of an appli­cation, simplifying 
the process of implementation, and making the resulting application code more concise allows developers 
to spend more time focused on producing compelling content. WebGLU is open source under the MIT license, 
the library it­self, some examples and more information is available at http: //github.com/OneGeek/WebGLU. 
Live examples are main­tained at http://www.cs.rit.edu/ bpd9116/WebGLU/ examples. 2 Features The primary 
WebGLU Object class maintains arrays of shader attribute data and the associated WebGL buffers, it also 
keeps track of any textures. The Object will dynamically bind those buffers and textures before rendering. 
Objects can be associated hier­archically and assigned procedural or keyframed animations, even mixing 
animation types within a single hierarchy. In progress de­velopments include both physical simulation 
and behaviorally gen­erated animation. WebGLU s Shader and ShaderProgram classes parse out shader attributes 
and uniforms before compiling and automatically associates common uniforms, such as the Model-View and 
Projec­tion matrices, with the per-frame actions that must be taken for each. Custom actions are also 
supported. Compile and Link sta­tus are also tracked, with compilation and linking done as needed. *e-mail: 
Benjamin.DeLillo@mail.rit.edu 3 The WebGLU Difference When working with traditional WebGL just getting 
data into a shader is verbose and tedious. As the code below illustrates, mov­ing data to a shader can 
be tedious. This varies for different shaders, shader programs, and sets of data. This results in a brittle 
system that requires modi.cation to incorporate even small changes. We­bGLU eliminates these drawbacks 
and allows for faster and more ef.cient WebGL development. gl.bindBuffer(gl.ARRAY_BUFFER, myVertexBuffer); 
gl.bufferData(gl.ARRAY_BUFFER, myVertexDataArray, gl.STATIC_DRAW); gl.vertexAttribPointer(gl.getAttribLocation( 
gl.getAttribLocation(myShaderProgram, "vertex"), 3, gl.FLOAT, false, 0, 0); gl.enableVertexAttribArray(gl.getAttribLocation( 
gl.getAttribLocation(myShaderProgram, "vertex")); WebGLU makes things much more easy and uni.ed, keeping 
track of all the buffers, attributes, uniforms, and ensures everything that needs to be called at render 
time is called. anObject = new $W.Object($W.GL.TRIANGLES); anObject.fillArray("vertex", myVertexDataArray); 
anObject.vertexCount = myVertexDataArray.length; 4 Future Work Future features include support for complex 
multi-pass rendering and object picking. It will be possible to harness the connected na­ture of the 
internet to build traditional 3D content tools like model, animation, and shader editors which work collaboratively. 
Acknowledgements To Reynold Bailey and Joe Geigel for their support and guidance.  References KHRONOS, 
2009. Khronos details webgl initiative to bring hardware-accelerated 3d graphics to the internet. http://www.khronos.org/news/press/releases/ 
khronos-webgl-initiative-hardware-accelerated-3d-graphics-internet/ August. VUKICEVIC, V., 2009. Webgl 
samples/demos and other bits. http://blog. vlad1.com/2009/09/21/webgl-samples/, September. Copyright 
is held by the author / owner(s). SIGGRAPH 2010, Los Angeles, California, July 25 29, 2010. ISBN 978-1-4503-0210-4/10/0007 
  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1836990</article_id>
		<sort_key>1450</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>136</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>22</seq_no>
		<title><![CDATA[WorldSeed]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836845.1836990</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1836990</url>
		<abstract>
			<par><![CDATA[<p>WorldSeed introduces a fractal architecture that allows to generate and render full scale planets in real-time. Similar to existing concepts (e.g. by Szeliskit and Terzopoulos [1989] [1]) WorldSeed uses self-similar fractal subdivision to generate landscape detail. By expanding the concepts introduced by Bokeloh and Wand [2006] [2] to use triangular patches rather than rectangles, WorldSeed is capable of generating relatively distortion free spherical surfaces. 64bit integer seeds are used to generate consistent worlds, similar to a concept suggest by Teong Joo Ong et al. [2005] [3]. WorldSeed will be an integral part of a virtual reality application within the CodeVenture research project to teach basic modeling and programming skills to teenagers [4].</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2264632</person_id>
				<author_profile_id><![CDATA[81466648457]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Stefan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Elsen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Trier, Germany]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[
Richard Szeliskit and Demetri Terzopoulos - From Splines to Fractals, July 1989
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[
Martin Bokeloh and Michael Wand -- Hardware Accelerated Multi-Resolution Geometry Synthesis, March 2006
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[
Toeng Joo Ong, Ryan Saunders John Keyser, and John J. Leggett -- Terrain Generation Using Genetic Algorithms, June 2005
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[
The CodeVenture project, University of Trier, May 2010. http://codeventure.uni-trier.de
]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Copyright is held by the author / owner(s). SIGGRAPH 2010, Los Angeles, California, July 25 29, 2010. 
ISBN 978-1-4503-0210-4/10/0007 1. Abstract WorldSeed introduces a fractal architecture that allows to 
generate and render full scale planets in real-time. Similar to existing concepts (e.g. by Szeliskit 
and Terzopoulos [1989] [1]) WorldSeed uses self-similar fractal subdivision to generate landscape detail. 
By expanding the concepts introduced by Bokeloh and Wand [2006] [2] to use triangular patches rather 
than rectangles, WorldSeed is capable of generating relatively distortion free spherical surfaces. 64bit 
integer seeds are used to generate consistent worlds, similar to a concept suggest by Teong Joo Ong et 
al. [2005] [3]. WorldSeed will be an integral part of a virtual reality application within the CodeVenture 
research project to teach basic modeling and programming skills to teenagers [4]. 2. Architecture Similar 
to existing fractal systems, The kernel and rendering architecture describe the fractal landscape as 
a network of triangular surface segments. The considerably increased complexity of triangles over rectangles 
is outweighed by the possibility of generating spherical surface layouts with minimal distortion. Different 
hardware-accelerated approaches have been investigated in the course of the project. Due of the complexity 
of triangle-based layouts WorldSeed currently focuses on CPU-driven approaches. 3. Depth WorldSeed increases 
detail by subdivision of existing data, however at a considerably increased depth. The fractal depth 
of a typical WorldSeed planet ranges between 16 and 22 exponential layers, thus making the entire geometry 
rather large in its full extend. Without the use of memory limitation and prioritization strategies, 
the fractal kernel would overwhelm any available memory capacities within minutes or even seconds. 4. 
Floating point precision and sectors Planetary geometries enforce a combination of very large coordinates 
and relatively narrow differences. Unsegmented single precision floating point space is incapable of 
handling this contrast but remains essential for passing the generated geometry to present-day realtime 
rendering pipelines. To cope with this, WorldSeed uses integer offset-coordinates to locate the point 
of origin of fractal planets, their segments, groups or individual objects, as well as cameras. The currently 
applied sector edge length of 50km allows the construction of entire solar systems in 32 bit sector space. 
Using 64 bit instead would expand this range to a thousand times the size of Milky Way. 5. ConfigurationOne 
of WorldSeed's primary characteristics is the ability to construct entire planets from an outlining configuration 
and a singular 64bit integer seed. Given identical configuration and seed the exact same planet is generated, 
thus making it interesting for distributed environments. The outlining configuration data incorporates 
a number of attributes such as basic radius and height variance, noise-level, etc. Additionally a planetary 
configuration specifies which visual class to apply to the surface of a new instance. Such a visual class 
provides information regarding the interpretation of the final fractal generated by the kernel, i.e. 
shaders, textures or atmospheric and water coloring. 6. Features The used techniques primarily attempt 
to shape earth-like globes, covered by stacked appearance layers in order to imitate natural planetary 
surfaces. The fractal kernel uses multiple differently paced noise channels, both interdependent and 
independent, in order to generate a manifold planet surface. The noise channels are compiled to derive 
a vertex's final height, water availability and layer coverage (rock, plant, sand, snow). By doing so 
the resulting landscape appears sufficiently diversified without creating repeating patterns as is typical 
for single-channel fractal maps. Recent enhancements also added macroscopic impact craters to the arsenal 
of features, thus opening up the possibility of further development towards image based imprinting techniques. 
7. References [1] RICHARD SZELISKIT AND DEMETRI TERZOPOULOS -From Splines to Fractals, July 1989 [2] 
MARTIN BOKELOH AND MICHAEL WAND Hardware Accelerated Multi-Resolution Geometry Synthesis, March 2006 
[3] TOENG JOO ONG, RYAN SAUNDERS JOHN KEYSER, AND JOHN J. LEGGETT Terrain Generation Using Genetic 
Algorithms, June 2005 [4] The CodeVenture project, University of Trier, May 2010. http://codeventure.uni-trier.de 
http://syssoft.uni trier.de/worldseed Copyright &#38;#169; 2010 Stefan Elsen
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	</section>
	<section>
		<section_id>1836991</section_id>
		<sort_key>1460</sort_key>
		<section_seq_no>10</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Virtual/Augmented Reality]]></section_title>
		<section_page_from>137</section_page_from>
	<article_rec>
		<article_id>1836992</article_id>
		<sort_key>1470</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>137</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[A framework for multifunctional Augmented Reality based on 2D barcodes]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836845.1836992</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1836992</url>
		<abstract>
			<par><![CDATA[<p>The field of Augmented Reality (AR) has grown and progressed remarkably in recent years and many useful AR applications have been developed focusing on different areas such as game and education. However, most of these AR systems are designed for closed applications with limited number of users and restricted 3D contents. They are inappropriate for public environment with diverse 3D contents due to the following issues: (1) <i>Limited number of markers</i>. To ensure recognition accuracy, the number of markers is typically limited. (2) <i>Marker registration</i>. Pervasive AR systems often require a registration process each time a new marker is included in the systems. (3) <i>Content management</i>. Traditional AR systems are closed systems with all of their contents stored in a system server. This mechanism is inefficient for public systems with a huge volume of 3D contents. (4) <i>Special Markers</i>. The markers of traditional AR systems are often designed with particular patterns. They are not public or universal patterns.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>H.5.1</cat_node>
				<descriptor>Artificial, augmented, and virtual realities</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010392</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Mixed / augmented reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010866</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Virtual reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010866</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Virtual reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2264633</person_id>
				<author_profile_id><![CDATA[81100134731]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Tai-Wei]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kan]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Yuan Ze University, Taiwan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264634</person_id>
				<author_profile_id><![CDATA[81100401308]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Chin-Hung]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Teng]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Yuan Ze University, Taiwan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1599321</ref_obj_id>
				<ref_obj_pid>1599301</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Gabriel, M. C. C. 2009. Mobile Tagging and Mixed Realities. In <i>SIGGRAPH '09</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 A Framework for Multifunctional Augmented Reality Based on 2D Barcodes Tai-Wei Kan Chin-Hung Teng* 
Yuan Ze University, Taiwan Figure 1: (a) A QR Code and its associated 3D model, the human body, on the 
screen of a mobile device. (b)(c) Integrated AR demonstration with another QR Code representing the model 
such as shirt or pants. (d) Further AR demonstration when all of the three QR Codes are presented. 1 
Introduction The .eld of Augmented Reality (AR) has grown and progressed remarkably in recent years and 
many useful AR applications have been developed focusing on different areas such as game and edu­cation. 
However, most of these AR systems are designed for closed applications with limited number of users and 
restricted 3D con­tents. They are inappropriate for public environment with diverse 3D contents due to 
the following issues: (1) Limited number of markers.To ensure recognition accuracy, the number of markers 
is typically limited. (2) Marker registration. Pervasive AR systems often require a registration process 
each time a new marker is in­cluded in the systems. (3) Content management. Traditional AR systems are 
closed systems with all of their contents stored in a sys­tem server. This mechanism is inef.cient for 
public systems with a huge volume of 3D contents. (4) Special Markers. The markers of traditional AR 
systems are often designed with particular patterns. They are not public or universal patterns. Gabriel[Gabriel 
2009] proposed a mixed reality idea that addresses the contents via 2D barcodes. Because of the universality 
of 2D bar­codes, the issues of limited number of markers, marker registration and special markers can 
be automatically eliminated if the barcode can serve as the marker of an AR system. However, Gabriel 
neither provided a feasible approach to treating 2D barcodes as AR mark­ers nor gave a complete framework 
how to apply her idea for public applications. In this work we aim at developing a comprehensive framework 
that can integrate 3D contents from different providers into an AR demonstration. The main purpose of 
this framework is that we want to separate content providers from sophisticated AR technology so that 
they only need focusing on content creation and getting rid of the technology issues of setting up an 
AR system. We expect that with this system different contents can be integrated seamlessly in the AR 
system even the content providers do not know what the other contents to be integrated are. To achieve 
this goal, all the control information required for AR joint demonstra­tion must be de.ned and embedded 
in the marker. In our system, we use a universal 2D barcode, the QR Code, as our AR marker as its information 
capacity is suf.cient large to afford our applica­tion. Currently, we have developed a tracking mechanism 
to esti­mate the 3D position of QR Codes so that we can place the contents on the QR Codes. On the other 
hand, AR content is addressed via a URL embedded in the code, content management is quite simple as contents 
are distributed on local servers and managed by content providers. *e-mail:chteng@saturn.yzu.edu.tw 
2 Proposed Framework As indicated above, the major goal of our framework is the joint demonstration of 
different contents. Figure 1 shows a example how to employ our framework for clothes demonstration. When 
only one QR Code is presented before the camera, the content repre­sented by the QR Code is displayed. 
If there are two QR Codes, the two contents can be integrated to achieve a seamless AR joint demonstration. 
Note that in this example, the contents may be pro­vided and managed by different vendors. To approach 
the results, we must provide suf.cient control information for integrating the two contents in the two 
QR Codes. At present, we adopt the Ex­tensible Markup Language (XML) to de.ne the control informa­tion. 
After a comprehensive survey of AR and QR Code applica­tions, we found that the required XML tags can 
be classi.ed into three categories: content addressing, joint demonstration, and con­tent manipulation. 
Content addressing indicates the tags are used to address the required contents, typically a URL. To 
achieve mul­tiresolution content exhibition, we can use different tags, such as <hiRes> and <lowRes> 
to index the contents with different res­olutions according to the available bandwidth. The tags of joint 
demonstration indicate that they are used to control the integration of different contents. Contents 
can be de.ned as major or minor with the major content representing the main body of AR demon­stration 
and the minor is the content being attached to the major. This can be achieved by a tag de.ning the hierarchical 
level of the content. To realize seamless content integration, we also need some tags such as <anchor_point> 
to de.ne possible attaching points and <scale> to adjust the relative dimensions of different contents. 
The tags of content manipulation indicate that the QR Code can be used to control the manipulation of 
content so as to achieve interactive AR demonstration. For example, if the content is an animation, the 
tags may be de.ned to control the playback of the animation, such as forward and backward, via the manipulation 
of QR Code. In Japan, QR Code is widely used for storing product information, thus a direct application 
of our framework is product joint demon­stration. However, the applications of proposed framework are 
not limited to this. Our system opens up a novel interaction and new idea for AR systems with 2D barcodes. 
Based on this framework, many useful AR applications can be developed in the areas of com­merce, industry 
and many others. References GABRIEL, M. C. C. 2009. Mobile Tagging and Mixed Realities. In SIGGRAPH 
09. Copyright is held by the author / owner(s). SIGGRAPH 2010, Los Angeles, California, July 25 29, 
2010. ISBN 978-1-4503-0210-4/10/0007  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1836993</article_id>
		<sort_key>1480</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>138</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[A shape-free, designable 6-DoF marker tracking method]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836845.1836993</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1836993</url>
		<abstract>
			<par><![CDATA[<p>Markers are widely used for camera-based interaction. Yet, most of the marker tracking methods have considerable limitations in shapes and designs; they are not usually visually meaningful to the users. Such an issue on visually communicative designs can be very important to provide visual cues in a mobile/pervasive environment where a user must first notice a marker and understand its meaning before initiating interaction, unlike in an immersive environment with a head-mounted-display that keeps displaying information on the detected markers.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.4.8</cat_node>
				<descriptor>Tracking</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010225.10010227</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision tasks->Scene understanding</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245.10010253</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems->Tracking</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2264635</person_id>
				<author_profile_id><![CDATA[81320493719]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Hiroki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nishino]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[National University of Singapore]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1518990</ref_obj_id>
				<ref_obj_pid>1518701</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Costanza, E. &amp; Huang, J (2009). "Designable Visual Markers", in Proceeding. of SIGCHI09, 1879--1888]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>615522</ref_obj_id>
				<ref_obj_pid>615254</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Wolfson, H. J. &amp; Rigoutsos, I (1997). "Geometric Hashing: An Overview", IEEE Computational Science and Engineering. 4(4), 10--21]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 AShape-Free, Designable 6-DoFMarkerTracking Method HirokiNISHINO NUS Graduate School for Integrative 
Sciences &#38; Engineering, National UniversityofSingapore g0901876@nus.edu.sg 1. Introduction Markers 
are widely used for camera-based interaction. Yet, most of the marker tracking methods have considerable 
limitations in shapes and designs; they are not usually visually meaningful to the users. Such an issue 
on visually communicative designs can be very important to provide visual cuesin a mobile/pervasive environment 
where a user must first notice a marker and understand its meaning before initiating interaction, unlike 
in an immersive environment with a head-mounted-display that keeps displayinginformation onthe detected 
markers. We have developed a novel tracking method with shape­free designable markers, based on topological 
region adjacency and a method similar to geometric hashing. It can also distinguish the different markers 
with the same topological structure and provide 6-DoF pose estimation. Lack of such features are significant 
deficits of existing topology-based methods. 2. Related Work We briefly review D-Touch system by Costanza 
et al. [1], since D-Touch is also based on topological region adjacency for its marker recognition and 
is focusing on visuallycommunicative designs similar to our newmethod. Figure 1 (left) shows an example 
of a D-Touch marker, taken from[1]. The topological regionadjacencygraph, (or the containership information 
of black/white regions) of this marker is shown on the right. Such a topological structure is sought 
from the binarized/segmentized input image to recognize a marker. The branch nodes of this tree are sorted 
by the number of their leaf nodes. Adding the color to this sequence, its marker ID is obtained. (In 
Figure 1, the ID is 1;1;1;4;black asshown .) Figure 1. A D-Touch marker (left), its topological region 
adjacency graph and ID(right)[1] Such a topology-based method gives considerable freedom in marker design, 
since the shapes of each region are not involved in recognition. However, such a method based only on 
topological information is inherently incapable of distinguishing two different markers with the same 
topological structure. It also cannot provide 6-DoF pose estimation, since topological information doesn 
t convey position information; to estimate pose in 3D space from a given input image, the matching of 
at least 4 points betweenthe input and the marker coordinate is required.  3. DescriptionofOurMethod 
Figure 2 describes our tracking method. Our method is also based on topology, yet we use the topological 
information onlyto find marker candidates (Figure 2. above). To determine the actual marker detected 
among these candidates, we apply a method similar to geometric hashing [2], voting the centroids of the 
leaf nodes in the region adjacencygraphto the hash tables (Figure 2. below). The models for each marker 
are prepared by projecting its leaf nodes to the hash table bins. In this phase, we first pick up 4 leaf 
nodes from the candidate, as a basis to find a homography between the input image and the model. The 
other leaf nodes are used to vote to test if the homography and the model are correct. If a leaf node 
can be projected to a bin where there is any preregistered entry, it is counted as a valid vote. If not, 
the vote simplyfails.  .select candidates by topolology  inputim age . find the model and the homography 
by voting. The answer. Figure 2. Candidates are selected by topological information (above). A model 
is determined by voting to the bin. (below). Notice the two markersabove share thesame topological structure. 
Among those possible combinations of 4-point basis and the models, the most voted combination is selected 
as the answer (the detected marker). Now that the marker is determined and the matching of 4 points to 
the given input isknown, theycanbe used for 6-DoF pose estimation. We also utilize topological information 
to reduce the computation cost when finding a homography and voting. Matching and voting are tried only 
between those points that are topologically plausible. Such a use of topological information is quite 
important in our method, since unlike traditional geometric-hashing, our method currently tries all the 
possible homographies to make marker detection and pose estimationmore robustand stable.  4. Experimental 
Application The pictures from our prototype are shown in Figure 3. Two markers with the same topological 
structures were used. The prototype runs in real-time even on a low power CPU for mobile internet devices, 
about 70ms/frame on Atom Z520-1.33GHz/WinXP in 640x480, excluding the time cost to overlay 3D models 
(for a desktop CPU, 9ms/frame onCore2Quad-2.5Ghz, WinVista).  Figure 3. Two different markers withthe 
same topological structure (left)are correctly distinguished andoverlaid3Dmodels(right2). References 
[1] Costanza, E. &#38; Huang, J (2009). Designable Visual Markers , in Proceeding. of SIGCHI09, 1879-1888 
[2] Wolfson, H.J. &#38; Rigoutsos, I (1997). Geometric Hashing: An Overview , IEEEComputationalScience 
and Engineering. 4(4), 10-21 Copyright is held by the author / owner(s). SIGGRAPH 2010, Los Angeles, 
California, July 25 29, 2010. ISBN 978-1-4503-0210-4/10/0007  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1836994</article_id>
		<sort_key>1490</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>139</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Brain activity underlying third person and first person perspective training in virtual environments]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836845.1836994</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1836994</url>
		<abstract>
			<par><![CDATA[<p>Over the years, different approaches have been explored to build effective learning methods in virtual reality but the design of effective 3D manipulation techniques still remains an important research problem. To this end, it is important to quantify behavioral and brain mechanisms underlying the geometrical mappings of the body with the environment and external objects, both within the virtual environments (VE), the real world and relative to each other. The successful mapping of such interactions entails the study of fundamental components of these interactions, such as the origin of the visuo-spatial perspective (1PP, 3PP) and how they contribute to the user's performance in the virtual environments. Here, we report data using a novel set-up exposing participants - during free navigation - with a scene view from either 3PP or the habitual first-person perspective (1PP).</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>H.5.1</cat_node>
				<descriptor>Artificial, augmented, and virtual realities</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>J.3</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010392</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Mixed / augmented reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010444</concept_id>
				<concept_desc>CCS->Applied computing->Life and medical sciences</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010866</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Virtual reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010866</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Virtual reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2264636</person_id>
				<author_profile_id><![CDATA[81466647355]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Tej]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tadi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Brain Mind Institute, Ecole Polytechnique F&#233;d&#233;rale de Lausanne (EPFL), Switzerland]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264637</person_id>
				<author_profile_id><![CDATA[81384613109]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Patrick]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Salamin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Ecole Polytechnique F&#233;d&#233;rale de Lausanne (EPFL)]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264638</person_id>
				<author_profile_id><![CDATA[81100250393]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Frederic]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Vexo]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Ecole Polytechnique F&#233;d&#233;rale de Lausanne (EPFL)]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264639</person_id>
				<author_profile_id><![CDATA[81100534488]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Daniel]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Thalmann]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Ecole Polytechnique F&#233;d&#233;rale de Lausanne (EPFL)]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264640</person_id>
				<author_profile_id><![CDATA[81466647996]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Olaf]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Blanke]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Brain Mind Institute, Ecole Polytechnique F&#233;d&#233;rale de Lausanne (EPFL), Switzerland and University Hospital, Geneva, Switzerland]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Brain Activity Underlying Third person and First person Perspective Training in Virtual Environments 
 Tej Tadi1, Patrick Salamin3, Frederic Vexo3, Daniel Thalmann3, and Olaf Blanke 1,2 1Laboratory of Cognitive 
Neuroscience, Brain Mind Institute, Ecole Polytechnique Fédérale de Lausanne (EPFL), 1015 Lausanne, Switzerland, 
2Department of Neurology, University Hospital, Geneva, Switzerland., 3Virtual Reality Lab, Ecole Polytechnique 
Fédérale de Lausanne (EPFL). tej.tadi@epfl.ch, patrick.salamin@epfl.ch, frederic.vexo@epfl.ch, daniel.thalmann@epfl.ch, 
olaf.blanke@epfl.ch 1. Introduction Over the years, different approaches have been explored to build 
effective learning methods in virtual reality but the design of effective 3D manipulation techniques 
still remains an important research problem. To this end, it is important to quantify behavioral and 
brain mechanisms underlying the geometrical mappings of the body with the environment and external objects, 
both within the virtual environments (VE), the real world and relative to each other. The successful 
mapping of such interactions entails the study of fundamental components of these interactions, such 
as the origin of the visuo-spatial perspective (1PP, 3PP) and how they contribute to the user s performance 
in the virtual environments. Here, we report data using a novel set-up exposing participants - during 
free navigation - with a scene view from either 3PP or the habitual first-person perspective (1PP). 2. 
Methods We asked 16 participants immediately after 1PP or 3PP exposure to judge the interception of a 
ball projected on the HMD at different trajectories (20cm, 60cm, &#38;150cm) from a fixed origin. The 
device consisted of a rigid backpack, a camera and a head mounted device (HMD) to which the video captured 
from the camera was relayed in real time [Salamin et al. 2006]. Brain activity was measured through continuous 
electroencephalogram (EEG) and was recorded from 16 electrodes on the scalp. 3. Results Analysis of the 
evoked potential (EP) mapping of the group averaged data from the 16 participants revealed the presence 
of two distinct brain activities between ~120ms to ~155ms and ~470ms to ~530ms post stimulus onset, whose 
duration (in ms) and strength (in µV) in the 3pp condition were significantly longer and stronger as 
compared to the 1pp purely for the ambiguous stimulus trajectory (60cm). These data reveal a functional 
modulation of the grasping space based on perspective training (3PP, 1PP) during the free walking, unrelated 
to the task and suggest that despite better distance and overview estimation in the 3PP condition, here 
the participants intercept the ball more often post 1PP training for the ambiguous trajectory. 4. Discussion 
The data gives us an insight into the mappings that are permissible and shows that exposure for as less 
as 15 minutes to a different perspective can degrade or enhance performance in different scenarios. In 
future studies, it will be important to look at effective usage of the perspectives for optimal learning 
methods in social interactions in virtual environments. Through my doctoral work, I study the functional 
and neural mechanisms underlying corporeal awareness and self-consciousness in humans using virtual reality 
in conjunction with neuroimaging techniques. This allows us to do multidisciplinary work combining cognitive 
psychology, neuroimaging, and VR. The current study gives us an insight into the malleability of the 
origin of the visuo-spatial perspectives as human interaction with the environment is a multi-modal process 
and VR can generate realistic scenarios where the user has an egocentric position similar to the real 
world. VR can be used to distort and modulate this egocentric position as well as ownership of the body 
and self location. 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1836995</article_id>
		<sort_key>1500</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>140</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[Digital display case]]></title>
		<subtitle><![CDATA[the exhibition sysytem for conveying the background information]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836845.1836995</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1836995</url>
		<abstract>
			<par><![CDATA[<p>In our research, we aim to construct an interactive exhibition system for museums to convey the background information about its exhibit, which today's museums need. Museums have to preserve their exhibits, and that was a limitation on the exhibition form. They cannot hold a quite new type of exhibition because it might jeopardize their exhibits. So they cannot do more than the exhibition with conventional display cases and panels, in other words a passive exhibition to convey the background information. Digital technologies untie them from this limitation. We can convey the background information effectively using CG in exhibitions, without jeopardizing real exhibits.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.4.0</cat_node>
				<descriptor>Image displays</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2264641</person_id>
				<author_profile_id><![CDATA[81466640651]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Takashi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kajinami]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264642</person_id>
				<author_profile_id><![CDATA[81466647429]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Oribe]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hayashi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264643</person_id>
				<author_profile_id><![CDATA[81331500452]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Takuji]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Narumi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264644</person_id>
				<author_profile_id><![CDATA[81100172183]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Tomohiro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tanikawa]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264645</person_id>
				<author_profile_id><![CDATA[81100257385]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Michitaka]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hirose]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>769964</ref_obj_id>
				<ref_obj_pid>769953</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Bimber, O., Encarnacao, L. M., and Schmalstieg, D. 2003. The virtual showcase as a new platform for augmented reality digital storytelling. <i>Proceedings of the workshop on Virtual environments 2003 39</i>, 87--95.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Digital Display Case: The Exhibition Sysytem for Conveying the Background Information Takashi Kajinami, 
Oribe Hayashi* Takuji Narumi Graduate School of Information Science and Technology, the University of 
Tokyo Graduate School of Engineering, the University of Tokyo Tomohiro Tanikawa, Michitaka Hirose Graduate 
School of Information Science and Technology, the University of Tokyo Figure 2: The appearance of the 
Figure 1: Digital Display Case system in operation 1 Introduction In our research, we aim to construct 
an interactive exhibition sys­tem for museums to convey the background information about its exhibit, 
which today s museums need. Museums have to preserve their exhibits, and that was a limitation on the 
exhibition form. They cannot hold a quite new type of exhibition because it might jeop­ardize their exhibits. 
So they cannot do more than the exhibition with conventional display cases and panels, in other words 
a passive exhibition to convey the background information. Digital technolo­gies untie them from this 
limitation. We can convey the background information effectively using CG in exhibitions, without jeopardiz­ing 
real exhibits. There are some researches about this kind of exhibition systems fea­turing digital technology 
like Virtual Showcase[Bimber et al. 2003]. But few of them are actually introduced into museums. That 
is be­cause they are greatly different from conventional display cases. Museums have a kind of knowhow 
about their exhibition. They know how to use conventional display cases ef.ciently in their ex­hibition. 
On the other hand, they are puzzled to use a quite new devices, then they cannot build such new devices 
into the exhibi­tion effectively. Therefore in this paper, we construct the system to convey the back­ground 
information about exhibits compatible with conventional display cases and panels. We categorized the 
background infor­mation which museums want to convey into two categories, syn­chronicity and diachronicity. 
· Syncronicity: Comparison of cutaneous exhibits · Diachronicity:Temporal change of exhibit Then we consider 
how to realize the exhibition to convey these two types of information with it based on the discussion 
with the Tokyo National Museum.  2 Exposition We made these two systems. Digital Display Case We made 
a system in the shape of conven­tional display cases which realizes the exhibition with CG (Fig.1). Figure2 
shows the appearance of the system in operation. We con­structed four 3D displays into box shape. A User 
wear glasses with *e-mail: {kaji,olive}@cyber.t.u-tokyo.ac.jp e-mail: narumi@cyber.t.u-tokyo.ac.jp e-mail: 
{tani,hirose}@cyber.t.u-tokyo.ac.jp   sensor, and the system measures the position of view point. Based 
on this, it calculate the images to display, and he can see around the exhibit as if it were in the case. 
he also can handle the exhibit by handling the cylindrical object and he can see it in greater detail 
by bring it near. Interactive Panel We also made a panel system (Fig.3). This can present many exhibits 
at the same time, and user can manipulate the contents on it with touch panel. It also has the synchronization 
function with the Case system. Then, we consider the way to realize the background-telling exhibi­tion 
with this system. We made exhibitions about the synchronicity and diachronicity of Doguu, which is Japanese 
ancient clay doll. We decided these contents based on the discussion with Tokyo Na­tion Museum. Exhibition 
of Synchronicity Doguus are generally made during the Joumon era, and there are variety of Doguus excavated 
in vari­ety of places in Japan. In this point Doguus has synchronicity. To convey this synchronicity, 
we displays Doguus made at the same period on a map of Japan like conventional panels (Fig.3). In ad­dition 
to this, when user select one of them, he can appreciate it in the Case System. With this, user can connect 
the information on the panel to the exhibit itself more ef.ciently. He can look down at exhibits on the 
panel and understand common or different fea­tures among them, and also appreciate one of them with his 
whole attention by selecting it. Exhibition of Diachronicity Doguus are made in ancient times, so it 
is deteriorated to some degree. Many researchers have tried to restore their original appearance. We 
can consider it as the di­achronicity of Doguus. Additionally, the atmosphere in Jomon Era that surrounded 
Doguus is also a part of their diachronicity. We use the easiness to switch the images with CG to convey 
this. We used 3D models of present appearance and past appearance of the exhibit, and switch them to 
show how it changed or was dete­riorated as time passes. We also reproduce the atmosphere of the exhibit 
by presenting it as background(Fig.4), and this shows the diachronicity around it.  References BIMBER, 
O., ENCARNACAO, L. M., AND SCHMALSTIEG, D. 2003. The virtual showcase as a new platform for augmented 
reality digital storytelling. Proceedings of the workshop on Vir­tual environments 2003 39, 87 95. Copyright 
is held by the author / owner(s). SIGGRAPH 2010, Los Angeles, California, July 25 29, 2010. ISBN 978-1-4503-0210-4/10/0007 
 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1836996</article_id>
		<sort_key>1510</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>141</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>5</seq_no>
		<title><![CDATA[Evaluating performance in immersive displays]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836845.1836996</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1836996</url>
		<abstract>
			<par><![CDATA[<p>3D Immersive visualization systems provide a novel platform to present complex datasets and virtual environments (VEs). The objective of the research presented here is to compare user-interaction and performance between two immersive displays: a low-cost, tiled, multi-screen immersive visualization system and a more expensive, continuous, immersive visualization facility. The low cost system is designed using off-the-shelf components and constructed by arranging LCD displays in a tiled hemispherical layout. The expensive system is a Rockwell-Collins semi-rigid, rear projected, continuous curved screen. With the low cost paradigm, seams are introduced into the image where the displays are tiled. We hypothesize that the tiled system presents an equivalent visual experience, despite the seams introduced by connecting the screens. Both systems will be tested through psychophysical experimentation designed to measure aspects of human performance. Proving our hypothesis will impact lower budget organizations, currently unable to afford such displays, by providing an opportunity to work with lower cost immersive visualization systems at no sacrifice to user-experience.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.4.0</cat_node>
				<descriptor>Image displays</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2264646</person_id>
				<author_profile_id><![CDATA[81466647591]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Megha]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Davalath]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Texas A&M University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264647</person_id>
				<author_profile_id><![CDATA[81466646043]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Mat]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sanford]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Texas A&M University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264648</person_id>
				<author_profile_id><![CDATA[81466648102]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Anton]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Agana]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Texas A&M University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264649</person_id>
				<author_profile_id><![CDATA[81100069081]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Ann]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[McNamara]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Texas A&M University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264650</person_id>
				<author_profile_id><![CDATA[81100304539]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Frederic]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Parke]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Texas A&M University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1549854</ref_obj_id>
				<ref_obj_pid>1549820</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Klein, E., II, J. E. S., Schmidt, G. S., Livingston, M. A., and Staadt, O. G. 2009. Measurement protocols for medium-field distance perception in large-screen immersive displays. In <i>Technical Papers, Proceedings of IEEE Virtual Reality 2009</i>, 107--113.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Evaluating Performance in Immersive Displays Megha Davalath* Mat Sanford Anton Agana Ann McNamara§ Frederic 
Parke¶ Department of Visualization, Texas A&#38;M University Figure 1: Tiled LCD immersive environment 
(left) and Continuous immersive system (right). 1 Introduction 3D Immersive visualization systems provide 
a novel platform to present complex datasets and virtual environments (VEs). The ob­jective of the research 
presented here is to compare user-interaction and performance between two immersive displays: a low-cost, 
tiled, multi-screen immersive visualization system and a more ex­pensive, continuous, immersive visualization 
facility. The low cost system is designed using off-the-shelf components and constructed by arranging 
LCD displays in a tiled hemispherical layout. The expensive system is a Rockwell-Collins semi-rigid, 
rear projected, continuous curved screen. With the low cost paradigm, seams are introduced into the image 
where the displays are tiled. We hy­pothesize that the tiled system presents an equivalent visual experi­ence, 
despite the seams introduced by connecting the screens. Both systems will be tested through psychophysical 
experimentation de­signed to measure aspects of human performance. Proving our hy­pothesis will impact 
lower budget organizations, currently unable to afford such displays, by providing an opportunity to 
work with lower cost immersive visualization systems at no sacri.ce to user­experience. 2 Approach 
The mode of interaction with immersive environments is vital to the correct perception of the data presented, 
whether it is a scienti.c visualization or a game level. Results from our experiments have the potential 
to advance our knowledge of human interaction with 3D immersive displays. This human-centered knowledge 
will in turn prove powerful when designing new interactive displays. To examine various modes of interaction 
our research is comprised of a suite of formal experiments that will compare different interaction techniques 
across the aforementioned immersive display systems. *megha@viz.tamu.edu mat@viz.tamu.edu agana@viz.tamu.edu 
§ann@viz.tamu.edu ¶parke@viz.tamu.edu  The experiments will span two years and include: Navigation 
and Way.nding: Navigation through a 3D VE us­ing a Nintendo Wii controller is evaluated in each paradigm. 
 Soccer Heading: Participants accuracy heading a soccer ball in the VE using a Nintendo Wii balance 
board is com­pared.  Exploration &#38; Map Construction: Participants draw a 2D map of the 3D VE they 
have just explored.  Driving Reaction Times: Accuracy of driving in a VE while avoiding obstacles is 
compared.  Twenty participants will perform each experiment. Performances on both immersive systems 
will be recorded, analyzed, and com­pared. Our aim is to investigate what disadvantage, if any, is present 
in the less-costly system due to image separation caused by screen borders. For this talk we present 
the results of the .rst experiment only. 3 First Experiment: Navigation &#38; Way.nding To correctly 
interact with a spatial immersive display, the user must understand the 3D VE, in particular structure 
and depth perception [Klein et al. 2009]. This experiment investigates how navigational skills might 
be affected by the physical separation of the imagery. A model of a building comprised of corridors with 
of.ces serves as the test environment for this experiment. Participants examine a 2D map of the .oor 
plan showing their initial position and end position in the environment. They then navigate the shortest 
route through the immersive 3D rendition of this environment. Navigation is con­trolled using a Nintendo 
Wii controller. The user remains stationary and simply points the controller in the direction they wish 
to travel having the freedom to control the speed at which the user travels. The faster the locomotion, 
the more con.dent the user is deemed to be in their choice. The response of interest is the time of travel 
measured in milliseconds. We compare timings in both immersive systems and will report differences between 
the two systems. References KLEIN, E., II, J. E. S., SCHMIDT, G. S., LIVINGSTON, M. A., AND STAADT, 
O. G. 2009. Measurement protocols for medium­.eld distance perception in large-screen immersive displays. 
In Technical Papers, Proceedings of IEEE Virtual Reality 2009, 107 113. Copyright is held by the author 
/ owner(s). SIGGRAPH 2010, Los Angeles, California, July 25 29, 2010. ISBN 978-1-4503-0210-4/10/0007 
  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1836997</article_id>
		<sort_key>1520</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>142</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>6</seq_no>
		<title><![CDATA[HCCMeshes]]></title>
		<subtitle><![CDATA[hierarchical-culling oriented compact meshes]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836845.1836997</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1836997</url>
		<abstract>
			<par><![CDATA[<p>Ray tracing and collision detection are widely used for providing high-quality visualizations and user interactions. In these algorithms, we need to detect intersecting primitives between two input objects (e.g., a ray and a 3D object in ray tracing and two 3D objects in collision detection). In order to efficiently detect these intersecting primitives, hierarchical traversal and culling by using bounding volume hierarchies (BVHs) are commonly used.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061.10010063</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures->Computational geometry</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2264651</person_id>
				<author_profile_id><![CDATA[81442603931]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Tae-Joon]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kim]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[KAIST]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264652</person_id>
				<author_profile_id><![CDATA[81464664097]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Yongyoung]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Byun]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[KAIST]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264653</person_id>
				<author_profile_id><![CDATA[81466646517]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Yongjin]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kim]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[POSTECH]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264654</person_id>
				<author_profile_id><![CDATA[81442602822]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Bochang]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Moon]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[KAIST]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264655</person_id>
				<author_profile_id><![CDATA[81466642529]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Seungyong]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lee]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[POSTECH]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264656</person_id>
				<author_profile_id><![CDATA[81100019061]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Sung-Eui]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yoon]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[KAIST]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1729607</ref_obj_id>
				<ref_obj_pid>1729476</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Kim, T.-J., Moon, B., Kim, D., and Yoon, S.-E. 2010. RACBVHs: Random-accessible compressed bounding volume hierarchies. <i>IEEE Trans. on Visualization and Computer Graphics 16</i>, 2, 273--286.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 HCCMeshes: Hierarchical-Culling oriented Compact Meshes Tae-Joon Kim1,Yongyoung Byun1,Yongjin Kim2, 
Bochang Moon1, Seungyong Lee2, and Sung-EuiYoon1 1 KAIST 2 POSTECH Figure 1: Applications: These .gures 
show images of applications using our HCCMesh representations. From left, we show a Whitted-style ray 
tracing of the St. Matthew, photon mapping on a transparent David model in the Sponza scene, a line-art 
style rendering of the Lucymodel re.ected on a sphere, and collision detection between the Lucyand a 
CAD turbine model 1 Introduction Ray tracing and collision detection are widely used for providing high-quality 
visualizations and user interactions. In these algorithms, we need to detect intersecting primitives 
between two input objects (e.g., a ray and a 3D object in ray tracing and two 3D objects in collision 
detection). In order to ef.ciently detect these intersecting primitives, hierarchical traversal and culling 
by using bounding vol­ume hierarchies (BVHs) are commonly used. Due to advances of model acquisition 
and computer-aided design techniques, massive models are easily generated these days. Such massive models 
can consist of hundreds of millions of triangles and thus use several gigabytes of memory. In addition, 
BVHs constructed from these massive models can use additional gigabytes of memory space. Although BVHs 
are intended to accelerate the performance of applications, the additional memory requirement of using 
BVHs can increase the working set size during the hierarchical traversal and can increase the data fetching 
time from the disk, which could negate the bene.ts of using BVHs. This high memory requirement of a BVH 
is likely to cause more serious performance issues in the coming years,given the well-known wideninggap 
between the com­putational speed and the data access speed on current commodity hardware. Only a few 
techniques have been proposed to design compact mesh and BVH representations in order to reduce the data 
access time and memory requirements during the hierarchical traversal. None of them supports various 
tree structures of BVHs, while pro­viding ef.cient hierarchical culling and a low runtime access over­head. 
Furthermore, these prior techniques do not provide enough compression ratios to handle large-scale models 
consisting of hun­dreds of millions of triangles on commodity hardware [Kim et al. 2010]. 2 Our Approach 
To compute the HCCMesh representation, we .rst construct a BVH from a mesh and then decompose the BVH 
into a single high-level BVH and multiple low-level BVHs. In order to reduce the data fetching time from 
external drives and to lower the memory require­ment of meshes and BVHs, we compute our HCCMesh representa­tion 
for each low-level BVH. Our HCCMesh representation has in­core and out-of-core parts. The in-core HCCMesh 
representation, i-HCCMesh, tightly integrates the mesh and BVH representations.We compress i-HCCMeshes 
further to reduce the expensive data access time from external drives for applications that run in an 
out-of-core mode. When a low-level BVH is requested at runtime, we fetch its cor­responding o-HCCMesh 
from an external drive, decompress it, and  Figure 2: Ray Tracing Time vs. Model Complexity: This graph 
shows the rendering time with various model complexities of the St. Matthew model shown in Fig. 1. We 
measure the performance of ray tracing with our HCCMesh, the original(Ori.), and naively compressed(NCom.)representations. 
store it in main memory as our i-HCCMesh representation which ef.ciently supports the random hierarchical 
traversal. In order to en­able a high overall performance improvement, our methods support high compression 
ratios andfast decompression performance. If all the i-HCCMeshes .t into main memory, the o-HCCMeshes 
are se­quentially fetched and decompressed into the i-HCCMeshes as the application begins. When all the 
o-HCCMeshes, but not all the i-HCCMeshes, .t into main memory, we load the o-HCCMeshes to main memory 
without anydecompression and then decompress them into the i-HCCMeshes when necessary, in order to remove 
the ex­pensive disk I/O access at runtime. Otherwise, the o-HCCMeshes are fetched on demand from the 
disk and decompressed into i-HCCMeshes while using the LRU-based memory management. 3 Advantages of Our 
Approach 1. Low memory requirement: Our i-HCCMesh and o-HCCMesh has 3.6:1 and 10.4:1 compression ratios 
on aver­age over a naively quantized representation. This low memory requirement reduces the data access 
time and the size of the working set during the hierarchical traversal. 2. High performance improvement: 
We test our method on ray tracing, photon mapping, non-photorealistic rendering, and collision detection 
(Fig. 1) and compare our method over the naively quantized representation. We can handle models ten times 
larger in these applications without the expensive disk I/O thrashing by using our representation (Fig. 
2). In the case when we can avoid the disk I/O thrashing, we can improve the performance by up to two 
orders of magnitude.  References KIM,T.-J.,MOON,B.,KIM,D., AND YOON,S.-E. 2010. RACBVHs: Random­accessible 
compressed bounding volume hierarchies. IEEE Trans. on Visualization and Computer Graphics 16, 2, 273 
286. Copyright is held by the author / owner(s). SIGGRAPH 2010, Los Angeles, California, July 25 29, 
2010. ISBN 978-1-4503-0210-4/10/0007  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1836998</article_id>
		<sort_key>1530</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>143</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>7</seq_no>
		<title><![CDATA[Meta cookie]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836845.1836998</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1836998</url>
		<abstract>
			<par><![CDATA[<p>So far, gustatory information has rarely been studied in relation to computers, even though there are lots of studies on visual, auditory, haptic and olfactory information. This scarcity of research on gustatory information has several reasons. One reason is that gustatory sensation is based on chemical signals, whose functions have not been fully understood yet. Another reason is that perception of gustatory sensation is affected by other factors, such as vision, olfaction, thermal sensation, and memories. Thus, complexity of cognition mechanism for gustatory sensation as described above makes it difficult to build up a gustatory display.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.6</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2264657</person_id>
				<author_profile_id><![CDATA[81331500452]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Takuji]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Narumi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264658</person_id>
				<author_profile_id><![CDATA[81466640651]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Takashi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kajinami]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264659</person_id>
				<author_profile_id><![CDATA[81100172183]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Tomohiro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tanikawa]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264660</person_id>
				<author_profile_id><![CDATA[81100257385]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Michitaka]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hirose]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[AR cookies: http://mikeclaremikeclare.com/index.php?/systems/ar-cookies/]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Meta Cookie Takuji Narumi , Takashi Kajinami , Tomohiro Tanikawa , Michitaka Hirose Graduate School 
of Engineering,School of Engineering, Graduate School of Information Graduate School of Information the 
University of Tokyo the University of Tokyo Science and Technology, Science and Technology, the University 
of Tokyo the University of Tokyo Fig. 1 Marked Cookie Fig. 2 Meta Cookie 1. Introduction So far, gustatory 
information has rarely been studied in relation to computers, even though there are lots of studies on 
visual, auditory, haptic and olfactory information. This scarcity of research on gustatory information 
has several reasons. One reason is that gustatory sensation is based on chemical signals, whose functions 
have not been fully understood yet. Another reason is that perception of gustatory sensation is affected 
by other factors, such as vision, olfaction, thermal sensation, and memories. Thus, complexity of cognition 
mechanism for gustatory sensation as described above makes it difficult to build up a gustatory display. 
Our research utilizes the complexity of cognition mechanism for gustatory sensation, in order to create 
a pseudo-gustatory display, which induces cross-modal effect by presenting scent. Therefore, for the 
realization of a novel gustatory display system, we aim to establish the method for letting humans feel 
various tastes without changing chemical substances by changing only accompanying information. In this 
paper, we propose the method to change perceived taste of a cookie when they eat by changing appearance 
and smell with augmented reality technology. 2. Meta Cookie: Pseudo-Gustation System We made the system 
to change perceived taste of a cookie by overlaying visual and olfactory information onto a real cookie 
with an AR marker and named it Meta Cookie (Fig. 2). Meta Cookie consists of four parts: a marked plain 
cookie, a marker detection unit, a overlaying visual information unit and an olfactory display. In this 
system, a user wears visual and olfactory display system (Fig. 3). The marker detection unit detects 
the marked cookie and calculates the 6DOF position of the cookie and a distance between the cookie and 
the nose of a user. The user can choose one cookie which he/she wants to eat from multiple kinds of cookies. 
A texture (a photograph of a cookie) and a scent of the cookie which the user selected are Fig. 3 Visual 
and Olfactory Display overlaid onto the cookie based on the calculated position information. We made 
a detectable plain cookie by a camera for this system by reference to Clare s AR Cookie [1]. Since Clare 
uses chocolate for the black part of AR cookie, his AR Cookie is chocolate-flavored. Therefore there 
is a chance that this flavor of chocolate interferes with cross-modal effect which our system evokes 
to change the perceived taste of a cookie. Consequently, we use a plain cookie on the market and print 
a AR marker on it by using branding iron. Fig. 1 illustrates our marked cookie. A camera and ARToolkit 
are used for the marker detection unit. Visual and olfactory display system consists of a HMD and an 
air pump- type olfactory display. A Video see-through HMD displays an appearance of several types of 
cookies (a chocolate cookie, strawberry cookie, tea cookie and so on) on the marked plain cookie based 
on the calculated position information by using ARToolkit. This visual effect let users feel that they 
are eating a selected cookie although they are just eating a marked cookie. Moreover, the air pump-type 
olfactory display produces a scent of the selected cookie. The olfactory display can eject fresh air 
and seven kinds of scented air. And it can control strength of these scents in 127 levels. In Meta Cookie 
, the strength of the produced scent is decided on the basis of the calculated position information. 
Nearer the marked cookie from the user s nose, stronger scent ejects from the olfactory display. This 
olfactory information evokes cross-modal effect between olfaction and gustation and let users feel that 
they are eating a flavored cookie although they are just eating a plain cookie. Currently more than a 
dozen people tried Meta Cookie and almost all of them answered that they feel a change of taste of the 
plain cookie by using our system. References [1]AR cookies: http://mikeclaremikeclare.com/index.php?/systems/ar-cookies/ 
email: {narumi, kaji, tani, hirose}@cyber.t.u-tokyo.ac.jp Copyright is held by the author / owner(s). 
SIGGRAPH 2010, Los Angeles, California, July 25 29, 2010. ISBN 978-1-4503-0210-4/10/0007  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1836999</article_id>
		<sort_key>1540</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>144</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>8</seq_no>
		<title><![CDATA[QR-code calibration for mobile augmented reality applications]]></title>
		<subtitle><![CDATA[linking a unique physical location to the digital world]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836845.1836999</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1836999</url>
		<abstract>
			<par><![CDATA[<p>Advancements in mobile technology have recently contributed to the surfacing of viable mobile augmented reality applications. Still, the main problem of mobile AR, as with all implementations of augmented reality, is the accurate and robust registration of the live camera feed and the digital contents (e.g. images, video, 3D models). So far, mobile AR applications make use of GPS and marker technology (fiducials) to solve this problem (e.g. Sekai Camera, Layar, AR-toolkit, Unifeye). The disadvantages are that, firstly, GPS can only guess the position of the device within a 5 to 10 meter radius, is subjected to weather changes, and does not work indoors. Secondly, although marker registration is very accurate, a marker has to be printed and visible by the camera in order to work.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>H.5.1</cat_node>
				<descriptor>Artificial, augmented, and virtual realities</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.8</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010507</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Graphics recognition and interpretation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010866</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Virtual reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010392</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Mixed / augmented reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010866</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Virtual reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2264661</person_id>
				<author_profile_id><![CDATA[81466640434]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Tsouknidas]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nikolaos]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Kyushu University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264662</person_id>
				<author_profile_id><![CDATA[81466641159]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Tomimatsu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kiyoshi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Kyushu University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Azuma, R. 1997. <i>A Survey of Augmented Reality</i> Hughes Research Laboratories.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Welch G. and Bishop G. 2006. <i>An Introduction to the Kalman Filter</i>, University of North Carolina.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Yao J. and Odobez J-M. 2007. <i>Multi-Layer Background Subtraction Based on Color and Texture</i>, IDIAP Institute.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 1 Introduction QR-Code Calibration for Mobile Augmented Reality Applications Linking a unique physical 
location to the digital world Tsouknidas Nikolaos [tsouk.n@gmail.com], Tomimatsu Kiyoshi [tomimatu@design.kyushu-u.ac.jp], 
Kyushu University 2010   Calibration Scenario The user finds a QR-Code and scans it. Calibration 
is achieved and digital contents are downloaded from the URL that is decoded from the QR-Code, for that 
unique location. The augmented contents are registered correctly using accelerometer and digital compass 
 Abstract Scenario Advancements in mobile technology have recently contributed to the surfacing of viable 
mobile augmented reality applications. Still, the main problem of mobile AR, as with all implementations 
of augmented reality, is the accurate and robust registration of the live camera feed and the digital 
contents (e.g. images, video, 3D models). So far, mobile AR applications make use of GPS and marker technology 
(fiducials) to solve this problem (e.g. Sekai Camera, Layar, AR-toolkit, Unifeye). The disadvantages 
are that, firstly, GPS can only guess the position of the device within a 5 to 10 meter radius, is subjected 
to weather changes, and does not work indoors. Secondly, although marker registration is very accurate, 
a marker has to be printed and visible by the camera in order to work. Mobile devices are already equipped 
with sensors that can tell us where the user is looking at: a gravity sensor and a 3-axis digital compass. 
Even very accurate digital compasses that are yet to be integrated in existing mobile devices are so 
small that future implementations are quite probable. Therefore, the only problem seems to be how to 
accurately discover the user s position. This is the problem we are trying to solve in this project. 
The concept of the solution is, what if we already know ? What if we can find a link to the user s position? 
 2 Proposed Solution QR-Codes are 2-dimensional bar codes that, in essence, contain some bits of encoded 
information in a duotone image, usually black &#38; white. Like all matrix code technologies (Semacode, 
Aztec Code, etc), QR-Codes are commonly used to identify objects or redirect the user to a website. We 
propose a different use of the QR-Code technology, in order to combine it with a mobile AR application. 
Instead of using a QR-Code as a link from an object to a web site, we use it as a link from a known and 
static physical location to a virtual repository of augmented data. When the user scans the QR-Code, 
calibration is achieved and two computational threads begin. One thread starts gathering data from the 
sensors to determine exactly where the user is looking at. The second one downloads the augmented reality 
contents from the Internet location dedicated to this unique QR-Code, and using the sensor data, superimposes 
them on the correct place. The accuracy of the system depends on the accuracy of the sensors themselves, 
and the correct positioning of the user at the moment of the scanning. The mobile device has to be positioned 
very close to the QR-Code and in the correct orientation in order to read it, making the way two different 
users scan a QR-Code almost identical. Of course, a user can stretch his/her hands and try to scan the 
code in a different way, but never further than arm s reach. QR-Code designs can provide the solution 
to this problem, like QR-Code street tiles, which the user will scan and then stand on. Finally, this 
registration technique suggests that the user will not walk away from the initial calibration point, 
but instead use the device to interact with the augmented world, and then move to another QR-Code location. 
Possible uses are: annotations in an urban environment, historical representations, digital locative 
art (clean graffiti), and more. 3 Prototyping &#38; Results A prototype was created on a notebook computer, 
using a Honeywell HMR3300 digital compass/accelerometer (through a Kalman filter) and a web camera. To 
test how accurate the registration is a system was developed where the user can take a picture or a video 
of a scene and create an augmented object instantaneously. A scenario suitable for a mobile device would 
be taking a picture of a person from the QR-Code spot and uploading it so other users can view it as 
augmented data, when they scan the same code. Two pictures are needed: one of the background and one 
of the person. Using the OpenCV library we created a background subtraction program to make such augmentations 
of image and video data. The accuracy of the result depends on how close the user is standing to the 
initial calibration spot. Far targets are less affected by user movements, for example an object 16 meters 
afar from the user will be displaced 25.3 pixels on a 320px wide screen, if the user walks 1 meter away 
from the calibration spot. When more than 5m displacement is measured on a GPS receiver, the user is 
warned that the registration is lost and the augmentations vanish. 4 - References AZUMA, R. 1997. A 
Survey of Augmented Reality Hughes Research Laboratories. WELCH G. AND BISHOP G. 2006. An Introduction 
to the Kalman Filter, University of North Carolina. YAO J. AND ODOBEZ J-M. 2007. Multi-Layer Background 
Subtraction Based on Color and Texture, IDIAP Institute. --- QR Code is registered trademark of DENSO 
WAVE INCORPORATED --- 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1837000</article_id>
		<sort_key>1550</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>145</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>9</seq_no>
		<title><![CDATA[The effect of desktop illumination realism on a user's sense of presence in a virtual learning environment]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836845.1837000</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1837000</url>
		<abstract>
			<par><![CDATA[<p>The application of virtual reality is becoming ever more important as technology reaches new heights allowing virtual environments (VE) complete with global illumination. One successful application of virtual environments is educational interventions meant to treat individuals with autism spectrum disorder (ASD) since VEs induces pretense and presence, without the social fear of failing in the real world. Pretense and presence improves the user's ability to learn social skills and enhances perception-taking capabilities. Because of the lack of conclusive research of improving presence of individuals with ASD in a VE, this study evaluated ways of enhancing presence to improve a VE intervention for individuals with ASD. In the field of computer science visual realism, new research has surfaced linking illumination realism to presence. Since this research was limited to Neurologically Typical (NT) individuals (those without ASD), and because generalization is particularly important to individuals with ASD, this study targeted these individuals. Further, since head mounted displays (HMD) are impractical for widespread delivery of a VE intervention application and since the literature is inconclusive about the effect of VEs without HMDs on presence, this study used standard desktop displays. This work measured the extent to which visual realism induces the presence of a VE intervention, enumerated the specific characteristics of rendering that promote the sense of presence and the ability to generalize, and statistically verified the enhanced outcomes from using these techniques. After conducting a between-group study with 24 individuals with ASD, illumination realism was found to have a positive effect (effect size=0.6) on the presence felt by these individuals. This work contributes to the field of visualization and special education by providing empirical evidence supporting the claim that illumination realism increases the presence felt by users with ASD when interacting with a PVE.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>H.5.1</cat_node>
				<descriptor>Artificial, augmented, and virtual realities</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Virtual reality</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010392</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Mixed / augmented reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010866</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Virtual reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010866</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Virtual reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010866</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Virtual reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Human Factors</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2264663</person_id>
				<author_profile_id><![CDATA[81440599407]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Justin]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ehrlich]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Kansas]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1837001</article_id>
		<sort_key>1560</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>146</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>10</seq_no>
		<title><![CDATA[Virtual Yamahoko parade in virtual Kyoto]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836845.1837001</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1837001</url>
		<abstract>
			<par><![CDATA[<p>Recently, extensive research has been undertaken on digital archiving of cultural properties in the field of cultural heritage. These investigations have examined the processes of recording and preserving both tangible and intangible materials through the use of digital technologies.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>H.5.1</cat_node>
				<descriptor>Artificial, augmented, and virtual realities</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Virtual reality</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010392</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Mixed / augmented reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010866</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Virtual reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010866</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Virtual reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010866</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Virtual reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2264664</person_id>
				<author_profile_id><![CDATA[81453635348]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Woong]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Choi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Ritsumeikan University in Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264665</person_id>
				<author_profile_id><![CDATA[81466648601]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Takahiro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Fukumori]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Ritsumeikan University in Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264666</person_id>
				<author_profile_id><![CDATA[81453633184]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Kohei]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Furukawa]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Ritsumeikan University in Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264667</person_id>
				<author_profile_id><![CDATA[81100078311]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Kozaburo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hachimura]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Ritsumeikan University in Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264668</person_id>
				<author_profile_id><![CDATA[81100066602]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Takanobu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nishiura]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Ritsumeikan University in Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264669</person_id>
				<author_profile_id><![CDATA[81320496720]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Keiji]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yano]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Ritsumeikan University in Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>2092132</ref_obj_id>
				<ref_obj_pid>2092088</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[
Kohei Furukawa, Choi Woong, K. H. K. A. 2006. Cg restoration of a historical noh stage and its use for edutainment. <i>Lecture Notes in Computer Science 4270</i>, 2006, 358--367.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[
Yano, K. 2007. <i>Virtual Kyoto: exploring the past, present and future of Kyoto.</i> Nakanishiya.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Virtual Yamahoko Parade in Virtual Kyoto Woong Choi, Takahiro Fukumori, Kohei Furukawa, Kozaburo Hachimura, 
Takanobu Nishiura, Keiji Yano Ritsumeikan University in Japan.  (a) (b) (c) (d) Figure 1: Virtual Yamahoko 
Parade. (a) Big crowds gather at Shijo Street, the city s main street, to watch this parade. (b), (c) 
and (d) are CG .oats of Naginata-hoko, Fune-hoko, and Kitakannon-yama, respectively. 1 Introduction Recently, 
extensive research has been undertaken on digital archiv­ing of cultural properties in the .eld of cultural 
heritage. These investigations have examined the processes of recording and pre­serving both tangible 
and intangible materials through the use of digital technologies. For decades, tangible, material cultural 
heritage including arche­ological sites, and historical buildings and monuments has been digitally recorded. 
Compared to that, however, the intangible cul­tural heritage such as performing arts, theatre, social 
practices, and events, has rarely been regenerated in the virtual world [Furukawa et al. 2006]. In our 
research, we speci.cally focused on assets of Yamahoko pa­rade in the Kyoto Gion Festival, which was 
added to the UNESCO s Representative List of the Intangible Cultural Heritage of Humanity in 2009. On 
July 17 of every year, this festival in Kyoto in central Japan culminates in a parade of yamahoko, .oats 
known as moving museums because of their elaborate decorations with centuries-old tapestries, wooden 
and metal ornaments. The festival is held by the Yasaka Shrine and the thirty-two .oats are paraded by 
the residents of the city s self-governing districts. Approximately 150 thousands spectetors gather to 
see the parade every year. To reproduce Yamahoko parade in virtual environment, we generate a content 
that combines motion data of Yamahoko parade, specte­tors and of.cials (Hikikata, Ondotori, Kurumakata) 
with visualiza­tions of virtual Kyoto within a virtual world platform. However this research used only 
the well-known 4 .oats out of thirty-two .oats mentioned above and few spectetors compared to real parade 
are generated in virtual world. 2 Our Approach The content of virtual Yamahoko parade includes CG .oats 
of yama and hoko, crowd simulation, acoustics and Shijo Street of virtual Kyoto. Four CG .oats of Naginata-hoko, 
Kanko-hoko, Fune-hoko, and Kitakannon-yama were included in this virtual parade. These CG .e-mail: wchoi@img.is.ritsumei.ac.jp 
models of the .oats were built based on our laser measurements of their miniatures, as well as surveyed 
drawings of the real ones which Kyoto City Tourism Bureau had made. Virtual Shijo Street is made by VRML 
data based on GIS [Yano, 2007]. A crowd sim­ulation constitutes an important element to indicate how 
they are supposed to behave at the time of the parade in the Japanese cul­tural context. To create Japanese 
character models, we modi.ed the models employed in Vizard . The acoustics sound of parade was captured 
by multi-point measurement technique during the time of real parade. The mixing of sound effect controled 
the sound pres­sure level (SPL) of each instrument and optimized the SPL balance of the speaker based 
on CG content. Vizard software enables us to create virtual Yamahoko parade in real time by integrating 
its assets. VRML data of Shijo Street, yama .oat and hoko .oats were imported into Vizard to construct 
the streetscape of the parade. Character models with motion were transformed from 3ds MAX format into 
CAL3D format. These data were then used to generate the crowd simulation in Vizard. Approximately 730 
characters were simulated in this virtual parade to make a realistic crowd behavior. Figure 1 shows some 
examples of the content, elucidated on the above. We gained favorable comments from visitors at the exhibi­tion 
of virtual Yamahoko parade held at the Museum of Kyoto in January 2010. Acknowledgements This research 
has been conducted partly by the support of the Dig­ital Museum Project in the Ministry of Education, 
Culture, Sports, Science and Technology, Japan and the Digital Museum of Kyoto in R-GIRO research projects, 
Ritsumeikan University. References KOHEI FURUKAWA,CHOI WOONG, K. H. K. A. 2006. Cg restoration of a 
historical noh stage and its use for edutainment. Lecture Notes in Computer Science 4270, 2006, 358 367. 
YANO, K. 2007. Virtual Kyoto: exploring the past, present and future of Kyoto. Nakanishiya. Copyright 
is held by the author / owner(s). SIGGRAPH 2010, Los Angeles, California, July 25 29, 2010. ISBN 978-1-4503-0210-4/10/0007 
  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	</section>
	<section>
		<section_id>1837002</section_id>
		<sort_key>1570</sort_key>
		<section_seq_no>11</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Visualization]]></section_title>
		<section_page_from>147</section_page_from>
	<article_rec>
		<article_id>1837003</article_id>
		<sort_key>1580</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>147</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[2020 3D media]]></title>
		<subtitle><![CDATA[new directions in immersive entertainment]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836845.1837003</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1837003</url>
		<abstract>
			<par><![CDATA[<p>This research project is conducted by a consortium of European industrial and academic partners that include companies like: Technicolor, Digital Projection, DTS Europe, Doremi, Mediapro, Creative Wokers (CREW) and Datasat, and research centers: Barcelona Media, Joaneum research, University of Hasselt, University of Reading and Fraunhoffer. It is aimed to research, develop and demonstrate novel forms of <i>compelling entertainment experiences</i> based on new technologies for the capture, production, networked distribution and display of <i>three-dimensional</i> sound and images.</p> <p>2020 3D Media research project is co-funded by the European Commission under the Seventh Framework Programme (FP7--ICT).</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.8</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010507</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Graphics recognition and interpretation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2264670</person_id>
				<author_profile_id><![CDATA[81319491699]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Santi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Fort]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Barcelona Media -- Innovation Centre]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[
Werner Bailer and Martin H&#246;ffernig, 2009 "Metadata for Creation and Distribution of Multi-view Video," in Workshop on Semantic Multimedia Database Technologies (SeMuDaTe), Graz.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[
Nele Wynants, Kurt Vanhoutte, Philippe Bekaert, 2008 "Being Inside the Image. Heightening the Sense of Presence in a Video Captured Environment through Artistic Means: The Case of CREW," in Proceedings of the 11th Annual International Workshop on Presence, Padova.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 2020 3D Media: New directions in Immersive Entertainment Santi Fort, Barcelona Media Innovation Centre 
ABSTRACT This research project is conducted by a consortium of European industrial and academic partners 
that include companies like: Technicolor, Digital Projection, DTS Europe, Doremi, Mediapro, Creative 
Wokers (CREW) and Datasat, and research centers: Barcelona Media, Joaneum research, University of Hasselt, 
University of Reading and Fraunhoffer. It is aimed to research, develop and demonstrate novel forms of 
compelling entertainment experiences based on new technologies for the capture, production, networked 
distribution and display of three-dimensional sound and images. 2020 3D Media1 research project is co-funded 
by the European Commission under the Seventh Framework Programme (FP7 ICT). Introduction The media industry 
knows that astonishing the public is still a route to large audiences and financial success. It is believed 
that high quality presentation of stereoscopic or immersive images in the home and in the public entertainment 
spaces (such as cinemas) can offer previously unimagined levels of experience. Objectives The users of 
the resulting technologies will be both media industry professionals across the current film, TV and 
new media sectors producing programme material as well as the general public. The 2020 3D Media has five 
Scientific and Technical Objectives: 1. To research and develop practical networked technologies for 
the capture, production, and display of sounds and images in three dimensions. 2. To create a heightened 
sense of presence, putting the spectator at the heart of the experience. 3. To develop means of navigating 
a virtualized world, based on captured data, that has a complete sense of reality. 4. To develop means 
of changing things in this world once it has been created. 5. To make it possible to repurpose and deploy 
multi­dimensional content in different contexts.  Development The overall project tasks are organized 
around four application areas: -Workflow and networked spatial media: The research undertaken entailed 
a comprehensive study on the metadata set at different workflow stages as well as metadata acquisition 
and metadata conversion processes [Bailer and Höffernig 2009]. The metadata defined was based on standards 
adding extensions for identified needs or gaps. The resulting metadata ontology was subject to a proof 
of concept and several demonstrators have been implemented to test the adequacy of the defined metadata. 
- Capture &#38; coding: The project researches and develops a novel camera architecture, where one platform 
can be used to realize different applications like the generation of depth information for 3D Image Capture 
and enhanced resolutions above 2K for Digital Cinematography. Three different depth estimation approaches 
have been researched, namely the time 1 www.20203dmedia.eu of flight, the trifocal and the structure-light 
principles. In the trifocal approach, a camera rig to mount satellite cameras around a main camera was 
available early in the project. It was used for first test-shoots using the trifocal as well as the structured-light 
approach. Spatial audio capture solutions are also developed. Figure1: 3D capture workflow for the trifocal 
approach -Post-production: This area of work includes tools to manipulate stereoscopic and panoramic 
sequences, dimensionalization tools to help to give depth to standard 2D sequences and 3D audio tools. 
The project has developed a software framework for real-time depth enhanced image processing and CG rendering 
into various formats. In the meantime, improved methods for depth computation, 3D reconstruction and 
view-synthesis for 3D content post­production are being developed. -Distribution and exhibition: A key 
part of this project is the development of a network-centric distribution system, capable of delivering 
3D and immersive entertainment to cinemas, public spaces or homes [Wynants 2008]. One example of the 
work in this field could be the research on audio reproduction, were a proposal for a spatial audio methodology 
and format suitable for distribution, which is independent from the exhibition speakers setup, was produced. 
Conclusion The market for stereoscopic 3D technologies is racing ahead, and this is leading to a growing 
awareness of the potential benefits of the more advanced technologies for depth cameras, omnidirectional 
video, and postproduction methods. 2020 3D Media works on an integrated plan of phased delivery of interim 
products, prototypes of medium­long term future technologies; and research results for future generations. 
 References WERNER BAILER AND MARTIN HÖFFERNIG, 2009 Metadata for Creation and Distribution of Multi-view 
Video, in Workshop on Semantic Multimedia Database Technologies (SeMuDaTe), Graz. NELE WYNANTS, KURT 
VANHOUTTE, PHILIPPE BEKAERT, 2008 Being Inside the Image. Heightening the Sense of Presence in a Video 
Captured Environment through Artistic Means: The Case of CREW, in Proceedings of the 11th Annual International 
Workshop on Presence, Padova. Copyright is held by the author / owner(s). SIGGRAPH 2010, Los Angeles, 
California, July 25 29, 2010. ISBN 978-1-4503-0210-4/10/0007  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1837004</article_id>
		<sort_key>1590</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>148</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[A mathematical model of deforming manifolds and their visualizations by CG animation]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836845.1837004</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1837004</url>
		<abstract>
			<par><![CDATA[<p>It is becoming an important field in computer art to visualize High Dimensional Manifolds. [Banchoff 1990] [Kusabuka and AlgorithmicArt 2006] In this research, we will find the dynamics equation which express deforming motion of the &alpha; dimensional manifold <i>r</i>, and visualize many interesting motions of the deformed manifold <i>r(t)</i> by CG animation.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Animation</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061.10010063</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures->Computational geometry</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2264671</person_id>
				<author_profile_id><![CDATA[81421598603]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Ippei]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Takauchi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Kanagawa Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264672</person_id>
				<author_profile_id><![CDATA[81466642605]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Yuta]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hara]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Kanagawa Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264673</person_id>
				<author_profile_id><![CDATA[81442600457]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Hiromu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Saito]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Kanagawa Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264674</person_id>
				<author_profile_id><![CDATA[81421593574]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Ryo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Asakura]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Kanagawa Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264675</person_id>
				<author_profile_id><![CDATA[81430601538]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Motofumi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hattori]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Kanagawa Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>533317</ref_obj_id>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[
Banchoff, T. F. 1990. <i>Beyond The Third Dimension -- Geometry, Computer Graphics and Higher Dimensions</i> --. Scientific American Library.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[
Kusabuka, K., and AlgorithmicArt, S. 2006. <i>20 th Century Computer Art: Beginning and Development -- The work and thought of pioneers and contemporary practitioners of algorithmic art</i> --. Tama Art University Museum.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 A Mathematical Model of Deforming Manifolds and Their Visualizations by CG Animation Ippei TAKAUCHI* 
Yuta HARA Hiromu SAITO Kanagawa Institute of Technology Kanagawa Institute of Technology Kanagawa Institute 
of Technology Ryo ASAKURA§ Kanagawa Institute of Technology It is becoming an important .eld in computer 
art to visualize High Dimensional Manifolds. [Banchoff 1990] [Kusabuka and Algorith­micArt 2006] In this 
research, we will .nd the dynamics equation which express deforming motion of the a dimensional manifold 
r , and visualize many interesting motions of the deformed manifold r(t) by CG animation. Let r be the 
deformed manifold and s be the natural con.gura­tion of the manifold, which is embedded in the general 
dimen­sional Euclidean space Rt+1 . The manifolds r and s are cov­ered by plural charts, i.e. they equal 
the union sets of some plu­ral charts. They are parameterized by r(u)= r(u1,u2, ··· ,ut) and s(u)= s(u1,u2, 
··· ,ut) on each chart, where the local coordinate u =(u1,u2, ··· ,ut) is a variable in a dimensional 
Euclidean space Rt. They also be parameterized by r(v)= r(v1,v2, ··· ,vt) and s(v)= s(v1,v2, ··· ,vt) 
on another chart. Let M(u) be the normal vector of the deformed manifold r(u) , and N(u) be the normal 
vector of the natural manifold s(u). Let .· , ·. denote the inner product ( the dot product ) in (a + 
1) dimensional Euclidean space Rt+1. Let fij be de.ned as . ... .2 .2 rs fij = ,M - ,N (1) .ui.uj .ui.uj 
Since the difference between the 2nd fundamental forms of the de­formed manifold r and the natural manifold 
s tt .. Di.erence(2nd)(r, s)= - fij duiduj (2) i=1 j=1 is conserved by the transformation from a local 
coordinate u = (u1,u2, ··· ,ut) to another local coordinate v =(v1,v2, ··· ,vt) , the potential function 
. tt .. J(2nd)(r, s)= duiduj (3) i=1 j=1 {. ...}2 .2 .2 rs ,M - ,N .ui.uj .ui.uj can be de.ned as an 
integral over the manifold. This potential func­tion J(2nd)(r, s) expresses the amount of differences 
between the con.guration of the deformed manifold r and the natural manifold s with respect to their 
bend and torsion. The Frechet derivative .J(2nd)(r, s)/.r of the potential function J(2nd)(r, s) with 
respect to r becomes .J(2nd)(r, s) = (4) .r ttttt+1 ..... 2 .el(-1)l i=1 j=1 k=1 m=1 l=1 det (l th raw 
is omitted from ckm) *e-mail: takauchi.kait@gmail.com e-mail:love.ghibli129@gmail.com e-mail:nelten569@gmail.com 
§e-mail:seventhexpress@yahoo.co.jp ¶e-mail:hattori@ic.kanagawa-it.ac.jp Copyright is held by the author 
/ owner(s). SIGGRAPH 2010, Los Angeles, California, July 25 29, 2010. ISBN 978-1-4503-0210-4/10/0007 
 Motofumi HATTORI¶ Kanagawa Institute of Technology tt .. .2 +2 (fij M) .ui.uj i=1 j=1 where .el is 
an (a + 1) dimensional raw vector whose l th element is 1 and all other elements are 0 , and the matrix 
ckm is de.ned as follows. For m = k , ckm = (5) ( ()) .2 , ··· ,,fij ,, ··· , .r .r . r.r .r .u1 .uk-1 
.uk .ui.uj .uk+1 .ut For m . = k , n th column ckmn of the matrix ckm is obtained by () . .r ckmn = for 
n = m (6) .uk .um .2 r ckmn = fij for n = k (7) .ui.uj .r ckmn = for n .= m and n .= k (8) .un The above 
equation (4) generates the inner forces of the manifold r with respect to bend and torsion. We can also 
de.ne the potential function J(1st)(r, s) from the dif­ference Di.erence(1st)(r, s) between the 1st fundamental 
forms of the deformed manifold r and the natural manifold s , and the Frechet derivative .J(1st)(r, s)/.r 
of this potential function J(1st)(r, s) with respect to r becomes tt ..() .J(1st)(r, s) . .r = -4 gij 
(9) .r .uj .ui i=1 j=1 This Equation (9) generates the inner forces of the manifold r with respect to 
stretch and shear. Let .(u) be the mass density ot the deformed manifold r where u =(u1,u2, ··· ,ut) 
is a local coordinate of some chart. Let r(t) be the con.gure of the deformed manifold r at time t. The 
motion equation of the deforming manifold r(t) becomes .2 r(t, u) .(u)= (10) .t2 .J(2nd) .J(1st) - (r(t, 
u),s(u)) - (r(t, u),s(u)) .r .r In the real world, the natural con.guration s of the manifold does not 
change as the time t goes on. But, in the virtual world, we can change the natural con.guration s as 
the time t goes on, i.e. we can consider time time varying natural con.guration s(t, u). Many interesting 
motions of the deformed manifold r(t, u) are obtained from the appropriate time varying natural con.guration 
s(t, u). References BANCHOFF, T. F. 1990. Beyond The Third Dimension Geom­etry, Computer Graphics and 
Higher Dimensions . Scienti.c American Library. KUSABUKA, K., AND ALGORITHMICART, S. 2006. 20 th Cen­tury 
Computer Art : Beginning and Development The work and thought of pioneers and contemporary practitioners 
of algorith­mic art . Tama Art University Museum.  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1837005</article_id>
		<sort_key>1600</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>149</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[An integrated interactive visualization and analysis environment to study the impact of fire on building structures]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836845.1837005</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1837005</url>
		<abstract>
			<par><![CDATA[<p>In order to move away from the current prescriptive design methods towards performance based methods for the design of structures under fire, we need validated computer models. The next section describes our approach for modeling and analysis.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.6.6</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.6.7</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010341.10010366</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Simulation support systems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341.10010370</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Simulation evaluation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2264676</person_id>
				<author_profile_id><![CDATA[81466646522]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Dilip]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Banerjee]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[NIST]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264678</person_id>
				<author_profile_id><![CDATA[81342496250]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[John]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gross]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[NIST]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264679</person_id>
				<author_profile_id><![CDATA[81466648306]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Pradeep]]></first_name>
				<middle_name><![CDATA[Reddy]]></middle_name>
				<last_name><![CDATA[Gaddam]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[NIST]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264680</person_id>
				<author_profile_id><![CDATA[81334487411]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Marc]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Olano]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Univ. of Maryland, Baltimore County]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264681</person_id>
				<author_profile_id><![CDATA[81466643880]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[William]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hess]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[NIST]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264682</person_id>
				<author_profile_id><![CDATA[81325490531]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Judith]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Terrill]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[NIST]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264683</person_id>
				<author_profile_id><![CDATA[81500656784]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>7</seq_no>
				<first_name><![CDATA[Terence]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Griffin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[NIST]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264684</person_id>
				<author_profile_id><![CDATA[81100425457]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>8</seq_no>
				<first_name><![CDATA[John]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hagedorn]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[NIST]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264685</person_id>
				<author_profile_id><![CDATA[81100078394]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>9</seq_no>
				<first_name><![CDATA[John]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kelso]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[NIST]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264677</person_id>
				<author_profile_id><![CDATA[81100347503]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>10</seq_no>
				<first_name><![CDATA[Steve]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Satterfield]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[NIST]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[
Banerjee, D. 2009. Visualization of structural behavior under fire. Technical Report NISTIR 7619, NIST, August.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[
McGrattan, K. B. 2007. Fire dynamics simulator (version 5). Technical Report NIST SP 1018-5, NIST, October.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1048658</ref_obj_id>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[
Moreland, K. 2004. <i>Fast High Accuracy Volume Rendering.</i> PhD thesis, University of New Mexico.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[
Open Dynamics Engine {online}. http://www.ode.org/.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 An Integrated Interactive Visualization and Analysis Environment to study the Impact of Fire on Building 
Structures Dilip Banerjee* John Gross Pradeep Reddy Gaddam Marc Olano William Hess JudithTerrill Terence 
Grif.n John Hagedorn JohnKelso Steve Satter.eld  Figure 1: Our visualization and analysis environment. 
Upper left: 3D visualization of .re, structure and room. Lower Left: example plot of temperature vs time 
fora user-selected pointin the beam. Right: control panels for interacting with, and analyzingthe visualization. 
1 Introduction Inordertomoveawayfromthe current prescriptivedesign methods towards performance based 
methods for the design of structures under .re, we need validated computer models. The next section describes 
our approach for modeling and analysis. 2 Method We have created an integrated interactive visualization 
and analysis environment, that can be used immersively or on the desktop, to study the interaction among 
.re, heat transfer, and structural defor­mation computed from three linked simulations that model the 
.re, the heat transfer, and the structure. In this initial study, we simulated the evolution of a .re 
in a room (with a small adjoining room) with furniture, a window and a sin­gle structural beam. A sequential 
process was followed in which .rst the NIST Fire Dynamics Simulator (FDS) [McGrattan 2007] was used to 
simulate the start and development of .re in the room. The adiabatic surface temperature at twelve marker 
points under­neath the beams bottom .ange location was used to capture the time dependent temperature 
during the FDS simulation. Thenasec­ond (.nite element) computer program was used to calculate how thegas 
temperature at the markers propagated into the beam. Fi­nally, a third (.nite element) computer program 
was used to com­pute how the beam deformed over time due to combined effects of thermal and mechanical 
loads [Banerjee 2009]. The three programs produced three separate and very large datasets. The .re model 
was computed on 64,000 grid points for 115,705 time steps, and 6 scalars were studied for each of these 
points. The heat transfer and the structure models were computed on 10,332 elements and 12,642 nodes 
for each time step and two scalars, two vectors and two tensors were computed in total. We used polygonal 
visualizations for the room and its contents, in­cluding the beam, and shaded the beam according to its 
tempera­ *Except as indicated, all authors are from NIST Univ. of Maryland, Baltimore County ture distribution. 
We used a ray traced volume shader to visualize the .re and smoke.We combined temperature, heat release 
rate per unit volume, and soot density to come up with a local color and opacity at each step along the 
ray. Many volume renderers assume that the material has a constant color and opacity within anygiven 
step of the numeric integration. Instead, we used an approxima­tion of linear attenuation within each 
step [Moreland 2004]. We usedtheopen dynamicsphysicsengine[ODE]todisplaywindow breakage at a time indicated 
in the .re simulation. To assist in the analysis, we can interactively display numeric values at individual 
points in the 3D space, and we can plot time histories of: temper­ature, the stress tensor in the beam, 
and deformation of the beam (location and orientation). We can also plot output values along a chosen 
line. 3 Conclusion and Future Work Preliminary investigation using our environment, provided initial 
validation of the linked model approach. The visualization and analysis environment was crucial to develop 
an understanding of the output of the three simulations. But this was a very simple case developed to 
validate the methodology. Future work consists of furthervalidation, and morecomplexbuilding and .re 
scenarios. Additionally, we will be studying how to best perform the analysis in the context of the visualization 
in order to advance the modeling effort and our understanding. References BANERJEE, D. 2009. Visualization 
of structural behavior under .re. Technical Report NISTIR 7619, NIST, August. MCGRATTAN,K.B. 2007. Fire 
dynamics simulator(version5). Technical Report NIST SP 1018-5, NIST, October. MORELAND, K. 2004. Fast 
High AccuracyVolume Rendering. PhD thesis, University of New Mexico. Open Dynamics Engine [online]. http://www.ode.org/. 
Copyright is held by the author / owner(s). SIGGRAPH 2010, Los Angeles, California, July 25 29, 2010. 
ISBN 978-1-4503-0210-4/10/0007  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1837006</article_id>
		<sort_key>1610</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>150</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[Forecast and visualization of future expenditure with logging and analyzing receipts]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836845.1837006</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1837006</url>
		<abstract>
			<par><![CDATA[<p>Logging images, voices, etc. of one's daily life is called "lifelog". Recently, it is done research by many researchers because of rapid increasing information with a highly information-oriented society and increasing capacity and lowering price of logging device.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.6.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010341</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2264686</person_id>
				<author_profile_id><![CDATA[81466646165]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Toshiki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Takeuchi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264687</person_id>
				<author_profile_id><![CDATA[81331500452]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Takuji]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Narumi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264688</person_id>
				<author_profile_id><![CDATA[81100461884]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Kunihiro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nishimura]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264689</person_id>
				<author_profile_id><![CDATA[81100172183]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Tomohiro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tanikawa]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264690</person_id>
				<author_profile_id><![CDATA[81100257385]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Michitaka]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hirose]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>945310</ref_obj_id>
				<ref_obj_pid>945305</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Daniel Ashbrook. Using GPS to Learn Significant Locations and Predict Movement across Multiple Users. <i>Pers Ubiquit Comput</i>, Vol. 7, pp. 275--286, 2003]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[
<i>Yasashiku Kakeibo:</i> http://mediadrive.jp/products/ykakeibo/index.html]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Forecast and Visualization of Future Expenditure Toshiki Takeuchi , Graduate School of Interdisciplinary 
Information Studies, the University of Tokyo  with Logging and Analyzing Receipts Takuji Narumi , Kunihiro 
Nishimura , Tomohiro Tanikawa , Graduate School of Graduate School of Graduate School of Engineering, 
Information Science and Information Science and the University of Technology, Technology, Tokyo the 
University of Tokyo the University of Tokyo Michitaka Hirose Graduate School of Information Science and 
Technology, the University of Tokyo  Fig. 1 Receiptlog Viewer Fig. 2 Expenditure Forecast 1 Fig. 3 Expenditure 
Forecast 2 1. Introduction Logging images, voices, etc. of one s daily life is called lifelog . Recently, 
it is done research by many researchers because of rapid increasing information with a highly information-oriented 
society and increasing capacity and lowering price of logging device. So far, most of lifelog research 
has been done for reading the past log efficiently by analyzing and ordering a lot of lifelog data. Our 
hypothesis is that utilizing lifelog and providing future information not past to users is also useful 
for improving personal life [1]. We focused on an expenditure behavior in daily life, and collect data 
from receipts. We call its log system Receiptlog . We developed a system Expenditure Forecast that forecast 
future information about expenditure behavior from receipts log data and present a user it to understand 
easily. In this poster, we explain the system of Receiptlog and Expenditure Forecast, the method for 
forecast of future expenditure, and influence on user s balance of expenditure by using Expenditure Forecast. 
 2. Receiptlog A receipt is usually made out whenever you buy something. It contains information of the 
store name, the date and time, the address, the telephone number, the commodities and prices, the expenditure, 
etc. We logged many people s expenditure behavior by reading their receipts with OCR (Optical Character 
Recognition). We used Yasashiku Kakeibo [2] as OCR software, because it had high level OCR function and 
the internal table of telephone number address. We succeeded in logging 2191 receipts data from 27 persons 
until Feb. 28, 2010. For forecast of expenditure, we needed to know something of rules about expenditure 
behavior. Therefore we developed Receiptlog Viewer (Fig. 1), and the cycles of time (weekly, email: {take, 
narumi, kuni, tani, hirose}@cyber.t.u-tokyo.ac.jp daily, etc.) and place were found out in expenditure 
behavior. 3. Expenditure Forecast From the expenditure cycle, we supposed that nearer future situation 
is to past one, more likely the similar expenditure occurs. We selected the day, day of week, time, longitude 
and latitude as features expressing the situation, and proposed the algorithm that forecast the future 
amount of money and possibility of expenditure, utilizing the information about date, time, position, 
and amount of money in Receiptlog. Next, we developed an application for iPhone called Expenditure Forecast 
to present the forecast to users (Fig. 2). It shows the amount of money and possibility of expenditure 
by place visually like a weather chart, and the warning and advisory. For example, Fig. 3 shows that 
You will spend 100-500 yen at current position after three hours. Advisory: Yayoi, Bunkyo, Tokyo 400 
yen . We had user test with nine users for eleven days. They thought that the forecast was almost right, 
and some of them spent different amount of money from they would after seeing the forecast. To be concrete, 
two persons of them spent more money, and other two persons spent less. And most of them had opinions 
that they paid much more attention to the expenditure. We conclude that user s balance of expenditure 
can be improved by seeing one s forecast. Future works are that we spread Receiptlog and Expenditure 
Forecast over the society, and a large number of people use them. Then, we observe their effects on not 
only the close society like a laboratory, but also open one. References [1] Daniel ASHBROOK. Using GPS 
to Learn Significant Locations and Predict Movement across Multiple Users. Pers Ubiquit Comput, Vol. 
7, pp. 275 286, 2003 [2] Yasashiku Kakeibo: http://mediadrive.jp/products/ykakeibo/index.html Copyright 
is held by the author / owner(s). SIGGRAPH 2010, Los Angeles, California, July 25 29, 2010. ISBN 978-1-4503-0210-4/10/0007 
  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1837007</article_id>
		<sort_key>1620</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>151</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>5</seq_no>
		<title><![CDATA[Navigation and exploration of large data-sets using a haptic feedback device]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836845.1837007</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1837007</url>
		<abstract>
			<par><![CDATA[<p>A haptic feedback device enables a user to manipulate three dimensional structures and feel forces contained within complex data-sets such as those resulting from computational biology. However, as the data-set grows in size it becomes difficult to ensure that the user can easily interact with every part of it. One could scale the data-set down to fit into the haptic workspace, however, this could result in important features being missed. A secondary problem is enabling the user to select points efficiently within the three dimensional data set, where the perception of depth can be difficult. In this paper we present novel techniques to rapidly navigate large and complex data-sets with a haptic feedback device, whilst still permitting accurate and fast selection of points in three dimensional space. We have applied these techniques as part of software dedicated to studying the response of biomolecules to externally applied forces using elastic network models.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[biomolecule]]></kw>
			<kw><![CDATA[elastic network model]]></kw>
			<kw><![CDATA[force-feedback]]></kw>
			<kw><![CDATA[haptic rendering]]></kw>
			<kw><![CDATA[protein]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.6</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2264691</person_id>
				<author_profile_id><![CDATA[81414610228]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[S.]]></first_name>
				<middle_name><![CDATA[D.]]></middle_name>
				<last_name><![CDATA[Laycock]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of East Anglia, UK]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264692</person_id>
				<author_profile_id><![CDATA[81466648061]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[M.]]></first_name>
				<middle_name><![CDATA[B.]]></middle_name>
				<last_name><![CDATA[Stocks]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of East Anglia, UK]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264693</person_id>
				<author_profile_id><![CDATA[81416600531]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[S.]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hayward]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of East Anglia, UK]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1050141</ref_obj_id>
				<ref_obj_pid>1048934</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Dominjon, L., Lecuyer, A., Burkhardt, J., Andrade-Barroso, G., and Richir, S. 2005. The "bubble" technique: Interacting with large virtual environments using haptic devices with limited workspace. In <i>Proc. WorldHaptics</i>, 639--640.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Navigation and exploration of large data-sets using a haptic feedback device * S.D.LaycockM.B.Stocks 
S.Hayward SchoolofComputingSciences,University ofEastAnglia,UK,NR4 7TJ (a) (b) (c) Figure 1: The Navigation 
Cube automatically transforms a scene for exploration of a molecule (a). The user manipulates a probe 
sphere using a Phantom Desktop (b). The Haptic Light aids the user in selecting atoms in an elastic network 
application (c). Abstract Ahapticfeedbackdevice enables a user to manipulate threedimen­sional structures 
andfeelforces contained within complexdata-sets such as those resulting from computational biology. However, 
as the data-set grows in size it becomes dif.cult to ensure that the user can easily interact with every 
part of it. One could scale the data-set down to .t into the haptic workspace, however, this could result 
in important features being missed. A secondary problem is enabling the user to selectpoints ef.ciently 
within the threedimen­sional data set, where the perception of depth can be dif.cult. In thispaper wepresent 
novel techniques to rapidly navigatelarge and complexdata-sets with ahapticfeedbackdevice, whilst stillpermit­ting 
accurate andfast selection ofpointsinthreedimensional space. We have applied these techniques as part 
of software dedicated to studying the response of biomolecules to externally applied forces using elastic 
network models. CR Categories: I.3.6 [Computing Methodologies]: Computer Graphics Methodology andTechniques; 
Keywords: Force-feedback, Haptic Rendering, Elastic Network Model,Biomolecule, Protein 1 Our Approach 
Utilising a haptic feedback device to navigate a large three dimen­sional volume can be dif.cult due 
to the limited workspace of the device. We introduce the concept of the Navigation Cube which automatically 
transforms the environment as the user reaches for an area of interest. The Navigation Cube is similar 
to the bubble technique[Dominjon et al.2005],however,the spherical navigation volumein that approachdoes 
not correspond well to the workspace of the haptic device, whereas here the navigation cube is automat­ically 
scaled to .t the workspace. Furthermore, unlike the bubble technique no forces ofinteractionbetween the 
navigation cube and theprobe areincluded to avoid confusion withforces arisingdue to interaction with 
the objects in the scene. To explore areas outside this cube, the user simply moves the haptic probe 
in the direction of the new location, without worrying about whether the new area * e-mail: sdl@cmp.uea.ac.uk 
is currently residing in the workspace of the device. The speed of translationisdependent on thedistance 
thehapticprobeisfrom the side of the cubeitpenetrated. This allows the user to move slightly outsidethecubefor 
.nenavigationcontrol and tomovefurtherfor faster translation. Accurately selecting three dimensional 
objects within complex data sets is another challenge, typically worsened by 2D viewing. Stereoscopicdisplayspresent 
one strategyfor overcoming this. We introduce a further enhancement termed the haptic light to aid in 
accurate selection of objects within a complex scene. Initially the scene is rendered with one light 
source located at the viewers po­sition. This light illuminates the structures with only ambient and 
diffuselighting;toprovide threedimensionalde.nitionbut without aspecularhighlight.A secondlight source,the 
hapticlight ,isat­tached to thelocation of theprobe and illuminatesin alldirections. This light source 
contains ambient, diffuse, specular and light at­tenuationparameters. Thelight attenuationis utilised 
to ensure that the light only illuminates the parts of the scene which are in close proximitytoit. The 
specular component aidsindetermining thelo­cation oftheprobein relationtothe objects surroundingit. Theillu­mination 
effects are achieved throughimplementing the techniques usingOpenGL and theOpenGLShading Language(GLSL). 
 2 Results To test the utility of the approach we have implemented our techniques within software for 
interacting with biomolecules (http://www.haptimol.com/).Figure1(a)illustratesthe Navigation Cube aidingtheuserinexploringaproteinrenderedinspace 
.lling mode. Figure 1(c) illustrates the Haptic Light assisting in object selectionin an elastic network 
model of aprotein.  References DOMINJON, L., LECUYER, A., BURKHARDT, J., ANDRADE-BARROSO,G., AND RICHIR,S.2005.The 
bubble technique: Interacting with large virtual environments using haptic devices withlimited workspace. 
In Proc. WorldHaptics, 639 640. Copyright is held by the author / owner(s). SIGGRAPH 2010, Los Angeles, 
California, July 25 29, 2010. ISBN 978-1-4503-0210-4/10/0007  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1837008</article_id>
		<sort_key>1630</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>152</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>6</seq_no>
		<title><![CDATA[Visual mining of neuro-metaspaces]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836845.1837008</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1837008</url>
		<abstract>
			<par><![CDATA[<p>Large scale neuroimaging data archival protocols are gradually becoming ubiquitous in both research as well as clinical settings. Current user-database interfaces are limited to textual searches and often require data-specific knowledge for performing queries. This is proving to be an obstacle for researchers who wish to obtain a holistic view of the data before designing pilot neuroscientific studies or even formulating statistical hypotheses. Instead of providing a restricted, unidimensional view of the data, we seek to place a multi-dimensional view of the entire neurodatabase at the user's disposal. With the aim of visual navigation of complete neuro-repositories, we introduce the concept of brain meta-spaces. The meta-space models the implicit nonlinear manifold where the neurological data resides, and encodes pair-wise dissimilarities between all individuals in a population. Additionally, the novelty in our approach lies in the user ability to simultaneously view and interact with many brains at once but doing so in a vast meta-space that encodes (dis)similarity in morphometry.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>H.2.8</cat_node>
				<descriptor>Data mining</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002951.10003227.10003351</concept_id>
				<concept_desc>CCS->Information systems->Information systems applications->Data mining</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2264694</person_id>
				<author_profile_id><![CDATA[81327489198]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Shantanu]]></first_name>
				<middle_name><![CDATA[H.]]></middle_name>
				<last_name><![CDATA[Joshi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of California, Los Angeles, Los Angeles]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264695</person_id>
				<author_profile_id><![CDATA[81466646370]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Ian]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bowman]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of California, Los Angeles, Los Angeles]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264696</person_id>
				<author_profile_id><![CDATA[81466640436]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Robin]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Jennings]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of California, Los Angeles, Los Angeles]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264697</person_id>
				<author_profile_id><![CDATA[81466643125]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[David]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hasson]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of California, Los Angeles, Los Angeles]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264698</person_id>
				<author_profile_id><![CDATA[81466645535]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Zhizhong]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Liu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of California, Los Angeles, Los Angeles]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264699</person_id>
				<author_profile_id><![CDATA[81408593387]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Arthur]]></first_name>
				<middle_name><![CDATA[W.]]></middle_name>
				<last_name><![CDATA[Toga]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of California, Los Angeles, Los Angeles]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264700</person_id>
				<author_profile_id><![CDATA[81453619830]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>7</seq_no>
				<first_name><![CDATA[John]]></first_name>
				<middle_name><![CDATA[D.]]></middle_name>
				<last_name><![CDATA[Van Horn]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of California, Los Angeles, Los Angeles]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[
Dinov, I., Van Horn, J. D., and et al. 2009. Efficient, distributed and interactive neuroimaging data analysis using the loni pipeline. <i>Frontiers in Neuroinformatics</i>, 38.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[
Joshi, S. H., Van Horn, J., and Toga, A. W. 2009. Interactive exploration of neuroanatomical meta-spaces. <i>Frontiers in Neuroinformatics 3</i>, 38.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Visual Mining of Neuro-Metaspaces Shantanu H. Joshi Ian Bowman Robin Jennings David Hasson Zhizhong 
Liu Arthur W. Toga John D. Van Horn Laboratory of Neuro Imaging, University of California, Los Angeles, 
Los Angeles, USA*  Figure 1: Two views of cortical representations of brains distributed in a meta-space. 
1 Introduction Large scale neuroimaging data archival protocols are gradually be­coming ubiquitous in 
both research as well as clinical settings. Cur­rent user-database interfaces are limited to textual 
searches and of­ten require data-speci.c knowledge for performing queries. This is proving to be an obstacle 
for researchers who wish to obtain a holistic view of the data before designing pilot neuroscienti.c 
studies or even formulating statistical hypotheses. Instead of pro­viding a restricted, unidimensional 
view of the data, we seek to place a multi-dimensional view of the entire neurodatabase at the user s 
disposal. With the aim of visual navigation of complete neuro-repositories, we introduce the concept 
of brain meta-spaces. The meta-space models the implicit nonlinear manifold where the neurological data 
resides, and encodes pair-wise dissimilarities be­tween all individuals in a population. Additionally, 
the novelty in our approach lies in the user ability to simultaneously view and in­teract with many brains 
at once but doing so in a vast meta-space that encodes (dis)similarity in morphometry. 2 Method The 
mining environment is composed of three primary stages, i) input-processing stage consisting of feature 
extraction, and repre­sentation, ii) data-analytics stage consisting of modeling, regres­sion, and clustering, 
and .nally iii) visualization stage that gives un­restricted, multi-faceted, 3D navigable and selectable 
views [Joshi et al. 2009] of the neuroimaging data. The input data comprises of structural neuroimaging 
acquisitions that are generated from 3D MRI scanners. The primary source of visualization is a surface 
ge­ometry representation of the cortex, obtained after pre-processing, skull removal, segmentation, cortical 
extraction, and topology cor­rection. All of these steps are automated and executed without user intervention 
on a grid [Dinov et al. 2009], and are only performed once, when new data is added into the repository. 
After prepro­cessing, we calculate several features on cortical surfaces, for e.g. shape foldedness index, 
principal curvatures, curvedness etc. Addi­tionally we also incorporate several volumetric features such 
as cor­tical thickness, and gray matter volumes, thereby de.ning a feature n-dimensional vector at each 
point on each cortex. The analyt­ics component is now responsible for constructing the meta-space by 
approximating the underlying manifold, purely based upon dis­criminative models on the above feature 
vectors. Finally, for visu­alization purposes, we project distances between features vectors in the Euclidean 
space after approximation by multi-dimensional scaling. The visualization environment is a dynamic view 
of the meta-space, enabling the user to navigate, discover, and verify the brain surface geometry simultaneously 
in relation to it s neighbors. Each brain surface is accompanied by an XML description of its meta-data 
that can be quickly displayed on the screen to get more information about the individual brain. 3 Conclusion 
and Future Work We foresee the development of graphical visualization tools that en­able and enhance 
scienti.c interaction with large-scale databases, as the next step in neuroimaging informatics. Though 
some basic image viewing tools exist, we have argued for a need for a next gen­eration visual interaction 
framework. We have also demonstrated a content-based solution that can be applied to any such archive 
in order for researchers to more easily examine dissimilarity between brains and to dynamically visualize 
patterns in the degree of prox­imity between brains. This may further be indicative of the demo­graphic 
and clinical attributes of the data themselves. In fact, all throughout our approach, we have made as 
few assumptions about the data as possible, and really let the data segregate itself based upon the characteristics 
of regional shape and geometry.  References DINOV, I., VAN HORN, J. D., AND ET AL. 2009. Ef.cient, dis­tributed 
and interactive neuroimaging data analysis using the loni pipeline. Frontiers in Neuroinformatics, 38. 
JOSHI, S. H., VAN HORN, J., AND TOGA, A. W. 2009. Inter­active exploration of neuroanatomical meta-spaces. 
Frontiers in Neuroinformatics 3, 38. * e-mail: {sjoshi,ian.bowman,rgjennings,david.hasson,zhizhong.liu,toga,jack.vanhorn}@loni.ucla.edu 
Copyright is held by the author / owner(s). SIGGRAPH 2010, Los Angeles, California, July 25 29, 2010. 
ISBN 978-1-4503-0210-4/10/0007  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1837009</article_id>
		<sort_key>1640</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>153</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>7</seq_no>
		<title><![CDATA[Visualization of multiple people's lifelog]]></title>
		<subtitle><![CDATA[collecting "Ant's-eye view" to generate "Bird's-eye view"]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836845.1837009</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1837009</url>
		<abstract>
			<par><![CDATA[<p>The concept of this study is to collect "ant's eye view" to generate "bird's eye view". When we can collect large number of ant's eye views, we can integrate them and can generate bird's eye view. The idea of this study is an assumption that we can grasp both whole view and situations at multiple places when we can see real-time-report from various points. To achieve this idea, we focus on lifelog technology. Using a wearable computer or small devices and sensors, it is easy to get our daily-life data. We can record our photos, sounds, positions, and so on. It will be lifelog data. When we can collect multiple people's lifelog data, we can utilize them much more. In this study, we propose a visualization method for multiple people's life log data. The lifelog data is uploaded to the server, and the viewer visualizes the data that provides us to see the whole view of the data. We combined position information and sensor information of remote places, and visualized these data.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.6</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2264701</person_id>
				<author_profile_id><![CDATA[81100461884]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Kunihiro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nishimura]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264702</person_id>
				<author_profile_id><![CDATA[81466645178]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Jun'ichi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nakano]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264703</person_id>
				<author_profile_id><![CDATA[81100172183]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Tomohiro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tanikawa]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264704</person_id>
				<author_profile_id><![CDATA[81100257385]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Michitaka]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hirose]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[
Cabspotting, http://cabspotting.org/
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[
Honda, INTERNAVI REALIZATION, http://www.honda.co.jp/internavi/realization/#/neuronroad
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[
Jun'ichi Nakano, Takashi Aoki, Kunihiro Nishimura, Tomohiro Tanikawa, Michitaka Hirose, "Robust Real-Time Lifelog Display System Using Picture Processing", ASIAGRAPH 2008, Vol.2, No.2, pp.238--241, 2008.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Visualization of Multiple People s Lifelog: Collecting "Ant's-eye view" to generate "Bird's-eye view" 
 Kunihiro Nishimura , Jun ichi Nakano , Tomohiro Tanikawa , Michitaka Hirose Graduate School of Information 
Science and Technology, the University of Tokyo  Fig. 1 System overview Fig. 2 Device 1. Introduction 
The concept of this study is to collect "ant's eye view" to generate "bird's eye view". When we can collect 
large number of ant's eye views, we can integrate them and can generate bird's eye view. The idea of 
this study is an assumption that we can grasp both whole view and situations at multiple places when 
we can see real-time-report from various points. To achieve this idea, we focus on lifelog technology. 
Using a wearable computer or small devices and sensors, it is easy to get our daily-life data. We can 
record our photos, sounds, positions, and so on. It will be lifelog data. When we can collect multiple 
people s lifelog data, we can utilize them much more. In this study, we propose a visualization method 
for multiple people s life log data. The lifelog data is uploaded to the server, and the viewer visualizes 
the data that provides us to see the whole view of the data. We combined position information and sensor 
information of remote places, and visualized these data. There are some examples of similar idea. In 
San Francisco, Cabspotting[1] is a project to provide whole view of taxi cabs position based on GPS information. 
It visualize trajectory of cabs by tracking them. INTERNAVI REALIZATION by Honda[2] is a screensaver 
that visualize car's moving points and their trajectory based on car navigation system. It visualize 
spontaneously main-street which many cars use. 2. Multiple People s Lifelog For collecting multiple 
people s lifelog data, we have developed a system based on a server and clients that capture each person 
s log (Fig. 1). The life-logging device consists of a mobile PC with a web camera, GPS, a posture sensor 
that can record the direction of camera, and cell-phone for data communication (Fig. 2). In this study, 
we take importance on email: { kuni, jnakano, tani, hirose }@cyber.t.u-tokyo.ac.jp Fig. 3 Visualization 
Results of Multipeople s Lifelog integrating multiple people s data, thus we only focus on camera and 
position information for lifelog data. The life-logging device uploads automatically an image with posture 
and position information using HTTP via cell phone network to the server in every 5 seconds. Every life-logging 
device has an ID that can distinguish users. For visualization of multiple people s lifelog data, we 
have implemented a viewer (Fig. 3). The viewer visualizes the data that are pulled from the server. Photos 
are distributed on a map based on position information; in addition, the direction of photos is corrected 
by posture information [3]. Thus, it is easy to understand relationships among photos. Experiments were 
conducted. The viewer visualizes multiple people s lifelog data almost in real time (every 5-6 sec). 
It is like live coverage of multiple television programs, thus it is easy to grasp a whole view of the 
target area where life-logging peoples are. It also increases a presence of the area when users are close 
place because they can see and sometimes can encounter each other.  3. Result and Discussion Visualization 
of multiple people s lifelog data almost in real-time is useful for grasping the same area. The applications 
of this visualization method are for events or groups in order to know popular or remarkable things. 
One concern is a privacy issue about sharing multiple people s lifelog data. Visualization methods based 
on privacy levels remain future works. References [1] Cabspotting, http://cabspotting.org/ [2] Honda, 
INTERNAVI REALIZATION, http://www.honda.co.jp/internavi/realization/#/neuronroad [3] Jun'ichi Nakano, 
Takashi Aoki, Kunihiro Nishimura, Tomohiro Tanikawa, Michitaka Hirose, "Robust Real-Time Lifelog Display 
System Using Picture Processing", ASIAGRAPH 2008, Vol.2, No.2, pp.238-241, 2008. Copyright is held by 
the author / owner(s). SIGGRAPH 2010, Los Angeles, California, July 25 29, 2010. ISBN 978-1-4503-0210-4/10/0007 
  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1837010</article_id>
		<sort_key>1650</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>154</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>8</seq_no>
		<title><![CDATA[Visualizing empires decline]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836845.1837010</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1837010</url>
		<abstract>
			<par><![CDATA[<p>This is an information visualization project that narrates the decline of the British, French, Portuguese and Spanish empires during the 19th and 20th centuries. These empires were the main maritime empires in terms of land area during the referred centuries [Wikipedia]. The land area of the empires and its former colonies is continuously represented in the simulation. The size of the empires varies during the simulation as they gain, or lose, territories. The graphic representation forms were selected to attain a narrative that depicts the volatility, instability and dynamics of the expansion and decline of the empires. Furthermore, the graphic representation also aims at emphasizing the contrast between their maximum and current size, and portraying the contemporary heritage and legacy of the empires.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.6</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.8</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010507</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Graphics recognition and interpretation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2264705</person_id>
				<author_profile_id><![CDATA[81318497551]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Pedro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Cruz]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Coimbra, Portugal]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264706</person_id>
				<author_profile_id><![CDATA[81100379916]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Penousal]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Machado]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Coimbra, Portugal]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Jakobsen, T. Advanced Character Physics. <i>http://www.teknikus.dk/tj/gdc2001.htm.</i>
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Schmidt, K. toxiclibs. <i>http://code.google.com/p/toxiclibs.</i>
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Wikipedia. List of largest empires. <i>http://en.wikipedia.org/wiki/List_of_largest_empires.</i>
]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Visualizing empires decline Pedro Cruz1 and Penousal Machado2 CISUC, Department of Informatics Engineering, 
University of Coimbra, Portugal  Figure 1 On the left, a snapshot of the simulation in .... when most 
of the French colonies gain their independence. On the right, the end of the simulation after .... and 
displays all the former colonies stabilized in their geographic locations. 1 Introduction This is an 
information visualization project that narrates the decline of the British, French, Portuguese and Spanish 
em­pires during the ..th and ..th centuries. These empires were the main maritime empires in terms of 
land area during the referred centuries [W........]. The land area of the empires and its former colonies 
is continuously represented in the simulation. The size of the empires varies during the simula­tion 
as they gain, or lose, territories. The graphic representa­tion forms were selected to attain a narrative 
that depicts the volatility, instability and dynamics of the expansion and de­cline of the empires. Furthermore, 
the graphic representation also aims at emphasizing the contrast between their maxi­mum and current size, 
and portraying the contemporary heri­tage and legacy of the empires. Soft-bodies are employed to represent 
the volatility and dy­namic nature of the empires. The physics engine responsible by intra and inter 
bodies interactions was implemented using springs, which promotes an aggressive behavior between .uid 
forms. These complex interactions between graphic represen­tation forms are used to synthesize large 
quantities of data and extract signi.cant conclusions. By these means, a simpli­.ed, compact and ludic 
narrative of the expansion and de­cline of these empires over a period of more than two centu­ries is 
obtained. 2 Implementation and behavior A circle that looks and acts like a soft-body is used for the 
representation of each empire. The area of each circle is di­rectly proportional to the area of the corresponding 
empire. When a colony gains independence a new soft-body is cre­ated that persists in time and is attracted 
to the geographical position of the colony. The behavior of each soft-body is im­plemented building a 
skeleton for the circle by connecting particles with springs. The springs implementation is pro­vided 
by toxiclibs .. physics engine [S......], which pro­vides Verlet integration that tends to be more stable 
than the classic Euler or the Runge-Kutta methods [J.......]. Springs are also used to implement the 
forces that act in the 1 pmcruz@student.dei.uc.pt 2 machado@dei.uc.pt Copyright is held by the author 
/ owner(s). SIGGRAPH 2010, Los Angeles, California, July 25 29, 2010. ISBN 978-1-4503-0210-4/10/0007 
 simulation world, being able to present a behavior that in­cludes collisions, attractions, repulsions, 
etc. For this purpose, all the particles in each body are evaluated against the parti­cles of other bodies. 
To attain repulsion, springs are created when the distance between particles of different bodies be­comes 
inferior to a predetermined value. When the particles become distant these springs are deleted. The outcome 
of this process in a non-deterministic simulation, that exhibits dif­ferent, yet similar, behaviors for 
the same data. Thus, the narrative is preserved. Nevertheless, the non-deterministic nature may cause 
glitches in the interactions between bodies. Figure 2 Detail of the springs responsible for the system 
s behavior. The white lines represent the springs that form each body skeleton, as well as the per body 
geographic attractor. The temporary springs that implement bodies collisions are represented in cyan. 
The simulation depicts the period of ....-.... using a non­linear timeline. The narrative advances one 
year per second, speeding up when no colonies become independent in the near future, which results in 
an animation of . minutes and .. seconds. The animations produced were published online on ..... and 
........ They spread virally through the blogosphere collecting ....... views, and gathering the attention 
of graphic designers, information visualization enthusiasts and history teachers. References J......., 
T. Advanced Character Physics. http://www.teknikus.dk/tj/gdc2001.htm. S......, K. toxiclibs. http://code.google.com/p/toxiclibs. 
W......... List of largest empires. http://en.wikipedia.org/wiki/List_of_largest_empires.  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1837011</article_id>
		<sort_key>1660</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>155</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>9</seq_no>
		<title><![CDATA[Visualizing the history of ideas]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1836845.1837011</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1837011</url>
		<abstract>
			<par><![CDATA[<p>This presentation surveys the field of citation data visualization and presents new prototypes that illuminate broad intellectual regions (epistemes) through their temporal structures (paradigm shifts).</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.8</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010507</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Graphics recognition and interpretation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2264707</person_id>
				<author_profile_id><![CDATA[81466647701]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Ian]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wojtowicz]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Massachusetts Institute of Technology, Culture & Technology, Cambridge, MA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Visualizing the History of Ideas Ian Wojtowicz Massachusetts Institute of Technology Program in Art, 
Culture &#38; Technology 400 Main St, E14-140Q Cambridge, MA, 02142 iwoj@mit.edu This presentation surveys 
the field of citation data visualization and presents new prototypes that illuminate broad intellectual 
regions (epistemes) through their temporal structures (paradigm shifts). Eugene Garfield founded the 
contemporary field of tracking and measuring the dynamics of scientific publishing in the 1955 when he 
wrote a seminal article for Science entitled Citation Indexes for Science: A New Dimension in Documentation 
through Associate of Ideas. A few years later, Garfield went on to start found the Institute for Scientific 
Information (ISI) whose various citation indexes (grouped together in their flagship product, The Web 
of Science) have for decades been the primary source of information about the growth dynamics of scientific 
knowledge in the English­speaking world. Concurrently, the historian of science Thomas Kuhn published 
his important text The Structure of Scientific Revolutions, in which he contests the notion of gradual 
change in scientific thinking. Instead, Kuhn posits that scientific ideas establish paradigms within 
which change is slow and referential, marked by occasional rapid revolutionary moments where new ideas 
replace existing ones. To go beyond science into other forms of knowledge production, it is useful to 
follow Michel Foucault s notion of the episteme as a historical a priori [in which] ideas could appear, 
sciences be established, experience be reflected in philosophies, rationalities to be formed. (Foucault, 
xxiii) It is within this notion that the desire for mapping the history of ideas can take striking new 
forms. With the growth of the Internet and ever more accessible databases of public knowledge, the field 
of data visualization has grown from a specialization into a necessity of understanding contemporary 
society. With this in mind, this talk will survey the current state of knowledge visualization along 
with examples of designs in other domains to suggest future directions that will bring alive the process 
of exploring ideas in throughout recorded history.   Figure 2: An example of how complex historical 
citation information can be reduced to a much simpler visual representation.  Figure 3: An example of 
how these citation histories can be portrayed compactly along with search criteria. Copyright is held 
by the author / owner(s). SIGGRAPH 2010, Los Angeles, California, July 25 29, 2010. ISBN 978-1-4503-0210-4/10/0007 
 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	</section>
</content>
</proceeding>
