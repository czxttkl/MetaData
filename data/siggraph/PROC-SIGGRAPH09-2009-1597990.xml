<?xml version="1.0" encoding="ISO-8859-1"?>
<!DOCTYPE proceeding SYSTEM "proceeding.dtd">
<proceeding ver="6.0" ts="04/10/2010">
<conference_rec>
	<conference_date>
		<start_date>08/03/2009</start_date>
		<end_date>08/07/2009</end_date>
	</conference_date>
	<conference_loc>
		<city><![CDATA[New Orleans]]></city>
		<state>Louisiana</state>
		<country></country>
	</conference_loc>
	<conference_url></conference_url>
</conference_rec>
<series_rec>
	<series_name>
		<series_id>SERIES382</series_id>
		<series_title><![CDATA[International Conference on Computer Graphics and Interactive Techniques]]></series_title>
		<series_vol></series_vol>
	</series_name>
</series_rec>
<proceeding_rec>
	<proc_id>1597990</proc_id>
	<acronym>SIGGRAPH '09</acronym>
	<proc_desc>SIGGRAPH 2009: Talks</proc_desc>
	<conference_number>2009</conference_number>
	<proc_class>conference</proc_class>
	<proc_title></proc_title>
	<proc_subtitle></proc_subtitle>
	<proc_volume_no></proc_volume_no>
	<isbn13>978-1-60558-834-6</isbn13>
	<issn></issn>
	<eissn></eissn>
	<copyright_year>2009</copyright_year>
	<publication_date>08-03-2009</publication_date>
	<pages>82</pages>
	<plus_pages></plus_pages>
	<price><![CDATA[]]></price>
	<other_source></other_source>
	<abstract>
		<par><![CDATA[<p>SIGGRAPH 2009 Talks provide a broad spectrum of presentations on recent achievements in all areas of computer graphics and interactive techniques, including art, design, animation, visual effects, interactive music, research, interactivity, and engineering.</p> <p>Talks often highlight the latest developments before their formal publication, present ideas that are still in progress, or showcase work in related fields that makes use of computer graphics and interactive techniques. Talks can also take you behind the scenes and into the minds of SIGGRAPH 2009 creators.</p>]]></par>
	</abstract>
	<publisher>
		<publisher_id>PUB27</publisher_id>
		<publisher_code>ACMNY</publisher_code>
		<publisher_name>ACM</publisher_name>
		<publisher_address>2 Penn Plaza, Suite 701</publisher_address>
		<publisher_city>New York</publisher_city>
		<publisher_state>NY</publisher_state>
		<publisher_country>USA</publisher_country>
		<publisher_zip_code>10121-0701</publisher_zip_code>
		<publisher_contact>Bernard Rous</publisher_contact>
		<publisher_phone>212 869-7440</publisher_phone>
		<publisher_isbn_prefix></publisher_isbn_prefix>
		<publisher_url>www.acm.org/publications</publisher_url>
	</publisher>
	<sponsor_rec>
		<sponsor>
			<sponsor_id>SP932</sponsor_id>
			<sponsor_name>ACM Special Interest Group on Computer Graphics and Interactive Techniques</sponsor_name>
			<sponsor_abbr>SIGGRAPH</sponsor_abbr>
		</sponsor>
	</sponsor_rec>
	<categories>
		<primary_category>
			<cat_node/>
			<descriptor/>
			<type/>
		</primary_category>
	</categories>
	<ccc>
		<copyright_holder>
			<copyright_holder_name>ACM</copyright_holder_name>
			<copyright_holder_year>2009</copyright_holder_year>
		</copyright_holder>
	</ccc>
</proceeding_rec>
<content>
	<article_rec>
		<article_id>1597991</article_id>
		<sort_key>10</sort_key>
		<display_label>Article No.</display_label>
		<display_no>1</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[well--formed.eigenfactor]]></title>
		<subtitle><![CDATA[visualizing information flow in science]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1597990.1597991</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1597991</url>
		<abstract>
			<par><![CDATA[<p>well--formed.eigenfactor presents interactive visualizations to explore emerging patterns in scientific citation networks. The Eigenfactor project calculates a measure of importance for individual journals -- the Eigenfactor score -- as well as measures of citation flow and a hierarchical clustering based thereon. Moritz Stefaner turns this information into a set of four information -- aesthetic visualizations, each highlighting different aspects of the data.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.5</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>A.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061.10010063</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures->Computational geometry</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P1617586</person_id>
				<author_profile_id><![CDATA[81350572664]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Moritz]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Stefaner]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617587</person_id>
				<author_profile_id><![CDATA[81442615587]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Martin]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Rosvall]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617588</person_id>
				<author_profile_id><![CDATA[81414618154]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Carl]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bergstrom]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1187772</ref_obj_id>
				<ref_obj_pid>1187627</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[fn1 Holten, D. (2006): Hierarchical Edge Bundles: Visualization of Adjacency Relations in Hierarchical Data. IEEE Transactions on Visualization and Computer Graphics, Volume 12, Issue 5, pp. 741-748.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[fn2 Bruls, M., Huizing, K. and van Wijk, J.J. (2006): Squarified Treemaps. Proceedings of the Joint Eurographics and IEEE TCVG Symposium on Visualization, pp. 33-42.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 well formed.eigenfactor: visualizing information flow in science Moritz Stefaner, Martin Rosvall, Carl 
Bergstrom well formed.eigenfactor1 presents interactive visualizations to explore emerging patterns in 
scientific citation networks. The Eigenfactor project calculates a measure of importance for individual 
journals the Eigenfactor score as well as measures of citation flow and a hierarchical clustering based 
thereon. Moritz Stefaner turns this information into a set of four information aesthetic visualizations, 
each high­lighting different aspects of the data. In visualizations of citation networks, both ball and-stick 
like network representations as well as maps are prevalent. Our project extends the visual vocabulary 
in this area: on the one hand, by re-purposing existing techniques, such as radial edge bundling and 
treemaps; on the other hand, by inventing novel ap­proaches like magnetic pins as flow indicators and 
an alluvial diagram to represent change over time in cluster structure. Citation patterns: This radial 
diagram gives an overview of the citation net­work. The colors mark large groups of journals, further 
subdivided into fields in the outer ring; segments of the inner ring represent the journals, scaled by 
Eigenfactor Score. The citation links follow the cluster structure, using the hierarchical edge bundling 
technique2. Change over time: This alluvial diagram displays changes in Eigenfactor score and clustering 
over time. The journals are grouped vertically by their cluster structure and horizontally by year. Bars 
belonging to the same journal are connected. Clicking highlights a journal over the years, and all clusters 
it has been part of, to track changes of influence and cluster structure. Clustering: Based on the squarified 
treemap layout algorithm3, this visualiza­tion features magnetic pins to indicate both incoming and outgoing 
citation flow for any selected journal. The size of square corresponds to the Eigenfac­tor score of the 
corresponding journal. Map: This map visualization puts journals, which frequently cite each other, closer 
together. You can drag the white magnification lens around to enlarge a part of the map for closer inspection. 
Clicking one of the nodes will high­light all its citation connections. Data: We use a subset of the 
citation data from Thomson Reuters' Journal Citation Reports 1997 2005. For the visualizations, 400 journals 
with their ca. 13 000 citation edges were selected, ensuring a cover­age of the top journals in each 
field. 1 http://well-formed.eigenfactor.org 2 Holten, D. (2006): Hierarchical Edge Bundles: Visualization 
of Adjacency Relations in Hierarchical Data. IEEE Transactions on Visualization and Computer Graphics, 
Volume 12 , Issue 5, pp. 741-748. 3 Bruls, M., Huizing, K. and van Wijk, J.J. (2006): Squarified Treemaps. 
Proceedings of the Joint Eurographics and IEEE TCVG Symposium on Visualization, pp. 33-42. Copyright 
is held by the author / owner(s). SIGGRAPH 2009, New Orleans, Louisiana, August 3 7, 2009. ISBN 978-1-60558-726-4/09/0008 
 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1597992</article_id>
		<sort_key>20</sort_key>
		<display_label>Article No.</display_label>
		<display_no>2</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Synchronous objects for one flat thing, reproduced]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1597990.1597992</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1597992</url>
		<abstract>
			<par><![CDATA[<p><i>Synchronous Objects for One Flat Thing, reproduced</i> (http://synchronousobjects.osu.edu) is an interactive screen-based work developed by The Ohio State University's Advanced Computing Center for the Arts and Design (ACCAD) and the Department of Dance in collaboration with renowned choreographer William Forsythe. Pivoting on Forsythe's masterwork of visual complexity, <i>One Flat Thing, Reproduced</i>, as its research resource, the Synchronous Objects project seeks to enrich cross-disciplinary investigation and creativity by revealing deep structures of choreographic thinking through a vivid collection of <i>information objects</i> in the form of 3D computer animation, annotation and interactive graphics.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.6</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>A.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P1617589</person_id>
				<author_profile_id><![CDATA[81319499178]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Maria]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Palazzi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The Ohio State University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617590</person_id>
				<author_profile_id><![CDATA[81442607060]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Norah]]></first_name>
				<middle_name><![CDATA[Zuniga]]></middle_name>
				<last_name><![CDATA[Shaw]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The Ohio State University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Synchronous Objects for One Flat Thing, reproduced Maria Palazzi Norah Zuniga Shaw The Ohio State University 
The Ohio State University palazzi.1@osu.edu zuniga-shaw.1@osu.edu 1. Introduction Synchronous Objects 
for One Flat Thing, reproduced (http://synchronousobjects.osu.edu) is an interactive screen-based work 
developed by The Ohio State University's Advanced Computing Center for the Arts and Design (ACCAD) and 
theDepartment of Dance in collaboration with renowned choreographer William Forsythe. Pivoting on Forsythe 
s masterwork of visual complexity, One Flat Thing, Reproduced, as its research resource, the Synchronous 
Objects project seeks to enrich cross-disciplinary investigation and creativity by revealing deep structures 
of choreographic thinking through a vivid collection of information objects in the form of 3D computer 
animation, annotation and interactive graphics.  2. Context The strategies inherent in dance are notoriously 
difficult tocapture and to document. In spite of the obstacles, choreographerWilliam Forsythe challenged 
our research group to develop a newkind of generative dance literature or a set of core visual references 
 to stimulate the exchange of ideas and innovation for students and professionals in a wide range of 
practices. His choreography in One Flat Thing, reproduced is particularlyexciting to analyze, due to 
the challenges it poses for visualizing ahigh density of interdependent relationships distributed across 
anetwork of 17 dancers navigating a landscape of a 20-table grid and resulting in a contrapuntal dance 
composition. 3. Process Ohio State co-creative directors Maria Palazzi and Norah Zuniga Shaw gathered 
a multidisciplinary team of researchers from architecture, cognitive science, computer science, dance, 
design,geography, philosophy, and statistics to apply and cross-pollinate their disciplinary visualization 
methodologies in examining Forsythe s strategies. Starting with One Flat Thing, reproduced as the research 
resource, we located and defined the dance s component parts or what we understood as the data of the 
piece.Not the choreography but the choreographic and performancestructures or systems. There are no existing 
methods by which toconduct this particular kind of dance analysis. The core of ourresearch involved extensive 
work with William Forsythe and TheForsythe Company to systematically analyze the material andsystems 
of exchange that make up One Flat Thing, reproduced. The Ohio State research team viewed and dissected 
the complex intertwining pieces of the dance, learned its history and origins,examined what it reveals 
about human perception in complexenvironments and explored its relationship to complexity science and 
information aesthetics.  As we parsed the dance into its hundreds of component parts wewere challenged 
to determine means of quantifying this data andthen using it to drive both concrete and abstract interpretations,transformations, 
derivations and interactive creative tools. With this growing data set in hand, nothing was off-limits, 
as Forsythe was open to radical reductions as well as elaborate visual embellishment. System of Alignments 
Animation Animator: Joshua Fry  The team created information visualizations of the dance that stand 
beside and apart from the dancers. Furthermore, the visualizations become a legible graphical language 
and an accessible conceptual framework through which researchers in other non-dance fields can approach 
dance as an interdisciplinaryresource for ideas about space, structure and movement. This work underscores 
the profound impact possible in collaborationsbetween major artists and interdisciplinary research teams 
usinginnovative and interpretive information visualization methods inmaking meaningful visual literatures 
that have relevance in contemporary society. Copyright is held by the author / owner(s). SIGGRAPH 2009, 
New Orleans, Louisiana, August 3 7, 2009. ISBN 978-1-60558-726-4/09/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1597993</article_id>
		<sort_key>30</sort_key>
		<display_label>Article No.</display_label>
		<display_no>3</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[GreenLite Dartmouth]]></title>
		<subtitle><![CDATA[unplug or the polar bear gets it]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1597990.1597993</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1597993</url>
		<abstract>
			<par><![CDATA[<p>With glaciers melting, sea levels rising and natural disasters---such as hurricanes and cyclones---intensifying, climate change is a growing concern. While innovations in renewable energy are critical, research shows that changing energy use behavior has become increasingly important in the fight against global warming. GreenLite Dartmouth focuses on changing behavior by making energy conservation a priority for students by creating both an intellectual and emotional connection between daily actions and their adverse effects on the environment. We combine computer graphics, art, engineering, sociology, environmental science, systems-thinking and behavioral psychology to turn real-time energy use data into a meaningful interactive display. GreenLite employs innovative methods for displaying complex data using interactivity, storytelling, animation, competition and goal-setting. Appealing animated information-display and "mood" algorithms put data into context to make it meaningful. We incorporate a system of digital energy meters, a custom database, computational analysis, 2D and 3D animations, interactive design and a game-engine to spur behavior change and, hopefully, reverse the course of climate change.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>A.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P1617591</person_id>
				<author_profile_id><![CDATA[81442593606]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Evan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tice]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Dartmouth College]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617598</person_id>
				<author_profile_id><![CDATA[81442619155]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Tim]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tregubov]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Dartmouth College]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617599</person_id>
				<author_profile_id><![CDATA[81442617630]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Kate]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Schnippering]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Dartmouth College]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617600</person_id>
				<author_profile_id><![CDATA[81442608667]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Yoon-Ki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Park]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Dartmouth College]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617601</person_id>
				<author_profile_id><![CDATA[81442618477]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Ray]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[diCiaccio]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Dartmouth College]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617602</person_id>
				<author_profile_id><![CDATA[81442600628]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Max]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Friedman]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Dartmouth College]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617603</person_id>
				<author_profile_id><![CDATA[81442610530]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>7</seq_no>
				<first_name><![CDATA[Jennifer]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Huang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Dartmouth College]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617604</person_id>
				<author_profile_id><![CDATA[81442614602]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>8</seq_no>
				<first_name><![CDATA[Justin]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Slick]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Dartmouth College]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617605</person_id>
				<author_profile_id><![CDATA[81442616277]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>9</seq_no>
				<first_name><![CDATA[Giulia]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Siccardo]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Dartmouth College]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617592</person_id>
				<author_profile_id><![CDATA[81442619345]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>10</seq_no>
				<first_name><![CDATA[Jessica]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Glago]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Dartmouth College]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617593</person_id>
				<author_profile_id><![CDATA[81442619315]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>11</seq_no>
				<first_name><![CDATA[Stephanie]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Trudeau]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Dartmouth College]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617594</person_id>
				<author_profile_id><![CDATA[81442615856]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>12</seq_no>
				<first_name><![CDATA[Daniel]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gobaud]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Dartmouth College]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617595</person_id>
				<author_profile_id><![CDATA[81442596914]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>13</seq_no>
				<first_name><![CDATA[Daniel]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Garcia]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Dartmouth College]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617596</person_id>
				<author_profile_id><![CDATA[81442604703]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>14</seq_no>
				<first_name><![CDATA[Craig]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Slagel]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Dartmouth College]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617597</person_id>
				<author_profile_id><![CDATA[81100576482]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>15</seq_no>
				<first_name><![CDATA[Lorie]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Loeb]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Dartmouth College]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 GreenLite Dartmouth: Unplug or the Polar Bear Gets It Evan Tice · Tim Tregubov · Kate Schnippering · 
Yoon-Ki Park · Ray diCiaccio · Max Friedman · Jennifer Huang · Justin Slick · Giulia Siccardo · Jessica 
Glago · Stephanie Trudeau · Daniel Gobaud · Daniel Garcia · Craig Slagel · Lorie Loeb Dartmouth College 
· June 22, 2009 With glaciers melting, sea levels rising and natural disasters such as hurricanes and 
cyclones intensifying, climate change is a growing concern. While innovations in renewable energy are 
critical, research shows that chang­ing energy use behavior has become increasingly impor­tant in the 
.ght against global warming. GreenLite Dart­mouth focuses on changing behavior by making energy conservation 
a priority for students by creating both an in­tellectual and emotional connection between daily actions 
and their adverse e.ects on the environment. We combine computer graphics, art, engineering, sociology, 
environ­mental science, systems-thinking and behavioral psychol­ogy to turn real-time energy use data 
into a meaningful interactive display. GreenLite employs innovative meth­ods for displaying complex data 
using interactivity, story­telling, animation, competition and goal-setting. Appeal­ing animated information-display 
and mood algorithms put data into context to make it meaningful. We incorpo­rate a system of digital 
energy meters, a custom database, computational analysis, 2D and 3D animations, interac­tive design and 
a game-engine to spur behavior change and, hopefully, reverse the course of climate change. GreenLite 
Dartmouth visualizes complex, real-time en­ergy data using interactive animations. When electricity use 
is low, for example, the animated polar bear is happy and playful: As use goes up, the bear becomes distressed 
and his well-being is endangered. We use a custom mood algorithm to determine which animation to display 
based on historical data and predic­tions of what energy use should be on a particular time and day. 
We created three-dimensional animations using motion capture and Autodesk Maya. At runtime, the an­imations 
run in the powerful Unity3D browser plugin; we also have a .ash version of the animations. We display 
content on low-energy displays in dormitory halls, a web­site, and a desktop widget. We chose the polar 
as the .rst character for the animation because it is an animal whose existence is signi.cantly impacted 
by climate change. In our animated world, the actions of the bear re.ect the amount of energy being used; 
to keep the bear safe and happy, dorm power use must improve. We supplement the animation with text and 
images about a user s goals, progress towards these goals, and information about cur­rent energy use. 
With just a quick glance, students can monitor the polar bear s state and instantly be informed about 
their energy impact without having to ponder any di.cult graphs or trying to decipher scienti.c units. 
As smart-grid technology brings digital meters into businesses, institutions and homes, there is a growing 
in­terest in displaying energy-use data via the web or mo- Copyright is held by the author / owner(s). 
SIGGRAPH 2009, New Orleans, Louisiana, August 3 7, 2009. ISBN 978-1-60558-726-4/09/0008 bile devices. 
Data is generally displayed using charts and graphs that focus on the amount of energy used and mon­etary 
cost. Over 40 studies have shown that this type of display can reduce energy use by 5-10%. Charts and 
graphs show important information, but they are not ap­pealing to a wide demographic. Our work tries 
to appeal to a wider group, tell a bigger story and produce a larger reduction in energy use. We translate 
data from ticks on a meter to numbers in a database and then into interactive animation. We use appealing 
animated characters in set­tings a.ected by climate change to turn real-time energy use into a meaningful 
story about current energy use and how it compares to prior behavior, where energy comes from, how our 
energy use behavior compares to others and the impact of our behavior on the environment. We believe 
this emotional connection will produce a signi.­cantly larger reduction in energy use and will appeal 
to a wider group of people including children and teens who could become the driving force in energy 
conservation. Initial tests of a prototype system in four dorms, resulted in a reduction of electric 
use (plug load and lighting only) of 14% and 22% on the two .oors. We will conduct addi­tional user studies 
in April and May, 2009. GreenLite Dartmouth combines computer science, in­formation visualization, digital 
art, engineering, behav­ioral psychology and environmental science to create a system that: Pulls data 
from digital meters,  Stores the data in a custom database,  Analyzes the data using a mood algorithm 
which looks at historical data and predicts what current energy use should be at this time and on this 
day,  Uses the mood score to call up animations in which the health and happiness of a polar bear is 
depen­dent on energy use, thereby creating an emotional connection between students and the data and 
an understanding of the impacts of their actions,  Analyzes the data with contextual mapping,  Displays 
the animations and graphs on the web, kiosks, widgets and social network applications,  Visualizes short 
term and long term data,  Normalizes data, and compares it using matrices,  Provides data on multiple 
types of energy use such as electricity, water, heat and printing.  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1597994</article_id>
		<sort_key>40</sort_key>
		<display_label>Article No.</display_label>
		<display_no>4</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[Non-reflective boundary conditions for incompressible free surface fluids]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1597990.1597994</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1597994</url>
		<abstract>
			<par><![CDATA[<p>We have developed a novel approach to open-boundaries for fluid animations. More specifically we present a highly efficient energy absorbing boundary condition for the incompressible Navier-Stokes equations in the prescence of a free surface. Our work extends and adapts a Perfectly Matched Layer (PML) approach [Berenger 1994; Johnson 2007], recently developed for the Navier-Stokes equations, to free surfaces in the context fluid animations. We show how our PML boundary condition is able to effectively eliminate reflections generated by the presence of solid boundaries in the simulation domain, and that our method is far superior to simpler approaches for reducing wave reflection. Furthermore, we have adapted our theoretical PML model to work with the Stable-Fluids Eulerian Navier-Stokes solver commonly used in computer graphics. Finally, we show that the cost of deploying our method in terms of memory and additional computations is small, and for a given quality significantly less than other known methods.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Boundary representations</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>A.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010400</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Point-based models</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P1617606</person_id>
				<author_profile_id><![CDATA[81319501753]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Andreas]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[S&#246;derstr&#246;m]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Link&#246;ping University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617607</person_id>
				<author_profile_id><![CDATA[81324492244]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Ken]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Museth]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Link&#246;ping University and Digital Domain, Inc. and Dream Works Animation, Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Berenger, J.-P. 1994. A perfectly matched layer for the absorption of electromagnetic waves. <i>Journal of Computational Physics 114</i>, 2, 185--200.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Johnson, S. G., 2007. Notes on perfectly matched layers. online MIT course notes (Aug. 2007).]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Non-Re.ective Boundary Conditions For Incompressible Free Surface Fluids Andreas S¨om1 and Ken Museth1,2,3 
oderstr¨1Link¨oping University, 2Digital Domain, Inc. and 3DreamWorks Animation, Inc. Figure 1: Frames 
from a free surface .uid animation employing our novel non-re.ective (i.e. open) boundary conditions. 
Note how the waves from the moving object (e.g. boat) are not re.ected when they hit the boundary of 
the .uid simulation domain, thus emulating an open ocean. Abstract: We have developed a novel approach 
to open-boundaries for .uid animations. More speci.cally we present a highly ef.­cient energy absorbing 
boundary condition for the incompressible Navier-Stokes equations in the prescence of a free surface. 
Our work extends and adapts a Perfectly Matched Layer (PML) ap­proach [Berenger 1994; Johnson 2007], 
recently developed for the Navier-Stokes equations, to free surfaces in the context .uid an­imations. 
We show how our PML boundary condition is able to effectively eliminate re.ections generated by the presence 
of solid boundaries in the simulation domain, and that our method is far superior to simpler approaches 
for reducing wave re.ection. Fur­thermore, we have adapted our theoretical PML model to work with the 
Stable-Fluids Eulerian Navier-Stokes solver commonly used in computer graphics. Finally, we show that 
the cost of deploying our method in terms of memory and additional computations is small, and for a given 
quality signi.cantly less than other known methods. Overview: Imagine a ship moving rapidly across the 
surface of a wide ocean. The water breaking around its bow as it cuts trough the waves and the powerful 
swell left in its wake as the massive vessel presses on towards its destination. Now imagine that you 
want to make an accurate and visually pleasing simulation of this scenario. This requires the use of 
a high quality simulation engine capable of resolving non-linear behaviour such as breaking waves. Due 
to the high computational cost the volume of .uid that can be simulated will typically be restricted 
to a close neighbourhood around the hull of the ship. Walls (i.e. boundaries) must then be added in order 
to contain the water within this region. The issue is now that if the simulation is allowed to run long 
enough the waves generated by the ship will travel outwards and eventually re.ect back towards the ship. 
The resulting wave pattern (i.e. inference) will be far from what is to be expected on an open body of 
water, thus breaking the illusion of a wide ocean. However, by deploying our energy dampening boundaries 
at the edge of the simulation domain these wave re.ections can be automatically cancelled out. Making 
the Finite Appear In.nite: The core of our problem is es­sentially that we attempt to make a .nite simulation 
domain behave like it is in.nitly large. In order to realize this we focus on the most obvious difference 
between these scenarios -wave re.ection from the walls containing our simulated body of water. We want 
to create boundaries that contain the .uid, but at the same time eliminate the re.ection of waves entering 
the boundary region. We address this problem by modifying the governing equations of the .uid close to 
the actual boundaries. More speci.cally we transform the Navier-Stokes equations in a neighberhood close 
to the actual walls such that they dampen waves moving trough this boundary region in a given direction. 
The governing equations in the boundary region become . u .p = f + µ.2u - (u · .)u -- sxAq1 - syAq2 - 
szAq3 (1) .t . .u = -sxBq1 - syBq2 - szBq3 (2) where we introduce the constants A and B together with 
the auxil­lary vector .elds q1, q2 and q3. These auxillary .elds only exist in the boundary region and 
will act to counter and dampen any waves that enter. As can be seen in equation (2) the auxillary variables 
also enter into the continuity equation, allowing for sources and sinks in our incompressible medium. 
The functions sx, sy and sz describe the magnitude of the dampen­ing and are applied trough a transfer 
function such that full damp­enig is achieved close to the physical walls of the simulation. The dampening 
is then reduced towards zero as we approach virtual in­ternal boundaries inside the simulation domain. 
Performance impact: Our method requires solving a modi.ed Navier-Stokes equation with four vector .elds 
instead of one. The computations on each of these .elds are similar to those for solving the regular 
Navier-Stokes equations. Thus, a conservative upper bound for the computational and memory cost will 
be four times what is used without our boundaries. However, the additional equa­tions only need to be 
solved in the boundary region which is typi­cally very small -in our tests only 5 to 10 percent of the 
width of the simulation domain. Thus the typical impact on memory and time will be signi.cantly less 
than the conservative case. References BERENGER, J.-P. 1994. A perfectly matched layer for the ab­sorption 
of electromagnetic waves. Journal of Computational Physics 114, 2, 185 200. JOHNSON, S. G., 2007. Notes 
on perfectly matched layers. online MIT course notes (Aug. 2007). Copyright is held by the author / owner(s). 
SIGGRAPH 2009, New Orleans, Louisiana, August 3 7, 2009. ISBN 978-1-60558-726-4/09/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1597995</article_id>
		<sort_key>50</sort_key>
		<display_label>Article No.</display_label>
		<display_no>5</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>5</seq_no>
		<title><![CDATA[An efficient level set toolkit for visual effects]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1597990.1597995</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1597995</url>
		<abstract>
			<par><![CDATA[<p>Over the past couple of years we have developed various tools for visual effects in movie production based on level sets. While most VFX studios, and even some commercial software packages, have had level sets tools for several years we are, to the best of our knowledge, among the first to employ highly optimized data structures. These compact and very fast volumetric data structures allow us to work at unprecedented grid resolutions, which in turn opens the door to new applications. The last two years we have demonstrated this with level set tools specifically developed for fluid surfacing in "Pirates of the Caribbean: At World's End" [Museth et al. 2007] and fragmentation in "The Mummy: Tomb Of The Dragon Emperor" [Museth and Clive 2008], However, none of these past presentations actually discussed the details of the novel data structure, DB-Grid, which forms the very backbone of our level set implementations. The same was true for many of our fundamental level set tools, like the fast and robust scan-converter of self-intersecting polygonal meshes. These are exactly the topics of the current presentation. We will also show how these tools are tightly integrated with Houdini.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>A.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P1617608</person_id>
				<author_profile_id><![CDATA[81324492244]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Ken]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Museth]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Digital Domain, Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1122508</ref_obj_id>
				<ref_obj_pid>1122501</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Houston, B., Nielsen, M., Batty, C., Nilsson, O., and Museth, K. 2006. Hierarchical RLE Level Set: A Compact and Versatile Deformable Surface Representation. <i>ACM Transactions on Graphics 25</i>, 1, 3--24.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1401110</ref_obj_id>
				<ref_obj_pid>1401032</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Museth, K., and Clive, M. 2008. Cracktastic: fast 3d fragmentation in "the mummy: Tomb of the dragon emperor". In <i>SIGGRAPH '08: ACM SIGGRAPH 2008 talks</i>, ACM, New York, NY, USA.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1278804</ref_obj_id>
				<ref_obj_pid>1278780</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Museth, K., Clive, M., and Zafar, N. B. 2007. Blobtacular: Surfacing particle systems in "pirates of the caribbean 3". In <i>SIGGRAPH '07: ACM SIGGRAPH 2007 talks</i>, ACM, New York, NY, USA.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1132035</ref_obj_id>
				<ref_obj_pid>1132033</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Nielsen, M. B., and Museth, K. 2006. Dynamic Tubular Grid: An efficient data structure and algorithms for high resolution level sets. <i>Journal of Scientific Computing 26</i>, 3, 261--299.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 An E.cient Level Set Toolkit for Visual E.ects Ken Museth Digital Domain, Inc. Figure 1: Examples of 
past production at Digital Domain that made extensive use of our LSTK based on ef.cient data structures. 
Speci.cally, these shots are from a Bacardi commercial where live dance performers are converted to animated 
level sets with .uid sources on their skin. Introduction Over the past couple of years we have developed 
various tools for visual effects in movie production based on level sets. While most VFX studios, and 
even some commercial software packages, have had level sets tools for several years we are, to the best 
of our knowl­edge, among the .rst to employ highly optimized data structures. These compact and very 
fast volumetric data structures allow us to work at unprecedented grid resolutions, which in turn opens 
the door to new applications. The last two years we have demonstrated this with level set tools speci.cally 
developed for .uid surfacing in Pirates of the Caribbean: At World s End [Museth et al. 2007] and fragmentation 
in The Mummy: Tomb Of The Dragon Em­peror [Museth and Clive 2008]. However, none of these past pre­sentations 
actually discussed the details of the novel data structure, DB-Grid, which forms the very backbone of 
our level set imple­mentations. The same was true for many of our fundamental level set tools, like the 
fast and robust scan-converter of self-intersecting polygonal meshes. These are exactly the topics of 
the current pre­sentation. We will also show how these tools are tightly integrated with Houdini. The 
Data Structure We have developed ef.cient data structures and algorithms for stor­age and manipulation 
of general volumetric data sampled on a spa­tially uniform 3D grid. Speci.cally, our data structure is 
based on a Dynamic Blocked Grid (hence the acronym DB-Grid) that exploits the spatial coherency of uniform 
grids to effectively and separately encode data values (e.g. signed distances) and topology (i.e. coor­dinates). 
These techniques allow for fast (i.e. constant time) data access into 3D grids of very high resolutions 
(exceeding 100003). In addition, our data structure is very general and can be applied to any spatially 
uniform 3D data regardless of topology and access patterns. In contrast, previous sparse level set data 
structures, like the very compact DT-Grid[Nielsen and Museth 2006] or the more versatile HRLE[Houston 
et al. 2006], assume .xed data topology (i.e. closed or at least connected surfaces), and require speci.c 
ac­cess patterns (i.e. memory-sequential) for fast data accesst. While DB-Grid is very .exible and general 
purpose, this talk will only fo­cus on it s applications to level sets. However, we stress that our data 
structure can be applied to virtually any problem that employs sparse uniform 3D grids, like for instance 
.uid animations or vol­ume rendering. The Level Set Tools DB-Grid forms the fundamental representation 
for all our geom­etry processing in the LSTK. This toolkit consists of many dif­ferent operations and 
algorithms that can be combined in numer­ous ways through their common data structure. DB-Grid offers 
some highly ef.cient implementations of level set operations like advection/propagation, re-normalization, 
narrow-band rebuilding, boolean operations, morphological operations, adaptive meshing, morphing, collision 
detection, mesh-to-level-set conversion and many more. We plan to discuss the details of these ef.cient 
im­plementations and showcase applications from actual production. For instance, our converter which 
allowed artists to convert self­intersecting meshes of motion captured dancers to level sets in near 
real-time (see .gure 1). Integration with Houdini To best deploy the LSTK to artists and technical directors, 
it needs to be integrated into a familiar 3rd party package. In our case, we chose Houdini because of 
its .exible SDK (HDK) and general pop­ularity at Digital Domain. It should be noted that Houdini already 
has support for level sets, but its implementation is based on dense 3D grids which suffer from large 
memory footprints (or low res­olution) and relatively poor computational complexity. However, using HDK 
we were able to tightly integrate our LSTK based on DB-Grid. This effectively allows us to take advantage 
of Houdini s existing modeling and animation tools as well as maintain an artist­friendly working environment. 
 References HOUSTON, B., NIELSEN, M., BATTY, C., NILSSON, O., AND MUSETH, K. 2006. Hierarchical RLE Level 
Set: A Compact and Versatile Deformable Surface Repre­ sentation. ACM Transactions on Graphics 25, 1, 
1 24. MUSETH, K., AND CLIVE, M. 2008. Cracktastic: fast 3d fragmentation in the mummy: Tomb of the dragon 
emperor . In SIGGRAPH 08: ACM SIGGRAPH 2008 talks, ACM, New York, NY, USA. MUSETH, K., CLIVE, M., AND 
ZAFAR, N. B. 2007. Blobtacular: Surfacing particle systems in pirates of the caribbean 3 . In SIGGRAPH 
07: ACM SIGGRAPH 2007 talks, ACM, New York, NY, USA. NIELSEN, M. B., AND MUSETH, K. 2006. Dynamic Tubular 
Grid: An ef.cient data structure and algorithms for high resolution level sets. Journal of Scienti.c 
Computing 26, 3, 261 299. Copyright is held by the author / owner(s). SIGGRAPH 2009, New Orleans, Louisiana, 
August 3 7, 2009. ISBN 978-1-60558-726-4/09/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1597996</article_id>
		<sort_key>60</sort_key>
		<display_label>Article No.</display_label>
		<display_no>6</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>6</seq_no>
		<title><![CDATA[Underground cave sequence for Land of the Lost]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1597990.1597996</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1597996</url>
		<abstract>
			<par><![CDATA[<p>For the Land of the Lost movie production, Rhythm & Hues was tasked to create the underwater Mystery Cave sequence. CG water was created to match water shots on stage. Then, the cave and the underwater river was extended, the cave broke apart and collapsed into the water, the river pours out into a waterfall, and falls into an underground vortex. The sequence had about 20 shots, 10 of them requiring FX work.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Animation</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>A.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P1617609</person_id>
				<author_profile_id><![CDATA[84459617257]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Lucio]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Flores]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Rhythm & Hues Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617610</person_id>
				<author_profile_id><![CDATA[81100288854]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[David]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Horsley]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Rhythm & Hues Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Underground Cave Sequence for Land of the Lost Lucio Flores David Horsley Rhythm &#38; Hues Studios 
Rhythm &#38; Hues Studios  For the Land of the Lost movie production, Rhythm &#38; Hues was tasked to 
create the underwater Mystery Cave sequence. CG water was created to match water shots on stage. Then, 
the cave and the underwater river was extended, the cave broke apart and collapsed into the water, the 
river pours out into a waterfall, and falls into an underground vortex. The sequence had about 20 shots, 
10 of them requiring FX work. Water Simulation Pipeline Creating a Cg River was a new scale of CG water 
to be simulated by R&#38;H. It also represented an order of magnitude increase in simulation size to 
get enough detail. We used Ahab, which is R&#38;H's proprietary fluid simulator, and we used Houdini 
9.7's fluid tools in DOPs. Both these tools helped us get through a lot of R&#38;D. Ultimately, it was 
Houdini's ability to do a distributed memory simulation across multiple machines which drove us to use 
it. Each machine communicates with the others by a thin voxel layer that overlaps neighboring domains, 
or 'slices'. We slaved 6 separate machines (each a 2.2Ghz quadcore with 8G of RAM), each to work on a 
slice of the simulation. As of the writing of this submission, we were able to sim up to 1500x96x300 
at approximately 15min per frame. It was very stable considering the simulation workload. We never had 
it crash, the machines would often fail before the simulations did. Collision geometry for the river 
base was generated from survey data taken on set. A start shape for the water was generated, then pulled 
forward using gravity skewed downhill. Water Lighting Pipeline We were able to identify 3 main elements 
to be generated from the velocity field and the fluid surface. With these elements it was possible to 
match the stage reference. Water Surface: The water surface we extracted from the fluid simulation was 
our starting point. A shader written for Houdini's Mantra renderer enabled us to render refraction, reflection, 
specular components, subsurface scattering, etc to get a nice fluid surface. Displacement shader used 
textures generated by the water flow to provide fine surface detail. Foam: Foam is an important part 
of the way water looks at this scale. It was important to get foam to not only look realistic, but also 
move realistically with the fluid. Particles were seeded on the surface of the water, advected using 
the fluid velocity, then projected onto the surface of the water using the gradient of level set from 
the water surface. Once we had this particle geometry, we could generate geometry for rendering. A high 
resolution texturemap of bubbles being advected by the fluid sim was rendered orthographically, then 
used as a texture map for color and displacement. This texturemap was rendered each frame to avoid the 
uv stretching issue. Splash: We would use particles for generating splash elements. The particles came 
from a Houdini POP simulation, or directly from the fluid simulation. We often ran the simulation as 
a hybrid solver, which generates particles as part of the solution. Each of these would be sufraced using 
Houdini's Particle Fluid Surface SOP.  Cave Rendering We rendered the entire cave in Mantra. It enabled 
us to design procedural fractal patterns to define the rough surfaces and rock patterns easily. The only 
texturemaps used were to project graphitti on the walls. The rest of the surfaces were procedural. Cave 
simulation We had to have the cave walls and its contents break apart and fall into the water. We used 
Houdini's Rigid Body simulation software to break apart the walls and have them fall convincingly into 
the water.  Vortex The vortex represents the wormhole which transports the actors from this world into 
the Land of the Lost. In it, we had to model a huge underground cavern with rocks and dirt lining the 
walls, traveling at high speeds, and accelerating towards the center. We experimented with different 
vortex shapes in an attempt to convey depth and scale. Once we found a shape we liked, we ran multiple 
particle simulations using Houdini POPs over the geometry to layer various stages of particle dynamics. 
No fluid simulation was used. We used Felt (R&#38;H's proprietary Field Expression Language) to stamp 
wisp primitives into Houdini volumes, then render them as volumetric primitives. Once rendered, the bright 
blue core and other lighting effects were done in compositing. Copyright is held by the author / owner(s). 
SIGGRAPH 2009, New Orleans, Louisiana, August 3 7, 2009. ISBN 978-1-60558-726-4/09/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1597997</article_id>
		<sort_key>70</sort_key>
		<display_label>Article No.</display_label>
		<display_no>7</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>7</seq_no>
		<title><![CDATA[Geometric fracture modeling in Bolt]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1597990.1597997</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1597997</url>
		<abstract>
			<par><![CDATA[<p>Modeling the geometry of solid materials cracking and shattering into elaborately shaped pieces is a painstaking task, which is often impractical to tune by hand when a large number of fragments are produced. In Walt Disney's animated feature film <i>Bolt</i>, cracking and shattering objects were prominent visual elements in a number of action sequences. We designed a system to facilitate the modeling of cracked and shattered objects, enabling the automatic generation of a large number of fragments while retaining the flexibility to artistically control the density and complexity of the crack formation, or even manually controlling the shape of the resulting pieces where necessary. Our method resolves every fragment <i>exactly</i> into a separate triangulated surface mesh, producing pieces that line up perfectly even upon close inspection, and allows straightforward transfer of texture and look properties from the un-fractured model.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>A.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P1617611</person_id>
				<author_profile_id><![CDATA[81442605747]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Jeffrey]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hellrung]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of California Los Angeles]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617612</person_id>
				<author_profile_id><![CDATA[81100351513]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Andrew]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Selle]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Walt Disney Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617613</person_id>
				<author_profile_id><![CDATA[81442614845]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Arthur]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Shek]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Walt Disney Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617614</person_id>
				<author_profile_id><![CDATA[81409594691]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Eftychios]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sifakis]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of California Los Angeles]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617615</person_id>
				<author_profile_id><![CDATA[81100222891]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Joseph]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Teran]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of California Los Angeles]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1272701</ref_obj_id>
				<ref_obj_pid>1272690</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Sifakis, E., Der, K., and Fedkiw, R. 2007. Arbitrary cutting of deformable tetrahedralized objects. In <i>Proceedings of the 2007 ACM SIGGRAPH/Eurographics symposium on Computer animation</i>, 73--80.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Geometric fracture modeling in BOLT Jeffrey Hellrung1 Andrew Selle2 Arthur Shek2 Eftychios Sifakis1,2 
Joseph Teran1,2 1University of California Los Angeles 2Walt Disney Animation Studios Figure 1: Left, 
middle: Rhino s ball is riddled with cracks as a metal gate crushes it down. Right: A roadway is torn 
up by Bolt s superbark . 1 Introduction Modeling the geometry of solid materials cracking and shattering 
into elaborately shaped pieces is a painstaking task, which is often impractical to tune by hand when 
a large number of fragments are produced. In Walt Disney s animated feature .lm Bolt, cracking and shattering 
objects were prominent visual elements in a number of action sequences. We designed a system to facilitate 
the model­ing of cracked and shattered objects, enabling the automatic genera­tion of a large number 
of fragments while retaining the .exibility to artistically control the density and complexity of the 
crack forma­tion, or even manually controlling the shape of the resulting pieces where necessary. Our 
method resolves every fragment exactly into a separate triangulated surface mesh, producing pieces that 
line up perfectly even upon close inspection, and allows straightforward transfer of texture and look 
properties from the un-fractured model. 2 Crack geometry generation The input to our system consists 
of a closed triangulated surface de.ning the (uncut) solid object to be fractured and one or more ad­ditional 
triangulated surfaces de.ning the geometry of the cracks. The geometry of this crack surface is not constrained 
by the shape of the material object itself; cracks are free to extend outside the material into the empty 
space, and can have non-manifold shapes, topological junctions, or even intersect themselves. Leveraging 
this .exibility, we can automatically create a fracture surface which par­titions the ambient space into 
any given number of regions by sim­ply introducing the same number of seed points and computing the 3D 
Voronoi regions of those seed locations. The fracture surface is then de.ned as the union of all boundaries 
between Voronoi cells (see Fig. 2, top right). In practice, we approximate these regions by laying down 
a background, procedurally generated tetrahedral mesh, and computing the Voronoi cells via a .ood-.ll 
on the tetra­hedral mesh, starting from the elements containing the seed points. The shape of the fracture 
surface can be controlled by specifying the number of seed points, or even by volumetric painting of 
seed point densities, to control which regions will shatter into more, smaller fragments. We also control 
the smoothness of the boundaries be­tween Voronoi regions by jittering the background tetrahedral mesh 
prior to the .ood-.ll, and selectively smoothing the boundary sur­face where smoother cracks are desired. 
Finally, in certain situa­tions (e.g. the cracks of the hamster ball in Fig. 1), more speci.c artist 
control of the fragment geometry is desired. For these cases, we extruded sets of artist-drawn 3D curves 
in the direction normal to the object surface to create a fracture surface that cuts through the material 
and re.ects the crack design intended by the artists. 3 Automatic fragment mesh generation We adapt 
the cutting algorithm of [Sifakis et al. 2007] to automat­ically generate triangulated meshes for the 
material fragments de­.ned by the object and fracture geometries of the previous section. In particular, 
their method begins with a tetrahedral volumetric rep­resentation of the material to be fractured, as 
opposed to the trian­gulated boundary geometry we assumed as input to our system. We handle this representation 
discrepancy by .rst generating a tetrahe­dral mesh that fully covers the object to be fractured . The 
triangu­lated surface of the uncut object itself is used as the .rst cut in an application of the algorithm 
of [Sifakis et al. 2007], effectively sec­tioning the background tetrahedral volume into the material 
and void regions. The fracture surface is then applied as the second cut, resulting in the separation 
of the material volume into separate fragments. The cutting algorithm computes the triangulated bound­ary 
of every volumetric fragment in a way that every triangle of a fragment is contained inside a triangle 
either of the uncut object, or of the fracture surface (Fig. 2, bottom). As a result, texture and look 
properties can be remapped simply by embedding each result­ing triangle barycentrically into either the 
material or the fracture surface respectively, and looking up the properties of the embed­ding triangle. 
Finally, although our framework is currently used for geometric modeling, we aim to employ it in conjunction 
with sim­ulation in the future, to model time-dependent crack propagation.  Figure 2: Top left: Simulation 
of the shattered fragments of Rhino s ball. Top right: Fracture surfaces de.ned as the boundaries of 
Voronoi regions in 3D. Bottom: The fragments are fully resolved as independent surface meshes, and can 
be separately manipulated. References SIFAKIS, E., DER, K., AND FEDKIW, R. 2007. Arbitrary cutting of 
de­formable tetrahedralized objects. In Proceedings of the 2007 ACM SIG­GRAPH/Eurographics symposium 
on Computer animation, 73 80. Copyright is held by the author / owner(s). SIGGRAPH 2009, New Orleans, 
Louisiana, August 3 7, 2009. ISBN 978-1-60558-726-4/09/0008   
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1597998</article_id>
		<sort_key>80</sort_key>
		<display_label>Article No.</display_label>
		<display_no>8</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>8</seq_no>
		<title><![CDATA[Simulating the Balloon Canopy in <i>Up</i>]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1597990.1597998</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1597998</url>
		<abstract>
			<par><![CDATA[<p>Comprised of over 10,000 brightly colored balloons, the Balloon Canopy in <i>Up</i> is one of the film's most recognizable visual icons. Propelling Carl and Russell skyward on the adventure of a lifetime, the Canopy endures the harsh elements of the South American jungle, with balloons popping, deflating, and responding to the dynamic environment of the story.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.5</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>A.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061.10010063</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures->Computational geometry</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P1617616</person_id>
				<author_profile_id><![CDATA[81335496472]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Jon]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Reisch]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Pixar Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617617</person_id>
				<author_profile_id><![CDATA[81442595983]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Eric]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Froemling]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Pixar Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Simulating the Balloon Canopy in Up Jon Reisch* Eric Froemling Pixar Animation Studios Pixar Animation 
Studios Comprised of over 10,000 brightly colored balloons, the Balloon Canopy in Up is one of the .lm 
s most recognizable visual icons. Propelling Carl and Russell skyward on the adventure of a lifetime, 
the Canopy endures the harsh elements of the South American jun­gle, with balloons popping, de.ating, 
and responding to the dy­namic environment of the story. Figure 1: The Balloon Canopy lifts off! c &#38;#169;2009 
Disney/Pixar. All rights reserved. The Up FX team was tasked with the enormous challenge of sim­ulating 
the motion of thousands of balloons along with the strings that tether them to Carl s house. The sheer 
number of dynamic bod­ies that comprise the Canopy required a reimagining of our rigid­body simulation 
environment, and as well as the development of concise rigs and .exible artists tools. We will discuss 
both the technical and artistic challenges of bringing the Balloon Canopy to life in close to 200 shots 
in Up. 1 Rigid Body Simulation Environment The number of balloons was a signi.cant challenge in this 
project. From early tests, it was determined that simulating full soft-body balloons would be cost-prohibitive, 
while simple procedural motion or particles would be unconvincing. Our chosen technique fell in the middle 
and consisted of rigid body simulation of balloons with particle-spring or linked-rigid-body strings, 
depending on the level of detail and interaction required. Handling such large simulations involved signi.cant 
retooling of our rigid body pipeline. Our existing simulator, run inside of Maya, consisted of a standard 
setup of individual nodes representing each body or constraint. This system became unwieldy when scaled 
past several thousand bodies. The new simulation environment devel­oped for the balloons separated the 
simulator into a standalone pro­gram with a thin Python control layer. A Maya interface was then set 
up to generate code for this simulator and display its cached output. This standalone approach afforded 
us greater speed and scalability and allowed the artist to easily run simultaneous wedge­tests in the 
background or on the render-farm. It also provided in­creased .exibility, such as the ability to create 
or destroy bodies, constraints, or other dynamic elements over time within a simula­tion. *e-mail: jreisch@pixar.com 
e-mail: ericf@pixar.com 2 Rig Development and Artists Tools With so many Canopy shots to realize in 
the .lm, minimzing artist iteration time was critical. One early step towards this goal was sep­arating 
the simulation of the balloons themselves from their strings. While initially we envisioned a fully-coupled 
dynamic relationship between the balloons and strings, the director s concern was much more focused on 
the motion and performance of the balloons in the shot. Our rigs therefore focused on minimizing iteration 
time in achieving believable balloon motion, and then using balloons cached simulation output to drive 
a less directed string simulation. The motion of the balloons was motivated primarily by simple force 
.elds, and the collision response that our rigid body simulator pro­vided. We developed a very intuitive, 
artist-friendly system of scult­ping our dynamic .elds which we refered to as .eld shapers. This component-based 
system allowed artists to composite the effects of simple functions that modulated the strength or direction 
of a .eld in a visual, hand-placeable way. cFigure 2: The Balloon Simulation Environment. &#38;#169;2009 
Dis­ ney/Pixar. All rights reserved. 3 Highly Dynamic Shots Certain shots in the .lm, particularly during 
the turbulent storm sequence, required the Canopy to undergo very large, but directed deformations and 
hit speci.c key poses. To address this challenge, we extended the basic canopy rig to include a central 
spine which each balloon was attached to. By attaching keyframed locators to articulation points along 
the spine via dynamic springs, fx artists could more readily pull the form of the Canopy into speci.c 
shapes. 4 Popping Balloons, Dangling Strings, and Other Dynamic Events Popping and releasing balloons 
in the Canopy are examples of phe­nomenon that were localized both to speci.c selections of balloons 
and speci.c points in time. By repurposing the same shapers we used to sculpt our dynamic .elds, we created 
localized, but random selections of balloons to operate on. Combined with the .exibility that our simulation 
environment provided to trigger callback func­tions at certain frames, or in response to dynamic events 
such as collisions, we had the basis for choreographing these complex pro­cedural actions. Copyright 
is held by the author / owner(s). SIGGRAPH 2009, New Orleans, Louisiana, August 3 7, 2009. ISBN 978-1-60558-726-4/09/0008 
  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1597999</article_id>
		<sort_key>90</sort_key>
		<display_label>Article No.</display_label>
		<display_no>9</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>9</seq_no>
		<title><![CDATA[Applying color theory to creating scientific visualizations]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1597990.1597999</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1597999</url>
		<abstract>
			<par><![CDATA[<p>In this talk, we highlight the process of building effective color maps for producing explanatory and aesthetically engaging scientific visualizations. We describe hands on techniques and tips we have developed over the many years we have designed visualizations ourselves. We address how you can also use these applied methods to build your own color maps that support static, interactive and animated displays. In this talk, we will go through at least two examples. The first is a visualization and anmation sequence depicting a computationally generated perfect storm and the second is an interactive visual depiction of a computationally modeled supernova or stellar explosion. We also hope to cover newly emerging visualization efforts during this presentation.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Color, shading, shadowing, and texture</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>A.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010384</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Texturing</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P1617618</person_id>
				<author_profile_id><![CDATA[81350590594]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Theresa-Marie]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Rhyne]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[North Carolina State University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Applying Color Theory to Creating Scientific Visualizations Theresa-Marie Rhyne, Director of the Renaissance 
Computing Institute s Engagement Center at North Carolina State University In this talk, we highlight 
the process of building effective color maps for producing explanatory and aesthetically engaging scientific 
visualizations. We describe hands on techniques and tips we have developed over the many years we have 
designed visualizations ourselves. We address how you can also use these applied methods to build your 
own color maps that support static, interactive and animated displays. In this talk, we will go through 
at least two examples. The first is a visualization and anmation sequence depicting a computationally 
generated perfect storm and the second is an interactive visual depiction of a computationally modeled 
supernova or stellar explosion. We also hope to cover newly emerging visualization efforts during this 
presentation. About the presenter: Theresa-Marie Rhyne has actively contributed to the ACM SIGGRAPH and 
IEEE Visualization communities for over twenty years. Since 1993, she has organized panels and tutorials 
at their annual conferences. She was the founding visualization expert at the United States Environmental 
Protection Agency s Scientific Visualization Center (1990 2000). Currently, she is the Director of the 
Renaissance Computing Institute s Engagement Center at North Carolina State University. She is also a 
pacticing computer artist and wrote the Portrait of a Computer Artist discussion that appeared in the 
ACM SIGGRAPH Career Handbook, published in February 1991.  Image from a Perfect Storm Visualization, 
created by Steve Chall &#38; Theresa-Marie Rhyne of the Renaissance Computing Institute s Engagement 
Facility at NC State University. Research and Computational Model runs conducted by Gary Lackmann, Megan 
Gentry and Kevin Hill of the Department of Marine, Earth and Atmospheric Sciences at North Carolina State 
University.  Image from a Super Nova Visualization, created by Steve Chall &#38; Theresa-Marie Rhyne 
of the Renaissance Computing Institute s Engagement Facility at NC State University. Research and Computational 
Model runs conducted by John Blondin of the Department of Physics at North Carolina State University. 
 Copyright is held by the author / owner(s). SIGGRAPH 2009, New Orleans, Louisiana, August 3 7, 2009. 
ISBN 978-1-60558-726-4/09/0008 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1598000</article_id>
		<sort_key>100</sort_key>
		<display_label>Article No.</display_label>
		<display_no>10</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>10</seq_no>
		<title><![CDATA[GPSFilm]]></title>
		<subtitle><![CDATA[location-based mobile cinema]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1597990.1598000</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1598000</url>
		<abstract>
			<par><![CDATA[<p>GPSFilm is a new media artwork that invents an innovative way of watching a movie based on the viewer's location. Using a GPS-enabled PDA or mobile phone, a movie is revealed as the viewer travels. By merging mobile computing with cinema, the audience creates a new type of film experience. The project uses emerging technologies to bring story into real space; to use neighborhoods, architecture, and landscapes as part of the cinema experience. The setting of a film can also be the setting for watching the film...a unique entertainment encounter.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>A.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P1617619</person_id>
				<author_profile_id><![CDATA[81442617926]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Scott]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hessels]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Nanyang Technological University, Singapore]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 GPSFilm: Location-Based Mobile Cinema Scott Hessels School of Art, Design and Media Nanyang Technological 
University, Singapore Figure 1: Using the GPSFilm System 1 Introduction GPSFilm is a new media artwork 
that invents an innovative wayof watching a movie based on the viewer s location. Using a GPS-enabled 
PDA or mobile phone, a movie is revealed as the viewer travels. By merging mobile computing with cinema, 
the audience creates a new type of film experience. The project usesemerging technologies to bring story 
into real space; to use neighborhoods, architecture, and landscapes as part of the cinema experience. 
The setting of a film can also be the setting for watching the film a unique entertainment encounter. 
 2 Explanation As the viewer explores a park, a neighborhood, or even a city or country, GPSFilm continually 
reads their location and playsscenes that are tied to those places. As they travel, each viewer edits 
his own version of the story based on his journey.Similar to a computer game, GPSFilm tells stories by 
exploring anenvironment but by taking it off the screen and back into the real world. Storytelling becomes 
a physical, viewer-controlled experience; a journey of fiction ties directly to a journey of fact. GPSFilm 
is being released as an open source application alongwith the first film made specifically for the system, 
Singaporeanfilmmaker Kenny Tan's chase comedy "Nine Lives . Nine Lives begins with the film s climax 
scene followed by each of nine neighborhoods tied to a separate flashback. As the viewer Figure 2: Poster 
for first GPSFilm Nine Lives travels around an area, they learn how the climax came to happenby seeing 
the crazy events that led to it. GPSFilm allows for a developer to create story spaces as small asparks 
to as big as the globe itself. The movies are alsointerchangeable and easily matched to any place. By 
creating thisnew type of film viewing experience in an open, collaborative way, the artists hope that 
location-based mobile cinema will be explored further. 3 Summary Navigational storytelling has been 
around for centuries from cave drawings to cathedrals where a path unveiled a story. Nowfilm can join 
other story genres that are shaped by their place andaudience. We re entering a new generation of cinema 
storytelling. Recent innovations in locative technology nowenable people to invisibly plant virtual objects 
in physicalspace to associate a story, a picture, a video with an actual place for others to encounter. 
This is a new relationship with our environment, we re giving place a memory. When a new media form emerges, 
the way it is used to tell stories is found at the crossroads of culture and technology. Instead of a 
fixed art form, cinema can also now be a type of experience design a film canchange dynamically based 
on location and the actions of the people watching. Movies can be personalized and localized. dshessels@ntu.edu.sg 
 Copyright is held by the author / owner(s). SIGGRAPH 2009, New Orleans, Louisiana, August 3 7, 2009. 
ISBN 978-1-60558-726-4/09/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1598001</article_id>
		<sort_key>110</sort_key>
		<display_label>Article No.</display_label>
		<display_no>11</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>11</seq_no>
		<title><![CDATA[Karma Chameleon]]></title>
		<subtitle><![CDATA[jacquard-woven photonic fiber display]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1597990.1598001</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1598001</url>
		<abstract>
			<par><![CDATA[<p>Karma Chameleon is a photonic textile display woven on a Jacquard loom, using photonic bandgap fibers that have the ability to change color when illuminated with ambient or transmitted light. The use of double weave structures allows us to further modulate the color and patterns on the textile.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[electronic textiles]]></kw>
			<kw><![CDATA[jacquard loom]]></kw>
			<kw><![CDATA[photonic bandgap fibers]]></kw>
			<kw><![CDATA[photonic displays]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.4.0</cat_node>
				<descriptor>Image displays</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>A.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P1617620</person_id>
				<author_profile_id><![CDATA[81309488703]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Joanna]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Berzowska]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Concordia University, Montreal, Quebec, Canada]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617621</person_id>
				<author_profile_id><![CDATA[81442605066]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Maksim]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Skorobogatiy]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Ecole Polytechnique de Montreal, Montreal, QC, Canada]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[B. Gauvreau, N. Guo, K. Schicker, K. Stoeffler, F. Boismenu, A. Ajji, R. Wingfield, C. Dubois, M. Skorobogatiy, "Color-changing and color-tunable photonic bandgap fiber textiles," Opt. Express, Vol. 16, pp. 15677--15]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1598002</article_id>
		<sort_key>120</sort_key>
		<display_label>Article No.</display_label>
		<display_no>12</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>12</seq_no>
		<title><![CDATA[Generalizing multi-touch direct manipulation]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1597990.1598002</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1598002</url>
		<abstract>
			<par><![CDATA[<p>The appeal of direct manipulation with multi-touch interfaces stems from the experience it offers. As the user slides their fingers along a touch surface, objects react by rotating, translating, and scaling themselves so that the same point on an object always remains underneath the same fingertip. Since objects move in a predictable and realistic fashion, users are given the impression of "gripping" real objects. Direct manipulation essentially provides an intuitive and controllable mapping between points in local space and points in screen space, without the need for any explicit gesture processing.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>H.5.2</cat_node>
				<descriptor>Haptic I/O</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>A.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10011752</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Haptic devices</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P1617622</person_id>
				<author_profile_id><![CDATA[81442611322]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Jason]]></first_name>
				<middle_name><![CDATA[L.]]></middle_name>
				<last_name><![CDATA[Reisman]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Perceptive Pixel, Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617623</person_id>
				<author_profile_id><![CDATA[81100097816]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Philip]]></first_name>
				<middle_name><![CDATA[L.]]></middle_name>
				<last_name><![CDATA[Davidson]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Perceptive Pixel, Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617624</person_id>
				<author_profile_id><![CDATA[81442594736]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Jefferson]]></first_name>
				<middle_name><![CDATA[Y.]]></middle_name>
				<last_name><![CDATA[Han]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Perceptive Pixel, Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Gleicher, M. and Witkin. A. 1992. Through-the-Lens Camera Control. In <i>Proceedings of the 19th Annual Conference on Computer Graphics and Interactive Techniques</i> J. J. Thomas, Ed. SIGGRAPH '92. ACM, New York, NY, 331--340.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1598003</article_id>
		<sort_key>130</sort_key>
		<display_label>Article No.</display_label>
		<display_no>13</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>13</seq_no>
		<title><![CDATA[BiDi screen]]></title>
		<subtitle><![CDATA[depth and lighting aware interaction and display]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1597990.1598003</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1598003</url>
		<abstract>
			<par><![CDATA[<p>We present a BiDirectional screen capable of both imaging and display, that uses an LCD as a spatial light modulator to support seamless transition from on-screen multi-touch interactions to off-screen hover-based gestures.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Color, shading, shadowing, and texture</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>A.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010384</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Texturing</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P1617625</person_id>
				<author_profile_id><![CDATA[81442601918]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Matthew]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hirsch]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MIT Media Lab]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617626</person_id>
				<author_profile_id><![CDATA[81319495323]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Douglas]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lanman]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Brown University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617627</person_id>
				<author_profile_id><![CDATA[81100022847]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Ramesh]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Raskar]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MIT Media Lab]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617628</person_id>
				<author_profile_id><![CDATA[81435601065]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Henry]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Holtzman]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MIT Media Lab]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Brown, C. J., Kato, H., Maeda, K., and Hadwen, B. 2007. A continuous-grain silicon-system lcd with optical input function. <i>IEEE Journal Of Solid State Circuits 42</i>, 12 (Dec).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1276463</ref_obj_id>
				<ref_obj_pid>1276377</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Veeraraghavan, A., Raskar, R., Agarwal, R., Mohan, A., and Tumblin, J. 2007. Dappled photography: Mask enhanced cameras for heterodyned light fields and coded aperture refocusing. <i>ACM Transactions on Graphics 26</i>, 3, 69.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 BiDi Screen: Depth and Lighting Aware Interaction and Display MatthewHirsch1 DouglasLanman2 RameshRaskar1 
HenryHoltzman1 1 MITMediaLab 2 BrownUniversity Figure 1: Wedemonstrateanovel modi.cationof atypicalLCD 
toallowco-locatedimagecaptureanddisplay.(Left)A usercanseamlessly transitionfromon-screenmultitouch tooff-screenhoverandgesture-based 
interaction.(Middle)Multi-vieworthographic imagery recorded in real-time using a mask displayed by the 
LCD-based spatial light modulator. (Right, Top)Image refocused at the depth of a user s hand. (Right,Bottom)Real-timedepthestimateprovidedasinput 
tohovermodeinteractionsystem. Abstract WepresentaBiDirectional screencapableofboth imaging anddis­play,thatusesanLCD 
asaspatiallight modulator tosupport seam­less transitionfromon-screenmulti-touchinteractions tooff-screen 
hover-basedgestures. 1. Introduction Anovel methodforusing lightsensors todetectmultiplepointsof contact 
with thesurfaceofliquidcrystaldisplays(LCDs)isemerg­ing. SharpCorporation[Brown et al.2007] and othershavedemon­strated 
LCDs with arrays of optical sensors interlaced within the pixel grid which can be used to drive a multi-touch 
user interface on the surface of the screen. We present the BiDirectional(BiDi) screen, a system inspired 
by optical multitouch,which usesanLCD asaspatial light modulator to allow both image capture and display. 
We use spatial heterody­ing[Veeraraghavanetal.2007] tocapture theangle,aswellas in­tensity, of light 
entering a co-located sensor array. This allows us to image objects, such as .ngers, that are located 
beyond the dis­play s surface and measure their distance from the display. In our prototype, imaging 
is performed in real-time, enabling the detec­tionof off-screengestures.Whenused witha light-emitting 
wand, the BiDi screen can determine not only where the wand is aimed, but alsotheincidenceangleof light 
cast onthedisplay surface. 2. Vision Earlier light sensing display designs have focused on promoting 
touchinterfaces.Ourdesignenhancesthe.eldby seamlessly tran­sitioningfromon-screenmultitouchinteractions 
tooff-screenhover andgesture-based interfaces.Thissingledevicealternatesbetween forming the displayed 
image and capturing a modulated light .eld through a liquid crystal spatial light modulator. Because 
the mask is formed on an LCD, we can vary the size and density of the ar­ray dynamically, allowing for 
scene-optimized imaging. The BiDi screen has the ability to create multiple orthographic images with­outblockingthebacklight 
orsacri.cingportionsof thedisplay.  Figure 2: Design of a BiDi screen. Image capture and display is 
achievedby rearranging traditionalLCD optical components. Our prototype BiDi screen can recognize on-screen 
as well as off­screen gestures. We also demonstrate its ability to detect the posi­tion and angle oflight-emitting 
widgets, showing novelinteractions betweendisplayedimagesand external lighting. 3. Design As shown in 
Figure 2, the BiDi screen is formed by repurposing typical LCD components. We create a large-aperture, 
multi-view image capture device by using the spatial light modulator to dis­play a pinhole array or tiled 
Modi.ed Uniform Redundant Array (MURA) mask. Our key insight is that, for simultaneous image capture 
and display using an LCD, the backlight diffuser must be moved away from the liquid crystal. In doing 
so, the modulated light .eld is captured on the diffuser. The backlight in the proto­type isprovidedby 
anarray ofLEDs. References BROWN, C. J., KATO, H., MAEDA, K., AND HADWEN, B. 2007. A continuous-grain 
silicon-systemlcdwith opticalinput function. IEEEJournalOfSolidStateCircuits 42,12 (Dec.). VEERARAGHAVAN, 
A., RASKAR, R., AGRAWAL, R., MOHAN, A., AND TUMBLIN, J. 2007. Dappled photography: Mask enhanced cameras 
for heterodyned light .elds and coded aperture refocusing. ACMTransactions onGraphics26,3,69. Copyright 
is held by the author / owner(s). SIGGRAPH 2009, New Orleans, Louisiana, August 3 7, 2009. ISBN 978-1-60558-726-4/09/0008 
 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1598004</article_id>
		<sort_key>140</sort_key>
		<display_label>Article No.</display_label>
		<display_no>14</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>14</seq_no>
		<title><![CDATA[Innovation in animation]]></title>
		<subtitle><![CDATA[exiting the comfort zone]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1597990.1598004</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1598004</url>
		<abstract>
			<par><![CDATA[<p>In approaching the broad arena of art animation in the studio classroom it is useful to first deconstruct the defining elements of animation, stripping it of its popular connotations. Working from that vantage point, it is informative to investigate the work of some key pioneering artists and to see what <i>innovations</i> their visions necessitated as they created images or mechanisms that could conjure images. Such an inquiry compels the student to engage in a rich studio activity that uses a hybrid of materials, both digital and tactile, and often unorthodox. The benefit of such an approach to teaching animation prompted a full course within the animation curriculum; a course dedicated to innovation. This was initiated by the need for the students to learn to discover new methods and approaches to image making, leaving their comfort zones which often revolved around one style of drawing and a favorite software program.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Animation</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>A.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P1617629</person_id>
				<author_profile_id><![CDATA[81442610743]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Pamela]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Taylor]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Virginia Commonwealth University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Innovation in Animation: Exiting the Comfort Zone Pamela Taylor, Virginia Commonwealth University 
    Abstract In approaching the broad arena of art animation in the studio classroom it is useful 
to first deconstruct the defining elements of animation, stripping it of its popular connotations. Working 
from that vantage point, it is informative to investigate the work of some key pioneering artists and 
to see what innovations their visions necessitated as they created images or mechanisms that could conjure 
images. Such an inquiry compels the student to engage in a rich studio activity that uses a hybrid of 
materials, both digital and tactile, and often unorthodox. The benefit of such an approach to teaching 
animation prompted a full course within the animation curriculum; a course dedicated to innovation. This 
was initiated by the need for the students to learn to discover new methods and approaches to image making, 
leaving their comfort zones which often revolved around one style of drawing and a favorite software 
program. 1 Introduction Teaching animation has many challenges, especially as the term animation has 
come to encompass such a variety of moving images. Students are well-versed in popular, entertainment 
based animations, but have little knowledge of the history of the broader scope, which includes experimental 
work. They usually replicate what they have seen. The goal is to motivate them to experiment and to move 
outside of their comfort zone, to try a variety of techniques and work with media digital and analog 
 that is not familiar. Instead, they can investigate the use of a variety of materials and approaches, 
with the intent of discovering new ways to make interesting and engaging imagery. 2 Approach First 
it is necessary to deconstruct the defining elements of animation, to engage the students in an examination 
of animation in a studio that only has virtual frames in a digital timeline. In questioning the boundaries 
of this practice, animation is stripped of its popular connotations and regarded as a rich arena to engage 
in. The students become more aware of the many possibilities present in today s extended studio; a studio 
housed with tactile, mechanical and digital tools. To root this type of inquiry in the ongoing trajectory 
of animation, the work of some key pioneering animation artists is investigated, to see what innovations 
their visions necessitated as they created images or mechanisms that could conjure images. Many of the 
earlier innovations of art animation, and related imagery, are unknown to current students, so the class 
requires a significant amount of screenings with accompanying information. These presentations are not 
limited to film but include early performances in the area of visual music, by artists such as Thomas 
Wilfred, who created an instrument (the Clavilux) that would play ethereal abstract color forms, Loie 
Fuller, the dancer who inspired so many Art Nouveau artists with her swirling robes that captured colored 
light, and numerous others. John Whitney Sr. and James Whitney s experiments are presented, noting John 
Whitney Sr. s contribution of the slit scan to the visual effects industry, as well as his camera control 
machine, built from army surplus. The innovative animations of Oskar Fischinger and Norman McLaren are 
examined to find potential areas of inspiration, as well as the lesser known work of Adam Beckett, who 
created a unique technique of additive cycles, using the optical printer, and who also was head of animation 
and rotoscoping on the first Star Wars movie. 3 Studio Supported within this historical and conceptual 
context, the students design their own experiments, engaging paint, water, Christmas lights whatever 
their imagination suggests. Such an approach compels the student to engage in a rich studio activity 
that uses a hybrid of materials and often unorthodox methodologies - a sort of software meets thrift 
store energy in the pursuit of individual expression and the innovation. They are required to leave their 
comfort zone to come up with a hypothesis and follow it to its conclusion. The first projects are experiments 
 they are not required to create finished pieces. Instead, the focus is on their investigation and discovery. 
The final project is a completed short animation that engages one or more of the discoveries they have 
made. It can be abstract or narrative. 4 Results The response to this approach has been overwhelmingly 
positive. Students often welcome the opportunity to work with their hands and with different materials 
such as clay, cotton balls, newsprint, watercolors and pipe cleaners. By the end of the course, their 
work demonstrates sophistication in content, style and creativity. This makes a significant difference 
in the work they produce as seniors; it loosens them up visually as they are open to more possibilities, 
and have a more richly tooled studio. 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1598005</article_id>
		<sort_key>150</sort_key>
		<display_label>Article No.</display_label>
		<display_no>15</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>15</seq_no>
		<title><![CDATA[Educate the educator]]></title>
		<subtitle><![CDATA[lessons from faculty education programs at Rhythm & Hues studios worldwide]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1597990.1598005</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1598005</url>
		<abstract>
			<par><![CDATA[<p>Educators of computer graphics and digital art play a pivotal role in the development and sustenance of technical and artistic talent by establishing the foundations of collaboration between the industry and the academe. This talk shares lessons learned from the Faculty Education Programs (FEP) instituted by Rhythm & Hues (R&H) Studios in 2007 and deployed in its three locations in the U.S. and in India -- Los Angeles, Mumbai and Hyderabad. Since, 2007, more than 60 faculty from more than 40 institutions in the U.S. and India have participated in these programs.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[academe-industry synergy]]></kw>
			<kw><![CDATA[education communities]]></kw>
			<kw><![CDATA[faculty education]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>A.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P1617630</person_id>
				<author_profile_id><![CDATA[81442599151]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Subhashish]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Aikat]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Rhythm & Hues Studios, Los Angeles]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617631</person_id>
				<author_profile_id><![CDATA[81319497500]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Barbara]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[McCullough]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Rhythm & Hues Studios, Los Angeles]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Rogers, Everett M. 1995. <i>Diffusion of Innovations</i>. New York, NY. The Free Press]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Educate the Educator: Lessons from Faculty Education Programs at Rhythm &#38; Hues Studios Worldwide 
Subhashish Aikat and Barbara McCullough Rhythm &#38; Hues Studios, Los Angeles * 1. Overview Educators 
of computer graphics and digital art play a pivotal role in the development and sustenance of technical 
and artistic talent by establishing the foundations of collaboration between the industry and the academe. 
This talk shares lessons learned from the Faculty Education Programs (FEP) instituted by Rhythm &#38; 
Hues (R&#38;H) Studios in 2007 and deployed in its three locations in the U.S. and in India -- Los Angeles, 
Mumbai and Hyderabad. Since, 2007, more than 60 faculty from more than 40 institutions in the U.S. and 
India have participated in these programs. Keywords: faculty education, academe-industry synergy, education 
communities 2. Discussion of Synergistic Approaches The focus of this talk is on the change agents, 
the educators, who create the environments for technical and academic excellence in their institutions 
and how educators collaborate. Drawing from the experience of multiple faculty programs in two countries, 
the U.S. and India, the talk seeks to illustrate pedagogical models and curriculum development approaches 
that work for programs globally. Copyright is held by the author / owner(s). SIGGRAPH 2009, New Orleans, 
Louisiana, August 3 7, 2009. ISBN 978-1-60558-726-4/09/0008  3. Design Components of FEP s The design 
and execution of FEP s involves enabling the faculty to attend in-house training sessions, involving 
them in a digital production environment, and helping train them on-site in the current tools and techniques 
of production.  4. The Common Goal of Academic Excellence While acknowledging the role of the student 
learners who eventually serve the industry with their talents and skills, this talk demonstrates how 
the industry should recognize that the educators in the academe are the gatekeepers, the agenda­setters, 
and catalysts of academic excellence that nurture the learners and grow the talent. For members of the 
academic community, this talk provides guidelines on how to benefit from FEP s. For educators and trainers 
in the industry, this talk shares ideas on how to develop synergistic relationships with feeder institutions 
within the academe. References: ROGERS, EVERETT M. 1995. Diffusion of Innovations. New York, NY. The 
Free Press  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1598006</article_id>
		<sort_key>160</sort_key>
		<display_label>Article No.</display_label>
		<display_no>16</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>16</seq_no>
		<title><![CDATA[Building bridges, not falling through cracks]]></title>
		<subtitle><![CDATA[what we have learned during ten years of Australian digital visual effects traineeships]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1597990.1598006</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1598006</url>
		<abstract>
			<par><![CDATA[<p>Australia's New South Wales FTO is a state government body chartered with the task of developing the film and television industries and promoting industrial growth for film & TV businesses based in New South Wales. When the transition from analogue to digital gained momentum in the mid-1990s, there was an imperative to help artists from other media cross-train to take up the new opportunities arising at that time and find ways to bring people with very limited experience with digital tools up to speed. Since then, there has been substantial growth in both educational programs and employment opportunities. The industry is well-served by formal courses, online mentorship and training programs, professional associations, and many formalized internship arrangements. Similarly, a new generation of artists has come to the industry with high levels of digital literacy. The industry itself has developed sophisticated production practices and infrastructure. It has grown to encompass areas of specialization and developed standards of professionalism that compare favourably, and indeed, exceed those of many fields with longer histories.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>A.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P1617632</person_id>
				<author_profile_id><![CDATA[81384598668]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Shilo]]></first_name>
				<middle_name><![CDATA[T.]]></middle_name>
				<last_name><![CDATA[McClean]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Building Bridges, Not Falling Through Cracks: what we have learned during ten years of Australian Digital 
Visual Effects Traineeships Speaker: Dr. Shilo T. McClean Australia s New South Wales FTO is a state 
government body chartered with the task of developing the film and television industries and promoting 
industrial growth for film &#38; TV businesses based in New South Wales. When the transition from analogue 
to digital gained momentum in the mid-1990s, there was an imperative to help artists from other media 
cross-train to take up the new opportunities arising at that time and find ways to bring people with 
very limited experience with digital tools up to speed. Since then, there has been substantial growth 
in both educational programs and employment opportunities. The industry is well-served by formal courses, 
online mentorship and training programs, professional associations, and many formalized internship arrangements. 
Similarly, a new generation of artists has come to the industry with high levels of digital literacy. 
The industry itself has developed sophisticated production practices and infrastructure. It has grown 
to encompass areas of specialization and developed standards of professionalism that compare favourably, 
and indeed, exceed those of many fields with longer histories. The experience in building an industry 
during the digital revolution is one of the things that makes DVFx especially valuable as a case study 
in strategy and planning for innovation in the 21st Century. There are many qualities and practices that 
have emerged from this experience that provide us with information that can be generalized to guide change 
management and plan for continued success in digital creative industries including our own. These qualities 
reflect the values of emerging digital an online cultures and workplaces that are grounded in collaborative 
practice. The FTO s traineeship initiative has monitored the changes both in the training and experience 
that applicants bring to the placement but the kinds of needs that exist for studios seeking to take 
on a trainee. Throughout this time there also have been fluctuations in industry reflecting demands for 
full CG or live action effects skills needed relative to the kinds of productions moving through the 
various pipelines. It is this need for innovation as a native state coupled the qualities of resilience 
and responsiveness as embedded processes that makes DVFx an exemplar of the industries that will thrive 
into the future. Yet as the DVFx/CG industry matures, there is a risk that a new status quo will arise 
introducing rigidity and strictures that will slow its growth and impede innovation. This is likely to 
be influenced by emerging factors of economic and social upheaval and impact upon the industry in terms 
of availability of capital for investment (particularly for R&#38;D initiatives), access to education 
and entry level positions for newcomers to the industry, and infrastructure development. The interconnectedness 
of education, employment, infrastructure and innovation needs to be uppermost in our strategies for ensuring 
that the industry s achievements continue and become more thoroughly embedded in, and a substantial contributor 
to, the ongoing transition to a world where digital and visual literacy is as crucial as numeracy and 
language skills. This analysis of the operation of the FTO Scheme provides some key qualities and strengths 
that can help guide plans for continued success. Copyright is held by the author / owner(s). SIGGRAPH 
2009, New Orleans, Louisiana, August 3 7, 2009. ISBN 978-1-60558-726-4/09/0008 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1598007</article_id>
		<sort_key>170</sort_key>
		<display_label>Article No.</display_label>
		<display_no>17</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>17</seq_no>
		<title><![CDATA[Hatching an imaginary bird]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1597990.1598007</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1598007</url>
		<abstract>
			<par><![CDATA[<p>Inhabiting the heights of a South American tepui, Kevin is the rare, wild bird that befriends the protagonists of <i>Up</i> while evading the prize-seeking villian and his pack of dogs.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>A.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P1617633</person_id>
				<author_profile_id><![CDATA[81430603549]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Byron]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bashforth]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Pixar Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617634</person_id>
				<author_profile_id><![CDATA[81448596702]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Laura]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hainke]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Pixar Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617635</person_id>
				<author_profile_id><![CDATA[81442597714]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Junyi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ling]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Pixar Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617636</person_id>
				<author_profile_id><![CDATA[81544089156]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Tom]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sanocki]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Pixar Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1598008</article_id>
		<sort_key>180</sort_key>
		<display_label>Article No.</display_label>
		<display_no>18</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>18</seq_no>
		<title><![CDATA[Rhino-palooza]]></title>
		<subtitle><![CDATA[procedural animation and mesh smoothing]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1597990.1598008</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1598008</url>
		<abstract>
			<par><![CDATA[<p>In Disney's <i>Bolt</i>, the character of Rhino poses many technical challenges. He spends a majority of the movie inside a plastic ball, frequently contacts the ground surface, and presents a complicated skinning problem. Innovative tools and technology were developed to solve these issues for the production.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Animation</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>A.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P1617637</person_id>
				<author_profile_id><![CDATA[81442598713]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Evan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Goldberg]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Walt Disney Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617638</person_id>
				<author_profile_id><![CDATA[81100542058]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Dmitriy]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Pinskiy]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Walt Disney Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Rhino-Palooza: Procedural Animation and Mesh Smoothing Evan Goldberg Dmitriy Pinskiy Walt Disney Animation 
Studios  Figure 1: (a) The rig. (b) Rolling diagram. (c) Rhino rendered in hamster ball. (d) Mesh with 
dents. (e) Dents removed. In Disney s Bolt, the character of Rhino poses many technical challenges. He 
spends a majority of the movie inside a plastic ball, frequently contacts the ground surface, and presents 
a complicated skinning problem. Innovative tools and technology were developed to solve these issues 
for the production. 1 Rigging: Rolling the Hamster Ball Rhino s hamster ball is his acting stage, a prop, 
and an extension of his personality. Hand animating the ball proved too tedious a task, so we developed 
an in-scene procedural animation solver that uses a single manipulator to interactively compute physically 
accurate rolling motion. We base our system on explicit integration methods to track position from initialization 
to the current time step. On each time delta, a velocity vector Vp is defined between the current and 
previous positions pi and pi-1, either from keyframe data or interactive manipulation. The cross product 
of Vp with the ground surface normal Vn yields a quaternion axis for rotation Vq (see Figure 1b). Angular 
rotation around this axis is computed in radians by dividing the velocity vector s Euclidean norm by 
the radius of the ball. The system can be enabled or disabled on any frame, which allows animators to 
provide custom rolling motion on a downstream node without disrupting the procedural animation solver. 
A constraint system interconnects the Rhino and hamster ball rigs, allowing them to translate in unison, 
while the ball rolls as a result of the aforementioned algorithm. Surface-constrained guides let animators 
easily position Rhino s hands and feet along the inner contour of the ball. The enable/disable functionality 
of the rolling animation system permits the guides to roll with the ball or slide independently. 2 Animation: 
Aiming and Walk Cycles The data collected to produce rolling animation can also be used to auto-orient 
Rhino in the direction of motion. On each time step, we align the character s forward axis with Vp. The 
remaining rotational degree of freedom around this vector is disambiguated via the ground surface normal 
(if none exists, the cross product of the current and previous velocity vectors is used). Additionally, 
if a walk cycle with known stride length is available, we correlate ||Vp|| with a distance-to-time conversion 
to link the animation cycle speed to the character s velocity. The converse holds true as well, where 
blocking animation can be re­timed to match cycle speed. The result is a simple-to-use path animation 
tool which allows for easier animator manipulation than those that constrain motion to a spline. evan.goldberg@disney.com, 
dmitriy.pinskiy@disney.com 3 Layout: Subdivision Visualization During layout and animation, it is common 
to use a simplified polygonal stand-in to quickly position the character. However, when a polygonal ground 
plane is subdivided to the limit surface in renders, the character appears to float above it. Since Rhino 
is frequently in contact with the ground, we developed a heavily­optimized subdivision visualization 
tool to accurately establish ground contact and maintain interactive frame rates. An adaptive subdivision 
scheme determines surface resolution as a function of camera distance while back-face and view frustum 
culling is performed based on camera orientation. Vertex shader optimization is achieved via parallelized 
limit surface evaluation on the GPU and a reduction of the data sent per glVertex call. 4 Character 
Finaling: Dent Pulling Animating Rhino s overweight physique often produces undesirable dents. These 
problematic regions produce visual artifacts that reduce the character s appeal. Traditional mesh relaxation 
based on iterative Laplacian smoothing (weighted averaging) is insufficient as it does not remove the 
dent, but rather, widens its width. Even smoothing methods that are highly sensitive to the direction 
of maximum curvature do not achieve the desired results because they work locally and do not account 
for the dent s global orientation. For regions that bend, such as Rhino s neck, a vertex may initially 
exhibit maximum curvature parallel to the dent, but after a few iterations, these smoothing schemes simply 
flatten the bent slope. Therefore, we introduce a novel fairing approach that dynamically adapts its 
relaxation scheme to the dent s orientation and is based on global curvature of geodesic curves in the 
dent. For each dent vertex pi, we maximize global curvature of a curve over all geodesic curves that 
pass through pi: b max K(y ) = max K (t)dt f pi Ey p E i y a where . is a geodesic curve that lies on 
the dented region and runs across pi,[a,b] is the parameter range of ., K is total curvature of ., and 
. is a curvature at a particular point of .. By maximizing the total curvature of ., we ensure that the 
curve spans the dent s valley across the dent. The direction of . serves as a reference for the dent's 
orientation. To pull a vertex, we still average its position with the positions of its neighboring vertices. 
However, unlike traditional Laplacian smoothing, our weights are based on how closely aligned the immediate 
neighbors are with the corresponding geodesic curve of the dent. Thus, the weights reflect the dent's 
overall shape, rather than local geometric particularities. Copyright is held by the author / owner(s). 
SIGGRAPH 2009, New Orleans, Louisiana, August 3 7, 2009. ISBN 978-1-60558-726-4/09/0008 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1598009</article_id>
		<sort_key>190</sort_key>
		<display_label>Article No.</display_label>
		<display_no>19</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>19</seq_no>
		<title><![CDATA[It's good to be Alpha]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1597990.1598009</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1598009</url>
		<abstract>
			<par><![CDATA[<p>Alpha, villain Muntz's deadly lieutenant, leads a pack of dogs to find a rare and elusive bird in Disney / Pixar's <i>Up</i>. Alpha was always imagined as the ideal of a dangerous canine. To implement this concept, Alpha went through a series of challenges, including his highly stylized geometry, caricatured articulation, sleek groom, and graphic color and illumination. We discuss our challenges and solutions, across multiple disciplines, in addressing this design.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Color, shading, shadowing, and texture</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>A.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010384</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Texturing</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P1617639</person_id>
				<author_profile_id><![CDATA[81442595985]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Paul]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Aichele]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Pixar Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617640</person_id>
				<author_profile_id><![CDATA[81430603549]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Byron]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bashforth]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Pixar Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617641</person_id>
				<author_profile_id><![CDATA[81448596702]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Laura]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hainke]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Pixar Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617642</person_id>
				<author_profile_id><![CDATA[81442604025]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Bob]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Moyer]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Pixar Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 It s Good to Be Alpha Paul Aichele* Byron Bashforth Pixar Animation Studios Pixar Animation Studios 
 Figure 1: Alpha from Up.c@Disney / Pixar. All rights reserved. Alpha, villain Muntz s deadly lieutenant, 
leads a pack of dogs to .nd a rare and elusive bird in Disney / Pixar s Up. Alpha was al­ways imagined 
as the ideal of a dangerous canine. To implement this concept, Alpha went through a series of challenges, 
including his highly stylized geometry, caricatured articulation, sleek groom, and graphic color and 
illumination. We discuss our challenges and solutions, across multiple disciplines, in addressing this 
design. 1 Modeling and Articulation The modeling and articulation approach to Alpha was challenging since 
we had to cater to both his stylized look while still incorpo­rating believable, organic dog anatomy. 
Some of the anatomy chal­lenges included clavicle-skin interaction and directable small and large muscle 
group .exing. Unlike most past Pixar characters, we modeled Alpha s muscular anatomy directly into the 
topology. For example, we structured the layout to support from primary muscles down to tendons and small 
facial details. This allowed us to articu­late the muscles and sculpt them into the appropriate deformers. 
Another challenge was integrating a new three-limb IK solver with specialized controls for animating 
IK segment rigidity. Animators could then transition between various action poses and stationary poses 
such as sitting or kneeling, as well as sprinting or running * e-mail: aichele@pixar.com e-mail: byron@pixar.com 
e-mail: lhainke@pixar.com §e-mail: bmoyer@pixar.com Laura Hainke Bob Moyer§ Pixar Animation Studios 
Pixar Animation Studios with local control over each bone. Since animation could control the amount of 
muscular deformation, they effectively blended be­tween stylization and realism. 2 Grooming The show 
s stylization also presented a set of challenges for groom­ing. We grounded the clean simplicity of Alpha 
s design by devel­oping as much micro detail in the groom as possible. This em­phasized the simpler detail 
in his coloration, while keeping him from appearing .at. Simpli.ed markings using bold shapes, such as 
the triangular patterns on his chest, helped keep the fur s geo­metric complexity within the stylized 
world of Up. We pushed the directional turns within the hair to support these designs, but still maintained 
a naturalistic .ow where possible. Hair quality, while sleek and clean in keeping with the character 
of Alpha, also needed to be slightly coarser to support the chunk­i.ed Up world of oversized materials. 
In addition, Alpha s smooth form was in danger of looking too perfect, so we slightly broke the pro.le 
on the backs of the legs and ears with longer hairs. This, along with a careful plan of hair .ow direction, 
balanced stylization and naturalism to support the geometry and animation. 3 Shading Likewise, Alpha 
s shading found areas to both punch up the al­pha dog caricature, while grounding as much as possible 
in an or­ganic realism. Alpha was designed very graphically, with sharp transitions and bold contrasting 
colors. To idealize the concept of a Doberman, the character needed to feel as black as possible while 
still readable in a dark tepui jungle. To shape his fur, we tinted the kajiya specular and diffuse strongly 
blue. We added a system to allow the groomer paint tinted illumination response to control this effect. 
This also separated Alpha out from the rest of the pack, even from large distances. Secondly, to avoid 
.at areas of solid colors, we added a paintable per-hair color randomization system. This allowed areas 
of simple color to have a subtle micro-variation. The groomer painted zones of randomization, choosing 
where to keep colors soft and constant or where to add more per-hair blend dithering. Finally, while 
Alpha is 90 percent covered in hair, the additional elements of his shading help him remain true to the 
Up design. Mouth, eyes, and collar are all over-sized and caricatured in color, pattern, and illumination. 
 4 Conclusion Alpha s dif.culty came from making a series of subtle choices that would emphasize the 
character s strong stylistic design. A team ef­fort, he portrays his role as lieutenant and consigliere 
with his pow­erful form, graphic colors, and sleek grooming. We provided a set of controls across all 
disciplines that blended between the de.ned stylization and the demands of realism. Copyright is held 
by the author / owner(s). SIGGRAPH 2009, New Orleans, Louisiana, August 3 7, 2009. ISBN 978-1-60558-726-4/09/0008 
  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1598010</article_id>
		<sort_key>200</sort_key>
		<display_label>Article No.</display_label>
		<display_no>20</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>20</seq_no>
		<title><![CDATA[Venomous cattle for <i>Australia</i>]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1597990.1598010</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1598010</url>
		<abstract>
			<par><![CDATA[<p>In Baz Luhrmann's <i>Australia</i> the main protagonists drive a massive herd of cattle through the Australian outback. To extend the on-set cattle herd with up to 5000 digital bovines we developed a crowd system based on a <i>Autodesk Maya</i> integration of our <i>Venom</i> infrastructure. This sketch will give an overview of the crowd workflow and how it utilized Venom features to arrange, simulate, reuse and render animated cattle for numerous shots.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>A.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P1617643</person_id>
				<author_profile_id><![CDATA[81421596511]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Carsten]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kolve]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Rising Sun Pictures]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617644</person_id>
				<author_profile_id><![CDATA[81442615145]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Dan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bethell]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Rising Sun Pictures]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617645</person_id>
				<author_profile_id><![CDATA[81442617360]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Rogier]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Fransen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Rising Sun Pictures]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617646</person_id>
				<author_profile_id><![CDATA[81361600783]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Johannes]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Saam]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Rising Sun Pictures]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617647</person_id>
				<author_profile_id><![CDATA[81442614708]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Premamurti]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Paetsch]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Rising Sun Pictures]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Venomous Cattle for Australia Carsten Kolve Dan Bethell Rogier Fransen Johannes Saam Premamurti Paetsch 
Rising Sun Pictures  Figure 1: Cattle extension. &#38;#169;20th Century Fox. All rights reserved. In 
Baz Luhrmann's Australia the main protagonists drive a massive herd of cattle through the Australian 
outback. To extend the on-set cattle herd with up to 5000 digital bovines we developed a crowd system 
based on a Autodesk Maya integration of our Venom infrastructure. This sketch will give an overview of 
the crowd workflow and how it utilized Venom features to arrange, simulate, reuse and render animated 
cattle for numerous shots. 1. Layout The base data structure for a virtual cattle herd is a crowd container. 
This container holds all general and per agent data necessary to generate a crowd. In each working step 
more data is added until we can render a fully animated herd. The most important information added at 
layout stage is a transformation matrix for each agent, indicating the initial world position and orientation. 
Transforms can be generated via scripting or, as the system is fully integrated into Maya, by sampling 
data like particle systems, geometry (used for positioning on a ground plane) and shaders (used for densities). 
A custom Venom Maya shape node allows for overall and per agent transform modification. Like any other 
shape node, deformers can be applied on the data as well. Full crowd layouts are used for shot blocking 
purposes, but typically not for the initial positioning of a full crowd simulation. Instead the crowd 
container layout is divided into smaller sub­layouts based on distance to camera and similar performance. 
 2. Simulation A per agent skeleton structure is now added to the crowd container which is animated using 
the animation library. This library is composed of interconnectable loops and transitions. Various techniques 
are utilizing it to produce animation: Simple hand crafted or randomly generated animation lists drive 
static crowds without much agent interaction. A function that finds the best fitting next set of animations 
in the library based on a predicted or predetermined path is used in shots that require more sophisticated 
navigation. Specific crowd behaviour and offset joint animation can also be scripted by accessing the 
crowd container directly. Blending is only used to smoothen transitions at the start and end of clips. 
Collisions between the often tightly packed cattle are resolved using a relaxation technique that uses 
clusters of spheres to approximate the collision shapes of agents and obstacles. The resulting offset 
translation and rotation can be applied gradually to the root, but foot sliding is not automatically 
prevented. Instead the artist controls the balance between amount of intersections and quality of animation. 
For performance reasons agent animation is generated in a two­step process: First, only the animation 
of the root is calculated as it is often sufficient to judge the quality of a simulation. Only if the 
root animation is successful the remaining joint animation is computed. 3. Crowd Building Blocks The 
result of a simulation is never directly used in shot context. Instead crowd containers are cached over 
a longer period of time (as compared to an average shot length) and then used as crowd building blocks. 
The Crowd-TD assembles the blocks using a browser like interface in the final scene. Crowd caches and 
their individual components can be transformed in space and time, all operations described in the Layout 
section are usable here, too. Common editing operations like deletion or duplication are also supported. 
This approach has multiple benefits: Producing convincing simulation on a big scale is hard and often 
requires lengthy iterations, focussing on smaller groups in contrast allows for more iterations and a 
higher quality result. After a small library of building blocks has been created, the ability to reuse 
them becomes a major time saver especially when dealing with multiple shots that require a similar crowd 
performance. On Australia only 6 small caches with an average of 300 agents were used to produce all 
cattle herding shots, even though the crowd shape changed dramatically. The herd can be assembled for 
the exact needs of the shot, with the ability to remove performances that are not working from the current 
camera angle while making successful actions more prominent. Assembly work can also be done by a much 
less technical artist than simulation work. Figure 2: Cattle extension, original plate marked red. &#38;#169;20th 
Century Fox. All rights reserved. 4. Rendering Crowd containers can be rendered in different contexts 
 the most commonly used one being an OpenGL context for preview in Maya and a 3Delight / Renderman context 
for high quality rendering. At the render pre-processing stage for each agent a procedural call is inserted 
into the rib-stream in conjunction with the agent's bounding box. Then at render-time this procedural 
uses the skeleton and added model with binding information in the crowd container to skin and render 
the character on demand. Crowd containers can be annotated with extra per-agent-data by the Lighting 
TD and these attributes in turn be utilized by the shader. All variations including colours, textures, 
displacements, id passes etc. are realized using this technique. Blend shapes controlled in the same 
way generate different cattle proportions. Agents have one unique id number used for targeted value assignment 
independent of production department. For example, the compositor (using the u-id pass) can provide information 
to a Lighting- or Crowd-TDs on how to change which animal. 5. Outlook Future work will focus on making 
the technical, script based workflow accessible to a broader artist base. The current animation system 
can be enhanced to allow more responsive blending between animation clips (for example using motion-graph 
techniques) and prevent foot-sliding by incorporating ik solutions. Copyright is held by the author / 
owner(s). SIGGRAPH 2009, New Orleans, Louisiana, August 3 7, 2009. ISBN 978-1-60558-726-4/09/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1598011</article_id>
		<sort_key>210</sort_key>
		<display_label>Article No.</display_label>
		<display_no>21</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>21</seq_no>
		<title><![CDATA[Applying painterly concepts in a CG film - Bolt]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1597990.1598011</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1598011</url>
		<abstract>
			<par><![CDATA[<p>The leadership of the animated film <i>Bolt</i> set out to bring back the textural warmth found in classic Disney animated movies, something not usually found in CG generated images. CG algorithms characteristically produce hard clean geometric solutions and can also often generate unlimited amounts of detail. We analyzed paintings from masters and also stepped back and thought about how a painter approached a painting here at Disney. We focused on fundamental ideas such as massing, a term in painting which refers to the process of editing detail into bigger shapes, and also edge quality, the use of the painter's brush to vary edges of shapes which can bring emphasis to the image and/or direct the eye. This lead to the development of new algorithms and tools for the film <i>Bolt</i>. It also required a change in the mind set and work flows for the CG artist. We will discuss things such as "raypainting" and painterly "normals" which were part of the toolbox created on Bolt. These concepts mixed with photographic ideas such as exposure and film response lead to the rich and unique look of Bolt.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>A.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P1617648</person_id>
				<author_profile_id><![CDATA[81328489617]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Adolph]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lusinsky]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617649</person_id>
				<author_profile_id><![CDATA[81546232656]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Paul]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Felix]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617650</person_id>
				<author_profile_id><![CDATA[81100305675]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Ernie]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Petti]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617651</person_id>
				<author_profile_id><![CDATA[81442600792]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Sean]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Jenkins]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617652</person_id>
				<author_profile_id><![CDATA[81442614166]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Adrienne]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Othon]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617653</person_id>
				<author_profile_id><![CDATA[81100572197]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Patrick]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Dalton]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617654</person_id>
				<author_profile_id><![CDATA[81328488131]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>7</seq_no>
				<first_name><![CDATA[Hank]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Driskill]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617655</person_id>
				<author_profile_id><![CDATA[81442618227]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>8</seq_no>
				<first_name><![CDATA[John]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Murrah]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1598012</article_id>
		<sort_key>220</sort_key>
		<display_label>Article No.</display_label>
		<display_no>22</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>22</seq_no>
		<title><![CDATA[Painting with polygons]]></title>
		<subtitle><![CDATA[non-photorealistic rendering using existing tools]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1597990.1598012</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1598012</url>
		<abstract>
			<par><![CDATA[<p>Most non-photorealistic-rendering solutions tend to involve brilliant but unwieldy new technologies, such as volume-based rendering engines or complex image analysis. Similar results can often be achieved using simpler methods and non-proprietary toolsets, even toolsets designed with other effects in mind. Our studio has been experimenting with ways to achieve a hand-painted look with basic tools that are common to most 3D applications.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>A.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P1617656</person_id>
				<author_profile_id><![CDATA[81442618338]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Isaac]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Botkin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Painting with Polygons Non-Photorealistic Rendering Using Existing Tools by Isaac Botkin Most non-photorealistic-rendering 
solutions tend to involve brilliant but unwieldy new technologies, such as volume-based rendering engines 
or complex image analysis. Similar results can often be achieved using simpler methods and non-proprietary 
toolsets, even toolsets designed with other effects in mind. Our studio has been experimenting with ways 
to achieve a hand-painted look with basic tools that are common to most 3D applications. For example, 
normal displacement, usually used to add detail to topology, can also be used to distort a surface without 
compromising the object volume (fig. 1). If this distortion is cycled once per frame and rendered with 
motion blur, the resulting effect simulates layered, transparent brush strokes (fig. 2). The strength 
of the displacement can be effected by vertex or image maps so areas of the model that permit low detail 
can have broad, loose brushstrokes, while areas that require higher detail are still clear. These overlapping 
strokes can be emphasized using various textures and shaders and lighting models, but effective results 
can be achieved even with basic adjustments to diffuse curves, edge lights, and colored shadows (fig. 
3). Hand-painted or procedural bump maps can amplify the brush strokes created by geometry, or run perpendicular 
to the mesh flow to blend between tonal regions and emulate crosshatching (fig. 4). Further control over 
the perceived brushstrokes is achieved by adjusting the number of antialiasing passes and subdivision 
level. Because the brushstrokes exist in 3D space, and their direction and coverage is governed by the 
mesh itself, the final result renders very quickly and maintains temporal coherence even under complex 
deformation. The resulting render will benefit from multi-pass compositing and 2D processing, but image 
analysis to create strokes is not required. And since this approach uses native polygonal rendering, 
it can be combined with all existing shaders and global illumination, is compatible with dynamics calculations, 
and easily integrates with existing workflows for rendered or real-time projects regardless of platform. 
When a project s art direction and object modeling are specifically designed to take advantage of this 
technique, a rich storybook quality can be achieved. Furthermore, it benefits from modeling techniques 
such as intersecting objects and low-poly primitives, enabling modelers to rough out sets and characters 
quickly, placing details in a flexible workflow more akin to traditional sculpting or painting. In conclusion, 
this techniques permits a handpainted look without specialized rendering engines, allowing studios to 
continue using the tools within their existing animation and modelling pipelines. Fig. 1 Base Mesh Fig. 
2 Normal Displacement Fig. 3 Diffuse Sharpness Fig. 4 Procedural Bump Map  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1598013</article_id>
		<sort_key>230</sort_key>
		<display_label>Article No.</display_label>
		<display_no>23</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>23</seq_no>
		<title><![CDATA[Smoother subsurface scattering]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1597990.1598013</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1598013</url>
		<abstract>
			<par><![CDATA[<p>The irradiance-cached dipole diffusion approximation proposed by [Jensen and Buhler 2002] is a practical tool for capturing the subsurface scattering of many common materials in the context of film-quality rendering. One drawback of this model is the high irradiance point density it requires to attenuate sampling noise at low mean free paths, which can be expensive in terms of time and memory. A clever smoothing technique by [Langlands and Mertens 2007] filters out this sampling noise by blending the noisy irradiance-cache-derived diffusion approximation with a noise-free precomputed diffusion integral that is weighted by local irradiance. But this smoothing leaves an artifact at low sample densities, as shown in Figure 1: abrupt changes in irradiance, e.g. across shadow edges, tend to produce hard edges in the rendered image, irrespective of the mean free path. Moreover, increasing the smoothing radius hardens rather than softens these edges.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Color, shading, shadowing, and texture</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>A.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010384</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Texturing</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P1617657</person_id>
				<author_profile_id><![CDATA[81100390217]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Ivan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Neulander]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Rhythm & Hues Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>566619</ref_obj_id>
				<ref_obj_pid>566570</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Jensen, H. W., and Buhler, J. 2002. A rapid hierarchical rendering technique for translucent materials. In <i>SIGGRAPH' 02 Proceedings</i>, ACM, 576--581.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1280917</ref_obj_id>
				<ref_obj_pid>1280720</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Langlands, A., and Mertens, T. 2007. Noise-free bssrdf rendering on the cheap. In <i>SIGGRAPH' 07 posters</i>, ACM, 182.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Smoother Subsurface Scattering Ivan Neulander Rhythm&#38;Hues Studios The irradiance-cached dipole 
diffusion approximation proposed by [Jensen and Buhler 2002] is a practical tool for capturing the sub­surface 
scattering of many common materials in the context of .lm­quality rendering. One drawback of this model 
is the high irradiance point densityit requiresto attenuate sampling noiseatlowmean free paths, which 
canbeexpensivein termsof time and memory.Aclever smoothing technique by [Langlands and Mertens 2007] 
.lters out this sampling noise by blending the noisy irradiance-cache-derived diffu­sion approximation 
with a noise-free precomputed diffusion integral that is weighted by local irradiance. But this smoothing 
leaves an arti­fact at low sample densities, as shown in Figure 1: abrupt changes in irradiance, e.g. 
across shadow edges, tend to produce hard edges in the rendered image, irrespective of the mean free 
path. Moreover, increas­ing the smoothing radius hardens rather than softens these edges. We extend the 
work of [Langlands and Mertens 2007] by adding two enhancements: 1) higher .delity of shadow edges, at 
the expense of adding a small amount of noise; 2) the ability to interpolate among a sparse set of precomputed 
diffusion integrals, allowing continuous variation of the mean free path. Figure 1: Results using 23k 
irradiance points: (a) no smoothing; (b) Langlands&#38;Mertens smoothing;(c) our smoothing;(d) ground 
truth. See supplemental notes for timing data on this and other images. Shadow Edge Fidelity The [Langlands 
and Mertens 2007] technique separates the dif­fusion component of the BSSRDF integral into a local term 
g w(r)Rd(r)E(x)dx and a global term [1- w(r)]Rd (r)E(x)dx, where w(r) is a weighting kernel de.ned so 
that w(0)= 1 and w(a ···8)= 0 for a chosen smoothing radius ofa. The global term is computed only from 
cached irradiance samples, as prescribed by [Jensen and Buhler 2002], while the local term is accurately 
precom­puted using a constant irradiance value and multiplied by the actual irradiance at shading point 
x. This arrangement supplants the noisy global term with the noise-free local term in the vicinity of 
irradiance­cached sample points. But because the local term varies linearly with local irradiance, rapid 
changes in the latter (within a radius of a)spill over into the overall radiant exitance. As the irradiance 
cache density increases, a naturally diminishes and the problem abates. However, as shown in Figure 2, 
the density required for passable shadow edges may be so high as to obviate the need for smoothing. Our 
solution is to modify the irradiance value that drives the local term: in addition to local irradiance, 
we include a contribution from the entire irradiance cache. Conceptually, we insert the local irradi­ance 
point into the cache, using the appropriate weighing factor of Rd(0) and an area weight equal to the 
average irradiance-cached sam­ple area. Then we query this modi.ed cache for the irradiance to apply 
to the local term. This query is identical to that for the global term, except for the extra point at 
r = 0 and the differing kernel weights. Our implementation performs both queries in a single traversal 
of the octree, with essentially no redundant computation. By incorporating cached irradiance into the 
local term, we rein­troduce some variability into the radiant exitance. However, because Rd peaks sharply 
at zero, the resulting noise is inconspicuous except where the local irradianceis low. In effect, we 
replace hard shadow edges with noisierbut appropriately soft ones. Figure 2: In the middle three columns, 
our method yields the best image. In the left column all three methods work poorly, while in the right 
all three are close in quality. Continuous Mean Free Path Variation The precomputed integral for the 
local term in [Langlands and Mertens 2007]varies nontrivially with the mean free path Ju,so rendering 
mul­tiple Ju values entails multiple integrations. This precludes ef.cient rendering when Ju varies continuously. 
We lift this limitation by evaluating and caching the integral on de­mand at exponentially distributed 
values of Ju, and linearly interpolat­ing between adjacent integrals. As Ju approaches zero, we limit 
the evaluation rate based ona user-de.ned threshold.Wehave found em­pirically that cachingat integerpowersof2is 
adequate,as illustrated in Figure 3. In general, our interpolated smoothing incurs a runtime overhead 
of under 5% relative to no smoothing. Figure 3:Texture-mapped Ju variation with our smoothing.  References 
JENSEN, H. W., AND BUHLER, J. 2002. A rapid hierarchical ren­dering technique for translucent materials. 
In SIGGRAPH 02 Pro­ceedings,ACM, 576 581. LANGLANDS, A., AND MERTENS, T. 2007. Noise-free bssrdf ren­dering 
on the cheap. In SIGGRAPH 07 posters,ACM, 182. Copyright is held by the author / owner(s). SIGGRAPH 
2009, New Orleans, Louisiana, August 3 7, 2009. ISBN 978-1-60558-726-4/09/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1598014</article_id>
		<sort_key>240</sort_key>
		<display_label>Article No.</display_label>
		<display_no>24</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>24</seq_no>
		<title><![CDATA[Radially-symmetric reflection maps]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1597990.1598014</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1598014</url>
		<abstract>
			<par><![CDATA[<p>When designing lighting for outdoor scenes, area lights can represent distant sources such as the sun and sky with greater shading fidelity than is possible with simple directional or hemispherical lights, but the combination of dynamic area light sources and complex BRDFs is challenging to render efficiently in real-time. Prefiltered reflection maps are a popular solution to this problem, but for applications which need a wide variety of light sources and materials, the texture data requirements of cubic reflection maps can be prohibitive in terms of memory overhead and render efficiency. In addition, traditional environment map representations are difficult for artists to directly paint and refine for use in scene lighting.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>A.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P1617658</person_id>
				<author_profile_id><![CDATA[81406596769]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Jonathan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Stone]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Double Fine Productions]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Kautz, J, and McCool, M. 2000. Approximation of Glossy Reflection with Prefiltered Environment Maps. In <i>Proceedings of Graphics Interface 2000</i>, 119--126.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Miller, G., and Hoffman, C. R. 1984. Illumination and Reflection Maps: Simulated Objects in Simulated and Real Environments. In <i>Course Notes for Advanced Computer Graphics Animation, ACM SIGGRAPH 84</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Radially-Symmetric Reflection Maps Jonathan Stone Double Fine Productions  Figure 1: A scene in Brütal 
Legend at two different times of day, with the blended reflection maps for sun and sky lights. 1 Introduction 
When designing lighting for outdoor scenes, area lights can represent distant sources such as the sun 
and sky with greater shading fidelity than is possible with simple directional or hemispherical lights, 
but the combination of dynamic area light sources and complex BRDFs is challenging to render efficiently 
in real-time. Prefiltered reflection maps are a popular solution to this problem, but for applications 
which need a wide variety of light sources and materials, the texture data requirements of cubic reflection 
maps can be prohibitive in terms of memory overhead and render efficiency. In addition, traditional environment 
map representations are difficult for artists to directly paint and refine for use in scene lighting. 
In this paper we propose a new representation for distant, radially­symmetric area light sources, one 
which requires far less texture memory, is more efficient to render, and which is more intuitive for 
artists to work with than cubic reflection maps. In Brütal Legend, the game for which this technique 
was developed, radially-symmetric reflection maps are the core element of our outdoor lighting model, 
providing a solution for distant area lights that meets both the artistic and engineering needs of a 
large, open­world game. 2 Radially-Symmetric Reflection Maps A radially-symmetric reflection map is 
a variant of a reflection map [Miller and Hoffman 1984] which is specialized to describe a distant radially-symmetric 
light source such as the sun or sky. The source environment map is a 1-dimensional texture with each 
texel representing the incoming radiance over a zone of the unit sphere, and the filtered reflection 
map is a 2-dimensional texture with each column corresponding to a spherical zone and each row corresponding 
to a BRDF lobe. The source environment map can be directly painted by an artist, who can view the 1-dimensional 
drawing as a gradient describing the color of incoming light from the north to south poles. Each texel 
of the source map covers a spherical zone of uniform height and solid angle, so all texels contribute 
equally to the radiant intensity of the light source. Once the source image has been painted, it is convolved 
with each BRDF kernel that is required by the run-time. In this paper, we consider only materials which 
are a linear combination of Lambert diffuse and Phong specular lobes, but as with standard reflection 
maps, RSRM can be extended to other combinations of radially-symmetric lobes [Kautz and McCool 2000]. 
Since the Lambert diffuse lobe has the same shape as the normalized Phong specular lobe of exponent 1, 
we will focus on the specular convolution, which is computed with the following summation:  (1)   
In Equation 1, the outgoing radiance of the spherical zone about zenith angle  is computed by summing 
the weighted contributions from each zone of incoming light. The contribution from each zone  is equal 
to its incoming radiance  times its total BRDF weight, which is estimated by summing the exponentiated 
cosine weight  for a series of azimuths across the spherical zone. The radial symmetry of the incoming 
and outgoing light allows us to use a fixed azimuth for  and to sample   over only half the zone. 
The constant is the BRDF normalization factor for Phong exponent , and is the uniform solid angle of 
spherical patches of incoming light. 3 Real-Time Rendering Once the convolutions have been generated 
for a light source, they are combined as the rows of a 2-dimensional reflection map for use in rendering. 
For specular light contributions, the reflection map is accessed with the dot product, where is the 
light s axis of symmetry, and a row index for the specular exponent of the material. For diffuse lighting, 
the reflection map is accessed with the dot product and a constant row index for exponent 1, returning 
values equivalent to a diffuse irradiance environment map. Surfaces with spatially-varying BRDFs can 
be supported by allowing the specular row index to vary per-pixel. Because of the compact size of an 
RSRM, these texture fetches are very lightweight, and the shader math is comparable to that for a simple 
directional light. Time of day transitions can also be accomplished by blending between RSRM textures 
on the GPU. The dimensions of RSRMs will depend on the needs of the application, but in Brütal Legend 
the filtered reflection maps are stored at 256x8 resolution with 8 bits per color component. These LDR 
values are modulated by the full HDR color of the light in a pixel shader. With each RSRM representing 
a separate light source, we have found this to provide enough texture detail to avoid banding artifacts, 
while maintaining fast texture fetches. References KAUTZ, J, and MCCOOL, M. 2000. Approximation of Glossy 
Reflection with Prefiltered Environment Maps. In Proceedings of Graphics Interface 2000, 119 126. MILLER, 
G., and HOFFMAN, C. R. 1984. Illumination and Reflection Maps: Simulated Objects in Simulated and Real 
Environments. In Course Notes for Advanced Computer Graphics Animation, ACM SIGGRAPH 84. Copyright is 
held by the author / owner(s). SIGGRAPH 2009, New Orleans, Louisiana, August 3 7, 2009. ISBN 978-1-60558-726-4/09/0008 
  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1598015</article_id>
		<sort_key>250</sort_key>
		<display_label>Article No.</display_label>
		<display_no>25</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>25</seq_no>
		<title><![CDATA[Interactive lighting of effects using point clouds in "Bolt"]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1597990.1598015</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1598015</url>
		<abstract>
			<par><![CDATA[<p>Integrating effects into an environment is often a challenging task. This is especially true when the light source is a dynamically changing organic shape such as a lick of flame against a wall of smoke or glowing embers wrapping around a character. Typically, this illumination can be faked with a large number of artistically placed animated lights. However, this often takes time and experimentation to get a pleasing and plausible result. We created a pipeline that is flexible and allows us to generate interactive light from any environmental effect that we would create.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>A.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P1617659</person_id>
				<author_profile_id><![CDATA[81442592544]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Dale]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Mayeda]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Walt Disney Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Interactive Lighting Of Effects Using Point Clouds In BOLT Dale Mayeda* Walt Disney Animation Studios 
 1. Introduction Integrating effects into an environment is often a challenging task. This is especially 
true when the light source is a dynamically changing organic shape such as a lick of flame against a 
wall of smoke or glowing embers wrapping around a character. Typically, this illumination can be faked 
with a large number of artistically placed animated lights. However, this often takes time and experimentation 
to get a pleasing and plausible result. We created a pipeline that is flexible and allows us to generate 
interactive light from any environmental effect that we would create. 2. Technique Typically, RenderMan 
point clouds and the indirectdiffuse calculation will generate colored bounce light from one surface 
to another. We have taken that basic idea and expanded it as a primary light source for a majority of 
environmental effects that we would create. In addition to having effects like fire illuminating hard 
surfaces, we integrated the lighting calculation into our volume smoke pipeline to also receive light 
from point clouds. a. All effects surface, volume and sprite shaders written with the bake3d call in 
order to generate point clouds from any effect. Point clouds also exportable directly from Houdini. b. 
Point clouds filtered in order to combine or reduce point counts to a manageable size using ptmerge and 
custom point cloud tools. c. All effects surface, volume and sprite shaders written with the indirectdiffuse 
call to receive illumination from the generated point clouds. d. Surfaces in our standard pipeline would 
receive our point clouds and use ptfilter to combine and pre-calculate the illumination for use on surfaces. 
 is color bleeding of surfaces as in the Cornell Box  3. Tools Developed For Flexibility In order to 
fully integrate point clouds into our effects pipeline, a number of useful tools were created. a. Houdini 
point cloud reader/writer. Point clouds generated from any source could be brought into Houdini to be 
visualized, manipulated and exported. Point clouds could also be generated directly from Houdini without 
rendering. b. Point cloud filter. Heavy point clouds could be filtered down in point count with this 
utility. A time filter option would mix in a percentage of before/after frames to avoid time aliasing 
due to culling random points. c. Maya point cloud visualizer. A visualizer inside of Maya displayed animated 
point clouds and any attributes. This was extremely useful for both effects and lighting artists. *Dale.Mayeda@disney.com 
Copyright is held by the author / owner(s). SIGGRAPH 2009, New Orleans, Louisiana, August 3 7, 2009. 
ISBN 978-1-60558-726-4/09/0008  &#38;#169; Disney Interactive display of point clouds, generated from 
fire and used to illuminate smoke 4. Illumination in Volumes Illuminating smoke from animated and organic 
light sources is extremely difficult using standard light types. By using point clouds generated from 
the actual effects provides interactive lighting, which is easily adjustable. In order to use the indirectdiffuse 
call on volumes and sprites, it is necessary that there is no falloff of intensity based on the surface 
normal or I vector. Setting the distribution to uniform will enable this behavior. Since a volume does 
not have surface normals, we would pass an arbitrary normal and it s inverted direction for each shaded 
point. We would then sum the lighting contribution from both directions. This would make sure that you 
were receiving the indirectdiffuse calculation from all directions for each volume point. &#38;#169; 
Disney Volume smoke pass illuminated only by point clouds 5. Conclusions Typically the integration of 
organic effects that emit light would require a great deal of effort from a lighting artist. By using 
point clouds in the pipeline, interactive illumination of effects is easily obtainable and art directable. 
These point clouds can also be used to provide interactive lighting on smoke, which would be difficult 
with conventional light sources. By creating supportive tools, use of this technique becomes a simple 
and robust part of both the effects and lighting workflow. &#38;#169; Disney A final frame from Bolt 
of the soundstage inferno 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1598016</article_id>
		<sort_key>260</sort_key>
		<display_label>Article No.</display_label>
		<display_no>26</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>26</seq_no>
		<title><![CDATA[Composite based refraction for fur and other complex objects on "Bolt"]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1597990.1598016</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1598016</url>
		<abstract>
			<par><![CDATA[<p>Creating the Rhino character for the film "Bolt" presented a unique challenge: generate realistic fur which is then viewed through a plastic, refractive hamster ball.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>A.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P1617660</person_id>
				<author_profile_id><![CDATA[81100574361]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Lewis]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Siegel]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Walt Disney Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617661</person_id>
				<author_profile_id><![CDATA[81442600792]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Sean]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Jenkins]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Walt Disney Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Composite Based Refraction for Fur and Other Complex Objects on Bolt Lewis Siegel* Sean Jenkins Walt Disney Animation Studios 
 Introduction Creating the Rhino character for the film "Bolt" presented a unique challenge: generate 
realistic fur which is then viewed through a plastic, refractive hamster ball.  Rendering hair and simulating 
refraction, when performed separately, are both time and memory intensive. When combined, the demand 
for time and memory resources increases tremendously, and likely will result in an unusable image. To 
overcome these problems, a process was developed for Bolt which involved exporting 3D data from the render 
stage so that the calculation of the refraction could be delayed until the composite stage. The technique 
allowed for quick and easy modification of the refraction effect using standard compositing techniques. 
The refraction technique was also successfully integrated into the stereo process developed for Bolt. 
2 Rendering In order to make the result of a compositing based refraction solution appear a realistic, 
it was desirable to maintain as many levels of refraction as possible. Therefore, it was necessary during 
the rendering stage to split the ball into two parts, front and back, along the ball's silhouette edge 
as seen from the camera, with the Rhino character floating in between the two hemispheres. Unfortunately, 
the ball is not a perfect sphere; the surface contains several openings and indentations. As a result, 
a simple front/back facing evaluation was not sufficient. Instead, a geometric approached was used, evaluating 
the angle between the camera, the surface position, and the center of the ball, resulting in an accurate 
split, even for the non­spherical details on the ball's surface. The three split components front, back 
and character are rendered as separate image layers, freeing the composite artist to color correct each 
independently, instead of manipulating all three elements baked into a single image. Also during rendering, 
a variety of 3D information for both the front and back of the ball was exported. This included surface 
position and normal data (in camera space), and also a painted distortion map which emulates irregularities 
on the surface of the hamster ball. In addition, as the Rhino character rolled the * Email: Lewis.Siegel@disney.com 
hamster ball through space, it was important that the refraction stay consistent in relation to the surface, 
and not swim as the distorted surface normals changed orientation. As a result, a coordinate system located 
in the center of the ball was used to export a set of color­coded orientation images which express the 
coordinate system as a set of basis vectors in camera space. The hamster ball surface normals could then 
be transformed, in compositing, back into the hamster ball's object coordinate system, resulting in consistent 
noise calculations regardless of the ball's position and orientation relative to the camera. 3 Compositing 
The render stage results in several types of raw, 3D output data, which can then be used in the composite 
stage to perform the final refraction calculation. The compositing software used, however, must provide 
the ability to perform arbitrary arithmetic calculations on per­pixel RGB data. The compositing software 
used on Bolt included advanced warping tools with these capabilities, along with a macro mechanism that 
allowed complex operations to be packaged into a user friendly format. The first step was to create a 
refraction macro, capable of refracting an arbitrary image through a surface for which 3D render data 
had been exported. After that was achieved, it became clear that the same data could be used to perform 
a Fresnel­like reflection effect near the edges of the hamster ball. The result was a corresponding reflection 
macro. Finally, to achieve a realistic result, several layers of refraction and reflection were combined: 
a layer of refraction and reflection on the back of the ball, underneath a refracted layer of the Rhino 
character inside the ball, followed by another layer of refraction and reflection through the front of 
the ball. The multiple layers of refraction provided depth cues and a parallax effect while the ball 
was in motion, enhancing the impression that the Rhino character was truly inside a refractive sphere. 
Additionally, the amounts of refraction could be modulated in compositing on a per­layer and even a per­pixel 
basis, allowing for rapid turn around of changes due to notes from art­direction and review. 4 Stereo 
Finally, with some minor modifications, the entire process was able to be made compatible with the multi­camera 
stereo rendering approach employed for the production of Bolt. The result was a seamless work­flow for 
the hamster ball refraction effects from start to finish. 5 Conclusion A render­time solution to the 
combined challenges of hair rendering and refraction, while technically accurate, was not possible for 
the production of Bolt due to the large amounts of time and memory required over a large number of shots, 
and due also to the inability to quickly and easily modify the result. A novel composite based approach 
was used instead, providing a fast, high quality and artist­friendly solution to the problem of making 
the Rhino character appear to be inside an encompassing, refractive sphere. Copyright is held by the 
author / owner(s). SIGGRAPH 2009, New Orleans, Louisiana, August 3 7, 2009. ISBN 978-1-60558-726-4/09/0008 
  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1598017</article_id>
		<sort_key>270</sort_key>
		<display_label>Article No.</display_label>
		<display_no>27</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>27</seq_no>
		<title><![CDATA[Dense stereo event capture for James Bond, Quantum of Solace]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1597990.1598017</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1598017</url>
		<abstract>
			<par><![CDATA[<p>Filming your lead actor jump out of an aeroplane without a parachute presents insurmountable safety problems for any attempt at an entirely practical shoot. However, if the production insists that a live action solution must be pursued, how can a convincing sequence be created?</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.4.8</cat_node>
				<descriptor>Stereo</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>A.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P1617662</person_id>
				<author_profile_id><![CDATA[81442618287]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Ted]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Waine]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Double Negative Ltd.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617663</person_id>
				<author_profile_id><![CDATA[81320491067]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Oliver]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[James]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Double Negative Ltd.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Dense Stereo Event Capture for James Bond, Quantum of Solace TedWaine and Oliver James (DoubleNegative 
Ltd.)* Figure 1: Multi-view performace capture. 1 Introduction Filming your lead actor jump out of an 
aeroplane without a parachute presents insurmountable safety problems for anyattempt at an entirely practical 
shoot. However, if the production insists that a live action solution must be pursued, how can a convincing 
sequence be created? Double Negative VFX and the production crew of the 22nd James Bond movie, Quantum 
of Solace, safely .lmed Daniel Craig and OlgaKurylencoin freefallby recording the actors performing sus­pended 
in the air stream of a wind tunnel normally used to train skydivers. Controlling the lighting and the 
camera s position within the con­.nes of the small tunnel presented a new set of problems. These were 
overcome by employing a dense stereo system to recover the surface geometry of the performers, allowing 
us to relight and re­render the action from a novel camera position without recourse to a fully CG solution. 
 Figure 2: Volume carved visual hull.  2 Exposition Fifteen cameras(8 Dalsa4K Origin and7SonyCinealta) 
were po­sitioned in the viewing portholes of the wind tunnel and electroni­cally synchronised. The lighting 
was arranged to be as diffuse and *e-mail: ted@dneg.com, oj@dneg.com Copyright is held by the author 
/ owner(s). SIGGRAPH 2009, New Orleans, Louisiana, August 3 7, 2009. ISBN 978-1-60558-726-4/09/0008 
as bright as possible, allowing narrow shutter angles to reduce mo­tion blur and small apertures to maximise 
depth of .eld. Sharp, low noise images were essential for the geometry recovery to be effective. The 
initialstagein geometryreconstructionwasbuildingthe visual hull of the actors based on their silhouettes. 
The volume is divided into a gridand voxels lying outside the silhouette of the performers as viewed 
from one or more cameras are carved away. The next stage involved using the image texture data to form 
the detailed surface. Inspiredby methods publishedby stereovision re­search groups (see presentation 
slides for references) the visual hull was representedasalevelset withinavolumetricgrid. Thisallowed 
us to evolve the surface without getting involved in the complexity of a mesh with changing topology. 
We developed our own error function based on optical .ow data computed from images repro­jected via the 
surface. The interface was evolved along the error function gradient in an iterative process until the 
reprojection error was minimized. The resulting surface, a complete and closed mesh, was typically accurate 
to better than 1cm, was low in noise and clutter and suit­able for texturing by image projection from 
the witness camera ar­ray. Since texturing the surface was done with photographic data the modelwas immediately 
photorealistic without requiring sophis­ticated material and lighting shaders. Basic lighting effects 
consis­tent with direct sunlight could be then synthesised and added and the scene could be rendered 
from a virtual camera move designed in post production to meet the needs of the action sequence. 3 Conclusion 
Dense stereography has been successfully demonstrated as an ef­fective method for recording the foreground 
surface geometry of a dynamic scene. The geometry is suf.ciently accurate to allow a photoreal reconstruction 
of the scene from novel viewpoints and under synthetic lighting conditions. Acknowledgements Dr. Colin 
Davidson contributed to this project with invaluable ad­vice on stereographic photography and provided 
the optical .ow libraries used in the system. 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1598018</article_id>
		<sort_key>280</sort_key>
		<display_label>Article No.</display_label>
		<display_no>28</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>28</seq_no>
		<title><![CDATA[2D and 3D facial correspondences via photometric alignment]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1597990.1598018</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1598018</url>
		<abstract>
			<par><![CDATA[<p>Capturing facial geometry that is high-resolution, yet easy to animate, remains a difficult challenge. While a single scanned geometry may be straightforward to animate smoothly, it may not always yield realistic fine scale detail when deformed into different facial expressions. Combining scans of multiple facial expressions, however, is only practical if geometrical correspondences between the different scanned expressions are available. Correspondences obtained based on locations of facial landmarks or of placed markers are often sparse, especially compared to fine-scale structures such as individual skin pores. The resulting misalignment of fine detail can introduce artifacts or blur out details we wish to preserve.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.5</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>A.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061.10010063</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures->Computational geometry</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P1617664</person_id>
				<author_profile_id><![CDATA[81448592774]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Cyrus]]></first_name>
				<middle_name><![CDATA[A.]]></middle_name>
				<last_name><![CDATA[Wilson]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Southern California]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617665</person_id>
				<author_profile_id><![CDATA[81385599077]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Abhijeet]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ghosh]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Southern California]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617666</person_id>
				<author_profile_id><![CDATA[81100016992]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Pieter]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Peers]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Southern California]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617667</person_id>
				<author_profile_id><![CDATA[81455605851]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Jen-Yuan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Chiang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Southern California]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617668</person_id>
				<author_profile_id><![CDATA[81440600975]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Jay]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Busch]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Southern California]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617669</person_id>
				<author_profile_id><![CDATA[81100086933]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Paul]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Debevec]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Southern California]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>2383873</ref_obj_id>
				<ref_obj_pid>2383847</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Ma, W.-C., Hawkins, T., Peers, P., Chabert, C.-F., Weiss, M., and Debevec, P. 2007. Rapid acquisition of specular and diffuse normal maps from polarized spherical gradient illumination. In <i>Rendering Techniques</i>, 183--194.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 2D and 3D Facial Correspondences via Photometric Alignment University of Southern California, Institute 
for Creative Technologies* Figure 1: Correspondences between 3D geometries: Facial expression geometries 
parameterized in a common 2D domain are blended to produce intermediate 3D geometries. 1 Introduction 
Capturing facial geometry that is high-resolution, yet easy to ani­mate, remains a dif.cult challenge. 
While a single scanned geome­try may be straightforward to animate smoothly, it may not always yield 
realistic .ne scale detail when deformed into different facial expressions. Combining scans of multiple 
facial expressions, how­ever, is only practical if geometrical correspondences between the different 
scanned expressions are available. Correspondences ob­tained based on locations of facial landmarks or 
of placed markers are often sparse, especially compared to .ne-scale structures such as individual skin 
pores. The resulting misalignment of .ne detail can introduce artifacts or blur out details we wish to 
preserve. We obtain dense correspondences by computing optical .ow on albedo images and photometric normal 
maps simultaneously, a pro­cess which we refer to here as photometric alignment. We .rst compute 3D geometry 
for each facial expression by combining stereo correspondences with photometric normal information. We 
then compute short-range correspondences between a given expres­sion and the neutral expression to produce 
a common UV parame­terization for all expressions. This consistent parameterization en­ables blends between 
geometries which preserve .ne detail while maintaining temporal coherence. 2 Method Our setup consists 
of a sphere of LEDs and a stereo pair of cameras. For each facial expression we capture an albedo image 
as well as photographs under a set of spherical gradient illumination patterns, similar to those used 
by [Ma et al. 2007], for computing a photo­metric normal map. However, unlike [Ma et al. 2007] we do 
not use structured illumination patterns. We compute coarse-scale 3D geometry using multi-view stereo. 
We perform optical .ow measurement, constrained to epipolar lines, *e-mail: {wilson, ghosh, debevec}@ict.usc.edu 
between the albedo images and normal maps of the two camera views. The disparity map obtained from the 
optical .ow correspon­dences yields a coarse 3D geometry, on which we emboss photo­metric normals to 
produce high-resolution geometry. We represent this geometry as a 3D point cloud: a 3D coordinate for 
each 2D coordinate in the UV domain of the primary camera. This 2D to 3D map, together with the albedo 
and photometric normal map, share the same 2D UV parameterization. We compute photometric alignment between 
a captured facial ex­pression (albedo and photometric normals) and the captured neutral expression. We 
use the correspondences to warp the non-neutral maps in 2D to align with the neutral maps. This photometric 
align­ment constitutes a reparameterization of the non-neutral geometry into the 2D UV domain of the 
neutral expression. 3 Results The product of our method is a set of geometries with associated albedos 
and normal maps for which corresponding points on the 3D geometries of different facial expressions are 
located at a com­mon 2D coordinate in the UV parameterization. This facilitates manipulations which combine 
information from multiple expres­sions, while coherently maintaining .ne detail. Here we demon­strate 
such a manipulation: a morph between facial expressions by simply calculating a linear weighted blend 
of the 2D-parameterized geometries (Figure 1, Supplemental Video). In this case the blend weights are 
spatially uniform; but by introducing weight variation with location in the 2D domain, this becomes a 
starting point for more sophisticated blend shape applications. References MA, W.-C., HAWKINS, T., PEERS, 
P., CHABERT, C.-F., WEISS, M., AND DEBEVEC, P. 2007. Rapid acquisition of specular and diffuse normal 
maps from polarized spherical gradient illumina­tion. In Rendering Techniques, 183 194. Copyright is 
held by the author / owner(s). SIGGRAPH 2009, New Orleans, Louisiana, August 3 7, 2009. ISBN 978-1-60558-726-4/09/0008 
  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1598019</article_id>
		<sort_key>290</sort_key>
		<display_label>Article No.</display_label>
		<display_no>29</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>29</seq_no>
		<title><![CDATA[ILM's multitrack]]></title>
		<subtitle><![CDATA[a new visual tracking framework for high-end VFX production]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1597990.1598019</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1598019</url>
		<abstract>
			<par><![CDATA[<p>Tracking 2D features on film footage is the starting point for several applications in a VFX pipeline such as camera calibration, match-moving, photomodeling, vision-based motion capture and object tracking. The diversity of captured footage and the accuracy requirements make the feature tracking problem very challenging. For instance, typical background footage exhibits drastic changes in lighting, motion blur, occlusions, and is usually corrupted with environment effects such as smoke or explosions. Tracking features on hero characters such as a human faces is equally challenging, especially near the eyes and lips, where the textures change continuously.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.4.8</cat_node>
				<descriptor>Tracking</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>A.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245.10010253</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems->Tracking</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010225.10010227</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision tasks->Scene understanding</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P1617670</person_id>
				<author_profile_id><![CDATA[81100027036]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Christoph]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bregler]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Industrial Light + Magic]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617671</person_id>
				<author_profile_id><![CDATA[82458976857]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Kiran]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bhat]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Industrial Light + Magic]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617672</person_id>
				<author_profile_id><![CDATA[81100418072]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Jeff]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Saltzman]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Industrial Light + Magic]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617673</person_id>
				<author_profile_id><![CDATA[81545796656]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Brett]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Allen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Industrial Light + Magic]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>964604</ref_obj_id>
				<ref_obj_pid>964568</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Baker, S., and Matthews, I. 2004. Lucas-kanade 20 years on: A unifying framework. <i>International Journal of Computer Vision 56, 3</i> (February), 221--255.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 ILM s Multitrack: A new visual tracking framework for high-end VFX production Christoph Bregler Kiran 
Bhat Jeff Saltzman Brett Allen Industrial Light + Magic 1 Introduction Tracking 2D features on .lm footage 
is the starting point for several applications in a VFX pipeline such as camera calibration, match­moving, 
photomodeling, vision-based motion capture and object tracking. The diversity of captured footage and 
the accuracy re­quirements make the feature tracking problem very challenging. For instance, typical 
background footage exhibits drastic changes in lighting, motion blur, occlusions, and is usually corrupted 
with environment effects such as smoke or explosions. Tracking features on hero characters such as a 
human faces is equally challenging, especially near the eyes and lips, where the textures change contin­uously. 
Recent commercial packages for tracking provide automatic ap­proaches for identifying and tracking markers 
through a sequence. These tools are very powerful in some situations, but have trouble with strong temporal 
discontinuities such as occlusions or lighting .ashes. Unfortunately, these outlier situations are very 
common in VFX work. A different and popular class of trackers are the single­marker systems, based on 
fast template matching algorithms. They typically have a sophisticated user interface that provide full 
man­ual controls to assist and guide the trackers through complex shots. However, because single-marker 
systems focus on individual fea­tures and not regions, they have dif.culty handling large-scale im­age 
motions such as scaling, rotation, blurring and large deforma­tions of the image regions. ILM s Multitrack 
system extends the single-marker system to simultaneously track a group of markers that de.ne a region 
of texture. The tracker internally maintains an adaptable texture and shape model which deforms and adapts 
its appearance to track the footage. The adaptable template is learned from a subset of exam­ple frames 
in the sequence that are speci.ed interactively by a user. Tracking with an adaptable template allows 
us to track through mo­tion blur, and handle drastic changes to the size, orientation, light­ing, and 
complex texture variations. 2 Algorithm Details The Multitrack algorithm has two stages training step 
to learn a shot-speci.c adaptable template, and the tracking step. The user sets up the training stage 
by labeling feature markers in few exam­ple frames for the shot (.g. 1). Multitrack generates a sequence 
of in-between morphs from the example templates, which are subse­quently used to train a shot speci.c 
PCA based model. The PCA model covers the texture and shape variation inside an image mask de.ned by 
the example markers. The tracking stage is implemented as a multi-pass search that best adapts the PCA 
texture and shape model to the current frame. The multi-pass search starts out with a global normalized 
cross-correlation based .tting, and then itera­tively .ts the best af.ne shape-deformation (a combination 
of af.ne and basis shape deformations), and the best linear combination of the texture bases using a 
coarse-to-.ne Newton Step optimiza­tion. Our work is related to recent extensions of Lucas-Kanade based 
region tracking and Active Appearance Models (AAM) ap­proaches, see [Baker and Matthews 2004] and their 
tech reports for an overview. Several other heuristics have been of great impor­tance, like the integration 
of contrast normalization, and automatic fall-back to af.ne-only and simpler template based tracking 
if there is only one reference chosen. (a) (b) (c) (d)  Figure 1: Tracking three markers around the 
eye. The user man­ually places the markers on four reference frames (a) that cover a range of lighting, 
eyelid state, and motion blur. Multitrack aligns the reference images, generates a set of morph images, 
and then calculates a mean image and .ve PCA components (b). We can now track the markers on new frames 
(c). Multitrack optimizes over an af.ne transformation and a set of PCA weights to construct a pat­tern 
(d) that matches the new frame. 3 Discussion The main requirements for Multitrack from ILM s production 
in­cluded high quality tracks, fast tracking speeds, and high degree of user control. To achieve these 
requirements, we came up with several innovations to the basic algorithm. For example, we have implemented 
a fast on-the-.y PCA algorithm for adding new exam­ples to the training set and a method for fast computation 
of normal­ized cross-correlation of non-rectangular regions. Additionally, we extended our existing single-marker 
UI to handle multiple markers and examples. This allows users experienced with our current tools to seamlessly 
transition to Multitrack, and fall back to our single­marker framework when necessary. The uni.ed multitrack 
UI in­cludes several tools to edit examples frames (add, delete, etc) and interactively modify the tracking 
region by choosing a subset of ex­ample markers. These tools, in conjuntion with the interactive PCA 
solver enables the artist to track through tricky production shots. References BAKER, S., AND MATTHEWS, 
I. 2004. Lucas-kanade 20 years on: A unifying framework. International Journal of Computer Vision 56, 
3 (February), 221 255. Copyright is held by the author / owner(s). SIGGRAPH 2009, New Orleans, Louisiana, 
August 3 7, 2009. ISBN 978-1-60558-726-4/09/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1598020</article_id>
		<sort_key>300</sort_key>
		<display_label>Article No.</display_label>
		<display_no>30</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>30</seq_no>
		<title><![CDATA[Estimating specular roughness from polarized second order spherical gradient illumination]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1597990.1598020</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1598020</url>
		<abstract>
			<par><![CDATA[<p>Measurement of spatially varying BRDFs of real world materials has been an active area of research in computer graphics with image-based measurements being the preferred approach in practice. In order to restrict the total number of measurements, existing techniques typically trade spatial variation for angular variation of the surface BRDF [Marschner et al. 1999]. Recently, Ma et al. [2007] introduced a technique for estimating high quality specular normals and albedo (Fig. 1, (a) & (b) respectively) of a specular object using polarized first order spherical gradient illumination conditions. In this work, we extend this technique to estimate per pixel specular <i>roughness</i> using polarized <i>second order</i> spherical gradients as a measure of the <i>variance</i> about the mean (reflection vector). We demonstrate that for isotropic BRDFs, only three spherical gradient illumination patterns related to the second order spherical harmonics are sufficient for a robust estimate of per pixel specular roughness (Fig. 1, (c)). Thus, we go further than previous work on image-based measurement of specular BRDFs that typically obtain sparse estimates of the spatial variation. Our technique also provides a direct estimate of the per pixel specular roughness and hence has the added advantage of not requiring any off-line numerical optimization that is typical of the measure and fit approach to BRDF modeling.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Raytracing</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>A.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372.10010374</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering->Ray tracing</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P1617674</person_id>
				<author_profile_id><![CDATA[81385599077]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Abhijeet]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ghosh]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[USC Institute for Creative Technologies]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617675</person_id>
				<author_profile_id><![CDATA[81442595190]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Tongbo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Chen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[USC Institute for Creative Technologies]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617676</person_id>
				<author_profile_id><![CDATA[81100016992]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Pieter]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Peers]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[USC Institute for Creative Technologies]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617677</person_id>
				<author_profile_id><![CDATA[81448592774]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Cyrus]]></first_name>
				<middle_name><![CDATA[A.]]></middle_name>
				<last_name><![CDATA[Wilson]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[USC Institute for Creative Technologies]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617678</person_id>
				<author_profile_id><![CDATA[81100086933]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Paul]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Debevec]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[USC Institute for Creative Technologies]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>2383873</ref_obj_id>
				<ref_obj_pid>2383847</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Ma, W.-C., Hawkins, T., Peers, P., Chabert, C.-F., Weiss, M., and Debevec, P. 2007. Rapid acquisition of specular and diffuse normal maps from polarized spherical gradient illumination. In <i>Rendering Techniques</i>, 183--194.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2383829</ref_obj_id>
				<ref_obj_pid>2383815</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Marschner, S. R., Westin, S. H., Lafortune, E. P. F., Torrance, K. E., and Greenberg, D. P. 1999. Image-based BRDF measurement including human skin. In <i>Eurographics Rendering Workshop 1999</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1598021</article_id>
		<sort_key>310</sort_key>
		<display_label>Article No.</display_label>
		<display_no>31</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>31</seq_no>
		<title><![CDATA[Computational thinking through programming and algorithmic art]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1597990.1598021</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1598021</url>
		<abstract>
			<par><![CDATA[<p>General education students can be taught computational thinking skills through courses that marry computer programming with algorithmic art. Algorithmic art is a varied and growing field where images are generated on the computer using mathematical and computer algorithms (see http://processing.org/exhibition/index.html for examples). Visually oriented students may be motivated to learn programming when it is taught in a context that is as much focused on art, artists, and design principles as it is on mathematics and programming. This presentation will include 1) a discussion of the challenges of teaching programming, 2) the role of teaching style, motivation, and programming environment, 3) a summary of common algorithms and their relation to design principles, and 4) examples of artwork that has been created by current artists.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>A.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P1617679</person_id>
				<author_profile_id><![CDATA[81100436441]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Genevieve]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Orr]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Willamette University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1598022</article_id>
		<sort_key>320</sort_key>
		<display_label>Article No.</display_label>
		<display_no>32</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>32</seq_no>
		<title><![CDATA[Universal panoramas]]></title>
		<subtitle><![CDATA[narrative, interactive panoramic universes on the internet]]></subtitle>
		<page_from>1</page_from>
		<page_to>2</page_to>
		<doi_number>10.1145/1597990.1598022</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1598022</url>
		<abstract>
			<par><![CDATA[<p>Universal Panoramas borrow the idea of Universal Design and accessibility from architecture. Humans navigate and understand the world by moving through and looking at space, yet the Internet is explored by browsing flat, book-like pages. The panoramic based approach for navigating and understanding information is an attempt to create a more natural way of exploring web spaces bound by the limitations of the hardware and software of today.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Virtual reality</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>A.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010866</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Virtual reality</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P1617680</person_id>
				<author_profile_id><![CDATA[81442608425]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Kim]]></first_name>
				<middle_name><![CDATA[Baumann]]></middle_name>
				<last_name><![CDATA[Larsen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Placebo Effects]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 SIGGRAPH 2009 Conference, New Orleans 3 - 7 August 2009 Universal Panoramas: narrative, interactive 
panoramic universes on the Internet Author: Kim Baumann Larsen (Placebo Effects) Universal Panoramas 
is an idea and an emerging toolset for creating narrative interactive universes on the Internet by combining 
panoramic photography and CG, sound and live action video and animation into coherent, immersive wholes 
for use in interactive entertainment, architecture visualization, and corporate presentations amongst 
others. Universal Panoramas borrow the idea of Universal Design and accessibility from architecture. 
Humans navigate and understand the world by moving through and looking at space, yet the Internet is 
explored by browsing flat, book-like pages. The panoramic based approach for navigating and understanding 
information is an attempt to create a more natural way of exploring web spaces bound by the limitations 
of the hardware and software of today. Universal Panoramas is based on Flash; the engine used is by krpano 
Gesellschaft mbH, and film and VFX techniques and conventions and it is inspired by the phenomenon Synesthesia. 
It is by putting these together into a coherent whole that something new arises. Using still images and 
Flash videos and animations with an alpha channel it is possible to create composite panoramas with real-time 
animated or live action elements. These can be placed in 3D panoramic space, have sound and react to 
user interaction. This creates a new level of immersion and engagement in panoramas and allows storytelling 
to take place. This approach opens up new arenas for the film and visual effects industry, for architects, 
designers and photographers to create web based services and presentations that work together with and 
reuse digital assets from films, architecture, commercials and games. Henry Jenkins says: World building 
has become the cornerstone of this new transmedia aesthetic, placing greater emphasis on art direction 
as a key component of the immersive narrative experience and requiring greater flexibility in conceiving 
a story which will play itself out effectively across many media platforms. (5D: The Future of Immersive 
Design Conference, Long Beach, October 4-5, 2008) This talk will show limited demonstrations of some 
of the application areas including architecture and entertainment, as well as speculate on the uses in 
other areas such as e­learning, fashion and commercials. A complete break down of the design and construction 
process of creating an Universal Panorama will give anyone with a basic understanding of a 3d modeling 
and rendering application, Flash, HTML and XML the ability to create their own interactive panoramic 
universes. C o p yright is he ld b y the au tho r / o w ne r(s ). S IG G R A P H 2009 , N e w Orle ans 
, L o u is iana , A u gu s t 3 7 , 200 9 . 1 of 2 IS B N 978 - 1 - 60558 - 726 - 4 /0 9 /0008 SIGGRAPH 
2009 Conference, New Orleans 3 - 7 August 2009 The author would like to recognize the work of Klaus Reinfeld, 
creator of krpano, the innovation in panoramic imaging by Eric Hanson and Greg Downing of xRez Studio 
and the use of hybrid panoramas in Speed Racer by John Gaeta et al. About the author: Kim Baumann Larsen 
is a Founding Partner and Creative Director at Placebo Effects, a design and visual effects company in 
Oslo, Norway. Kim has worked on some of Norway s largest architectural projects including the Oslo Gardermoen 
Airport, Telenor Fornebu and the new borough Bjorvika in Oslo. He is involved in creating and speaking 
at conferences world wide investigating the future of design and visual effects. His work has been published 
in numerous architecture books, featured in exhibits, and he did visual effects for an Emmy­nominated 
TV documentary on Apollo 13. Kim is a founding advisory board member of the CGSociety.org and member 
of the 5D Founding Committee. 2 of 2 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1598023</article_id>
		<sort_key>330</sort_key>
		<display_label>Article No.</display_label>
		<display_no>33</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>33</seq_no>
		<title><![CDATA[Genetic Stair]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1597990.1598023</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1598023</url>
		<abstract>
			<par><![CDATA[<p>Designed, fabricated and installed by a single team of architects and metalworkers, the Genetic Stair represents the culmination of a fully integrated generative design process. Digital design techniques formed the heart of the research and development process from the earliest conceptual stages, through performative analysis and onwards to fabrication.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>A.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P1617681</person_id>
				<author_profile_id><![CDATA[81442619431]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Nicholas]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Desbiens]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Caliper Studio]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 GENETIC STAIR Nicholas Desbiens Caliper Studio ndesbiens@caliperstudio.com  Figure 1: (a) Sectional 
cut of digital parametric model. (b) Screen capture of FEA software. (c) Laser cut structural pipe components. 
(d) Detail showing typical rod-to-tube connection and four-way miter joint. (e) (f) Final stair in space. 
Introduction Designed, fabricated and installed by a single team of architects and metalworkers, the 
Genetic Stair represents the culmination of a fully integrated generative design process. Digital design 
techniques formed the heart of the research and development process from the earliest conceptual stages, 
through performative analysis and onwards to fabrication. Design The design intent of the Genetic Stair 
was to arrive at configuration of simple elements that would be expressive of the complex set of forces 
at work in the structure. To that end, a cross-platform generative design program was developed that 
married the precision and adaptability of the 3D CAD environment Rhinoceros 4.0 (Rhino) with the analytic 
power finite element analysis (FEA) software. This program implemented a specialized genetic algorithm 
(GA) that integrated project-specific fabrication constraints into the generative process in Rhino while 
utilizing FEA to assess structural performance and assign evolutionary fitness to individual stair configurations. 
The first step in the design of the stair was the development of a script (in Rhino) to create the overall 
three-dimensional layout. This was essentially a function to set the stair s bounding volume based on 
variable input such as building code requirements, fabrication constraints and on-site measurements. 
The result was a three-dimensional parametric digital model with line elements representing all components 
excluding the diagonal rods used to stiffen the stair and distribute the forces within it. The layout 
of these connecting elements was the focus of the GA. To create the initial generation of stair configurations, 
rods were distributed in a random process that nonetheless required adherence to programmed fabrication 
constraints. The configurations in this population, while structurally entirely inadequate, still followed 
rules which allowed any of them to be built using predetermined construction methods. Once a population 
of three-dimensional configurations was created, it became the genetic material for the creation of a 
new generation. To this end, each stair configuration was imported from Rhino via custom-formatted text 
files into the FEA software. Then degrees of freedom were assigned to its nodal connections, units of 
force were applied to the nodes and specific material properties were assigned to the linear elements. 
Then the FEA model was solved and structural performance data was exported to form the means of assigning 
the probability of that individual s genetic information being passed on to the next generation. The 
final step in the cross-platform loop that formed the core of the GA was to create a new generation of 
configurations based on weighted selection of information from the previous population. Once two individuals 
were selected for reproduction, a crossover point was assigned randomly at some point along the length 
of the stair layout. Rods located below the crossover point on the first individual were added to rods 
located above the crossover point from the second individual to produce a new configuration that was 
a combination of the two. Further operations carried out on the new configuration included auditing for 
constructability and occasional statistically-determined mutation functions. 2 Fabrication Fabrication 
methods were developed and tested concurrently with the design and development of the stair. In the end, 
though the generative design process was highly automated, its actual construction was a combination 
of digital and traditional techniques. The Genetic Stair is made of 48 unique stainless steel pipes with 
1400 holes and 253 connecting steel rods cut to length. Using five-axis CNC laser cutting equipment, 
these holes could be cut with enough precision in both position and angle to allow the pipes themselves 
to act as a kind of three-dimensional jig for assembling the stair. One quickly identified limitation 
of laser cutting technology was the relatively small range of incidence angles (< 15°) that can be precisely 
cut into polished stainless steel due to refraction of the beam. This constraint led to the development 
of highly specialized design and detailing solutions, such as fabrication-conscious generative processes 
and hybrid digital/analog templating procedures. Though the laser cut pipes themselves were fabricated 
with a high degree of dimensional and angular accuracy, the way in which these components were joined 
one to the other (structural welding) was significantly less precise. To bridge the gap between the precision 
of digitally fabricated smart components and traditional metal shop fabrication techniques, custom jigs 
were made using additional digital equipment such as a CNC router, on the high end, and a standard office 
laser printer on the low. 3 Conclusion The result of this process of design, analysis, and fabrication 
is a complex yet understandable architectural experience in steel and glass in which every component 
works together as a unified whole to push and pull the stair upward through 270 degrees with no intermediary 
supports. Copyright is held by the author / owner(s). SIGGRAPH 2009, New Orleans, Louisiana, August 3 
7, 2009. ISBN 978-1-60558-726-4/09/0008   
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1598024</article_id>
		<sort_key>340</sort_key>
		<display_label>Article No.</display_label>
		<display_no>34</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>34</seq_no>
		<title><![CDATA[Visual Zen art]]></title>
		<subtitle><![CDATA[aesthetic cognitive dissonance in Japanese dry stone garden measured in visual PageRank]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1597990.1598024</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1598024</url>
		<categories>
			<primary_category>
				<cat_node>I.4.0</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>A.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010383</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Image processing</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P1617682</person_id>
				<author_profile_id><![CDATA[81539921056]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[D.]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Cai]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617683</person_id>
				<author_profile_id><![CDATA[81442615731]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[S.]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Mochiuki]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617684</person_id>
				<author_profile_id><![CDATA[81442595366]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Y.]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617685</person_id>
				<author_profile_id><![CDATA[81442595758]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[N.]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Asai]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617686</person_id>
				<author_profile_id><![CDATA[81442598649]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[A.]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Fukumoto]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[fr1 GABAIX, X. 1999. Zipf's Law For Cities: An Explanation. <i>Quarterly Journal of Economics 114</i>, 739-767.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[fr2 PAGE, L., BRIN, S., MOTWANI, R. AND WINOGRAD, T. 1998. The pagerank citation ranking: Bringing order to the web.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[fr3 VOSS, R. 1985. Evolution of long-range fractal correlations and 1/f noise in DNA base sequences. <i>Commun. ACM Phys Rev Lett 68</i>, 3805.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Visual Zen Art: Aesthetic Cognitive Dissonance in Japanese Dry Stone Garden Measured in Visual PageRank 
 D. Cai, S. Mochiuki, Y. Wang, N. Asai, and A. Fukumoto, One of the most famous Japanese landscape of 
drystone Zen gardens in Ryoan temple established AC1450 at Kyoto by the deputy Shogun Katsumoto Hosokawa, 
a UNESCO World Heritage Site, has thesmall 248 m2 empty rectangle where white sand waslaid in between, 
in the abstract, placements of 15 rocks with mosses that seem to be scattered in seemingly haphazard. 
The landscape attracts many visitors, including Queen Elizabeth II, 1975, to hundreds of thousands for 
long years. The stone placements designed by the ancient anonymouslandscape designer look random at a 
glance, however,the placement structure is hidden in white sand of thegarden space in sophisticated manner, 
and that we arein perfect harmony with the temple buildings and landscapes. The stone structure looks 
puzzling at glance. However, the general stone placement structure can immediately be revealed byunderstanding 
the ancient garden design rule textsdescribed in, for example, Sakuteiki ( Landscape Design in Japanese, 
published around AC 1200). According to this instruction text, the stones are placed recursively and 
in fractal as an obtuse inequilateral triangle in different three level scales.The three stones are placed 
as vertices entitled . (very formal) at the obtuse angle, .(formal) at the vertex closest to very formal 
, and .(Casual) at theremaining vertex. These three stones form a clusterand three clusters form a different 
obtuse inequilateral triangle, recursively, as shown in Image(lower). If we plot the size and the rank 
of the three level obtuse triangles in log scale, we obtain the perfect Zipf s law[Gabaix 1999] or 1/f 
pink noise[Voss 1985]. This means that the stones are placed in a perfect fractal manner, and this fractal 
stone placement is completely understood by the garden viewers unconsciously in our eyetracking measurements 
shown in Image (upper). Eye tracking experiments are performed and the visual PageRank [Page et al. 1998] 
of eye movement are measured. We measured 10 testers eye movementwhile they were watching the landscape 
garden. Alltesters spend more than 90% of their time to watch thestone objects, and move their eyes from 
a stone vertexto another, following the triangles, recursively. Image(upper) shows a typical eye movement 
trajectory in 20seconds, and the trajectory follows the three level offractal triangle structure displayed 
in Image (lower).Five human figures 1 to 5 in different colors representdifferent viewing positions corresponding 
to different eye movement trajectories in their colors. We assume that the eye movement from one stone 
or one cluster toanother is a forward link . Using the same technique as PageRank , the eye movement 
from one stone toanother is taken as a directed graph. First, we generate an adjacency matrix of this 
graph structurewith weighted sum of the number of eye visits. Next, we obtain the largest absolute eigenvalue 
of this matrix and its eigenvector. Third, the normalized component values of the eigenvector are the 
visual PageRanks , and now we rank the stones or the cluster in order. The visual PageRank in the Japanese 
dry stone Zen garden reveals the amazinghidden structure in this old landscape garden, whichcannot be 
discovered by a simple hot spot diagram thatonly represents the length of fixation time. Figure 1 indicates 
that the PageRanks in two . (veryformal) stones (number 2-2 and 3-2) are significantlyhigher than the 
others, where the general fractalplacement manner is somehow violated. At these stones circled in red 
in Image, the eye trajectories arestrongly disturbed and some psychological tension caused by a disparity 
between what one expects to seeand what one actually sees is observed. This is a well-known phenomenon 
in social psychology todaycalled cognitive dissonance or visual dissonance although totally not known 
in 560 years ago when thetemple was build. The cognitive dissonance happenswhen we perceive a discrepancy 
between our attitudesand our behaviors. Our eye sees the world of art with athousand of expectations 
based on our personalitiesand our cognitive structure or knowledge system.Sometimes those expectations 
are fulfilled, sometimesnot. In the case of unfulfilled expectations, the vieweris required to resolve 
his or her tension, or simplyabandon the piece and consider another. An important part of human motivation 
is found in dissonance reduction , in that people do not normally choose tolive in a state of psychological 
tension. In psychologicalterms, it is better to be avoided or resolved such an aversive state. The technique 
to produce unexpectedvisual forms is widely practiced by modern artists whoseek to gain our attentions. 
However, the most strikingfeature here is that this cognitive dissonance was implemented in a very naïve 
way and viewer observethis only unconsciously, not like many visual dissonance in modern arts. When people 
view thisgarden people feels deep serenity and perfect harmonywith very small disharmony i. e. dissonance 
like inmuch beautiful music.  Figure 1: Visual PageRanks and eye movements transition possibilities. 
Here 1 is formal , 2 is very formal , 3 is casual , 1-1 is formal in formal triangle etc. This eye movement 
transition diagram shows the second level of fractal recursion. GABAIX, X. 1999. Zipf's Law For Cities: 
An Explanation*. Quarterly Journal of Economics 114, 739-767. PAGE, L., BRIN, S., MOTWANI, R. AND WINOGRAD, 
T. 1998. The pagerank citation ranking: Bringing order to the web. VOSS, R. 1985. Evolution of long-range 
fractal correlations and 1/f noise in DNA base sequences. Commun. ACM Phys Rev Lett 68, 3805. Copyright 
is held by the author / owner(s). SIGGRAPH 2009, New Orleans, Louisiana, August 3 7, 2009. ISBN 978-1-60558-726-4/09/0008 
 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1598025</article_id>
		<sort_key>350</sort_key>
		<display_label>Article No.</display_label>
		<display_no>35</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>35</seq_no>
		<title><![CDATA[The blues machine]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1597990.1598025</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1598025</url>
		<abstract>
			<par><![CDATA[<p>Hardware based musical instruments are, in general, from the performer point of view, merely copies of real physical instruments. They do not provide facilities for being played, especially for musically untrained people.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>A.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P1617687</person_id>
				<author_profile_id><![CDATA[81466640832]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[M.]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Cicconet]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[IMPA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617688</person_id>
				<author_profile_id><![CDATA[81442602956]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[I.]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Paterman]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[IMPA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617689</person_id>
				<author_profile_id><![CDATA[81388598371]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[P.]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Carvalho]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[IMPA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617690</person_id>
				<author_profile_id><![CDATA[81442611891]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[L.]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Velho]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[IMPA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1598026</article_id>
		<sort_key>360</sort_key>
		<display_label>Article No.</display_label>
		<display_no>36</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>36</seq_no>
		<title><![CDATA[Computer-mediated performance and extended instrument design]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1597990.1598026</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1598026</url>
		<abstract>
			<par><![CDATA[<p>This talk will explore conceptual and technical approaches to computer-mediated improvisational performance. Many artists are pursuing a musical practice that combines the roles of composer, system/instrument designer, and performer/improviser. In addition to integrating these often overlapping roles, it is also possible to use computation and interactive systems to refashion the very nature of improvisational performance practice. Many improvisers feel that their musical intuition and ideas are more developed than their physical skills or instrumental technique. This leads to the idealistic notion that somehow, we should be able to focus our energy on the higher-level conceptual aspects of improvisational music/sound making, and use the available technology to reduce some of the physical constraints, steer the larger formal progression, and manage the low-level necessities.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>A.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P1617691</person_id>
				<author_profile_id><![CDATA[81319489947]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Thomas]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ciufo]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1085159</ref_obj_id>
				<ref_obj_pid>1085152</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Bahn, C., and Trueman, D. 2001. Interface: Electronic Chamber Ensemble. In <i>New Interfaces for Musical Expression at the CHI2001 Conference for Human/Computer Interaction</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Ciufo, T. 2005. Beginner's Mind: an Environment for Sonic Improvisation. In <i>Proceedings of the International Computer Music Conference</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Ciufo, T., and Birchfield, D. 2007. Computer-Mediated / Interactive Performance: Moving Boundary Problem - Case Study. In <i>Enaction in Arts Symposium proceedings</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Computer-Mediated Performance and Extended Instrument Design Thomas Ciufo tc@ciufo.org 1 Overview This 
talk will explore conceptual and technical approaches tocomputer-mediated improvisational performance. 
Many artistsare pursuing a musical practice that combines the roles ofcomposer, system/instrument designer, 
and performer/improviser. In addition to integrating these often overlapping roles, it is alsopossible 
to use computation and interactive systems to refashionthe very nature of improvisational performance 
practice. Manyimprovisers feel that their musical intuition and ideas are moredeveloped than their physical 
skills or instrumental technique.This leads to the idealistic notion that somehow, we should be able 
to focus our energy on the higher-level conceptual aspects of improvisational music/sound making, and 
use the availabletechnology to reduce some of the physical constraints, steer thelarger formal progression, 
and manage the low-level necessities. This is not laziness or naive 'better living through technology'rhetoric. 
The history of instrument building and musicalperformance has followed a path of developing better, more 
expressive, easier to play instruments using availabletechnologies. In acoustic instrument design, this 
has been arelatively slow evolution and is usually a re.nement of an existinginstrument or performance 
practice, rather than a total paradigmshift. I am following this evolution using computer technology, 
but the opportunity to radically rede.ne the composer-improviser­instrument relationship is unprecedented. 
This involves designingperformance systems that re.ect our own aesthetic and conceptualorientation towards 
music making, and encoding in the software/hardware the features and functionality that are important 
to us ascomposer-improvisers. One clear advantage of designing systemsfor our own use is the opportunity 
to design for our particularstrengths, and around certain weaknesses.  Copyright is held by the author 
/ owner(s). SIGGRAPH 2009, New Orleans, Louisiana, August 3 7, 2009. ISBN 978-1-60558-726-4/09/0008 
 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1598027</article_id>
		<sort_key>370</sort_key>
		<display_label>Article No.</display_label>
		<display_no>37</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>37</seq_no>
		<title><![CDATA[InTune]]></title>
		<subtitle><![CDATA[a musician's intonation visualization system]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1597990.1598027</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1598027</url>
		<abstract>
			<par><![CDATA[<p>A beloved music teacher taught us to embrace the "face the music" approach, in which we learn to hear ourselves as others hear us by listening to recordings. We apply the "face the music" approach to the practice of intonation --- the precise tuning of frequencies corresponding to different musical pitches. While good intonation, "playing in tune," is often neglected in the earliest years of musical practice, it is as essential a part of technique as the playing of fast notes or the control of emphasis. Intonation is also central to what some see as the <i>illusion</i> of tonal beauty --- that is, for a sound to be beautiful it must commit itself clearly to the "correct" pitch. We introduce a system that allows musicians to visualize pitch in ways that leverage the centuries-long tradition of music notation, and are intuitive to the non-scientist.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.8</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>A.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010507</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Graphics recognition and interpretation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P1617692</person_id>
				<author_profile_id><![CDATA[81470651641]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Kyung]]></first_name>
				<middle_name><![CDATA[Ae]]></middle_name>
				<last_name><![CDATA[Lim]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Indiana Univ.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617693</person_id>
				<author_profile_id><![CDATA[81100459128]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Christopher]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Raphael]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Indiana Univ.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 InTune: A Musician s Intonation Visualization System Kyung Ae Lim* Christopher Raphael Indiana Univ. 
 Figure 1: InTune s three displays of a performance. The score image (left) colors the heads of suspicious 
notes. The pitch pitch trace (upperright) shows precise pitch evolving over time. The spectrogram (lower 
right) is the traditional display of frequency content evolving overtime. The vertical lines show the 
measure or beat boundaries (red), the note boundaries (white), and the current position (green). Figure 
2: Example showing the rising pitch tendency, .rst madeclear to the player by the program. 1 Overview 
A beloved music teacher taught us to embrace the face the music approach, in which we learn to hear ourselves 
as others hear us bylistening to recordings. We apply the face the music approachto the practice of intonation 
 the precise tuning of frequenciescorresponding to different musical pitches. While good intonation, 
playing in tune, is often neglected in the earliest years of musicalpractice, it is as essential a part 
of technique as the playing of fastnotes or the control of emphasis. Intonation is also central to whatsome 
see as the illusion of tonal beauty that is, for a sound tobe beautiful it must commit itself clearly 
to the correct pitch. Weintroduce a system that allows musicians to visualize pitch in ways * kalim@indinaa.edu 
craphael@indiana.edu that leverage the centuries-long tradition of music notation, and areintuitive to 
the non-scientist. The electronic tuner is, without doubt, one of the most widely usedpractice tools 
for the classically-oriented musician, thus justifyingefforts to improve this tool. The tuner provides 
an objective mea­surement of the pitch or frequency with which a musician playsa note, which can be judged 
in relation to some standard of cor­rectness (say equal tempered tuning at A=440 Hz.). Though thetuner 
has been embraced by a large contingent of performing musi­cians, it does have its weaknesses, as follows. 
The tuner gives onlyreal-time feedback, requiring the user to synthesize its output as itis generated. 
The tuner takes time to respond to each individualnote, making it nearly impossible to get useful feedback 
with onlymoderately fast notes. The tuner cannot handle simultaneous notes,such as double stops this 
is actually part of the reason the tunerfails on fast notes, since past notes linger in the air, thus 
confusingthe instrument. Perhaps most signi.cantly, the tuner does not re­late its output through the 
usual conventions of notated music, thushiding tendencies and patterns that show themselves more clearly 
when presented as part of a musical score. Our program, InTune seeks to overcome these weaknesses by 
presenting its observationsin an intuitive and readily appreciated format. We present our system, InTune, 
describing the three different viewsof audio the program allows, as well as the backbone of score­following 
that distinguishes our approach from others. We considerother approaches to this problem and place ours 
appropriately inthis context. Finally we present a user study, giving reactions to our effort from a 
highly sophisticated collection of users. The program was developed in close consultation with Allen 
andHelga Winold, professors emeriti of music in the Jacobs School ofMusic at Indiana University, and 
is freely available for download athttp://www.music.informatics.indiana.edu/programs/InTune. Copyright 
is held by the author / owner(s). SIGGRAPH 2009, New Orleans, Louisiana, August 3 7, 2009. ISBN 978-1-60558-726-4/09/0008 
  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1598028</article_id>
		<sort_key>380</sort_key>
		<display_label>Article No.</display_label>
		<display_no>38</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>38</seq_no>
		<title><![CDATA[Concurrent monoscopic and stereoscopic animated film production]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1597990.1598028</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1598028</url>
		<abstract>
			<par><![CDATA[<p>The number of theater screens domestically that are equipped for digital 3D exhibition is currently only about one quarter of the total number that are reached by an animated feature film in wide release. Any such film could not ignore the aesthetic demands particular to 2D exhibition on a statistical basis alone. However, the cost and effort of producing a 3D version, despite the numerical disadvantage, might indicate the type of commitment to this burgeoning medium that would dictate putting out only the best 3D product. As it is not always practical to create two completely artistically divergent versions of a film, the manner in which a production navigates through the compromises between the two will determine the success of the results. In the face of this reality, the production pipeline for <i>Bolt</i> was designed with the goal of delivering the full artistic vision of the directors for the 2D film that the majority of filmgoers would see, yet deliver an uncompromising immersive experience to 3D audiences.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Animation</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>A.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P1617694</person_id>
				<author_profile_id><![CDATA[81442601792]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Robert]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Neuman]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Walt Disney Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Concurrent Monoscopic and Stereoscopic Animated Film Production Robert Neuman * Walt Disney Animation 
Studios 1. Introduction The number of theater screens domestically that are equipped for digital 3D1 
exhibition is currently only about one quarter of the total number that are reached by an animated feature 
film in wide release. Any such film could not ignore the aesthetic demands particular to 2D exhibition 
on a statistical basis alone. However, the cost and effort of producing a 3D version, despite the numerical 
disadvantage, might indicate the type of commitment to this burgeoning medium that would dictate putting 
out only the best 3D product. As it is not always practical to create two completely artistically divergent 
versions of a film, the manner in which a production navigates through the compromises between the two 
will determine the success of the results. In the face of this reality, the production pipeline for Bolt 
was designed with the goal of delivering the full artistic vision of the directors for the 2D film that 
the majority of filmgoers would see, yet deliver an uncompromising immersive experience to 3D audiences. 
 &#38;#169; Disney The left and right eye images of a frame from the 3D film.  2. 3D Philosophy and 
Techniques The philosophy for using 3D on Bolt was to optimize for both the immersive effect and the 
viewing comfort. To make Bolt immersive our goal was to create scenes with full, believable volume, and 
use depth as a storytelling tool. By using depth in this manner we hoped to draw the audience into the 
story, rather than pulling them out of it with cardboard like scenes and random 3D gimmicks. Following 
a depth script that mirrored the emotional intensity of the film, and adopting a grammar for the application 
of depth, allowed stereoscopic depth to be meaningful. This simultaneously optimized for comfort by regulating 
the amount of depth, reserving it for big moments in the story. Viewing comfort would also be maintained 
by adopting parallax limits, controlling the continuity of stereoscopic convergence across shots, and 
minimizing retinal rivalry. The use of a technique that we refer to as the floating window helped to 
reduce one of the main causes of retinal rivalry, the window violation . The window violation is the 
paradoxical effect that is created when an element that is being occluded by the vertical edge of the 
frameline, lies in front of it in depth. The result is a perceptual flattening of the image, as well 
as uncomfortable retinal rivalry due to part of the element only appearing in one eye. The floating window 
is a masking of the left and right eye images that is given a stereoscopic disparity in order to float 
it in front of the element that is breaking frame. This eliminates the window violation and removes the 
retinal rivalry. The floating window also helped us to use depth for storytelling by giving us a means 
of independently controlling the perceived location of the screen, allowing that to become part of our 
3D film grammar. 3. The 2D/3D Hybrid Pipeline The Bolt production pipeline was designed with the goal 
of producing a single version of the film in which the left eye image of the stereoscopic version would 
be identical to the monoscopic version of the film. This would obviously be the most efficient scenario, 
both in terms of storage and rendering requirements. However, acknowledging that an acceptable solution 
meeting the aesthetic requirements of both formats would always be possible, the pipeline would have 
to allow for certain shots to have a unique left eye image for the 3D film. Sequence based work, such 
as storyboarding, editorial and animatics, was done in 2D. Once this process resulted in final shot breaks 
being determined, the shots went to the layout department. From this point on, at the inception of each 
shot, the Bolt pipeline was fully stereoscopic. The exception was the initial 1K resolution lighting 
and compositing pass which was performed in 2D and then picked up by a render team, which created the 
final 2K renders. The render team was also responsible for correcting 3D artifacts arising from the renders 
as well as from 2D mattes and paint fixes used in the composite. 4. Bridging 2D and 3D Aesthetic Differences 
Let s examine a few aesthetic differences dealt with on Bolt. 4.1 Editorial Cutting Pace Creating two 
cuts of Bolt would quickly and irreparably thwart our goal of unified 2D and 3D films. Acknowledging 
a perceptual difference in how editorial pacing works in 3D, however, we took the approach of delivering 
the right 3D film for the cut, adjusting depth to make the pacing work. 4.2 Composition and Lens Choice 
By creating the cinematography for both formats at the same time, we were able to head off some camera 
choices that would have been disastrous for 3D, but for which an equally successful solution for 2D was 
available. In 2D cinematography there is often a compelling reason to use a longer lens. Long lenses 
however, will tend to cause the cardboard effect in 3D, in which a scene appears to be composed of flat 
objects arrayed in depth. Our pipeline allowed for such schisms in lens choice, through the use of a 
multi-rigging technique, in which individual elements in the shot are assigned to separate stereo bases, 
allowing the depth to be re­sculpted. 4.3 Depth of Field The creative use of depth of field has become 
an important part of the 2D film grammar, causing animated films such as Bolt to simulate this artifact 
of live action photography. In 3D, limited depth of field is not always as desirable, as the viewer s 
eyes which are free in terms of vergence to roam throughout the depth of the scene are not being afforded 
the same latitude in terms of focus. Fortunately on Bolt, the depth of field effect was not rendered 
in camera, but was introduced in the final composite, allowing it to be dialed down or removed for 3D. 
 5. Conclusion The 2D/3D hybrid production pipeline utilized on Bolt allowed the efficient creation 
of both a monoscopic and stereoscopic version of the film while being flexible enough to accommodate 
their individual aesthetic requirements. * robert.neuman@disney.com 1 For the sake of brevity, the terms, 
3D and 2D will be used to refer to stereoscopic and monoscopic exhibition formats, and will not be describing 
a method of content creation (i.e. CGI vs. traditional). Copyright is held by the author / owner(s). 
SIGGRAPH 2009, New Orleans, Louisiana, August 3 7, 2009. ISBN 978-1-60558-726-4/09/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1598029</article_id>
		<sort_key>390</sort_key>
		<display_label>Article No.</display_label>
		<display_no>39</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>39</seq_no>
		<title><![CDATA[Pushing tailoring techniques to reinforce <i>Up</i> character design]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1597990.1598029</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1598029</url>
		<abstract>
			<par><![CDATA[<p>Traditional patternmaking techniques have evolved to fit typically proportioned humans. Although these techniques have proven useful in altering patterns to fit characters in previous computer-animated films, the design of the characters in <i>Up</i> necessitated exaggerated alterations. The extremely caricatured characters were born of simple geometric shapes -- a boy as a circle, an old man as a square, and a lady as an hourglass. Understanding the characters' designs determined the appropriate pattern alteration techniques and how they would need to be exaggerated for each garment.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>A.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P1617695</person_id>
				<author_profile_id><![CDATA[81442619143]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Fran]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kalal]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Pixar Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617696</person_id>
				<author_profile_id><![CDATA[81442602601]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Carmen]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ngai]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Pixar Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617697</person_id>
				<author_profile_id><![CDATA[81442611425]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Claudia]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Chung]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Pixar Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1598030</article_id>
		<sort_key>400</sort_key>
		<display_label>Article No.</display_label>
		<display_no>40</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>40</seq_no>
		<title><![CDATA[The net effect]]></title>
		<subtitle><![CDATA[simulated bird-catching in <i>Up</i>]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1597990.1598030</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1598030</url>
		<abstract>
			<par><![CDATA[<p>A key sequence in <i>Up</i> involves the ensnaring of Kevin, the film's giant colorful fictitiuous bird, in a net launched by those intent on its capture. The Up FX team was tasked with the challenge of creating a net model which could be believably launched, tangled with a thrashing, feathered character, stretched taut, cut with a knife, and dragged across the ground by a pack of dogs. We will discuss the technical and artistic techniques which made this possible.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>A.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P1617698</person_id>
				<author_profile_id><![CDATA[81442595983]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Eric]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Froemling]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Pixar Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1598031</article_id>
		<sort_key>410</sort_key>
		<display_label>Article No.</display_label>
		<display_no>41</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>41</seq_no>
		<title><![CDATA[The immersive computer-controlled audio sound theater]]></title>
		<subtitle><![CDATA[history and current trends in multi-modal sound diffusion]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1597990.1598031</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1598031</url>
		<abstract>
			<par><![CDATA[<p>Multi-channel sound diffusion has been an essential part of the electronic and computer music performance from the beginnings of the genre in the 1950's. We see early experimentation in sound spatialization in Var&#232;se's use of loudspeaker "paths" in <i>Po&#233;me Electronique</i>, Stockhausen's experiments in quad and cube-based speaker arrangements [Chadabe 1997], and Chowning's computational approach to sound diffusion in works like <i>Turenas</i> [Roads and Strawn 1985].</p> <p>Experimentation in the deployment and usage of large arrays of audio loudspeakers is seen in the loudspeaker orchestras the <i>GME-Baphone</i> from Bourges [Clozier 2001], the <i>Acousmonium</i> of <i>Le Groupe de Recherches Musicales</i> in Paris and the <i>BEAST</i> from the University of Birmingham [Harrison 1999]. The use quadraphonic speaker configurations are first seen with Richard Moore's notion of loudspeakers as "windows" to the virtual world beyond [Moore 1989]. A standard reference for home theater surround sound is seen in the development of standard configurations for multichannel audio in digital video (5.1, 6.1, 7.1, 10.2) [Stampfl and Dobler 2004]. And the recent interest in Ambisonic recording technologies [Malham and Myatt 1995] has furthered the aesthetic goal of an immersive audio environments that place the listener in an alternative sonic world -- a "virtual world" created by composers and sound artists.</p> <p>The Immersive Computer-controlled Audio Sound Theater (<b>ICAST</b>, pronounced "eye-cast") is a project that addresses the various needs of multiple performance modes through a computer-controlled loudspeaker environment that is malleable, adaptable and intuitive. This research explores experimental technologies for audio that go far beyond the conventional surround-sound approaches and allows composers the ability to place sound in any apparent location in a concert hall, as well as manipulate the apparent size of the sound.</p> <p>The system explores new and novel metaphors for sound control using both conventional and unconventional interfaces. ICAST is a hardware and software solution, based on a client-server computer control system, commodity audio interface hardware, and high-quality audio amplifiers and loudspeakers. Our system currently uses 27 discrete audio channels but has the capacity to support 44 channels. We have been using this system over the past four years, with continual development of the software and hardware interfaces.</p> <p>This paper will give a brief history and context to the use loudspeaker arrays and orchestras in electroacoustic music, an overview of the underlying aesthetics of this kind of sound diffusion, as well as describe our research activities and concert experiences with ICAST. We will also explore the concept of "Cinema for the Ears" which has guided us towards new modes of presenting, manipulating and experiencing audio only concerts. This aesthetic engages listeners in a world of real, synthetic, or altered realities where composers manipulate the sonic environment, create musical structures, and explore sounds at a micro-level. What we have learned from this research has applications that go well beyond concert hall performances, and include virtual reality environments, cinema, and video games.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[ambisonics]]></kw>
			<kw><![CDATA[electroacoustic music performance]]></kw>
			<kw><![CDATA[immersive audio]]></kw>
			<kw><![CDATA[interactive sound diffusion]]></kw>
			<kw><![CDATA[loudspeaker orchestras]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>H.5.5</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>A.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010469.10010475</concept_id>
				<concept_desc>CCS->Applied computing->Arts and humanities->Sound and music computing</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002951.10003317.10003371.10003386.10003390</concept_id>
				<concept_desc>CCS->Information systems->Information retrieval->Specialized information retrieval->Multimedia and multimodal retrieval->Music retrieval</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P1617699</person_id>
				<author_profile_id><![CDATA[81100558043]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Stephen]]></first_name>
				<middle_name><![CDATA[David]]></middle_name>
				<last_name><![CDATA[Beck]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Louisiana State University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Chadabe, J. 1997. <i>Electric sound: the past and promise of electronic music</i>. Prentice Hall.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1245181</ref_obj_id>
				<ref_obj_pid>1245172</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Clozier, C. 2001. The gmebaphone concept and the cybern&#233;phone instrument. <i>Computer Music Journal 21</i>, 4, 81--90.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Harrison, J. 1999. Diffusion: theories and practices, with particular reference to the beast system. <i>eContact 2.4</i> (9).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Malham, D. G., and Myatt, A. 1995. 3d sound spatialization using ambisonic techniques. <i>Computer Music Journal 19</i>, 4, 58--70.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Moore, F. R. 1989. A general model for spatial processing of sounds. In <i>The Music Machine</i>, C. Roads, Ed. MIT Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Roads, C., and Strawn, J. 1985. <i>Foundations of Computer Music</i>. MIT Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1103925</ref_obj_id>
				<ref_obj_pid>1103900</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Stampfl, P., and Dobler, D. 2004. Enhancing three-dimensional vision with three-dimensional sound. In <i>SIGGRAPH Proceedings</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1598032</article_id>
		<sort_key>420</sort_key>
		<display_label>Article No.</display_label>
		<display_no>42</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>42</seq_no>
		<title><![CDATA[Multi-touch everywhere!]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1597990.1598032</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1598032</url>
		<abstract>
			<par><![CDATA[<p>We present a portable device that enables users to turn any flat surface into a multi-touch controller for music and other media applications. The device is playable either with the hands, mallets or sticks. We also present a software editor for configuring the surface and creating control interfaces using a library of control elements, such as buttons, sliders, and pads.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[computer vision]]></kw>
			<kw><![CDATA[multi-touch interaction]]></kw>
			<kw><![CDATA[musical interfaces]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>H.5.2</cat_node>
				<descriptor>Haptic I/O</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>A.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10011752</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Haptic devices</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P1617700</person_id>
				<author_profile_id><![CDATA[81314493693]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Alain]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Crevoisier]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Applied Sciences Western Switzerland]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617701</person_id>
				<author_profile_id><![CDATA[81442619928]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Greg]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kellum]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Applied Sciences Western Switzerland]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Crevoisier, A., Kellum, G. Transforming Ordinary Surfaces into Multi-touch Controllers. In <i>Proc. of the Conf. on New Interfaces for Musical Expression (NIME)</i>, 2008.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1598033</article_id>
		<sort_key>430</sort_key>
		<display_label>Article No.</display_label>
		<display_no>43</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>43</seq_no>
		<title><![CDATA[Sound scope headphones]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1597990.1598033</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1598033</url>
		<abstract>
			<par><![CDATA[<p>We designed the Sound Scope Headphones so that they would let users control an audio mixer through natural movements, and thus enable a musical novice to separately listen to each player's performance. The main advantage of the headphones is that they detect natural movement, such as head movement or placing a hand behind an ear, and uses the detected movements to control an audio mixer while the user listens to music. Three sensors are mounted on the headphones: a digital compass, a tilt sensor, and a distance sensor (Figure 1).</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>H.5.5</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>A.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010469.10010475</concept_id>
				<concept_desc>CCS->Applied computing->Arts and humanities->Sound and music computing</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002951.10003317.10003371.10003386.10003390</concept_id>
				<concept_desc>CCS->Information systems->Information retrieval->Specialized information retrieval->Multimedia and multimodal retrieval->Music retrieval</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P1617702</person_id>
				<author_profile_id><![CDATA[81444599496]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Masatoshi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hamanaka]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Univirsity of Tsukuba]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617703</person_id>
				<author_profile_id><![CDATA[81442595282]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[SuengHee]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lee]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Univirsity of Tsukuba]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Goudeseune, C., and Kaczmarshi, H. Composing outdoor augmented-reality sound environments. In <i>In Proceedings of the International Computer Music Conference</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Warusfel, O., and Eckel, G. Listen - augmenting eeveryday environments through interactive soundscapes. In <i>In Proceedings of IEEE Workshop on VR for public consumption</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>261140</ref_obj_id>
				<ref_obj_pid>261135</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Wu, J., Duh, C., Ouhyoung, M., and Wu, J. Head motion and latency compensation on localization of 3d sound in virtual reality. In <i>In Proceedings of the ACM symposium on Virtual reality software and technology</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Figure 1: Three sensors mounted to headphones. 2 How parts are scoped The usability of our headphones 
depends on the quality of the links between the mixer manipulations and the natural movement while the 
user listens to music. We use three links. Link from the facing direction. When a user moves his head 
left­wards (rightwards), the part normally heard from the left (right) side can be heard from a frontal 
position as the digital compass de­tects the change in the direction the user faces. This allows a user, 
through natural movement, to scope the part which he wants to hear most clearly and hear it from a frontal 
position. Link from the face s angle of elevation. When there are several parts in the frontal position, 
the user might not be able to hear the desired part clearly after turning his head left or right to hear 
it from a frontal position. In such a case, the user can change the mix *e-mail:hamanaka@iit.tsukuba.ac.jp 
e-mail:lee@kansei.tsukuba.ac.jp by moving his head up or down and the tilt sensor will detect the change 
in the face s angle of elevation. By looking up or down, re­spectively, the user increases the volume 
of each part located farther away or more closely. Link from the distance between hand and ear. The distance 
sen­sor detects the motion of putting a hand behind one s ear while lis­tening to the sound from a frontal 
position. The distance between hand and ear determines the area indicating whether each part is audible. 
For example, when a user places her hand close to her ear, she can hear only the parts from a frontal 
position. When the user removes her hand, she can hear all the parts except those be­hind her. When the 
user puts her hand in a middle position, she can hear the parts located in the front half position. Therefore, 
the user can focus on a part she wants to listen to by adjusting the distance between her hand and ear. 
 3 Conclusion The Sound Scope Headphones enable the wearer to control an au­dio mixer through natural 
movements. We are now developing ap­plications for the headphones. For example, Figure 2 shows real instruments 
where the light brightness is controlled depending on each part s sound-level. This allows the user to 
understand each part s sound-level visually as well as aurally. This should help mu­sical novices who 
do not know the sound of each instrument learn the relationship between instrument and sound. Figure 
2: Lighting depending on each part s sound-level. References GOUDESEUNE, C., AND KACZMARSKI, H. Composing 
outdoor augmented-reality sound environments. In In Proceedings of the International Computer Music Conference. 
WARUSFEL, O., AND ECKEL, G. Listen -augmenting eeveryday environments through interactive soundscapes. 
In In Proceed­ings of IEEE Workshop on VR for public consumption. WU, J., DUH, C., OUHYOUNG, M., AND 
WU, J. Head motion and latency compensation on localization of 3d sound in virtual reality. In In Proceedings 
of the ACM symposium on Virtual reality software and technology. Copyright is held by the author / owner(s). 
SIGGRAPH 2009, New Orleans, Louisiana, August 3 7, 2009. ISBN 978-1-60558-726-4/09/0008 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1598034</article_id>
		<sort_key>440</sort_key>
		<display_label>Article No.</display_label>
		<display_no>44</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>44</seq_no>
		<title><![CDATA[Practical uses of a ray tracer for <i>Cloudy with a Chance of Meatballs</i>]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1597990.1598034</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1598034</url>
		<abstract>
			<par><![CDATA[<p>For the feature film 'Cloudy With A Chance Of Meatballs' we were presented with many lighting/rendering challenges that required us to adopt a new lighting pipeline. This new pipeline would use Arnold, a ray tracing renderer with global illumination that could provide interactive feedback to the artist. The pipeline needed to work with large data sets and provide clever ways to speed up indirect lighting, refraction, and reflection, humans with subsurface scattering skin and fine hair in complex scenes. The mix of graphic, stylized shapes, fine patterns of texture, and extreme ranges in color and contrast required the use of non-traditional ray tracing methods with hardware and software development for interactive feedback for the lighting artists. Ray tracing with global illumination provides great photorealistic effects, but often at a cost due to poor anti-aliasing, lack of flexibility, and the inability to mix non-photorealistic techniques into a single render.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Raytracing</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>A.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372.10010374</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering->Ray tracing</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P1617704</person_id>
				<author_profile_id><![CDATA[81335489922]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Danny]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Dimian]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Sony Imageworks]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617705</person_id>
				<author_profile_id><![CDATA[81442610084]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Karl]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Herbst]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Sony Imageworks]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Practical uses of a ray tracer for Cloudy with a Chance of Meatballs Karl Herbst Danny Dimian Sony Imageworks 
Sony Imageworks kherbst@imageworks.com danny@imageworks.com 1 Introduction For the feature film Cloudy 
With A Chance Of Meatballs we were presented with many lighting/rendering challenges that re­quired us 
to adopt a new lighting pipeline. This new pipeline would use Arnold, a ray tracing renderer with global 
illumination that could provide interactive feedback to the artist. The pipeline needed to work with 
large data sets and provide clever ways to speed up indirect lighting, refraction, and reflection, humans 
with subsurface scattering skin and fine hair in complex scenes. The mix of graphic, stylized shapes, 
fine patterns of texture, and ex­treme ranges in color and contrast required the use of non­traditional 
ray tracing methods with hardware and software devel­opment for interactive feedback for the lighting 
artists. Ray trac­ing with global illumination provides great photorealistic effects, but often at a 
cost due to poor anti-aliasing, lack of flexibility, and the inability to mix non-photorealistic techniques 
into a single render. 2 Our Approach Our solution to these challenges started with the use of a renderer 
that could use multithreading on the desktop and was memory efficient allowing artists to have interactive 
feedback at their desks without the support of a large render farm. The Arnold ren­derer is able to do 
these things with very complex scenes by lev­eraging instancing, subdivision surfaces, non-traditional 
shading models and hardware that could support it. A dual processor eight core machine with 8 gigs of 
memory was needed to fit the hard­ware needs allowing the renderer to give us this type of feedback with 
a production scene. Artists are able to start interactive light­ing sessions where they can make light 
position and shader changes with instant feedback on their local desktop. They can also make selected 
surface material adjustments while making these lighting adjustments. All of the scene is active and 
cached making it even possible to move the camera and have the scene update giving artists the ability 
to view how their changes effect the whole scene. 2.1 Look Development Our look development pipeline 
had guidelines that required artists to test their assets under multiple lighting environments that were 
designed to test different aspects of the lighting model and/or extremes in color and contrast. These 
guidelines also required the artist to test multiple assets together for final approval. This proc­ess 
made sure all aspects of the global illumination interaction could be fully tested before releasing the 
asset to production. These lighting environments also made sure that all objects could interact in any 
scene without the need for pre-passes or special lighting rigs. 2.2 Shading The main shading goals were 
to leverage the complex, physically based lighting provided by a ray tracer, while maintaining the flexibility 
and control required to produce the stylized imagery of our film. The shading team developed quick shading 
methods for use by secondary ray types allowing faster rendering by using this ap­proximation. For indirect 
lighting, only a subset of the shader was calculated. This allowed the use of indirect (or bounce) lighting 
in all scenes and reduced the number of lights needed to fill in dead areas. The resulting color bleeding 
and light bounce ef­fects were major contributors to the overall look and feel of the film. New shading 
models were also developed. Examples include a gelatinous shader that was used to render both the inside 
and out­side of a castle made entirely of Jell-O and an iridescent, lumi­nous paint that was used to 
light the inside of the laboratory. 2.3 Shot Lighting During shot production our approach was to allow 
the artist to spend more of their time lighting than managing pre-passes and output layers for compositing. 
This was possible by the use of clever ray management in the shaders, simple ray and scene com­plexity 
management tools provided to the artist via the lighting package, and assets which all worked together 
from the start. This allowed artists to concentrate on light and shadow quality on eve­rything in a scene 
all at once while needing fewer lights in gen­eral. The interactive feedback allowed them to iterate 
faster than previously possible while also gaining subtle lighting cues from the global illumination 
model. When needed, they could still break layers out for compositing needs, or add light passes for 
better control.  3 Conclusion Ray tracing, global illumination, and interactive rendering are not new 
concepts, but the combined use for Cloudy With A Chance Of Meatballs allowed artist to mix and match 
photorealistic with stylized non-photorealistic lighting in one environment while gaining all of the 
traditional benefits of ray tracing. This de­creased the time needed to light a shot, but also allowed 
for better reuse of lighting rigs from scene to scene helping maintain quality and continuity. Copyright 
is held by the author / owner(s). SIGGRAPH 2009, New Orleans, Louisiana, August 3 7, 2009. ISBN 978-1-60558-726-4/09/0008 
  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1598035</article_id>
		<sort_key>450</sort_key>
		<display_label>Article No.</display_label>
		<display_no>45</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>45</seq_no>
		<title><![CDATA[Multi-layer dual-resolution screen-space ambient occlusion]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1597990.1598035</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1598035</url>
		<abstract>
			<par><![CDATA[<p>Ambient occlusion (AO) is a lighting model that approximates the diffuse illumination of a surface based on its directly visible occluders. It can be rendered by tracing rays through the normal-oriented unit hemisphere, and returning the percentage of rays that do no hit any geometry at a distance <i>d</i> &lt; <i>R</i>. Screen-space ambient occlusion (SSAO) is an image-based approach that uses the depth buffer of the current rendered scene as an approximation of its geometry. Therefore, compared to other AO algorithms such as ray tracing, SSAO has the advantage of handling dynamic geometry with significantly lower overhead.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.4.0</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>A.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010383</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Image processing</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P1617706</person_id>
				<author_profile_id><![CDATA[81319488052]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Louis]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bavoil]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[NVIDIA Corporation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617707</person_id>
				<author_profile_id><![CDATA[81100048438]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Miguel]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sainz]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[NVIDIA Corporation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Akenine-M&#246;ller, T., Haines, E., and Hoffman, N. 2008. <i>Real-Time Rendering 3rd Edition</i>. A. K. Peters, Ltd.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Bavoil, L., and Sainz, M. 2009. Image-space horizon-based ambient occlusion. In <i>ShaderX7 - Advanced Rendering Techniques</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Cantlay, I. 2007. High-speed, off-screen particles. In <i>GPU Gems 3</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Everitt, C. 2001. Interactive order-independent transparency. Tech. rep., NVIDIA.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1507161</ref_obj_id>
				<ref_obj_pid>1507149</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Ritschel, T., Grosch, T., and Seidel, H.-P. 2009. Approximating dynamic global illumination in image space. In <i>ACM Symposium on Interactive 3D Graphics and Games (i3D)</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Multi-Layer Dual-Resolution Screen-Space Ambient Occlusion Louis Bavoil Miguel Sainz NVIDIA Corporation 
 (a) (b) (c) (d) (e) Figure 1: Ambient occlusion without any shading. (a) 90 fps, single-layer dual-resolution 
screen space ambient occlusion (SSAO) with 8x16 depth samples per AO pixel. (b) 35 fps, multi-layer dual-resolution 
SSAO with 3 depth-peeled layers, and 3x8x16 depth samples per AO pixel. (c) 21 fps, full-resolution SSAO 
with 3 depth-peeled layers. (d) 2min 45s, reference ray-traced AO using Gelato with 64 rays per pixel. 
(e) Breakdown of image (b) where the green pixels are rendered with full-resolution AO and the others 
with half-resolution AO. SSAO images rendered with (800 + 100)x(600 + 100) pixels with a GeForce 9800 
GT. 1 Introduction 3 Dual-Resolution SSAO Ambient occlusion (AO) is a lighting model that approximates 
the To improve performance in regions where the AO variability is low, diffuse illumination of a surface 
based on its directly visible occlud-we take a similar approach to [Cantlay 2007] for rendering alpha­ers. 
It can be rendered by tracing rays through the normal-oriented blended particles. We start with a half-resolution 
AO pass that takes unit hemisphere, and returning the percentage of rays that do no hit as input Nl enlarged 
depth layers in eye space, and the per-pixel any geometry at a distance d < R. Screen-space ambient occlusion 
normals associated with the nearest depth layer. We then perform (SSAO) is an image-based approach that 
uses the depth buffer of the a full-resolution pass which computes for each pixel the local AO current 
rendered scene as an approximation of its geometry. There-range in the half-resolution AO image and re.nes 
the pixels that fore, compared to other AO algorithms such as ray tracing, SSAO have range >= T , where 
T is a parameter. The AO min-max range has the advantage of handling dynamic geometry with signi.cantly 
is computed in a kernel larger than 2x2 half-resolution pixels. The lower overhead. performance improvement 
of this algorithm depends on the thresh­ old parameter T and on the size of the range kernel. In our 
dual-All SSAO algorithms such as the ones described in [Akenine-resolution results, we use a 5x5 range 
kernel and T = 0.1. M¨ oller et al. 2008] suffer from missing information outside the view frustum, and 
behind the depth buffer where fragments oc­ 4 Discussion cluded from the eye can be visible from other 
surface points. As proposed in [Ritschel et al. 2009], we use depth peeling [Everitt As shown here, our 
multi-layer method improves the quality of 2001] for capturing additional information behind the nearest 
depth SSAO algorithms substantially. In the scenes were there are large layer to the eye, and we use 
an enlarged depth buffer and a cor-numbers of polygons facing the camera at grazing angles, this re­responding 
enlarged .eld of view for capturing depths outside the quire a large number of depth layers to be captured. 
As mentioned screen bounds. Depth peeling makes the SSAO slower, because of in [Ritschel et al. 2009], 
this can be addressed by using multiple the additional geometry passes to generate the depth layers, 
and the cameras, but this solution also requires depth peeling from each SSAO cost for processing all 
the layers. However, adding just 3 camera to handle all cases. This problem may also be addressed by 
depth layers can remove most signi.cant artifacts (see Figure 1). rasterizing all the fragments per pixel 
into an A-Buffer. The two methods that we present are orthogonal to each other and Our dual-resolution 
optimization provides signi.cant performance are applicable to any SSAO algorithm. The multi-layer part 
is for improvements with controllable quality (see Figure 1). One could improving quality and the dual-resolution 
part is for improving per-argue that a limitation of this approach occurs with thin objects such formance 
based on an error threshold parameter. In this talk, we as grass blades which may be too small to be 
rasterized in the half­use the SSAO algorithm described in [Bavoil and Sainz 2009]. resolution pass and 
therefore will not contribute to the AO ranges used to evaluate the re.nemet of a pixel. In practice, 
we have found this not to be a problem because thin objects such as grass .elds are 2 Multi-Layer SSAO 
typically rendered on large regions of the screen which makes them Our method takes as input multiple 
enlarged depth-peeled layers more likely to be detected by our range kernel. generated using an enlarged 
.eld of view. We clamp the kernel footprint of the SSAO algorithm so that the kernel radius in pixels 
 References is bounded by a given parameter B. For a .nal image resolution of W xH, the enlarged depth 
buffer size is (W + 2B)x(H + 2B) and AKENINE-M ¨ OLLER, T., HAINES, E., AND HOFFMAN, N. 2008. Real-Time 
Render­the enlarged .eld of view is such that the intersection of the view ing 3rd Edition. A. K. Peters, 
Ltd. frustum with the image plane is scaled up by (1 + 2B/H). BAVOIL, L., AND SAINZ, M. 2009. Image-space 
horizon-based ambient occlusion. In ShaderX7 -Advanced Rendering Techniques. The algorithm used in [Ritschel 
et al. 2009] assumes that a point CANTLAY, I. 2007. High-speed, off-screen particles. In GPU Gems 3. 
is known to be inside a closed object if its depth is inside a pair of EVERITT, C. 2001. Interactive 
order-independent transparency. Tech. rep., NVIDIA. consecutive layers. This does not hold for non-closed 
objects such RITSCHEL, T., GROSCH, T., AND SEIDEL, H.-P. 2009. Approximating dynamic as vegetation billboards. 
Our approach does not have this limita­ global illumination in image space. In ACM Symposium on Interactive 
3D Graphics tion. With Nl depth-peeled layers, for each texture coordinates uv, and Games (i3D). we compute 
the AO contribution AO(uv, j) from each depth layer j and we use AO(uv)= max(AO(uv, j), j = 0..Nl - 1). 
Copyright is held by the author / owner(s). SIGGRAPH 2009, New Orleans, Louisiana, August 3 7, 2009. 
ISBN 978-1-60558-726-4/09/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1598036</article_id>
		<sort_key>460</sort_key>
		<display_label>Article No.</display_label>
		<display_no>46</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>46</seq_no>
		<title><![CDATA[RACBVHs]]></title>
		<subtitle><![CDATA[random-accessible compressed bounding volume hierarchies]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1597990.1598036</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1598036</url>
		<abstract>
			<par><![CDATA[<p>Bounding volume hierarchies (BVHs) are widely used to accelerate the performance of various geometric and graphics applications. These applications include ray tracing, collision detection, visibility queries, dynamic simulation, and motion planning. These applications typically precompute BVHs of input models and traverse the BVHs at runtime in order to perform intersection or culling tests.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.4.10</cat_node>
				<descriptor>Volumetric</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>A.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010240.10010242</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision representations->Shape representations</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P1617708</person_id>
				<author_profile_id><![CDATA[81442603931]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Tae-Joon]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kim]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[KAIST (Korea Advanced Institute of Science and Technology)]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617709</person_id>
				<author_profile_id><![CDATA[81442602822]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Bochang]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Moon]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[KAIST (Korea Advanced Institute of Science and Technology)]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617710</person_id>
				<author_profile_id><![CDATA[81442593431]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Duksu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kim]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[KAIST (Korea Advanced Institute of Science and Technology)]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617711</person_id>
				<author_profile_id><![CDATA[81100019061]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Sung-Eui]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yoon]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[KAIST (Korea Advanced Institute of Science and Technology)]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1313121</ref_obj_id>
				<ref_obj_pid>1313046</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Yoon, S.-E., and Lindstrom, P. 2007. Random-accessible compressed triangle meshes. <i>IEEE Trans, on Visualization and Computer Graphics (Proc. Visualization) 13</i>, 6, 1536--1543.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 RACBVHs: Random-Accessible Compressed Bounding Volume Hierarchies Tae-Joon Kim Bochang Moon Duksu Kim 
Sung-EuiYoon KAIST(Korea Advanced Instituteof Science andTechnology) 1 Introduction Bounding volume 
hierarchies (BVHs) are widely used to acceler­ate the performance of various geometric and graphics applications. 
These applications include ray tracing, collision detection, visibility queries, dynamic simulation, 
and motion planning. These applica­tions typically precompute BVHs of input models and traverse the BVHs 
at runtime in order to perform intersection or culling tests. A major problem with using BVHs is that 
BVHs require large amounts of the memory space. Therefore, BVHs of large models consisting of hundreds 
of millions of triangles can take tens of giga­bytes of space. Moreover, the typical data access pattern 
on BVHs cannot be determined at the preprocessing time and is random at run­time. Therefore, accessing 
BVHs at runtime can have low I/O ef.­ciency and cache utilization. An aggravating trend is that the growth 
rate of the data access speed is signi.cantlyslower than that of the processing speed. There­fore, the 
problem of high storage requirements and low I/O ef.­ciency/cache utilization of BVHs will become more 
pronounced in the future. Several approaches have been developed to address this problem. One class of 
methods uses compact in-core BV representations by using quantizedBVinformationorbyexploitingthe connectivityin­formation 
of an original mesh and coupling the mesh and the BVH. Another class of methods stores BV nodes in a 
cache-coherent man­ner to improve cache utilization and, thus, improve the performance of traversing 
BVHs. However, due to the wideninggap between data access speeds and processing speeds, prior work may 
not provide enough reduction in storage requirements nor achieve high I/O ef.­ciency during the BVH traversal. 
2 Our Approach In order to ef.ciently access BVHs and improve the performance of various applications 
using BVHs, we propose a novel BVH com­pression and decompression method supporting random access. We 
compress BVs of a BVH by sequentially reading BVs in the BV lay­out of the BVH.We choose our compression 
method to preserve the original layout of the BVH in order to achieve the high cache uti­lization which 
the original layouts may maintain. We decompose the original layout of the BVH into a set of clusters. 
We assign consec­utive BVs in the BV layout to each cluster and set each cluster to have the same number 
of BVs to quickly identify a cluster contain­ing a BV node requested by an application at runtime. We 
compress each cluster separately from other clusters so that the clusters can be decompressed in any 
order. We de.ne an atomic BVH access API supporting transparent and random access on the compressed BVHs. 
The runtime framework fetches and decompresses the cluster into an in-core representation. Based on our 
in-core representation, we can very ef.ciently support random access to applications. Our run­time BVH 
access framework is guaranteed to return the correct BV information of the requested data when applications 
access the com­pressed data via our BVH access API. We employthe RACM representation [Yoon and Lindstrom 
2007] to further reduce the storage requirement of meshes, which are used together with BVHs for various 
applications. We achive up to a 12 : 1 compression ratio in our benchmark models. We implement two different 
applications, ray tracing and collision detection, to verify the bene.ts of our proposed method. In 
 (a) St. Matthew scene (b) Lucyand CAD turbine models Figure 1: The left image shows the result of ray 
tracing using our random-accessible compressed bounding volume hierarchies (RACBVHs) of St. Matthew model 
consisting of 128M triangles. The right image showsa frame duringa rigid-body simulation using collision 
detection between two models including the Lucy model consisting of 28 M triangles.ByusingRACBVHs,wecan 
reducethestorage requirementby afactorof10:1 and, more importantly, improve the performance of ray tracing 
and collision detection by up to a factor of four over using uncompressed data. Our representation enables 
ray tracing and collision detection with large models using commodity hardware. Figure 2: Hubo and Power 
Plant Models: The Hubo robot model (16 K triangles) is placed in the upper left corner of the power plant 
model (13 M triangles). The entire powerplant modelisshownonthe right.Weimprovethe performancebyafactorof2:1 
for collision detection using our RACBVH representation. these applications, we improve the performance 
by up to a factor of four over using uncompressed data. 3 Advantages of Our Approach 1. Wide applicability: 
The provided BVH access API allows var­ious applications to transparently access the compressed BVHs. 
Moreover, our BVH access API supports random access and does not restrict the access pattern of BVH-based 
applications. 2. Low storage requirement: Our RACBVH representation has up to a 12:1 compression ratio 
compared to an uncompressed BV representation. 3. Improved performance: We can achieve up to a 4:1 perfor­mance 
improvement on our tested applications over using un­compressed data.  References YOON, S.-E., AND 
LINDSTROM, P. 2007. Random-accessible compressed triangle meshes. IEEE Trans. on Visualization and Computer 
Graphics (Proc. Visualization) 13, 6, 1536 1543. Copyright is held by the author / owner(s). SIGGRAPH 
2009, New Orleans, Louisiana, August 3 7, 2009. ISBN 978-1-60558-726-4/09/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1598037</article_id>
		<sort_key>470</sort_key>
		<display_label>Article No.</display_label>
		<display_no>47</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>47</seq_no>
		<title><![CDATA[Rendering volumes with microvoxels]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1597990.1598037</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1598037</url>
		<abstract>
			<par><![CDATA[<p>The REYES [Cook et al. 1987] rendering architecture is a well known divide and conquer rendering algorithm. The algorithm subdivides complex primitives into <i>micropolygons</i> which are then shaded and sampled to produce final pixel colors. The most important advantage of the REYES algorithm over other rendering approaches such as raytracing is that shading quality is fully decoupled from image sampling - making it possible to adjust image sampling quality without affecting shading.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.4.10</cat_node>
				<descriptor>Volumetric</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>A.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010240.10010242</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision representations->Shape representations</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P1617712</person_id>
				<author_profile_id><![CDATA[81442613418]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Andrew]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Clinton]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Side Effects Software Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617713</person_id>
				<author_profile_id><![CDATA[81100457004]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Mark]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Elendt]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Side Effects Software Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Cook, R. L., Carpenter, L., and Catmull, E. 1987. The reyes image rendering architecture. <i>SIGGRAPH Comput. Graph. 21</i>, 4, 95--102.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Rendering Volumes With Microvoxels Andrew Clinton* Mark Elendt Side Effects Software Inc. Side Effects 
Software Inc.  (a) A volume primitive (b) Transparent surfaces mixed with two volume primitives 1 Introduction 
The REYES [Cook et al. 1987] rendering architecture is a well known divide and conquer rendering algorithm. 
The algorithm subdivides complex primitives into micropolygons which are then shaded and sampled to produce 
.nal pixel colors. The most im­portant advantage of the REYES algorithm over other rendering approaches 
such as raytracing is that shading quality is fully de­coupled from image sampling -making it possible 
to adjust image sampling quality without affecting shading. Volumetric effects are commonly produced 
in REYES rendering systems using custom raymarching code to step through the volume and accumulate shading 
samples. The problem with this approach is that integrated features of the rendering system (such as 
motion blur, depth of .eld, and compositing) are dif.cult or impossible to use in conjunction with volume 
primitives. We have extended the REYES algorithm to fully support volume primitives. Just like surfaces, 
volume primitives are diced into mi­crovoxels -an entity analogous to a micropolygon, but subtending 
a 3D volume of space. Microvoxels are then shaded and sampled along with micropolygons. Our volume primitives 
are also hybrid raytracing primitives, making it possible to render secondary effects using raytracing. 
 2 Shading Volume primitives are subdivided using a .exible dicing scheme that permits shading quality 
adjustment, view-dependent tessela­tion, and adaptive subdivision. The result of the dicing algorithm 
is a sequence of microvoxels .lling the volume primitive. Com­monly used features of surface rendering 
such as opacity culling work equally well with volumes, as the dicing procedure inserts both surfaces 
and volumes into a depth-ordered priority queue. Given a sequence of voxels, we run the full REYES shading 
pipeline on the microvoxel corner points. This includes displace­ment shaders, which can move around 
the shading points in an arbi­trary fashion before passing them to the surface shader. The surface shader 
is then responsible for computing the color and opacity for each shading point. An additional global 
variable dPdz indicates the depth of the volumetric region that should be composited (this variable is 
set to 0 when shading surfaces). *e-mail: andrew@sidefx.com e-mail: mark@sidefx.com (c) Volumetric motion 
blur (d) Emission and light interaction One powerful feature of the REYES architecture is the ability 
to bind arbitrary geometric information to shader variables. In our implementation, user-de.ned volumetric 
.elds are part of our ge­ometry speci.cation, making it possible to pass any desired value by name to 
the surface shader. Well-known bindings include den­sity and temperature for shading of .uid simulations, 
and velocity for motion blur.  3 Sampling Once microvoxels are shaded, we run a decoupled pixel sampling 
algorithm to generate sample lists. The sampling algorithm runs a fast SIMD raymarcher to generate hit 
lists for each subpixel sam­ple, interpolating the shaded results for each hit. The same sam­pling parameters 
are used for volumes and surfaces, allowing sev­eral important features of surface rendering to work 
equally well with volumes. Interleaved Transparency: Sample lists maintain z-order, so sur­faces and 
volumes can be arbitrarily interleaved with correct compositing. Motion Blur, Depth Of Field: Subpixel 
samples have an asso­ciated time and depth of .eld sample. Voxel motion is han­dled by moving the voxel 
vertices to the time for each sub­pixel sample, and then performing intersection tests against the transformed 
voxel edges. Depth of .eld simply changes the raymarch origin and direction. Deep Image Export: Since 
volumetric sample lists are visible to the renderer, it is easy to export them to deep .le formats like 
deep shadow maps. 4 Geometry Types Several volume primitive types exist, including voxel arrays, meta­balls, 
and paged disk .les. Volumes are implemented using a simple C++ API based on .ltered evaluation to look 
up volume attributes. Rendering additionally uses an acceleration data structure based on a KD-Tree to 
cull empty space. References COOK, R. L., CARPENTER, L., AND CATMULL, E. 1987. The reyes image rendering 
architecture. SIGGRAPH Comput. Graph. 21, 4, 95 102. Copyright is held by the author / owner(s). SIGGRAPH 
2009, New Orleans, Louisiana, August 3 7, 2009. ISBN 978-1-60558-726-4/09/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1598038</article_id>
		<sort_key>480</sort_key>
		<display_label>Article No.</display_label>
		<display_no>48</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>48</seq_no>
		<title><![CDATA[Wildfire forecasting using an open source 3D multilayer geographical framework]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1597990.1598038</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1598038</url>
		<abstract>
			<par><![CDATA[<p>This abstract describes the development of a wildfire forecasting plugin using Capaware. Capaware is designed as an easy to use open source framework to develop 3D graphics applications over large geographic areas offering high performance 3D visualization and powerful interaction tools for the Geographic Information Systems (GIS) community. Some other samples addressing this problem can be found in [Sherman et al. 2007] and [Thon et al. 2007].</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.5</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>A.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061.10010063</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures->Computational geometry</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P1617714</person_id>
				<author_profile_id><![CDATA[81100029271]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Modesto]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Castrill&#243;n]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Las Palmas G.C., Spain]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617716</person_id>
				<author_profile_id><![CDATA[81442612887]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Pedro]]></first_name>
				<middle_name><![CDATA[A.]]></middle_name>
				<last_name><![CDATA[Jorge]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Las Palmas G.C., Spain]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617717</person_id>
				<author_profile_id><![CDATA[81442594566]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Adri&#225;n]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Mac&#237;as]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Las Palmas G.C., Spain]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617718</person_id>
				<author_profile_id><![CDATA[81341495920]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Antonio]]></first_name>
				<middle_name><![CDATA[J.]]></middle_name>
				<last_name><![CDATA[S&#225;nchez]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Las Palmas G.C., Spain]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617719</person_id>
				<author_profile_id><![CDATA[81461655376]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Javier]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[S&#225;nchez]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Las Palmas G.C., Spain]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617720</person_id>
				<author_profile_id><![CDATA[81100422336]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Jos&#233;]]></first_name>
				<middle_name><![CDATA[P.]]></middle_name>
				<last_name><![CDATA[Su&#225;rez]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Las Palmas G.C., Spain]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617721</person_id>
				<author_profile_id><![CDATA[81442595529]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>7</seq_no>
				<first_name><![CDATA[Agust&#237;n]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Trujillo]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Las Palmas G.C., Spain]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617722</person_id>
				<author_profile_id><![CDATA[81442617873]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>8</seq_no>
				<first_name><![CDATA[Izzat]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sabbagh]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Canary Islands Technological Institute, Spain]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617723</person_id>
				<author_profile_id><![CDATA[81482644810]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>9</seq_no>
				<first_name><![CDATA[Ignacio]]></first_name>
				<middle_name><![CDATA[J.]]></middle_name>
				<last_name><![CDATA[L&#243;pez]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Canary Islands Technological Institute, Spain]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617715</person_id>
				<author_profile_id><![CDATA[81442619289]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>10</seq_no>
				<first_name><![CDATA[Rafael]]></first_name>
				<middle_name><![CDATA[J.]]></middle_name>
				<last_name><![CDATA[Nebot]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Canary Islands Technological Institute, Spain]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Sherman, W., Penick, M., Su, S., Brown, T., and Harris, F. 2007. Vrfire: an immersive visualization experience for wildfire spread analysis. In <i>Proceedings of IEEE Virtual Reality Conference</i>, 243--246.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Thon, S., Remy, E., Raffin, R., and Gesquire, G. 2007. Combining gis and forest fire simulation in a virtual reality environment for environmental management. In <i>ACE: Architecture, City and Environment</i>, vol. 2, 741--748.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1598039</article_id>
		<sort_key>490</sort_key>
		<display_label>Article No.</display_label>
		<display_no>49</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>49</seq_no>
		<title><![CDATA[Volumetric shadow mapping]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1597990.1598039</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1598039</url>
		<abstract>
			<par><![CDATA[<p>Classical rendering methods usually consider that light travels in vacuum, hence overlooking its interactions with its medium of transmission: the air. However, interactions between light and particles in suspension in the air generate phenomena such as smoke, fog, dust... Existing methods for simulating such interactions often rely on the native fog attenuation of graphics hardware, ignoring the occlusion effects shown in Figure 1. Mitchell [2005] tackles this problem by shading series of semi-transparent volume slices using a shadow map. While providing visually pleasant results, the method is not physically accurate and is prone to artifacts in given viewing directions due to undersampling.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.4.10</cat_node>
				<descriptor>Volumetric</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>A.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010240.10010242</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision representations->Shape representations</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P1617724</person_id>
				<author_profile_id><![CDATA[81100339836]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Pascal]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gautron]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Thomson Corporate Research]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617725</person_id>
				<author_profile_id><![CDATA[81100029219]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Jean-Eudes]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Marvie]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Thomson Corporate Research]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617726</person_id>
				<author_profile_id><![CDATA[81319491424]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Guillaume]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Fran&#231;ois]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The Moving Picture Company]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1339851</ref_obj_id>
				<ref_obj_pid>1339815</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Francois, G., Pattanaik, S., Bouatouch, K., and Breton, G. 2008. Subsurface texture mapping. <i>IEEE Computer Graphics & Applications 28</i>, 34--42.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Mitchell, J. L. 2005. <i>ShaderX3: Light Shaft Rendering</i>. Charles River Media, 573--588.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Williams, L. 1978. Casting curved shadows on curved surfaces. In <i>Proceedings of SIGGRAPH</i>, 270--274.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Volumetric Shadow Mapping Pascal Gautron Jean-Eudes Marvie Guillaume Franc¸ois Thomson Corporate Research 
Thomson Corporate Research The Moving Picture Company pascal.gautron@thomson.net jean-eudes.marvie@thomson.net 
guillaume-f@moving-picture.com  (a) Hebemissin -200 steps -28 fps (b) Dark Alley -200 steps -21 fps 
(c) Sibenik -200 steps -50 fps (d) Eddie -150 steps Figure 1: The light volume is sampled in shadow map 
space to compute single scattering, accounting for occlusions and shadows. The computation is carried 
out using graphics hardware for real-time performance (a, b, c), or RenderMan for production-quality 
rendering (d). Classical rendering methods usually consider that light travels in vacuum, hence overlooking 
its interactions with its medium of transmission: the air. However, interactions between light and par­ticles 
in suspension in the air generate phenomena such as smoke, fog, dust... Existing methods for simulating 
such interactions often rely on the native fog attenuation of graphics hardware, ignoring the occlusion 
effects shown in Figure 1. Mitchell [2005] tackles this problem by shading series of semi-transparent 
volume slices using a shadow map. While providing visually pleasant results, the method is not physically 
accurate and is prone to artifacts in given viewing directions due to undersampling. Ourapproachbuilds 
upontwo unrelated rendering techniques: the well-known shadow mapping algorithm [Williams 1978] and sub­surface 
texture mapping [Francois et al. 2008]for real-time scatter­ingsimulationin multilayered materials de.nedby2Dtextures.We 
simulate single scattering by evenly sampling the 2D light space to reduce aliasing artifacts due to 
undersampling of the shadow map. The designofvolumetric lighting commonly featuresgobotextures to aesthetically 
control the light shaft effects. The ray marching in the 2D light space uniformly samples such textures, 
hence pre­serving the gobo details which are unlikely to be preserved when sampling the camera ray in 
world space. Volumetric Shadow Mapping Our rendering method, based on shadow mapping (Figure 2), is di­vided 
into two passes: We .rst render the scene from the cameraC intoa RGBAbuffer in which the3 .rst channels 
encode the radiance receivedby the visible objects according to both the shadow map and the average extinction 
of the participating medium. The alpha channel stores the distance from the camera to the closest visible 
object. In the second pass we render a screen-aligned quadrilateral bound­ing the screen region covered 
by the light cones as seen from C. The fragment shader .rst intersects the cones with the viewing ray 
to determine the length of the light path through the cones. The possible presence of opaque objects 
within the cones is taken into account using the distances computed in the .rst pass (Figure 2(a)). This 
path is then projected into the 2D space of the shadow map and sampled using a user-de.ned number of 
sampling points (Fig­ure 2(b)). For each point, we determine the light source visibility using the shadow 
map, and compute the contribution ofthat point to the total radiance outgoing towards the camera. We 
also introduce ascattered ambient lighting term to enforce the visual coherencyof the lighting throughout 
the scene. (a) (b) Figure 2: Determination of the traversal distance (a) and ray march­ing in shadow 
map space (b). Results Our algorithmoffers real-time performanceona3.6GHzXeonwith an nVidia Geforce 
GTX280. The scenes of Figures 1(a, b, c) are rendered at a resolution of 1280 × 720. Production renderers 
also bene.tfromourmethodasitprovidesauser-de.ned,robust control of the sampling quality of the light 
shafts andgobos (Figure 1(d)). Volumetric Shadow Mappingisa uni.ed method forfast compu­tation of light 
shafts in participating media, oriented towards the preservation of the lighting features. The sole tuning 
of the number of ray marching stepsyields rendering qualities ranging from real­time to production. Futureworkwill 
particularly consideranextensionto multiple scat­tering simulation, as well as an accurate simulation 
of volumetric penumbras due to translucent objects using deep shadow mapping. References FRANCOIS, G., 
PATTANAIK, S., BOUATOUCH, K., AND BRE-TON, G. 2008. Subsurface texture mapping. IEEE Computer Graphics 
&#38; Applications 28, 34 42. MITCHELL,J.L. 2005. ShaderX3: Light Shaft Rendering. Charles River Media, 
573 588. WILLIAMS,L. 1978. Casting curved shadows on curved surfaces. In Proceedings of SIGGRAPH, 270 
274. Copyright is held by the author / owner(s). SIGGRAPH 2009, New Orleans, Louisiana, August 3 7, 2009. 
ISBN 978-1-60558-726-4/09/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1598040</article_id>
		<sort_key>500</sort_key>
		<display_label>Article No.</display_label>
		<display_no>50</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>50</seq_no>
		<title><![CDATA[Bucket depth peeling]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1597990.1598040</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1598040</url>
		<abstract>
			<par><![CDATA[<p>Efficient rendering of multi-fragment effects has long been a great challenge in computer graphics. The classical depth peeling algorithm [Everitt 2001] provides a simple but robust solution by peeling off one layer per pass, but multi rasterizations will lead to performance bottleneck for large and complex scenes. The k-buffer [Bavoil et al. 2007] captures <i>k</i> fragments in a single pass but suffers from read-modify-write(RMW) hazards which can be alleviated by multi passes [Liu et al. 2006]. Our approach exploits multiple render targets (MRT) as bucket array per pixel. Fragments are scattered into different buckets and sorted by a bucket sort. We describe two efficient schemes to reduce collisions when multiple fragments are routed to the same bucket. Our algorithm shows up to 32 times speedup to depth peeling especially for large scenes and the results are visually faithful. Also it has no requirement of pre-sorting geometries or post-sorting fragments, and is free of RMW hazards.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.6</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>A.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P1617727</person_id>
				<author_profile_id><![CDATA[81440596288]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Fang]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Liu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Chinese Academy of Sciences]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617728</person_id>
				<author_profile_id><![CDATA[81440592538]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Meng-Cheng]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Huang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Chinese Academy of Sciences]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617729</person_id>
				<author_profile_id><![CDATA[81410593094]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Xue-Hui]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Liu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Chinese Academy of Sciences]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617730</person_id>
				<author_profile_id><![CDATA[81100657893]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[En-Hua]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Macau]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1230117</ref_obj_id>
				<ref_obj_pid>1230100</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Bavoil, L., Callahan, S. P., Lefohn, A., Comba, J. a. L. D., and Silva, C. T. 2007. Multi-fragment effects on the gpu using the k-buffer. In <i>Proceedings of the 2007 symposium on Interactive 3D graphics and games</i>, ACM, 97--104.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Everitt, C. 2001. Interactive order-independent transparency. Tech. rep., NVIDIA Corporation.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Liu, B.-Q., Wei, L.-Y., and Xu, Y.-Q. 2006. Multi-layer depth peeling via fragment sort. Tech. rep., Microsoft Research Asia.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Bucket Depth Peeling Fang Liu * Meng-Cheng Huang Xue-Hui Liu En-Hua Wu § Institute of Software, Chinese 
Academy of Sciences University of Macau§ (a) (b) (c) (d) Figure 1: (a) Transparent effect on Stanford 
Dragon by UP1, 871K triangles. (b) Translucent effect on Lucy by UP2, 28.0M triangles. (c) Fresnel s 
effect on Buddha by AP2, 1.0M triangles. (d) Transparent effect on the .rst 32 layers of UNC Powerplant 
by AP2, 12.7M triangles. 1 Introduction Ef.cient rendering of multi-fragment effects has long been a 
great challenge in computer graphics. The classical depth peeling algo­rithm [Everitt 2001] provides 
a simple but robust solution by peel­ing off one layer per pass, but multi rasterizations will lead to 
per­formance bottleneck for large and complex scenes. The k-buffer [Bavoil et al. 2007] captures k fragments 
in a single pass but suffers from read-modify-write(RMW) hazards which can be alleviated by multi passes 
[Liu et al. 2006]. Our approach exploits multiple ren­der targets (MRT) as bucket array per pixel. Fragments 
are scattered into different buckets and sorted by a bucket sort. We describe two ef.cient schemes to 
reduce collisions when multiple fragments are routed to the same bucket. Our algorithm shows up to 32 
times speedup to depth peeling especially for large scenes and the results are visually faithful. Also 
it has no requirement of pre-sorting ge­ometries or post-sorting fragments, and is free of RMW hazards. 
 2 Bucket Depth Peeling Modern GPUs can support up to 8 MRTs with format RGBA32F, thus a bucket array 
of size 32 can be constructed per pixel. To guar­antee correct concurrent updates of buckets, we utilize 
MAX blend­ing, which compares the source and the destination values to keep the greater one. Each bucket 
is initialized to 0 so that the .rst update of any bucket will always succeed. When collision happens, 
the maximum fragment will survive .nally. As for other buckets, we simultaneously update them by 0 to 
keep their values unchanged. This atomic operation assures the correct result and avoids RMW hazards. 
We then describe two schemes to alleviate collisions. Uniform bucket depth peeling. A bounding box or 
a coarse visual hull is .rst rendered to get the approximate depth range [zNear,zFar] of each pixel. 
Then we group consecutive buckets into pairs and divide the depth range into 16 uniform subintervals. 
An incoming fragment with a depth value within the kth subinter­val will update the kth pair of buckets 
by (1 - df ,df ) and the rest by 0. After the .rst pass, the minimum and maximum fragment depth values 
within the kth subinterval dmink and dmaxk can be obtained from the kth pair, with correct depth ordering: 
dmin0 = dmax0 = dmin1 = dmax1 = ··· = dmin15 = dmax15. Other frag­ment attributes such as color can be 
packed into a .oating point and captured in a similar way. For complex scenes, multi-pass approach would 
be a natural extension to further reduce collisions. *China Basic S&#38;T 973 Research Grant(2009CB320802), 
National 863 High-Tec Grant(2008AA01Z301), NSFC(60573155)&#38;UM Research Grant. Adaptive bucket depth 
peeling. We consider each channel of MRT as a vector of 32 bits and construct an array of size 1024 called 
depth histogram. The depth range is divided into 1024 subintervals which are tiny enough to distinguish 
almost any two close layers. In the .rst pass, fragments within the kth subinterval will set the kth 
bit to 1 to indicate their presence in that subinterval. The histogram is equalized in a fullscreen pass 
by searching for the nonzero bits and the corresponding upper bounds will be stored. In the second pass, 
these upper bounds will redivide the depth range into 32 subinter­vals and the fragments will be routed 
into corresponding buckets. The one-to-one correspondence between fragments and subinter­vals reduces 
most collisions and makes full use of the bucket array. 3 Results Model Dragon Buddha Lucy Powerplant 
UP1 215fps 212fps 10.93fps 24.15fps UP2 128fps 106fps 5.71fps 12.79fps AP2 106fps 91fps 5.37fps 12.31fps 
[Liu 2006] 49fps/5g 39fps/6g 0.75fps/14g 0.83fps/27g DP 24fps/13g 20fps/13g 0.54fps/21g 0.76fps/32g 
Table 1: Comparison of frame rates(fps) and geometry passes(g). Figure 1 shows the results generated 
on Geforce 8800 GTX us­ing the uniform scheme with a single pass (UP1) and two passes (UP2), and the 
adaptive scheme (AP2). Table 1 compares our per­formances with that of [Liu et al. 2006] and depth peeling 
(DP) at 512x512 resolution. For large scenes with N layers, UP1 can achieve nearly N times (up to 32) 
speedup since the cost of frag­ment shader is negligible. Both UP2 and AP2 produce better results at 
the cost of an extra pass. Our algorithm is more appropriate to handle large scenes with high depth complexity. 
 References BAVOIL, L., CALLAHAN, S. P., LEFOHN, A., COMBA, J. A. L. D., AND SILVA, C. T. 2007. Multi-fragment 
effects on the gpu using the k-buffer. In Proceedings of the 2007 symposium on Interactive 3D graphics 
and games, ACM, 97 104. EVERITT, C. 2001. Interactive order-independent transparency. Tech. rep., NVIDIA 
Corporation. LIU, B.-Q., WEI, L.-Y., AND XU, Y.-Q. 2006. Multi-layer depth peeling via fragment sort. 
Tech. rep., Microsoft Research Asia. Copyright is held by the author / owner(s). SIGGRAPH 2009, New Orleans, 
Louisiana, August 3 7, 2009. ISBN 978-1-60558-726-4/09/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1598041</article_id>
		<sort_key>510</sort_key>
		<display_label>Article No.</display_label>
		<display_no>51</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>51</seq_no>
		<title><![CDATA[BVH for efficient raytracing of dynamic metaballs on GPU]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1597990.1598041</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1598041</url>
		<abstract>
			<par><![CDATA[<p>Metaballs [Bloomenthal 1997] are effective to represent fluids and similar complex and deformable geometries, but their implicit nature makes difficult their visualization in real time. A common strategy is to tessellate the resulting isosurface and to render it on GPU, but it scales poorly as the number of metaballs increases. Kanamori et al. [2008] efficiently raycast thousands of metaballs without intermediate representations. Their method assumes that rays are shot from a single viewpoint, thus preventing secondary effects (no shadows, reflections, etc.), and is limited to polynomial density functions. We propose to exploit the culling capacity of dynamic bounding volume hierarchies (BVH) [Wald 2007], the secant method for ray-surface intersection, and CPU-GPU parallelism to alleviate the restrictions of their method. This results in a general raytracing method, allowing arbitrary ray intersection (visibility, shadow, reflection, refraction, etc.) with metaballs of any finite-support at interactive performances.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Raytracing</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>A.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372.10010374</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering->Ray tracing</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P1617731</person_id>
				<author_profile_id><![CDATA[81442613972]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Olivier]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gourmel]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Toulouse, France]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617732</person_id>
				<author_profile_id><![CDATA[81442614400]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Anthony]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Pajot]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Toulouse, France]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617733</person_id>
				<author_profile_id><![CDATA[81100111550]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Lo&#239;c]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Barthe]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Toulouse, France]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617734</person_id>
				<author_profile_id><![CDATA[81100348923]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Mathias]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Paulin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Toulouse, France]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617735</person_id>
				<author_profile_id><![CDATA[81100349049]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Pierre]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Poulin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Universit&#233; de Montr&#233;al]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Bloomenthal, J. 1997. <i>Introduction to Implicit Surfaces</i>. Morgan Kaufmann, August.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Kanamori, Y., Szego, Z., and Nishita, T. 2008. GPU-based fast ray casting for a large number of metaballs. In <i>Eurographics</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1524953</ref_obj_id>
				<ref_obj_pid>1524874</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Wald, I. 2007. On fast construction of SAH based bounding volume hierarchies. In <i>Symposium on Interactive Ray Tracing</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 BVH for Ef.cient Raytracing of Dynamic Metaballs on GPU Olivier Gourmel, Anthony Pajot, Lo¨ic Barthe, 
Mathias Paulin* Pierre Poulin IRIT, University of Toulouse, France Universit´eal e de Montr´  (a) Raytracing 
of metaballs, with shadows and 3 levels of mirror re.ections: (left) 1000 metaballs at 13 fps at a 640 
× 480 resolution without multi-sampling; (center) 200K metaballs at 0.3 fps at 1600 × 1200 with 4 rays 
per pixel; (right) 3000 metaballs at 1.1 fps at 1600 × 1200 with 4 rays per pixel.  1 Introduction Metaballs 
[Bloomenthal 1997] are effective to represent .uids and similar complex and deformable geometries, but 
their implicit na­ture makes dif.cult their visualization in real time. A common strategy is to tessellate 
the resulting isosurface and to render it on GPU, but it scales poorly as the number of metaballs increases. 
Kanamori et al. [2008] ef.ciently raycast thousands of metaballs without intermediate representations. 
Their method assumes that rays are shot from a single viewpoint, thus preventing secondary effects (no 
shadows, re.ections, etc.), and is limited to polyno­mial density functions. We propose to exploit the 
culling capacity of dynamic bounding volume hierarchies (BVH) [Wald 2007], the secant method for ray-surface 
intersection, and CPU-GPU paral­lelism to alleviate the restrictions of their method. This results in 
a general raytracing method, allowing arbitrary ray intersection (vis­ibility, shadow, re.ection, refraction, 
etc.) with metaballs of any .nite-support at interactive performances. 2 Raytracing Metaballs on GPU 
De.nitions: A set of metaballs creating a connected surface is de.ned as a metaball connected set (MCS). 
A dynamic scene can consist of a varying number of MCSs, with completely changing con.gurations at each 
frame. BVHs for Metaballs: While a BVH can be ef.ciently built [Wald 2007], it does not scale well with 
large MCSs, as the BVH would contain only a few leaves, each encompassing a large MCS com­posed of many 
metaballs. Moreover at any given point, the ma­jority of the metaballs do not contribute to their MCS, 
because of their .nite support. Each metaball has a bounding box de.ned by its support, and will be included 
in one leaf node in the BVH. Dur­ing construction of the BVH, all metaballs potentially contributing 
to the MCS of one metaball belonging to a BVH node, are ef.­ciently duplicated in this node. Once a leaf 
node is reached, other conservative tests (sphere-sphere and maximum combined spheres) are performed 
to remove some non-contributing metaballs. As the number of duplicated metaballs cannot be predicted, 
dynamic al­location is needed, hence the BVH is constructed on the CPU. In Figure 1(b), the leftmost 
leaf node of the BVH contains two meta­balls (the green one being duplicated), the uppermost leaf node 
has three metaballs, etc. For leaf nodes A and B, the red curves indicate the shape of the partial MCS, 
in their respective leaf nodes, that will be tested for intersection. Ray-Metaball Intersection: Our 
method runs in two steps. First *email: {gourmel, pajot, lbarthe, paulin}@irit.fr email: poulin@iro.umontreal.ca 
 we traverse the BVH and search for a point Pi along the ray and inside the .rst leaf node intersected 
by the ray. The projections on the ray of the center of each (non-duplicated) metaball in this leaf node 
are good candidates for intersection. We then check the value of the density function at these projected 
points and keep the nearest among those inside the leaf partial MCS. To compute the in­tersection point, 
we need a point Po outside the MCS, for instance the origin of the ray. The intersection point is found 
by using the secant method with the interval [Po,Pi] as an initial guess. The se­cant method has some 
advantages over other iterative methods: it is very robust and quickly converges (in about 10 iterations, 
implying 10 evaluations of the density function). Kanamori et al. [2008] use Bezier clipping, which in 
some cases needs slightly fewer evalua­tions, but the number of evaluations increases with the degree 
of the polynomial density function. CUDA Implementation: We store the metaballs and BVH nodes in textures 
to bene.t from texture cache during random memory access due to BVH traversal. Animation of the metaballs 
and ren­dering are done on GPU, while BVH construction is done on CPU. This implies memory transfers, 
but also an inherent CPU-GPU par­allelism that is exploited by using streams and asynchronous exe­cution 
and transfers. The GTX-280 GPU allows transfer and com­putation simultaneously on GPU and CPU. By using 
buffers and forward computation of up to two frames, the animation is com­puted and downloaded to the 
CPU while the BVH begins to build, and rendering is performed while the BVH is completed and copied on 
the GPU. 3 Results We introduced techniques to robustly and ef.ciently raytrace large numbers of metaballs 
at interactive framerates, while still capturing secondary effects such as shadows and mirror re.ections 
(Fig. 1(a)). Due to the .nite support of a metaball, metaball density increases will directly affect 
performances. Moreover, BVH construction can become a limiting factor when the number of metaballs grows 
over 200K in our tests. Future work will focus on those issues. References BLOOMENTHAL, J. 1997. Introduction 
to Implicit Surfaces. Mor­gan Kaufmann, August. KANAMORI, Y., SZEGO, Z., AND NISHITA, T. 2008. GPU-based 
fast ray casting for a large number of metaballs. In Eurographics. WALD, I. 2007. On fast construction 
of SAH based bounding volume hierarchies. In Symposium on Interactive Ray Tracing. Copyright is held 
by the author / owner(s). SIGGRAPH 2009, New Orleans, Louisiana, August 3 7, 2009. ISBN 978-1-60558-726-4/09/0008 
 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1598042</article_id>
		<sort_key>520</sort_key>
		<display_label>Article No.</display_label>
		<display_no>52</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>52</seq_no>
		<title><![CDATA[Normal mapping with low-frequency precomputed visibility]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1597990.1598042</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1598042</url>
		<abstract>
			<par><![CDATA[<p>Normal mapping, ubiquitous in video games, decouples details stored at high spatial frequencies, often tiled or repeated, from unique lighting information stored at a lower sampling rates. Our technique enables normal maps on static geometry to interact with soft shadows from smooth distant lighting more efficiently compared to previous work. The visibility function is represented using low-order spherical harmonics, and the product of the cosine function and the lighting environment is tabulated in textures, decoupling normal variation from visibility. PCA compresses the results and accelerate the computation.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.4.0</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>A.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010383</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Image processing</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P1617736</person_id>
				<author_profile_id><![CDATA[81442616814]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Michal]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Iwanicki]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[CD Projekt RED]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617737</person_id>
				<author_profile_id><![CDATA[81100524617]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Peter-Pike]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sloan]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Disney Interactive Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1141982</ref_obj_id>
				<ref_obj_pid>1141911</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Ren, Z., Wang, R., Snyder, J., Zhou, K., Liu, X., Sun, B., Sloan, P.-P., Bao, H., Peng, Q., and Guo, B. 2006. Realtime soft shadows in dynamic scenes using spherical harmonic exponentiation. <i>ACM Transactions on Graphics 25</i>, 3 (July), 977--986.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>566612</ref_obj_id>
				<ref_obj_pid>566654</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Sloan, P.-P., Kautz, J., and Snyder, J. 2002. Precomputed radiance transfer for real-time rendering in dynamic, low-frequency lighting environments. <i>ACM Transactions on Graphics 21</i>, 3 (July), 527--536.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1111415</ref_obj_id>
				<ref_obj_pid>1111411</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Sloan, P.-P. 2006. Normal mapping for precomputed radiance transfer. In <i>ACM Symposium on Interactive 3D Graphics and Games</i>, ACM Digital Library, 23--26.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Normal Mapping with Low-Frequency Precomputed Visibility Michal Iwanicki* Peter-Pike Sloan CD Projekt 
RED Disney Interactive Studios Abstract Normal mapping, ubiquitous in video games, decouples details 
stored at high spatial frequencies, often tiled or repeated, from unique lighting information stored 
at a lower sampling rates. Our technique enables normal maps on static geometry to interact with soft 
shadows from smooth distant lighting more ef.ciently com­ pared to previous work. The visibility function 
is represented using low-order spherical harmonics, and the product of the cosine func­ tion and the 
lighting environment is tabulated in textures, decou­ pling normal variation from visibility. PCA compresses 
the results and accelerate the computation. 1 Introduction Games traditionally have used a combination 
of static light maps and multiple dynamic light sources. The surfaces generally are tex­ tured with both 
re.ectance properties and normal maps that approx­ imate complex surface details. While precomputed lighting 
is usu­ left with this expression: ally stored at low sampling rates, it is impractical to sample these 
unique details at the effective composite sampling rate. Our work decouples normal variations from lighting 
for static scenes. Method focuses on just shadows, not inter-re.ections, but has a signi.cant reduction 
in storage costs and can store the visibility information in I(vv) = M.n=1 wn (1O L(v textures unlike 
earlier work [Sloan 2006]. 2 Method bility vector and wn Let us consider a diffuse-only surface with 
no inter-re.ections. Outgoing radiance for such surface can be expressed as: I(vv) = 1O L(vv)V (vv)H(vv)d. 
(1) Where I is the outgoing radiance, L represents the distant lighting environment, V is the visibility 
function and H is the cosine lobe oriented for a given normal. In a typical PRT formulation [Sloan et 
al. 2002] of direct light, the lighting is projected onto some basis and plugged into the integral. Factoring 
the unknown lighting projection coef.cients out results in a transfer vector that depends on normal variation. 
We instead project both light and visibility into SH, and effectively tabulate the References product 
of light and the cosine term over the space of normals in textures [Ren et al. 2006]. This allows us 
to decouple the smooth visibility function from high frequency normal variations. The inte­ gral then 
is simply a dot product of the SH coef.cients for visibility V and L * H coef.cients. exponentiation. 
977 986. V coef.cients are stored in a texture, in a manner similar to lightmaps. L * H coef.cients should 
be stored for all possible ori- SLOAN, P.-P., KAUTZ, entations of the surface normal vector. Cube map 
textures are a natural solution for such look-up tables, but due to texture sampler count limits and 
lack of continuity they are problematic on DX9­ ics 21, 3 (July), 527 536. class hardware. Our experiments 
have shown that dual paraboloid SLOAN, P.-P. 2006. *e-mail: michal.iwanicki@cdprojektred.com transfer. 
e-mail: peter-pike.sloan@disney.com projection gives results comparable to those obtained using cube 
maps, and they can be modi.ed to be continuous across the seam. While this is effective, it turns out 
that using PCA reduces both stor­age and computation. Plugging the PCA projection of the visibility function 
into (1) and exploiting the linearity of integration you are )1 v)Vn(vv)H(vv)d.+L(vv)Ve(vv)H(vv)d. O 
The sum is over the M PCA basis vectors Vn, Ve is the mean visi­are spatially varying projection coef.cients. 
You simply need to compute the triple products of the PCA basis vectors and the light and cosine term. 
Unlike the uncompressed case, it is more ef.cient to compute the product matrix of the basis vectors 
since they are constant, multiply the light and simply blend quadratic SH vectors to store in textures. 
As the signal tabulated is at most a quadratic SH, it is smoother than the light*cosine func­tions stored 
in the uncompressed case and lower directional resolu­tion can be used. This factorization of the triple 
product turns out to be more ef.cient than the one used in the uncompressed case. Compressing the visibility 
function instead of a transfer matrix [Sloan 2006] the signal has lower entropy and can be more effec­tively 
compressed. Images are rendered at hundreds of frames a second on a notebook with an NVIDIA Quadro 3600M 
GPU. REN, Z., WANG, R., SNYDER, J., ZHOU, K., LIU, X., SUN, B., SLOAN, P.-P., BAO, H., PENG, Q., AND 
GUO, B. 2006. Real­time soft shadows in dynamic scenes using spherical harmonic ACM Transactions on Graphics 
25, 3 (July), J., AND SNYDER, J. 2002. Precom­puted radiance transfer for real-time rendering in dynamic, 
low­frequency lighting environments. ACM Transactions on Graph- Normal mapping for precomputed radiance 
In ACM Symposium on Interactive 3D Graphics and Games, ACM Digital Library, 23 26. Copyright is held 
by the author / owner(s). SIGGRAPH 2009, New Orleans, Louisiana, August 3 7, 2009. ISBN 978-1-60558-726-4/09/0008 
 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1598043</article_id>
		<sort_key>530</sort_key>
		<display_label>Article No.</display_label>
		<display_no>53</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>53</seq_no>
		<title><![CDATA[Animation and simulation of octopus arms in 'The Night at the Museum 2']]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1597990.1598043</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1598043</url>
		<abstract>
			<par><![CDATA[<p>We present a set of techniques we used to animate flexible arms of the monstrous octopus character in 'The Night at the Museum 2'. For this production, we adopted layered approaches, where key motions of the arms were animated with conventional methods by animators, while details were <i>cleaned up</i> by multiple passes of simulations of skeleton, flesh and tentacles.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Animation</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>A.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P1617738</person_id>
				<author_profile_id><![CDATA[81421596522]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Matt]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Derksen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Rhythm and Hues Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617739</person_id>
				<author_profile_id><![CDATA[81442603913]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Tae-Yong]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kim]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Rhythm and Hues Studios.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1598044</article_id>
		<sort_key>540</sort_key>
		<display_label>Article No.</display_label>
		<display_no>54</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>54</seq_no>
		<title><![CDATA[Methods for fast skeleton sketching]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1597990.1598044</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1598044</url>
		<abstract>
			<par><![CDATA[<p>Rigging characters for animation is a tedious task traditionally accomplished by extrusions of joints and direct bone manipulations. This work introduces different methods and concepts for a fast and effective sketching interface for skeleton creation. Unlike previous work on sketching motion paths [Thorne et al. 2004] or spline deformation [Blanco and Oliveira 2008], this is a direct way to create and adapt complex deformation skeletons on any character.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>A.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061.10010063</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures->Computational geometry</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P1617740</person_id>
				<author_profile_id><![CDATA[81435610105]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Martin]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Poirier]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Multimedia Lab, &#201;cole de technologie sup&#233;rieure, Montreal, Canada]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617741</person_id>
				<author_profile_id><![CDATA[81339521250]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Eric]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Paquette]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Multimedia Lab, &#201;cole de technologie sup&#233;rieure, Montreal, Canada]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1342261</ref_obj_id>
				<ref_obj_pid>1342250</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Blanco, F. R., and Oliveira, M. M. 2008. Instant mesh deformation. In <i>I3D '08: Proceedings of the 2008 symposium on Interactive 3D graphics and games</i>, ACM, 71--78.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1015740</ref_obj_id>
				<ref_obj_pid>1015706</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Thorne, M., Burke, D., and van de Panne, M. 2004. Motion doodles: an interface for sketching character motion. <i>ACM Trans. Graph. 23</i>, 3, 424--431.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Methods for Fast Skeleton Sketching Martin Poirier* EricPaquette ´ Multimedia Lab, Ecole de technologie 
sup´erieure, Montreal, Canada (a) (b) (c) (d) (e) Figure 1: Sketched skeletons on organic (a-b) and 
mechanical models (c). Template retargetting (d-e). 1 Introduction Rigging characters for animation is 
a tedious task traditionally ac­complished by extrusions of joints and direct bone manipulations. Thiswork 
introduces different methods and concepts forafast and effective sketching interface for skeleton creation. 
Unlike previous work on sketching motion paths [Thorne et al. 2004] or spline de­formation [Blanco and 
Oliveira 2008], this is a direct way to create and adapt complex deformation skeletons on anycharacter. 
 2 Sketching and Gestures As can be seen in the accompanying video, the sketching imple­mentation provides 
several traditional 3D sketching tools for free­hand strokes or polylines. Overdrawing (sketching on 
top of an existing stroke) can be used for simple stroke corrections. Gestures are also provided to cut, 
trim or delete strokes. Embeddingis usedtofacilitatesketchingbyautomatically position­ingstrokesinsidelimbs.Thisisdonethroughavariantofthe 
classic depth peeling algorithm, which can be applied inter objects or in­traobjects.The.rstisbettersuitedfor 
mechanicalparts(Fig.1c) whilethe secondis moregearedtowardorganicshapes(Fig.1a-b). 3 Bone Creation Next, 
strokes have to be converted into skeletons. Joints are in­serted at vertices of polylines strokes while 
automatic subdivision is used to insert joints along freehand strokes. Rules are available for subdividing 
a .xed number of time, subdividing into bones of .xed length or adaptively subdividing in areas of strong 
curvature. The resulting bones are fully oriented in 3D space using the direc­tion of the stroke and 
the orientation of the viewport when it was drawn. Template retargeting (Fig.1 d-e) is useful for complex 
and often used setups. It works by adapting a user-created skeleton to a stroke. This is done by minimizing 
the penalty function of Eq. 1, *e-mail: martin.poirier.4@ens.etsmtl.ca e-mail: eric.paquette@etsmtl.ca 
 controlling .tness to the original skeleton and .tness to the stroke. t-1t tt weight = ...i+ .lli+ .xxi(1) 
revi revi revi i=1 i=1 angle length distance If the skeleton contains control bones (inverse kinematic 
solver, pole target, action control, etc.), the retargeting process automat­ically repositions them with 
respect to the appropriate deformation bone. 4 Results and Conclusion Our prototype has been implemented 
in Blender (blender.org) and has been tested by a number of professional users. Testers had to accomplish 
prede.ned tasks corresponding to different types of rig­ging work. Of all the bone creation methods, 
users found the templating tech­niques most useful followed by the length and .xed subdivision techniques. 
Templating was found particularly good at reducing the time needed for rigging (by an estimated 10 to 
50%) in cases where manysimilar limbs were needed, such as when rigging .n­gers or insect legs. The interface 
was noted as being more natural and .uid than traditional methods, especially for tablet users. We demonstrated 
that a sketching metaphor can be adapted to de­formation skeleton creation to de.nitely speed up the 
creation and adaptation process in a .uid and effective way. Our implementa­tion can rig typical characters 
with the use of an effective stroke embedding method and with automatic template retargeting. We would 
like to thank ProMotion studios, Stanford University Computer Graphics Laboratory,and Jean-S´ebastien 
Guillemette for models and skeletons as well as everyone who participatedin the usability tests. References 
BLANCO, F. R., AND OLIVEIRA, M. M. 2008. Instant mesh deformation. In I3D 08: Proceedings of the 2008 
symposium on Interactive 3D graphics and games,ACM, 71 78. THORNE,M.,BURKE,D., ANDVANDE PANNE,M. 2004. 
Mo­tion doodles: an interface for sketching character motion. ACM Trans. Graph. 23, 3, 424 431. Copyright 
is held by the author / owner(s). SIGGRAPH 2009, New Orleans, Louisiana, August 3 7, 2009. ISBN 978-1-60558-726-4/09/0008 
 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1598045</article_id>
		<sort_key>550</sort_key>
		<display_label>Article No.</display_label>
		<display_no>55</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>55</seq_no>
		<title><![CDATA[Practical character physics for animators]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1597990.1598045</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1598045</url>
		<abstract>
			<par><![CDATA[<p>Physical realism is an important aspect of producing convincing 3D character animation. This is particularly true for live-action visual effects where animated characters occupy the same scene as the live actors. In such a scenario, a virtual character's movements must visually match the behavior and movements of the live environment, else the discrepancy will be obvious to the viewer.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Animation</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.6</cat_node>
				<descriptor>Interaction techniques</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10010391</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Graphics input devices</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P1617742</person_id>
				<author_profile_id><![CDATA[81100431086]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Ari]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Shapiro]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Rhythm & Hues Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617743</person_id>
				<author_profile_id><![CDATA[81100410859]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Sung-Hee]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lee]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[UCLA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Practical Character Physics for Animators Ari Shapiro Sung-Hee Lee Rhythm &#38; Hues Studios UCLA ashapiro@rhythm.com 
slee@cs.ucla.edu  Figure 1: The blue curve is the trajectory of the center of mass of the character 
s animation that an animator created manually. Our system suggests the physically correct ballistic path 
(the red curve) that the character s center of mass should follow. The system allows an animator to automatically 
change the original animation to match physical laws. CR Categories: I.3.7 [Computer Graphics]: Three-Dimensional 
Graphics and Realism Animation; I.3.6 [Computer Graphics]: Methodology and Techniques Interaction Techniques 
1 Exposition Physical realism is an important aspect of producing convincing 3D character animation. 
This is particularly true for live-action visual effects where animated characters occupy the same scene 
as the live actors. In such a scenario, a virtual character s movements must vi­sually match the behavior 
and movements of the live environment, else the discrepancy will be obvious to the viewer. We present 
an animation system that signi.cantly improves the visual quality of certain types of 3D character motion 
animated through traditional means by inferring physical properties and cor­recting the results through 
the use of dynamics. These physical characteristics are visualized and provide information not normally 
available to traditional 3D animators, such as displaying the center of mass, angular momentum and zero 
moment point. By compar­ing the original path as generated by an animator, against a proper physically-based 
path generated by our tool, the animator is able to interactively modify the original motion path to 
more closely match the generated physics-based path. This often results in better qual­ity character 
motion. Two different types of motion can be adjusted: animations which involve ballistic paths, such 
as falling and jump­ing, as well as animations involving character movement which re­quire balance and 
posture, such as walking or running. This dynam­ics visualization method is integrated into a professional 
software system for use in a visual effects studio that incorporates live-action with 3D animated characters 
in feature .lm production. Our re­search shows that between 10% and 16% of the shots of a character­heavy 
feature .lm will incorporate ballistic motions that may be improved by our system. Additionally, we investigate 
the physical accuracy of the high qual­ity animations that are manually created by professional animators. 
Our study shows the capabilities and limitations of the conventional kinematic animation process in terms 
of physical realism. We com­pute the center of mass and momentum of the ballistic motions and investigate 
how accurately these properties follow Newton s laws. For walking and running animations, we compute 
the zero moment point (ZMP) and measure how far it falls from the support polygon. Many animators found 
this system useful for improving physical re­alism of the keyframed animation. Interestingly, we found 
that the visualization tools introduced here can also serve as a gavel. Indi­vidual animators may have 
their own sense of physical correctness, which can cause debate over how a character should move when 
 Figure 2: Visualization of angular momentum, shown as a vector protruding from the center of mass of 
the character. The yellow ar­rows indicate the direction of the motion. The length of the momen­tum vectors 
indicate the relative amount of rotation about that axis. Note that our system is able to detect subtle 
animation problems, such as large discrepancies in angular momentum during the bal­listic phase. In this 
case, the character rotates counter-clockwise, and then clockwise during .ight, which is not physically 
valid.  Figure 3: Measurement of physical correctness of ballistic motions created created by professional 
animators conventional key framing process . The y-axis indicates the relative measurement of gravita­tional 
force in the scene, where 1 = normal gravity. collaborating animators have a different sense of physical 
intuition. We have observed that our tool helps animators reach an agreement on physically correct animation 
by quantifying the discrepancy in the motion. For example, our system can indicate exactly how many frames 
an animation should be slowed down or sped up. Copyright is held by the author / owner(s). SIGGRAPH 2009, 
New Orleans, Louisiana, August 3 7, 2009. ISBN 978-1-60558-726-4/09/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1598046</article_id>
		<sort_key>560</sort_key>
		<display_label>Article No.</display_label>
		<display_no>56</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>56</seq_no>
		<title><![CDATA[Surface motion graphs for character animation from 3D video]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1597990.1598046</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1598046</url>
		<abstract>
			<par><![CDATA[<p>Multiple view reconstruction of human performance as a 3D video has advanced to the stage of capturing detailed non-rigid dynamic surface shape and appearance of the body, clothing and hair during motion [Aguiar et al. 2008; Starck and Hilton 2007]. Full 3D video scene capture holds the potential to create truly realistic synthetic animated content by reproducing the dynamics of shape and appearance currently missing from marker-based motion capture. However, the acquisition results are in an unstructured volumetric or mesh approximation of the surface shape at each frame without temporal correspondence, which makes the reuse of this kind of data more challenging than conventional mocap data. In this paper, we introduce a framework that automatically constructs motion graphs for 3D video sequences and synthesizes novel animations to best satisfy user specified constraints on movement, location and timing.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Animation</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>A.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P1617744</person_id>
				<author_profile_id><![CDATA[81339505702]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Peng]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Huang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Surrey, UK]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617745</person_id>
				<author_profile_id><![CDATA[81100027411]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Adrian]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hilton]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Surrey, UK]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1360697</ref_obj_id>
				<ref_obj_pid>1360612</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Aguiar, E., Stoll, C., Theobalt, C., Ahmed, N., Seidel, H.-P., and Thrun, S. 2008. Performance capture from sparse multi-view video. <i>ACM Trans. Graph. 27</i>, 3, 1--10.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1299188</ref_obj_id>
				<ref_obj_pid>1299128</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Huang, P., Starck, J., and Hilton, A. 2007. A study of shape similarity for temporal surface sequences of people. In <i>3DIM '07: Proceedings of the Sixth International Conference on 3-D Digital Imaging and Modeling</i>, IEEE Computer Society, Washington, DC, USA, 408--418.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>566605</ref_obj_id>
				<ref_obj_pid>566570</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Kovar, L., Gleicher, M., and Pighin, F. 2002. Motion graphs. In <i>SIGGRAPH '02: Proceedings of the 29th annual conference on Computer graphics and interactive techniques</i>, ACM Press, New York, NY, USA, vol. 21, 473--482.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1251634</ref_obj_id>
				<ref_obj_pid>1251555</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Starck, J., and Hilton, A. 2007. Surface capture for performance-based animation. <i>IEEE Computer Graphics and Applications 27</i>, 3, 21--31.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1360696</ref_obj_id>
				<ref_obj_pid>1360612</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Vlasic, D., Baran, I., Matusik, W., and Popovi&#263;, J. 2008. Articulated mesh animation from multi-view silhouettes. <i>ACM Trans. Graph. 27</i>, 3, 1--9.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Surface Motion Graphs for Character Animation from 3D Video Peng Huang* and Adrian Hilton Centre for 
Vision Speech and Signal Processing University of Surrey, UK, GU2 7YW  Figure 1: An example of synthesized 
3D character animation (10 transitions). Target: Stand#1.Hit#45, 10 metres, 250 frames. 1 Introduction 
Multiple view reconstruction of human performance as a 3D video has advanced to the stage of capturing 
detailed non-rigid dynamic surface shape and appearance of the body, clothing and hair dur­ing motion 
[Aguiar et al. 2008; Starck and Hilton 2007]. Full 3D video scene capture holds the potential to create 
truly realistic syn­thetic animated content by reproducing the dynamics of shape and appearance currently 
missing from marker-based motion capture. However, the acquisition results are in an unstructured volumetric 
or mesh approximation of the surface shape at each frame with­out temporal correspondence, which makes 
the reuse of this kind of data more challenging than conventional mocap data. In this pa­per, we introduce 
a framework that automatically constructs motion graphs for 3D video sequences and synthesizes novel 
animations to best satisfy user speci.ed constraints on movement, location and timing. 2 Methodology 
The framework comprises two stages: pre-processing the database of the 3D video sequences into a surface 
motion graph; and mo­tion synthesis by optimizing the graph-path to satisfy user-de.ned constraints and 
minimize the transition cost. The surface motion graph represents the possible transitions which is analogous 
to mo­tion graphs introduced by [Kovar et al. 2002] for skeletal motion capture sequences. Skeletal motion 
capture data has known tem­poral correspondence allowing the de.nition of similarity metrics to identify 
transitions that will not cause unnatural motion. Accu­rate estimation of temporal surface correspondence 
for 3D video sequences of human motion is an open-problem [Vlasic et al. 2008; Aguiar et al. 2008]. Current 
approaches are computationally ex­pensive and require manual intervention to obtain correct corre­spondences 
for complex non-rigid surface motion. In this paper, transition points between 3D video sequences are 
identi.ed without temporal correspondence using a volumetric temporal shape simi­larity metric which 
measures similarity in both 3D shape and mo­tion [Huang et al. 2007]. The database of 3D video sequences 
is structured as a two-level di­rected surface motion graph (SMG). In the higher-level, each node represents 
a motion and each edge a transition. In the lower-level, each node represents a frame serving as a possible 
transition point and each edge a sequence of frames connecting them. The cost of *e-mail: p.huang@surrey.ac.uk 
a path through the surface motion graph is de.ned according to the total transition cost Cs, location 
cost Cd and time cost Ct. The op­timal path minimizes this cost. Given a path F the total distance, time 
and smoothness are computed independently and combined together as a weighted sum. The optimal path Fopt 
can be found by minimizing the combination: Fopt = arg min {Cs(F)+wdCd(F)+wtCt(F)} (1) F where wd and 
wt are weights for distance and time constraints re­spectively. The path F can be decomposed into paths 
without loops l0 between the start and end key-frame and a set of loops for each path {li},i =1..Nl. 
The task then becomes to .nd an optimal set of integer number {ni}opt corresponding to repetitions of 
loops which minimizes (1): Nl {ni}opt = arg min X{Cs(li)+wdCd(li)+wtCt(li)}· ni (2) {ni} i=0 This optimisation 
can be solved ef.ciently as an Integer Linear Pro­gramming problem. Results demonstrate that concatenative 
synthe­sis of novel sequences accurately satisfy the user constraints and produce motions which preserve 
the detailed non-rigid dynamics of clothing and loose hair. Figure 1 shows an example of synthe­sized 
3D character animation from a public available database of 3D video [Starck and Hilton 2007] using view-dependant 
render­ing. References AGUIAR, E., STOLL, C., THEOBALT, C., AHMED, N., SEIDEL, H.-P., AND THRUN, S. 
2008. Performance capture from sparse multi-view video. ACM Trans. Graph. 27, 3, 1 10. HUANG, P., STARCK, 
J., AND HILTON, A. 2007. A study of shape similarity for temporal surface sequences of people. In 3DIM 
07: Proceedings of the Sixth International Conference on 3-D Digital Imaging and Modeling, IEEE Computer 
Society, Washington, DC, USA, 408 418. KOVAR, L., GLEICHER, M., AND PIGHIN, F. 2002. Motion graphs. In 
SIGGRAPH 02: Proceedings of the 29th annual conference on Computer graphics and inter­ active techniques, 
ACM Press, New York, NY, USA, vol. 21, 473 482. STARCK, J., AND HILTON, A. 2007. Surface capture for 
performance-based anima­tion. IEEE Computer Graphics and Applications 27, 3, 21 31. VLASIC, D., BARAN, 
I., MATUSIK, W., AND POPOVI C´, J. 2008. Articulated mesh animation from multi-view silhouettes. ACM 
Trans. Graph. 27, 3, 1 9. Copyright is held by the author / owner(s). SIGGRAPH 2009, New Orleans, Louisiana, 
August 3 7, 2009. ISBN 978-1-60558-726-4/09/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1598047</article_id>
		<sort_key>570</sort_key>
		<display_label>Article No.</display_label>
		<display_no>57</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>57</seq_no>
		<title><![CDATA[Non-linear aperture for stylized depth of field]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1597990.1598047</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1598047</url>
		<abstract>
			<par><![CDATA[<p>We introduce in this paper <i>non-linear apertures</i> that produce stylized depth of field from lightfield data. A non linear aperture is similar in spirit to the conventional aperture of a camera, except that it replaces the blur of shallow depth of field by a more complex detail removal filtering. The resulting non-realistic apertures produce stylized images where the amount of abstraction varies automatically with depth.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.4.8</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>A.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010225.10010227</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision tasks->Scene understanding</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P1617746</person_id>
				<author_profile_id><![CDATA[81314487615]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Adrien]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bousseau]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[INRIA / Grenoble University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Ng, R., Levoy, M., Br&#233;dif, M., Duval, G., Horowitz, M., and Hanrahan, P. 2005. Light field photography with a hand-held plenoptic camera. Tech. rep.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1142018</ref_obj_id>
				<ref_obj_pid>1141911</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Winnem&#246;ller, H., Olsen, S. C., and Gooch, B. 2006. Real-time video abstraction. <i>ACM TOG (Proc. of SIGGRAPH 2006) 25</i>, 3.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Non-Linear Aperture for Stylized Depth of Field Adrien Bousseau -INRIA / Grenoble University (a) Pinhole 
(b) Average (c) Bilateral (d) Median Figure 1: Compared to a traditional aperture that blurs out of focus 
points (b), non-linear apertures produce depth dependent non photo­realistic simpli.cations. For example 
a bilateral .lter smooths out details while preserving contours (c) and a median .lter simpli.es out 
of focus shapes (d). Abstract We introduce in this paper non-linear apertures that produce stylized depth 
of .eld from light.eld data. A non linear aperture is similar in spirit to the conventional aperture 
of a cam­era, except that it replaces the blur of shallow depth of .eld by a more complex detail removal 
.ltering. The resulting non-realistic apertures produce stylized images where the amount of abstraction 
varies automatically with depth. 1 Introduction and Related Work Shallow depth of .eld is widely used 
by photographers to blur the background and isolate the object of interest in a picture. On con­ventional 
cameras, the amount of blur of out of focus points of the scene is controled by the size of the aperture 
and the distance of the objects from the plane in focus. Light.eld cameras have been pro­posed to allow 
a user to vary the size of the aperture and the plane in focus after the shot [Ng et al. 2005]. In the 
.eld of Non Photorealistic Rendering, similar attention gra­bing effects are obtained by removing the 
details of background objects with various abstraction .lters. The use of different .lters allows a greater 
variety of styles compared to the conventional out of focus blur of realistic photography. For instance, 
Winnem¨oller et al. [2006] applies a bilateral .lter on images and videos to ob­tain a cartoon-like abstraction. 
However, in order to obtain depth dependent level of abstraction, a depth map is required. Such depth 
information can be painted by a user, which is tedious and not ac­curate, or estimated by vision algorithms, 
which is prone to errors. We propose to combine abstraction and refocusing, and use non­linear .lters 
such as the bilateral .lter in place of the average .lter traditionally used in light.eld refocusing. 
Our approach is simple to integrate in existing light.eld viewers and allows depth dependant image abstraction 
without the need for explicit depth information. 2 Linear and Non-Linear Apertures Box Filtering In 
traditional cameras, out of focus blur is pro­duced by the optical integration of incoming light rays 
over the camera aperture. In light.eld refocusing, similar blur can be simu­lated by averaging the light 
rays over the synthetic aperture: 1 X p = W Lu,v,s,t u,v where p is the resulting blurred pixel, u and 
v are the coordinates of the aperture samples, s and t are the coordinates of the intersection of the 
aperture rays with the focal plane, L is the light.eld and W is a normalization factor equal to the number 
of samples in the aperture. Figure 1(b)1 illustrates this realistic refocusing. In the 1Light.eld data 
from http://light.eld.stanford.edu/ following, we propose to modify the synthetic aperture equation to 
obtain varied stylization effects. We examplify here the median and bilateral .lter. Median Filtering 
The .rst non-linear aperture we propose con­sists in substituting the integration over the aperture by 
a median .lter that selects the median value of the incoming light rays. Al­though such aperture could 
not be manufactured in reality, it can be simulated by the following synthetic aperture: p = medianu,v{Lu,v,s,t} 
The resulting abstraction .lter removes small details and simpli.es shapes, as illustrated in Figure 
1(d). Note that the amount of ab­straction automatically increases with the distance to the plane in 
focus. This .lter can be used as a basis for more complex styliza­tion such as watercolor where the shape 
simpli.cation mimics the use of a coarse paint brush. Bilateral Filtering Finally, we propose to consider 
the aperture as the spatial support of a bilateral .lter. The corresponding synthetic aperture can be 
expressed as: 1 X p = Wp u,v Gsr (|Lu,v,s,t - Lu0,v0,s,t|) Lu,v,s,t X with Wp = Gsr (|Lu,v,s,t - Lu0,v0,s,t|) 
u,v where Gsr is a gaussian function de.ning the range support, u0 and v0 are the coordinates of the 
central sample of the aperture. This .lter removes the low contrast features of the image while preserving 
the strong edges (Figure 1(c)). The spatial support of the bilateral .lter varies automatically with 
depth. 3 Conclusion and Future Work We have presented in this paper a simple yet effective method to 
produce depth dependent stylized abstraction of photographs using light.eld data. We believe that similar 
light.eld stylization methods will be of great interest in the near future when light.eld cameras will 
become available to the average consumer. References NG, R., LEVOY, M., BR´ EDIF, M., DUVAL, G., HOROWITZ, 
M., AND HANRAHAN, P. 2005. Light .eld photography with a hand-held plenop­tic camera. Tech. rep. WINNEM 
¨2006. Real-time OLLER, H., OLSEN, S. C., AND GOOCH, B. video abstraction. ACM TOG (Proc. of SIGGRAPH 
2006) 25, 3. Copyright is held by the author / owner(s). SIGGRAPH 2009, New Orleans, Louisiana, August 
3 7, 2009. ISBN 978-1-60558-726-4/09/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1598048</article_id>
		<sort_key>580</sort_key>
		<display_label>Article No.</display_label>
		<display_no>58</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>58</seq_no>
		<title><![CDATA[Shooting up]]></title>
		<subtitle><![CDATA[a trip through the camera structure of up]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1597990.1598048</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1598048</url>
		<abstract>
			<par><![CDATA[<p>In visual story telling it is essential to not only have beautifully composed shots but meaningful compositions as well. A camera plan is crucial to achieving this goal. A camera plan maps out the camera's visual structure and progression that parallels the story and follows the emotional arc of the main characters. This presentation will take you through the aesthetic thought process in creating and implementing Up's camera structure.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.8</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>A.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010507</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Graphics recognition and interpretation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P1617747</person_id>
				<author_profile_id><![CDATA[81442599101]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Patrick]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Pixar Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Shooting UP: A Trip through the Camera Structure of UP Patrick Lin* Pixar Animation Studios In visual 
story telling it is essential to not only have beautifully composed shots but meaningful compositions 
as well. A camera plan is crucial to achieving this goal. A camera plan maps out the camera's visual 
structure and progression that parallels the story and follows the emotional arc of the main characters. 
This presentation will take you through the aesthetic thought process in creating and implementing Up's 
camera structure. 1. Story Analysis We started by analyzing the story to determine the ups and downs 
of our characters. We identi.ed scenes that were exciting or calm, funny or poignant, terrifying or reassuring, 
to guide our decisions on how to approach the shooting and staging. Each scene was rated with a number 
on a scale of 0 to 10, with 0 being the lowest emotional state and 10 the highest. A number for a particular 
scene was not as important as its relationship to other scenes in the .lm. This gave us a bird's eye 
view of the big picture. A road map showing us where we were coming from and where we needed to go. 
 2. Complimenting Production Design Besides Story, Up's camera plan was also inspired by Ricky Nierva's 
production design. Squares and circles were the seeds of Up's design. For example, Carl is essentially 
a square while Russell is a circle. These symbols came to represent the past and the future, pessimism 
and optimism, the static and the dynamic. In order to have a camera plan that complemented the production 
design, we explored the static and dynamic language through the camera. *e-mail: lin@pixar.com 3. Static 
vs. Dynamic We divided layout into its four basic components: Lens, Camera motion, Framing, and Staging 
and examined each of their properties to identify our camera palette, which ranged from static to dynamic. 
Lens Long lens vs. Wide Lens  Heavy Depth of Field vs. Sharp Focus  Heavy Lens Distortion vs. Light 
Lens Distortion  Repetitive lens vs. Variable lens Camera Motion  Locked off vs. Unlocked vs. Hand 
held  Track/Boom vs. Pan/Tilt/Zoom  2D camera move vs. 3D camera move Framing  Squash &#38; Squeeze 
vs. No Squash &#38; Squeeze  Frame Division vs. No Frame Division  Frame within Frame vs. No Frame 
within Frame  Vertical lines (Level) vs. Diagonal lines (Dutch)  Down angle vs. Up angle Staging 
 Screen direction left vs. screen direction right  Screen direction down vs. up  Character introduction 
on Screen Left vs. Screen Right  None of these camera techniques are revolutionary but the idea is to 
use them in a structured and purposeful way to give the shots and staging a deeper meaning. A simple 
example is long vs wide lens. The longer the lens, the more the visual elements are spatially compressed 
and .attened, which translates into a more static composition. On the other hand, a wide lens exaggerates 
space, creating a more dynamic composition. By referring to our story road map, we tag this camera property 
to scenes that are emotionally low (after Ellie's death) to emotionally high (Carl letting go of his 
past in Paradise Falls). With this guide of Static vs. Dynamic serving as the dictionary of Up's .lm 
language, we assessed each scene based on the emotional and story arcs, and assigned different properties 
to create a range of static and dynamic cameras. It allowed us to extend the story and character progression 
while remaining extremely .exible to story changes. 4. Implementation With the understanding of the 
story road map and the camera plan, the bulk of this presentation is to step through key sequences and 
shots explaining in detail how the static and dynamic modes were applied, and discuss their successes 
and failures. Copyright is held by the author / owner(s). SIGGRAPH 2009, New Orleans, Louisiana, August 
3 7, 2009. ISBN 978-1-60558-726-4/09/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1598049</article_id>
		<sort_key>590</sort_key>
		<display_label>Article No.</display_label>
		<display_no>59</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>59</seq_no>
		<title><![CDATA[Automatic colorization of grayscale images using multiple images on the web]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1597990.1598049</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1598049</url>
		<abstract>
			<par><![CDATA[<p>Colorization is the process of adding color to monochrome images and video. It is used to increase the visual appeal of images such as old black and white photos, classic movies, and scientific visualizations. Since colorizing grayscale images involves assigning three-dimensional (RGB) pixel values to an image whose elements are characterized by one feature (luminance) only, the colorization problem does not have a unique solution. Hence, human interaction is typically required in the colorization process. Although existing colorization methods attempt to minimize the amount of user intervention, they require users to manually sellect a similar image to the target image or input a set of color seeds for different regions of the target image. In this paper, we present an entirely automatic colorization method using multiple images collected from the Web. The method generates various and natural colorized images from an input monochrome image by using the information of the scene structure.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.6</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>A.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P1617748</person_id>
				<author_profile_id><![CDATA[81442610726]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Yuji]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Morimoto]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617749</person_id>
				<author_profile_id><![CDATA[81320495523]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Yuichi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Taguchi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617750</person_id>
				<author_profile_id><![CDATA[81100095332]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Takeshi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Naemura]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1276382</ref_obj_id>
				<ref_obj_pid>1276377</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Hays, J., and Efros, A. A. 2007. Scene completion using millions of photographs. <i>ACM Trans. Graph (SIGGRAPH 2007) 26</i>, 3.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>566576</ref_obj_id>
				<ref_obj_pid>566654</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Welsh, T., Ashikhmin, M., and Mueller, K. 2002. Transferring color to greyscale images. <i>ACM Trans. Graph (SIGGRAPH 2002) 21</i>, 3, 277--280.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Automatic colorization of grayscale images using multiple images on the Web Yuji Morimoto* YuichiTaguchi 
The UniversityofTokyo The UniversityofTokyo  Figure 1: Monochrome image Im Figure 2: Automatically 
selected reference images whose composi­ tion are similar to Im 1 Introduction Colorization is the process 
of adding color to monochrome images and video. It is used to increase the visual appeal of images such 
as old black and white photos, classic movies, and scienti.c vi­sualizations. Since colorizing grayscale 
images involves assigning three-dimensional(RGB)pixelvaluestoanimagewhose elements are characterized 
by one feature (luminance) only, the colorization problem does nothaveaunique solution. Hence, human 
interaction is typically required in the colorization process. Although existing colorization methods 
attempt to minimize the amount of user inter­vention, they require users to manually sellect a similar 
image to the target image or input a set of color seeds for different regions of the target image. In 
this paper, we present an entirely automatic colorization method using multiple images collected from 
theWeb. The method generates various and natural colorized images from an input monochrome image by using 
the information of the scene structure.  2 Our Method Our colorization method is entirely automatic 
and outputs multi­ple natural colorized images by using one million images down­loaded from Flickr (http://www..ickr.com/) 
. To colorize an input grayscale image Im (Figure 1) , the method .rst selects reference color images 
(source images) from the image database. It then gen­erates multiple color images by colorizing Im using 
each source image. We select images that have similar scene structure withIm as the source images, because 
images that havesimilar structure are likely tohavea similar color.To .nd the source images from one 
million images, we used gist scene descriptor, which is a feature vector to describe the global scene 
in lower dimension. The gist scene descriptor aggregates oriented edge responses at multiscales into 
very coarse spatial bins.We useda gist scene descriptorbuilt from 6 oriented edge responses at5scales 
aggregated toa4×4spatial resolution and converted one million images to 480 dimensional vectors [Hays 
and Efros 2007].We calculated the SSD between the gist of Im and every gist of the one million images 
and selected the most similar 100 images. From these images, we used the 20 images that have the most 
similar aspect ratio to Im as the source images (Figure 2). *E-mail: morimoto@nae-lab.org Figure 3: 
Result of automatic col­orization of Im (1) Next, we colorize Im using these source images. For this 
pur­pose, we modi.edWelsh et al. s method [2002], which colorizes a grayscale image by transferring color 
from a source color image. In order to transfer chromaticity values from a source image to Im, each pixel 
in Im must be matched to a pixel in the source image. The comparison is based on the luminance value 
and the standard deviation of the luminance in a pixel neighborhood with size of 5×5pixels. Since our 
method selects a source image that is similar to Im not only in color trendbut also in scene structure, 
we can also use the structure information for colorization. For example, since two images have similar 
structure, a color at the top left of the image is likely to appear in the same part of the other image. 
We embedded this information in the pixel matching algorithm by penalizing the matching score when the 
location between the two pixels is separated. Once the best matching pixel is found, chro­maticity values 
are transferred to the target pixel. To transfer only chromaticity values, images are converted to the 
lc( color space. The lc( color space consists of an achromatic luminance channel (l)and two chromatic 
channelsc and (. This color space mini­mizes the correlation between the three coordinate axes of the 
color space. By transferring c and (, we can transfer only chromaticity values without altering the luminance. 
As a result, our method produces multiple colorized images (Fig­ures3and4), from which users can choose 
the one that theylike. 3 Conclusions and Future Work We presented a novel colorization method using 
one million im­ages collected from theWeb. Our methodis entirely automatic and produces multiple natural 
colorized images by exploiting the simi­larity of the scene structure. Currently, the limitation of our 
method is that it sometimes produces unnatural colorized images, caused by source images that are structurally 
similarbut semantically dif­ferent. To solve this problem, we are considering combining the obtained 
color images to generate a more robust result. References HAYS, J., AND EFROS, A. A. 2007. Scene completion 
using millions of photographs. ACM Trans. Graph (SIGGRAPH 2007) 26, 3. WELSH,T.,ASHIKHMIN,M., AND MUELLER,K. 
2002.Trans­ferring color to greyscale images. ACM Trans. Graph (SIG-GRAPH 2002) 21, 3, 277 280. Figure 
4: Result of automatic col­orization of Im (2) Copyright is held by the author / owner(s). SIGGRAPH 2009, 
New Orleans, Louisiana, August 3 7, 2009. ISBN 978-1-60558-726-4/09/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1598050</article_id>
		<sort_key>600</sort_key>
		<display_label>Article No.</display_label>
		<display_no>60</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>60</seq_no>
		<title><![CDATA[PhotoSketch]]></title>
		<subtitle><![CDATA[a sketch based image query and compositing system]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1597990.1598050</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1598050</url>
		<abstract>
			<par><![CDATA[<p>We introduce a system for progressively creating images through a simple sketching and compositing interface. A large database of over 1.5 million images is searched for matches to a user's binary outline sketch; the results of this search can be combined interactively to synthesize the desired image. We introduce image descriptors for the task of estimating the difference between images and binary outline sketches. The compositing part is based on graph cut and Poisson blending. We demonstrate that the resulting system allows generating complex images in an intuitive way.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.4</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>A.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P1617751</person_id>
				<author_profile_id><![CDATA[81336488955]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Mathias]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Eitz]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[TU Berlin]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617752</person_id>
				<author_profile_id><![CDATA[81309509166]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Kristian]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hildebrand]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[TU Berlin]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617753</person_id>
				<author_profile_id><![CDATA[81309487978]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Tamy]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Boubekeur]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Telecom ParisTech & LTCI CNRS]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617754</person_id>
				<author_profile_id><![CDATA[81100235480]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Marc]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Alexa]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[TU Berlin]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1015718</ref_obj_id>
				<ref_obj_pid>1015706</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Agarwala, A., Dontcheva, M., Agarwala, M., Drucker, S., Colburn, A., Curless, B., Salesin, D., and Cohen, M. 2004. Interactive digital photomontage. <i>ACM Transactions on Graphics (TOG) 23</i>, 3, 294--302.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1572747</ref_obj_id>
				<ref_obj_pid>1572741</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Eitz, M., Hildebrand, K., Boubekeur, T., and Alexa, M. 2009. A descriptor for large scale image retrieval based on sketched feature lines. In <i>Sketch-Based Interfaces and Modeling, to appear</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1280759</ref_obj_id>
				<ref_obj_pid>1280720</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Gavilan, D., Saito, S., and Nakajima, M. 2007. Sketch-to-collage. In <i>ACM SIGGRAPH 2007 posters</i>, 35.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Hirata, K., and Kato, T. 1992. Query by visual example-content based image retrieval. In <i>Advances in Database Technology</i>, Springer, 56--71.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 PhotoSketch:ASketchBased Image Queryand Compositing System Mathias Eitz1, Kristian Hildebrand1,Tamy 
Boubekeur2 and Marc Alexa1 1TU Berlin, 2TelecomParisTech&#38;LTCI CNRS Figure 1: PhotoSketch: realistic 
images from user sketches using large image database queries and variational image compositing. 1 Introduction 
We introduce a system for progressively creating images through a simple sketching and compositing interface. 
Alarge database of over 1.5 million images is searched for matches to a user s binary outline sketch; 
the results of this search can be combined interac­tivelyto synthesizethe desiredimage.Weintroduceimage 
descrip­tors for the task of estimating the difference between images and binary outline sketches. The 
compositing part is based on graph cut and Poisson blending. We demonstrate that the resulting system 
allows generating complex images in an intuitive way. 2 Overview, work.ow, interface The aim of PhotoSketch 
is to provide the user with an intuitive in­terface for composing new images from parts of existing images. 
The fundamental principle is a loop, in which the user queries a large database of images for matches 
to his outline sketch, chooses a query result, extracts a region from the chosen image, and pastes it 
into the current image (see Figure 1). A main question is how to query the database. It turns out that 
sketching binary feature linesisa naturalwayto quickly communi­catethementalconceptofashape[HirataandKato1992]. 
Clearly, sketching only feature lines allows matching images with different colors: this has the bene.t 
of generating more matches but will also include undesired ones. We compensate the latter problem by 
clustering the query result so that each cluster represents similarly colored images. In particular, 
the following steps are performed in one loop of the PhotoSketch image generation process. Binary outlines 
aresketchedbythe userto de.nethe desired shape of the content and used to query the large image database. 
The re­sult of the query for shapes is a small collection of pictures with similar structurebut spanninga 
potentiallylarge rangeof hues.In our system we typically query for approximately 50 to 100 im­ages. In 
order to provide the user with a mechanism for quickly .nding the correctly colored image in the result 
set, we cluster the search results accordingtoa color histogram descriptor intoasmall number of clusters 
(typically in the order of .ve to ten). Then the user can select the cluster with the desired color and 
choose from this cluster the image best matching the shape outlined in the sketch. Once a query result 
is chosen, the image is rendered half­transparentlyover the current composition. The user can then trans­formthenewimageuntilthe 
desiredpartisintheright position.We provide the user with a stroke based interface for quickly marking 
the desired region and seamlessly merging the selection with the target image [Agarwala et al. 2004]. 
Using GraphCut, we optimize the selection such that both a seamless composition with the exist­ing background 
image and an exact selection of the desired image region is achieved. We further reduce possible seams 
by perform­ing the merging step in the gradient domain. Once the selection is merged into one image, 
the user continues with sketching the next desired object or part. This loop repeats until the user is 
satis.ed with the result. The work.ow is visualized in Figure 1. 3 Sketch-vs-Image Descriptors Clearly, 
the main type of information in the user input is the direc­tion of the stroke relative to its position. 
This information relates besttothe directionof gradients.Wehave designedimage descrip­tors based on cell 
feature descriptors to address this problem [Eitz et al. 2009]. These descriptors capture the main directions 
in each part of the image and are computed for all images in the database in an of.ine process. During 
interaction, the user sketch provides direction information for each spatial region in the sketch. The 
de­scriptor generated from it is then compared against all descriptors in the database and the images 
corresponding to the best matching descriptors are displayed to the user. The resulting light weight 
descriptors (9 Kb per image) can all be loaded into a linear array in main memory. As a consequence, 
the image queries arefast enough (0.4 to 3.5 seconds ona Apple MacPro, Xeon 2.8 GHz QuadCore, 32 GB Ram) 
to allow users to interactively compose realistic images from millions of pictures. Compared to existing 
approaches, e.g. [Gavilan et al. 2007], Pho­toSketch enables interactive image creation from large databases, 
avoids the need for presegmentedimages and provides an intuitive way to de.ne a region of interest on 
the .y. References AGARWALA, A., DONTCHEVA, M., AGRAWALA, M., DRUCKER, S., COLBURN, A., CURLESS, B., 
SALESIN, D., AND COHEN, M. 2004. Interactive digital photomontage. ACM Transactions on Graphics (TOG) 
23, 3, 294 302. EITZ,M.,HILDEBRAND,K.,BOUBEKEUR,T., AND ALEXA,M. 2009.Adescriptor for large scale image 
retrieval based on sketched feature lines. In Sketch-Based Interfaces and Modeling, to appear. GAVILAN, 
D., SAITO, S., AND NAKAJIMA, M. 2007. Sketch-to-collage. In ACM SIGGRAPH 2007 posters, 35. HIRATA, K., 
AND KATO, T. 1992. Query by visual example-content based image retrieval. In Advances in Database Technology, 
Springer, 56 71. Copyright is held by the author / owner(s). SIGGRAPH 2009, New Orleans, Louisiana, August 
3 7, 2009. ISBN 978-1-60558-726-4/09/0008   
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1598051</article_id>
		<sort_key>610</sort_key>
		<display_label>Article No.</display_label>
		<display_no>61</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>61</seq_no>
		<title><![CDATA[Houdini in a games pipeline]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1597990.1598051</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1598051</url>
		<abstract>
			<par><![CDATA[<p>After the last generation of Computer Game Consoles was introduced audiences have been expecting bigger and more visually complex games, while the task of creating these visuals is still a very time consuming process. This increasing demand has greatly increased the workload for artists while production schedules have mostly stayed the same.</p> <p>During the initial phase of production on the Sony Playstation 3 title "Killzone 2" developer Guerrilla Games decided to look into SideFX Houdini for it's procedural approach to 3d content creation.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>K.8.0</cat_node>
				<descriptor>Games</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>A.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010476.10011187.10011190</concept_id>
				<concept_desc>CCS->Applied computing->Computers in other domains->Personal computers and PC applications->Computer games</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10011007.10010940.10010941.10010969.10010970</concept_id>
				<concept_desc>CCS->Software and its engineering->Software organization and properties->Contextual software domains->Virtual worlds software->Interactive games</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002951.10003227.10003251.10003258</concept_id>
				<concept_desc>CCS->Information systems->Information systems applications->Multimedia information systems->Massively multiplayer online games</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P1617755</person_id>
				<author_profile_id><![CDATA[81442619700]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Paulus]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bannink]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Guerrilla Games]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 HOUDINI IN A GAMES PIPELINE Paulus Bannink Guerrilla Games paulus@guerrilla-games.com  1 Introduction 
 After the last generation of Computer Game Consoles was introduced audiences have been expecting bigger 
and more visually complex games, while the task of creating these visuals is still a very time consuming 
process. This increasing demand has greatly increased the workload for artists while production schedules 
have mostly stayed the same. During the initial phase of production on the Sony Playstation 3 title Killzone 
2 developer Guerrilla Games decided to look into SideFX Houdini for it s procedural approach to 3d content 
creation.  2 Destructible Environments Killzone 2 is a first person shooter game with a SciFi theme 
that relies heavily on the environment to give the player a sense of emersion in the game world. This 
means that the environment should react to bullet and projectile impacts in a believable way, so the 
first problem that we tried to solve with Houdini was that of destructible objects. The pipeline that 
we ended up with was one where we would export a finished environment asset from Autodesk Maya 8.5 into 
Houdini where a procedural process controlled by our Houdini artists would generate a destructible version 
of the same asset. This meant constructing separate pieces out of the original object complete with interior 
detail shaders and uv coordinates. For some asset types (concrete objects) it would also create extra 
geometry to add more detail (metal bars). Where creating the destructed version of a typical game asset 
would take days to do by hand we can now produce them within hours.  3 Dramatic Weather Effects The 
planet that Killzone 2 takes place on is permanently attacked by massive lightning storms. For this we 
were again able to cut down on production time by using Houdini to create a massive set of lighting objects 
that could be triggered during the game. The Houdini tool was able to look at the level geometry and 
find the logical points in the environment where lighting would strike and generate an appropriate mesh 
that we could export to the game. For detailed areas it would also setup a chain of lighting effects 
to go around the environment.       4 Production Integration Because Houdini is normally not 
used for creating computer game content, people have to rethink the production process to take the new 
possibilities into account. This turned out to be one of the harder challenges in the end. In order to 
overcome this we had to constantly be aware of any situation where people might be using more traditional 
and time-consuming ways to solve content creation instead of involving the houdini artists. By doing 
this we also ended up using houdini to solve the following issues : . Pre-Baked cloth and wire animation 
 . Organic terrain creation  . Correct Object Scattering  . Impact Crater Creation  In the end using 
Houdini in our game pipeline worked so well that during the production of Killzone 2 we hired an additional 
Houdini Artist to help increase the work we could do with these tools.  5 Future For our next project 
we are integrating Houdini even closer into our pipeline and looking into introducing some of our more 
"traditional" artists to the software. 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1598052</article_id>
		<sort_key>620</sort_key>
		<display_label>Article No.</display_label>
		<display_no>62</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>62</seq_no>
		<title><![CDATA[Spore API]]></title>
		<subtitle><![CDATA[accessing a unique database of player creativity]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1597990.1598052</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1598052</url>
		<abstract>
			<par><![CDATA[<p>In the video game Spore, players create their own creatures, buildings, and vehicles using intuitive yet powerful editing tools. During the course of game play, these creations are automatically published to our servers and made available in our online catalog for browsing, downloading, and sharing. Players can add tags and write descriptions about their creations, as well as assign ratings, write comments, organize creations into feeds called Sporecasts, or download other players' creations into their own game. Players can also choose to organize the flow of creations into their game by subscribing to Sporecasts and adding other players as buddies. Since its release in September 2008, Spore players have created and published millions of creations resulting in a unique database of player imagination and creativity.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>H.2.8</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>A.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002951.10003227</concept_id>
				<concept_desc>CCS->Information systems->Information systems applications</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P1617756</person_id>
				<author_profile_id><![CDATA[81100038268]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Dan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Moskowitz]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Maxis, Electronic Arts]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617757</person_id>
				<author_profile_id><![CDATA[81335497377]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Shalin]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Shodhan]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Maxis, Electronic Arts]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617758</person_id>
				<author_profile_id><![CDATA[81442618848]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Michael]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Twardos]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Maxis, Electronic Arts]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Spore API: Accessing a Unique Database of Player Creativity Dan Moskowitz Shalin Shodhan Michael Twardos 
Maxis, Electronic Arts Maxis, Electronic Arts Maxis, Electronic Arts dmoskowitz@maxis.com sshodhan@maxis.com 
mtwardos@ea.com 1 Introduction In the video game Spore, players create their own creatures, buildings, 
and vehicles using intuitive yet powerful editing tools. During the course of game play, these creations 
are automatically published to our servers and made available in our online catalog for browsing, downloading, 
and sharing. Players can add tags and write descriptions about their creations, as well as assign ratings, 
write comments, organize creations into feeds called Sporecasts, or download other players creations 
into their own game. Players can also choose to organize the flow of creations into their game by subscribing 
to Sporecasts and adding other players as buddies. Since its release in September 2008, Spore players 
have created and published millions of creations resulting in a unique database of player imagination 
and creativity. 2 The Spore API In addition to a player s creations being shared on the Spore website, 
the data is also accessible with a public web service called the Spore API. For each creation, the web 
service provides information about its construction, morphology, color, and other statistics. This information 
is sufficient to reconstruct the asset on any other machine running Spore. The API also provides rich 
data about relationships between users and creations, such as lists of creations per user, lists of buddies 
per user, or the assets rated highest by the community. This web service enables developers to build 
applications that use Spore data in new and unique ways. In this talk, we provide examples of applications 
and visualizations that use the Spore API to explore this data. Figure 1: Two examples of creature taxonomy 
visualizations Creature Taxonomy Inside the creature editor, each rigblock assigns the creature a corresponding 
game play attribute and level, such as carnivorous , bite +3 , or speed +2 . Using the web service, which 
provides these abilities and other statistics about a creature, we can classify creatures based on behavior, 
diet, cost, complexity, and size. We use these attributes as nodes in a binary tree to sort a vast range 
of creatures into appropriate leaf nodes. This reveals the amount of creations with specific attribute 
combinations. We experiment with different formats for visualizing this data. (Figure 1) These visualizations 
have been fed back to the community as a challenge for them to make creatures with rarer combinations 
of traits. 4 Asset Reconstruction and Breeding Using the Spore API, we show that it is possible to reconstruct 
Spore creations in a web browser without the core data included on the Spore game disc. Our application 
renders Spore creatures in a stylized 2D form using Flash. (Figure 2) An artist created simplified 2D 
versions of each rigblock, which we position and color appropriately based off the data in the XML. The 
ubiquity of the web means that we are able to create a low barrier of entry for developers wishing to 
showcase Spore creatures inside their own applications. One such application is the Creature Breeder 
which combines traits of two creatures to form unique mixed-breed offspring. The results of this application 
are visualized using the stylized 2D look. Figure 2: Using the XML to reconstruct creatures in 2D  
5 Creation Lineage and Aesthetic Matching When a player uploads a modification to a Spore creation, the 
upload transaction contains the ID of its predecessor, which is accessible in the API. This allows us 
to look at the evolutionary paths of creatures that progress through the game, as well as observe patterns 
of modification of the most popular creations on the website. We experiment with interesting ways of 
visualizing this data and tracking lineage trends. Another area of work is aesthetic matching. Given 
one creation as a target, we experiment with ways of searching our database for other creations that 
visually match the look and/or theme of the target creation. This is a complex problem given the staggering 
number of creations stored in the database. To aid in the search, certain rigblocks have been pre-tagged 
with aesthetic markers allowing us to more easily match, for example, vehicles made with the same style 
of chassis. Copyright is held by the author / owner(s). SIGGRAPH 2009, New Orleans, Louisiana, August 
3 7, 2009. ISBN 978-1-60558-726-4/09/0008   
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1598053</article_id>
		<sort_key>630</sort_key>
		<display_label>Article No.</display_label>
		<display_no>63</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>63</seq_no>
		<title><![CDATA[Creativity in video game design as pedagogy]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1597990.1598053</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1598053</url>
		<abstract>
			<par><![CDATA[<p>The study of children's creative thinking as they use technology to produce works of art, has only gotten humble attention in the literature (Loveless, 2002; Resnick, 2006). Although within the learning sciences, and in the field of new literacy studies, a growing number of compelling articles have made attempts to document the creative acts of young people using technology (Kafai, 2006; Sefton-Green, 2006; Gustavson, 2007; Squire, 2008), it is evident that there is little agreement on how to clearly conceive of this thing we call creativity. Quite often, when documenting the practices related to making things with technology, creativity is defined as the process of active engagement, some call it play (Lindqvist, 2003).</p> <p>Nevertheless, as we can identify through much of the data, not all children engage in the process of design and production in the same way, or to the same degree. This highlights how paramount the role and the thinking of the creative agent is to the act, and why it is in need of further investigation by psychologists, social researchers, designers and educators (Middleton, 2007).</p> <p>Additionally, it is vital to recognize that the technology plays a unique role in the process of creative expression (by scaffolding and supporting design with abstract representations) (Dede et. al, 1997; Bransford, 1999; Black, 2006). How then, can we say that all of these acts are creative? Furthermore, there is great and ever growing diversity of what new things individuals and groups actually make with technology - from video games, to blogs, to music. Some maintain that with the study of creativity in children all products that a child or youth produces should be considered creative (Amabile, 1996), despite acknowledging that creativity must be cultivated and developed. Can we assume that all the products young people produce with technology are creative?</p> <p>We will present findings from a study that undertook an examination of the iterative design process, and how certain cognitive skills relate to creativity. The study examines a group of 4<sup>th</sup> grade boys, ages 9--11, who participated in a twelve week after school video game design course at an elementary school in New York City. The course focused on the design of video games, with curriculum that introduced computer programming, mathematics, storytelling, and the digital arts. This effort was also grounded in the theory of media literacy, examining how video games and interactive texts extend the new literacy paradigm (Gee, 2003; Leu et. al, 2004). Both qualitative and quantitative data analysis was employed to examine artifacts produced by all subjects. Observational methods of research, including field notes, student interviews, were employed to examine the social relationships and the dynamic learning environment. Additionally, student work including storyboards, art and video games code, serve as a major source of data.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>K.8.0</cat_node>
				<descriptor>Games</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>A.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010476.10011187.10011190</concept_id>
				<concept_desc>CCS->Applied computing->Computers in other domains->Personal computers and PC applications->Computer games</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10011007.10010940.10010941.10010969.10010970</concept_id>
				<concept_desc>CCS->Software and its engineering->Software organization and properties->Contextual software domains->Virtual worlds software->Interactive games</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002951.10003227.10003251.10003258</concept_id>
				<concept_desc>CCS->Information systems->Information systems applications->Multimedia information systems->Massively multiplayer online games</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P1617759</person_id>
				<author_profile_id><![CDATA[81442611042]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Ronah]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Harris]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Columbia University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617760</person_id>
				<author_profile_id><![CDATA[81442616353]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Cameron]]></first_name>
				<middle_name><![CDATA[L.]]></middle_name>
				<last_name><![CDATA[Fadjo]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Columbia University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617761</person_id>
				<author_profile_id><![CDATA[81539934956]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Eric]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Carson]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Columbia University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617762</person_id>
				<author_profile_id><![CDATA[81442618403]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Greg]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hallman]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Columbia University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617763</person_id>
				<author_profile_id><![CDATA[81442601070]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Michael]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Swart]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Columbia University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Creativity in Video Game Design as Pedagogy Ronah Harris, Cameron L. Fadjo, Eric Carson, Greg Hallman, 
Michael Swart Institute for Learning Technologies Teachers College, Columbia University rsh2115@columbia.edu, 
clf2110@columbia.edu, eoc2102@columbia.edu, glh2103@columbia.edu, mis2125@columbia.edu Abstract The 
study of children s creative thinking as they use technology to produce works of art, has only gotten 
humble attention in the literature (Loveless, 2002; Resnick, 2006). Although within the learning sciences, 
and in the field of new literacy studies, a growing number of compelling articles have made attempts 
to document the creative acts of young people using technology (Kafai, 2006; Sefton-Green, 2006; Gustavson, 
2007; Squire, 2008), it is evident that there is little agreement on how to clearly conceive of this 
thing we call creativity. Quite often, when documenting the practices related to making things with technology, 
creativity is defined as the process of active engagement, some call it play (Lindqvist, 2003). Nevertheless, 
as we can identify through much of the data, not all children engage in the process of design and production 
in the same way, or to the same degree. This highlights how paramount the role and the thinking of the 
creative agent is to the act, and why it is in need of further investigation by psychologists, social 
researchers, designers and educators (Middleton, 2007). Additionally, it is vital to recognize that the 
technology plays a unique role in the process of creative expression (by scaffolding and supporting design 
with abstract representations) (Dede et. al, 1997; Bransford, 1999; Black, 2006). How then, can we say 
that all of these acts are creative? Furthermore, there is great and ever growing diversity of what new 
things individuals and groups actually make with technology from video games, to blogs, to music. Some 
maintain that with the study of creativity in children all products that a child or youth produces should 
be considered creative (Amabile, 1996), despite acknowledging that creativity must be cultivated and 
developed. Can we assume that all the products young people produce with technology are creative? We 
will present findings from a study that undertook an examination of the iterative design process, and 
how certain cognitive skills relate to creativity. The study examines a group of 4th grade boys, ages 
9 11, who participated in a twelve week after school video game design course at an elementary school 
in New York City. The course focused on the design of video games, with curriculum that introduced computer 
programming, mathematics, storytelling, and the digital arts. This effort was also grounded in the theory 
of media literacy, examining how video games and interactive texts extend the new literacy paradigm (Gee, 
2003; Leu et. al, 2004). Both qualitative and quantitative data analysis was employed to examine artifacts 
produced by all subjects. Observational methods of research, including field notes, student interviews, 
were employed to examine the social relationships and the dynamic learning environment. Additionally, 
student work including storyboards, art and video games code, serve as a major source of data. Copyright 
is held by the author / owner(s). SIGGRAPH 2009, New Orleans, Louisiana, August 3 7, 2009. ISBN 978-1-60558-726-4/09/0008 
 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1598054</article_id>
		<sort_key>640</sort_key>
		<display_label>Article No.</display_label>
		<display_no>64</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>64</seq_no>
		<title><![CDATA[Teaching animation in Second Life]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1597990.1598054</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1598054</url>
		<abstract>
			<par><![CDATA[<p>In the Spring of 2009, the Computer Science Department at the Rochester Institute of Technology gave an experimental offering of their course: <i>Computer Animation: Algorithm and Techniques</i>, entirely in the virtual world of Second Life. The goal of the experiment was to discover whether worlds like Second Life could be used effectively to teach computer graphics in a distance learning environment.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Animation</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>A.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P1617764</person_id>
				<author_profile_id><![CDATA[81100612714]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Joe]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Geigel]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Rochester Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1554677</ref_obj_id>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Parent, R. 2008. <i>Computer Animation: Algorithms and Techniques, 2&#60;sup&#62;nd&#60;/sup&#62; Edition</i>, Morgan-Kauffman.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1352403</ref_obj_id>
				<ref_obj_pid>1352383</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Ritzema, T. and Harris, B. 2008. The use of Second Life for distance education. <i>J. Comput. Small Coll</i>. 23, 6 (Jun. 2008), 110--116.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Teaching Animation in Second Life Joe Geigel Rochester Institute of Technology jmg@cs.rit.edu 1. INTRODUCTION 
In the Spring of 2009, the Computer Science Department at the Rochester Institute of Technology gave 
an experimental offering of their course: Computer Animation: Algorithm and Techniques, entirely in the 
virtual world of Second Life. The goal of the experiment was to discover whether worlds like Second Life 
could be used effectively to teach computer graphics in a distance learning environment. Use of Second 
Life for education in general, and for distance learning in particular, has skyrocketed [Ritzema and 
Harris 2008] though very little has been reported on the use these worlds for teaching Graphics and Animation. 
This talk will focus on the unique opportunities and challenges of offering a course in Computer Animation 
in Second Life and explore the possibilities for teaching animation as an online course. 2. THE COURSE 
Computer Animation: Algorithm and Techniques is a course that takes a look at computer animation from 
a programmer s perspective; focusing on both the theory and implementation of fundamental animation algorithms. 
Inspired by the text of the same name [Parent 2008], it covers topics such as quaternions, interpolation, 
dynamics, numerical integration, articulated figure motion, behavioural motion, and particle systems. 
The course is structured around weekly lectures. Students are required to read, summarize, and discuss 
seminal papers introducing each topic and then complete a number of programming assignments based on 
the content presented in lecture. The course culminates with the presentation of a quarter long project 
which is chosen by each student and approved at the start of the quarter. The course has been offered 
annually, using a traditional classroom delivery, since 2002. In these past offerings, students generally 
completed their assignments and projects using C, C++, or C# in conjunction with a 3D graphics API (e.g. 
OpenGL, DirectX, XNA). 3. DELIVERY This offering of the course was presented entirely and exclusively 
in Second Life on the RIT Island. Although a Web based course management system is used to manage course 
content and facilitate on-line discussions, all other interaction including lectures, student assessment, 
and instructor office hours, are conducted in-world. Weekly lectures are presented in an in-world amphitheatre 
complete with slide presentations. Lectures are augmented with live demonstrations of animation algorithms 
using objects within the 3D world. (Figure 1) A sandbox area of the island has been set up for students 
to develop programming assignments. All assignments are implemented using Second Life s native scripting 
language (Linden Scripting Language-LSL) and submitted to the instructor via in-world demonstrations 
(Figure 2). During the final exam, student projects are showcased in a virtual poster session. Figure 
2 Student assignment demo 4. PRELIMINARY FINDINGS The approach used in the class offers a fine alternative 
to traditional course delivery in physical classroom and shows much possibility for teaching animation 
as an online offering. Student satisfaction is high. The live demonstration of assignments provides a 
more exciting and productive venue for both students and instructor alike, allowing for more immediate 
feedback and providing a more interactive and friendly assessment environment. The use of the LSL for 
assignments, however, proved to be challenging due to more restrictive level of access provided by the 
scripting language in comparison to other APIs such as OpenGL or DirectX. REFERENCES RITZEMA, T. and 
HARRIS, B. 2008. The use of Second Life for distance education. J. Comput. Small Coll. 23, 6 (Jun. 2008), 
110-116. Figure 1 Interactive demo during lecture Copyright is held by the author / owner(s). SIGGRAPH 
2009, New Orleans, Louisiana, August 3 7, 2009. ISBN 978-1-60558-726-4/09/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1598055</article_id>
		<sort_key>650</sort_key>
		<display_label>Article No.</display_label>
		<display_no>65</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>65</seq_no>
		<title><![CDATA[Collaborative animation productions using original music in a unique teaching environment]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1597990.1598055</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1598055</url>
		<abstract>
			<par><![CDATA[<p>The Department of Digital Media at the University of Central Florida has developed a unique Visual Language Track that offers a select group of students the opportunity to learn skills needed for a career in computer animation. The Junior Year focuses on the design aspects of story telling, such as traditional animation, art direction and story-boarding followed by intense workshops applying those principles while learning all the steps of the computer animation production process. Senior Year students work together as members of the class cohort on a narrative digital animation production through a highly integrated structure of courses. This affords each student the opportunity to learn and contribute to every artistic and technical step in the computer animation production process.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Animation</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>A.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P1617765</person_id>
				<author_profile_id><![CDATA[81442617466]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Darlene]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hadrika]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Central Florida]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617766</person_id>
				<author_profile_id><![CDATA[81442612655]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Stella]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sung]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Central Florida]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 COLLABORATIVE ANIMATION PRODUCTIONS USING ORIGINAL MUSIC IN A UNIQUE TEACHING ENVIRONMENT Darlene Hadrika, 
Research AssociateStella Sung, Professor of MusicDepartment of Digital MediaDepartment of Digital MediaUniversity 
of Central FloridaUniversity of Central Florida dhadrika@mail.ucf.edu ssung@pegasus.cc.ucf.edu 1 Introduction 
The Department of Digital Media at the University of CentralFlorida has developed a unique Visual Language 
Track that offersa select group of students the opportunity to learn skills needed for a career in computer 
animation. The Junior Year focuses on the design aspects of story telling, such as traditional animation, 
artdirection and story-boarding followed by intense workshops applying those principles while learning 
all the steps of the computer animation production process. Senior Year students work together as members 
of the class cohort on a narrative digitalanimation production through a highly integrated structure 
of courses. This affords each student the opportunity to learn and contribute to every artistic and technical 
step in the computeranimation production process. 2 Story, Production, Teamwork and Music The Visual 
Language Track was influenced by feedback from theACM SIGGRAPH 2006 Educator s Forum, where industry 
leaders expressed that students should learn a general understanding ofdigital tools, be good problem 
solvers and communicators, and betrained to work as a member of a team. There are four uniqueconcepts 
that drive the philosophy of the Visual Language track:story, production, teamwork, and music integration. 
Students learn that every visual and musical element within a digital productiondeliberately contributes 
to the exposition of a narrative story. This unusual approach resulted in the students increased awareness 
of how music and sound impact animation, including character development, length of film, environment, 
timing, etc. During the course of the production, the students came to understand theimpact of the music, 
and not only learned to synchronize images tomusic, but came to heighten their listening abilities during 
thecreative process. 3 Variations on Music Integration Techniques Each film was developed with a unique 
story and a correspondingmusical approach. Uncle Monday, the first film was directed and storyboarded 
by a faculty member. A faculty composer composed the music specifically for the film. Little Bee Girl, 
thesecond film was storyboarded by students with music composedby a student composer, who worked alongside 
the student animators. UCF faculty members heavily supervised these students. For Ladybug: Action Hero, 
the third film, students weregiven a script and were allowed to expand upon the story. The music was 
created entirely by a student composer after the storyboards were completed. Atlas Revenge, the fourth 
film, was directed by a graduate student using pre-composed music by a faculty member. Students working 
on this film storyboarded andanimated to the score. In each of these situations, students were forced 
to come up with creative solutions for their animation,depending upon the given musical parameters. 
4 Conclusion This talk will focus upon strategies that were developed duringthe production process to 
successfully complete each film withinthe academic year. Copyright is held by the author / owner(s). 
SIGGRAPH 2009, New Orleans, Louisiana, August 3 7, 2009. ISBN 978-1-60558-726-4/09/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1598056</article_id>
		<sort_key>660</sort_key>
		<display_label>Article No.</display_label>
		<display_no>66</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>66</seq_no>
		<title><![CDATA[MyWorld4D]]></title>
		<subtitle><![CDATA[introduction to CG with a modeling and simulation twist]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1597990.1598056</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1598056</url>
		<abstract>
			<par><![CDATA[<p>We describe the design, implementation and results of a senior-level Introduction to Computer Graphics course that successfully blends Modeling and Simulation concepts in the "classical" rendering-pipeline syllabus. Evaluation over two years shows record enrollment-retention and student satisfaction when compared to the simple geometric primitive rendering approach at the same institution. Students demonstrate an increased ability to handle computational challenges and to incorporate concepts from senior-level courses such as Artificial Intelligence, Scientific Computing, or Networking into graphics applications.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[computer graphics]]></kw>
			<kw><![CDATA[education]]></kw>
			<kw><![CDATA[modeling]]></kw>
			<kw><![CDATA[simulation]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>A.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P1617767</person_id>
				<author_profile_id><![CDATA[81100207360]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[G.]]></first_name>
				<middle_name><![CDATA[Elisabeta]]></middle_name>
				<last_name><![CDATA[Marai]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Pittsburgh]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 MyWorld4D: Introduction to CG with a Modeling and Simulation Twist G. Elisabeta Marai * Department of 
Computer Science, University of Pittsburgh  Figure 1: A navigatable model of Downtown Pittsburgh complete 
with bridges, waves, animated funiculars, and a Let s Go Pens blimp, created by student Nat Wetzel as 
a .nal project for the Pitt CS Introduction to Computer Graphics course. The project is based on OpenGL. 
Abstract We describe the design, implementation and results of a senior­level Introduction to Computer 
Graphics course that success­fully blends Modeling and Simulation concepts in the classi­cal rendering-pipeline 
syllabus. Evaluation over two years shows record enrollment-retention and student satisfaction when com­pared 
to the simple geometric primitive rendering approach at the same institution. Students demonstrate an 
increased ability to han­dle computational challenges and to incorporate concepts from senior-level courses 
such as Arti.cial Intelligence, Scienti.c Com­puting, or Networking into graphics applications. Keywords: 
education, computer graphics, modeling, simulation 1 Introduction Education in Computer Graphics at our 
midtier-I university typ­ically starts at the senior level, with a target group of primarily computer 
science and computer engineering students. However, a recent university-wide initiative in Computational 
Modeling and Simulation is leading to tighter coupling of the curriculum across Computer Science, Math, 
Engineering, and Natural Sciences. We see increasing numbers of CG students majoring in Computer Sci­ence 
and either Math, Physics, Chemistry etc., whose primary in­terest is clearly in physically-based modeling 
and simulation. To answer the needs and interest of this target audience, we de­signed and developed 
an Introduction to Computer Graphics course that successfully blends modeling and simulation concepts 
with the basics of rendering. 2 Approach Our challenge is to engage students into deep understanding 
of the computer graphics rendering basics, while enabling them to exper­iment early on in the course 
with modeling and physically-based * e-mail: marai@cs.pitt.edu simulation. We enable modeling and simulation 
by giving the students ready, although gradual access to the OpenGL rendering pipeline. For example, 
support code for the .rst assignment covers the rather tedious basic OpenGL and callback setup; the students 
are encouraged to experiment instead with vertex-based geometric shapes and to use timeout events to 
control the geometric coordi­nates of the shapes. Later assignments show examples of setting up the camera, 
lighting, shading or texturing in OpenGL, while asking the students to implement their own parametric 
shapes, geometric transformations, scenegraphs, and lighting schemes. To ensure the students stay motivated 
in understanding the render­ing pipeline, particular emphasis is placed on highlighting the math behind 
each algorithm, the computational cost of each approach, and the toll this computation takes on realistic 
models and simu­lations. Furthermore, while students are not asked to implement neither a virtual camera 
from scratch, nor ray tracing, they use the same principles to pick a 3D object using a mouse, or to 
compute the post-collision direction of a moving object. The syllabus fol­lows the rendering-pipeline 
structure of Foley et al. (the graphics framework, geometric transformations, viewing, scan conversion 
and clipping, color and the visual system, shading, texture map­ping, and ray tracing), interspersed 
with lectures on tessellation, particle systems, collision detection, and video games.  3 Results and 
Conclusion The .nal projects (see http://vis.cs.pitt.edu/teaching/cs1566/) and student feedback show 
the students are fascinated by physically­based modeling and simulation. Evaluation over two years shows 
this new approach to be superior in terms of retention and student satisfaction when compared to the 
rendering-based approach at the same institution. Acknowledgments Grateful acknowledgments to Andy van 
Dam, John Hughes, Michael Black, Rebecca Hwa, Nancy Pollard, the wonderful Pitt CS1566 students, and 
the anonymous reviewers. Copyright is held by the author / owner(s). SIGGRAPH 2009, New Orleans, Louisiana, 
August 3 7, 2009. ISBN 978-1-60558-726-4/09/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1598057</article_id>
		<sort_key>670</sort_key>
		<display_label>Article No.</display_label>
		<display_no>67</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>67</seq_no>
		<title><![CDATA[High-tech chocolate]]></title>
		<subtitle><![CDATA[exploring 3D and mobile applications for factories]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1597990.1598057</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1598057</url>
		<abstract>
			<par><![CDATA[<p>TCHO, a chocolate manufacturer in San Francisco, and FXPAL, a research lab in Silicon Valley, have been collaborating on exploring emerging technologies for industry. The two companies seek ways to bring people closer to the products they consume, clarifying end-to-end production processes with technologies like sensor networks for fine-grained monitoring and control, mobile process control, and real/virtual mashups using virtual and augmented realities. This work lies within and extends the area of research called mixed- or cross-reality [1].</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>A.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P1617768</person_id>
				<author_profile_id><![CDATA[81100005666]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Maribeth]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Back]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[FX Palo Alto Laboratory, Palo Alto, CA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617769</person_id>
				<author_profile_id><![CDATA[81100038990]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Timothy]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Childs]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[TCHO Inc., San Francisco, CA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Paradiso, Joseph, Landay, James, and Verbeck, Sibley. IEEE Pervasive Computing Call for Papers, Special Issue on Cross-Reality: &#60;b&#62;http://tinyurl.com/con9yq&#60;/b&#62; (link checked 5/5/2009).]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 High-tech Chocolate: Exploring 3D and mobile applications for factories Maribeth Back FX Palo Alto 
Laboratory 3400 Hillview, Bldg. 4, Palo Alto, CA 94304 back@fxpal.com ABSTRACT TCHO, a chocolate manufacturer 
in San Francisco, and FXPAL, a research lab in Silicon Valley, have been collaborating on exploring emerging 
technologies for industry. The two companies seek ways to bring people closer to the products they consume, 
clarifying end-to-end production processes with technologies like sensor networks for fine-grained monitoring 
and control, mobile process control, and real/virtual mashups using virtual and augmented realities. 
This work lies within and extends the area of research called mixed-or cross-reality [1]. INTRODUCTION 
This research collaboration focuses on new methods for collaboration and control in industrial environments, 
in particular methods using multi-user 3D virtual worlds. We import real-world sensor data (such as temperature 
and machine state) and multi-camera imagery from the real factory and chocolate lab. We are also looking 
at appropriate uses for mobile devices such as cellphones and tablet computers, and how they intersect 
with virtual worlds. For example, inside the newly developed TCHO Factory 3D virtual world, you can click 
on a machine to read its sensors' status, or move closer to it to trigger an in­world video overview 
of its function. The virtual environment will import real-time data from hundreds of sensors on the factory 
floor, and will be capable of tracking processes in detail. This multi-user collaborative space can be 
used for tasks like remote factory observation, virtual inspections, customer visits, education/training 
of employees, process monitoring and inventory tracking.   Timothy Childs TCHO Inc. Pier 17, San Francisco, 
CA timothy@tcho.com Meanwhile, an experimental iPhone app provides mobile laboratory monitoring and 
control. In the TCHO development lab, where intricate processes are developed to bring out the best in 
each bean, accurate tracking of time and temperature are essential. The app allows a real-time view into 
the lab (via PTZ steerable camera) and individual control over machines and sensors. Data from the lab 
is also represented in the 3D virtual world. Finally, a network of high-definition cameras installed 
around the chocolate factory streams live video via web, virtual world, or iPhone. These three systems 
were deployed at the TCHO factory and lab in late 2008 and 2009, and are now in beta development. Through 
this mashup of mobile, social, mixed and virtual technologies, we hope for enhanced collaboration between 
physically remote people and places for example, factories in China with managers in Japan. In the process, 
we are finding new applications for existing technologies, as well as insight into real-world needs in 
globally distributed systems. More information on this research can be found online at http://www.fxpal.com/VirtualFactory/ 
. PRESENTATION The presentation at SIGGRAPH is designed to be very interactive, and will include a chocolate 
tasting for attendees (as will the accompanying poster). Presenters both have considerable prior SIGGRAPH 
experience. Bios: Timothy Childs, founder and chief chocolate officer at TCHO Inc., is a successful chocolate 
entrepreneur. Prior to his initiation in the chocolate industry, he worked on machine vision with NASA 
s Space Shuttle program, and launched several early-stage companies in the internet and computer graphics 
industries. Timothy was a cofounder and MC of the Web3D RoundUP at SIGGRAPH. Maribeth Back is a senior 
research scientist at the FX Palo Alto Laboratory (FXPAL). Her work is focused on smart environments, 
whether real, augmented, embedded, or virtual. Her work has appeared several times at SIGGRAPH in the 
Emerging Technologies, Art, and Education tracks.  REFERENCES 1. PARADISO, JOSEPH, LANDAY, JAMES, and 
VERBECK, SIBLEY. IEEE Pervasive Computing Call for Papers, Special Issue on Cross-Reality: http://tinyurl.com/con9yq 
(link checked 5/5/2009). Copyright is held by the author / owner(s). SIGGRAPH 2009, New Orleans, Louisiana, 
August 3 7, 2009. 1 ISBN 978-1-60558-726-4/09/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1598058</article_id>
		<sort_key>680</sort_key>
		<display_label>Article No.</display_label>
		<display_no>68</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>68</seq_no>
		<title><![CDATA[Adaptive coded aperture projection]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1597990.1598058</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1598058</url>
		<abstract>
			<par><![CDATA[<p>With adaptive coded aperture projection, we present solutions for taking projectors to the next level. By placing a programmable liquid crystal array at a projectors aperture plane we show how the depth of field (DOF) of a projection can be greatly enhanced. This allows focussed imagery to be shown on complex screens with varying distances to the projectors focal plane, such as projection domes as in planetariums or cylindrical canvases as in IMAX theaters. We demonstrate that adaptive apertures outperform previous methods of projector defocus compensation for objective lenses with static apertures. In addition, our adaptive apertures can perform the type of temporal contrast enhancement employed by common auto-iris projection lenses, and also produce high-quality depixelated images. The latter is beneficial for close-view displays with limited resolution, such as rear-projected TV sets.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.4.1</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>A.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P1617770</person_id>
				<author_profile_id><![CDATA[81421597701]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Max]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Grosse]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Bauhaus-University Weimar]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617771</person_id>
				<author_profile_id><![CDATA[81335499586]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Gordon]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wetzstein]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of British Columbia]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617772</person_id>
				<author_profile_id><![CDATA[81100622976]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Oliver]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bimber]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Bauhaus-University Weimar]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617773</person_id>
				<author_profile_id><![CDATA[81100631719]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Anselm]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Grundh&#246;fer]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Bauhaus-University Weimar]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Adaptive Coded Aperture Projection Max Grosse* Gordon Wetzstein Anselm Grundh¨ofer Bauhaus-University 
Weimar University of British Columbia Bauhaus-University Weimar Oliver Bimber§ Bauhaus-University Weimar 
 Figure 1: Placing a transparent liquid crystal array at the aperture plane of a projector lens allows 
encoding the aperture s mask pattern dynamically depending on the perceivable frequencies of the displayed 
images. Such adaptive coded apertures signi.cantly improve depth­of-.eld through inverse .ltering when 
compared to static circular aperture stops or broadband masks. They can also be applied for high quality 
projector de-pixelation and for increasing the temporal contrast of video sequences in a similar way 
as auto-iris projection lenses. The inlays illustrate the computed intensity codes and the applied binary 
masks. All images displayed with an adaptive coded aperture are perception optimized and have been computed 
for the given viewing conditions (50 cm distance to screen, displayed at a diagonal of 20 cm / 10 cm 
for top row / bottom row). Possible artifacts can only be perceived when observing the images at closer 
distances or larger sizes. The bottom row illustrates a comparison of adaptive apertures with various 
static (circular and broadband) ones. While the .ve images on the bottom-left are adjusted to a similar 
brightness to enable a better comparison of focus, the lower right image is captured using the same exposure 
as the image left to it. A small circular aperture has been applied that achieves the same depth-of-.eld 
as with the binarized adaptive coded aperture at the cost of a signi.cant loss of brightness. With adaptive 
coded aperture projection, we present solutions for taking projectors to the next level. By placing a 
programmable liquid crystal array at a projectors aperture plane we show how the depth of .eld (DOF) 
of a projection can be greatly enhanced. This allows focussed imagery to be shown on complex screens 
with varying distances to the projectors focal plane, such as projection domes as in planetariums or 
cylindrical canvases as in IMAX the­aters. We demonstrate that adaptive apertures outperform previ­ous 
methods of projector defocus compensation for objective lenses with static apertures. In addition, our 
adaptive apertures can per­form the type of temporal contrast enhancement employed by com­mon auto-iris 
projection lenses, and also produce high-quality de­pixelated images. The latter is bene.cial for close-view 
displays with limited resolution, such as rear-projected TV sets. Several approaches have been proposed 
to increase the DOF of con­ventional projectors using image deconvolution with known point­spread functions 
(PSF). All of these approaches share two limita­tions: Firstly, they are far from being able to reach 
real-time perfor­mance not even if the time necessary for measuring the local blur functions is not 
considered. This prevents them from displaying dy­namic content. Secondly, the amount of defocus that 
can be com­pensated through deconvolution is clearly limited when the PSF is Gaussian. Ringing artifacts 
will dominate if the blur becomes too large. In fact, only little defocus can be compensated ef.ciently 
with such techniques. *e-mail:max.grosse@uni-weimar.de e-mail: wetzste1@cs.ubc.ca e-mail:grundhoe@uni-weimar.de 
§e-mail:bimber@uni-weimar.de We show that if adaptive coded apertures are applied instead of sim­ple 
static ones (circular or coded), more image details can be recov­ered from optical defocus while a high 
light throughput is main­tained. Furthermore, our implementation uses the graphics hard­ware for computation 
and thus achieves interactive frame-rates of currently 8-16 fps at XGA resolution. Our approach computes 
and displays a dynamic aperture pattern, based on the analysis of the projected image content and on 
limita­tions of human visual perception. This analysis allows us to deter­mine and .lter out spatial 
frequencies of the input image that cannot be perceived by a human observer under the given viewing condi­tions. 
An optimal aperture can then be computed by maximizing its light transmission while preserving the perceivable 
frequencies, rather than being restricted to support a constant frequency band. We show that our adaptive 
apertures produce better results than pre­vious methods with the same or even an increased amount of 
light transmission. Adaptive apertures are also useful for planar screens that do not re­quire a large 
DOF: Defocussing the projector optically to make the pixel structure vanish, and applying deconvolution 
to recover the image details leads to better image quality. This is known as projec­tor de-pixelation. 
Our technique enhances projector de-pixelation signi.cantly. For video frames with different brightness, 
our adap­tive aperture can be scaled with respect to the mean image bright­ness to increase the temporal 
contrast of a video sequence as con­ventional auto-iris projection lenses but with a larger DOF. In 
combination with re.ective spatial light modulators, adaptive coded apertures can potentially lead to 
a new generation of auto­iris projector lenses. Copyright is held by the author / owner(s). SIGGRAPH 
2009, New Orleans, Louisiana, August 3 7, 2009. ISBN 978-1-60558-726-4/09/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1598059</article_id>
		<sort_key>690</sort_key>
		<display_label>Article No.</display_label>
		<display_no>69</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>69</seq_no>
		<title><![CDATA[Projected light microscopy]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1597990.1598059</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1598059</url>
		<abstract>
			<par><![CDATA[<p>In light microscopy (or optical microscopy) visible light is either transmitted through or reflected from the specimen before it is observed or recorded. In its simplest imaging mode, bright field microscopy, the illumination light is modulated in intensity or color depending on the specimen's transmission or reflection properties before it enters the objective lens. The general drawbacks of this are limited resolution (which is constrained by the wavelength of visible light) and limited contrast. Several techniques exist for enhancing the contrast of light microscopes, such as dark field microscopy, phase contrast microscopy, (differential) interference contrast microscopy, or fluorescence microscopy. Most of them are applied to make otherwise invisible transparent objects, such a biological structures like cells, visible. Specimens that are too thick for transmitting light, however, require a reflected illumination - for which there are few alternatives for contrast enhancements.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.4.1</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>A.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P1617774</person_id>
				<author_profile_id><![CDATA[81100622976]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Oliver]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bimber]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Bauhaus-University Weimar]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617775</person_id>
				<author_profile_id><![CDATA[81100631719]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Anselm]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Grundh&#246;fer]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Bauhaus-University Weimar]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617776</person_id>
				<author_profile_id><![CDATA[81381595983]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Daniel]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kurz]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Bauhaus-University Weimar]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617777</person_id>
				<author_profile_id><![CDATA[81442600922]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Sebastian]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Thiele]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Bauhaus-University Weimar]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617778</person_id>
				<author_profile_id><![CDATA[81418597047]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Ferry]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[H&#228;ntsch]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Bauhaus-University Weimar]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617779</person_id>
				<author_profile_id><![CDATA[81100586977]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Toshiyuki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Amano]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Nara Institute of Science and Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617780</person_id>
				<author_profile_id><![CDATA[81442614795]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>7</seq_no>
				<first_name><![CDATA[Daniel]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kl&#246;ck]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Brandenburg Technical University Cottbus]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Projected Light Microscopy Oliver Bimber, Anselm Grundh¨antsch * ofer, Daniel Kurz, Sebastian Thiele, 
Ferry H¨ Bauhaus-University Weimar Toshiyuki Amano Daniel Kl¨ock Nara Institute of Science and Technology 
Brandenburg Technical University Cottbus Figure 1: Projected Light Microscopy: Spatially modulated (projected) 
light is used for illumination in a microscope. Top row: The two pro­totypes show con.gurations for re.ected 
illumination and for transmitted illumination. The numbers in the .gures indicate the magni.cation factors 
of the individual components. The green arrows illustrate the illumination paths. Bottom row: Compared 
to uniform illumination, a carefully computed projected illumination enhances the perceived contrast 
of observed specimens and compensates for highlights caused by surface-and subsurface re.ections. This 
leads to improved visibility in optical microscopy when making observations directly through the oculars. 
In addition, the signal-to-noise ratio in captured images can be increased using spatially varying illumination. 
Controlled illumi­nation therefore also improves image analysis tasks in digital microscopy as shown 
in the automatic thresholding example. Right column: Applying a decorrelation stretch to the RGB channels 
of the illumination greatly enhances the color contrast of the observed specimen. In light microscopy 
(or optical microscopy) visible light is either transmitted through or re.ected from the specimen before 
it is ob­served or recorded. In its simplest imaging mode, bright .eld mi­croscopy, the illumination 
light is modulated in intensity or color depending on the specimen s transmission or re.ection properties 
before it enters the objective lens. The general drawbacks of this are limited resolution (which is constrained 
by the wavelength of visi­ble light) and limited contrast. Several techniques exist for enhanc­ing the 
contrast of light microscopes, such as dark .eld microscopy, phase contrast microscopy, (differential) 
interference contrast mi­croscopy, or .uorescence microscopy. Most of them are applied to make otherwise 
invisible transparent objects, such a biological structures like cells, visible. Specimens that are too 
thick for trans­mitting light, however, require a re.ected illumination -for which there are few alternatives 
for contrast enhancements. We present a new illumination technique for optical microscopy that we refer 
to as projected light microscopy. Instead of using uni­form (low-frequent) light for illumination, we 
apply high-frequent (projected) light. This allows a controlled spatial and temporal modulation with 
re.ective or transmissive specimens. Since we measure the modulation behaviour of the specimen initially 
or on the .y, we can project a carefully computed illumination pattern which is then modulated by the 
specimen itself. The .nally formed image is either observed through the oculars or is sensed by a cam­era. 
It can provide an improved visibility through the enhancement of contrast, or the reduction of surface-and 
sub-surface re.ections. *e-mail:{bimber,grundhoe,kurz,thiele,haentsch}@uni-weimar.de e-mail: amano@is.naist.jp 
e-mail:dkloeck@informatik.tu-cottbus.de Compared to related microscopy techniques that apply spatial 
light modulation, our technique implements a closed projector-camera feedback loop that neutralizes the 
modulation of an arbitrary pro­jection pattern on the specimen. Thus, it estimates the modulation of 
uniform white light even though the specimen has been phys­ically illuminated with a non-uniform (possibly 
colored) pattern. From this estimation, a new optimal illumination pattern is com­puted and projected. 
This allows adjusting the illumination on the .y according to movements of the specimen, or to focus 
and mag­ni.cation changes of the microscope. It enables enhanced real-time observations and recordings. 
Domains that utilize light microscopes, such as operation mi­croscopy, forensic analysis, material science, 
bioimaging and others often suffer from too low or too high contrast images when mak­ing observations 
directly through the oculars. A low contrast can be caused by low contrast samples or a high degree of 
scattering within the specimen. A too high contrast can be caused by specular highlights on the specimen. 
Furthermore, the acquisition of ex­trem (low or high) contrast, photon limited images is problematic 
for many digital microscopy applications that rely on robust image analysis. All of these areas can bene.t 
from a controlled illumi­nation. Our approach is complementary to the classical contrast techniques that 
are outlined above. Once structures can be detected (e.g., by a sensor such as a camera, not necessarily 
by the human eye) their appearance can be optimized for visual perception or for image analysis. We also 
envision applications of our technique in other .elds, such as endoscopy. Measuring and compensating 
individual aspects of the light trans­port between projector and camera, visualizing context information, 
and formal user studies are directions of our future research. Copyright is held by the author / owner(s). 
SIGGRAPH 2009, New Orleans, Louisiana, August 3 7, 2009. ISBN 978-1-60558-726-4/09/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1598060</article_id>
		<sort_key>700</sort_key>
		<display_label>Article No.</display_label>
		<display_no>70</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>70</seq_no>
		<title><![CDATA[Tablescape animation]]></title>
		<subtitle><![CDATA[a support system for making animations using tabletop physical objects]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1597990.1598060</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1598060</url>
		<abstract>
			<par><![CDATA[<p>From childhood, we often make stories extemporarily by drawing characters and/or playing puppets by hands. On the other hand, when we make animations using computer software, we usually have to use a mouse and a keyboard. For creating animation stories easily and extemporarily, more intuitive interfaces are needed.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Animation</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>A.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P1617781</person_id>
				<author_profile_id><![CDATA[81100527572]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Yasuaki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kakehi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617782</person_id>
				<author_profile_id><![CDATA[81537453256]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Junichi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yamaoka]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617783</person_id>
				<author_profile_id><![CDATA[81442616779]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Daisuke]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Akatsuka]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617784</person_id>
				<author_profile_id><![CDATA[81100095332]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Takeshi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Naemura]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1409077</ref_obj_id>
				<ref_obj_pid>1409060</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Barnes, C., et al. 2008. Video Puppetry: A Performative Interface for Cutout Animation. In <i>Transactions on Graphics (Proc. SIGGRAPH ASIA) 27 (5)</i>, ACM.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1073323</ref_obj_id>
				<ref_obj_pid>1186822</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Igarashi, T., et al. 2005. As-Rigid-As-Possible Shape Manipulation. In <i>Transactions on Computer Graphics(Proc. SIGGRAPH2005) 24 (3)</i>, ACM.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Kakehi, Y., Naemura, T., and Matsushita, M. 2007. Tablescape Plus: Interactive Small-sized Displays Upstanding on Tabletop Display. In <i>Tabletop 2007</i>, IEEE, 155--162.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2175774</ref_obj_id>
				<ref_obj_pid>2175737</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Lindinger, C., et al. 2006. Mixed Reality Installation 'Gulliver's World': Interactive Content Creation in Nonlinear Exhibition Design. In <i>Technologies for Interactive Digital Storytelling and Entertainment</i>, 312--323.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Figure1: Drawing Characters Figure2: Creating Stories onTabletop Figure3:ASnapshotof Animation 1 Introduction 
From childhood, we often make stories extemporarily by drawing characters and/or playing puppets by hands. 
On the other hand, when we make animations using computer software, we usually haveto usea mouseandakeyboard.For 
creating animation stories easily and extemporarily, more intuitive interfaces are needed. To improve 
these conditions, recently several systems for digi­tal storytelling have been proposed [Lindinger et 
al. 2006][Barnes et al. 2008][Igarashi et al. 2005]. This time, we focused on tabletop physical objects 
as an interface and propose a novel support sys­tem for creating animations named Tablescape Animation 
. In this system, users can draw charactersby hands and control the charac­ter actions by handling tabletop 
physical objects as well as playing puppets. In the result, an animation is generated in a computer. 
 2 Tablescape Animation In this system, users make animations in three processes as follow­ings. In the 
.rst process, users draw characters on a tablet monitor. Users illustrateseveral patternsofa character(e.g. 
frontview,back view, side view, running pose, jumping pose). For switching these illustrations of the 
character, users also set animation rules. The second process is for controlling characters. Character 
im­ages are projected onto the physical objects on a tabletop display. Users can control the characters 
by moving and connecting these objects. According to the animation rules, images change in real­time. 
Moreover,users can record theirvoices throughamicrophone to match characters actions taking place on 
the tabletop. In the third process, an animation is generated in a monitor accord­ingtothe characterimages,theactiondataandthe 
usersvoicedata. In this process, users can change the virtual camera position inter­actively, while watching 
the animation. Finally, this system writes this animation to a video .le. *e-mail: tabletop@hc.ic.i.u-tokyo.ac.jp 
 3 Implementation and Future Works We have implemented a prototype system of Tablescape Anima­tion. As 
shown in Figure 1, users can draw character images and set rules interactively. In the current implementation, 
we utilized a pentablet(WACOMCintiq21UX).Asfortherulesetting, usersas­sign conditions for switching illustrations 
among prepared options. To display and control the characters, we utilized our tabeltop dis­play system 
Tablescape Plus [Kakehi et al. 2007]. This system can project different images both onto the table surface 
and onto upstandingphysical objects simultaneously. Thus, users can see the characters and a background 
image on the tabletop without wear­ing anyspecial equipment (see Figure 2). Moreover, the actions of 
the characters change accordingto the rules. Then, users canwatch the generated animationina computer 
monitor. Figure3showsa snapshot of the animation. In the future, we plan to hold workshops for kids with 
this system anddevelopa graphical programmingenvironmenttoset morevar­ious and complex animation rules 
and effects in intuitive ways. References BARNES,C., ETAL. 2008.Video Puppetry:APerformative Inter­face 
for Cutout Animation. In Transactions on Graphics (Proc. SIGGRAPH ASIA) 27 (5),ACM. IGARASHI, T., ET 
AL. 2005. As-Rigid-As-Possible Shape Ma­nipulation. In Transactions on Computer Graphics(Proc. SIG-GRAPH2005) 
24 (3),ACM. KAKEHI, Y., NAEMURA, T., AND MATSUSHITA, M. 2007. Ta­blescape Plus: Interactive Small-sized 
Displays Upstanding on Tabletop Display. InTabletop 2007, IEEE, 155 162. LINDINGER, C., ET AL. 2006. 
Mixed Reality Installation Gul­liver s World : Interactive Content Creation in Nonlinear Ex­hibition 
Design. In Technologies for Interactive Digital Story­telling and Entertainment, 312 323. Copyright is 
held by the author / owner(s). SIGGRAPH 2009, New Orleans, Louisiana, August 3 7, 2009. ISBN 978-1-60558-726-4/09/0008 
 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1598061</article_id>
		<sort_key>710</sort_key>
		<display_label>Article No.</display_label>
		<display_no>71</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>71</seq_no>
		<title><![CDATA[Clouds with character]]></title>
		<subtitle><![CDATA[<i>Partly Cloudy</i>]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1597990.1598061</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1598061</url>
		<abstract>
			<par><![CDATA[<p>The short film <i>Partly Cloudy</i> is about the relationship between a stork and a cloud named Gus (Figure 1). To achieve the goals of the film, Gus needed to emote while maintaining a cloudy physical quality. Our talk will discuss the development and challenges of creating a cloud with the appeal of a human character.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Animation</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>A.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P1617785</person_id>
				<author_profile_id><![CDATA[81539949656]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[David]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Batte]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Pixar Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617786</person_id>
				<author_profile_id><![CDATA[81442610996]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Michael]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Fu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Pixar Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1401114</ref_obj_id>
				<ref_obj_pid>1401032</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Scheepers, F., and Angelidis, A., 2008. Atmos: A system for building volume shaders. ACM SIGGRAPH 2008 Talk.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Clouds with Character: Partly Cloudy David Batte Michael Fu Pixar Animation Studios Pixar Animation 
Studios The short .lm Partly Cloudy is about the relationship between a stork and a cloud named Gus 
(Figure 1). To achieve the goals of the .lm, Gus needed to emote while maintaining a cloudy physical 
quality. Our talk will discuss the development and challenges of creating a cloud with the appeal of 
a human character. Figure 1: Volumetric Character Gus with Stork Partly Cloudy c @Disney / Pixar. All 
rights reserved. 1 Approach We explored several techniques to achieve a cloudy character and settled 
on a volumetric approach that allowed us to leverage our existing tools. A traditionally rigged subdivision 
surface charac­ter was built, which represented the approximate outer surface of the cloud volume. This 
allowed animators to use Pixar s existing animation pipeline to drive Gus. Particle control meshes were 
ex­tracted from the surface model. The animation of the CG model was also transferred and applied to 
the control meshes. Particles were goaled to the control meshes so that the particles track with the 
an­imation, and procedural expressions were added to the particles to create secondary motion within 
and off of the control mesh. The particles were then exported and used in combination with an At­mos 
volume shader [Scheepers and Angelidis 2008] and rendered with Pixar s Renderman to create the .nal look. 
Other approaches explored were surface-based and sprite-based methods. The surface method attempted to 
create a volume cloud with a ray-marching shader on animated geometry. It was dif.­cult to develop a 
cloudy look with this technique. The sprite-based method offered visual interest but neither achieved 
the cloudy look nor scaled as well as the volumetric approach. 2 Artistic Control De.nition of shape 
is an important factor in creating a readable character. Attempting to match the carefully posed face 
and hand animation proved particularly challenging. Due to the particle ra­dius we used to approximate 
the control surface, there was an er­ror between the apparent isosurface created by the volume particles 
and the original animated surface model. Our procedural secondary motion further complicates this. This 
error and variation is nec­essary to obtain the cloudy look, but simultaneously disturbs the carefully 
posed shape the animators have created. To address the issue, we used control maps to de.ne particle 
radius, emission rate and amount of secondary motion. These control maps were tested against extreme 
character poses to make sure they worked for a full range of motion. Gus s eyes presented an interesting 
challenge. The artistic direc­tion given for his eyes required the use of surface geometry, but the cloud 
material Gus is composed of was not dense enough to occlude the eyes behind the eyelid. This was particularly 
apparent when he closed his eyelids mere .lms of cloud. To solve this problem, we pre-rendered texture 
masks, matting the eyeball with the original eyelid surface model. The eye shader used these texture 
to mask out any region of the eyeball that should be occluded. We discovered that our volumetric objects 
were very sensitive to light and shadow placement Gus is vertical in a horizontal cloud base, making 
it dif.cult to .nd a lighting setup that worked for both objects. We decided to build Gus in sections 
that could be lit inde­pendently and merged together. This allowed for greater creative control in lighting 
and therefore, a more appealing character. 3 Optimization Volume rendering can be computationally expensive. 
A rendering pro.le of Gus showed that large portions of our renders were be­ing spent calculating noise. 
For each volume sample, there can be multiple overlapping particles contributing density. Each of these 
particles make calls to the noise functions. We attempted to min­imize the noise calls by balancing the 
particle count, the particle size and the number of octaves of fractal noise that were being used while 
maintaining the look of the character. A second method was also employed to optimize our renders. The 
Atmos shader utilizes an opacity threshold to halt marching along the ray. We employed an offset interior 
mesh that was articulated to closely follow the exterior surface. This mesh was used to gen­erate a second 
set of particles, which helped .ll the void inside the character. By making these interior particles 
more dense, we in­creased the accumulated opacity more quickly, thereby halting the marching sooner. 
 References SCHEEPERS, F., AND ANGELIDIS, A., 2008. Atmos: A system for building volume shaders. ACM 
SIGGRAPH 2008 Talk. Copyright is held by the author / owner(s). SIGGRAPH 2009, New Orleans, Louisiana, 
August 3 7, 2009. ISBN 978-1-60558-726-4/09/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1598062</article_id>
		<sort_key>720</sort_key>
		<display_label>Article No.</display_label>
		<display_no>72</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>72</seq_no>
		<title><![CDATA[The hair motion compositor]]></title>
		<subtitle><![CDATA[compositing dynamic hair animations in a production environment]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1597990.1598062</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1598062</url>
		<abstract>
			<par><![CDATA[<p>Digital characters can appear in both live action and computer-animated movies. Most of them involve some form of dynamic hair or fur, often having to be both believable and heavily art-directed. No matter which dynamic hair solver a facility relies on, there's only so much that can be achieved with a physically realistic model, and most of the artist's time is spent fixing or fine-tuning hair simulations to satisfy a Director or supervisor. To address this, Sony Pictures Imageworks developed the Hair Motion Compositor: a powerful hair animation framework that allows artists to direct, combine, offset and override hair animations that go in or come out of a dynamics solver.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Animation</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>A.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P1617787</person_id>
				<author_profile_id><![CDATA[81317487930]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Fran&#231;ois]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Chardavoine]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Sony Pictures Imageworks]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617788</person_id>
				<author_profile_id><![CDATA[81100008103]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Armin]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bruderlin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Sony Pictures Imageworks]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 The Hair Motion Compositor: Compositing Dynamic Hair Animations in a Production Environment François 
Chardavoine, Armin Bruderlin Sony Pictures Imageworks  1 -Introduction Digital characters can appear 
in both live action and computer­animated movies. Most of them involve some form of dynamic hair or fur, 
often having to be both believable and heavily art­directed. No matter which dynamic hair solver a facility 
relies on, there's only so much that can be achieved with a physically realistic model, and most of the 
artist's time is spent fixing or fine­tuning hair simulations to satisfy a Director or supervisor. To 
address this, Sony Pictures Imageworks developed the Hair Motion Compositor: a powerful hair animation 
framework that allows artists to direct, combine, offset and override hair animations that go in or come 
out of a dynamics solver. 2 -Hair Motion Compositing Like a 2D image compositing tool, the Hair Motion 
Compositor (HMC) is a framework that allows the user to combine and modify hair/fur animations by graphically 
building a network of nodes, where each node can either represent animations or operations applied to 
these animations. A core building block of the HMC is the blend node which can blend between two (or 
more) hair curve animations: By cascading blends between different, but similar, hair simulations, the 
artist can combine them by only keeping the best parts of each instead of trying to find solver settings 
that would provide a similar combined result. Blends can apply to all the hair curves or just a subset, 
and can be weighted along the length of the hair so the gross motion of one animation is used, but the 
tips of the hair behave according to another. This is implemented using linear or rotational interpolation 
of hair curve segments, between two incoming sets of curves. Blend "balls" can be used to dynamically 
choose which hairs are affected: on the movie G-Force, a really windy simulation was cached out once, 
and spheres were animated to interactively decide where gusts of wind should go through the guinea pig's 
fur without ever having to re-compute the physics. The dynamic hair solver is simply another operational 
node in the graph, which takes a goal animation as input and produces a physically believable motion 
as output. A typical graph (shown above) has the solver goal being driven by a blend of rigged animation 
and static hair (which matches the original comb). If the artist is asked to make the simulation "stiffer" 
or better match the goal, this can be done interactively by blending back to the goal without having 
to re-simulate. Any point in the graph can be cached out to disk and brought back in as an input that 
can be combined with other animations. 3 -Fixing a Shot Typically, dynamics simulations of hair do not 
get the artists exactly what they want even after a few iterations. Thus, the animations often need to 
be corrected or overridden in certain areas. Offset nodes allow the artist to modify an animation using 
a simplified controller which follows along with the incoming animation. Any tweaks are propagated to 
the animation of the hair. This is useful when a supervisor likes the movement of a character's hair 
in general, but wants it to "swing a little higher", as was the case with Mary Jane's hair in some shots 
of Spider-Man 3. The controller can be a lower-resolution curve, which takes on the average shape and 
animation of the incoming hair curves. An "offset sock" can be used instead of a simple curve if the 
volume of a group of curves needs to be modified (such as getting a ponytail to expand or contract. In 
this case, NURBS circles are bound at set parametric positions on the "average curve" and lofted to create 
a sock shape. Their radius is computed dynamically so the sock encompasses all the hair the user wants 
to edit, forming a smooth convex hull that can be expanded, contracted or moved. A "super-hair" node 
can be used to make the outgoing hairs exactly take on the shape/animation of the controller (instead 
of being offset by it). Internally, this works identically to a blend node that would blend all the incoming 
curves to a single controller curve. In Open Season, super-hairs with blend balls (to localize their 
effect) were attached to rabbits running through the grass, and shaped to make the grass curves bend 
away from them, without any dynamics solving necessary.  4 -The Payoff Over time, most facilities end 
up developing a catalog of hair rigging solutions and scripts around a dynamic hair solver. With little 
overhead, these can be made modular and implemented as nodes in a graphical framework, such as the one 
introduced here, to allow caching, blending, targeting and correcting of hair animations. In our experience, 
the building blocks are simple to implement, and the benefits of having a consolidated, powerful, yet 
easy to understand and use GUI can save a lot of time, money and frustration in achieving desired hair 
animations. Copyright is held by the author / owner(s). SIGGRAPH 2009, New Orleans, Louisiana, August 
3 7, 2009.  ISBN 978-1-60558-726-4/09/0008 &#38;#169;2009 Sony Pictures Imageworks Inc. All rights reserved. 
 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1598063</article_id>
		<sort_key>730</sort_key>
		<display_label>Article No.</display_label>
		<display_no>73</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>73</seq_no>
		<title><![CDATA[iBind]]></title>
		<subtitle><![CDATA[smooth indirect binding using segmented thin-layers]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1597990.1598063</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1598063</url>
		<abstract>
			<par><![CDATA[<p>This talk introduces iBind, a novel indirect binding technique improving on mean value coordinates (MVC)[1]. iBind smoothly deforms vertices using a control cage by uniquely leveraging heat diffusion on closed, thin layers across a structured set of mean value coordinates. The talk begins by discussing the limitations of previous techniques, and proceeds with an explanation of iBind, which produces fast, stable, and appealing spacial deformation.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.4.6</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>A.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245.10010247</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems->Image segmentation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245.10010248</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems->Video segmentation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P1617790</person_id>
				<author_profile_id><![CDATA[81442607798]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Erick]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Miller]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Walt Disney Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617789</person_id>
				<author_profile_id><![CDATA[81335494902]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Chung-An]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Walt Disney Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617791</person_id>
				<author_profile_id><![CDATA[81442593437]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Gene]]></first_name>
				<middle_name><![CDATA[S.]]></middle_name>
				<last_name><![CDATA[Lee]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Walt Disney Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1073229</ref_obj_id>
				<ref_obj_pid>1186822</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Ju, T., Schaefer, S., and Warren, J. 2005 <i>Mean Value Coordinates for Closed Triangular Meshes</i>. In Siggraph '05, ACM Press, 561--566.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1360677</ref_obj_id>
				<ref_obj_pid>1399504</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Lipman, Y., Levin D., and Cohen-Or, D. 2008 <i>Green Coordinates</i>, In Siggraph '08, ACM Press. 78: 1--78: 10]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1276466</ref_obj_id>
				<ref_obj_pid>1275808</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Joshi, P., et al., 2007 <i>Harmonic Coordinates for Character Articulation</i>. In Siggraph '07, ACM Press, 71: 1--71: 9]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 iBind: Smooth Indirect Binding using Segmented Thin-Layers Chung-An Lin Erick Miller Gene S. Lee Walt 
Disney Animation Studios  Figure 1: (a) User control cage (b) Segmented overlapping thin-layers This 
talk introduces iBind, a novel indirect binding techniqueimproving on mean value coordinates (MVC)[1]. 
iBind smoothlydeforms vertices using a control cage by uniquely leveraging heatdiffusion on closed, thin 
layers across a structured set of meanvalue coordinates. The talk begins by discussing the limitations 
ofprevious techniques, and proceeds with an explanation of iBind,which produces fast, stable, and appealing 
spacial deformation. Previous Techniques Previous binding techniques have limitations for production 
users. Green coordinates [2] suffers from scaling artifacts, no control over localized regions, and being 
numerically unstable. Harmonic coordinates [3] encounters problems when binding points outside the control 
cage, and is inaccurate unless used with a dense solver (but the dense solver is slow). MVC is fast and 
appealing for low resolution cages, but has a large computation footprint as resolution of the control 
cage is increased to real-world examples. MVC also has limited local controls, and must perform a rebinding 
operation if the incoming vertex of the deformed mesh is moving. Finally, neither of the three algorithms 
generally allow arbitrary control cages with open-holes, and each requires a special case using barycentric 
coords if a vertex lies coplanar with a control cage face. Segmented Thin-layers iBind begins by building 
segmented, thin-layers from a user­created control cage (1a). A thin-layer is an extruded, mesh segment 
that expands one face of the control cage by its n surrounding faces. The vertices of the thin-layer 
are extruded along the normals of the control cage, and the layer is set to a variable thickness of value 
t. The results of this step creates a thin­layer, closed form, geometric structure, which serves as a 
cage for storing and evaluating a four dimensional MVC structure(1b). Heat Diffusion Weights Influence 
weights for each thin-layer to a vertex is achieved by performing surface-based, heat diffusion to the 
underlying mesh (1c). Given as the density of the thin segment at vertex and as the diffusion coefficient, 
heat diffusion is computed as follows: Afterwards, the weights are normalized, and a (structured set 
of) MVC of each vertex is computed for each non-zero influencing thin-layer. These MVC values are then 
used with a static binding method to blend between overlapping thin-layers. Static Binding vs. Dynamic 
Re-Binding The static binding method of iBind performs a sampling of the MVC space of each layer to support 
dynamic vertices. This extension to MVC permits the vertices of the underlying mesh to freely change 
position without being dynamically rebound. In cg production, when articulating character skin, dynamic 
rebinding is undesirable since it often rebinds vertices to unintended locations, (c) Heat diffusion 
segment weights . such as lips being rebound to a nose during facial animation. Dynamic rebinding is 
still useful, so iBind implements it, but for most cases of character articulation, a static binding 
option is used. Quadruple Structured Coordinates The static binding method of iBind is stable and controllable 
due to its use of Quadruple Structured Coordinates (QSC). QSC are constructed using the original vertex 
and three additional coordinates sampled along each Cartesian axis. QSC are the mean value coordinates 
stored for all four samples, which are then applied to build a non-singular matrix for transforming, 
and ultimately deforming the dynamically changing vertices: Linear precision, smoothness and interpolation 
of this method relies on the stability and behaviors of the mean value interpolant. Evaluation &#38; 
Future Work The use of segmented thin-layers with QSC overcomes the drawbacks of MVC. First, the local 
influence of the control cage can be defined explicitly by allowing variable expansion of the surrounding 
face segments in conjunction with heat diffusion settings. Second, control cages with open holes are 
handled elegantly and result in smooth deformations. Third, the memory footprint is dramatically reduced, 
and run-time performance scales well to real-world examples. And, lastly, it inherently resolves numeric 
instabilities found in co-planar conditions, without requiring special case barycentric coordinates. 
The thin-layer extrusion stage checks for and simply avoids such conditions. iBind is efficient, stable, 
and practical. It adds predictability to geometric lattice deformations, and allows the incoming vertex 
position to change freely while staying statically bound. iBind's static binding is faster, uses less 
memory and is more effective than dynamic binding of MVC. Our method can be more than 30x faster than 
dynamic binding, due to the isolated segment approach. iBind was implemented with OpenMP in C++, yielding 
nearly a 5x improvement in speed due to parallelization, and resulting in interactive deformations on 
real-world cg production examples. Ideas for extending this algorithm include the use of greater interpolation 
methods for blending multiple overlapping, weighted control cages, as well as a smooth subdivision surface 
scheme for evaluating the segmented thin-layers to an implicit limit surface. References [1] Ju, T., 
Schaefer, S., and Warren, J. 2005 Mean Value Coordinates for Closed Triangular Meshes. In Siggraph '05, 
ACM Press, 561-566. [2] Lipman, Y., Levin D., and Cohen-Or, D. 2008 Green Coordinates, In Siggraph '08, 
ACM Press. 78:1-78:10 [3] Joshi, P., et. al., 2007 Harmonic Coordinates for Character Articulation. In 
Siggraph '07, ACM Press, 71:1-71:9 Copyright is held by the author / owner(s). SIGGRAPH 2009, New Orleans, 
Louisiana, August 3 7, 2009. ISBN 978-1-60558-726-4/09/0008 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1598064</article_id>
		<sort_key>740</sort_key>
		<display_label>Article No.</display_label>
		<display_no>74</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>74</seq_no>
		<title><![CDATA[Medial axis techniques for stereoscopic extraction]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1597990.1598064</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1598064</url>
		<abstract>
			<par><![CDATA[<p>To perform the stereoscopic conversion of Disney's <i>Beauty and the Beast</i>, we developed novel extensions to standard medial axis techniques. Distance transforms, directional influences, and segmentation variations were used with medial axis skeletonization to automatically generate depth maps from the hand-drawn images. Overall, our methods significantly reduced the amount of manual rotoscoping and modeling traditionally required for a stereoscopic conversion process.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.6</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>A.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P1617792</person_id>
				<author_profile_id><![CDATA[81442598713]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Evan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Goldberg]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Walt Disney Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Pfaltz, J. L. and Rosenfeld, A. 1967. Computer Representation of Planar Regions by Their Skeletons, <i>Communications of the ACM</i>, 10, 2, 119--125.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Medial Axis Techniques for Stereoscopic Extraction Evan Goldberg Walt Disney Animation Studios 1 Introduction 
4 Directional Influence To perform the stereoscopic conversion of Disney s Beauty and the Beast, we developed 
novel extensions to standard medial axis techniques. Distance transforms, directional influences, and 
segmentation variations were used with medial axis skeletonization to automatically generate depth maps 
from the hand-drawn images. Overall, our methods significantly reduced the amount of manual rotoscoping 
and modeling traditionally required for a stereoscopic conversion process. We define a depth map where 
white pixels are closer to the viewer and black pixels are farther away. Given the distance between two 
cameras and their convergence angle, we can derive an invertible set of functions between depth maps, 
displacement maps, physical geometry, inter-ocular disparity maps, and our final stereoscopic images. 
Medial Axis Figure 1: (a) Input (b) Depth map In computer graphics, the medial axis is a common technique 
to procedurally calculate skeletal structure from a volumetric region. We use the Pfaltz and Rosenfeld 
[1967] definition for the medial axis MA of a shape S described as the set of points that lie at the 
center of all maximally inscribed circles in S (i.e. all points equidistant to at least two closest points 
along the perimeter of S). A medial axis computation of color regions allows us to generate depth maps 
and ultimately a stereoscopic image (Figure 1b). The first step of our process is to partition the original 
image into segmented regions Si. This can be done manually via rotoscope techniques or programmatically 
by defining a threshold over red, green, blue, alpha, or luma color channels. Once appropriate regions 
have been defined, a medial axis MAi is constructed through each Si. Distance Transform Next, we define 
a distance transform for the medial axis skeleton. The borders of Si can be likened to the seam of a 
Mylar balloon, where minimum inflation exists along the edges and maximum inflation at MAi. The depth 
value of all intermediate pixels can be defined by an interpolation function that achieves the desired 
contour from edge to center. The profile of this curve can be linear (for regions with sharp peaks), 
convex (for areas that bow outward), or concave (for sections that bend inward). Depth gradation is heavily 
based on our choice of curvature, so it is important to define this interpolation function judiciously. 
evan.goldberg@disney.com Figure 2: (a) Input (b) Vertical (c) Horizontal (d) Rotated (b-d) Inscribed 
ellipse superimposed in red and MAi as dotted line A traditional medial axis is based on a symmetrical 
inward traversal from the shape periphery --we extend this definition to accommodate cases where it is 
beneficial to influence this computation in a particular direction. A character s face, for example, 
often requires a gradual rounding around the cheeks (sides) and an abrupt falloff near the forehead and 
chin (top and bottom). In this case, we modify our medial axis definition to inscribe ellipses in Si 
with a short vertical axis and a long horizontal axis (Figure 2c). This produces the ideal skeletal structure. 
Other scenarios benefit from ellipses with a short horizontal and a long vertical axis (Figure 2b). Thin, 
pointed areas, such as Lumiere s nose, are best fitted with an ellipse that is rotated about its center 
to capture the region s narrow shape as well as its upwards tilt (Figure 2d). As the character animates 
over time, the best directional influence for Si may be different on every frame. Thus, we animate the 
inscribed shape to capture the most salient geometric features of each pose. 5 Segmentation Variation 
 Figure 3: (a) Input (b) Detailed (c) General The key to a successful depth map is the combination of 
large segmentation regions and small, localized details. A large area, such as a character s alpha mask, 
produces a good general rounding of the entire figure when inflated to its medial axis (Figure 3c). To 
extract interior details, the image is analyzed by its color channels, allowing the ink lines to form 
segmentation boundaries (Figure 3b). Individually, the global extraction is too general and the local 
extraction too extreme. However, using the local as a detailed bump map on top of the global produces 
the desired result.  References PFALTZ, J. L. and ROSENFELD, A. 1967. Computer Representation of Planar 
Regions by Their Skeletons, Communications of the ACM, 10, 2, 119-125. Copyright is held by the author 
/ owner(s). SIGGRAPH 2009, New Orleans, Louisiana, August 3 7, 2009. ISBN 978-1-60558-726-4/09/0008 
 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1598065</article_id>
		<sort_key>750</sort_key>
		<display_label>Article No.</display_label>
		<display_no>75</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>75</seq_no>
		<title><![CDATA[Realistic eye motion using procedural geometric methods]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1597990.1598065</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1598065</url>
		<abstract>
			<par><![CDATA[<p>For late-breaking R&D on the upcoming Disney film <i>King of the Elves</i>, we have embarked upon an adventure called the Realistic Eye Initiative; to investigate realistic procedural methods for one of the most important aspects of a digital character's face -- the eyes. We will leap beyond the simple spheres of previous Disney films, into fantastic, striking realism. Using an anatomically motivated approach, our method to produce realistic convincing deformations of the skin and flesh surrounding the eye is unique, not only due to the novel approaches employed, but also because our method is entirely procedural, based on geometric analysis and packaged into a production friendly, compact, efficient mathematical apparatus in the form of a single black-box deformer that can be easily applied onto any digital creature's face.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.5</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>A.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061.10010063</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures->Computational geometry</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P1617794</person_id>
				<author_profile_id><![CDATA[81442607798]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Erick]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Miller]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Walt Disney Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617793</person_id>
				<author_profile_id><![CDATA[81100542058]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Dmitriy]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Pinskiy]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Walt Disney Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Realistic Eye Motion Using Procedural Geometric Methods Dmitriy Pinskiy Erick Miller Walt Disney Animation 
Studios  Figure 1: (a) Model (b) primitives, coordinate system, quadrants (c) gray-scale render (d) 
spherical coordinate skinning (e) vertex gaze weights For late-breaking R&#38;D on the upcoming Disney 
film King of the Elves, we have embarked upon an adventure called the RealisticEye Initiative; to investigate 
realistic procedural methods for one ofthe most important aspects of a digital character's face the 
eyes.We will leap beyond the simple spheres of previous Disney films,into fantastic, striking realism. 
Using an anatomically motivatedapproach, our method to produce realistic convincing deformationsof the 
skin and flesh surrounding the eye is unique, not only due tothe novel approaches employed, but also 
because our method isentirely procedural, based on geometric analysis and packaged intoa production friendly, 
compact, efficient mathematical apparatus inthe form of a single black-box deformer that can be easily 
appliedonto any digital creature's face. 1 Spherical Coordinate Lid Skinning The movement of the lid 
skin over the eye ball is the mostsignificant motion that occurs during a blink. To simulate this skindeformation, 
we use transformation-propagation. The deformation basis function is given by internally computed spline 
patches. Thepatches are defined by a curvature-continuous blend between keyshapes that include a neutral 
shape (obtained by least-squaresfitting of spline patches into the original eye opening) and user­defined 
closed, surprised, and intermediate shapes. Once the patches are in place, we define a smooth vector 
field ofdisplacements D(x) on the eye-lid skin as follows. First, attachmentweights are set on the skin 
region directly driven by the patches.Next, vertex weights are computed to define a pulling area thatsmoothly 
follows the attachment. Finally, D(x) is set to zero outside of the pulling and attachment regions. In 
the pulling region of D(x) we require nearly perfect harmonic behaviors (i.e. .D(x) = 0); in addition 
our resulting surface shouldmaintain the curvature defined by the sum of the eye ball radiusand thickness 
of the skin. This requires solving a differential equation that defines D(x) for the pulled vertices. 
Instead of solving Poisson's equation with special conditions, which would make computations too expensive 
for interactive use, we introduce anovel, fast relaxation scheme. The essence of this scheme is Laplacian 
smoothing based on spherical coordinates. The system ofspherical coordinates is defined by the location 
and orientation ofthe eye ball E; constructing the map function S(x, E) converts x to local spherical 
coordinates. We define the Laplacian operator as 22 2 .. . .D(S(x,E)) = ( 2 + 2 + 2 )· D(S(x,E)) . S 
(x,E) . S (x,E) . S (x,E) u v radius We are guaranteed convergence of the smoothing iterations to a surface 
with vertices equidistant from the eye's center while direct manipulation of the radius component gives 
us exact control over curvature. 2 Procedurally Unfolding Skin Wrinkles Another blink-driven deformation 
is the unfolding of wrinklesbetween the lower brow and the lids. To make this appear natural,we allow 
the pace of the unfolding to be independent from the gross motion of the blink. The actual unfold is 
done using anisotropic relaxation along the blink's primary direction. dmitriy.pinskiy@disney.com erick.miller@disney.com 
 3 Driven Wrinkles and Lid Pressure As the lids blink, the skin uncompresses and unfolds into asmooth, 
relaxed spherical coordinate space. Thus, we interpolatenew wrinkles and bulging that occur when the 
eye closes. When thespline surfaces meet at blink, a pressure value activates a bulgeforce perpendicular 
to blink direction. The blink also activates normal displaced wrinkles around the eye. Wrinkles and bulgeshave 
vertex weights, allowing subtle control over shape of wrinkleor flesh as it bulges or creases. 4 Shape 
Shifting based on Gaze Angle A blink is only a portion of eye motion; flesh and skin aroundthe eye reacts 
as look direction changes due to underlying anatomy.To mimic this effect we implement three additive 
layers to causeskin reaction to the eye's gaze angle: a flesh layer, a driven lidprogression layer and 
a shape shifting layer. First, the coordinate system of the eye is stored and a bind iscomputed. The 
inverse parent matrix ensures deformation onlyoccurs from changes in local eye space. The eye coordinate 
systemis then decomposed and quaternions are used to extract rotations,initially removing twist from 
the eye matrix. Subtle diffuse weighting is computed, radiating from the eye center. Weightedskinning 
is then applied for a smooth fleshy effect when the eyelooks around. Next, to apply driven lid progression, 
we extract side lookingrotation and build a matrix that only represents upward and downward motion. The 
sign of the lid motion is set by alignment ofthe bind Y-axis and run-time Z-axis (the look vector). An 
additiveweighted binding is applied, and the transition seamlessly blendswhen the eye is neutral and 
also fades off entirely as the blink occurs. Finally, the shape shifting effect is applied. In order 
to achievesubtle almond shaped deformations, the eye coordinate system ispartitioned into six gaze quadrants: 
up, down, and diagonally 45°on left/right, up/down sides. As the eye's look vector aligns with agaze 
quadrant, a weighted transformation causes the skin to changeshape. 5 Cornea / Sclera Collision Primitives 
The anatomical domed like nature of our cornea requires skin-to­eyeball collision and sliding interaction. 
This is achieved using fastsphere primitives fitted to the dome of the cornea/sclera mesh,instead of 
complex polygons. A fast radius test detects lid intersection, and a smooth non-dynamic weighted collision 
is applied, pushing the lid back onto the limit of the primitive withcontrol over magnitude and attenuation, 
resulting in a smooth,convincing sclera/corneal bulge sliding beneath the lids. 6 Future and Planned 
Work We're currently extending this procedural deformation systemwith a layered compression based wrinkle 
algorithm, skin and fleshsimulation to resolve self collisions, and better automatic weightgeneration 
using heat diffusion. Future R&#38;D is also slated for rendering; areas to research here are novel eye 
reflectance models,corneal refraction-distortion texture un-warping, and procedural irisgeneration using 
texture synthesis and Markov Random Fields. Copyright is held by the author / owner(s). SIGGRAPH 2009, 
New Orleans, Louisiana, August 3 7, 2009. ISBN 978-1-60558-726-4/09/0008 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1598066</article_id>
		<sort_key>760</sort_key>
		<display_label>Article No.</display_label>
		<display_no>76</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>76</seq_no>
		<title><![CDATA[Connecting the dots]]></title>
		<subtitle><![CDATA[discovering what's important for creature motion]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1597990.1598066</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1598066</url>
		<abstract>
			<par><![CDATA[<p>We present an experiment designed to reveal some of the key features necessary for conveying creature motion. The ultimate goal is to find the minimal representation necessary to communicate recognizable locomotion or traits that may be communicated to the viewer through motion such as size and attitude. Motion and form are separable for digital characters and each contributes to viewer comprehension of action and intent. Advances in motion capture techniques have increased the amount and fidelity of data available to recreate performances digitally. However, even minimal information contained in point light displays can be sufficient for human gait perception [Johansson 1973]. Manipulating this minimal information can even affect the perceived gender of point light movement. For example, exaggerating the movement of points representing hips or shoulders can bias gender recognition [Cutting et al. 1978]. Minimal representation of animal motion could benefit animators in a number of ways. Creature animators often use animal motion <i>video</i> as visual reference. However, video does not disclose precise anatomical detail, especially when compared to motion capture data. For wild animals MoCap is generally not a viable option. Therefore, such motion must be created through manual key-framing. Even with a well-built rig, this is not efficient or intuitive for defining motion. To build a better system, we need to determine what level of detail of motion information from reference video is required for recognition and, conversely, what details can be safely ignored. To develop a new way of creating and managing animation and animation controls, a better understanding of how we perceive motion itself is necessary. This work takes a first step toward improving our understanding of animal motion and how it can be mapped to controlling creature motion.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[animation]]></kw>
			<kw><![CDATA[eyetracking]]></kw>
			<kw><![CDATA[motion]]></kw>
			<kw><![CDATA[perception]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.8</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010507</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Graphics recognition and interpretation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010507</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Graphics recognition and interpretation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P1617795</person_id>
				<author_profile_id><![CDATA[81442618475]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Meredith]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[McLendon]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Texas A&M University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617796</person_id>
				<author_profile_id><![CDATA[81100069081]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Ann]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[McNamara]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Texas A&M University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617797</person_id>
				<author_profile_id><![CDATA[81332515066]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Tim]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[McLaughlin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Texas A&M University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617798</person_id>
				<author_profile_id><![CDATA[81442601881]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Ravindra]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Dwivedi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Texas A&M University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Cutting, J., Proffit, D., and Kozlowski, L. 1978. A biomechanical invariant for gait perception. <i>J. Experimental Psychology: Human Perception and Performance 4</i>, 3, 357--372.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Johansson, G. 1973. Visual perception of biological motion and a model for its analysis. <i>Perception & Psychophysics 14</i>, 2, 201--211.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Connecting the dots: Discovering what s important for creature motion Meredith McLendon* Ann McNamara 
Tim McLaughlin Ravindra Dwivedi§ Department of Visualization Texas A&#38;M University  Figure 1: A single 
frame (from left to right) full resolution, dot representation, eyetracked data over full and eyetracked 
data over dot . CR Categories: I.3.8 [Computing Methodologies]: Computer Graphics Applications; Keywords: 
animation, motion, perception, eyetracking 1 Introduction We present an experiment designed to reveal 
some of the key fea­tures necessary for conveying creature motion. The ultimate goal is to .nd the minimal 
representation necessary to communicate rec­ognizable locomotion or traits that may be communicated to 
the viewer through motion such as size and attitude. Motion and form are separable for digital characters 
and each contributes to viewer comprehension of action and intent. Advances in motion capture techniques 
have increased the amount and .delity of data available to recreate performances digitally. However, 
even minimal infor­mation contained in point light displays can be suf.cient for human gait perception 
[Johansson 1973]. Manipulating this minimal in­formation can even affect the perceived gender of point 
light move­ment. For example, exaggerating the movement of points repre­senting hips or shoulders can 
bias gender recognition [Cutting et al. 1978]. Minimal representation of animal motion could bene.t an­imators 
in a number of ways. Creature animators often use animal motion video as visual reference. However, video 
does not dis­close precise anatomical detail, especially when compared to mo­tion capture data. For wild 
animals MoCap is generally not a viable option. Therefore, such motion must be created through manual 
key-framing. Even with a well-built rig, this is not ef.cient or intu­itive for de.ning motion. To build 
a better system, we need to de­termine what level of detail of motion information from reference video 
is required for recognition and, conversely, what details can be safely ignored. To develop a new way 
of creating and managing animation and animation controls, a better understanding of how we perceive 
motion itself is necessary. This work takes a .rst step toward improving our understanding of animal 
motion and how it can be mapped to controlling creature motion. 2 Experiment We conducted similar experiments 
to those of [Cutting et al. 1978] using motion video of animals in side view where kinematic motion *mgmclendon@viz.tamu.edu 
ann@viz.tamu.edu timm@viz.tamu.edu §ravin@viz.tamu.edu is most apparent. Five participants were eyetracked 
while viewing 20 video segments. Half the video was full resolution while the other half featured a sparse 
point representation of an animal. The sparse representations were viewed .rst, then the full. Within 
each type presentation was randomized. Representative points were se­lected by hand based on knowledge 
of the animals anatomy. In addition to asking the subjects to identify the animal and charac­teristics, 
we used eye-tracking to determine where people looked in the hopes of learning what is important for 
species recognition. Eye-tracking data allows comparisons between how the viewers vi­sually process the 
full resolution video and how they visually pro­cess sparse point display, a single example is shown 
in Figure 1. 2.1 Results In the full resolution images, as expected, participants correctly identi.ed 
the animal 100% of the time. What is interesting is that 38% of the time people could identify the animal 
correctly just from the sparse point representation. However, some animals proved more recognizable in 
sparse form than others. The giraffe for exam­ple was correctly identi.ed from points by all but one 
participant, whereas all .ve failed to recognize the bear (not side facing) or the zebra correctly. 
 3 Conclusion &#38; Future Work Our investigation represents an initial step to answer the question of 
how best to minimally represent the signature motion of a creature. Results show that in certain cases 
a collection of points can ade­quately represent recognizable motion. Analysis of eye movements help 
reveal similarities and differences in how representations are processed. Further experiments should 
help us reveal the minimal information necessary. References CUTTING, J., PROFFIT, D., AND KOZLOWSKI, 
L. 1978. A biome­chanical invariant for gait perception. J. Experimental Psychol­ogy: Human Perception 
and Performance 4, 3, 357 372. JOHANSSON, G. 1973. Visual perception of biological motion and a model 
for its analysis. Perception &#38; Psychophysics 14, 2, 201 211. Copyright is held by the author / owner(s). 
SIGGRAPH 2009, New Orleans, Louisiana, August 3 7, 2009. ISBN 978-1-60558-726-4/09/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1598067</article_id>
		<sort_key>770</sort_key>
		<display_label>Article No.</display_label>
		<display_no>77</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>77</seq_no>
		<title><![CDATA[Motion capture for natural tree animation]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1597990.1598067</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1598067</url>
		<abstract>
			<par><![CDATA[<p>Animated trees are important in the creation of virtual worlds, games, and animation, and can be used as visual cues for weather and mood in visual storytelling. We address the problem of animating natural trees in the wind, making use of an optical motion capture system. By recording tree motion using a motion capture system, we avoid building complicated physical and mathematical models, but we must solve problems associated with modeling the exact geometry of a natural tree, repairing gaps in motion capture data, adding leaves and twigs to the model, and applying motion capture data to the resulting model.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Animation</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>A.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P1617799</person_id>
				<author_profile_id><![CDATA[81442598392]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Jie]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Long]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[BYU Computer Science]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617800</person_id>
				<author_profile_id><![CDATA[81442616320]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Cory]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Reimschussel]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[BYU Computer Science]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617801</person_id>
				<author_profile_id><![CDATA[81442610648]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Ontario]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Britton]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[BYU Computer Science]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617802</person_id>
				<author_profile_id><![CDATA[81407592837]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Michael]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Jones]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[BYU Computer Science]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Diener, J., Rodriguez, M., Baboud, L., and Reveret, L. 2009. Wind projection basis for real-time animation of trees. <i>Computer Graphics Forum (Proceedings of Eurographics 2009) 28</i>, 2, 533--540.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Habel, R., Kusternig, A., and Wimmer, M. 2009. Physically guided animation of trees. <i>Computer Graphics Forum (Proceedings of Eurographics 2009) 28</i>, 2, 523--532.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1598068</article_id>
		<sort_key>780</sort_key>
		<display_label>Article No.</display_label>
		<display_no>78</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>78</seq_no>
		<title><![CDATA[Beyond triangles]]></title>
		<subtitle><![CDATA[gigavoxels effects in video games]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1597990.1598068</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1598068</url>
		<abstract>
			<par><![CDATA[<p>Voxel representations are commonly used for scientific data visualization, but also for many special effects involving complex or fuzzy data (e.g., clouds, smoke, foam). Since voxel rendering permits better and easier filtering than triangle-based representations it is also an efficient high-quality choice for complex meshes (with several triangles per pixel) and detailed geometric data (<i>e.g.</i>, boats in <i>Pirates of the Caribbean</i>).</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>K.8.0</cat_node>
				<descriptor>Games</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>A.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010476.10011187.10011190</concept_id>
				<concept_desc>CCS->Applied computing->Computers in other domains->Personal computers and PC applications->Computer games</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10011007.10010940.10010941.10010969.10010970</concept_id>
				<concept_desc>CCS->Software and its engineering->Software organization and properties->Contextual software domains->Virtual worlds software->Interactive games</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002951.10003227.10003251.10003258</concept_id>
				<concept_desc>CCS->Information systems->Information systems applications->Multimedia information systems->Massively multiplayer online games</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P1617803</person_id>
				<author_profile_id><![CDATA[81421599021]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Cyril]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Crassin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[INRIA / Grenoble Univ.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617804</person_id>
				<author_profile_id><![CDATA[81100506206]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Fabrice]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Neyret]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[INRIA / Grenoble Univ.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617805</person_id>
				<author_profile_id><![CDATA[81100198784]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Sylvain]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lefebvre]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[INRIA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617806</person_id>
				<author_profile_id><![CDATA[81100048438]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Miguel]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sainz]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[NVIDIA Corporation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617807</person_id>
				<author_profile_id><![CDATA[81310501633]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Elmar]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Eisemann]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Saarland Univ. / MPI Informatik]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1507152</ref_obj_id>
				<ref_obj_pid>1507149</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Crassin, C., Neyret, F., Lefebvre, S., and Eisemann, E. 2009. Gigavoxels: Ray-guided streaming for efficient and detailed voxel rendering. In <i>ACM SIGGRAPH Symposium on Interactive 3D Graphics and Games (13D)</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1598069</article_id>
		<sort_key>790</sort_key>
		<display_label>Article No.</display_label>
		<display_no>79</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>79</seq_no>
		<title><![CDATA[Single pass depth peeling via CUDA rasterizer]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1597990.1598069</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1598069</url>
		<abstract>
			<par><![CDATA[<p>Multi-fragment effects play important roles on many graphics applications, which require operations on more than one fragment per pixel. The classical depth peeling algorithm [Everitt 2001] peels off one layer each pass, but the performance degrades for large scenes. We prefer to capture multiple fragments in a single pass, which is difficult because the fragments generated in graphics pipeline are not allowed to be scattered to arbitrary positions of the buffers. Compute unified device architecture (CUDA) [NVIDIA 2008] provides more flexible control over the GPU memory, but accessing of the fragments generated by graphics pipeline is not yet supported. In this work we design a CUDA rasterizer so that many graphics applications can benefit from the free control of GPU memory, especially for the multi-fragment effects. We present two efficient schemes to capture and sort multiple fragments per pixel in a single geometry pass via the atomic operations of CUDA without read-modify-write (RMW) hazards. Experimental results show significant speedup to classical depth peeling, especially for large scenes.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.1</cat_node>
				<descriptor>Raster display devices</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>A.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372.10010373</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering->Rasterization</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010583.10010588.10010591</concept_id>
				<concept_desc>CCS->Hardware->Communication hardware, interfaces and storage->Displays and imagers</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P1617808</person_id>
				<author_profile_id><![CDATA[81440596288]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Fang]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Liu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Chinese Academy of Sciences]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617809</person_id>
				<author_profile_id><![CDATA[81440592538]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Meng-Cheng]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Huang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Chinese Academy of Sciences]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617810</person_id>
				<author_profile_id><![CDATA[81410593094]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Xue-Hui]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Liu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Chinese Academy of Sciences]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617811</person_id>
				<author_profile_id><![CDATA[81100657893]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[En-Hua]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Chinese Academy of Sciences and University of Macau]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Carpenter, L. 1984. The a-buffer, an antialiased hidden surface method. In <i>Proceedings of the 11th annual conference on Computer graphics and interactive techniques</i>, 103--108.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Everitt, C. 2001. Interactive order-independent transparency. Tech. rep., NVIDIA Corporation.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1278806</ref_obj_id>
				<ref_obj_pid>1278780</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Myers, K., and Bavoil, L. 2007. Stencil routed a-buffer. <i>ACM SIGGRAPH 2007 Technical Sketch Program</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[NVIDIA. 2008. Nvidia cuda: Compute unified device architecture.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<article_sponsors>
			<funding_agency>NSFC</funding_agency>
			<grant_numbers>
				<grant_number>60573155</grant_number>
			</grant_numbers>
		</article_sponsors>
		<article_sponsors>
			<funding_agency>National 863 High-Tec Grant</funding_agency>
			<grant_numbers>
				<grant_number>2008AA01Z301</grant_number>
			</grant_numbers>
		</article_sponsors>
		<article_sponsors>
			<funding_agency>China Basic S&#38;T 973 Research Grant</funding_agency>
			<grant_numbers>
				<grant_number>2009CB320802</grant_number>
			</grant_numbers>
		</article_sponsors>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1598070</article_id>
		<sort_key>800</sort_key>
		<display_label>Article No.</display_label>
		<display_no>80</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>80</seq_no>
		<title><![CDATA[Design and self-assembly of DNA into nanoscale 3D shapes]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1597990.1598070</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1598070</url>
		<abstract>
			<par><![CDATA[<p>We have developed a general method for solving a key challenge for nanotechnology: programmable self-assembly of complex, three-dimensional nanostructures [Douglas, Dietz, et al. 2009]. Previously, scaffolded DNA origami has been used to build arbitrary flat shapes 100 nm in diameter and almost twice the mass of a ribosome [Rothemund 2006]. Now we have succeeded in building custom three-dimensional structures that can be conceived as stacks of nearly flat layers of DNA. Successful extension from two dimensions to three dimensions in this way depended critically on calibration of folding conditions. A general capability for building complex, three-dimensional nanostructures will be of great interest to biologists, chemists, physicists, engineers, and computer scientists.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.5</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>A.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061.10010063</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures->Computational geometry</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P1617812</person_id>
				<author_profile_id><![CDATA[81442598646]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Shawn]]></first_name>
				<middle_name><![CDATA[M.]]></middle_name>
				<last_name><![CDATA[Douglas]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Harvard University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617814</person_id>
				<author_profile_id><![CDATA[81442608509]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Hendrik]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Dietz]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Harvard University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617815</person_id>
				<author_profile_id><![CDATA[81442619215]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Tim]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Liedl]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Harvard University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617816</person_id>
				<author_profile_id><![CDATA[81442613156]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Bj&#246;rn]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[H&#246;gberg]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Harvard University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617817</person_id>
				<author_profile_id><![CDATA[81442605563]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Franziska]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Graf]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Harvard University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617818</person_id>
				<author_profile_id><![CDATA[81442615065]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Adam]]></first_name>
				<middle_name><![CDATA[H.]]></middle_name>
				<last_name><![CDATA[Marblestone]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Harvard University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617819</person_id>
				<author_profile_id><![CDATA[81442617806]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>7</seq_no>
				<first_name><![CDATA[Surat]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Teerapittayanon]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Harvard University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617820</person_id>
				<author_profile_id><![CDATA[81442609645]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>8</seq_no>
				<first_name><![CDATA[Alejandro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Vazquez]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Harvard University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617821</person_id>
				<author_profile_id><![CDATA[81547099956]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>9</seq_no>
				<first_name><![CDATA[George]]></first_name>
				<middle_name><![CDATA[M.]]></middle_name>
				<last_name><![CDATA[Church]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Harvard University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1617813</person_id>
				<author_profile_id><![CDATA[81442611751]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>10</seq_no>
				<first_name><![CDATA[William]]></first_name>
				<middle_name><![CDATA[M.]]></middle_name>
				<last_name><![CDATA[Shih]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Harvard University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Dietz, H., Douglas, S. M., Shih, W. M. 2009. Folding DNA into twisted and curved nanoscale shapes. <i>Science</i>. In review.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Douglas, S. M., Dietz H., Liedl, T., H&#246;gberg, B., Graf, F., Shih, W. M. 2009. Self assembly of DNA into three-dimensional nanoscale shapes. <i>Nature</i>, 459, 414--418.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Douglas, S. M., Marblestone, A. M., Teerapittayanon, S., Vazquez, A., Church, G. M., Shih, W. M. 2009. Rapid prototyping of three-dimensional DNA-origami shapes with caDNAno. <i>Nucleic Acids Res</i>. In press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Rothemund, P. W. K. 2006. Folding DNA to create nanoscale shapes and patterns. <i>Nature</i>, 440, 297--302.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Design and Self-Assembly of DNA into Nanoscale 3D Shapes Figure 1: Three dimensional DNA origami shapes. 
(a) Self-assembled monolith shape comprised of 60 DNA double helices. Top: cylinder model, where each 
cylinder represents a DNA double helix. Bottom: negative-stain micrograph obtained by transmission electron 
microscopy (TEM). (b) Cylinder model and TEM micrograph of square nut shape with internal channel. (c) 
Icosahedron, hierarchically assembled from three monomers (red, blue, green). (d) Screenshot of computer-aided 
design software, caDNAno (http://cadnano.org/). 1 Introduction We have developed a general method for 
solving a key challenge for nanotechnology: programmable self-assembly of complex, three-dimensional 
nanostructures [Douglas, Dietz, et al. 2009]. Previously, scaffolded DNA origami has been used to build 
arbitrary .at shapes 100 nm in diameter and almost twice the mass of a ribosome [Rothemund 2006]. Now 
we have succeeded in building custom three-dimensional structures that can be conceived as stacks of 
nearly .at layers of DNA. Successful extension from two dimensions to three dimensions in this way depended 
critically on calibration of folding conditions. A general capability for building complex, three-dimensional 
nanostructures will be of great interest to biologists, chemists, physicists, engineers, and computer 
scientists. 2 Computer-aided Design This technology has matured to the point where proper software support 
has become a critical bottleneck, as the design challenge requires keeping track of 8000 DNA base pairs 
or more in three dimensions. Such software, which we present in this talk, will be key for adoption of 
this general building methodology in the scienti.c community [Douglas, Marblestone, et al. 2009]. We 
describe in detail a graphical-interface-based computer-aided­design environment that greatly accelerates 
design of our three- dimensional DNA objects. We demonstrate the utility of caDNAno by using it for rapid 
design of a series of rectangular blocks of varying dimensions and .nd that one particular motif is more 
robust than the others. The software is available at http://cadnano.org/, along with example designs 
and video tutorials demonstrating their construction. The source code is released under the MIT license. 
 3 Recent Advances and Future Directions The ability to produce complex shapes that are nonlinear is 
a cornerstone for macroscale technology and for the function of cellular machinery. We also introduce 
key methods to engineer an unprecedented diversity of nonlinear shapes on the nanoscale [Dietz et al. 
2009]. These methods will be critical for future development of advanced functional nanotechnology. We 
hope to attract new users and interdisciplinary collaborators for development of more advanced versions 
of the software,complementary experimental methods, and demand-meetingapplications of the technology, 
such as nanoscale electronic circuitry, smart drug-delivery systems, and scienti.c tools for biophysics 
research. References DIETZ, H., DOUGLAS, SM., SHIH, WM. 2009. Folding DNA into twisted and curved nanoscale 
shapes. Science. In review. DOUGLAS, S.M., DIETZ H., LIEDL, T., HÖGBERG, B., GRAF, F., SHIH, W.M. 2009. 
Self assembly of DNA into three­dimensional nanoscale shapes. Nature, 459, 414 418. DOUGLAS, S.M., MARBLESTONE, 
A.M., TEERAPITTAYANON, S., VAZQUEZ, A., CHURCH, G.M., SHIH, W.M. 2009. Rapid prototyping of three-dimensional 
DNA-origami shapes withcaDNAno. Nucleic Acids Res. In press. ROTHEMUND, P.W.K. 2006. Folding DNA to create 
nanoscale shapes and patterns. Nature, 440, 297 302. * email: shawn.douglas@wyss.harvard.edu william_shih@dfci.harvard.edu 
Copyright is held by the author / owner(s). SIGGRAPH 2009, New Orleans, Louisiana, August 3 7, 2009. 
ISBN 978-1-60558-726-4/09/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
</content>
</proceeding>
