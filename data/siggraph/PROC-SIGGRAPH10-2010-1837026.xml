<?xml version="1.0" encoding="ISO-8859-1"?>
<!DOCTYPE proceeding SYSTEM "proceeding.dtd">
<proceeding ver="6.0" ts="04/10/2010">
<conference_rec>
	<conference_date>
		<start_date>07/26/2010</start_date>
		<end_date>07/30/2010</end_date>
	</conference_date>
	<conference_loc>
		<city><![CDATA[Los Angeles]]></city>
		<state>California</state>
		<country></country>
	</conference_loc>
	<conference_url></conference_url>
</conference_rec>
<series_rec>
	<series_name>
		<series_id>SERIES382</series_id>
		<series_title><![CDATA[International Conference on Computer Graphics and Interactive Techniques]]></series_title>
		<series_vol></series_vol>
	</series_name>
</series_rec>
<proceeding_rec>
	<proc_id>1837026</proc_id>
	<acronym>SIGGRAPH '10</acronym>
	<proc_desc>ACM SIGGRAPH 2010 Talks</proc_desc>
	<conference_number>2010</conference_number>
	<proc_class>conference</proc_class>
	<proc_title></proc_title>
	<proc_subtitle></proc_subtitle>
	<proc_volume_no></proc_volume_no>
	<isbn13>978-1-4503-0394-1</isbn13>
	<issn></issn>
	<eissn></eissn>
	<copyright_year>2010</copyright_year>
	<publication_date>07-26-2010</publication_date>
	<pages>56</pages>
	<plus_pages></plus_pages>
	<price><![CDATA[]]></price>
	<other_source></other_source>
	<abstract>
		<par><![CDATA[<p>SIGGRAPH 2010 Talks provide a broad spectrum of presentations on recent achievements in all areas of computer graphics and interactive techniques, including art, design, animation, visual effects, interactivity, research, and engineering.</p> <p>Talks provide a lightweight alternative to formal publication. They often highlight the latest developments before publication, present ideas that are still in progress, or showcase how computer graphics and interactive techniques are actually implemented and used, in graphics production or other fields. Talks can take you behind the scenes and into the minds of SIGGRAPH 2010 creators.</p>]]></par>
	</abstract>
	<publisher>
		<publisher_id>PUB27</publisher_id>
		<publisher_code>ACMNY</publisher_code>
		<publisher_name>ACM</publisher_name>
		<publisher_address>2 Penn Plaza, Suite 701</publisher_address>
		<publisher_city>New York</publisher_city>
		<publisher_state>NY</publisher_state>
		<publisher_country>USA</publisher_country>
		<publisher_zip_code>10121-0701</publisher_zip_code>
		<publisher_contact>Bernard Rous</publisher_contact>
		<publisher_phone>212 869-7440</publisher_phone>
		<publisher_isbn_prefix></publisher_isbn_prefix>
		<publisher_url>www.acm.org/publications</publisher_url>
	</publisher>
	<sponsor_rec>
		<sponsor>
			<sponsor_id>SP932</sponsor_id>
			<sponsor_name>ACM Special Interest Group on Computer Graphics and Interactive Techniques</sponsor_name>
			<sponsor_abbr>SIGGRAPH</sponsor_abbr>
		</sponsor>
	</sponsor_rec>
	<categories>
		<primary_category>
			<cat_node/>
			<descriptor/>
			<type/>
		</primary_category>
	</categories>
	<chair_editor>
		<ch_ed>
			<person_id>P2264764</person_id>
			<author_profile_id><![CDATA[81341494013]]></author_profile_id>
			<orcid_id></orcid_id>
			<seq_no>1</seq_no>
			<first_name><![CDATA[James]]></first_name>
			<middle_name><![CDATA[]]></middle_name>
			<last_name><![CDATA[Mohler]]></last_name>
			<suffix><![CDATA[]]></suffix>
			<affiliation><![CDATA[Purdue University]]></affiliation>
			<role><![CDATA[Conference Chair]]></role>
			<email_address><![CDATA[]]></email_address>
		</ch_ed>
	</chair_editor>
	<ccc>
		<copyright_holder>
			<copyright_holder_name>ACM</copyright_holder_name>
			<copyright_holder_year>2010</copyright_holder_year>
		</copyright_holder>
	</ccc>
</proceeding_rec>
<content>
	<section>
		<section_id>1837027</section_id>
		<sort_key>10</sort_key>
		<section_seq_no>1</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Rendering Intangibles]]></section_title>
		<section_page_from>1</section_page_from>
	<article_rec>
		<article_id>1837028</article_id>
		<sort_key>20</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>1</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Lighting and rendering <i>Alice in Wonderland</i>]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1837026.1837028</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1837028</url>
		<abstract>
			<par><![CDATA[<p>Recent films such as <i>Cloudy with a Chance of Meatballs, 2012</i> and <i>Alice in Wonderland (Alice)</i> have represented dramatic changes in the way Sony Pictures Imageworks produces imagery. We have moved away from the traditional biased, multi-pass rendering system to a largely single-pass, global illumination system incorporating modern ray-tracing and physically based shading techniques. We will discuss how these changes applied to our work on <i>Alice</i>, and look at specific examples from the film. Topics discussed will include: motivations behind moving to a physically based rendering system and how such motivations would ideally manifest, micro-facet illumination models, global illumination and layered materials. We will talk about the impact of these developments on our creative approach and lighting procedures in production, as well as some of the unintended consequences of these changes.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.3</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2264765</person_id>
				<author_profile_id><![CDATA[81466641907]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Adam]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Martinez]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Sony Pictures Imageworks]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264766</person_id>
				<author_profile_id><![CDATA[81335498514]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Terrance]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tornberg]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Sony Pictures Imageworks]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>2383874</ref_obj_id>
				<ref_obj_pid>2383847</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[
{WALTER07} Walter, B., Marschner, S. R., Li, H., and Torrance, K. E. Microfacet Models for Refraction through Rough Surfaces. In <i>Rendering Techniques (Proc. EG Symposium on Rendering</i>). (2007)
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>927297</ref_obj_id>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[{VEACH97} Eric Veach Robust Monte Carlo Methods for Light Transport Simulation, Stanford University, (1997)]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Lighting and Rendering Alice in Wonderland Adam Martinez Terrance Tornberg Sony Pictures Imageworks 
 Introduction Recent films such as Cloudy with a Chance of Meatballs, 2012 and Alice in Wonderland (Alice) 
have represented dramatic changes in the way Sony Pictures Imageworks produces imagery. We have moved 
away from the traditional biased, multi-pass rendering system to a largely single-pass, global illumination 
system incorporating modern ray-tracing and physically based shading techniques. We will discuss how 
these changes applied to our work on Alice, and look at specific examples from the film. Topics discussed 
will include: motivations behind moving to a physically based rendering system and how such motivations 
would ideally manifest, micro-facet illumination models, global illumination and layered materials. We 
will talk about the impact of these developments on our creative approach and lighting procedures in 
production, as well as some of the unintended consequences of these changes. 1. Historical Background 
For projects as recent as Watchmen, Sony Pictures Imageworks employed a rendering system that does not 
natively account for global illumination effects. This system was heavily pass-dependant: shadows were 
depth-mapped, color bleeding came from point clouds, and reflection or environment occlusion was rendered 
in a separate pass, often in a separate renderer. Biased techniques such as shadow map and point cloud 
generation meant smaller sub-pipelines had to be developed and maintained. These techniques are largely 
phenomenological and physically inaccurate. Fidelity of the final result was subject to the quality of 
the passes that were generated previously. In addition, there was an enormous potential for version mismatches 
between separate passes. Over the long term, this means more passes and more time devoted to managing 
those passes. 1.1 Arnold Renderer Imageworks internal global illumination renderer, Arnold, was largely 
considered an experimental and utility renderer. The facility moved it into production as the primary 
renderer for Cloudy with a Chance of Meatballs and proved it could handle the volume of data required 
for a large project. The decision to use Arnold on Alice was not taken lightly. This would mean a considerable 
commitment of development resources to author a pipeline to meet the needs of a large visual effects 
project. A massive educational effort also had to be undertaken as a lot of the practices from the previous 
system were no longer applicable. 2. Arnold in Wonderland For Alice, there was a strong desire to leverage 
the power of the ray tracer to implement a physically based shading system, but also to expand the capabilities 
of the shading system during the look development stage. This coincided with a facility mandate to provide 
photo-realistic results out of the gate. Previously, realism was difficult to control, and maintain, 
in the face of creative demands. Supervisors wanted a system that would allow them to start the creative 
process sooner, and maintain a consistent level of realism. We implemented three key techniques to meet 
these goals: 2.1 Shading For Alice we began with a direct implementation of the BSDF and Monte-Carlo 
sampling functions as presented in [WALTER07]. This gave us out-of-the-box coherency between the direct 
specular highlights and traced glossy reflections and refractions. These physically plausible shading 
models provided users a very realistic look with limited parameterization. Our implementation also allowed 
us to easily deploy multiple importance sampling techniques to reduce noise in final images [VEACH97]. 
2.2 Material Layering We created a generalized methodology for arbitrarily layering discrete materials. 
This afforded a previously unavailable level of flexibility and experimentation in the look development 
process. While material layering has been implemented in commercial applications for a long time, the 
potential for added computational expense makes it a rare occurrence in feature film production. Our 
approach allowed artists to create separate materials with entirely distinct settings, such as metal, 
paint, glass or dirt. Artists could then composite these materials in an intuitive manner while preserving 
consistent behavior and reducing computational overhead. 2.3 Workflow Ray tracing and physically based 
shading techniques had a significant impact on the workflow of look development and shot lighting. A 
key distinction was turning away from the traditional composition of materials as a function of highlight 
size and shape. Instead, there was new emphasis on surface roughness and dielectric response. Look developers 
were constructing materials in a manner more consistent with the behavior of materials in the real world. 
Highlight size, shadow softness and diffuse illumination wrap were all functions of the area light source 
size, rather than a property of the material. First takes of assets and shots had the full complement 
of shading effects out of the box. Color bleed, traced reflections, refractions and soft shadowed area 
lights with corresponding specular highlights were all part of the first render. Consistent material 
behavior meant that lighters focused on lighting to the sequence more than to the individual shot, allowing 
creative feedback to begin earlier. 3. REFERENCES [1] [WALTER07] Walter, B., Marschner, S. R., Li, H., 
and Torrance, K. E. Microfacet Models for Refraction through Rough Surfaces. In Rendering Techniques 
(Proc. EG Symposium on Rendering). (2007) [2] [VEACH97] Eric Veach Robust Monte Carlo Methods for Light 
Transport Simulation, Stanford University, (1997)  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2010</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1837029</article_id>
		<sort_key>30</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>2</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Fast furry ray gathering]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1837026.1837029</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1837029</url>
		<abstract>
			<par><![CDATA[<p>We present several techniques for efficiently gathering diffuse and specular reflection rays originating from hair, recently put in production at Rhythm & Hues. We refine the hair BRDFs from [Neulander 2004] into a new <i>cone-shell</i> model that is more generally applicable and realistic, yet remains simple and fast to evaluate. Next, we address the chief shortcoming of [Neulander 2004]'s hair occlusion model by coupling it with rigorous occlusion-testing of skin geometry. Finally, we deploy a per-strand shading cache to improve performance, adopting a filtering technique in the spirit of trilinear mip-mapping.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.2.10</cat_node>
				<descriptor>Intensity, color, photometry, and thresholding</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.4.8</cat_node>
				<descriptor>Photometry</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.1</cat_node>
				<descriptor>Reflectance</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.8</cat_node>
				<descriptor>Shading</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010225.10010227</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision tasks->Scene understanding</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372.10010376</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering->Reflectance modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010225.10010227</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision tasks->Scene understanding</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010225</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision tasks</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2264767</person_id>
				<author_profile_id><![CDATA[81100390217]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Ivan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Neulander]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Rhythm & Hues Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>74361</ref_obj_id>
				<ref_obj_pid>74334</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Kajiya, J. T., and Kay, T. L. 1989. Rendering fur with three dimensional textures. <i>SIGGRAPH Comput. Graph. 23</i>, 3, 271--280.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1186277</ref_obj_id>
				<ref_obj_pid>1186223</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Neulander, I. 2004. Quick image-based lighting of hair. In <i>SIGGRAPH 2004 Sketches</i>, 43.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Fast Furry Ray Gathering Ivan Neulander Rhythm&#38;Hues Studios We present several techniques for ef.cientlygathering 
diffuse and specular re.ection rays originating from hair, recently put in produc­tionatRhythm&#38;Hues.We 
re.nethehair BRDFsfrom [Neulander 2004] into a new cone-shell model that is more generally applicable 
and realistic,yet remainssimpleandfasttoevaluate.Next,we address the chief shortcoming of [Neulander 
2004] s hair occlusion model by coupling it with rigorous occlusion-testing of skin geometry. Finally, 
we deploy a per-strand shading cache to improve performance, adopt­ing a .ltering technique in the spirit 
of trilinear mip-mapping. Figure 1: 640x480 images, each featuring 52k strands with 50 diffuse and30 
speculargather rays per shading point: renderedin57 seconds per frame on an Athlon64 X2 2800MHz, single 
thread. The Cone-Shell Model Our cone-shell BRDFis basedonthe in.nitesimal cylindermodelof [Kajiya and 
Kay 1989] and is designed for ef.cient generation of scat­tered diffuse and specular re.ection rays with 
nonzero weight. Unlike the IBL-centric BRDFs in [Neulander 2004], we sample rays within a nonzero solid 
angle: the volume between two concentric cones. In our model, .min, .max are the angular bounds of the 
ray scatter cone­shell (measured from the z-axis), ranging from an in.nitesimally thin surface when .min 
= .max to the full sphere when [.min,.max] = [0,p]. Given a pair of random variables .1 and .2 in U(0, 
1), we generate sample directions in a canonical coordinate system that aligns the z­axis with the hair 
tangent. x =1- z2cos(2p.2) y =1- z2sin(2p.2) z = cos .max + .1(cos .min - cos . max) Our diffuse scattering 
over the full sphere matches the Kajiya model, though ours canbe limitedtoalesser solid angleif desired. 
Our specular scattering differs from Kajiya s in that our rays are restricted to the [.min,.max] range 
with a linearly zero-tapered cos f weight1 in­stead of being distributed over the full sphere with cosn 
weight. This makes our specular sampling potentially more concise yet visually similar to Kajiya s for 
suitably chosen . bounds. 1f is the angle betweenagather ray and the midpoint cone at .min+.max .For 
2 full-sphere scattering, the midpoint cone is the xy plane, so cos f equates to the sine of the angle 
between the ray and the z-axis, as per Kajiya s diffuse model. Copyright is held by the author / owner(s). 
SIGGRAPH 2010, Los Angeles, California, July 25 29, 2010. ISBN 978-1-4503-0210-4/10/0007 For importance 
sampling, we can generate rays with density roughly proportional to our BRDF weight.2 Assuming uniform 
incident ra­diance, full-sphere scattering would ideally draw .1 from a Wigner Semicircle Distribution 
shifted to the interval [0,1]. Since incident ra­diancevariesin practice,weneeda .atter distributionthan 
this. Based on empirical trials, we selected a raised triangular distribution with a pdf of 49( 52 -|x 
- 21|).We remap . from U(0,1) as follows: . . 12(8+ 9. ) - 2: . = 1/2 .1 = .3-12(17- 9. ) : . > 1/2 
Volumetric Occlusion Approximation Computing accurate hair occlusion tends to be slow due to the geo­metric 
complexity and shading cost involved. Although we provide this option with ray tracing, we have found 
that occlusion by rela­tively short, dense hair (e.g. most fur) can be modeled by a combi­nation of accurate 
skin occlusion and approximate hair occlusion via the volumetric occlusion model in [Neulander 2004]. 
The latter works well at short range, is extremely fast, immune to noise, and highly art-directable. 
Its main drawback is itsfailure to account for nonlocal occlusion, as with cast shadows. By tracing rays 
to the skin and other hard-surface geometry, we address this with an acceptable balance of speed and 
.delity. We enhance [Neulander 2004] s original volumetric model slightly by computing the occlusion 
height hO at each control point as a pro­jection of that point s position onto the occlusion normal NO. 
This noticeably improves accuracyfor strands that run parallel to the skin, a common occurrence in practice. 
 Shading Cache Traditional irradiance caching, as describedbyWard,is unsuitable for hairduetoabruptsurfacevariation 
perpendiculartoeach strand.How­ever, irradiance and even re.ected radiance vary more gradually along 
anygiven strand. Hence,we cachethe resultsof ourdiffuseand specu­largather operationsata discrete numberof 
shading points along each strand, and interpolate in between. Wede.neadiffuseand specular cache densityin 
termsofpixelsper shading point, so that the total number of cached shading points varies with the screen-space 
length of each strand, subject to some limits. Our cache is populated on demand: only the cache points 
surrounding a requested shading sample will ever be populated. For frame-to-frame coherence, each shading 
point uses the same set of canonical sample rays (driven by the same strand-based ran­dom number sequence), 
rotated to match the hair tangent at that point. Further, we constrain sample point counts to powers 
of two and eval­uate the cache using both the full sample count and half the sample count, interpolating 
the .nal color between these intermediate results. Thisavoids poppingasthe cache density changesfrom 
frameto frame, similar to how trilinear texture .ltering interpolates between two suc­cessive mipmap 
resolutions. References KAJIYA, J. T., AND KAY, T. L. 1989. Rendering fur with three dimensional textures. 
SIGGRAPH Comput. Graph. 23, 3, 271 280. NEULANDER, I. 2004. Quick image-based lighting of hair. In SIG-GRAPH 
2004 Sketches, 43. 2Importance sampling is crucial for a specular lobe such as cosn but less bene.cial 
for ours, which spans a smaller nonzero range.  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2010</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1837030</article_id>
		<sort_key>40</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>3</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[An error estimation framework for photon density estimation]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1837026.1837030</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1837030</url>
		<abstract>
			<par><![CDATA[<p>Estimating error is an important task in rendering. For many predictive rendering applications such as simulation of car headlights, lighting design, or architectural design it is import to provide an estimate of the actual error to ensure confidence and accuracy of the results. Even for applications where accuracy is not critical, error estimation is still useful for improving aspects of the rendering algorithm. Examples include terminating the rendering algorithm automatically, adaptive sampling where the parameters of the rendering algorithm are adjusted dynamically to minimize the error, and interpolating sparsely sampled radiance within a given error bound.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.3</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2264768</person_id>
				<author_profile_id><![CDATA[81320490282]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Toshiya]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hachisuka]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[UC San Diego]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264769</person_id>
				<author_profile_id><![CDATA[81100389194]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Wojciech]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Jarosz]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[UC San Diego and Disney Research Z&#252;rich]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264770</person_id>
				<author_profile_id><![CDATA[81100640205]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Henrik]]></first_name>
				<middle_name><![CDATA[Wann]]></middle_name>
				<last_name><![CDATA[Jensen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[UC San Diego]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1409083</ref_obj_id>
				<ref_obj_pid>1457515</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Hachisuka, T., Ogaki, S., and Jensen, H. W. 2008. Progressive photon mapping. <i>ACM Transactions on Graphics (Proceedings of SIGGRAPH Asia 2008) 27</i>, 5, Article 130.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Silverman, B. 1986. <i>Density Estimation for Statistics and Data Analysis</i>. Chapman and Hall, New York, NY.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 An Error Estimation Framework for Photon Density Estimation Toshiya Hachisuka* Wojciech Jarosz ,* Henrik 
Wann Jensen* *UC San Diego Disney Research Z¨urich Figure 1: Top: error bound estimation on a test scene. 
We show the reference rendering (left), as well as the actual error (red) and estimated error bound (green) 
at the three points (a,b,c) shown in the reference images. The speci.ed con.dence is 90%. Each iteration 
uses 15K photons. Bottom: rendering with speci.ed error thresholds. The rendering process is terminated 
once the error estimate reaches the speci.ed threshold. Our conservative error bound predicts the rate 
of convergence of the true average error automatically (log-log plot). Introduction Estimating error 
is an important task in rendering. For many predictive rendering applications such as simulation of car 
headlights, lighting design, or architectural design it is import to provide an estimate of the actual 
error to ensure con.dence and accuracy of the results. Even for applications where accuracy is not critical, 
error estimation is still useful for improving aspects of the rendering algorithm. Examples include terminating 
the rendering algorithm automatically, adaptive sampling where the parameters of the rendering algorithm 
are adjusted dynamically to minimize the error, and interpolating sparsely sampled radiance within a 
given error bound. We present a general error bound estimation framework for global illumination rendering 
using photon density estimation. Our method estimates bias due to density estimation of photons, and 
variance due to photon tracing. Our error bound estimation is robust for any light transport con.guration 
since it is based on progressive photon map­ping (PPM). We demonstrate that our estimated error bound 
captures error under complex light transport. Figure 1 shows that our error bound estimation works well 
under complex illumination including caustics. As a proof of concept of our error bound estimation, we 
demonstrate that it can be used to automatically terminate rendering without any subjective trial and 
error by a user. Our framework is the .rst general error estimation framework for photon based ren­dering 
methods that can handle complex global light transport with arbitrary light paths and materials. Existing 
work is either based on heuristics that do not capture error, or are limited to speci.c light paths or 
materials. We believe that our work is the .rst step towards answering the important question: How many 
photons are needed to render this scene? . Our Framework Unbiased Monte Carlo ray tracing algorithms 
are often preferred for predictive rendering since the error bound can be estimated based on variance. 
However, unbiased methods are not robust in the presence of specular re.ections or refractions of caustics 
from small light sources, which can be often seen in applications of lighting simulation (light bulbs, 
headlights etc). We therefore use progressive photon mapping (PPM) [Hachisuka et al. 2008], which is 
a biased Monte Carlo algorithm that is robust under these lighting conditions. Our error estimation framework 
estimates Copyright is held by the author / owner(s). SIGGRAPH 2010, Los Angeles, California, July 25 
 29, 2010. ISBN 978-1-4503-0210-4/10/0007 the following stochastic error bound: P (-Ei <Li - L - Bi 
< : Ei)=1-ß, where Ei = t(i, 1- ß 2 ) Vi i , t(i, x) is the x percentile of the t-distribution with degree 
i, Li is estimated radiance, Bi is estimated bias, Vi is estimated variance, L is the correct radiance, 
and 1 - ß is user-de.ned con.dence of this stochastic bound. Using this stochastic error bound, the user 
can simply specify their desired con.dence as 1 - ß, and our framework can tell how far the current estimated 
radiance is from the correct radiance without knowing L. We estimate the bias as Bi 12 Ri 2k2.L [Silverman 
1986], where k2 is a constant derived from the kernel and Ri is the radius for density estimation. We 
show how to apply this technique to the progressive density estimation used in progressive photon mapping. 
This equation uses the Laplacian of the unknown, correct radiance .L. We estimate this value using the 
Laplacian of the kernel, which has been used in standard density estimation techniques. Although the 
original PPM does not support smooth kernels that have a Lapla­cian, we derive the necessary conditions 
for incorporating these kernel functions within PPM. Using our kernel-based PPM, we can estimate any 
order of derivatives including the Laplacian of radiance in a consistent way, which has not been done 
in existing work and would be useful for analysis of illumination. Unfortunately, the standard procedure 
to estimate variance cannot be used in biased methods. We therefore propose estimating vari­ance using 
bias-corrected radiance Li - Bi, which also removes the dependency between samples in PPM. The same framework 
can be applied in a straight forward way for photon mapping or grid-based photon-density estimation by 
simply estimating the Laplacian. The advantage of using PPM is that the estimation of the Laplacian con­verges 
to the correct Laplacian in the limit, not just an approximation. That means that the entire framework 
is consistent except for the approximation of bias. References HACHISUKA, T., OGAKI, S., AND JENSEN, 
H. W. 2008. Progressive photon mapping. ACM Transactions on Graphics (Proceedings of SIGGRAPH Asia 2008) 
27, 5, Article 130. SILVERMAN, B. 1986. Density Estimation for Statistics and Data Analysis. Chapman 
and Hall, New York, NY.  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2010</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	</section>
	<section>
		<section_id>1837031</section_id>
		<sort_key>50</sort_key>
		<section_seq_no>2</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Detailed Surfaces]]></section_title>
		<section_page_from>4</section_page_from>
	<article_rec>
		<article_id>1837032</article_id>
		<sort_key>60</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>4</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[An accurate method for acquiring high resolution skin displacement maps]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1837026.1837032</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1837032</url>
		<abstract>
			<par><![CDATA[<p>The talk will focus on a fast, affordable, validated method for acquiring very high-resolution, high-quality displacement detail using alginate, plaster, silicone, a flat-bed scanner and digital image processing techniques. We reliably achieve facial and hand displacement maps at a resolution and accuracy that surpasses that of expensive capture/scanning systems.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.4.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.4.1</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.4.8</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010225.10010227</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision tasks->Scene understanding</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2264771</person_id>
				<author_profile_id><![CDATA[81466648331]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Gino]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Acevedo]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Weta Digital]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264772</person_id>
				<author_profile_id><![CDATA[81466647835]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Sergei]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nevshupov]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Weta Digital]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264773</person_id>
				<author_profile_id><![CDATA[81466647704]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Jess]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Cowely]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Weta Digital]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264774</person_id>
				<author_profile_id><![CDATA[81466645741]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Kevin]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Norris]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Weta Digital]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 An Accurate Method for Acquiring High Resolution Skin Displacement Maps Gino Acevedo Sergei Nevshupov 
Jess Cowely Kevin Norris Weta Digital Abstract The talk will focus on a fast, affordable, validated 
method for ac­quiring very high-resolution, high-quality displacement detail us­ing alginate, plaster, 
silicone, a .at-bed scanner and digital image processing techniques. We reliably achieve facial and hand 
dis­placement maps at a resolution and accuracy that surpasses that of expensive capture/scanning systems. 
1 Introduction The ultimate goal for a CG artist has always been to create a totally believable synthetic 
human. The human body has many complexi­ties -everything from shape, color, and the delicate .ow of pores 
in the skin -that have always made this a dif.cult challenge. Complete realism requires modeling or accurately 
capturing the highly de­tailed mesostructure for skin, especially for faces, for which human observers 
are sensitive to incredibly small details. We describe a technique developed during the production of 
"Avatar" that is very easy to perform, requires no expensive scanning equipment or cal­ibration and produces 
accurate, detailed displacement data that can be used directly to create "digital doubles" of the scanned 
actors or through projection painting to add realism to animated character s appearance. Acquiring accurate 
and precise displacement maps is essen­tial for rendering realistic skin as many elements of the shading 
model heavily depend on an accurate representation of the skin s mesostructure. For example it is very 
hard to properly represent the appearance of a face or a hand without modeling in very high detail the 
pore, crease and wrinkle detail, since most of the appearance of the skin is determined by high frequency 
detail. 2 The Technique Our process was inspired by our previous experience applying sil­icone for creating 
realistic synthetic prosthetics and puppets in the physical effects world. Silicone has a unique ability 
to capture the required mesostructure in a very thin, easily to manipulate layer, and also is translucent 
and .exible, enabling a simple, high­resolution capture technique using a .at bed scanner. The entire 
process involves three casting steps followed by a scanning step and an image processing step. An initial 
cast is .rst made with a material that sets quickly and can be safely removed from the face or hands. 
Plaster is used to create a second cast from .rst onto which the silicone can then be cured (a 12 hour 
process). Thin silicone layers can then be easily scanned, and a .nal data processing step generates 
the displacement data from the scanned images. Life casting is done using a material called alginate, 
which is made from seaweed and is mainly used by dentists and orthodon­tists to make impressions of teeth. 
A batch of alginate is mixed and then slowly rubbed onto the face, pushing it into every pore and wrinkle 
-but leaving the nostrils open so the person can breathe. Once the face is covered with an even coating, 
it is left to set into a soft, .exible rubber. To support the shape of the person s face, a "jacket" 
must be made by applying plaster bandages onto the surface of the alginate. Once the bandages have set 
into a hard shell, they can be removed and the alginate skin can be carefully peeled away from the face 
and put back into the jacket. This leaves a perfect negative impression of the person s face. Next, a 
very hard plaster is mixed up and poured into the negative. Once the plaster has set, it is removed from 
the negative casting leaving a Figure 1: From left to right: Alginate application, plaster bandages 
applied, negative cast, positive plaster cast. Figure 2: Sample displacement detail. solid duplicate 
of the persons face. Next we mix up the translucent silicone, which we rub onto the surface of the plaster 
cast and then leave to drain off. What remains is a very thin layer of silicone spread out evenly across 
the face. Once cured, it is then peeled off leaving a thin, negative impression of the face. The silicone 
skins are scanned after .rst being cut up so that each piece will lay as .at as possible on the scanner 
and will represent a mostly homogeneous pore structure (given the different character­istics of the various 
areas of the face or hand). This step basically corresponds to the texture unwrapping that would normally 
be per­formed digitally as part of the UV mapping. If the displacement maps are to be directly applied 
to a digital double, replicating the appearance of the scanned actor, care needs to be applied to prop­erly 
map the texture to the model, following the almost uniform stretch applied to the silicon layer. If instead 
the displacement map is to be used as the basis for projection painting, the scale and orientation will 
be decided by the texture artist. By performing a physical unwrap, we are able to minimize stretches, 
and make them homogeneous across the various sections. We split the head up into 3 sections, the forehead, 
nose, and sides of the face and neck all as one large piece maintaining as much of the face as a whole 
as possible. A small layer of baby oil on the scanner between the glass and the silicone acts to merge 
the two with minimal refraction. The method is capable and capturing details that are visibly resolved 
in a 1200 dpi resolution scan. An image processing step to derive displacement from the scanned images 
follows: the images are normalized to remove low frequency density variations due to the uneven distribution 
of the silicon, touched up to remove artifacts, bubbles, and are aligned and applied to characters either 
directly or through projection painting in Mari, Weta Digital s 3D painting software. Different techniques 
can be used to perform the normalization step, however, given the need for some touch up and preparation 
for painting, in most cases, manual correction is the easiest and best solution. The end result is a 
very highly detailed displacement map that shows the intricacy of the natural lines and pore .ow greatly 
helping the realism of digital humans and animated characters. Copyright is held by the author / owner(s). 
SIGGRAPH 2010, Los Angeles, California, July 25 29, 2010. ISBN 978-1-4503-0210-4/10/0007  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2010</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1837033</article_id>
		<sort_key>70</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>5</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Sketch-based 3D shape retrieval]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1837026.1837033</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1837033</url>
		<abstract>
			<par><![CDATA[<p>As large collections of 3D models are starting to become as common as public image collections, the need arises to <i>quickly</i> locate models in such collections. Models are often insufficiently annotated such that a keyword based search is not promising. Our approach for content based searching of 3D models relies entirely on visual analysis and is based on the observation that a large part of our perception of shapes stems from their salient features, usually captured by dominant lines in their display. Recent research on such feature lines has shown that 1) people mostly draw the same lines when asked to depict a certain model and 2) the shape of an object is well represented by the set of feature lines generated by recent NPR line drawing algorithms [Cole et al. 2009]. Consequently, we suggest an image based approach for 3D shape retrieval, exploiting the similarity of human sketches and the results of current line drawing algorithms. Our search engine takes a sketch of the desired model drawn by a user as the input and compares this sketch to a set of line drawings automatically generated for each of the models in the collection.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.5</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>H.3.1</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>H.3.3</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002951.10003317</concept_id>
				<concept_desc>CCS->Information systems->Information retrieval</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002951.10003317.10003318</concept_id>
				<concept_desc>CCS->Information systems->Information retrieval->Document representation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061.10010063</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures->Computational geometry</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2264775</person_id>
				<author_profile_id><![CDATA[81336488955]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Mathias]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Eitz]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[TU Berlin]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264776</person_id>
				<author_profile_id><![CDATA[81309509166]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Kristian]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hildebrand]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[TU Berlin]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264777</person_id>
				<author_profile_id><![CDATA[81309487978]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Tamy]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Boubekeur]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Telecom ParisTech & LTCI CNRS]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264778</person_id>
				<author_profile_id><![CDATA[81100235480]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Marc]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Alexa]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[TU Berlin]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Cole, F., Sanik, K., DeCarlo, D., Finkelstein, A., Funkhouser, T., Rusinkiewicz, S., and Singh, M. 2009. How well do line drawings depict shape?]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>588279</ref_obj_id>
				<ref_obj_pid>588272</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Funkhouser, T., Min, P., Kazhdan, M., Chen, J., Halderman, A., Dobkin, D., and Jacobs, D. 2003. A search engine for 3D models. <i>ACM Transactions on Graphics 22</i>, 1.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1276401</ref_obj_id>
				<ref_obj_pid>1276377</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Judd, T., Durand, F., and Adelson, E. H. 2007. Apparent ridges for line drawing. <i>ACM Transactions on Graphics 26</i>, 3.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Squire, D., Mueller, W., Mueller, H., and Raki, J. 1999. Content-based query of image databases. In <i>Scand. Conf. on Image Analysis</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Sketch-based 3D shape retrieval Mathias Eitz1, Kristian Hildebrand1, Tamy Boubekeur2 and Marc Alexa1 
1TU Berlin, 2 Telecom ParisTech &#38; LTCI CNRS Figure 1: 3D model search from simple user sketches: 
a) random view generation; b) retrieval results given a sketch of a car. 1 Introduction As large collections 
of 3D models are starting to become as com­mon as public image collections, the need arises to quickly 
locate models in such collections. Models are often insuf.ciently anno­tated such that a keyword based 
search is not promising. Our ap­proach for content based searching of 3D models relies entirely on visual 
analysis and is based on the observation that a large part of our perception of shapes stems from their 
salient features, usually captured by dominant lines in their display. Recent research on such feature 
lines has shown that 1) people mostly draw the same lines when asked to depict a certain model and 2) 
the shape of an object is well represented by the set of feature lines generated by recent NPR line drawing 
algorithms [Cole et al. 2009]. Consequently, we suggest an image based approach for 3D shape retrieval, 
exploit­ing the similarity of human sketches and the results of current line drawing algorithms. Our 
search engine takes a sketch of the desired model drawn by a user as the input and compares this sketch 
to a set of line drawings automatically generated for each of the models in the collection. 2 Feature 
Lines based Search We build our engine upon a bag-of-features search [Squire et al. 1999], which has 
become the method of choice for af.ne invariant image retrieval during the last years. This allows us 
to map the dif.cult 3D retrieval problem to a simpler image retrieval problem. In a bag-of-features approach, 
each image is represented by a large set of small local features that do not carry any spatial information. 
Those features are quantized to form a visual vocabulary in a training step and each image is then represented 
by its speci.c dis­tribution of visual words . To query a collection, the distribution of visual words 
of the query image is computed and the collection images with most similar distributions are returned. 
We accelerate this process by using inverted indices. As a result, only images that have at least one 
visual word in common with the query need to be compared and the query is extremely quick. Consequently, 
we need to de.ne two speci.c components for our scenario: 1) gener­ating a complete set of representative 
line drawings for each model and 2) new line-based local descriptors, which provide a compact translation-invariant 
representation of these lines. Note that image­based descriptors cannot be used since sketches are usually 
rough and texture-less. We generate the images by sampling a set of camera position uni­formly from the 
bounding sphere of the model, see Figure 1 a) (we used 50 views per model in this work). The camera position 
is used as input for view-dependent line drawing algorithms, such as Ap­parent Ridges [Judd et al. 2007]. 
This results in a set of curves in the plane, which are the input for computing the descriptors. Note 
that the combination of random views and descriptors based on lines naturally supports retrieval based 
on parts of the shape a task that is usually dif.cult in systems based on global shape descriptors. 
As descriptors for the projected lines we propose to use a histogram of oriented gradients extracted 
from small local regions. Note that the queries generated by users are binary sketches and thus only 
contain information about the direction of a line so information about the length of the gradient is 
missing. 3 Results and Discussion Since our proposed shape retrieval engine is based on a bag-of­features 
search it comes with several desirable properties compared to existing methods [Funkhouser et al. 2003]. 
Since we use small local features, users do not need to sketch the full shape, part-based retrieval is 
naturally supported. The retrieval is translation invari­ant we are currently working on integrating 
full invariance to af.ne transformations. Rotation invariance can be easily achieved by computing the 
local histograms with respect to a reference frame de.ne by the image content. A query is very fast and 
we believe that this approach can be readily scaled to much larger shape col­lections. We have built 
our initial system on the 2,000 models con­tained in the Princeton shape database; a query on this collection 
takes only about 10 ms. Finally, as most 3D models do not carry texture information, we can use our engine 
to search them from pic­tures as well, by using recent image contour extraction methods. References 
COLE, F., SANIK, K., DECARLO, D., FINKELSTEIN, A., FUNKHOUSER, T., RUSINKIEWICZ, S., AND SINGH, M. 2009. 
How well do line drawings depict shape? FUNKHOUSER, T., MIN, P., KAZHDAN, M., CHEN, J., HALDERMAN, A., 
DOBKIN, D., AND JACOBS, D. 2003. A search engine for 3D models. ACM Transactions on Graphics 22, 1. JUDD, 
T., DURAND, F., AND ADELSON, E. H. 2007. Apparent ridges for line drawing. ACM Transactions on Graphics 
26, 3. SQUIRE, D., MUELLER, W., MUELLER, H., AND RAKI, J. 1999. Content-based query of image databases. 
In Scand. Conf. on Image Analysis. Copyright is held by the author / owner(s). SIGGRAPH 2010, Los Angeles, 
California, July 25 29, 2010. ISBN 978-1-4503-0210-4/10/0007   
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2010</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1837034</article_id>
		<sort_key>80</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>6</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[meshmixer]]></title>
		<subtitle><![CDATA[an interface for rapid mesh composition]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1837026.1837034</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1837034</url>
		<abstract>
			<par><![CDATA[<p>In recent years a wide variety of mesh editing techniques have been developed for tasks such as smoothing, deformation, and parameterization. Comparatively fewer alternatives are available for composition tasks, such as combining parts of existing meshes. As designs often evolve from a combination of existing ideas and models, rapid composition techniques could significantly improve the workflow of mesh-based modeling tools. In our work we are exploring shape reuse and composition problems in 3D mesh modeling.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.4</cat_node>
				<descriptor>Graphics editors</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061.10010063</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures->Computational geometry</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2264779</person_id>
				<author_profile_id><![CDATA[81300305201]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Ryan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Schmidt]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Toronto]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264780</person_id>
				<author_profile_id><![CDATA[81100278455]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Karan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Singh]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Toronto]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>566583</ref_obj_id>
				<ref_obj_pid>566654</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Biermann, H., Martin, I., Bernardini, F., and Zorin, D. 2002. Cut-and-paste editing of multiresolution surfaces. <i>ACM Trans. Graph. 21</i>, 3, 312--321.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1320845</ref_obj_id>
				<ref_obj_pid>1320303</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Botsch, M., and Sorkine, O. 2008. On linear variational surface deformation methods. <i>IEEE Trans. Vis. Comp. Graph. 14</i>, 1, 213--230.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1141930</ref_obj_id>
				<ref_obj_pid>1141911</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Schmidt, R., Grimm, C., and Wyvill, B. 2006. Interactive decal compositing with discrete exponential maps. <i>ACM Trans. Graph. 25</i>, 3, 605--613.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1165430</ref_obj_id>
				<ref_obj_pid>1165407</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Sharf, A., Blumenkrants, M., Shamir, A., and Cohenor, D. 2006. SnapPaste: an interactive technique for easy mesh composition. <i>Vis. Comput. 22</i>, 9, 835--844.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 meshmixer: an interface for rapid mesh composition Ryan Schmidt Karan Singh University of Toronto University 
of Toronto The Problem In recent years a wide variety of mesh editing techniques have been developed 
for tasks such as smoothing, deformation, and parame­terization. Comparatively fewer alternatives are 
available for com­position tasks, such as combining parts of existing meshes. As de­signs often evolve 
from a combination of existing ideas and models, rapid composition techniques could signi.cantly improve 
the work­.ow of mesh-based modeling tools. In our work we are exploring shape reuse and composition problems 
in 3D mesh modeling. The Research Shape composition approaches generally fall into two categories ­part 
fusion and detail transfer. Part fusion methods allow arbitrary surface parts to be cut from one model 
and automatically stitched into an existing hole in another [Sharf et al. 2006], while detail transfer 
techniques copy displacement maps [Biermann et al. 2002] or differential information [Botsch and Sorkine 
2008] from source to target via compatible planar parameterizations. Although the technical aspects of 
shape composition have been well-explored, the modeling tools described to date focus on fully automatic 
solutions, which have the side effect of taking the artist out of the loop . Design exploration is also 
inef.cient, as when the result is unsatisfactory one must start from scratch. For exam­ple, most part 
fusion methods assume that a suitable hole already exists in the target surface. Such work.ows clearly 
lack the simplicity and ef.ciency of similar interactions found in 2D image editors, where an artist 
can simply drag selected pixels from one location to another. Similarly, detail transfer techniques have 
focused on cut-and-paste edits, copying entire enclosed regions. Examining 2D image editors, we note 
that a much more powerful and ef.cient interaction is the clone brush, an intuitive tool which allows 
the artist to selectively transfer details between the corresponding areas. Inspired by these 2D interactions, 
we have developed meshmixer,a composition tool for arbitrary surface meshes based on two novel interfaces. 
To perform fusion-style tasks for boundary-based fea­tures, we present Geometry Drag-and-Drop. This technique 
al­lows an artist to select a complex part and drag it along the sur­face to a new location, or onto 
another surface. We automatically .ll the hole left behind, smoothly deform the part to conform to the 
target surface, and provide additional blending and part-rigidity enhancements. For detail transfer we 
introduce the Mesh Clone Brush, which allows the artist to paint directly onto the surface to precisely 
specify the spatial extent and intensity of the transferred details. Both tools operate in real-time, 
providing instant feedback and opportunity for re.nement. To implement these tools, we adapt and extend 
recent techniques for dynamic mesh parameterization [Schmidt et al. 2006] and lin­ear variational deformation 
[Botsch and Sorkine 2008]. We have also explored a novel geometric differential deformation, which produces 
results similar to variational methods but is not depen­dent on mesh topology. Since we imagine that 
mesh composition will be most useful in the context of other mesh modeling tech­niques, meshmixer also 
incorporates state-of-the-art smoothing and deformation tools, again based on recent work in linear variational 
 Figure 1: We present a novel artist-oriented interface for surface composition. Arbitrary mesh parts 
can be dragged-and-dropped from either one location on a surface to another (a), or between two completely 
different surfaces (b,c). In either case, the hole left be­hind is automatically .lled. Our mesh clone 
brush supports precise control over the extent of detail transfer tasks by painting directly onto the 
mesh surface (d,e). modeling. We will discuss lessons learned from attempting to apply these methods 
in a practical system. The .uidity and ef.ciency of our tools supports an interaction style that has 
not been available in any previous system, allowing detailed 3D models to be quickly assembled from arbitrary 
input meshes. We are evaluating meshmixer by distributing it freely over the in­ternet, to date it has 
been tested by hundreds of artists, hobbyists, and 3D professionals. We distill the highly positive feedback 
gath­ered during this experience into a set of work.ows encompassing tasks common to different user groups, 
and explain how our tech­niques can be applied to simplify these work.ows. References BIERMANN, H., 
MARTIN, I., BERNARDINI, F., AND ZORIN, D. 2002. Cut-and-paste editing of multiresolution surfaces. ACM 
Trans. Graph. 21, 3, 312 321. BOTSCH, M., AND SORKINE, O. 2008. On linear variational sur­face deformation 
methods. IEEE Trans. Vis. Comp. Graph. 14, 1, 213 230. SCHMIDT, R., GRIMM, C., AND WYVILL, B. 2006. Interactive 
decal compositing with discrete exponential maps. ACM Trans. Graph. 25, 3, 605 613. SHARF, A., BLUMENKRANTS, 
M., SHAMIR, A., AND COHEN-OR, D. 2006. SnapPaste: an interactive technique for easy mesh composition. 
Vis. Comput. 22, 9, 835 844. Copyright is held by the author / owner(s). SIGGRAPH 2010, Los Angeles, 
California, July 25 29, 2010. ISBN 978-1-4503-0210-4/10/0007  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2010</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1837035</article_id>
		<sort_key>90</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>7</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[Cyclic twill-woven objects]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1837026.1837035</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1837035</url>
		<abstract>
			<par><![CDATA[<p>Any arbitrary twist of the edges of an extended graph rotation system induces a cyclic weaving on the corresponding surface [Akleman et al. 2009]. This recent theoretical result allows us to study generalized versions of textile weaving structures as cyclic weaving structures on arbitrary surfaces. In this work, we extend the study to twill weaving, which is used in fabrics such as denim or gabardine. Biaxial twill is a textile weave in which the weft (filling) threads pass over and under two consecutive warp threads and each row is obtained from the row above it by a shift of 1 unit to the right or to the left. The shift operation creates the characteristic diagonal pattern that makes the twill fabric visually appealing.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2264781</person_id>
				<author_profile_id><![CDATA[81100124987]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Ergun]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Akleman]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Texas A&M University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264782</person_id>
				<author_profile_id><![CDATA[81342491147]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Jianer]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Chen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Texas A&M University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264783</person_id>
				<author_profile_id><![CDATA[81448598429]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Yen-Lin]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Chen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Texas A&M University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264784</person_id>
				<author_profile_id><![CDATA[81100014535]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Qing]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Xing]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Texas A&M University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1531384</ref_obj_id>
				<ref_obj_pid>1531326</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Akleman, E., Chen, J., Xing, Q., and Gross, J. 2009. Cyclic plain-weaving with extended graph rotation systems. <i>ACM Transactions on Graphics; Proceedings of SIGGRAPH'2009</i>, 78.1--78.8.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Cyclic Twill-Woven Objects Ergun Akleman Jianer Chen Yen-Lin Chen Qing Xing Visualization Dept. Computer 
Science Dept. Computer Science Dept. Architecture Dept. Texas A&#38;M University Texas A&#38;M University 
Texas A&#38;M University Texas A&#38;M University  (a) Venus (b) Bunny (c) Cubical (d) Geodesic Dome 
Figure 1. Examples of cyclic twill weaving on polygonal mesh surfaces constructed with the voting algorithm. 
Any arbitrary twist of the edges of an extended graph rotation sys­tem induces a cyclic weaving on the 
corresponding surface [Akle­man et al. 2009]. This recent theoretical result allows us to study generalized 
versions of textile weaving structures as cyclic weaving structures on arbitrary surfaces. In this work, 
we extend the study to twill weaving, which is used in fabrics such as denim or gabardine. Biaxial twill 
is a textile weave in which the weft (.lling) threads pass over and under two consecutive warp threads 
and each row is obtained from the row above it by a shift of 1 unit to the right or to the left. The 
shift operation creates the characteristic diagonal pattern that makes the twill fabric visually appealing. 
In this work, we introduce the de.nition of a twill as a cyclic weav­ing structure on general surfaces. 
Based on this de.nition, we prove that three mesh conditions are necessary and suf.cient to obtain twill 
weaving from a given mesh. We show that many arbitrary meshes do not satisfy these three conditions. 
It is, therefore, not possible to obtain exact twill for many meshes. On the other hand, for mostly (4, 
4) meshes, i.e. meshes with large areas of quadri­laterals with 4-valent vertices, it is possible to 
obtain a reasonably good result of twill in most places. Based on this intuition, we have developed a 
voting algorithm that guarantees to satisfy most of the twill conditions that allow to demonstrate diagonal 
patterns every­where as shown in examples in Figure 1.(a), (b) and (c). The voting algorithm also creates 
exact twill if the mesh is twillable. The subdivided meshes are good candidates for twillable meshes 
since subdivisions can make the number of crossings in each cycle divisible by 4 and they can populate 
a mesh with regular regions. Triaxial twill is created from meshes that are populated with (3, 6) regions 
(i.e triangles with 6-valent vertices). Such meshes can be obtained by triangular subdivision schemes, 
such as midpoint sub­division. We prove that every mesh obtained by a midpoint subdi­vision is twillable. 
Patterns created by triaxial twill are visually interesting and remind some of the M. C. Escher s tilings 
as shown in Figure 1.(d). How­ever, triaxial twill does not demonstrate the characteristic diagonal pattern 
of biaxial twill, which can only be obtained from meshes populated by (4, 4) regions. Any mesh can be 
populated by (4, 4) regions by iteratively applying a quad-remeshing scheme such as Catmull-Clark or 
Doo-Sabin sub­divisions. We have investigated the meshes that can be converted to or can continue to 
be twillable after applications of quad-remeshing schemes. We call these meshes descendent twillable 
meshes. We have identi.ed a set of conditions to obtain descendent twill­able meshes with quad-remeshing 
schemes as shown in Figure 2. We prove that threads of such biaxial twill woven objects can al­ways be 
classi.ed as warp and weft and can be two-colored. We have also developed a coloring algorithm to paint 
threads of woven object with minimum number of colors. All images in this work are created using this 
minimum coloring algorithm. Figure 2: Three biaxial twill weaving objects obtained by the de­scendants 
of the same descendent-twillable mesh. References AKLEMAN, E., CHEN, J., XING, Q., AND GROSS, J. 2009. 
Cyclic plain-weaving with extended graph rotation systems. ACM Transactions on Graphics; Proceedings 
of SIGGRAPH 2009, 78.1 78.8. Copyright is held by the author / owner(s). SIGGRAPH 2010, Los Angeles, 
California, July 25 29, 2010. ISBN 978-1-4503-0210-4/10/0007  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2010</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	</section>
	<section>
		<section_id>1837036</section_id>
		<sort_key>100</sort_key>
		<section_seq_no>3</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Tissue & Medical Analysis]]></section_title>
		<section_page_from>8</section_page_from>
	<article_rec>
		<article_id>1837037</article_id>
		<sort_key>110</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>8</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Dynamic hard-soft tissue models for orofacial biomechanics]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1837026.1837037</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1837037</url>
		<abstract>
			<par><![CDATA[<p>Dynamic modeling of muscle-driven anatomical structures provides a means to better understand complex biomechanical systems. A rich application area for such models is the human orofacial anatomy, shown in Figure 1A, which is involved in a wide range of important functions including chewing, swallowing, breathing, and speaking. Comprehensive models of orofacial biomechanics will enhance our limited understanding of dysfunctions such as obstructive sleep apnea, swallowing disorders, and post-surgical deficits, such as reconstructive jaw surgery (Figure 1D).</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>J.3</cat_node>
				<descriptor>Biology and genetics</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.4.9</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.6.5</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010341.10010342</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Model development and analysis</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010225</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision tasks</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010444.10010087</concept_id>
				<concept_desc>CCS->Applied computing->Life and medical sciences->Computational biology</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010444.10010095</concept_id>
				<concept_desc>CCS->Applied computing->Life and medical sciences->Systems biology</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010444.10010935</concept_id>
				<concept_desc>CCS->Applied computing->Life and medical sciences->Genetics</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2264785</person_id>
				<author_profile_id><![CDATA[81319501712]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Ian]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Stavness]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of British Columbia]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264786</person_id>
				<author_profile_id><![CDATA[81339514644]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[John]]></first_name>
				<middle_name><![CDATA[E.]]></middle_name>
				<last_name><![CDATA[Lloyd]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of British Columbia]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264787</person_id>
				<author_profile_id><![CDATA[81327488395]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Sidney]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Fels]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of British Columbia]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264788</person_id>
				<author_profile_id><![CDATA[81100092022]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Yohan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Payan]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[TIMC-IMAG Lab, CNRS, France]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[
Hannam, A., Stavness, I., Lloyd, J., and Fels, S. 2008. "A dynamic model of jaw and hyoid biomechanics ... chewing". <i>J Biomech 41</i>, 5, 1069--1076.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[
Buchaillard, S., Perrier, P., and Payan, Y. 2009. "A biomechanical model of cardinal vowel production ... ". <i>J Acoust Soc Am 126</i>, 4, 2033--2051.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[
Bucki, M., Nazari, M., and Payan, Y. 2010. "Finite element speaker-specific face model for ... speech production". <i>Comput Methods Biomed, in press</i>.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Dynamic Hard-Soft Tissue Models for Orofacial Biomechanics Ian Stavness*Yohan Payan , John E. Lloyd, 
Sidney Fels University of British Columbia TIMC-IMAG Lab, CNRS, France Figure 1: CT image data (A) is 
used to adapt a reference model to a subject (B). Model components include rigid bony structures (jaw, 
maxilla, hyoid bone), a deformable tongue model, and point-to-point Hill-type muscles (C). Alterations 
that mimic surgical procedures (D) can be used to assess post-operative outcomes and de.cits. We are 
coupling the jaw-tongue-hyoid model to a deformable face model (E). 1 Introduction Dynamic modeling of 
muscle-driven anatomical structures pro­vides a means to better understand complex biomechanical sys­tems. 
A rich application area for such models is the human orofacial anatomy, shown in Figure 1A, which is 
involved in a wide range of important functions including chewing, swallowing, breathing, and speaking. 
Comprehensive models of orofacial biomechanics will enhance our limited understanding of dysfunctions 
such as obstruc­tive sleep apnea, swallowing disorders, and post-surgical de.cits, such as reconstructive 
jaw surgery (Figure 1D). Orofacial anatomy is particularly challenging to model as it is com­posed of 
an intricate arrangement of rigid structures including the skull, jaw, and hyoid bone as well as highly 
deformable muscle­activated tissues such as the tongue, lips, soft palate, pharynx, and larynx. Orofacial 
function involves the coordination of many mus­cles that can exert large forces, generate fast movements 
(up to 80 cm/s in speech), and create extensive contact between struc­tures. In this talk, we present 
work which uses recent medical imag­ing segmentation and registration techniques together with coupled 
hard/soft tissue simulation, to create a .rst-of-its-kind model of jaw-tongue-hyoid biomechanics. 2 
Approach Combining medical image data and reference models Our coupled jaw-tongue-hyoid model (Figure 
1B,C) is based on two previous isolated biomechanical models: a multi-body model of the jaw and hyoid 
[Hannam et al. 2008] and an FEM tongue model [Buchaillard et al. 2009]. Starting from these, we adapted 
their skeletal and muscle geometry, with a non-elastic mesh-based registration algorithm [Bucki et al. 
2010], to .t CT data1 for a spe­ci.c subject (Figure 1A,B). After registration, the models were cou­pled 
together by attaching FEM nodes of the tongue model to loca­tions on the jaw and hyoid rigid-bodies, 
allowing the tongue and its muscles to act on the surrounding bone structures, and vice versa. *e-mail: 
stavness@ece.ubc.ca 1CT data provided by P. Badin at Gipsa-Lab Dynamic simulation method Dynamic model 
simulation is done using ArtiSynth (www.artisynth.org), an open-source Java­based biomechanical modeling 
toolkit that provides graphical model editing and simulation control and fast computation times. ArtiSynth 
uses a semi-implicit integrator based on a mixed lin­ear complementarity formulation to solve the combined 
dynamics of particles, rigid bodies, linear or nonlinear FEMs, and bilateral and unilateral constraints 
including contact. Attachments between different model components are effected using point-based con­straints. 
These constraints can also be used to add marker points to rigid bodies or FEM elements for purposes 
such as attaching muscles or tracing motions. Muscle activation is achieved using point-to-point force 
effectors that can implement a variety of mus­cle types, including those based on the Hill model. Muscle 
activated tissue can be modeled by embedding these force effectors within an FEM model. Our tongue model 
uses hexahedral elements and a nearly-incompressible Mooney-Rivlin type material. Results and future 
directions We simulated a number of tasks, including unilateral chewing and tongue motions used in speech 
production, to test the model and evaluate the importance of jaw­tongue coupling. The results were consistent 
with literature data and also showed that coupling is indeed important; in particular, the presence of 
passive tongue tissue between the jaw and hyoid ap­pears to signi.cantly resist jaw movement. Simulation 
times were also much faster compared to commercial packages: 100 ms tasks could be computed within 10 
seconds, compared with the 40 min­utes required for the tongue model alone as reported in [Buchaillard 
et al. 2009]. We are currently coupling the jaw-tongue-hyoid model to a deformable face model (Figure 
1E) and have created prelimi­nary simulations of integrated orofacial biomechanics. Hannam, A., Stavness, 
I., Lloyd, J., and Fels, S. 2008. A dynamic model of jaw and hyoid biomechanics . . . chewing . J Biomech 
41, 5, 1069 1076. Buchaillard, S., Perrier, P., and Payan, Y. 2009. A biomechanical model of cardinal 
vowel production ... . J Acoust Soc Am 126, 4, 2033 2051. Bucki, M., Nazari, M., and Payan, Y. 2010. 
Finite element speaker-speci.c face model for . . . speech production . Comput Methods Biomed, in press. 
Copyright is held by the author / owner(s). SIGGRAPH 2010, Los Angeles, California, July 25 29, 2010. 
ISBN 978-1-4503-0210-4/10/0007  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2010</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1837038</article_id>
		<sort_key>120</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>9</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Distance visualization of ultrascale data with explorable images]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1837026.1837038</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1837038</url>
		<abstract>
			<par><![CDATA[<p>This talk presents a new approach to distance visualization of very large data sets output from scientific supercomputing. The processing power of massively parallel supercomputers increases at a rather fast rate, about an order of magnitude faster every three years, enabling scientists to model complex physical phenomena and chemical processes at unprecedented fidelity. Several petascale computers are already in operation (http://www.top500.org) and exascale computing is around the corner. Each run of a petascale simulation typically outputs several hundred terabytes of data to disk. Transferring data at this scale over wide-area networks to the scientist's laboratory for post-processing analysis is not an option. Even the data files may be transferred, existing desktop data analysis and visualization tools cannot effectively handle such large-scale data. If the scientists may use the same supercomputing facility for data analysis and visualization, there are three viable solutions:</p> <p>&#8226; in situ visualization, where visualization is computed during the simulation on the same supercomputer,</p> <p>&#8226; co-processing visualization, where visualization is computed during the simulation on a separate computer, and</p> <p>&#8226; post-processing visualization, where visualization is computed after simulation is over.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.2</cat_node>
				<descriptor>Distributed/network graphics</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>J.3</cat_node>
				<descriptor>Medical information systems</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>H.3.4</cat_node>
				<descriptor>Distributed systems</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>C.2.4</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.6.6</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10002951.10003152.10003517.10003519</concept_id>
				<concept_desc>CCS->Information systems->Information storage systems->Storage architectures->Distributed storage</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10011007.10010940.10010971.10011120</concept_id>
				<concept_desc>CCS->Software and its engineering->Software organization and properties->Software system structures->Distributed systems organizing principles</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010444.10010447</concept_id>
				<concept_desc>CCS->Applied computing->Life and medical sciences->Health care information systems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341.10010370</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Simulation evaluation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010520.10010521.10010537</concept_id>
				<concept_desc>CCS->Computer systems organization->Architectures->Distributed architectures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002951.10003317.10003365.10003368</concept_id>
				<concept_desc>CCS->Information systems->Information retrieval->Search engine architectures and scalability->Distributed retrieval</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002951.10003317.10003365.10003369</concept_id>
				<concept_desc>CCS->Information systems->Information retrieval->Search engine architectures and scalability->Peer-to-peer retrieval</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010520.10010521.10010542.10011714</concept_id>
				<concept_desc>CCS->Computer systems organization->Architectures->Other architectures->Special purpose systems</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2264789</person_id>
				<author_profile_id><![CDATA[81452595774]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Kwan-Liu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ma]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of California, Davis]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264790</person_id>
				<author_profile_id><![CDATA[81466647636]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Anna]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tikhonova]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of California, Davis]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264791</person_id>
				<author_profile_id><![CDATA[81100085579]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Carlos]]></first_name>
				<middle_name><![CDATA[D.]]></middle_name>
				<last_name><![CDATA[Correa]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of California, Davis]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[
Tikhonova, A., Correa, C. D., and Ma, K.-L. 2010. Explorable images for visualizing volume data. In <i>Proceedings of IEEE Pacific Visualization Symposium</i>, 177--184.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[
Tikhonova, A., Correa, C. D., and Ma, K.-L. 2010. An exploratory technique for coherent visualization of time-varying volume data. <i>Computer Graphics Forum 29</i>, 3.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Distance Visualization of Ultrascale Data with Explorable Images Kwan-Liu Ma Anna Tikhonova Carlos D. 
Correa University of California, Davis University of California, Davis University of California, Davis 
 Figure 1: Distance visualization of large-scale data anywhere anytime based on the concept of Explorable 
Images, which may be thought as a video that can be explored in multiple dimensions. This talk presents 
a new approach to distance visualization of very large data sets output from scienti.c supercomputing. 
The process­ing power of massively parallel supercomputers increases at a rather fast rate, about an 
order of magnitude faster every three years, en­abling scientists to model complex physical phenomena 
and chem­ical processes at unprecedented .delity. Several petascale comput­ers are already in operation 
(http://www.top500.org) and exascale computing is around the corner. Each run of a petascale simulation 
typically outputs several hundred terabytes of data to disk. Trans­ferring data at this scale over wide-area 
networks to the scientist s laboratory for post-processing analysis is not an option. Even the data .les 
may be transferred, existing desktop data analysis and vi­sualization tools cannot effectively handle 
such large-scale data. If the scientists may use the same supercomputing facility for data analysis and 
visualization, there are three viable solutions: in situ visualization, where visualization is computed 
during the simulation on the same supercomputer,  co-processing visualization, where visualization is 
computed during the simulation on a separate computer, and  post-processing visualization, where visualization 
is com­puted after simulation is over.  Here, the word visualization has a broader meaning, which may 
include any pre-processing, data reduction, feature extraction, and rendering. No matter which solution 
is adopted, scientists still need a way to see the resulting visualization on their desktop display. 
That is, distance visualization must be effectively supported. There are three ways to do distance visualization: 
move data, move extracts, or move images. Moving raw data is not feasible for ob­vious reasons. Moving 
extracts, a small fraction of the the data characterizing some features of interest in the data, signi.cantly 
re­duces the amount of data that must be transferred over networks, but rendering the extracts at interactive 
rates could demand a pow­erful computer and a sophisticated user interface. Moving images can support 
two different modes of visualization: image-based ren­dering, which could require a large number of images, 
and video viewing, which is browsing images in sequential order. We aim to enable scientists to visualize 
their data anywhere anytime on any display device, which may not have a powerful processor nor a large 
memory space, like an iPad, as depicted in Fig. 1. While videos can meet this need, videos only allow 
very limited explo­ration, i.e., playing forward and backward. We have been develop­ing a technology 
which we call Explorable Images to enhance ex­plorability. Our preliminary work [Tikhonova et al. 2010a] 
demon­strates both view exploration and the possibility to adjust color and opacity mapping. Our re.ned 
design [Tikhonova et al. 2010b] lets users also do relighting and visualize time-varying data. Overall, 
Explorable Images allows users to explore in the spatial domain of the data,  temporal domain of the 
data,  transfer function space, and  rendering parameter space  without re-rendering and access to 
the original data. The storage re­quirement of Explorable Images without compression is 4X a nor­mal 
image size, much smaller than the original data. The degree of explorability has a limit, but it already 
proves useful. As portable display devices and wireless Internet are commonplace, the concept of Explorable 
Images becomes very powerful, suggesting scientists to rethink how they do their routine data analysis 
and visualization. Acknowledgements This work was sponsored in part by the U.S. Department of En­ergy 
s SciDAC program and U.S. National Science Foundation s PetaApps program. References TIKHONOVA, A., 
CORREA, C. D., AND MA, K.-L. 2010. Ex­plorable images for visualizing volume data. In Proceedings of 
IEEE Paci.c Visualization Symposium, 177 184. TIKHONOVA, A., CORREA, C. D., AND MA, K.-L. 2010. An exploratory 
technique for coherent visualization of time-varying volume data. Computer Graphics Forum 29, 3. Copyright 
is held by the author / owner(s). SIGGRAPH 2010, Los Angeles, California, July 25 29, 2010. ISBN 978-1-4503-0210-4/10/0007 
 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2010</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1837039</article_id>
		<sort_key>130</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>10</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Exploration of bat wing morphology through a strip method and visualization]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1837026.1837039</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1837039</url>
		<abstract>
			<par><![CDATA[<p>We present a visual exploration tool that facilitates biologists navigating through complex bat wing geometry by combining a novel modeling method and an interactive visualization approach. Our work contributes to the following: a new method to quantify the dynamic kinematics during flight, a new curve fitting method that measures camber, and a new tool for time-varying data visualization for biological knowledge discovery.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[computational modeling]]></kw>
			<kw><![CDATA[flight dynamics]]></kw>
			<kw><![CDATA[simulation and visualization]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.5</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>J.3</cat_node>
				<descriptor>Biology and genetics</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010444.10010087</concept_id>
				<concept_desc>CCS->Applied computing->Life and medical sciences->Computational biology</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010444.10010095</concept_id>
				<concept_desc>CCS->Applied computing->Life and medical sciences->Systems biology</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010444.10010935</concept_id>
				<concept_desc>CCS->Applied computing->Life and medical sciences->Genetics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061.10010063</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures->Computational geometry</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2264792</person_id>
				<author_profile_id><![CDATA[81466644671]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Jian]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Chen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[USM]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264793</person_id>
				<author_profile_id><![CDATA[81466648271]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Daniel]]></first_name>
				<middle_name><![CDATA[K.]]></middle_name>
				<last_name><![CDATA[Riskin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Brown University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264794</person_id>
				<author_profile_id><![CDATA[81466647285]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Tatjana]]></first_name>
				<middle_name><![CDATA[Y.]]></middle_name>
				<last_name><![CDATA[Hubel]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Brown University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264795</person_id>
				<author_profile_id><![CDATA[81540655956]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[David]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Willis]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[UMass, Lowell]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264796</person_id>
				<author_profile_id><![CDATA[81466645544]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Arnold]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Song]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Brown University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264797</person_id>
				<author_profile_id><![CDATA[81466643637]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Hanyu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Liu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[USM]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264798</person_id>
				<author_profile_id><![CDATA[81100048505]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>7</seq_no>
				<first_name><![CDATA[Kenneth]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Breuer]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Brown University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264799</person_id>
				<author_profile_id><![CDATA[81421597209]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>8</seq_no>
				<first_name><![CDATA[Sharon]]></first_name>
				<middle_name><![CDATA[M.]]></middle_name>
				<last_name><![CDATA[Swartz]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Brown University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264800</person_id>
				<author_profile_id><![CDATA[81100589961]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>9</seq_no>
				<first_name><![CDATA[David]]></first_name>
				<middle_name><![CDATA[H.]]></middle_name>
				<last_name><![CDATA[Laidlaw]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Brown University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>208249</ref_obj_id>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Foley, J., van Dam, A., Feiner, S., and Hughes, J. 1995. <i>Computer graphics: principles and practice</i>. Addison-Wesley Professional.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Exploration of Bat Wing Morphology through A Strip Method and Visualization JianChen * DanielK.Riskin 
TatjanaY.Hubel DavidWillis ArnoldSong HanyuLiu USM BrownUniversity BrownUniversity UMass,Lowell BrownUniversity 
USM KennethBreuer SharonM.Swartz DavidH.Laidlaw BrownUniversity BrownUniversity BrownUniversity (a) 
(b) (c) Figure 1: (a) Frame from the video recorded in a wind tunnel; (b) visualization tool showing 
wing wake structure (in blue) and camber (in colors from a L*a*b color space); (c) wing mesh (in blue). 
Abstract We present a visual exploration tool that facilitates biologists nav­igating through complex 
bat wing geometry by combining a novel modeling method and an interactive visualization approach. Our 
work contributes to the following: a new method to quantify the dynamic kinematics during .ight, a new 
curve .tting method that measures camber, and a new tool for time-varying data visualiza­tionforbiologicalknowledgediscovery. 
Keywords: simulationand visualization, .ightdynamics,compu­tational modeling 1 Our Approach Batsareknown 
to.y withamazing maneuverabilityand agility, in partbecauseoftheiruniqueaeromechanicalfeaturessuchashighly 
elastic wing membranes and deforming wing bones. However, the details of how the wing membrane changes 
shape during .ight are poorlyunderstood.Ourmodeling researchquanti.eswing camber (whichmeasures the curvature 
of the wing) using an aerodynami­cally meaningful approach, called a strip method. Unlike an exist­ing 
approach tocalculating thecrosssectionparallel to thesagittal plane, our method takes the section parallel 
to the oncoming air­.ow, coinciding with aerodynamic theory. Ourvisualization tool(Figure1(b)) allowsbiologiststoquery 
and compare camber at any time instance. The camber is calculated in four steps. 3D wing motions are 
tracked and digitized at sev­enteen anatomical marker locations on the wing and body (Fig­ure 1(a)). 
Marker points are interpolated using an overconstrained least-square polynomial .t. A third-order polynomial 
is used for * email:jian.chen@usm.edu sharon swartz@brown.edu dhl@cs.brown.edu .lling gaps. In each frame, 
the wing mesh is reconstructed using eight trianglesand sixquadrilaterals(Figure1(c)).Thechoicewas made 
according to the observed movement: the joints that were likely to move together were placed on same 
patch. We then cal­culate the cross section that is parallel to the oncoming .ow and perpendicular tothewing 
surface.Ahalf-edgealgorithmwasused to computer the intersection [Foleyet al. 1995]. We use a third­order 
Fourier sine series to .t a smooth curve to the discrete line segments on the cross section. Finally, 
we calculate the camber by dividing the maximum distance between that curve and the chord by the length 
of the chord. Cambershapeis timevarying andinterpreting suchdatain3Dim­poses cognitive and perceptual 
load to biologists. To address this issue, we further provide a visualization tool to show camber vari­ations 
through coloring inaperceptuallyunformed coloring space. A color space is said to be perceptually uniform 
if the perceptual differencebetweenanytwocolors injustnoticeabledifferenceunits is equal to the Euclidean 
distance between the two colors in that color space. Resultsofwing camber ismappedto theL*a*b colorspaceatL=30 
(Figure 1(b)). The green to blue colors represent higher cambers; and the red and orange colors represent 
the lower cambers. The visualization shows that the larger camber variation occurs at the beginning and 
the end of the stroke. The wake structure is shown in a light blue color to provide a spatial reference. 
Preliminary re­sultssuggested that themodeling approachproduced moreprecise representationofthedynamicsof.ights 
thusenabledfast and more accurate interpretation of camber shape among many.ights. Acknowledgements 
This work was supported partly by the following awards NIH 1R01EB00415501A1,NSFCNS-0427374,NSFIOS-0702392, 
and NASA AISR NNX08AC63G. Jian Chen was supported in part by theBrownUniversityCenterforVisionResearchFellowship. 
 References FOLEY, J., VAN DAM, A., FEINER, S., AND HUGHES, J. 1995. Computer graphics: principles and 
practice. Addison-Wesley Professional. Copyright is held by the author / owner(s). SIGGRAPH 2010, Los 
Angeles, California, July 25 29, 2010. ISBN 978-1-4503-0210-4/10/0007  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<article_sponsors>
			<funding_agency>NASA</funding_agency>
			<grant_numbers>
				<grant_number>AISR NNX08AC63G</grant_number>
			</grant_numbers>
		</article_sponsors>
		<article_sponsors>
			<funding_agency>NSF</funding_agency>
			<grant_numbers>
				<grant_number>CNS-0427374IOS-0702392</grant_number>
			</grant_numbers>
		</article_sponsors>
		<article_sponsors>
			<funding_agency>NIH</funding_agency>
			<grant_numbers>
				<grant_number>1R01EB00415501A1</grant_number>
			</grant_numbers>
		</article_sponsors>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2010</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	</section>
	<section>
		<section_id>1837040</section_id>
		<sort_key>140</sort_key>
		<section_seq_no>4</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Volumes and Precipitation]]></section_title>
		<section_page_from>11</section_page_from>
	<article_rec>
		<article_id>1837041</article_id>
		<sort_key>150</sort_key>
		<display_label>Article No.</display_label>
		<display_no>11</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Digital Water for Avatar]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1837026.1837041</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1837041</url>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2264801</person_id>
				<author_profile_id><![CDATA[81314491744]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Christoph]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sprenger]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264802</person_id>
				<author_profile_id><![CDATA[81466648167]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Diego]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Trazzi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264803</person_id>
				<author_profile_id><![CDATA[81466648726]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Allen]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hemberger]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264804</person_id>
				<author_profile_id><![CDATA[81100602513]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Sebastian]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Marino]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2010</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1837042</article_id>
		<sort_key>160</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>12</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Prep and landing]]></title>
		<subtitle><![CDATA[Christmas in July: the effects snow process]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1837026.1837042</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1837042</url>
		<abstract>
			<par><![CDATA[<p><b><i>Prep and Landing</i></b> had some variant of snowfall in a large number of shots - ranging from gentle falling snow outside windows, to near blizzard-like conditions 1(b,c). Snowfall was required to support the characters by helping to make the world they inhabit believable.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.6.8</cat_node>
				<descriptor>Animation</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.6.5</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010341.10010342</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Model development and analysis</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061.10010063</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures->Computational geometry</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341.10010349.10011310</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Simulation types and techniques->Simulation by animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2264805</person_id>
				<author_profile_id><![CDATA[81320488843]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Ian]]></first_name>
				<middle_name><![CDATA[J.]]></middle_name>
				<last_name><![CDATA[Coony]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Walt Disney Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264806</person_id>
				<author_profile_id><![CDATA[81466647053]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[David]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hutchins]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Walt Disney Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264807</person_id>
				<author_profile_id><![CDATA[81466645957]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Kevin]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lee]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Walt Disney Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264808</person_id>
				<author_profile_id><![CDATA[81466645444]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Mike]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Harris]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Walt Disney Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264809</person_id>
				<author_profile_id><![CDATA[81466647466]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Tim]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Molinder]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Walt Disney Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264810</person_id>
				<author_profile_id><![CDATA[81466648380]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Kee]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Suong]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Walt Disney Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Prep and Landing Christmas in July: The Effects Snow Process  figure 1:(a) Snow Globe (b) Gentle Snow (c) Snow Interaction (d) Painted Ski Tracks (e) gpuFX Snow  
1. Introduction Prep and Landing had some variant of snowfall in a large number of shots -ranging from 
gentle falling snow outside windows, to near blizzard-like conditions 1(b,c). Snowfall was required to 
support the characters by helping to make the world they inhabit believable. 2. Managing Snow Shots 
The best way to manage the volume and varying type of snowfall was to completely automate the effects 
snow setups as shots were built. This turned out to be much more of a practical approach than attempting 
to simulate everything from scratch for each shot. Time was spent up front creating a stylized look that 
could be defined by a library of cached snow simulations. Categories for the various snow types were 
created and could be previewed as movies for artist reference. As shots came into the department, an 
artist could select the type of snowfall, then position the red, green and blue snow layers through the 
shot camera. Depth was based on this color coding and allowed the characters to be sandwiched between 
the layers in the composite without the need for complex character holdout mattes. The benefits of working 
in layers were a reduction of character matting issues and significantly faster renders. If the directors 
called out for a flurry here or a gust there , the modular nature of the snowfall libraries allowed for 
quick import and approval iteration. Sprite based snowflakes could be seen and adjusted interactively 
using OpenGL texture proxies and caching to speed playback. If there wasn t a pre-created snow type that 
worked well in shot, the original simulation rigs could be imported into a scene for resimulation and 
caching.  3. The Pipeline A python scripted pipeline was created that could build the snow shots from 
published libraries. Text based meta­data files could be edited to swap in blizzard conditions or import 
gently falling snow per shot. The snow pipeline had the ability to work autonomously across a broad number 
of shots each night with fully auto-rendered snow dailies viewable the next morning. GpuFX 1(e), our 
GPU based hardware accelerated renderer, was utilized across a dedicated farm of machines for rapid render 
iterations. This setup made it possible to render multiple takes of snowfall until we had exactly what 
was called for by our show Directors. Copyright is held by the author / owner(s). SIGGRAPH 2010, Los 
Angeles, California, July 25 29, 2010. ISBN 978-1-4503-0210-4/10/0007  4. Snow Interaction A technique 
for ground interaction was developed to allow complete control over the snow surface generated as characters 
slid, jumped, or fell face first into the fluffy white powder 1(c,d). Surface displacement was created 
using a particle simulation to push points onto a cropped higher resolution copy of the ground geometry. 
Holdout mattes were generated local to the displaced regions and allowed us to seamlessly blend the rendered 
displaced surfaces with non-displaced snow renders. Kicked up snow chunks were created using a separate 
particle simulation, which was instanced with chunk geometry at render time. These chunks could collide 
with the displaced snow geometry in scene, which is typically not easy to do when using height fields 
or z-depth maps to generate a ground surface. At any time during the process, it was possible to override 
the displacement sim and add in keyable target shapes to achieve a specific look if requested. In certain 
instances, it was more efficient to hand animate or paint in snow interaction 1(d) and reveal matte the 
painted element over time. 5. The Snow Globe Thousands of swirling snowflake particles were required 
to create the snow globe effect 1(a). Initial tests of particles in a fluid container proved this would 
be no easy task. This was due to the fact that the fluid volume containing them had to be animated and 
shaken through space -typically recipe for an unstable dynamic result. Completely breaking from physical 
reality proved key. The snow particles were animated at the world origin and timed to the character's 
hand motions. The resulting cached simulation was then parented to the globe translations and rendered 
with motion blur. 6. Conclusion Deadlines motivate creativity. This was certainly the case for the Prep 
and Landing effects department. In this 22-minute television Christmas special, nearly 300 shots contained 
some kind of effects animation all completed on schedule by a handful of effects artists... no quality 
compromised. From up front research and element reuse, to hand drawn snow interaction if it helped to 
create a plausible world for our characters in a reasonable amount of time, that was the best way to 
do it. all images copyrighted &#38;#169;Disney Enterprises  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2010</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1837043</article_id>
		<sort_key>170</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>13</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[I love it when a cloud comes together]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1837026.1837043</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1837043</url>
		<abstract>
			<par><![CDATA[<p>One of the action sequences in the film <i>The A-Team</i> takes place within a growing system of storm clouds. The story plot required the creation of a fully 3D environment of clouds covering tens of kilometers of evolving storm, modeled and simulated at high resolution because the camera and story elements are embedded within it. The cloud system covered approximately 20x10x5 kilometers, at a resolution as small at 0.1 meters. A variety of clouds types were modeled, corresponding to the different cloud taxonomies in different regions of a storm supercell. Individual cloud structure was controlled and directed down to arbitrarily fine spatial detail. This talk discusses the software tools developed to model, simulate, and render this complex, high resolution cloud system.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.6.8</cat_node>
				<descriptor>Animation</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.6.5</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010341.10010342</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Model development and analysis</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061.10010063</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures->Computational geometry</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341.10010349.10011310</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Simulation types and techniques->Simulation by animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2264811</person_id>
				<author_profile_id><![CDATA[81466643048]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Sho]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hasegawa]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Rhythm and Hues Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264812</person_id>
				<author_profile_id><![CDATA[81320490975]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Jason]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Iversen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Rhythm and Hues Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264813</person_id>
				<author_profile_id><![CDATA[81466646588]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Hideki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Okano]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Rhythm and Hues Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264814</person_id>
				<author_profile_id><![CDATA[81100445884]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Jerry]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tessendorf]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Rhythm and Hues Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 I Love It When A Cloud Comes Together Sho Hasegawa Jason Iversen Hideki Okano Jerry Tessendorf Rhythm 
and Hues Studios*  Figure 1: Top: 3D cloud environment for a scene from The A-Team. Bottom: Progression 
of cloud modeling techniques in Cumulo. From left to right: bunny cloud base shape; bunny cloud with 
one layer of pyroclastic displacement; bunny cloud with two layers of displacement; bunny cloud with 
three layers of displacement and .eld control of displacement amplitude; bunny cloud with three layers 
of displacement, .eld control and render-time advection. 1 Introduction One of the action sequences in 
the .lm The A-Team takes place within a growing system of storm clouds. The story plot required the creation 
of a fully 3D environment of clouds covering tens of kilometers of evolving storm, modeled and simulated 
at high reso­lution because the camera and story elements are embedded within it. The cloud system covered 
approximately 20x 10x 5 kilometers, at a resolution as small at 0.1 meters. A variety of clouds types 
were modeled, corresponding to the different cloud taxonomies in differ­ent regions of a storm supercell. 
Individual cloud structure was controlled and directed down to arbitrarily .ne spatial detail. This talk 
discusses the software tools developed to model, simulate, and render this complex, high resolution cloud 
system. 2 Cumulo One of the major cloud features needed was the puffy cauli.ower­like structure in cumulus 
clouds. The pyroclastic noise method1 exhibits the kind of features of interest, but for a cumulus cloud 
we needed to apply pyroclastic displacements onto arbitrary shapes, and to apply multiple layers of them. 
Also, cumulus cloud pyro­clastic structures clear the displacement noise out of the valleys be­tween 
major puffs. To accomplish these aspects, our Cumulo tool converts modeled base cloud shapes into levelsets 
and applies dis­placement noise, and generates a new levelset for pyroclastically displacement geometry. 
Cumulo iteratively applies multiple gen­erations of noise to create displacements on top of displacements, 
puffs on top of puffs. Clearing between puff structures happens by modulating each layer of displacement 
with scaled versions of the previous layers. * e-mail: {sho, jiversen, hideki, jerryt}@rhythm.com 1Alan 
Kaplar, Avalanche! snowy FX for XXX , Siggraph 2003 Sketch.  3 Advection Cumulo displacement of base 
shapes produces hard-edged cloud structures. A few steps of advection by vector noise was used to soften 
the edges and create small, ragged, separated pieces of cloud near the larger cloud. Advection was applied 
during cloud mod­eling, where it would be burned into the density grid after the Cumulo process. It was 
also applied at render time independent of gridding so that very .ne sub-grid detail was created. 4 
Field Controls No two clouds are alike. Individual clouds have large amounts of structual variation within 
them. To reproduce this, the parameters driving the Cumulo displacements and advection velocity had to 
be controllable from point to point on the cloud. Parameters were con­trolled by the artists as point 
attributes on the surface of the cloud base shape. These point attributes were transfered to a volumetric 
.eld control. Field controls were also built procedurally, sometimes driven by animations and simulations. 
This approach allows com­plete freedom to control the cloud structure with arbitrary precision, and facilitates 
modifying existing cloud structure locally without al­tering the rest of the cloud. 5 Volume Render 
Cumulus and storm clouds are participating media which have rel­atively little absorption (the single 
scatter albedo is between 0.95 and 0.999), and large amounts of multiple scattering (number of scattering 
lengths frequently in excess of 100). Ideally, rendering would employ a robust multiple scattering method 
which handles many orders of scattering. We employed several techniques in dif­ferent parts of the sequence, 
including multiple light sources, in­ternal glow lights, and new yet-to-be-published multiple scattering 
algorithms. Copyright is held by the author / owner(s). SIGGRAPH 2010, Los Angeles, California, July 
25 29, 2010. ISBN 978-1-4503-0210-4/10/0007  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2010</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1837044</article_id>
		<sort_key>180</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>14</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[Single scattering in heterogenous participating media]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1837026.1837044</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1837044</url>
		<abstract>
			<par><![CDATA[<p>The interaction between light and light-transmitting objects, known as <i>participating media</i>, involves complex physical phenomena such as light absorption and scattering. Media such as clouds, smoke and translucent materials often feature heterogeneous scattering properties. Hence the radiance transmitted by the medium potentially depends on such varying properties on the entire light paths, yielding soft light shafts and opacity variations (Figure 1).</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Color, shading, shadowing, and texture</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010384</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Texturing</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2264815</person_id>
				<author_profile_id><![CDATA[81466647298]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Cyril]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Delalandre]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Technicolor Research & Innovation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264816</person_id>
				<author_profile_id><![CDATA[81100339836]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Pascal]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gautron]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Technicolor Research & Innovation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264817</person_id>
				<author_profile_id><![CDATA[81100029219]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Jean-Eudes]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Marvie]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Technicolor Research & Innovation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264818</person_id>
				<author_profile_id><![CDATA[81319491424]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Guillaume]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Fran&#231;ois]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The Moving Picture Company]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[
Cerezo, E., Perez, F., Pueyo, X., Seron, F., and Sillion, F. 2005. A survey on participating media rendering techniques. <i>The Visual Computer 21</i>, 5, 303--328.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1598039</ref_obj_id>
				<ref_obj_pid>1597990</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[
Gautron, P., Marvie, J.-E., and Fran&#231;ois, G. 2009. Volumetric shadow mapping. In <i>SIGGRAPH 2009 talks</i>.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1360635</ref_obj_id>
				<ref_obj_pid>1399504</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[
Zhou, K., Ren, Z., Lin, S., Bao, H., Guo, B., and Shum, H.-Y. 2008. Real-time smoke rendering using compensated ray marching. In <i>Proceedings of SIGGRAPH 2008</i>, 1--12.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Single Scattering in Heterogenous Participating Media Cyril Delalandre Pascal Gautron Jean-Eudes Marvie 
Guillaume Franc¸ois {cyril.delalandre, pascal.gautron, jean-eudes.marvie}@technicolor.com guillaume-f@moving-picture.com 
Technicolor Research &#38; Innovation The Moving Picture Company (a) Liberty Cloud -10 fps (b) Siggraph 
Volume-17 fps (c) Smoke -20 fps (d) Smoke in a Box-17 fps Figure 1: Our algorithm introduces attenuation 
function maps for the computation of light scattering within heterogeneous media -500 steps The interaction 
between light and light-transmitting objects, known as participating media, involves complex physical 
phenomena such as light absorption and scattering. Media such as clouds, smoke and translucent materials 
often feature heterogeneous scattering proper­ties. Hence the radiance transmitted by the medium potentially 
de­pends on such varying properties on the entire light paths, yielding soft light shafts and opacity 
variations (Figure 1). Computing light scattering in these media usually requires com­plex of.ine computations 
[Cerezo et al. 2005]. Real-time applica­tions are either based on heavy precomputations [Zhou et al. 
2008], limited to homogeneous media [Gautron et al. 2009] or relying on simplistic rendering techniques 
such as billboards. We propose a generic method for fast estimation of single scattering within het­erogeneous 
participating media. Introducing the concept of atten­uation function maps, our method leverages graphics 
hardware to support dynamic light sources, viewpoints and participating media interactively. Our method 
also accounts for the shadows cast from solid objects, providing a full-featured solution for fast rendering 
of participating media which potentially embrace the entire world. The Attenuation Function Map The classical 
method for estimating single scattering in hetero­geneous media relies on a dual ray marching. When entering 
the medium, the viewing ray is sampled into n sample points Pi, i .{1..n}. For each sample point, the 
attenuation of the light in­coming at Pi is computed by ray marching along the ray from Pi to the light 
source. However, in most cases, this computation­intensive method cannot be used for real-time or even 
interactive high quality rendering. Our method cuts the cost of reduced inten­sity calculations by generating 
an attenuation function map which is then queried to estimate single scattering in the medium. The .rst 
step of our method is inspired from shadow mapping, a virtual camera being placed at the location of 
the light source. For each pixel of the image captured by this camera, we .rst determine the entry and 
exit points Kin and Kout of the ray in the bounding box of the medium (Figure 2). As we aim at representing 
the reduced intensity at any point Ki along KinKout , we sample the light attenu­ation on KinKout at 
a number of points Ki on the light ray. To avoid the costs of explicit storage of the attenuation function 
values, we project this function using the well-suited Discrete Cosine Trans­form. The projection coef.cients 
are computed on graphics hard­ware, and are then stored in the channels of multiple .oating-point textures. 
Compared to the explicit storage of the attenuation val­ues, this method both reduces the storage and 
provides continuous, smooth attenuation estimates. As we also focus on the in.uence of opaque objects 
on the lighting within the medium, we generate a classical shadow map to account for such shadowing. 
 Figure 2: The attenuation function map is used to directly obtain the reduced intensity value at each 
point Pi. In the second step, we render the scene from the actual viewpoint. We .rst compute the amount 
of light re.ected from opaque ob­jects. The incoming lighting at each surface point is obtained by .rst 
querying the classical shadow map and evaluating the attenu­ation function. A second pass performs the 
single scattering com­putations: For each pixel of the image, we compute the entry and exit points Pin 
and Pout , and then perform a classical ray march­ing. However, at each sample point Pi, the reduced 
light intensity is directly fetched from the shadow and attenuation function maps. Results Our algorithm 
offers interactive performance on a 3.6GHz Xeon with an nVidia GeForce GTX280 GPU. The presented images 
were rendered at a resolution of 1280 × 720 using a 1024 × 1024 attenu­ation function map with 16 coef.cients 
per pixel. Note that the sole tuning of the number of coef.cients and marching steps is suf.cient to 
generate images ranging from real-time to production-quality, providing a uni.ed and generic solution 
for fast high quality ren­dering of heterogeneous participating media. Future work will par­ticularly 
consider multiple scattering and image-based lighting. References CEREZO, E., PEREZ, F., PUEYO, X., 
SERON, F., AND SILLION, F. 2005. A survey on participating media rendering techniques. The Visual Computer 
21, 5, 303 328. GAUTRON, P., MARVIE, J.-E., AND FRANC¸OIS, G. 2009. Volu­metric shadow mapping. In SIGGRAPH 
2009 talks. ZHOU, K., REN, Z., LIN, S., BAO, H., GUO, B., AND SHUM, H.-Y. 2008. Real-time smoke rendering 
using compensated ray marching. In Proceedings of SIGGRAPH 2008, 1 12. Copyright is held by the author 
/ owner(s). SIGGRAPH 2010, Los Angeles, California, July 25 29, 2010. ISBN 978-1-4503-0210-4/10/0007 
  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2010</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	</section>
	<section>
		<section_id>1837045</section_id>
		<sort_key>190</sort_key>
		<section_seq_no>5</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Split Second Screen Space]]></section_title>
		<section_page_from>15</section_page_from>
	<article_rec>
		<article_id>1837046</article_id>
		<sort_key>200</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>15</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Screen space classification for efficient deferred shading]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1837026.1837046</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1837046</url>
		<abstract>
			<par><![CDATA[<p>Deferred shading is an increasingly popular technique for video game rendering. In the standard implementation a geometry pass writes depths, normals and other properties to a geometry buffer (G-buffer) before a lighting pass is applied as a screen space operation. Deferred shading is often combined with deferred shadowing where the occlusion values due to one or more lights are gathered in a screen space shadow buffer. The universal application of complex shaders to the entire screen during the shadow and light passes of these techniques can contribute to poor performance. A more optimal approach would take different shading paths for different parts of the scene. For example we would prefer to only apply expensive shadow filtering to known shadow edges. However this typically involves the use of dynamic branches within shaders which can lead to poor performance on current video game shading hardware.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Color, shading, shadowing, and texture</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>K.8.0</cat_node>
				<descriptor>Games</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10011007.10010940.10010941.10010969.10010970</concept_id>
				<concept_desc>CCS->Software and its engineering->Software organization and properties->Contextual software domains->Virtual worlds software->Interactive games</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010476.10011187.10011190</concept_id>
				<concept_desc>CCS->Applied computing->Computers in other domains->Personal computers and PC applications->Computer games</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002951.10003227.10003251.10003258</concept_id>
				<concept_desc>CCS->Information systems->Information systems applications->Multimedia information systems->Massively multiplayer online games</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010384</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Texturing</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2264819</person_id>
				<author_profile_id><![CDATA[81466644956]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Neil]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hutchinson]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Disney Interactive Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264820</person_id>
				<author_profile_id><![CDATA[81466646836]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Balor]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Knight]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Disney Interactive Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264821</person_id>
				<author_profile_id><![CDATA[81543285256]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Matthew]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ritchie]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Disney Interactive Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264822</person_id>
				<author_profile_id><![CDATA[81466647415]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[George]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Parrish]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Disney Interactive Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264823</person_id>
				<author_profile_id><![CDATA[81466645396]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Jeremy]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Moore]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Disney Interactive Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[
Swoboda, M. 2009. Deferred Lighting and Post Processing on PlayStation&#174;3. <i>Game Developers Conference 2009</i>.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[
Moore, J., Jefferies, D. 2009. Rendering Techniques in Split/Second. <i>Advanced Real-Time Rendering in 3D Graphics and Games -- SIGGRAPH 2009</i>.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Screen Space Cla assification for Efficient Deferred Shadingr Neil Hutchinson, Ba alor Knight, Matthew 
Ritchie, George Parrish, Jerremy Moore Disney Interactive Studios     (a)             (b) 
             (c)               (d) Figure 1: (a) Final rendered image, (b) 
Soft shadoow classification, (c) Sky classification, (d) MSAA edge classsification zoomed to show tile 
structure.  Introduction Deferred shading is an increasingly popular technique for video game rendering. 
In the standard implementat tion a geometry pass writes depths, normals and other properties too a geometry 
buffer (G-buffer) before a lighting pass is applied d as a screen space operation. Deferred shading is 
often comb bined with deferred shadowing where the occlusion values due to o one or more lights are gathered 
in a screen space shadow bu uffer. The universal application of complex shaders to the entiree screen 
during the shadow and light passes of these techniques ca an contribute to poor performance. A more optimal 
approach w would take different shading paths for different parts of the scen ne. For example we would 
prefer to only apply expensive shadow w filtering to known shadow edges. However this typically in nvolves 
the use of dynamic branches within shaders which can lead to poor performance on current video game shading 
haardware. [Swoboda 2009] proposes using the Synergis stic Processing Unit (SPU) of Sony s PlayStation®3 
to categorize e areas of the screen with the aim of increasing rendering efficienc cy in post processing 
effects such as depth of field. [Moore, Jeffe eries 2009] propose using screen space buffers to classify 
the s screen as a way to optimize MSAA deferred shading and deferredd shadow rendering. We extend these 
works to provide a general fr ramework for screen classification. We introduce a method for deco omposing 
the screen into tiles and define a number of useful tile cllassification criteria. We show how this approach 
can be used to redduce the complexity of the lighting pass in deferred shading and t to optimize a further 
range of rendering and post processing effects.. We describe how our approach has bee en implemented 
in Split/Second, a major racing game developedd at Disney s Black Rock Studio We provide details of the 
imp plementation for the Microsoft Xbox® 360 and Sony PlaySta ation® 3 hardware platforms.  Our Approach 
 In our approach we divide the screen into tile es. For each tile we aim to apply lighting shaders that 
contain th he minimum locally required complexity. To achieve this we first t apply a full screen classification 
pass which determines which components of our lighting calculation are required in each screen tile. 
This classification pass can be carried out using ei ither the GPU or the CPU or a combination of the 
two. We use a palette of screen attributes from which the clas ssification pass selects a combination 
for each tile. In our example c case the attributes include but are not limited to: sky , MSAA ed dge 
, sun-facing , and soft shadow edge . The presence of th hese attributes in any tile can be determined 
by analyzing th he G-buffer and the screen space deferred shadow mask. For each tile the collected scr 
reen attributes are combined to create a shader ID. This determinees the shader that will be used to 
process the tile during the lighhting pass. The maximum number of shaders used in the lighting p pass 
is 2n where n is the total number of screen attributes. For large e n combinatorial complexity can be 
avoided by grouping our scre een attributes and defining more than one pass per tile. In our exam mple 
we use 8 screen attributes so that the shader ID fits convenientlly into an 8-bit render target channel. 
The shaders themselves can be generated by pre-processing an uber-shader during shader commpilation. 
 Once the shader ID for each tile has been calculated we generate an index buffer with which t to submit 
a single draw call for each shader found in our scene. In our implementation this index buffer generation 
phase is carried ou ut on the CPU and synchronized with the GPU submission and renddering. The optimum 
tile size to trad de off average tile complexity against vertex cost can vary accordin ng to hardware 
and scene complexity. For Split/Second we found t that a tile size of 4x4 pixels gave a good balance. 
Vertex proce essing cost is further reduced by a simple aggregation of adjacennt tiles with the same 
shader ID. We have adapted the techniq que for our different target hardware platforms. On the PlayStationn® 
3 we move work away from the GPU by carrying out part of f the classification pass on the SPU. The SPU 
is also used to gen nerate the final index buffers. On the Xbox® 360 we make use o of the Procedural 
Synthesis feature (XPS) to optimally synchroni ize the CPU submission of draw calls with the GPU.  References 
 SWOBODA, M. 2009. Deferred Lighting and Post Processing on PlayySStation®3. Game Developers Conference 
2009. MOORE, J., JEFFERIES, D. 2009. Rendering Techniques in Split/Second. Advance ed Real-Time Rendering 
in 3D Graphics and Games SIGGRAPH 2009. Copyright is held by the author / owner(s). SIGGRAPH 2010, Los 
Angeles, California, July 25 29, 2010. ISBN 978-1-4503-0210-4/10/0007  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2010</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1837047</article_id>
		<sort_key>210</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>16</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Real-time frame rate up-conversion for video games]]></title>
		<subtitle><![CDATA[or how to get from 30 to 60 fps for "free"]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1837026.1837047</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1837047</url>
		<abstract>
			<par><![CDATA[<p>Currently in the video game industry, the holy grail of rendering is achieving ultra realistic visual quality while maintaining a consistent 60 frames per second. However, the cost of doing so means having half the time to construct an image when compared to simply running at 30 fps. Most 30 fps games roughly split up their rendering time in half, standard scene and alpha rendering, and all the other glamorous post processing effects and shadows. This inevitably makes it extremely difficult for game studios to achieve high quality visual images with post processing and dynamic shadows while maintaining a consistent 60 frames per second.</p> <p>Video games that run at 30 fps suffer from very noticeable motion "flickering" artifacts due to fast movements of objects and/or the camera. One common technique to help remedy this flickering artifact is to introduce motion blur. In one way this technique helps smooth out the perceived motion of objects, but is only able to compensate properly relative to the virtual camera. In other words, the game does not know where the user's eyes are focused to and tracking, so in practice in interactive applications, the images tend to be more blurry all the time even in areas where the user's eye is tracking an object's motion and expecting it to be sharp. Running an application at 60 fps on a 60 hertz display eliminates multiple expositions of a single frame to the eye, reducing "flickering" artifacts and increasing sharpness of the moving objects under the eye tracking conditions. Motion blur only needs to be added to compensate for motions in the image that move faster then the eye can track. Figure 1 shows the difference in motion from a 30 fps rendering image, 30 fps with motion blur, and the 60 fps respectively on a 60 hertz display.</p> <p>Film typically has an easier work around to this problem because of the nature of the entertainment medium not being user interactive. The presentation of shots, usage of depth of field, and limiting the use of fast motions around the focal point are common techniques that are used because the user's eye can be guided to focus on certain parts of the image. Each shot is tuned specifically assuming the user's eye is focusing on certain material, thus avoiding the problem completely.</p> <p>The proposed solution is a novel technique that combines the best of both high quality rendering at 30 fps with the natural motion of objects refreshing at 60 fps, introducing very minimal overhead in terms of memory and performance. The basic concept is to approximate the middle frame between what has previously rendered and what is currently being constructed and present it as the new "predicted" image exactly in the middle of rendering at a 30 fps rate, thus empowering a product to still "feel" as if it is refreshing at 60. Televisions use a similar trick to achieve refresh rates near 120 hertz. However, for video games, more information is present regarding the frames construction, such as depth and velocity, and creating the predicted frame can be significantly simplified. This proposed technique is useful for all real-time user interactive applications to help guarantee that a very high quality of rendering is achieved, by allowing more time to construct a frame, but still refresh at a higher rate such as 60 frames per second.</p> <p>Additionally, further techniques are considered such as rendering more static / slow moving parts of the scene at an even lower refresh rate, such as 15 fps, because some tests have shown that the predicted images are "good enough" approximations of the slower moving data from the user's perspective. This would thereby increase the time an application would have to construct a frame, and still maintain 60 frames per second. Also, always maintaining 60 fps by automatically presenting more predicted frames during the instances that the game slows down. This would additionally balance visual quality of fast moving objects with refresh rate consistency.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[display synchronization]]></kw>
			<kw><![CDATA[frame rate increase]]></kw>
			<kw><![CDATA[image interpolation]]></kw>
			<kw><![CDATA[interactive]]></kw>
			<kw><![CDATA[up-conversion]]></kw>
			<kw><![CDATA[velocity rendering]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.3</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2264824</person_id>
				<author_profile_id><![CDATA[81466642793]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Dmitry]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Andreev]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[LucasArts]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Real-time Frame Rate Up-conversion for Video Games or how to get from 30 to 60 fps for free Dmitry Andreev 
LucasArts  (a) (b) (c) Figure 1 Observations of eye tracking object movement on a 60 Hz display. (a) 
30 fps rendering, (b) 30 fps rendering with motion blur, (c) our method.  Abstract Currently in the 
video game industry, the holy grail of rendering is achieving ultra realistic visual quality while maintaining 
a consistent 60 frames per second. However, the cost of doing so means having half the time to construct 
an image when compared to simply running at 30 fps. Most 30 fps games roughly split up their rendering 
time in half, standard scene and alpha rendering, and all the other glamorous post processing effects 
and shadows. This inevitably makes it extremely difficult for game studios to achieve high quality visual 
images with post processing and dynamic shadows while maintaining a consistent 60 frames per second. 
Video games that run at 30 fps suffer from very noticeable motion flickering artifacts due to fast movements 
of objects and/or the cam­era. One common technique to help remedy this flickering artifact is to introduce 
motion blur. In one way this technique helps smooth out the perceived motion of objects, but is only 
able to compensate properly relative to the virtual camera. In other words, the game does not know where 
the user s eyes are focused to and tracking, so in practice in interactive applications, the images tend 
to be more blurry all the time even in areas where the user s eye is tracking an object s motion and 
expecting it to be sharp. Running an application at 60 fps on a 60 hertz display eliminates multiple 
expositions of a single frame to the eye, reducing flickering artifacts and increasing sharpness of the 
moving objects under the eye tracking conditions. Motion blur only needs to be added to compensate for 
motions in the image that move faster then the eye can track. Figure 1 shows the difference in motion 
from a 30 fps rendering image, 30 fps with motion blur, and the 60 fps respectively on a 60 hertz display. 
Film typically has an easier work around to this problem because of the nature of the entertainment medium 
not being user interactive. The presentation of shots, usage of depth of field, and limiting the use 
of fast motions around the focal point are common techniques that are used because the user s eye can 
be guided to focus on certain parts of the image. Each shot is tuned specifically assuming the user s 
eye is focusing on certain material, thus avoiding the problem completely. The proposed solution is a 
novel technique that combines the best of both high quality rendering at 30 fps with the natural motion 
of ob­jects refreshing at 60 fps, introducing very minimal overhead in terms of memory and performance. 
The basic concept is to approximate the middle frame between what has previously rendered and what is 
currently being constructed and present it as the new predicted im­age exactly in the middle of rendering 
at a 30 fps rate, thus empowering a product to still feel as if it is refreshing at 60. Televisions use 
a similar trick to achieve refresh rates near 120 hertz. However, for video games, more information is 
present regarding the frames construc­tion, such as depth and velocity, and creating the predicted frame 
can be significantly simplified. This proposed technique is useful for all real-time user interactive 
applications to help guarantee that a very high quality of rendering is achieved, by allowing more time 
to con­struct a frame, but still refresh at a higher rate such as 60 frames per second. Additionally, 
further techniques are considered such as rendering more static / slow moving parts of the scene at an 
even lower refresh rate, such as 15 fps, because some tests have shown that the predicted images are 
good enough approximations of the slower moving data from the user s perspective. This would thereby 
increase the time an application would have to construct a frame, and still maintain 60 frames per second. 
Also, always maintaining 60 fps by automatically presenting more predicted frames during the instances 
that the game slows down. This would additionally balance visual quality of fast moving objects with 
refresh rate consistency. CR Categories and Subject Descriptors: I.3.3 [Computer Graphics]: Picture/Image 
Generation. Keywords: Frame rate increase, up-conversion, image interpolation, velocity rendering, display 
synchronization, interactive Copyright is held by the author / owner(s). SIGGRAPH 2010, Los Angeles, 
California, July 25 29, 2010. ISBN 978-1-4503-0210-4/10/0007  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2010</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1837048</article_id>
		<sort_key>220</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>17</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Split second motion blur]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1837026.1837048</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1837048</url>
		<abstract>
			<par><![CDATA[<p>Motion blur is key to delivering a sense of speed in interactive video game rendering. Further, simulating accurate camera optical exposure properties and reduction of temporal aliasing brings us closer to high quality real-time rendering productions. We describe a motion blur system that integrates image and texture space motion blur for smooth results with less than one sample per pixel. We apply the algorithm in the context of a deferred shading rendering engine used in Split/Second (Disney: Black Rock), but the method also applies to forward rendering, ray tracing or REYES style architectures.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>K.8.0</cat_node>
				<descriptor>Games</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.8</cat_node>
				<descriptor>Motion</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010476.10011187.10011190</concept_id>
				<concept_desc>CCS->Applied computing->Computers in other domains->Personal computers and PC applications->Computer games</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352.10010238</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation->Motion capture</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10011007.10010940.10010941.10010969.10010970</concept_id>
				<concept_desc>CCS->Software and its engineering->Software organization and properties->Contextual software domains->Virtual worlds software->Interactive games</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010238</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->Motion capture</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002951.10003227.10003251.10003258</concept_id>
				<concept_desc>CCS->Information systems->Information systems applications->Multimedia information systems->Massively multiplayer online games</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352.10010380</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation->Motion processing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2264825</person_id>
				<author_profile_id><![CDATA[81543285256]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Matt]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ritchie]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Disney Interactive Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264826</person_id>
				<author_profile_id><![CDATA[81466648428]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Greg]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Modern]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Disney Interactive Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264827</person_id>
				<author_profile_id><![CDATA[81490663992]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Kenny]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Mitchell]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Disney Interactive Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1531399</ref_obj_id>
				<ref_obj_pid>1531326</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[
Egan, K. Tseng, Y.-T., Holzschuch, N., Durand, F. and Ramamoorthi, R. 2009. Frequency Analysis and Sheared Reconstruction for Rendering Motion Blur. In ACM Transactions on Graphics 28, 3.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[
Rosado, G. 2007. Motion Blur as a Post-Processing Effect. In GPU Gems 3, H. Nguyen, Ed. 575--582.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2383669</ref_obj_id>
				<ref_obj_pid>2383654</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[
Loviscach, J. 2005. Motion Blur for Textures by Means of Anisotropic Filtering. EG Symposium on Rendering, 105--110.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Split Second Motion Blur Matt Ritchie Greg Modern Kenny Mitchell Disney Interactive Studios   Figure 
1: Motion blur system in Split/Second from left to right; image space adaptive at high speed, augmented 
with texture space blur. Introduction Motion blur is key to delivering a sense of speed in interactive 
video game rendering. Further, simulating accurate camera optical exposure properties and reduction of 
temporal aliasing brings us closer to high quality real-time rendering productions. We describe a motion 
blur system that integrates image and texture space motion blur for smooth results with less than one 
sample per pixel. We apply the algorithm in the context of a deferred shading rendering engine used in 
Split/Second (Disney: Black Rock), but the method also applies to forward rendering, ray tracing or REYES 
style architectures. Method Image Space Motion Blur Motion blur may be performed as a post process in 
image space using scene velocities stored per­pixel [Rosada 2007]. Such vectors are typically held in 
a packed G-Buffer format as 2D 8-bit components or further packed to an 8-bit vector palette. The averaging 
filter kernel then performs sheared sampling along the direction of such vectors from a stored rendered 
image. The quality of this method depends on the number of samples contributing to the averaging calculation 
of each resulting blurred pixel. To guarantee artifact free results we require to recover enough samples 
to satisfy spatial and temporal frequency bounds [Egan et al 2009]. For example, a simple pixel with 
velocity of 16 pixels per second requires a sampling kernel with 16 samples. With a single image to sample 
from and no layered depth information, we do not account for temporal occlusion effects. The speed of 
this method depends on the bandwidth of generating and storing motion vectors, then recovering image 
space veloci­ties for blur sampling in the post process. Here, we describe an improved algorithm through 
the use of vector motion IDs. Vector Motion IDs For each visible pixel of each rendered object a pixel 
velocity is stored. Under perspective project image space motion vectors vary greatly across the screen. 
However, in scenes of high velocity coherence, where world space velocity is constant for the majority 
of the scene, these motion vectors may be reduced to a shared ID representing the world space velocity. 
In the post process stage, a recovery of the image space velocity is achieved by inverse projection of 
the world space velocity matching the sampled motion ID. With 4 bits sufficient to store motion IDs of 
16 scene elements with different velocities, this minimizes the storage space and bandwidth required 
for velocities and reduces time spent in the G-Buffer generation pass. Tiled Adaptive Sampling With a 
few scene velocities under perspective projection we observe a typically smooth and well partitioned 
variation in the magnitude of image space vectors. Applying a fixed size filter kernel sufficient to 
account for maximum velocity without artifacts results in too many samples overall to perform the effect 
in real time. Without scope for variable length sampling loops supported by shader model 3.0 graphics 
hardware, we therefore apply a tiled classification of velocities to select from a small number of pre-optimized 
shaders with fixed kernel sizes. This classification may be pre-determined for known camera/scene behaviors, 
calculated from image space bounds of object based velocities or extracted from results of motion ID 
generation in either a CPU or GPU compute style processing framework. Texture Space Motion Blur At this 
point we still have a limit on the velocities we can represent effectively in real-time with high quality. 
We eliminate this limitation by combining above with texture space motion blur. Loviscach [2005] shows 
that existing MIP mapping hardware may be used to apply motion blur to textures. While this method samples 
anisotropically along the motion vector, we use smaller MIP levels primarily to act as a pre-computation 
of the wider sampling kernel. Thus allowing us to perform less than one motion blur sample per pixel 
at run-time without blemishes. For fine grained control, we override regular shader sampling calculations 
to bias the computed texture level of detail based on coarse object velocity and per pixel distance. 
 Discussion Combining image and texture space motion blur methods enable us to smoothly blur the scene 
s content with high quality. Adaptive sampling provides sufficient blur on geometry motions not handled 
by texture space blurs. In addition, non-anisotropic texture fetches, which would otherwise only blur 
the textures appearance without direction, are possible when augmented with image space vector motion 
blur. A run-time frequency analysis of image content may further reduce required number of samples, e.g. 
on smoothly varying low frequency textures. EGAN, K. TSENG, Y.-T., HOLZSCHUCH, N., DURAND, F. AND RAMAMOORTHI, 
R. 2009. Frequency Analysis and Sheared Reconstruction for Rendering Motion Blur. In ACM Transac­tions 
on Graphics 28, 3. ROSADO, G. 2007. Motion Blur as a Post-Processing Effect. In GPU Gems 3, H. Nguyen, 
Ed. 575-582. LOVISCACH, J. 2005. Motion Blur for Textures by Means of Anisotropic Filtering. EG Symposium 
on Rendering, 105-110. Copyright is held by the author / owner(s). SIGGRAPH 2010, Los Angeles, California, 
July 25 29, 2010. ISBN 978-1-4503-0210-4/10/0007  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2010</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1837049</article_id>
		<sort_key>230</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>18</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[A deferred shading pipeline for real-time indirect illumination]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1837026.1837049</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1837049</url>
		<abstract>
			<par><![CDATA[<p>Computing indirect lighting in video games simultaneously improves gameplay and scene realism. However, the context of 3D video games brings very restrictive constraints: (1) the computation should be very fast (less than 10 ms) and --most importantly-- with a constant cost, <i>i.e.</i> independent of both the geometric and lighting complexity of the scene; (2) the indirect illumination algorithm should work seamlessly on dynamic scenes, with any source of direct illumination --not only point light sources-- and easily take place into a game engine pipeline; and (3) the computed result may be approximate but must be artifact-free and temporally coherent.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Color, shading, shadowing, and texture</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>K.8.0</cat_node>
				<descriptor>Games</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010476.10011187.10011190</concept_id>
				<concept_desc>CCS->Applied computing->Computers in other domains->Personal computers and PC applications->Computer games</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10011007.10010940.10010941.10010969.10010970</concept_id>
				<concept_desc>CCS->Software and its engineering->Software organization and properties->Contextual software domains->Virtual worlds software->Interactive games</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002951.10003227.10003251.10003258</concept_id>
				<concept_desc>CCS->Information systems->Information systems applications->Multimedia information systems->Massively multiplayer online games</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010384</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Texturing</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2264828</person_id>
				<author_profile_id><![CDATA[81100200748]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Cyril]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Soler]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[INRIA, Grenoble university]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264829</person_id>
				<author_profile_id><![CDATA[81466647404]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Olivier]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hoel]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[INRIA, Grenoble university]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264830</person_id>
				<author_profile_id><![CDATA[81466648281]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Frank]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Rochet]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Eden Games]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1507162</ref_obj_id>
				<ref_obj_pid>1507149</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[
Nichols, G., and Wyman, C. 2009. Multiresolution splatting for indirect illumination. In <i>Proc. ACM Symposium on Interactive 3D Graphics and Games 2009</i>.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2383600</ref_obj_id>
				<ref_obj_pid>2383586</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[
Nichols, G., Shopf, J., and Wyman, C. 2009. Hierarchical image-space radiosity for interactive global illumination. In <i>Proceedings of Eurographics Symposium on Rendering 2009</i>, H. Lensch and P.-P. Sloan, Eds.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1409082</ref_obj_id>
				<ref_obj_pid>1409060</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[
Ritschel, T., Grosch, T., Kim, M. H., Seidel, H.-P., Dachsbacher, C., and Kautz, J. 2008. Imperfect shadow maps for efficient computation of indirect illumination. <i>ACM Transactions on Graphics 27</i>, 5 (December).
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1507161</ref_obj_id>
				<ref_obj_pid>1507149</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[
Ritschel, T., Grosch, T., and Seidel, H.-P. 2009. Approximating dynamic global illumination in image space. In <i>Proc. ACM Symposium on Interactive 3D Graphics and Games 2009 (I3D '09)</i>.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 A Deferred Shading Pipeline for Real-Time Indirect Illumination Cyril Soler Olivier Hoel Frank Rochet 
INRIA, Grenoble university* INRIA, Grenoble university Eden Games 1 Introduction Computing indirect lighting 
in video games simultaneously im­proves gameplay and scene realism. However, the context of 3D video 
games brings very restrictive constraints: (1) the computa­tion should be very fast (less than 10 ms) 
and most importantly with a constant cost, i.e. independent of both the geometric and lighting complexity 
of the scene; (2) the indirect illumination algo­rithm should work seamlessly on dynamic scenes, with 
any source of direct illumination not only point light sources and easily take place into a game engine 
pipeline; and (3) the computed result may be approximate but must be artifact-free and temporally coherent. 
We present a deferred-shading algorithm for computing indirect illumination that exactly suites these 
constraints. The method is approximate as the indirect visibility is not accounted for, but it naturally 
allows multiple bounces and the last bounce of indirect illumination can re.ect on non-diffuse materials. 
 Direct only Direct + indirect  Direct only Direct + indirect Figure 1: One-bounce indirect illumination 
computed by our tech­nique (top row in a scene with 3M polygons), and bottom row in the the game engine 
"Alone in the Dark, Near Death Investigation". In the bottom the indirect illumination is importance 
sampled and re.ected on the glossy metallic bin. Computation time is 10 ms for both on a GTX260 NVidia 
card, at resolution 1280x1024.  2 Deferred shading indirect illumination Our algorithm works entirely 
in screen-space, hierarchically. It takes G-Buffers as input: normals, re.ectance parameters, depth, 
and diffuse direct illumination. These buffers are mipmap-ed, and we compute indirect illumination at 
each level using a common shader. This shader gathers light from points randomly sampled in a .xed-size 
circular region of the image. This eventually gath­ers light from arbitrary distances. To do this, we 
reformulate the Monte-Carlo sampling of the bouncing light equation to .t uniform screen-space samples. 
The contributions from all mipmap levels are then up-sampled (with bilateral up-sampling) and added together. 
The resulting image contains indirect lighting between all visible parts of the scene. To remove the 
noise due to sampling, we aver­age indirect lighting across successive UN-warped frames (at now extra 
cost), and we combine multiple indirect illumination compo­nents from several cameras. The indirect illumination 
is combined with the re.ectance buffer. It is added to the direct illumination, and possibly further 
used as input to compute the next bounce. As a consequence, our technique computes light interaction 
at arbitrary distance in the image plane, at a constant cost. Further­more, it allows a computation time 
/ accuracy trade-off by selecting starting from which screen resolution the light gathering shader is 
applied. This approximation results in low-pass .ltering the in­direct illumination rather than loss 
of illumination. It works with arbitrary sources of direct illumination (most games commonly use e.g. 
a huge number of omni-directional point light sources), and the cost is independent on visual complexity 
and illumination. Finally, our algorithm is real-time (less than 10 ms per frame) on recent graphics 
hardware as shown in the table below: Resolution/samples Quarter/64 Quarter/256 Half/256 Half/64 Architectures 
(a) (b) (a) (b) (a) (b) (a) (b) Total time (ms) 13.9 4.6 33.6 11.2 110.6 32.6 34.8 10.5 Figure 2: Computation 
times in ms for different parameter sets, measured using (a) NVidia 8800GTS and (b) NVidia GTX260 cards, 
for a display resolution of 1280 × 720p for the images on Figure 1. 3 Related work We extend to arbitrary 
distances the local indirect illumination com­putation proposed by Ritschel [2009]. Multiresolution Splatting 
of Indirect Illumination [Nichols and Wyman 2009] and Hierarchical Image-Space Radiosity [Nichols et 
al. 2009] also compute long­distance indirect illumination. They use virtual point lights (VPL) to represent 
the direct illumination, which they collect with Re.ec­tive Shadow Maps (RSM). While the former adaptively 
splats illu­mination for all VPLs, the later further organizes the VPLs into a tree structure and performs 
adaptive gathering. RSMs may need a large number of render passes to approximate low frequency sources 
of direct illumination (e.g. envmaps, or indi­rect lighting for 2nd bounce). Methods based on RSMs thus 
depend on the complexity of the direct illumination and therefore are less robust to varying lighting 
conditions and less suited to more than one bounce of indirect illumination. Because we use and compute 
screen-space illumination, we don t have this limitation. The cost of RSMs also depends on the scene 
complexity, whereas our tech­nique has a constant cost w.r.t to geometric and illumination com­plexity. 
Splatting indirect illumination from VPLs does not allow to importance sample non diffuse BRDFs, because 
all pixels share the same VPLs. With our method, each pixel may have it s own sets of lights samples 
in screen-space. Imperfect shadow maps [Ritschel et al. 2008] approximately simulates indirect shadows, 
for large enough objects. In addition to using VPLs, this method requires preprocessing of the geometry, 
which makes it incompatible with fully dynamic scenes where the geometry content may not be known in 
advance, such as in games. References NICHOLS, G., AND WYMAN, C. 2009. Multiresolution splatting for 
indirect illumi­nation. In Proc. ACM Symposium on Interactive 3D Graphics and Games 2009. NICHOLS, G., 
SHOPF, J., AND WYMAN, C. 2009. Hierarchical image-space radiosity for interactive global illumination. 
In Proceedings of Eurographics Symposium on Rendering 2009, H. Lensch and P.-P. Sloan, Eds. RITSCHEL, 
T., GROSCH, T., KIM, M. H., SEIDEL, H.-P., DACHSBACHER, C., AND KAUTZ, J. 2008. Imperfect shadow maps 
for ef.cient computation of indirect illumination. ACM Transactions on Graphics 27, 5 (December). RITSCHEL, 
T., GROSCH, T., AND SEIDEL, H.-P. 2009. Approximating dynamic global illumination in image space. In 
Proc. ACM Symposium on Interactive 3D Graphics and Games 2009 (I3D 09). Copyright is held by the author 
/ owner(s). SIGGRAPH 2010, Los Angeles, California, July 25 29, 2010. ISBN 978-1-4503-0210-4/10/0007 
  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2010</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	</section>
	<section>
		<section_id>1837050</section_id>
		<sort_key>240</sort_key>
		<section_seq_no>6</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Biomedical]]></section_title>
		<section_page_from>19</section_page_from>
	<article_rec>
		<article_id>1837051</article_id>
		<sort_key>250</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>19</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Estimating subject-specific parameters for modeling hand joints]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1837026.1837051</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1837051</url>
		<abstract>
			<par><![CDATA[<p>In both biomedical and graphics applications, quantifying skeletal motion requires a kinematic model describing the joint location and axis directions. In this talk, we present techniques we have recently developed for determining parameters of three types of anatomic joints. In particular, it is important but difficult to obtain high-quality parameter estimates for the joints of the human hand. Due to the small scale of hand segments, mis-location of joint centers by even 1 cm leads to unacceptable errors in the joint angle measurement. In addition, the axis directions of hand joints are not aligned with standard anatomical planes [1], and each individual has subject-specific axis directions. The errors in axis location and direction from standard estimation techniques result in noticeable differences in reconstructed hand shape and the grasp contact points of the fingers.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>J.3</cat_node>
				<descriptor>Biology and genetics</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.4.9</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.6.5</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010341.10010342</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Model development and analysis</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010225</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision tasks</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010444.10010087</concept_id>
				<concept_desc>CCS->Applied computing->Life and medical sciences->Computational biology</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010444.10010095</concept_id>
				<concept_desc>CCS->Applied computing->Life and medical sciences->Systems biology</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010444.10010935</concept_id>
				<concept_desc>CCS->Applied computing->Life and medical sciences->Genetics</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2264831</person_id>
				<author_profile_id><![CDATA[81466641456]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Lillian]]></first_name>
				<middle_name><![CDATA[Y.]]></middle_name>
				<last_name><![CDATA[Chang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264832</person_id>
				<author_profile_id><![CDATA[81100495142]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Nancy]]></first_name>
				<middle_name><![CDATA[S.]]></middle_name>
				<last_name><![CDATA[Pollard]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[P. W. Brand and A. M. Hollister, <i>Clinical Mechanics of the Hand</i>, 3rd ed. C. V. Mosby, 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[L. Y. Chang and N. S. Pollard, "Constrained least-squares optimization for robust estimation of center of rotation." <i>Journal of Biomechanics</i>, 40(6): 1392--1400, Jul 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[L. Y. Chang and N. S. Pollard, "Robust estimation of dominant axis of rotation." <i>Journal of Biomechanics</i>, 40(12): 2707--2715, Mar 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[L. Y. Chang and N. S. Pollard, "Method for determining kinematic parameters of the in vivo thumb carpometacarpal joint." <i>IEEE Transactions on Biomedical Engineering</i>, 55(7):1897--1906, Jul 2008.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Estimating subject-speci.c parameters for modeling hand joints Lillian Y. Chang and Nancy S. Pollard 
In both biomedical and graphics applications, quantifying skeletal motion requires a kinematic modeldescribing 
thejointlocation and axisdirections. Inthistalk, wepresenttechniques wehave recentlydevelopedfordeterminingparameters 
of three types of anatomicjoints. Inparticular,it isimportantbutdi.cult to obtainhigh-qualityparameter 
estimatesfor thejoints of thehuman hand. Duetothe small scale ofhand segments, mis-location ofjoint centersby 
even1cmleadsto unacceptable errorsinthejoint angle measurement. In addition,the axisdirections ofhandjoints 
are not aligned with standard anatomicalplanes[1], and eachindividualhas subject-speci.c axis directions. 
The errors in axis location and direction from standard estimation techniques result in noticeable di.erences 
in reconstructed hand shape and the grasp contact points of the .ngers. Our methods address these challenges 
arising from the small range of motion and complex anatomy ofhandjoints. These methodshavebeenpreviouslypublished 
tothebiomechanics com­munity, but they may also be used for subject-speci.c hand models in animation 
or virtual reality applications. Geometric joint models. Parametersforasphericaljointandasingle-axishingejointare 
.t using geometry-based optimization approaches. Previous methods for these joints work well for near-idealjoint 
movementwithlargeranges ofmotion. However,thesmallrange ofmotion and noisein realhanddata resultinpoor 
estimates(over1cmposition error and over30. direction error) due to plane singularities of the existing 
techniques. For a spherical joint, measured data points are .t to concentric spheres. Our solution [2] 
relocatestheplane singularity ofthe standardleast squares solution to apoint singularity associated with 
zero-radii spheres, a case which is not encountered in practice. The constrained least-squares formulation 
provides a repeatable solution with a direct global optimization that is not subject to local minima 
and does not require manually-tuned parameters typical in iterative techniques. Forahingejoint,thedatapointsare 
.ttocircularpathsinparallelplanes. Oursolution[3] avoids the singularity from minimizing only planar 
errors by additionally penalizing the paths radialerrors. Thisre.ectsthejointconstraintsmoreaccurately, 
andthedominantrotationaxis can be determined even in the presence of secondary motion. Anatomy-based 
model. Thecarpo-metacarpal(CMC) saddlejoint isdi.culttomodelbecause thetwo axes arenon-orthogonal(skew) 
norintersect ata commonpoint(separated). The complex modelfor separated skew axeshas ahigh-dimensionalparameter 
space, and apurelygeometric cost functionofdistanceerrorsisextremely .atinthisspace. Ouranatomy-basedtechnique[4] 
uses a reduced parameter space representing equivalent solution families and incorporates anatomical 
constraints to bias the search. Our resulting subject-speci.c models were consistent with clinical anatomical 
descriptions of the CMC, and our evaluation of 48 human hands contributes range of motion measurements 
for un-impaired thumbs. Our talk will present the spherical joint estimation as the primary example to 
highlight the importance of choosing an appropriate optimization function. We will also brie.y describe 
our insightsintothecostfunctionchoicesfor .tting thehingejointand thesaddlejointmodels. References [1] 
P. W. Brand and A. M. Hollister, Clinical Mechanics of the Hand, 3rd ed. C.V. Mosby, 1999. [2] L. Y. 
Chang and N. S. Pollard, Constrained least-squares optimization for robust estimation of center of rota­tion. 
Journal of Biomechanics, 40(6): 1392 1400, Jul 2007. [3] , Robust estimation of dominant axis of rotation. 
Journal of Biomechanics, 40(12): 2707 2715, Mar 2007. [4] , Method for determining kinematic parameters 
of the in vivo thumb carpometacarpal joint. IEEE Trans­actions on Biomedical Engineering, 55(7):1897 
1906, Jul 2008. Copyright is held by the author / owner(s). SIGGRAPH 2010, Los Angeles, California, July 
25 29, 2010. ISBN 978-1-4503-0210-4/10/0007 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2010</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1837052</article_id>
		<sort_key>260</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>20</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Multigrid optical flow for deformable medical volume registration]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1837026.1837052</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1837052</url>
		<abstract>
			<par><![CDATA[<p>In medical imaging, a need often arises to compare images or volumes in a time series. For example, one approach to improving cancer detection acquires a sequence of volumes while injecting a contrast agent. Differences in the uptake of the contrast agent can differentiate potential malignancies from normal tissue.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>J.3</cat_node>
				<descriptor>Medical information systems</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.4.8</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010225.10010227</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision tasks->Scene understanding</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010444.10010447</concept_id>
				<concept_desc>CCS->Applied computing->Life and medical sciences->Health care information systems</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2264833</person_id>
				<author_profile_id><![CDATA[81466643270]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Ariel]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bernal]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Intel of Canada Ltd., Software & Services Group]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264834</person_id>
				<author_profile_id><![CDATA[81466647387]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Ashok]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Thirumurthi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Intel of Canada Ltd., Software & Services Group]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264835</person_id>
				<author_profile_id><![CDATA[81466647467]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Hans]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Pabst]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Intel of Canada Ltd., Software & Services Group]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264836</person_id>
				<author_profile_id><![CDATA[81466644852]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Tyler]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nowicki]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Intel of Canada Ltd., Software & Services Group]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264837</person_id>
				<author_profile_id><![CDATA[81100112613]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Michael]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[McCool]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Intel of Canada Ltd., Software & Services Group]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Multigrid Optical Flow for Deformable Medical Volume Registration Ariel Bernal, Ashok Thirumurthi, Hans 
Pabst, Tyler Nowicki, Michael McCool Intel of Canada Ltd., Software &#38; Services Group BreastDefContrast.bmp 
In medical imaging, a need often arises to compare images or volumes in a time series. For example, one 
approach to improving cancer detection acquires a sequence of volumes while injecting a contrast agent. 
Differences in the uptake of the contrast agent can differentiate potential malignancies from normal 
tissue. However, motion of the patient between volume acquisitions means that the volumes need to be 
registered before they can be compared. In the case of soft tissues, deformable registration is required. 
We will present a deformable registration algorithm that is robust to changes in contrast and uses both 
multigrid and many-core parallelism for high performance. Optical Flow Optical flow is one approach 
to computing a deformable registration. Computation of optical flow requires finding a deformation of 
one volume (the moving volume) that satisfies the optical flow equation, which is given as a constraint 
on differentials: ....·....+....=0. Here E is the 4D space-time volume (3 spatial positions, indexed 
by the vector .., and one time dimension). Subscripts indicate partial derivatives. The vector .... indicates 
the deformation as a function of time. However, the optical flow equation underconstrains the solution, 
so an additional regularization constraint that specifies a smooth solution also needs to be included. 
 Solution Algorithm Finding a solution to the optical flow equation is done iteratively. During each 
iteration, the moving volume must be resampled with the current deformation and must be smoothed with 
a convolution in order to compute the regularization. Differential operators must also be computed, in 
particular the gradient of the fixed image Extensions and Results Optical flow can be an expensive algorithm 
when applied to a large number of high-resolution volumes. In addition, the basic formulation of the 
optical flow algorithm does not actually allow for brightness changes other than those induced by motion. 
Of course, in applications of interest, such as contrast agent injection, such changes are present. 
In fact, the goal is often to differentiate such contrast changes from those due to motion. We will present 
an extension of optical flow that allows for contrast changes, while still computing an accurate deformation. 
This extension does not add significant computational overhead. In addition, we will present the results 
of many-core parallelization combined with the use of a multigrid extension to optical flow. Together 
these optimizations improve the performance of the baseline algorithm by two orders of magnitude. Results 
computed by our algorithm are shown in the image below. Multigrid not only significantly improves performance, 
it also leads to more accurate and robust solutions. Top: (a) fixed image (b) moving image. Bottom: 
(a) registered volumes with contrast removed (b) recovered contrast. Dataset courtesy of Sentinelle Medical. 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2010</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1837053</article_id>
		<sort_key>270</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>21</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Sensorimotor physiology]]></title>
		<subtitle><![CDATA[modeling, imaging, and neural control]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1837026.1837053</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1837053</url>
		<abstract>
			<par><![CDATA[<p>Based on recent developments in computer simulation, medical imaging, and neurophysiology, we can now construct detailed functional models of the human biomechanical plant, its neural control, and its physical interaction with the environment. These new developments will have a profound impact on both how computer animation will be done in the future and how we understand human movement in science and medicine.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>J.3</cat_node>
				<descriptor>Biology and genetics</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.6.5</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010341.10010342</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Model development and analysis</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061.10010063</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures->Computational geometry</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010444.10010935</concept_id>
				<concept_desc>CCS->Applied computing->Life and medical sciences->Genetics</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010444.10010087</concept_id>
				<concept_desc>CCS->Applied computing->Life and medical sciences->Computational biology</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010444.10010095</concept_id>
				<concept_desc>CCS->Applied computing->Life and medical sciences->Systems biology</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2264838</person_id>
				<author_profile_id><![CDATA[81100642559]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Dinesh]]></first_name>
				<middle_name><![CDATA[K.]]></middle_name>
				<last_name><![CDATA[Pai]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of British Columbia]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264839</person_id>
				<author_profile_id><![CDATA[81322508645]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Kees]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[van den Doel]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of British Columbia]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264840</person_id>
				<author_profile_id><![CDATA[81100295179]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Timothy]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Edmunds]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of British Columbia]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264841</person_id>
				<author_profile_id><![CDATA[81388599480]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Benjamin]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gilles]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of British Columbia]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264842</person_id>
				<author_profile_id><![CDATA[81466645026]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[David]]></first_name>
				<middle_name><![CDATA[I. W.]]></middle_name>
				<last_name><![CDATA[Levin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of British Columbia]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264843</person_id>
				<author_profile_id><![CDATA[81335498230]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Shinjiro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sueda]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of British Columbia]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264844</person_id>
				<author_profile_id><![CDATA[81320496267]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>7</seq_no>
				<first_name><![CDATA[Qi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wei]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of British Columbia]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264845</person_id>
				<author_profile_id><![CDATA[81466645169]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>8</seq_no>
				<first_name><![CDATA[Sang]]></first_name>
				<middle_name><![CDATA[Hoon]]></middle_name>
				<last_name><![CDATA[Yeo]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of British Columbia]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[
Gilles, B., and Pai, D. K. 2008. Fast musculoskeletal registration based on shape matching. In <i>Proceedings of Medical Image Computing and Computer-Assisted Intervention - MICCAI 2008</i>, LNCS 5242 Part II, 822--827.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1360682</ref_obj_id>
				<ref_obj_pid>1399504</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[
Sueda, S., Kaufman, A., and Pai, D. K. 2008. Musculotendon simulation for hand animation. <i>ACM Trans. Graph. (Proc. SIGGRAPH) 27</i>, 3, 83:1--83:8.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[
van den Doel, K., Ascher, U. M., and Pai, D. K. 2008. Computed myography: three dimensional reconstruction of motor functions from surface emg data. <i>Inverse Problems 24</i>, 065010, 17 pages.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2174964</ref_obj_id>
				<ref_obj_pid>2174949</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[
Wei, Q., Sueda, S., and Pai, D. K. 2010. Biomechanical simulation of human eye movement. In <i>Proceedings of the International Symposium on Biomedical Simulation (ISBMS10)</i>, 108--118.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Sensorimotor Physiology: Modeling, Imaging, and Neural Control Dinesh K. Pai* Kees van den Doel Timothy 
Edmunds Benjamin Gilles David I. W. Levin Shinjiro Sueda Qi Wei Sang Hoon Yeo Sensorimotor Systems Laboratory, 
University of British Columbia http://www.cs.ubc.ca/~pai    Based on recent developments in computer 
simulation, medical imaging, and neurophysiology, we can now construct detailed func­tional models of 
the human biomechanical plant, its neural control, and its physical interaction with the environment. 
These new de­velopments will have a profound impact on both how computer an­imation will be done in the 
future and how we understand human movement in science and medicine. This approach could lay a new foundation 
for computer animation of human-like characters based on simulation of human physiology and neural control. 
It will allow human character animation to be speci.ed at a much higher level and will automatically 
reproduce important features such as the visible movement of muscles, tissue deformation on contact, 
and dynamic constraints. Conversely, these developments could lead to a deeper, more con­structive understanding 
of human movement. The human body and brain are among the most complex systems studied in science. They 
will place enormous new demands on computer graphics to sim­ulate these complex systems ef.ciently (preferably 
at interactive rates) and to visualize complex behaviors. There are many appli­cations in medicine, including 
diagnosis of residual hand function following spinal cord injury, stroke, or repetitive stress injuries; 
sur­gical planning and rehabilitation; and control of neuroprosthetics. We will describe ongoing work 
in our group for constructing such models of the musculoskeletal system, using .ber-like 3D elastic elements 
we call strands [Sueda et al. 2008]. We will also de­scribe how these models can be constructed using 
many imaging techniques, including MRI (Magnetic Resonance Imaging) [Gilles and Pai 2008], DTI (Diffusion 
Tensor Imaging), CMG (Computed Myography, which estimates activity of muscles)[van den Doel et al. 2008], 
and CT (Computed Tomography). We will provide examples of modeling the human hand and eye [Wei et al. 
2010], and animals including frogs and rats. Taken together, these tech­niques demonstrate a new constructive 
approach to understanding *Authors after the .rst are listed in alphabetical order. This work was supported 
in part by the the Canada Research Chairs Program, Peter Wall Institute for Advanced Studies, the Human 
Frontier Science Program, NIH grant R01AR053608, NSERC, and CIHR (CRCNS). Figure 4: Three dimensional 
model of frog acquired from high res­olution micro CT images (left), producing an animatable skeleton 
and skin (right). biological sensing and motor control. References GILLES, B., AND PAI, D. K. 2008. Fast 
musculoskeletal registra­tion based on shape matching. In Proceedings of Medical Image Computing and 
Computer-Assisted Intervention -MICCAI 2008, LNCS 5242 Part II, 822 827. SUEDA, S., KAUFMAN, A., AND 
PAI, D. K. 2008. Musculoten­don simulation for hand animation. ACM Trans. Graph. (Proc. SIGGRAPH) 27, 
3, 83:1 83:8. VAN DEN DOEL, K., ASCHER, U. M., AND PAI, D. K. 2008. Computed myography: three dimensional 
reconstruction of mo­tor functions from surface emg data. Inverse Problems 24, 065010, 17 pages. WEI, 
Q., SUEDA, S., AND PAI, D. K. 2010. Biomechanical sim­ulation of human eye movement. In Proceedings of 
the Interna­tional Symposium on Biomedical Simulation (ISBMS10), 108 118. Copyright is held by the author 
/ owner(s). SIGGRAPH 2010, Los Angeles, California, July 25 29, 2010. ISBN 978-1-4503-0210-4/10/0007 
 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<article_sponsors>
			<funding_agency>NIH</funding_agency>
			<grant_numbers>
				<grant_number>R01AR053608</grant_number>
			</grant_numbers>
		</article_sponsors>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2010</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	</section>
	<section>
		<section_id>1837054</section_id>
		<sort_key>280</sort_key>
		<section_seq_no>7</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Simulation In Production]]></section_title>
		<section_page_from>22</section_page_from>
	<article_rec>
		<article_id>1837055</article_id>
		<sort_key>290</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>22</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Simulating Rapunzel's hair in Disney's <i>Tangled</i>]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1837026.1837055</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1837055</url>
		<abstract>
			<par><![CDATA[<p>Hair simulation is known to be a complex problem in animation. In <i>Tangled</i> we have the extreme task of simulating 70 feet of hair for the film's main character, Rapunzel. The excessive hair length in addition to the loose style and intricate structure of the hair present many unique challenges in simulation. Moreover, the specific art direction of the film adds to the complexity of the simulation.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.6.8</cat_node>
				<descriptor>Animation</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.6.5</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010341.10010342</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Model development and analysis</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061.10010063</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures->Computational geometry</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341.10010349.10011310</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Simulation types and techniques->Simulation by animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2264846</person_id>
				<author_profile_id><![CDATA[81450592780]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Kelly]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ward]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Walt Disney Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264847</person_id>
				<author_profile_id><![CDATA[81341496532]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Maryann]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Simmons]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Walt Disney Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264848</person_id>
				<author_profile_id><![CDATA[81350579664]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Andy]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Milne]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Walt Disney Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264849</person_id>
				<author_profile_id><![CDATA[81466648668]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Hidetaka]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yosumi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Walt Disney Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264850</person_id>
				<author_profile_id><![CDATA[81466644111]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Xinmin]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Zhao]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Walt Disney Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Simulating Rapunzel s Hair in Disney s Tangled KellyWard Maryann Simmons Andy Milne HidetakaYosumi Xinmin 
Zhao Walt DisneyAnimation Studios  1 Introduction Hair simulationisknowntobea complex problemin animation.In 
Tangled we have the extreme task of simulating 70 feet of hair for the .lm s main character, Rapunzel. 
The excessive hair length in addition to the loose style and intricate structure of the hair present 
many unique challenges in simulation. Moreover, the speci.c art direction of the .lm adds to the complexity 
of the simulation. 2 Simulation Features To simulate Rapunzel s hair we use our proprietary hair simulation 
software, dynamicWires, which utilizes a mass-spring system for the dynamics of curves. We have extended 
this system to handle the unique challenges of Rapunzel s hair. Hair Piling and Volume Preservation Rapunzel 
s hair con­stantly piles on top of itself on the ground and any other objects or characters in the scene. 
During simulation, spring forces are ap­plied to colliding segments on the .y to help maintain the volume 
as well as act as a frictional force on the strands moving over each other or moving apart. The springs 
only form on direct contact and break after the curve segments have moved beyond a distance threshold 
that is tuned based on how the hair strands need to slide over each other or stick together. For run-time 
ef.ciencywe offset the complexity of simulating the immense length of hair by simulating a sparse set 
of guide curves, around 200. The lack of dense coverage of the curves is most ev­ident near her head 
where it is easy for curves on the outer, most­visibleregionsofthehairtofall throughthegapsinthehairvolume 
into the inner, bottom layers of hairs. We add extra collision sup­port structuresto.llthegapsinthehairvolumeby 
connectingthe control points of neighboring curves and use the segments that are created from the connections 
as additional collision in.uences. The simulation curves then collide with these support structures as 
they do with anyother curve. Effortless Dragging In reality, Rapunzel would need to exert a great deal 
of physical effort to drag her hair behind her. The art direction of the .lm calls for Rapunzel to be 
quite adept at moving with this massive amount of hair; the hair needs to glide easily with her when 
she moves, causing no strain on her motion, and come to a rest when she stops. Tocontrolthedegreeofpullonherhair,weaddatangential 
friction parameter for the ground contacts by separating the component of friction that lies in the direction 
tangent to the hair strands. The tan­gential friction component is then scaled down by as much as two 
ordersof magnitudeincomparisonwiththe remaining friction.The hair is then able to easily slide along 
its length without spreading outward on the ground. Moreover, using a high static friction for ground 
contacts helps stop the hair from continually sliding along the ground after she stops moving. Simulation 
Freezing Considering that the visible length and ac­tivity of the hair vary per shot, the minimum length 
of hair needed for simulation also varies per shot. We use a simulation-freezing feature that turns-off 
the simulation for the back part of the hair, allowing us to adjust the length of simulated hair per-shot, 
acceler­ating the run-time of the simulation signi.cantly. For scenes where the backof the hairis not 
visiblebut Rapunzelis still quite active, the non-visible, non-simulating end of the hair is detachedfromthe 
restofthehairsoitdoesnotpullbackonthestill active portions. When the detachment makes the weight of the 
hair appear too light, a constraint is added to the end of the simulated portiontopullback slightly,givingthe 
illusionofweight. Alternatively, large amounts of hair are often visible lying on the groundbut are notin 
motion. When these inactive portionsof hair are visibly connected to active portions wekeep the frozen 
section attached and use spring forces to smoothly blend from the simu­lating to the non-simulating portions. 
The computational resources of the simulation are thus used for the highly active regions of the hair 
without wasting time simulating regions of the hair that do not move. Hair-Hair Constraints Controlling 
the hair while still allowing it to move naturally takes special consideration. We place loose springs 
between nearby curves to subtly in.uence the hair to hold its resting con.guration, preventing it from 
falling everywhere, such as spilling over her shoulders, covering her face and body. Given a distance 
breakaway parameter, these constraints automati­cally attenuate their strength past a certain distance 
and break com­pletely beyond another distance. Thiskeeps the hair generally to­gether without forcing 
it to move as a single mass. The hair can still break apart from thevolume into smaller pieces,givinga 
natu­ral look to the overall motion. 3 Results The simulation features described have enabled us to 
simulate the extreme length of Rapunzel s hair. Simulation freezing and the col­lision support structures 
helped us to simulate only a fraction of hertotal amountofhairpershot,easingtheburdenofthiscomplex hair 
immensely. Effortless dragging and hair-hair constraintsgave us control over the hair s motion to let 
the hair move naturally yet still adhere to the art direction of the .lm. All images are c@DisneyEnterprises, 
Inc. Copyright is held by the author / owner(s). SIGGRAPH 2010, Los Angeles, California, July 25 29, 
2010. ISBN 978-1-4503-0210-4/10/0007  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2010</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1837056</article_id>
		<sort_key>300</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>23</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Seamless fracture in a production pipeline]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1837026.1837056</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1837056</url>
		<abstract>
			<par><![CDATA[<p>To alleviate the laborious task of generating volumetric fracture pieces by hand while preserving the integrity of the artistically modeled shape, we develop a practical, robust, and effective solution for artistically directed volumetric fracture. Unlike previous approaches our procedural fracture system maintains intrinsic attributes (e.g., texture coordinates), preserves discontinuities such as texture seams, and generates procedurally fragmented "chunks" with <i>seamless</i> boundaries. The key steps in our fracture pipeline are: robust scan-conversion from an input geometry, generation of fragmented fracture pieces with artistic control, and high fidelity mesh generation of the disjoint fracture fragments.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.5</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.4.10</cat_node>
				<descriptor>Volumetric</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.4</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010240.10010242</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision representations->Shape representations</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061.10010063</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures->Computational geometry</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2264851</person_id>
				<author_profile_id><![CDATA[81466645936]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Akash]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Garg]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[DreamWorks Animation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264852</person_id>
				<author_profile_id><![CDATA[81466645317]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Kyle]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Maxwell]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[DreamWorks Animation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>566586</ref_obj_id>
				<ref_obj_pid>566570</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Ju, T., Losasso, F., Schaefer, S., and Warren, J. 2002. Dual contouring of hermite data. In <i>SIGGRAPH '02: Proceedings of the 29th annual conference on Computer graphics and interactive techniques</i>, ACM, New York, NY, USA, 339--346.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1401110</ref_obj_id>
				<ref_obj_pid>1401032</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Museth, K., and Clive, M. 2008. Cracktastic: fast 3d fragmentation in "the mummy: Tomb of the dragon emperor". In <i>SIGGRAPH '08: ACM SIGGRAPH 2008 talks</i>, ACM, New York, NY, USA, 1--1.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>776770</ref_obj_id>
				<ref_obj_pid>776751</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Nooruddin, F. S., and Turk, G. 2003. Simplification and repair of polygonal models using volumetric techniques. <i>IEEE Transactions on Visualization and Computer Graphics 9</i>, 2, 191--205.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Seamless Fracture in a Production Pipeline Akash Garg* Kyle Maxwell DreamWorks Animation  Figure 1: 
Fracturing a cliff. Left: The original surface geometry, Middle: the scan-converted, fractured, and reconstructed 
geometry at frame 1, Right: the fractured geometry at frame 25. Note the .delity to the original geometry 
and preservation of sharp features. Contributions To alleviate the laborious task of generating vol­umetric 
fracture pieces by hand while preserving the integrity of the artistically modeled shape, we develop 
a practical, robust, and effective solution for artistically directed volumetric fracture. Un­like previous 
approaches our procedural fracture system maintains intrinsic attributes (e.g., texture coordinates), 
preserves discontinu­ities such as texture seams, and generates procedurally fragmented chunks with seamless 
boundaries. The key steps in our fracture pipeline are: robust scan-conversion from an input geometry, 
gen­eration of fragmented fracture pieces with artistic control, and high .delity mesh generation of 
the disjoint fracture fragments. Scan-conversion The input to our fracture tool is a base geom­etry, 
usually a textured polygonal or NURBS surface. To handle self-intersecting, open, and non-manifold geometry, 
we use a ro­bust scan-conversion algorithm based on the work of Nooruddin &#38; Turk [Nooruddin and Turk 
2003]. The scan-converted volume stores 2 key pieces of information: (i) inside/outside markers for fast 
CSG operations and (ii) surface normals and attribute informa­tion at the edges of voxels that intersect 
the surface. Storing this data intrinsically in the volume aids faithful mesh reconstruction of the input 
geometry, while preserving sharp features and attribute discontinuities, such as texture seams. The high 
.delity between the pre-fractured and post-fractured geometry allows fracturing com­plex geometry with 
little to no post-processing, see Figure 1. Artistic Driven Fracture Once we have scan-converted the 
origi­nal geometry, we use the resulting volume to generate disjoint frag­ments. The user has two options 
for controlling the creation of these fragments. The .rst, gives the user .ne grained control over the 
fracture locations via an intuitive 3D painting interface; the sec­ond creates a multitude of fragments 
by automatically generating Voronoi cells in R3, useful for large-scale destruction. Once cre­ated, boolean 
intersections are carried out between these fragments and the scan-converted volumetric object. The resulting 
fractured pieces are then reconstructed as polygonal meshes, used for anima­tion/simulation and rendering. 
*akash.garg@dreamworks.com kyle.maxwell@pdi.dreamworks.com Seamless Mesh Generation Once the volume has 
been frag­mented using boolean intersections, each individual fracture piece is output explicitly as 
a mesh. To avoid rendering artifacts, it s important that no seams be present when neighboring fragments 
adjoin. Traditional tessellation algorithms like the one used in [Museth and Clive 2008] leave small 
gaps between adjacent frac­ture pieces. To this end, we use the surface normals stored during scan-conversion 
and augment Ju et. al s Dual Contouring method [Ju et al. 2002] to tessellate individual fragments. Our 
dual contour­ing tessellator ensures not only that sharp features are preserved but also that no gaps 
are present between adjoining fracture pieces (see Figure 2). In addition to preserving geometric seams 
between ad- Figure 2: Left: each fractured piece colored differently. Right: the fractured pieces with 
textures applied. Note that both textures and geometry are continuous across fracture boundaries. jacent 
fragments, attribute values also need to vary smoothly across fragment boundaries. Typically a post-process 
such as closest point projection is used, which although ef.cient, is problematic for high curvature 
geometry. To share attribute values across disjoint frag­ments, we introduce ghost-faces, faces that 
are never displayed and are only used as a means to transfer attribute information. Consider the following 
scenario on the left. The red vertex is shared among two different fractured pieces. Consistent interpolation 
of attributes can only be achieved if data from the adjacent faces in the ad­ joining fracture piece 
is present. To this end, The black face con­sists of the 3 blue ghost-faces such that the vertex s attribute 
value can be interpolated accurately. References JU, T., LOSASSO, F., SCHAEFER, S., AND WARREN, J. 2002. 
Dual contouring of hermite data. In SIGGRAPH 02: Proceedings of the 29th annual conference on Computer 
graphics and interactive techniques, ACM, New York, NY, USA, 339 346. MUSETH, K., AND CLIVE, M. 2008. 
Cracktastic: fast 3d fragmentation in the mummy: Tomb of the dragon emperor . In SIGGRAPH 08: ACM SIGGRAPH 
2008 talks, ACM, New York, NY, USA, 1 1. NOORUDDIN, F. S., AND TURK, G. 2003. Simpli.cation and repair 
of polygo­nal models using volumetric techniques. IEEE Transactions on Visualization and Computer Graphics 
9, 2, 191 205. Copyright is held by the author / owner(s). SIGGRAPH 2010, Los Angeles, California, July 
25 29, 2010. ISBN 978-1-4503-0210-4/10/0007  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2010</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1837057</article_id>
		<sort_key>310</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>24</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[High-detailed fluid simulations on the GPU]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1837026.1837057</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1837057</url>
		<abstract>
			<par><![CDATA[<p>During the last few years, Fido has seen an increasing demand for highly detailed water effects in production. Fluid simulations, aside from being complex and hard to choreograph, put high demands on scalability and simulation speed. As of today, commercial software is slow and does not allow the artist to get quick feedback on simulation results. Following the increasing use of general purpose GPU programming within the special effects industry, Fido wanted to enable artists to create highly detailed and realistic water effects, while maintaining production effectivity and avoiding investments of large server clusters.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>J.2</cat_node>
				<descriptor>Physics</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.6.8</cat_node>
				<descriptor>Animation</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.1</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.6.5</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010432.10010441</concept_id>
				<concept_desc>CCS->Applied computing->Physical sciences and engineering->Physics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341.10010342</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Model development and analysis</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341.10010349.10011310</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Simulation types and techniques->Simulation by animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2264853</person_id>
				<author_profile_id><![CDATA[81466640894]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Mattias]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lagergren]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Fido Film]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264854</person_id>
				<author_profile_id><![CDATA[81466647680]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Fredrik]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lims&#228;ter]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Fido Film]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264855</person_id>
				<author_profile_id><![CDATA[81466648140]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Bj&#246;rn]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Rydahl]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Fido Film]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>846298</ref_obj_id>
				<ref_obj_pid>846276</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[M&#252;ller, M., Charypar, D., and Gross, M 2003. "<i>Particle-based fluid simulation for interactive applications</i>", In SCA 03: Proceedings of the 2003 ACM SIG- GRAPH/Eurographics symposium on Computer animation, pages 154159. Eurographics Association, Aire-la-Ville, Switzerland, Switzerland, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1272719</ref_obj_id>
				<ref_obj_pid>1272690</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Becker, M. and Teschner, M. 2007. "<i>Weakly compressible SPH for free surface flows</i>.", SCA 07: Proceedings of the 2007 ACM SIGGRAPH/Eurographics symposium on Computer animation, pages 209217. 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1587667</ref_obj_id>
				<ref_obj_pid>1586640</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Satish, N., Harris, M., and Garland, M. 2009. "<i>Designing Efficient Sorting Algorithms for Manycore GPUs</i>", In Proceedings of IEEE International Parallel &amp; Distributed Processing Symposium 2009 (IPDPS 2009).]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 High-detailed .uid simulations on the GPU Mattias Lagergren* Fredrik Lims¨Bj¨ ater orn Rydahl Fido 
Film Fido Film Fido Film Introduction During the last few years, Fido has seen an increasing demand for 
highly detailed water effects in production. Fluid simulations, aside from being complex and hard to 
choreograph, put high demands on scalability and simulation speed. As of today, commercial software is 
slow and does not allow the artist to get quick feedback on sim­ulation results. Following the increasing 
use of general purpose GPU programming within the special effects industry, Fido wanted to enable artists 
to create highly detailed and realistic water effects, while maintaining production effectivity and avoiding 
investments of large server clusters. This talk will describe the method of simulation and why it is 
suit­able for high-detailed .uid simulations. We will discuss how to make it run simultaneously on modern 
graphics hardware and the major impact this kind of system has on the way artists work with simulated 
water effects. Simulating .ne-detail water effects In order to perform physically based simulations of 
complex, high­detail water effects, we have developed a toolkit based on the La­grangian method of smoothed 
particle hydrodynamics (SPH). SPH is commonly used for .uid simulations and allows for .ne detail ef­fects, 
which make it particularly suited for splashes and other free surface phenomena. When using SPH, the 
.uid body is represented by a cloud of particles. Through the use of smooth sampling ker­nels, the particles 
own arbitrary properties (such as internal forces) can be calculated. SPH was .rst introduced as a viable 
.uid sim­ulation method for the graphics community by [1], and further en­hanced by [2] with the weakly 
compressible SPH. GPU The data parallelism inherent to the SPH method makes it espe­cially suited to 
be solved on multi-core systems such as modern GPUs. To fully exploit the computational power of the 
GPU we parallelized the entire .uid solver, encompassing collisions, volu­metric forces and sinks. Being 
an interpolation method which relies heavily on identifying the closed neighboring particles, advanced 
data structures and search algorithms are required. This is achieved by hashing the particles to a grid, 
sorting them by their hash and then using the hash to .nd particles in neighboring cells. To per­form 
this sorting we utilize the radix sorting method as developed by [3]. Collisions and volumetric force 
.elds utilize cached texture memory, making these features computationaly cheap. Directability Moving 
computations from the CPU to the GPU gives an enormous increase in speed and a tremendous boost in terms 
of artist interac­tion. By gaining the ability to simulate millions of particles in just a few seconds 
per frame we allow artists to get very quick feedback on what they are doing. The reduced time spent 
in simulations en­ables them to have quick turnarounds in order to get the desired result in a faster 
time and at a higher quality. This way of working *e-mail: mattias.lagergren@.do.se e-mail: fredrik.limsater@.do.se 
e-mail: bjorn.rydahl@.do.se with .uid simulations is something we have found virtually impos­sible with 
commercial software, which takes many hours or even days to complete large simulations. Particles Houdini 
Our system Speedup 50.000 30.6s 0.44s 70x 125.000 113.6s 1.1s 103x 500.000 389.4s 3.0s 130x Table 1. 
Our system compared to the built-in Houdini solver, messured in seconds per simulated frame. Spec. Dual 
Quad Core Xeon 5450, Geforce GTX 470, 8gb of RAM. We have integrated our simulation software into Houdini, 
making for a very powerful combination. Artists can easily use all the built in Houdini tools to specify 
particle emitters, collision objects, sinks and volumetric force .elds, which are then solved by the 
GPU. The resulting particle simulation can then be scrubbed through or con­verted to a .ipbook to get 
realtime feedback. The individual output particles can also carry a wide range of metadata describing 
for in­stance density, curvature and collision data. Besides being fast and enabling us to increase the 
detail of our simu­lations, it remains extremely cost effective. Our simulation software will run on 
any readily available commercial CUDA-enabled graph­ics hardware, which allows us to be very .exible 
when undertaking projects involving water effects. Artists computers can easily be upgraded, avoiding 
heavy investments of large server clusters. For Fido, a company working mostly with TVCs, this is an 
enormous advantage when it comes to the use of simulated .uid effects. Figure 1. Large water splash, 
SB12 commercial. References [1] M¨uller, M., Charypar, D., and Gross, M 2003. Particle-based .uid simulation 
for interactive applications , In SCA 03: Pro­ceedings of the 2003 ACM SIG-GRAPH/Eurographics sympo­sium 
on Computer animation, pages 154159. Eurographics As­sociation, Aire-la-Ville, Switzerland, Switzerland, 
2003. [2] Becker, M. and Teschner, M. 2007. Weakly compressible SPH for free surface .ows. , SCA 07: 
Proceedings of the 2007 ACM SIGGRAPH/Eurographics symposium on Computer animation, pages 209217. 2007. 
[3] Satish, N., Harris, M., and Garland, M. 2009. Designing Ef.­cient Sorting Algorithms for Manycore 
GPUs , In Proceedings of IEEE International Parallel &#38; Distributed Processing Sympo­sium 2009 (IPDPS 
2009). Copyright is held by the author / owner(s). SIGGRAPH 2010, Los Angeles, California, July 25 29, 
2010. ISBN 978-1-4503-0210-4/10/0007  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2010</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	</section>
	<section>
		<section_id>1837058</section_id>
		<sort_key>320</sort_key>
		<section_seq_no>8</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Blowing $h!t Up]]></section_title>
		<section_page_from>25</section_page_from>
	<article_rec>
		<article_id>1837059</article_id>
		<sort_key>330</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>25</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Destroying LA for "2012"]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1837026.1837059</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1837059</url>
		<abstract>
			<par><![CDATA[<p>For the movie "2012", Digital Domain was tasked with destroying its home city of Los Angeles... digitally. The destruction effects would be seen up close, and there had to be a lot of it. The solution involved turning to geometric fracturing tools, and rigid body dynamics (RBD) simulations. However, this sequence was going to require two orders of magnitude more detail than we had accomplished before.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.6.8</cat_node>
				<descriptor>Animation</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.6.5</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010341.10010342</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Model development and analysis</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061.10010063</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures->Computational geometry</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341.10010349.10011310</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Simulation types and techniques->Simulation by animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2264856</person_id>
				<author_profile_id><![CDATA[81335500074]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Nafees]]></first_name>
				<middle_name><![CDATA[Bin]]></middle_name>
				<last_name><![CDATA[Zafar]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Digital Domain]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264858</person_id>
				<author_profile_id><![CDATA[81100406835]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[David]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Stephens]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Digital Domain]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264859</person_id>
				<author_profile_id><![CDATA[81408598310]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[M&#229;rten]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Larsson]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Digital Domain]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264860</person_id>
				<author_profile_id><![CDATA[81320494322]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Ryo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sakaguchi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Digital Domain]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264861</person_id>
				<author_profile_id><![CDATA[81421598300]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Michael]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Clive]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Digital Domain]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264862</person_id>
				<author_profile_id><![CDATA[81466646598]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Ramprasad]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sampath]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Digital Domain]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264863</person_id>
				<author_profile_id><![CDATA[81324492244]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>7</seq_no>
				<first_name><![CDATA[Ken]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Museth]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Digital Domain]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264864</person_id>
				<author_profile_id><![CDATA[81466648121]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>8</seq_no>
				<first_name><![CDATA[Dennis]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Blakey]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Digital Domain]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264865</person_id>
				<author_profile_id><![CDATA[81466647368]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>9</seq_no>
				<first_name><![CDATA[Brian]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gazdik]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Digital Domain]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264857</person_id>
				<author_profile_id><![CDATA[81466646906]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>10</seq_no>
				<first_name><![CDATA[Robby]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Thomas]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Digital Domain]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Destroying LA for 2012 Nafees Bin Zafar Ramprasad Sampath David Stephens Ken Museth M°arten Larsson 
Dennis Blakey Ryo Sakaguchi Brian Gazdik Michael Clive Robby Thomas Digital Domain  For the movie 2012 
, Digital Domain was tasked with destroying its home city of Los Angeles... digitally. The destruction 
effects would be seen up close, and there had to be a lot of it. The solu­tion involved turning to geometric 
fracturing tools, and rigid body dynamics (RBD) simulations. However, this sequence was going to require 
two orders of magnitude more detail than we had accom­plished before. Destruction Toolset Organic fracture 
shapes such as the rock formations in the .ssure wall could be created using high resolution level set 
techniques (CrackTastic). However when fracturing buildings this approach tends to produce too many polygons 
in order to maintain sharp corners and seamless fracture lines. Hence we also developed a polygon-based 
fracture tool dubbed PolyBuster. A user controlled distribution of points de.ned 3D voronoi cells which 
served as cut­ting planes. The input geometry is then iteratively cut, and the newly opened faces are 
capped. Various rendertime displacements were employed to introduce additional geometric details on the 
ex­posed faces. Managing upwards of 300,000 RBD objects stresses the limits of RBD solvers and animation 
systems. The available third party solutions would not meet our requirements. Several physics en­gine 
SDKs used in video games .t our performance and scalabil­ity needs. We chose to use the Bullet Physics 
engine because it was open source and had a very active user community. Our RBD toolset, called Drop, 
was implemented as a set of Surface OPerator (SOP) plugins to the Houdini animation software. All the 
objects in a simulation are input as triangle meshes, and given a unique ID. Instead of introducing a 
custom data type to rep­resent an RBD object, we generate a single point for each object with attributes 
for RBD control parameters such as mass and veloc­ity. This allows artists to manipulate objects and 
simulation param­eters using built in Houdini operators, and avoids the overhead of processing all the 
geometry for that object. The object geometry is fed directly into the .nal solver operation. The solver 
node outputs control points containing the object transforms and body state. The output transforms are 
then applied back to the input geometry or to high resolution render geometry. Airport and Fissure Sequence 
The main simulation components in the Airport and .ssure se­quence were the ground, roads, buildings, 
and street level elements like trees and cars. These vastly different types of objects had to behave 
in an art directed way. In controlling how an object moves we were essentially trying to represent elastic 
deformation behav­ior with rigid bodies. This kind of behavior can be approximated by the use of constraints. 
Bullet provided many different types of constraints such as point-to-point, and hinge types. Each constraint 
was represented in our system as a point with control attributes and IDs of the objects they constrained. 
By controlling placement of constraints and setting their intrinsic parameters, we could create many 
different material behaviors. For example, rigging a building involved fracturing the building geometry, 
then seeding point-to­point constraints between the pieces based on nearest neighbors. Additional constraints 
could be placed to control zones of collapse. A .nal set of constraints would bind the bottom of the 
object to the ground. Adding a little bit of noise to the constraint parameters pro­vided very natural 
spatial variations for the destruction animation. We had to enhance the constraint system in Bullet by 
adding dy­namic constraint parameter updates, including breaking constraints based on displacement or 
constraint restoration impulse limits. Rigging vehicles and trees involved a speci.c geometric arrange­ment 
of constraints. We built several different vehicle rigs to rep­resent different types of cars and buses. 
The car simulation results were then applied to the speci.c car models. Results of the tree simulations 
were applied to the tree geometry with a lattice based deformer. While every building had a unique simulation, 
the .ssure wall sec­tions were more generic and less visually recognizable. Therefore we created a reusable 
library of simulations. Each library element was a long simulation (800+ frames) of a square patch of 
ground collapsing. The artists would lay down curves to indicate the .s­sure path in their shot, with 
each curve segment indicating which library element to load at that location. Collapsing Downtown LA 
The buildings in downtown LA had to collapse in a very speci.c way. Some of this control could be achieved 
with the constraints, but in certain cases we had to incorporate keyframed animation. The transition 
from keyframed animation to simulation was han­dled dynamically per object by thresholding collision 
impact mag­nitudes. Though we had to represent a wide range of object sizes, the effect of small objects 
on a large one is negligible. In order to optimize simulation runtimes we used a layering approach. The 
output of the large object simulations would be reimported into a simulation for smaller objects. We 
also added information about the maximum and total collision based impulses on the simulation out­put 
points, and generated information about constraint states. This data was then used to determine location 
and timing of the smaller objects. For example, broken constraints from the building simula­tions were 
used to trigger particle simulations to represent broken shards of glass. Each building required simulating 
50k to 100k dy­namic objects, and each frame required less than 2 minutes. Conclusions Our goal was 
to create an ef.cient set of tools, and a .exible artist work.ow to address 90% of our needs. Utilizing 
a robust and opti­mized RBD system and reducing execution of the operator network in Houdini gave us 
very fast simulations. The point based data rep­resentation reduced data processing in Houdini and provided 
inter­active work.ow for the artists. It also allowed them to build a lot of custom art direction tools. 
Methods of optimizing geometry for the simulation will lead to faster artist throughput. Copyright is 
held by the author / owner(s). SIGGRAPH 2010, Los Angeles, California, July 25 29, 2010. ISBN 978-1-4503-0210-4/10/0007 
 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2010</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	</section>
	<section>
		<section_id>1837060</section_id>
		<sort_key>340</sort_key>
		<section_seq_no>9</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Visualization for Art & Design]]></section_title>
		<section_page_from>26</section_page_from>
	<article_rec>
		<article_id>1837061</article_id>
		<sort_key>350</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>26</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Visualizing a classic CPU in action]]></title>
		<subtitle><![CDATA[the 6502]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1837026.1837061</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1837061</url>
		<abstract>
			<par><![CDATA[<p>We present a real-time interactive simulation and visualization of the classic 6502 microprocessor as it executes instructions and processes data. A polygon model of each physical layer of the 6502 was created and transformed into a network of wires and transistors suitable for our logic simulator. Tools were developed to provide constant visual feedback of the work-in-progress, which yielded a correct model immediately after finishing the vectorization. This avoided the errors and long debug phase encountered in similar projects.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[MOS 6502]]></kw>
			<kw><![CDATA[computer history]]></kw>
			<kw><![CDATA[integrated circuit]]></kw>
			<kw><![CDATA[microprocessor]]></kw>
			<kw><![CDATA[simulation]]></kw>
			<kw><![CDATA[visualization]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.6.8</cat_node>
				<descriptor>Animation</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>C.1.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.6.4</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.6.5</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010341.10010342.10010344</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Model development and analysis->Model verification and validation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341.10010342</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Model development and analysis</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010520.10010521</concept_id>
				<concept_desc>CCS->Computer systems organization->Architectures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341.10010342</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Model development and analysis</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341.10010349.10011310</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Simulation types and techniques->Simulation by animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2264866</person_id>
				<author_profile_id><![CDATA[81466644400]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Greg]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[James]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Industrial Light + Magic]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264867</person_id>
				<author_profile_id><![CDATA[81100001809]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Barry]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Silverman]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Disus Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264868</person_id>
				<author_profile_id><![CDATA[81100001812]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Brian]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Silverman]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Playful Invention Co.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[
McNerney, T., 2006. Intel 4004 project. website www.4004.com.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Visualizing a Classic CPU in Action: The 6502 GregJames * BarrySilverman BrianSilverman IndustrialLight+Magic 
DisusInc. PlayfulInventionCo. Figure 1: A section of the 6502 showing, from left to right, (1) a chip 
die photograph from which a polygon representation was derived, (2) all polygons corresponding to physical 
chip layers (3) polygons reduced to substrate regions split by transistors, where green and red indicate 
substrate connected to ground and supply, and yellow indicates substrate switched between ground and 
supply by transistors. Abstract We present a real-time interactive simulation and visualization of theclassic6502 
microprocessoras itexecutes instructionsandpro­cesses data. A polygon model of each physical layer of 
the 6502 was created andtransformedinto a network of wires and transistors suitable for our logic simulator. 
Tools were developed to provide constant visual feedback of the work-in-progress, which yielded a correct 
model immediately after .nishing the vectorization. This avoided the errors and long debug phase encountered 
in similar projects. Keywords: visualization, microprocessor,MOS6502, simulation, computerhistory, integrated 
circuit 1 Introduction and Motivation Most users have a basic understanding of computer hardware, but 
there is a signi.cant barrier to discovering more about how that hardware operates. Chip features are 
microscopic, embedded in tough opaque material, and easily destroyed by attempts to reveal them.Very 
fewpeoplehaveseena transistor,much lessanetwork of transistors put together to accomplish some goal. 
Even when photographs and diagrams of circuits are available, the static im­ages reveal next to nothing 
about how circuits operate. In creating an interactive visual representation of a physical chip and its 
logic state, we reveal every detail of its operation and can easily explain thefunctionof itsvariousparts. 
Unlike today s microprocessors, the 6502 was laid out by hand on drafting boards. A digital representation 
of the chip was not avail­able. Itsfeatures were not routed or optimized by computer, which makesthemanattractivetargetforstudy.Thoughknowledgeof 
its instructionsetiswidespread,thephysical componentsused toexe­cute thatset werenotknown; componentswhich 
liveon in today s processordesigns.Withour logicsimulationderiveddirectlyfrom physical chip features, 
it is now possible for an educated laymen to study a microprocessor in full detail. Our project enables 
the preservation, study, and appreciation of this classic CPU and the earlyAppleandAtari computers inwhich 
itwasused. * e-mail:gjames@gmail.com email:barry@disus.com email:brians@playfulinvention.com  2 Our 
Approach High resolution photomicrographs of the surface of a 6502 chip were shot and assembled into 
a single image using a custom GPU­accelerated applicationdrivenbyPythonscript.Thechipwas then stripped 
down to its substrate features, re-photographed, and the substrate images were assembled into a second 
full layer aligned with thesurface image.From these twoimages,wederivedpolyg­onal representations of 
each physical layer of the chip, including conductive substrate regions,polysilicongate wires,buried 
contact areas, vias, and the topmost metal layer. Connectivity between these layers is described by simple 
rules which, in turn,allow thecreationof afullchipnetlist andlogicsim­ulation by simple geometric intersection 
of various layers. Unlike previouswork[McNerney2006]inwhichlogicalmodelswerede­rivedfrom tabulated or 
translateddata,prone toerrors,and required protracted and dif.cult debugging, our approach of using nothing 
but an image-based representationproduced anaccuratenetlist im­mediatley after the polygonization was 
complete. We encountered only 8 errors in forming our representation of over 20,000 com­ponents, and 
each of these errors was spotted and corrected as the vectorization proceeded. We believe this approach 
is best suited to thepreservation and study of other early microprocessors. 3 Conclusion Attendees should 
come away with a new, intuitive, visual under­standing of microprocessorsand anappreciationfor thedesignand 
elegance of a chip that played a major role in the home computer revolution. The form and function of 
chip registers, the decoding and execution of program instructions, and the challenges of pro­cessor 
design will be explained on a level that both non-technical users and advanced engineers can appreciate. 
 References MCNERNEY,T.,2006.Intel4004project. website www.4004.com. Copyright is held by the author 
/ owner(s). SIGGRAPH 2010, Los Angeles, California, July 25 29, 2010. ISBN 978-1-4503-0210-4/10/0007 
 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2010</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1837062</article_id>
		<sort_key>360</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>27</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[The universe of fonts, charted by machine]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1837026.1837062</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1837062</url>
		<abstract>
			<par><![CDATA[<p>Today, computer fonts come in hundreds and thousands. How do you find a font that fits for the task at hand? Which general types are available? Which ones are related---or not related at all---to the ones you know well? To address such issues, this work applies strategies from Music Information Retrieval (MIR) to organizing fonts. Finding similar music and graphically laying out a music collection according to the similarity of the tracks are standard tasks in MIR, tackled by automatically extracting meaningful but low-level descriptors from the bare content data---that is: waveforms---and discovering high-level meaning through machine learning.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.7.2</cat_node>
				<descriptor>Format and notation</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>J.5</cat_node>
				<descriptor>Fine arts</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.5.3</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.5.4</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010469.10010471</concept_id>
				<concept_desc>CCS->Applied computing->Arts and humanities->Performing arts</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010469.10010470</concept_id>
				<concept_desc>CCS->Applied computing->Arts and humanities->Fine arts</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010257.10010258.10010260.10003697</concept_id>
				<concept_desc>CCS->Computing methodologies->Machine learning->Learning paradigms->Unsupervised learning->Cluster analysis</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010257</concept_id>
				<concept_desc>CCS->Computing methodologies->Machine learning</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010497.10010510.10010514</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document preparation->Format and notation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2264869</person_id>
				<author_profile_id><![CDATA[81408599742]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[J&#246;rn]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Loviscach]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Fachhochschule Bielefeld (University of Applied Sciences)]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 The Universe of Fonts, Charted by Machine J¨orn Loviscach* Fachhochschule Bielefeld (University of Applied 
Sciences) 1 Introduction Today, computer fonts come in hundreds and thousands. How do you .nd a font 
that .ts for the task at hand? Which general types are available? Which ones are related or not related 
at all to the ones you know well? To address such issues, this work applies strategies from Music Information 
Retrieval (MIR) to organizing fonts. Finding similar music and graphically laying out a music collection 
according to the similarity of the tracks are standard tasks in MIR, tackled by automatically extracting 
meaningful but low­level descriptors from the bare content data that is: waveforms and discovering high-level 
meaning through machine learning. In the same spirit, this work introduces a set of descriptors to be 
extracted from the font .les. These descriptors are used to create a two-dimensional layout of fonts 
according to their similarity, see Figure 1. This readily shows duplicates and clones as well as clus­ters. 
It also enables a serendipitous approach for .nding fonts that are in some surprising respect similar 
to a given font. 2 Font Similarity and Charting The software prototype fetches the outline curves of 
all installed fonts and computes six descriptors for each character. These are averaged according to 
the characters frequency in a given text .le. Apparent Height. The maximum height is divided in 1000 
bins. Each character s area in each height bin is computed to create a height histogram. The apparent 
height is taken to be the 95th per­centile minus the 10th percentile of this histogram, which reliably 
removes descenders and spuriously tall special characters. The ap­parent height is not used directly 
for similarity computations, but helps to normalize other parameters since fonts of nominally equal size 
come at wildly differing apparent sizes. Weight. Each character s area is divided by the square of the 
arc length of its contour. This is independent of the overall size, equals 1/4p for a circle, and tends 
to zero as the shape gets more meager. Roundness. A histogram of the slopes that occur in the outlines 
is computed by stepping along the contours, weighting by the steps arc lengths. A low entropy of this 
histogram means that some direc­tions are strongly favored. Rounded fonts possess a high entropy. Slant. 
To suppress near-horizontal directions, the same histogram is weighted with the sine of the slope angle. 
Then, the mean value of the cosine of the angle is computed, which is zero for upright lines and positive 
for forward-slanted lines. The mean value is con­verted back to an angle by taking the arc cosine. The 
resulting angle characterizes the font outline s mean non-horizontal direction. Curvature. To compute 
a robust measure of the local curvature such as serifs or grunge-style font features, a pair of points 
is swept along all contours. Measured along the curve, these two points have a distance of 0.05 times 
the apparent font height (which is to be computed beforehand, as described above). Then the triangle 
is considered that is formed by this pair of points and by the point on the contour that sits at their 
middle in terms of arc length. The ratio of the height of the middle point in this triangle to the apparent 
font *e-mail: joern.loviscach@fh-bielefeld.de Figure 1: 2000 font styles laid out as a landscape. Note 
that the computation of similarity takes a complete text into account, not only the single character 
displayed here. height characterizes how far the contour deviates from a line. The characteristic value 
used is the mean of the square of this quantity. Diagonalness. Black letter fonts can be identi.ed by 
diagonal cor­ners that point up or down. For this, the square height mentioned in the former descriptor 
is weighted depending on the slope of the line connecting the original pair of points. To cope with their 
wide variation in range, all values for each sin­gle descriptor are sorted to form a rank order. The 
rank numbers are used instead of the descriptors actual values. The resulting 5D vectors are used to 
train a toroidal or non-toroidal self-organizing map sized 50 × 50. The distance in the 5D space is taken 
to be Euclidean, with the exception of the visually preponderant charac­ v teristic weight appearing 
with a factor of 2. Each font ends up at one speci.c spot on the map. To avoid overlap, the fonts posi­tions 
are spread out using incremental motions. In the toroidal case, the map is shifted vertically and horizontally 
in a toroidal fashion to minimize the number of fonts close to the boundary. 3 Outlook The 2D arrangement 
could for be used, for instance, as a replace­ment of the standard font selection dialog. Collections 
of 10,000 or more fonts could be handled through a zoomable interface that when zooming out replaces 
each cluster of fonts by a single, pro­totypical font. There is a vast set of further options from MIR 
to explore: The relative weights of the characteristics could be learned from examples provided by the 
user. Furthermore, analogously to MIR automatically building playlists, one could recommend type­faces 
that go well with another. Copyright is held by the author / owner(s). SIGGRAPH 2010, Los Angeles, California, 
July 25 29, 2010. ISBN 978-1-4503-0210-4/10/0007  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2010</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1837063</article_id>
		<sort_key>370</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>28</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Synesthetic color scheme in Fantasia]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1837026.1837063</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1837063</url>
		<abstract>
			<par><![CDATA[<p>Synesthesia is a neurological condition in which stimulation of one sensory or cognitive pathway includes automatic and involuntary responses in a second sensory or cognitive pathway. People who report such experiences are known as "synesthetes". A subset of synesthetes, sound-color synesthetes "see" colors and simple animated shapes in association with music and other sounds. We can find a color-sound synesthete out of two to three thousands. It has been suggested [Stein et al. 1993] that "..every infant is synesthetic, although most gradually grow out of it" p 12 (Stein and Meredith 1993). We performed a set of test based on a standard battery of tests [Eagleman et al. 2007] that include: (1) a pitch test, (2) a chord test, and (3) a musical key test. The test subjects select colors from a color map after hearing sounds or short music generated by various pitches, chords, and keys. We judge the level of the synesthesia based on the battery score formula. Forty-three synesthetes took the tests and we selected only those with the high scores. We were surprised at how the test subjects associated colors that were strongly correlated with the keys, especially with major keys. In both the pitch and chords tests, they choose almost the same colors as in the key test. The key has a tonic and a tonic chord. In both the pitch and chord tests, the chosen color for a certain pitch and chord is almost the same as the color chosen for this key in the key test. One of test subjects results are shown in Fig. 1. The chosen tonics, tonic chords, and their keys, and their associated colors are shown in Fig. 1. In the figure, only the upper half of the circle of fifths is shown. Each column shows the colors associated with the tonic in the upper, the tonic chord in the middle, and the key in the bottom. This strongly suggests that the synesthetes perceive the sound as a structure characterized by the "key". At the same time, the colors they picked are strongly clustered about several colors for each key. We use the k-means method [Hartigan and Wong 1979]to cluster the selected colors, and obtained representative colors. Then, we list the three to five colors with highest frequencies for tonic, tonic chord, and key from right to left in Fig. 2. The representative colors for the pitch, chord, and key are very much correlated and similar as shown in Fig. 2 (left, only for F major). We compared these representative colors with these in "Fantasia". Fantasia is a 1940 American animated film produced by Walt Disney. It is the third feature-length film produced by the Walt Disney Company. It is well known various synesthetic effects appear in this film although there is no evidence that the film was made by synesthetic artists. In this talk, we discuss the synesthetic colors are associated with the musical keys and how these colors create special effects. For example, in Fig. 2, the synesthetic "fade-in" effect is used in Chapter 6, the pastoral symphony in Fantasia. We also found a synesthetic "cross-fading" is used before the changing of key in this chapter.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Color, shading, shadowing, and texture</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>H.1.2</cat_node>
				<descriptor>Human information processing</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010216.10010217</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Philosophical/theoretical foundations of artificial intelligence->Cognitive science</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010384</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Texturing</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2264870</person_id>
				<author_profile_id><![CDATA[81539920756]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[D.]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Cai]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264871</person_id>
				<author_profile_id><![CDATA[81466641913]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[S.]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Goto]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264872</person_id>
				<author_profile_id><![CDATA[81466641758]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[T.]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Shinohara]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264873</person_id>
				<author_profile_id><![CDATA[81466645750]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[N.]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nagata]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264874</person_id>
				<author_profile_id><![CDATA[81100192864]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[J.]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kurumisawa]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264875</person_id>
				<author_profile_id><![CDATA[81466643890]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[A.]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Fukumoto]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[
Eagleman, D., Kagan, A., Nelson, S., Sagaram, D. and Sarma, A. 2007. A standardized test battery for the study of synesthesia. <i>Journal of neuroscience methods 159</i>, 139--145.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[
Hartigan, J. and Wong, M. 1979. A k-means clustering algorithm. <i>JR Stat. Soc., Ser. C 28</i>, 100--108.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[
Stein, B., Meredith, M. and Wolf, S. 1993. The merging of the senses.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Synesthetic Color Scheme in Fantasia D. Cai, S. Goto, T. Shinohara, N. Nagata, J. Kurumisawa, and A. 
Fukumoto, Synesthesia is a neurological condition in which stimulation of one sensory or cognitive pathway 
includes automatic and involuntary responses in a second sensory or cognitive pathway. People who report 
such experiences are known as synesthetes . A subset of synesthetes, sound-color synesthetes see colors 
and simple animated shapes in association with music and other sounds. We can find a color-sound synesthete 
out of two to three thousands. It has been suggested [Stein et al. 1993] that "..every infant is synesthetic, 
although most gradually grow out of it" p 12 (Stein and Meredith 1993). We performed a set of test based 
on a standard battery of tests [Eagleman et al. 2007] that include:(1) a pitch test, (2) a chord test, 
and (3) a musical key test. The test subjects select colors from a color map after hearing sounds or 
short music generated by various pitches, chords, and keys. We judge the level of the synesthesia based 
on the battery score formula. Forty-three synesthetes took the tests and we selected only those with 
the high scores. We were surprised at how the test subjects associated colors that were strongly correlated 
with the keys, especially with major keys. In both the pitch and chords tests, they choose almost the 
same colors as in the key test. The key has a tonic and a tonic chord. In both the pitch and chord tests, 
the chosen color for a certain pitch and chord is almost the same as the color chosen for this key in 
the key test. One of test subjects results are shown in Fig. 1. The chosen tonics, tonic chords, and 
their keys, and their associated colors are shown in Fig. 1. In the figure, only the upper half of the 
circle of fifths is shown. Each column shows the colors associated with the tonic in the upper, the tonic 
chord in the middle, and the key in the bottom. This strongly suggests that the synesthetes perceive 
the sound as a structure characterized by the key . At the same time, the colors they picked are strongly 
clustered about several colors for each key. We use the k-means method [Hartigan and Wong 1979]to cluster 
the selected colors, and obtained representative colors. Then, we list the three to five colors with 
highest frequencies for tonic, tonic chord, and key from right to left in Fig. 2. The representative 
colors for the pitch, chord, and key are very much correlated and similar as shown in Fig. 2 (left, only 
for F major). We compared these representative colors with these in Fantasia . Fantasia is a 1940 American 
animated film produced by Walt Disney. It is the third feature-length film produced by the Walt Disney 
Company. It is well known various synesthetic effects appear in this film although there is no evidence 
that the film was made by synesthetic artists. In this talk, we discuss the synesthetic colors are associated 
with the musical keys and how these colors create special effects. For example, in Fig. 2, the synesthetic 
fade-in effect is used in Chapter 6, the pastoral symphony in Fantasia. We also found a synesthetic cross-fading 
is used before the changing of key in this chapter. Figure 1: One of the test results indicated the 
strong correlation between the tonics, tonic Figure 2: Synesthetic fade-in effect in Chapter 6, the pastoral 
symphony in chords, and the associated keys. Only the Fantasia. The arrows maps the k-means clustered 
representative colors to upper of circle of fifths is shown. Fantasia pictures. In the column, three 
representative colors with highestfrequencies are listed. EAGLEMAN, D., KAGAN, A., NELSON, S., SAGARAM, 
D. AND SARMA, A. 2007. A standardized test battery for the study of synesthesia. Journal of neuroscience 
methods 159, 139-145. HARTIGAN, J. AND WONG, M. 1979. A k-means clustering algorithm. JR Stat. Soc., 
Ser. C 28, 100-108. STEIN, B., MEREDITH, M. AND WOLF, S. 1993. The merging of the senses. Copyright 
is held by the author / owner(s). SIGGRAPH 2010, Los Angeles, California, July 25 29, 2010. ISBN 978-1-4503-0210-4/10/0007 
 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2010</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1837064</article_id>
		<sort_key>380</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>29</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[Splash]]></title>
		<subtitle><![CDATA[real or virtual?]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1837026.1837064</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1837064</url>
		<abstract>
			<par><![CDATA[<p>In this presentation two of the four creatives who comprise PROGRAM COLLECTIVE (Mona Kim, Todd Palmer, Simon Taylor, Olga Subir&#243;s) discuss how the multi-media installations of SPLASH and WATER FOR LIFE uniquely challenged conceptual and practical "gaps" between the computer-digital realm and experiences in real time and space.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.2.10</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.4.8</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010225.10010227</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision tasks->Scene understanding</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010225.10010227</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision tasks->Scene understanding</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2264876</person_id>
				<author_profile_id><![CDATA[81466648154]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Olga]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Subir&#243;s]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264877</person_id>
				<author_profile_id><![CDATA[81466640983]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Todd]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Palmer]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 SPLASH: REAL OR VIRTUAL? Olga Subirós, Todd Palmer Creative partners: PROGRAM COLLECTIVE (Barcelona/Chicago/London/Paris) 
 Figure 1. SPLASH installation view at World Expo 2008. In this presentation two of the four creatives 
who comprisePROGRAM COLLECTIVE (Mona Kim, Todd Palmer, SimonTaylor, Olga Subirós) discuss how the multi-media 
installations of SPLASH and WATER FOR LIFE uniquely challenged conceptual and practical gaps between 
the computer-digital realm and experiences in real time and space. The first part of the talk (roughly 
five minutes) addresses thechallenges presented by the client who commissioned the work(the World Expo 
authority in Spain, Expoagua Zaragoza 2008)and by the site for which these works were commissioned. Theproject 
was set within a 20-story atrium at the core of a newskyscraper which was to be an icon of the Expo and 
the region stallest building. The presentation illustrates how the collective arrived at this concept 
of a sculptural rendition of a digitally­defined droplet SPLASH, augmented by an ambient audio-visual 
experience entitled Water for Life. At the conceptual stage, the synthesis of object and a time-based 
experience involving moving images, ambient sound and the movement of the audience through space below, 
around andabove the artwork, addressed the narrative expectations of theExpo event. It was a three-month 
global gathering intended toboth celebrate an essential natural resource and also to focus public attention 
on the importance of sustainability, in a uniqueand inspiring way. The SPLASH concept embraced these 
ideas, while also introducing the possibility of a virtual and real experience. The second part of the 
talk (about ten minutes) looks at the making of SPLASH. Realizing the concept pushed computergraphics 
that simulate and give visual form to complex naturalphenomena (in this case the intricate motion of 
fluids impacted bygravity, wind and other forces) into a real-world setting. The presentation illustrates 
some of the unique challenges that emerged in translating the virtually-derived form simulating a micro-scale 
moment into a 74-foot tall sculptural form made of134 discrete objects suspended from cables in a monumental 
(131-foot tall) public space, experienced in real-time and by an audience in motion. The presentation 
will briefly survey the key tools and collaborative teamwork that were involved --from software and structural 
engineering expertise, to new modes of digitalfabrication. These were conjoined with traditional techniques 
likepolishing and smoothing by the human hand. The presentationwill illustrate the technical innovations 
and creative solutions required to synthesize a variety of software platforms and processes. During the 
presentation s concluding section (approximately fiveminutes) the artists discuss how the concept of 
merging real andvirtual modes engaged the audience s experience of the work in apublic space, as well 
as ways in which the experience resonated invirtual realms including chat forums, video blogs and social 
media. A particular irony is that this representation of an ephemeral moment a SPLASH was intended to 
be a temporary installation for a three month Expo. But the impact was such that on-line groups have 
rallied to preserve or revive theSPLASH as a permanent public artwork. The presentation muses on the 
possibilities of intermingling ofvirtual and real experience as they resonate with the theme of sustainability 
which the commission was intended to represent,and also as a potential avenue for further exploration 
in future artand design projects. Copyright is held by the author / owner(s). SIGGRAPH 2010, Los Angeles, 
California, July 25 29, 2010. ISBN 978-1-4503-0210-4/10/0007  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2010</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	</section>
	<section>
		<section_id>1837065</section_id>
		<sort_key>390</sort_key>
		<section_seq_no>10</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Pipelines and Asset Management]]></section_title>
		<section_page_from>30</section_page_from>
	<article_rec>
		<article_id>1837066</article_id>
		<sort_key>400</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>30</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Managing thousands of assets for the <i>Prince of Persia</i> city of Alamut]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1837026.1837066</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1837066</url>
		<abstract>
			<par><![CDATA[<p>The medieval Persian city of Alamut was constructed with over one hundred thousand assets. From pot to palace, it provided the stunning playground in which an adventurous prince struggled to protect the sands of time.</p> <p>This colossal environment was subject to intense scrutiny as dizzying up-close action seamlessly transitioned to sweeping panoramic cityscape views.</p> <p>Short schedules and the necessity to accommodate dynamic changes at any point in the production, precluded the use of the existing environments pipeline and forced a re-think. In its place, a pipeline enabling highly parallel workflow across multiple departments was created with an artist-friendly toolset.</p> <p>This talk discusses the methodology and challenges involved in creating effective pipelines for the construction of large assets such as the greating city of Alamut.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>K.6.1</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.4</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003456.10003457.10003490.10003491</concept_id>
				<concept_desc>CCS->Social and professional topics->Professional topics->Management of computing and information systems->Project and people management</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2264878</person_id>
				<author_profile_id><![CDATA[81466648550]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Gregory]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Meeres-Young]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MPC]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264879</person_id>
				<author_profile_id><![CDATA[81421597655]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Hannes]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ricklefs]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MPC]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264880</person_id>
				<author_profile_id><![CDATA[81466648579]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Robert]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tovell]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MPC]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1401125</ref_obj_id>
				<ref_obj_pid>1401032</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Butler, G., Langlands, A., and Ricklefs, H. 2008. A pipeline for 800+ shots. In <i>SIGGRAPH '08: ACM SIGGRAPH 2008 talks</i>, ACM, New York, NY, USA, 1--1.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Managing thousands of assets for the Prince of Persia city of Alamut Gregory Meeres-Young* Hannes Ricklefs 
RobertTovell MPC MPC MPC Abstract ThemedievalPersiancityofAlamutwas constructedwithoverone hundred thousand 
assets. From pot to palace, it provided the stun­ning playground in which an adventurous prince struggled 
to pro­tect the sands of time. This colossal environment was subjectto intense scrutiny as dizzy­ing 
up-close action seamlessly transitioned to sweeping panoramic cityscape views. Short schedules and the 
necessity to accommodate dynamic changes at any point in the production, precluded the use of the existing 
environments pipeline and forced a re-think. In its place, a pipeline enabling highly parallel work.ow 
across multiple depart­ments was created with an artist-friendly toolset. This talk discusses the methodology 
and challenges involved in cre­ating effective pipelines for the construction of large assets such as 
the greating city of Alamut. Figure 1: the city of Alamut. c&#38;#169;2010 Walt Disney Pictures. 1 The 
path to parallel success The notion of multi-discipline parallel development was .rst effec­tively realised 
at MPC during the production of Prince Caspian: The Chronicles of Narnia [Butler et al. 2008]. This predominantly 
character-driven pipeline armed us with some robust production techniques and proprietary software. Packaging, 
Streams and Ap­provals were someofthekeypipeline components re.nedto tackle the very different problem 
of one huge environment asset. MPC s asset management system, Packaging, enables the de.ni­tion of relationships 
between different assets whilst simultaneously keepingarecordofanychangestotheseduringthe productiontime 
line. The environment package de.nitions used on previous .lms could only describe .at groupings of model 
instances. This proved insuf.cient as it could not intuitively represent the hierarchical na­ture of 
a city. ModelHierarchyPkgs were created to represent a hi­erarchyof multiple instances of model packages. 
This newpackage de.nition enabled propstobe grouped withbuildings,buildingsto be grouped into Streets, 
and streets to be grouped into districts. Distinct departments such as modelling/texturing, layout, and 
lighting/look-dev worked on different package types that existed *e-mail: greg-my@moving-picture.com 
e-mail: hannes-r@moving-picture.com e-mail: robert-t@moving-picture.com in independent streams. Isolating 
these department-speci.c assets proved an essential component of the parallel work.ow paradigm. It provided 
a safe sandbox and diminished the risk of propagating broken assets between departments. Syncing of streams 
propagates the work done between the isolated departments.Package differences are compared and resolved 
where appropriate. Not all changes get synced, an approval system acts asagatekeeper between department 
streamsgiving artists control over which assets they want to be made available to other depart­ments. 
 2 Artists tools for asset creation Effectiveand transparent artist creation/modi.cationofthecitywas 
provided by a set of packaging tools integrated into Maya. Pack­ageEditor and TownPlanner were developed 
speci.cally for the rapid layout, creation, extraction and exploration of ModelHierar­chyPkgs. The layout 
department used TownPlanner to block out large sections of the city procedurally, sourcing model packages 
createdby the modelling/texturing department. Hand layout re.ne­ments were performed with PackageEditor. 
Both tools generated ModelHierarchyPkgs that were pickedupby the lighting and look­devdepartments to 
be lit and shaded. An OpenGL viewof packages representing large sections of the city was exposed in Maya 
s view port trough a single node. This provided a number of advantages, such as light scene .les, non 
destructive updating of scene assets and intuitive city interaction. As changes were frequent and time­pressured, 
lighters used PackageEditor to extract sections of the city letting them perform layout changes catering 
to shot-speci.c conditions such as modifying buildings, level of detail and posi­tions. A migration from 
implementing pipeline tools in MEL and Lua to Python provedfar moreversatile. Python operated nicely 
asa scripting glue between Maya and MPC s C++ 3D description li­brary Muggins, and the use ofPyQt for 
user interface design uni.ed tool aesthetics for operation both in and outside of Maya. 3 Summary The 
pipeline and tools developed for Prince of Persia: The Sands of Time empowered layout artists to become 
architects of great cityscapes whilst modellers created and re.ned the intricatebuild­ings that lined 
the streets. Meanwhile, lighters were able to update their scenes of entire districts with the latest 
changes whilst still maintaining the control to manipulate the smallest pot that adorned the heaving 
market stalls.  References BUTLER, G., LANGLANDS, A., AND RICKLEFS, H. 2008. A pipeline for 800+ shots. 
In SIGGRAPH 08: ACM SIGGRAPH 2008 talks,ACM,NewYork,NY, USA, 1 1. Copyright is held by the author / owner(s). 
SIGGRAPH 2010, Los Angeles, California, July 25 29, 2010. ISBN 978-1-4503-0210-4/10/0007  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2010</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1837067</article_id>
		<sort_key>410</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>31</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Prep and landing set'm and forget'm]]></title>
		<subtitle><![CDATA[a motion graphics pipeline for effects]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1837026.1837067</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1837067</url>
		<abstract>
			<par><![CDATA[<p>For the creation of brilliant light displays, flickering control tower buttons and vibrant computer monitors, the goal was to build a motion graphics pipeline capable of running nearly unattended. Effects work would have to feed into the production front-end and allow for easy artistic intervention downstream. Once in place, the design allowed our department to work more efficiently to save time for other effects related challenges.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.4</cat_node>
				<descriptor>Software support</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>K.6.1</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003456.10003457.10003490.10003491</concept_id>
				<concept_desc>CCS->Social and professional topics->Professional topics->Management of computing and information systems->Project and people management</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10011007.10011074</concept_id>
				<concept_desc>CCS->Software and its engineering->Software creation and management</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2264881</person_id>
				<author_profile_id><![CDATA[81320488843]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Ian]]></first_name>
				<middle_name><![CDATA[J.]]></middle_name>
				<last_name><![CDATA[Coony]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Walt Disney Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264882</person_id>
				<author_profile_id><![CDATA[81466644388]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Brian]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Silva]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Walt Disney Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264883</person_id>
				<author_profile_id><![CDATA[81537680456]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Bruce]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wright]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Walt Disney Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264884</person_id>
				<author_profile_id><![CDATA[81466644586]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Andrew]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kinney]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Walt Disney Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Prep and Landing Set m and Forget m: A Motion Graphics Pipeline for Effects  figure 1: (a) 2D Loop ( b) Gingerbread Man (c) Programmed Light Displays (d) Button Animation (e) Monitor 
Graphics 1. Introduction For the creation of brilliant light displays, flickering control tower buttons 
and vibrant computer monitors, the goal was to build a motion graphics pipeline capable of running nearly 
unattended. Effects work would have to feed into the production front-end and allow for easy artistic 
intervention downstream. Once in place, the design allowed our department to work more efficiently to 
save time for other effects related challenges. 2. Front loading Effects resources were devoted early 
on to help determine a way to deal with the volume of motion graphic shots. Graphics for monitors 1(e), 
the GBM 1(b) and other gadgets were animated and composited as sequences of 2D loops 1(a). The aim was 
to achieve front-end director approval of all loops prior to being shown rendered in a shot. Early approval 
and front loading our efforts allowed blocks of shots to bypass the effects department altogether and 
head straight into lighting.  3. Event Handling Production efficiencies were gained by carefully considering 
each department s needs for an element and scripting in automated event handling. For example, hardware 
accelerated textures may be fantastic to visualize and adjust in one department, while causing untold 
headaches in others. Sometimes just viewing them in a scene file may slow animation scrubbing to a crawl. 
A simple solution model elements were published with embedded scripts that could query the current department 
and automatically turn off hardware textured graphics for that element if needed. Simple utility functions 
were added as embedded script nodes should animators want to view them in scene for reference. Attributes 
were also procedurally added to each model and acted as pointers to the graphic loops on disk. These 
could easily be updated in the pipeline external to the 3D package using simple text meta-data. Other 
attributes added allowed control of loop start and end frame, time offset and scale, reverse loop, bounce 
and loop rock modes. Hardware and software shader expressions would then determine assignments based 
on these preset attributes. Since two identical sets of these loops were composited (Tiff and Ptex**) 
as part of our front loading process, hardware shaders in Maya would use the tiffs as cached texture 
maps for interactive feedback. 1(b,d). Once the shot reached lighting, the same attributes would allow 
Renderman to software render using the corresponding Ptex file in the shader 1(b). A Button Script interface 
was created to allow procedural animation of the control room button glows and could also be used to 
choreograph strands of lights. Simply pick each light in their firing order and run the script. Expressions 
drove stored attributes for each light geometry, defining intensity, phase, offset, and fcurve type over 
time, (sawtooth, static, binary, Copyright is held by the author / owner(s). SIGGRAPH 2010, Los Angeles, 
California, July 25 29, 2010. ISBN 978-1-4503-0210-4/10/0007 sine, weighted tangents, etc.). This setup 
allowed for a large variety of light animation on the various button panels. Lights could simply switch 
on and off in moving marquee fashion or be displayed as sophisticated glowing, flickering pulses 1(c,d,e). 
This could all be visualized by front-end departments as animated material shaders played back in the 
scene no rendering required. Director approvals could now happen on hardware playback alone. This allowed 
these elements to be published along with the layout master sets for downstream travel to the character, 
look and lighting departments. Attributes were easily adjusted as needed all the way up to the final 
lighting render. The look department developed shader expressions that read the attributes stored on 
each piece of geometry and allowed them to software render accordingly ­very similar to the way the GBM 
and monitor graphics setups were handled.  4. Fix On Fail Element progression through the pipeline could 
be verified by watching daily auto renders . These ambient lit renders allowed quick feedback should 
one of our elements go awry in a shot. Fixes were implemented in the pipeline by publishing a snippet 
of scripted meta-data. Embedded attributes were adjusted by this script to offset an animation loop or 
change a distracting button sequence without ever having to crack open a scene file. Fix On Fail required 
a certain leap of faith, but turned out to be a time saver, as shots didn t need to come through our 
department in the majority of cases. Characters would occasionally need to interact directly with on 
screen buttons 1(b) (GBM). By visualizing hardware textures in a scene, character animators could see 
exactly where to place a finger on a button. In some cases, the loops needed to be modified if a preset 
loop didn t fit the bill. By building the scene in the effects department with the character animation 
as reference, it was easy to update loops of screen controls in the composite. These updated loops were 
then reloaded as textures in hardware and rechecked for registration to the character animation. 4. 
Conclusion Effects pipeline workflows supporting the combined power and flexibility of departments working 
in parallel, proved key to achieving high quality and consistent efficiency for the production of Prep 
and Landing. Future extension of this may be possible by even tighter integration and control of textures 
for display in up and downstream departments. Ideally, one multi­resolution texture format could serve 
as both hardware texture preview and final software render. This may be accomplished via plug-in with 
the ability to quickly convert the .ptx format to a commonly viewable native hardware format (e.g., tiff) 
then downsample resolution on demand for quick interactivity. **Ptex: Per-Face Texture Mapping for Production 
Rendering Burley, Lacewell-Eurographics Symposium on Rendering 2008 all images copyrighted &#38;#169;Disney 
Enterprises  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2010</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1837068</article_id>
		<sort_key>420</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>32</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Example-based texture synthesis on Disney's <i>Tangled</i>]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1837026.1837068</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1837068</url>
		<abstract>
			<par><![CDATA[<p>Look development on Walt Disney's animated feature <i>Tangled</i> called for artists to paint hundreds of organic elements with high-resolution textures on a tight schedule. With our Ptex format [Burley and Lacewell 2008], we had the infrastructure to handle massive textures within our pipeline, but the task of manually painting the patterned textures would still involve tedious effort.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Color, shading, shadowing, and texture</descriptor>
				<type>S</type>
			</primary_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010384</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Texturing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010384</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Texturing</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2264885</person_id>
				<author_profile_id><![CDATA[81414621264]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Christian]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Eisenacher]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Erlangen-Nuremberg]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264886</person_id>
				<author_profile_id><![CDATA[81100279764]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Chuck]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tappan]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Walt Disney Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264887</person_id>
				<author_profile_id><![CDATA[81332491776]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Brent]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Burley]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Walt Disney Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264888</person_id>
				<author_profile_id><![CDATA[81341497366]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Daniel]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Teece]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Walt Disney Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264889</person_id>
				<author_profile_id><![CDATA[81442614845]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Arthur]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Shek]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Walt Disney Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[
Burley, B., and Lacewell, D. 2008. Ptex: Per-face texture mapping for production rendering. <i>Computer Graphics Forum 27</i>, 4 (June), 1155--1164.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1141921</ref_obj_id>
				<ref_obj_pid>1141911</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[
Lefebvre, S., and Hoppe, H. 2006. Appearance-space texture synthesis. <i>ACM Transactions on Graphics 25</i>, 3, pp. 541--548.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Example-based Texture Synthesis on Disney s Tangled Christian Eisenacher1 Chuck Tappan2 Brent Burley2 
Daniel Teece2 Arthur Shek2 1University of Erlangen-Nuremberg 2Walt Disney Animation Studios Figure 1: 
Left, displacement for scales, synthesized onto a chameleon with varying size. Middle, displacement for 
stones on a road, synthesis oriented by a vector .ow .eld. Right, a texture to specify instancing sites 
for tile geometry, synthesized to arrange tiles organically on a roof. 1 Overview Look development on 
Walt Disney s animated feature Tangled called for artists to paint hundreds of organic elements with 
high­resolution textures on a tight schedule. With our Ptex format [Bur­ley and Lacewell 2008], we had 
the infrastructure to handle massive textures within our pipeline, but the task of manually painting 
the patterned textures would still involve tedious effort. We found that example-based texture synthesis, 
where an artist paints a small exemplar texture indicating a desired pattern that the system synthesizes 
over arbitrary surfaces, would alleviate some of the burden. In this talk we describe how we adapted 
existing synthe­sis methods to our Ptex-based work.ow, scaled them to production­sized textures, and 
addressed other engineering challenges. 2 Synthesis Details Our typical models use up to one giga-texel 
per surface, and com­plex models easily exceed that. However, few algorithms are even capable of synthesizing 
more than one mega-texel on a continuous surface. Our system is based on neighborhood matching, inspired 
by Lefebvre and Hoppe [2006], but addresses a number of chal­lenges, including support for the texture 
sizes we require. We chose their method because it is inherently parallel, trades pre-processing for 
very fast synthesis, and has shown excellent synthesis quality over a wide range of inputs. Exemplar 
Analysis: Like Lefebvre and Hoppe, we pre-process the exemplar to speed up runtime synthesis. We create 
a set of box­.ltered images, extract neighborhoods, perform dimensionality re­duction using PCA, and 
prune the search space using k-coherence. However, instead of determining the k most similar candidates 
for each neighborhood using a kd-tree, we pseudo-randomly sample the exemplar several times and keep 
the most similar neighborhood of 64, 16, and 4 samples. That way, we obtain k=3 suf.ciently sim­ilar 
candidates with increasing randomness. This is considerably faster and allows signi.cantly larger exemplars: 
up to 4096x4096 compared to the typical 64x64 or 128x128. The added variation turns out to be bene.cial 
for synthesis quality with the hand painted exemplars we use. Our analysis is multi-threaded and a 1024x1024 
exemplar is analyzed in about one minute on an 8-core machine. Synthesis: We cannot .t all temporary 
data required to synthesize such large textures into main memory. Therefore, we directly op­erate on 
Ptex data and perform synthesis out-of-core, taking advan­tage of Ptex s per-face granularity. We create 
the texture hierarchi­cally from coarse to .ne. For each level, we request each face and its adjacent 
faces from disk, perform a correction pass with proper .ltering, and send the result back to disk. Synthesizing 
600 million color texels requires 14 GB of temporary data and 30 min on an 8­core machine. We are I/O 
bandwidth bound, as the complete texture needs to be streamed to/from disk for each iteration, but manage 
an average 7x multithreaded speedup with a two disk RAID 0. 3 Work.ow Integration Texture synthesis 
integrates nicely into our proprietary digital paint­ing tool. Exposing it there complements existing 
methods for pro­cedural texture generation via a custom shader expression language. Artists can conveniently 
browse from a shared library or paint an exemplar, and synthesize a texture across a surface with one 
click. The texture is immediately available for manual correction of prob­lem areas. Our system allows 
artists to have .ne control over varying feature size by either painting or procedurally generating a 
scale map, as done for the chameleon body in Fig. 1. Artists can also specify the orientation of the 
synthesized texture locally, using an interactively brushed vector .ow .eld. It is represented as an 
oriented point cloud for easy interaction with external generators. Note the subtle interweaved curve-pattern 
of the stones on the road in Fig. 1. 4 Conclusion and Future Work We have had great success synthesizing 
many organic and semi­regular textures for Tangled. Artists enjoy the increased texture painting ef.ciency 
and high level of art-directability. Next, we would like to explore ways to improve synthesis quality 
for very regular patterns like stone courses on irregular geometry, better in­tegrate coarse and .ne 
detail when using higher-res exemplars, and provide more intuitive tools for generating effective exemplars. 
 References BURLEY, B., AND LACEWELL, D. 2008. Ptex: Per-face texture mapping for production rendering. 
Computer Graphics Forum 27, 4 (June), 1155 1164. LEFEBVRE, S., AND HOPPE, H. 2006. Appearance-space texture 
synthesis. ACM Transactions on Graphics 25, 3, pp. 541 548. Copyright is held by the author / owner(s). 
SIGGRAPH 2010, Los Angeles, California, July 25 29, 2010. ISBN 978-1-4503-0210-4/10/0007  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2010</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	</section>
	<section>
		<section_id>1837069</section_id>
		<sort_key>430</sort_key>
		<section_seq_no>11</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[APIs for Rendering]]></section_title>
		<section_page_from>33</section_page_from>
	<article_rec>
		<article_id>1837070</article_id>
		<sort_key>440</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>33</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Open Shading Language]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1837026.1837070</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1837070</url>
		<abstract>
			<par><![CDATA[<p>Open Shading Language (OSL) was developed by Sony Pictures Imageworks for use in its in-house renderer used for feature film animation and visual effects. OSL's specification and full implementation have been released as open source software. [Sony Pictures Imageworks 2010]</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Color, shading, shadowing, and texture</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>D.3.2</cat_node>
				<descriptor>Specialized application languages</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.4</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10011007.10011006.10011050.10011023</concept_id>
				<concept_desc>CCS->Software and its engineering->Software notations and tools->Context specific languages->Specialized application languages</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010384</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Texturing</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Languages</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2264890</person_id>
				<author_profile_id><![CDATA[81100091740]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Larry]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gritz]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Sony Pictures Imageworks]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264891</person_id>
				<author_profile_id><![CDATA[81466643124]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Clifford]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Stein]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Sony Pictures Imageworks]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264892</person_id>
				<author_profile_id><![CDATA[81100127436]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Chris]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kulla]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Sony Pictures Imageworks]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264893</person_id>
				<author_profile_id><![CDATA[81466647402]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Alejandro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Conty]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Sony Pictures Imageworks]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Piponi, D. 2004. Automatic differentiation, c++ templates, and photogrammetry. <i>journal of graphics, gpu, and game tools 9</i>, 4, 41--55.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Sony Pictures Imageworks, 2010. http://code.google.com/p/openshadinglanguage. Web site.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Open Shading Language Larry Gritz Clifford Stein Chris Kulla Alejandro Conty Sony Pictures Imageworks* 
1 Introduction Open Shading Language (OSL) was developed by Sony Pictures Imageworks for use in its in-house 
renderer used for feature .lm animation and visual effects. OSL s speci.cation and full imple­mentation 
have been released as open source software. [Sony Pic­tures Imageworks 2010] OSL has syntax similar to 
C, as well as other shading languages. However, it is speci.cally designed for advanced rendering algo­rithms 
and has features such as radiance closures, BSDFs, and de­ferred ray tracing as .rst-class concepts. 
OSL shaders can be organized into networks, with named outputs of nodes being connected to named inputs 
of other downstream nodes within the network. These connections may be done dynamically at render time, 
and do not affect compilation of individual shader nodes. Furthermore, the individual nodes are evaluated 
lazily, only when their outputs are pulled from the later nodes that depend on them (shader writers may 
remain blissfully unaware of these details, and write shaders as if everything is evaluated normally). 
 2 Radiance Closures Traditionally, shaders compute just the surface color visible from a particular 
direction. These are black boxes that a renderer can do little with but execute to .nd this one value 
(for example, there is no way to discover which directions are important to sample). Furthermore, in 
other languages, the physical units of lights and surfaces are often underspeci.ed, making it very dif.cult 
to ensure that shaders are behaving in a physically correct manner. OSL s surface and volume shaders 
compute an explicit symbolic description, called a closure, of the way a surface or volume scat­ters 
or emits light, in units of radiance. These radiance closures may be evaluated in particular directions, 
sampled to .nd important di­rections, or saved for later evaluation and re-evaluation. This new approach 
is ideal for a physically-based renderer that supports ray tracing and global illumination. There are 
different types of closure primitives for BSDF, BSSRDF, emission, and volume scattering. The integrator 
knows how to handle them properly and each closure type may have different sets of methods internally. 
But shaders may combine them, manipulate them uniformly, and behave as if they all are simply returning 
an exitant radiance. There are no light loops or explicitly traced rays in OSL shaders. Effects that 
would ordinarily require explicit ray tracing, such as re­.ection and refraction, are simply part of 
the radiance closure and look like any other BSDF. OSL does not have separate surface and light shaders; 
lights are simply surfaces that are emissive, and all lights are area lights. The radiance closures generalize 
both scat­tering and emission. You also don t need to explicitly set opacity variables in the shader. 
Transparency is just another way for light to interact with a surface, and is included in the main radiance 
closure computed by a surface shader. The radiance closures produced by shaders are passed to a part 
of the renderer called an integrator that evaluates the closures (and lights) to determine which directions 
are important, and to compute the exitant radiance in the viewing direction. Advantages of this *email: 
{lg,cstein,ckulla,aconty}@imageworks.com approach include that integration and sampling may be batched 
or re-ordered to increase ray coherence; a ray budget can be allo­cated to optimally sample the BSDF; 
the closures may be used for multi-importance sampling, bidirectional ray tracing, or Metropo­lis light 
transport; and the closures may be rapidly re-evaluated with new lighting without having to re-run the 
shaders. 3 Key runtime technologies AOVs are speci.ed using light path expressions Production users 
often output many images containing partial lighting components such as specular, diffuse, re.ection, 
individ­ual lights, etc. In other languages, this is usually accomplished by adding a plethora of arbitrary 
output variables (AOVs) the shaders that collect these individual quantities. OSL shaders need not be 
cluttered with any code or output variables to accomplish this. Instead, there is a regular-expression-based 
no­tation for describing which light paths should contribute to which outputs. For example, CD+L isolates 
just the indirect diffuse il­lumination, and CS+D*L isolates just re.ections and refractions. If you 
desire a new output, there is no need to modify the shaders at all; you only need to tell the renderer 
the new light path expression. No uniform and varying keywords in the language In our OSL runtime implementation, 
shaders are evaluated in SIMD fashion on many points at once, but there is no need to burden shader writers 
with declaring which variables need to be uniform or varying. In OSL, this is done both automatically 
and dynamically, meaning that a variable can switch back and forth between uniform and varying, on an 
instruction-by-instruction basis, depending on what is assigned to it and the current conditional state. 
 Automatic differentiation for computed derivatives OSL allows shaders to take derivatives of any computed 
quantity, and to use arbitrary computations as texture coordinates and expect correct .ltering. In our 
runtime implementation of OSL, derivatives are computed using automatic differentiation utilizing dual 
arith­metic [Piponi 2004], computing partial differentials for the vari­ables that lead to derivatives, 
without any intervention required by the shader writer. This has advantages over .nite difference meth­ods: 
it does not require that shaded points be arranged in a rect­angular grid (or have any particular connectivity 
 very important for ray tracing), or require that any extra points be shaded. It also means that it is 
safe to use derivatives inside conditional statements and loops.  References PIPONI, D. 2004. Automatic 
differentiation, c++ templates, and photogrammetry. journal of graphics, gpu, and game tools 9, 4, 41 
55. SONY PICTURES IMAGEWORKS, 2010. http://code.google.com/p/openshadinglanguage. Web site. Copyright 
is held by the author / owner(s). SIGGRAPH 2010, Los Angeles, California, July 25 29, 2010. ISBN 978-1-4503-0210-4/10/0007 
  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2010</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1837071</article_id>
		<sort_key>450</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>34</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Reyes using DirectX 11]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1837026.1837071</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1837071</url>
		<abstract>
			<par><![CDATA[<p>REYES is a common approach to generating fine-quality pictures, which is widely used in movie industry for creating CG and cartoons. REYES pipeline is a micro-polygonal pipeline, which subdivides and dices any input primitive into sub-pixel primitives, which are shaded in object-space and sampled in screen-space by a huge number of samples, thus achieving high-quality analytical anti-aliasing and precise attribute interpolation, which help to create fine images that can be used in movies and cartoons.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.3</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.4</cat_node>
				<descriptor>Software support</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10011007.10011074</concept_id>
				<concept_desc>CCS->Software and its engineering->Software creation and management</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2264894</person_id>
				<author_profile_id><![CDATA[81466645862]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Andrei]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tatarinov]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[NVIDIA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 REYES using DirectX 11 Andrei Tatarinov NVIDIA REYES is a common approach to generating fine-quality 
pictures, which is widely used in movie industry for creating CG and cartoons. REYES pipeline is a micro-polygonal 
pipeline, which subdivides and dices any input primitive into sub-pixel primitives, which are shaded 
in object-space and sampled in screen-space by a huge number of samples, thus achieving high-quality 
analytical anti-aliasing and precise attribute interpolation, which help to create fine images that can 
be used in movies and cartoons. REYES is a highly parallel pipeline by its nature, performing similar 
actions on a huge set of primitives, making it a good task for GPU to solve. Several publications were 
made, describing different ways of implementing REYES on GPU, including Kun Zhou et al., RenderAnts: 
Interactive REYES Rendering on GPUs , 2009  Anjul Patney and John D. Owens, Real-Time REYES Style Adaptive 
Surface Subdivision , 2008  Andrei Tatarinov, Alexander Kharlamov, Alternative Rendering Pipelines on 
NVIDIA CUDA , 2009  These implementations are mostly using Compute capabilities of GPU, without using 
GPU s graphics pipeline, or any particular part of graphics pipeline. Nowadays DirectX11-capable GPUs 
are becoming available, making it possible for software engineers to use DirectX11 features to accelerate 
existing and develop new rendering approaches and techniques. DirectX11 presents new rendering pipeline, 
which includes Tessellation stage, and new feature, which is called Compute and is a natural addition 
to graphics pipeline, providing fast interop between compute and graphics. The main purpose of this talk 
is to show that REYES can benefit from both of these features, achieving better performance.  The scheme 
above shows how REYES pipeline can be mapped on DirectX11 pipeline. Subdivision stage requires recursion 
 it performs recursive subdivision of input primitives, and does bounds-check on every iteration. Tessellation 
with stream-out can be used to perform this stage using graphics pipeline, or DirectCompute can be used. 
In case when DirectCompute is used, persistent threads are implemented which read and write data to the 
same input buffer, allowing recursion. Performance details and analysis of both approaches are provided 
during the talk. Tessellation stage of DirectX11 pipeline looks like a perfect candidate to be used for 
dicing stage of REYES pipeline, since dicing stage commonly uses tessellation to generate sub-pixel quads 
from a set of subdivided primitives. DirectX11 Hull and Domain shaders are used to compute dicing factors 
based on a shading rate, and non-programmable tessellation unit is used to generate UV-coordinates for 
micro-quads. Dicing stage of REYES pipeline alone usually generates huge amounts of data, which can t 
fit into GPU s on-board memory. This is why this stage needs to be pipelined with subsequent stages 
shading and sampling, so that data generated during dicing would be processed immediately and not be 
stored anywhere. Shading in REYES is done per-vertex, not per-pixel, this is why Domain shader perfectly 
fits to be used to implement this stage. Rasterization stage of GPU graphics pipeline is used to perform 
sampling. All primitives are rendered to a highly­supersampled rendertarget, providing high levels of 
anti-aliasing and solving a problem of GPU being inefficient at rendering polygons of sub-pixel size 
(due to one pixel being expanded into a whole region of pixels in a supersampled surface). One of the 
main difficulties in Compute-based implementation of dicing, shading and sampling stages is the need 
to combine them into a pipeline (due to memory pressure problem described above), which involves solutions 
like uberkernels, data storage structures and task-switching schemes. The idea of using DirectX11 tessellation 
to perform dicing is to fully utilize hardware features of GPU, since GPU already has intermediate primitive 
storage structures and scheduling mechanisms which can be used for REYES purposes. This eliminates the 
overhead required for maintaining custom memory structures and performing custom scheduling schemes. 
Transition from Compute to DirectX11 allowed author to achieve speed-ups up to 50% (with equal shading 
rates and sample counts). While subdivision stage runs at approximately the same speed for both CUDA 
and Compute, it adds negligible value to the overall performance. However, using DirectX11 tessellation 
to pipeline dicing, shading and sampling stages allows significant performance gain. During the talk 
author is going to show the advantages of using DirectX11 and graphics pipeline to implement REYES compared 
to purely compute-based approaches (both from development time and execution time perspectives). Author 
provides performance analysis of different stages of REYES pipeline, including comparison of software 
and hardware rasterization and benefits of using natural pipeline of GPU. Copyright is held by the author 
/ owner(s). SIGGRAPH 2010, Los Angeles, California, July 25 29, 2010. ISBN 978-1-4503-0210-4/10/0007 
 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2010</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1837072</article_id>
		<sort_key>460</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>35</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[WebGLot]]></title>
		<subtitle><![CDATA[high-performance visualization in the browser]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1837026.1837072</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1837072</url>
		<abstract>
			<par><![CDATA[<p>Rendering mathematical primitives should be as easy as describing them. There are several tools available for doing much of this but there is still room for improvement in interactivity, flexibility and robustness. WebGL + plot = WebGLot seeks to fill this role, providing a library for function plotting and data visualization while maintaining flexibly and performance from within the browser [Khronos b][Khronos a].</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[gpgpu]]></kw>
			<kw><![CDATA[mathematical rendering]]></kw>
			<kw><![CDATA[real-time]]></kw>
			<kw><![CDATA[visualization]]></kw>
			<kw><![CDATA[volume rendering]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.3</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>H.3.5</cat_node>
				<descriptor>Web-based services</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.4</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002951.10003260.10003282</concept_id>
				<concept_desc>CCS->Information systems->World Wide Web->Web applications</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002951.10003260.10003304</concept_id>
				<concept_desc>CCS->Information systems->World Wide Web->Web services</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2264895</person_id>
				<author_profile_id><![CDATA[81466647977]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Dan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lecocq]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264896</person_id>
				<author_profile_id><![CDATA[81100644757]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Markus]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hadwiger]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264897</person_id>
				<author_profile_id><![CDATA[81100334998]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Alyn]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Rockwood]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Khronos. Webgl public wiki. http://www.khronos.org/webgl/wiki/Main_Page.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Khronos. Webgl specification. https://cvs.khronos.org/svn/repos/registry/trunk/public/webg1/doc/spec/WebGL-spec.html.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Roettger, S. The volume library. http://www9.informatik.uni-erlangen.de/External/vollib/.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 WebGLot: High-Performance Visualization in the Browser Dan Lecocq * Markus Hadwiger Alyn Rockwood 
Figure 1: A few capabilities of WebGLot: (a) a visualization of a .ow-.eld using noise texture advection, 
(b) the solution to a Poisson equation calculated on a 2200x2200 mesh with a high-order stencil (c) a 
texture-mapped surface de.ned by a function of x, y, and t (d) raycasting an isosurface from CT scan 
data of a foot. Keywords: real-time, visualization, mathematical rendering, vol­ume rendering, gpgpu 
1 Introduction Rendering mathematical primitives should be as easy as describing them. There are several 
tools available for doing much of this but there is still room for improvement in interactivity, .exibility 
and robustness. WebGL + plot = WebGLot seeks to .ll this role, provid­ing a library for function plotting 
and data visualization while main­taining .exibly and performance from within the browser [Khronos b][Khronos 
a]. Among its strengths is its ability to of.oad intensive algorithms effectively onto the GPU while 
providing a scripting interface for easy exploration or application-building. The fact that it runs within 
the browser makes it a good candidate for cross-platform portabil­ity and ease of update. There has already 
been a great deal of work in the .eld of streaming and asynchronous communication, which solve many problems 
surrounding interactive web use. WebGLot is poised to build upon much of that existing work to the end 
of streaming .eld data or simulation results to a remote machines for live review and experimentation. 
 2 Applicability All major modern browsers support JavaScript (and many are get­ting quite fast interpreters). 
This means that WebGL can in princi­ple run on any system on a number of devices, and WebGLot seeks to 
use it to be a highly portable and ubiquitously supported visual­ization and plotting library. Researchers 
often spend a great deal of time struggling to get application-speci.c code to compile on a new machine; 
By embracing a very widely-used distribution format we can eliminate much of this dif.culty. The only 
additional requirements are to have a graphics card sup­porting OpenGL ES 2.0 or better, and a reasonable 
OpenGL driver. Both of these are often requirements for visualization for most re­search applications 
anyway. By exploiting existing streaming techniques in JavaScript, we are able to easily support a number 
of important applications for streaming data visualization. From off-site visualization of cluster­or 
supercomputer-run simulations, to monitoring data from sensor *email: dan.lecocq@kaust.edu.sa email: 
markus.hadwiger@kaust.edu.sa email: alyn.rockwood@kaust.edu.sa networks in real time. We envision it 
providing support for a large number of such applications of growing importance. This also allows for 
visualization techniques and applications to be decoupled. We augmented an existing MPI-based n-body 
simula­tion with a line to transmit intermediate results to a server instead of outputting to a .le. 
In an afternoon, we developed a WebGLot application to visualize the results as they were calculated. 
APE (the Ajax Push Engine) does not require clients to poll a server for updates, but rather they are 
noti.ed when new data arrives. With minimal code changes, existing simulations can be augmented with 
streaming visualization. WebGLot could provide graphical and visualization support for e­learning projects, 
allowing students to explore dif.cult or unintu­itive phenomena. From understanding complex data without 
need­ing to install highly speci.c applications to provide a way to experi­ment with mathematical concepts. 
Many computers have relatively high-powered graphics cards when compared to the machine s CPU and many 
medium-sized compute-intensive tasks can .nd life on the GPU. With WebGLot we were able to achieve a 
peak perfor­mance of 24 giga.ops on a NVIDIA 9400m (45% ef.ciency) with a 2D Jacobi solver. WebGL has 
the potential to serve as a delivery mechanism for HPC kernel use, and advanced visualization tech­niques. 
We aim to lower the barrier for using advanced techniques in visualization and GPGPU. 3 Goals WebGLot 
seeks to be extremely easy-to-use, and only requires ba­sic knowledge of JavaScript and familiarity with 
desired visualiza­tion techniques. It s .exible enough for a wide variety of appli­cations, but robust 
enough for computation-intensive work. Adja­cent to streaming technologies it enjoys easy support for 
streaming, while requiring only JavaScript and a graphics card. References KHRONOS. Webgl public wiki. 
http://www.khronos.org/ webgl/wiki/Main_Page. KHRONOS. Webgl speci.cation. https://cvs.khronos. org/svn/repos/registry/trunk/public/webgl/ 
doc/spec/WebGL-spec.html. ROETTGER, S. The volume library. http://www9. informatik.uni-erlangen.de/External/ 
vollib/. Copyright is held by the author / owner(s). SIGGRAPH 2010, Los Angeles, California, July 25 
 29, 2010. ISBN 978-1-4503-0210-4/10/0007  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2010</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	</section>
	<section>
		<section_id>1837073</section_id>
		<sort_key>470</sort_key>
		<section_seq_no>12</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Motion & Emotion]]></section_title>
		<section_page_from>36</section_page_from>
	<article_rec>
		<article_id>1837074</article_id>
		<sort_key>480</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>36</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[On-site real-time 3D match move for MR-based previsualization with relighting]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1837026.1837074</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1837074</url>
		<abstract>
			<par><![CDATA[<p>We are developing a previsualization method called MR-PreViz, which utilizes mixed reality technology for filmmaking [Tenmoku et al. 2006]. To determine camera-work at the shooting site, estimating camera position and posture is required. In this paper, we introduce a method for on-site real-time 3D match move and relighting for MR-PreViz. To realize the match move, we developed a computer vision-based camera tracking method using natural feature tracking. This method is based on details about the site captured in advance. The method can automatically construct a feature landmark database (LMDB) using a fiducial marker. Moreover, the result of the method enables MR-PreViz to design lighting for the site using a relighting method. To add lighting effects to the real objects, the relighting method uses reflectance properties of the real objects and LMDB.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.4.8</cat_node>
				<descriptor>Tracking</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.5.4</cat_node>
				<descriptor>Computer vision</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.2.10</cat_node>
				<descriptor>3D/stereo scene analysis</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.1</cat_node>
				<descriptor>Reflectance</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010372.10010376</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering->Reflectance modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245.10010253</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems->Tracking</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010225.10010227</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision tasks->Scene understanding</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2264898</person_id>
				<author_profile_id><![CDATA[81414619646]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Ryosuke]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ichikari]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Ritsumeikan University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264899</person_id>
				<author_profile_id><![CDATA[81466643817]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Kaori]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kikuchi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Ritsumeikan University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264900</person_id>
				<author_profile_id><![CDATA[81442619972]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Wataru]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Toishita]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Ritsumeikan University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264901</person_id>
				<author_profile_id><![CDATA[81309483074]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Ryuhei]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tenmoku]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Ritsumeikan University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264902</person_id>
				<author_profile_id><![CDATA[81414619198]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Fumihisa]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Shibata]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Ritsumeikan University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264903</person_id>
				<author_profile_id><![CDATA[81100322306]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Hideyuki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tamura]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Ritsumeikan University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1666813</ref_obj_id>
				<ref_obj_pid>1666778</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[
Ichikari, R., Hatano, R., Ohshima, T., Shibata, F., and Tamura, H. 2009. Designing cinematic lighting by relighting in mr-based pre-visualization. In <i>SIGGRAPH ASIA Posters</i>.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[
Tenmoku, R., Ichikari, R., Shibata, F., Kimura, A., and Tamura, H. 2006. Design and prototype implementation of mr pre-visualization workflow. In <i>DVD-ROM Proc. of the Int'l Workshop on Mixed Reality Technology for Filmmaking</i>, 1--7.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 On-Site Real-Time 3D Match Move for MR-based Previsualization with Relighting Ryosuke Ichikari:, Kaori 
Kikuchi, Wataru Toishita, Ryuhei Tenmoku, Fumihisa Shibata, Hideyuki Tamura Ritsumeikan University  
Figure 1: Our method (left: Structure-from-motion, middle: Match move, right: Relighting) 1 Introduction 
We are developing a previsualization method called MR-PreViz, which utilizes mixed reality technology 
for .lmmaking [Tenmoku et al. 2006]. To determine camera-work at the shooting site, esti­mating camera 
position and posture is required. In this paper, we introduce a method for on-site real-time 3D match 
move and re­lighting for MR-PreViz. To realize the match move, we developed a computer vision-based camera 
tracking method using natural fea­ture tracking. This method is based on details about the site cap­tured 
in advance. The method can automatically construct a feature landmark database (LMDB) using a .ducial 
marker. Moreover, the result of the method enables MR-PreViz to design lighting for the site using a 
relighting method. To add lighting effects to the real objects, the relighting method uses re.ectance 
properties of the real objects and LMDB. 2 Flow of On-Site Match Move and Relighting The proposed method 
realizes pre-visualizing a movie s scene with­out using specialized equipment as shown in the following 
steps. Step 1 Preparation of CG Action Data This step collects CG environment data and action data be­fore 
making MR-PreViz movies. The action data are sepa­rately recorded, and their timings and positions are 
adjusted for building one-on-one .ghting action. Moreover, we devel­oped a method of adjusting the action 
displacement to .t into the motion area derived from the actual site. Step 2 Gathering Geometrical and 
Photometrical Information of the Shooting Site (shown in the left image of Figure 1) This step gathers 
geometrical information including the 3D positions of the feature points and re.ectance property of the 
surfaces at the site for MR-PreViz shooting. This process is automatically realized by utilizing a .ducial 
marker. Step 3 Compositing MR-PreViz Image Using Real-Time 3D Match Move and Relighting (shown in the 
middle/right im­age of Figure 1) The geometrical information is used for real-time match move during 
the MR-PreViz shooting. The lighting condition of the MR-PreViz movie can be changed using the geometry 
and re­.ectance property gathered in Step 2. *e-mail: ichikari@rm.is.ritsumei.ac.jp 3 Gathering Geometrical 
and Photometrical Information of the Shooting Site In preparation for the MR-PreViz shooting, geometrical 
and photo­metrical information of the site is gathered by pre-shooting using markers. Geometry of the 
site is estimated using a Structure-From-Motion technique. In particular, the positions of feature points 
in 3D space are .rst estimated by using epipolar geometry on several frames in the video sequence. Secondly, 
6DOFs of the camera and positions of new feature points are simultaneously calculated by tracking the 
feature points. Finally, the coordinates of the 3D points are transformed into world coordinates by recognizing 
a .ducial marker. The data of the feature points are entered into our LMDB. For the relighting, the relationship 
between pixel value and illu­minance is obtained as a re.ectance property. Illuminance is au­tomatically 
calculated under several lighting conditions by using a reference marker where the re.ectance property 
of the marker is known. 4 Compositing MR-PreViz Images Using Real-Time 3D Match Move and Relighting 
In this step, the .ducial marker is removed for MR-PreViz shooting. Match move is realized by correlating 
the 2D feature points in the images with the 3D points in the LMDB. After compositing the MR-PreViz movie, 
lighting conditions of the MR-PreViz images are intentionally changed in according to the method described 
in [Ichikari et al. 2009]. In this case, the method uses the geometry and the re.ectance property of 
the site captured in Step 2. Thus, our method realizes the interactive design capabil­ities of camera-work 
and lighting at the site in real-time for MR­based pre-visualization. Acknowledgements This research 
is partly supported by the CREST Program of JST. References ICHIKARI, R., HATANO, R., OHSHIMA, T., SHIBATA, 
F., AND TAMURA, H. 2009. Designing cinematic lighting by relighting in mr-based pre­visualization. In 
SIGGRAPH ASIA Posters. TENMOKU, R., ICHIKARI, R., SHIBATA, F., KIMURA, A., AND TAMURA, H. 2006. Design 
and prototype implementation of mr pre-visualization work.ow. In DVD-ROM Proc. of the Int l Workshop 
on Mixed Reality Technology for Filmmaking, 1 7. Copyright is held by the author / owner(s). SIGGRAPH 
2010, Los Angeles, California, July 25 29, 2010. ISBN 978-1-4503-0210-4/10/0007  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2010</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1837075</article_id>
		<sort_key>490</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>37</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Motion regularization for matting motion blurred objects]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1837026.1837075</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1837075</url>
		<abstract>
			<par><![CDATA[<p>We address the problem of matting motion blurred objects from a single image. Existing single-image matting methods are designed to extract static objects that have fractional pixel occupancy. This arises because the real scene object has a finer resolution than the discrete image pixel and therefore only occupies a portion of the pixel. For a motion blurred object, however, fractional pixel occupancy is attributed almost entirely to the object's motion over the exposure time. While conventional matting techniques can be used to matte motion blurred object, they are not formulated in a manner that considers the object's local motion. Not surprisingly, these existing techniques often produce less than satisfactory results when used to matte motion blurred objects, especially when not on solid colored background.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.4.8</cat_node>
				<descriptor>Motion</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.2.10</cat_node>
				<descriptor>Motion</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.3</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010224</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352.10010380</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation->Motion processing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352.10010238</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation->Motion capture</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010238</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->Motion capture</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010238</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->Motion capture</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352.10010238</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation->Motion capture</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352.10010380</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation->Motion processing</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2264904</person_id>
				<author_profile_id><![CDATA[81466643126]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Lin]]></first_name>
				<middle_name><![CDATA[Hai]]></middle_name>
				<last_name><![CDATA[Ting]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264905</person_id>
				<author_profile_id><![CDATA[81466647906]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Tai]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yu-Wing]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264906</person_id>
				<author_profile_id><![CDATA[81466646083]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Michael]]></first_name>
				<middle_name><![CDATA[S.]]></middle_name>
				<last_name><![CDATA[Brown]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1153398</ref_obj_id>
				<ref_obj_pid>1153170</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[
Levin, A., Lischinski, D., and Weiss, Y. 2006. A closed form solution to natural image matting. In <i>CVPR'06</i>.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[
Levin, A. 2006. Blind motion deblurring using image statistics. In <i>NIPS'06</i>.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[
Liu, R., Li, Z., and Jia, J. 2008. Image partial blur detection and classification. In <i>CVPR'08</i>.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[
Wang, J., and Cohen, M. 2007. Optimized color sampling for robust matting. In <i>CVPR'07</i>.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1276389</ref_obj_id>
				<ref_obj_pid>1276377</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[
Wang, J., Agrawala, M., and Cohen, M. 2007. Soft scissors: An interactive tool for realtime high quality matting.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Motion Regularization for Matting Motion Blurred Objects Lin Hai Ting* Tai Yu-Wing Michael S. Brown 
 Input image Estimated motion Matte with motion regularization Matte without motion regularization Figure 
1: Example showing our estimated motion and its ability to improve the alpha matte of motion blurred 
objects. 1 Introduction We address the problem of matting motion blurred objects from a single image. 
Existing single-image matting methods are designed to extract static objects that have fractional pixel 
occupancy. This arises because the real scene object has a .ner resolution than the discrete image pixel 
and therefore only occupies a portion of the pixel. For a motion blurred object, however, fractional 
pixel occu­pancy is attributed almost entirely to the object s motion over the exposure time. While conventional 
matting techniques can be used to matte motion blurred object, they are not formulated in a manner that 
considers the object s local motion. Not surprisingly, these ex­isting techniques often produce less 
than satisfactory results when used to matte motion blurred objects, especially when not on solid colored 
background. In this work, we show how to obtain better alpha mattes by impos­ing a simple regularization 
in the matting formulation to account for the object s motion. In addition, we introduce a method for 
es­timating the local object motion based on local gradient statistics from the original image. For completeness 
sake, we also discuss how user markup can be used to denote the local direction in lieu of motion estimation. 
As far as we are aware, this work serves as the .rst attempt to explicitly modify the matting procedure 
to deal with motion blurred objects. 2 Our Approach Our approach works by .rst determining the local 
motion of the foreground object. Instead of assigning a single motion vector per pixel, we assign a weight, 
wd, for each of discrete angular direc­ p 2p 6p 7p tions d, where d .{0, , ,..., , }. These motion direction 
88 88 weights can be estimated directly from the input image by examin­ing local gradient properties 
in the blurred regions. In particular, we can compute the local gradient distributions within a sliding 
win­dow along the eight different radial directions. For each of these eight directions we analyze the 
shape of the gradient distributions by .tting a Laplacian to each directions distribution. The idea is 
that there are fewer image gradients in the direction of the motion due to blurring, thus distributions 
with more gradients about 0 rep­resent the underlying local motion. The weight wd is computed as the 
area under the estimated Lalpacian for each direction d. Simi­ *e-mail: linhait@comp.nus.edu.sg e-mail:yuwing@cs.kaist.ac.kr 
e-mail:brown@comp.nus.edu.sg lar analysis of the gradient distribution has been exploited for blur detection 
[Levin 2006] and blur classi.cation [Liu et al. 2008], but not yet for estimating local blur direction. 
We also allow the user to markup local motion by drawing scrib­bles on top of the image in the direction 
of the motion. Based on the user provided directions, we obtain a set of sparse local motion directions 
along the scribbles. These sparse direction labels can be propagated to other unmarked via a diffusion 
process. To compute the discrete direction weights, the user supplied motion is projected to the two 
closest of the eight discrete directions. For regions with no motion, the user can simply draw a dot 
meaning that the regu­larization weights at that local region is zero in all directions. Based on the 
local motion, we add the following regularization term to constrain each alpha value per-pixel, de.ned 
as: 8 8 Rm(a)= wd(\da)T (\da), (1) d=1 where \da is the a-gradient in direction d, wd is the weight of 
regularization for direction d. This constraint suppresses the matte gradient according to the local 
motion direction. If an image region does not contain motion blur, wd will be similar in all directions 
and thus have little effect on the alpha matte s solution. We have compared the results obtained by adding 
our regular­ization into two matting approaches: closed-form matting [Levin et al. 2006] and robust-matting 
[Wang and Cohen 2007]. We found that results are noticeably improved when using our regularization scheme. 
Moreover, we found that our automatic motion estimation is effective on the vast majority of input images. 
Future work is to incorporate this same constraint into other conventional matting procedures and to 
extend this to matting of video objects. References LEVIN, A., LISCHINSKI, D., AND WEISS, Y. 2006. A 
closed form solution to natural image matting. In CVPR 06. LEVIN, A. 2006. Blind motion deblurring using 
image statistics. In NIPS 06. LIU, R., LI, Z., AND JIA, J. 2008. Image partial blur detection and classi.cation. 
In CVPR 08. WANG, J., AND COHEN, M. 2007. Optimized color sampling for robust matting. In CVPR 07. WANG, 
J., AGRAWALA, M., AND COHEN, M. 2007. Soft scissors: An interactive tool for realtime high quality matting. 
Copyright is held by the author / owner(s). SIGGRAPH 2010, Los Angeles, California, July 25 29, 2010. 
ISBN 978-1-4503-0210-4/10/0007  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2010</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1837076</article_id>
		<sort_key>500</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>38</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[The mimic game]]></title>
		<subtitle><![CDATA[real-time recognition and imitation of emotional facial expressions]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1837026.1837076</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1837076</url>
		<abstract>
			<par><![CDATA[<p>We present an interactive animated agent system that mimics the facial expressions of a person facing a webcam. Related studies have pursued the same objective from a <i>retargeting</i> perspective, by capturing the facial deformations of a source face to apply them on a virtual character one frame at a time. Our system differs from these approaches in that it works at the semantic level, and not the morphological one. The system first captures the current facial configuration of the user and recognizes his expressive state. The agent then autonomously produces a facial expression based on the identified state, using its own deformation and dynamic characteristics. This behavior is therefore referred to as mimicking instead of retargeting or cloning. Rigid head movements are also captured and mimicked by the agent.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.2.10</cat_node>
				<descriptor>Perceptual reasoning</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.5.4</cat_node>
				<descriptor>Computer vision</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.2.11</cat_node>
				<descriptor>Intelligent agents</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.8</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010225.10010227</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision tasks->Scene understanding</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010219.10010221</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Distributed artificial intelligence->Intelligent agents</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010393</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Perception</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2264907</person_id>
				<author_profile_id><![CDATA[81413605328]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Nicolas]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Stoiber]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Orange Labs/Supelec]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264908</person_id>
				<author_profile_id><![CDATA[81320487852]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Olivier]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Aubault]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Orange Labs]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264909</person_id>
				<author_profile_id><![CDATA[81100650202]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Renaud]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Seguier]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Supelec SCEE]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264910</person_id>
				<author_profile_id><![CDATA[81309499650]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Gaspard]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Breton]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Orange Labs]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[
Ekman, P. 1982. <i>Emotion in the Human Face</i>. Cambridge University Press, New York.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[
Sattar, A., Aidarous, Y., Le Gallou, S., and Seguier, R. 2007. Face alignment by 2.5d active appearance model optimized by simplex. <i>ICVS</i>.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1502681</ref_obj_id>
				<ref_obj_pid>1502650</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[
Stoiber, N., Seguier, R., and Breton, G. 2009. Automatic design of a control interface for a synthetic face. <i>Proceedings of the conference on Intelligent User Interfaces</i>, 207--216.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 The Mimic Game: Real-time Recognition and Imitation of Emotional Facial Expressions Nicolas Stoiber* 
Olivier Aubault Renaud Seguier Gaspard Breton Orange Labs/Supelec Orange Labs Supelec SCEE Orange Labs 
We present an interactive animated agent system that mimics the facial expressions of a person facing 
a webcam. Related studies have pursued the same objective from a retargeting perspective, by capturing 
the facial deformations of a source face to apply them on a virtual character one frame at a time. Our 
system differs from these approaches in that it works at the semantic level, and not the morphological 
one. The system .rst captures the current facial con­.guration of the user and recognizes his expressive 
state. The agent then autonomously produces a facial expression based on the iden­ti.ed state, using 
its own deformation and dynamic characteristics. This behavior is therefore referred to as mimicking 
instead of re­targeting or cloning. Rigid head movements are also captured and mimicked by the agent. 
1 Facial Motion Capture The markerless capture of the user s facial con.guration is based on an Active 
Appearance Model (AAM) [Sattar et al. 2007]. AAM are commonly used to describe the appearance changes 
of deformable objects using geometric shape and texture information. They are particularly well-adapted 
to facial deformations which cause both shape changes (movement of the facial elements) and pixel inten­sity 
changes (wrinkles). Our particular AAM model is trained on a person-speci.c facial expression, and accurately 
tracks both rigid head movements and non-rigid expressive deformations. 2 Facial Expression Recognition 
The tracker s output (a high-dimensional vector describing the cur­rent facial con.guration) does not 
have any semantic meaning. In­terpreting the facial deformations implies converting this con.gura­tion 
vector to a more meaningful representation, related to higher­level concepts. Previous emotion recognition 
methods have tradi­tionally relied on theoretical representations of human emotion [Ek­man 1982]. These 
representations were associated to facial defor­mation data through supervised learning. Instead, we 
propose a more objective representation of emotional facial expressions, ex­tracted from the data itself. 
We form an optimized 2D approxima­tion of the nonlinear space of facial expressions, using a manifold 
.tting technique on a database of expressive images [Stoiber et al. 2009]. The obtained 2D manifold can 
be unfolded to form a simple yet consistent visual representation of the space of facial expres­sions. 
Moreover, this representation is convenient for semantic in­terpretations. Our real-time imitation system 
eventually projects the high­dimensional features detected by the AAM on the 2D manifold. The result 
is displayed as a point on its disk-shaped unfolded rep­resentation (see .gure 1). 3 Facial Expression 
Synthesis The disk-shaped intuitive representation of .gure 1 can also be used as a command interface 
for the virtual character [Stoiber et al. 2009]. The system can thus easily select target facial expressions 
for the agent to display, according to the identi.ed state of the user. Ultimately, a dynamics-aware 
animation system computes the fa­cial movements necessary to transition from the current expression to 
the target one. Our animation system relies on a collection of input-output nonlin­ear motion models 
inspired by automatic control science. Any tar­get expression can be given as input to the motion models, 
and the *e-mail: nicolas.stoiber@orange-ftgroup.com Copyright is held by the author / owner(s). SIGGRAPH 
2010, Los Angeles, California, July 25 29, 2010. ISBN 978-1-4503-0210-4/10/0007  Figure 1: Snapshot 
of the real-time imitation system. Facial deformations and head movements are captured on the hu­man 
face (left), and facial deformation are interpreted as a point in the simpli.ed facial expressions space 
(top right). The virtual character then reproduces the position and the expressive state of the user 
(right) using its own motion models. corresponding facial movements are outputted in real time; inputs 
can be changed on-the-.y, providing a good reactivity to evolving targets. The models parameters are 
learned from real facial motion data, ensuring the naturalness of the produced animations. Our mo­tion 
models also integrate a stochastic component that accounts for the inherent variability of human motion. 
 4 Conclusion Our embodied agent successfully mimics the expressions of the user (see the accompanying 
video). Thanks to the recognition step, the system does not just capture the facial deformations, but 
is able to interpret them. Therefore, instead of simply imitating the user, the agent could trigger more 
constructive reactions, such as express­ing surprise when the user looks sad or angry. In real-world 
appli­cations, other information channels such as speech or gaze direction need to be accounted for. 
Our system would however help de.ne more realistic agent behaviors by ef.ciently handling the recogni­tion 
and the synthesis of facial cues. This work was funded by the Companions project sponsored by the European 
Commission as part of the Information Society Technolo­gies program (IST-FP6-034434). References EKMAN, 
P. 1982. Emotion in the Human Face. Cambridge Uni­versity Press, New York. SATTAR, A., AIDAROUS, Y., 
LE GALLOU, S., AND SEGUIER, R. 2007. Face alignment by 2.5d active appearance model opti­mized by simplex. 
ICVS. STOIBER, N., SEGUIER, R., AND BRETON, G. 2009. Automatic design of a control interface for a synthetic 
face. Proceedings of the conference on Intelligent User Interfaces, 207 216.  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<article_sponsors>
			<funding_agency>European Commission Information Society Technologies program</funding_agency>
			<grant_numbers>
				<grant_number>IST-FP6-034434</grant_number>
			</grant_numbers>
		</article_sponsors>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2010</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1837077</article_id>
		<sort_key>510</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>39</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[Effective animation of sign language with prosodic elements for annotation of digital educational content]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1837026.1837077</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1837077</url>
		<abstract>
			<par><![CDATA[<p>Computer animation of American Sign Language (ASL) has the potential to remove many educational barriers for deaf students, because it provides a low-cost, effective means for adding sign language translation to any type of digital content.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Animation</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.8</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>K.3.1</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010489</concept_id>
				<concept_desc>CCS->Applied computing->Education</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010507</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Graphics recognition and interpretation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2264911</person_id>
				<author_profile_id><![CDATA[81319487435]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Nicoletta]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Adamo-Villani]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Purdue University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264912</person_id>
				<author_profile_id><![CDATA[81466642463]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Kyle]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hayward]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Purdue University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264913</person_id>
				<author_profile_id><![CDATA[81466647635]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Jason]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lestina]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Purdue University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264914</person_id>
				<author_profile_id><![CDATA[81332535851]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Ronnie]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wilbur]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Purdue University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Effective Animation of Sign Language with Prosodic Elements for Annotation of Digital Educational Content 
 Nicoletta Adamo-Villani, Kyle Hayward, Jason Lestina, Ronnie Wilbur, Purdue University* 1. Introduction 
Computer animation of American Sign Language (ASL) has the potential to remove many educational barriers 
for deaf students, because it provides a low-cost, effective means for adding sign language translation 
to any type of digital content. Several research groups [1-3] have investigated the benefits of rendering 
ASL in 3D animations. Although the quality of animated ASL has improved in the past few years and it 
shows strong potential for revolutionizing accessibility to digital media, its effectiveness and wide-spread 
use is still precluded by two main limitations: (a) low realism of the signing characters, which result 
in limited legibility of animated signs and low appeal of virtual signers, and (b) lack of easy-to-use 
public domain authoring systems that allow educators to create animated ASL annotated educational materials. 
The general goal of our research is to overcome both limitations. Specifically, the objective of the 
work reported in the paper was to research and develop a software system for annotating math/science 
digital educational content for grades 1-3 with expressive ASL animation with prosodic elements. The 
system provides educators of the Deaf with an effective means of creating and adding grammatically correct, 
life-like sign language translation to learning materials such as interactive activities, texts, images, 
slide presentations, and videos. 2. The ASL Authoring System The system has been iteratively developed 
with continuous feedback from teachers and students at the Indiana School for the Deaf (ISD). It includes 
3 components: 3D Model Support Component. This component allows importing 3D models of characters and 
background 3D scenes. Animation Support Component. This component enables the user to (a) import signs 
from a sign database, (b) create new signs, (c) create facial articulations, (d) smoothly link signs 
and facial articulations in ASL continuous discourse, and (e) type an ASL script in the script editor 
and automatically generate the corresponding ASL animation. (a) The system includes an initial database 
of animated signs for mathematics for grades 1-2; more signs can be added to the library. (b) If a needed 
sign is not available in the database, it can be created by defining character hand, limb, body poses. 
(c) Facial articulations are created by combining morph targets in a variety of ways, and applying them 
to the character. (d) The animation support module computes realistic transitions between consecutive 
poses and signs. (e) The ASL system includes a tool that understands ASL script syntax (which is very 
similar to ASL gloss): the ASL Script Editor. The ASL script editor enables a user with knowledge of 
ASL gloss, to type an ASL script including both ASL gloss and mathematical equations; the script is then 
automatically converted to the correct animations with prosodic elements. Rendering Support Component. 
This component implements advanced rendering effects such as ambient occlusion, motion blur, and depth-of-field 
to enhance visual comprehension of signs. It exports the final ASL sequences to various movie formats. 
*email: {nadamovi, khayward, jlestina, wilbur}@purdue.edu 3. ASL animation with prosodic elements Although 
various attempts at animating ASL for purposes of deaf education and entertainment currently exist, they 
all fail to provide regular, linguistically appropriate grammatical markers that are made with the hands, 
face, head, and body, producing animation that is stilted and difficult to process (as an analogy, try 
to imagine someone speaking with no intonation). That is, they lack what linguists call prosody . Prosodic 
markers (e.g. head nod, hand clasp, body lean, mouth gestures, shoulder raise, etc.) and prosodic modifiers 
(e.g. sign lengthening, jerk, pauses, etc.) are used in ASL to convey and clarify the syntactic structure 
of the signed discourse [4]. Research has identified over 20 complex prosodic markers/modifiers and has 
measured frequencies of up to 7 prosodic markers/modifiers in a two second span [5]. Adding such number 
and variety of prosodic elements by hand through a graphical user interface (GUI) is prohibitively slow. 
Our system includes a novel algorithm that automates the process of enhancing ASL animation with prosodic 
elements. The algorithm interprets the ASL script entered in the Script Editor (described in section 
2) and identifies the signs and the prosodic markers/modifiers needed to animate the input sequence. 
The prosodic elements are added automatically from ASL prosody rules. Example prosody rules automated 
by our algorithm are: -High-level sentence structure. Appropriate prosodic modifiers are added to mark 
the beginning (i.e. blink before hands move) and end (i.e. longer last sign) of the sentence. Periods 
and commas are automatically translated to their respective prosodic modifier (longer and shorter pauses, 
respectively). -Sentence type. Interrogative, imperative, and conditional sentences are detected based 
on punctuation marks (?, !) and key words (e.g. wh-words , if , whether ) and appropriate prosodic markers 
(i.e. raised eyebrows) are added. The final animation is assembled by retrieving the required signs from 
the sign database and by translating the identified prosodic elements to corresponding animation markers/modifiers. 
A multi­track animation timeline is populated with the animated signs and animation markers/modifiers. 
Most prosody markers are layered on top of the animated signs. Some prosody markers, such as e.g. hand 
clasp, are inserted in between signs. Prosody modifiers are layered on top of the signs they modify. 
The supplementary video includes examples of ASL animated sequences enhanced with algorithmically generated 
prosodic elements. 4. Discussion and Conclusion The system described in the paper is the first and only 
animation-based sign language program that produces fluid ASL animation enhanced with automatically generated 
prosodic elements. The problem of advancing Deaf education decisively can only be solved if the process 
of increasing ASL animation quality is automated. Scalability to all age groups and disciplines can only 
be achieved if educational content can be easily annotated with life-like, grammatically correct ASL 
animation by teachers with no computer animation expertise. Our system provides a solution to this problem 
because it enables users with no technical background to create high quality ASL animation by simply 
typing an ASL script. Copyright is held by the author / owner(s). SIGGRAPH 2010, Los Angeles, California, 
July 25 29, 2010. ISBN 978-1-4503-0210-4/10/0007  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2010</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	</section>
	<section>
		<section_id>1837078</section_id>
		<sort_key>520</sort_key>
		<section_seq_no>13</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Fun In Flatland]]></section_title>
		<section_page_from>40</section_page_from>
	<article_rec>
		<article_id>1837079</article_id>
		<sort_key>530</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>40</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[NPR Gabor noise for coherent stylization]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1837026.1837079</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1837079</url>
		<abstract>
			<par><![CDATA[<p>Stylized rendering algorithms, which aim at depicting 3D animated scenes with a <i>pattern</i> (i.e., watercolor pigments, strokes, paper grain, etc.), are facing the temporal coherence problem. To achieve successful temporal coherent stylization three constraints must be fulfilled: <b>flatness</b> (2D impression of the style), <b>coherent motion</b> (high correlation between the 3D motion field and the motion of the pattern) and <b>temporal continuity</b> (minimal changes from frame to frame). The conflicting nature of these goals implies that any solution to this problem is necessarily a compromise.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.3</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2264915</person_id>
				<author_profile_id><![CDATA[81466644838]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[P.]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[B&#233;nard]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Grenoble University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264916</person_id>
				<author_profile_id><![CDATA[81300379401]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[A.]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lagae]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Katholieke Universiteit Leuven and INRIA Sophia-Antipolis]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264917</person_id>
				<author_profile_id><![CDATA[81319503110]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[P.]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Vangorp]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[INRIA Sophia-Antipolis]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264918</person_id>
				<author_profile_id><![CDATA[81100198784]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[S.]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lefebvre]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[INRIA Nancy Grand-Est / Loria]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264919</person_id>
				<author_profile_id><![CDATA[81100408270]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[G.]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Drettakis]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[INRIA Sophia-Antipolis]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264920</person_id>
				<author_profile_id><![CDATA[81466645622]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[J.]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Thollot]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Grenoble University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1124751</ref_obj_id>
				<ref_obj_pid>1124728</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[
Bousseau, A., Kaplan, M., Thollot, J., and Sillion, F. 2006. Interactive watercolor rendering with temporal coherence and abstraction. In <i>Proc. NPAR '06</i>, 141--149.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2381227</ref_obj_id>
				<ref_obj_pid>2381219</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[
Kaplan, M., and Cohen, E. 2005. A generative model for dynamic canvas motion. In <i>Proc. CAe '05</i>, 49--56.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1531360</ref_obj_id>
				<ref_obj_pid>1531326</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[
Lagae, A., Lefebvre, S., Drettakis, G., and Dutr&#233;, P. 2009. Procedural noise using sparse Gabor convolution. <i>ACM Trans. Graphics 28</i>, 3, 54:1--54:10.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 NPR Gabor Noise for Coherent Stylization P.B´enard1 A.Lagae2,3 P.Vangorp3 S.Lefebvre4 G.Drettakis3 J.Thollot1 
1 GrenobleUniversity 2 KatholiekeUniversiteitLeuven 3 INRIASophia-Antipolis 4 INRIANancyGrand-Est/Loria 
 Figure1: We present a new noise primitive for NPR that keeps the 2D aspect of noise, conveys the 3D 
motion of the scene, and is temporally continuous. This allows us to use standard techniques from procedural 
texturing to create various styles for interactive NPR applications. 1 Introduction Stylizedrendering 
algorithms, which aim atdepicting3D animated scenes with a pattern (i.e., watercolor pigments, strokes, 
paper grain,etc.),arefacingthe temporalcoherenceproblem.Toachieve successful temporal coherent stylization 
three constraints must be ful.lled: .atness (2D impression of the style), coherent motion (highcorrelationbetween 
the3D motion .eldandthemotionofthe pattern) and temporal continuity (minimalchangesfromframe to frame).Thecon.icting 
natureofthesegoalsimplies that any solu­tion tothisproblemisnecessarilyacompromise. Most previous approaches 
can be classi.ed into two categories, primitive-basedandtexture-based methods, each ofthem exhibiting 
differentartifacts. We also identify a third category, many-primitive methods,establishing aconnectionbetweenprimitive-based 
meth­ods in NPR and sparse convolution noise in procedural texturing. ThedynamiccanvasofKaplanandCohen[2005] 
and the interac­tivewatercolorrenderingpipelineofBousseauet al.[2006] are the only two related methods 
in this category. In this work, we propose a generalization of these approaches by introducing NPR Gabor 
noise which makes the connection with sparse convolution noise explicit and, thus, allows the use of 
stan­dard techniques from procedural texturing to create various styles. Asamany-primitivemethod, itfeaturesgoodmotioncoherenceand 
agood sense of .atness. 2 Our Approach We base our noise primitive on Gabor noise [Lagae et al. 2009] 
because it features a precise local spectral control. Ga­bor noise is a sum of randomly weighted and 
positioned Ga­bor kernels. Three kinds of Gabor noise have been proposed so far 2D, surface and 3D Gabor 
noises but none of them addresses the problem of temporal coherence for NPR. The .rstone isdecorrelatedfrom 
the3D scene motion and the two later behave like regular texture mapping. To obtain a noise with a 2D 
aspect, we de.ne the parameters(e.g.,frequency and orienta­tion) and evaluate thenoise in2D screen space. 
To ensure a coherent motion of the noise, we de.ne the point distribu­tion used by the noise on the surface 
of the triangle3D model(seeinset.gure).  We generate this temporally coherent Poisson distribution using 
a pseudo-randomnumbergeneratorwithaconstantseedper triangle. Inorder tominimizingpopping,we introduceanLOD 
mechanism which ensures a continuous and constant density of point distribu­tionsin2D screenspacethroughout 
time.Weexpressthenoiseasa linear interpolation between two noises using a weighting function which preserves 
its statistical properties. In contrast to most NPR techniques that blend between two different noises 
or textures, our schemedoesnot affect theappearanceof thenoise. Finally, we process and combine multiple 
coherent noise layers to create styles ranging from continuous to discrete (see video). We model binary 
styles by thresholding a noise using a smooth stepfunction,whichproduceshatches inthecaseof ananisotropic 
noise, and stipples in the case of an isotropic noise. We create color styles(painting,watercolor, ink)by 
compositing thenoisewith the scenecolor.Ournoiseprimitivedoesnot offerdirect control at the stroke level. 
However, in contrast to most other texture-based ap­proaches, we achieve local control by linking noise 
parameters to scene attributes(e.g., shading,geometric curvature). 3 Conclusion Becauseany solution 
tothe temporal coherenceproblemisa trade­offbetweencontradictorygoals, itsevaluationiscomplex.Previous 
approaches only relied on speculative visual inspection. However, each goal has speci.c artifacts which 
a user can observe. To eval­uate them, we conducted a user study comparing six methods. It shows that 
motion coherence is the most important quality to pre­serve in theoverallcompromise,andthat ournewsolutionprovides 
agood compromisebetween the2D aspect of thestyleand3D mo­tion. Weplan todomoreuserstudiesbutalso toexploreobjective 
measures tofurtherquantify these trade-offs. References BOUSSEAU, A., KAPLAN, M., THOLLOT, J., AND SILLION, 
F. 2006. Interactive watercolor rendering with temporal coherence and abstraction. In Proc. NPAR 06,141 
149. KAPLAN, M., AND COHEN, E. 2005. A generative model for dynamic canvas motion. In Proc. CAe 05,49 
56. LAGAE, A., LEFEBVRE, S., DRETTAKIS, G., AND DUTRE´, P. 2009. Procedural noise using sparse Gabor 
convolution. ACM Trans. Graphics 28,3,54:1 54:10. Copyright is held by the author / owner(s). SIGGRAPH 
2010, Los Angeles, California, July 25 29, 2010. ISBN 978-1-4503-0210-4/10/0007  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2010</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1837080</article_id>
		<sort_key>540</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>41</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Project Gustav]]></title>
		<subtitle><![CDATA[immersive digital painting]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1837026.1837080</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1837080</url>
		<abstract>
			<par><![CDATA[<p>Project Gustav is a digital painting system that enables artists to become immersed in the digital painting experience. It achieves interactivity and realism by leveraging the GPU to implement our novel art media modeling and brush simulation algorithms.</p> <p>Compared with current digital painting packages, there are two main advances over state-of-the-art. The first is a highly efficient, data-driven 3D deformable brush simulation [Baxter and Govindaraju 2010]. The second is paint modeling algorithms for oil and pastel that preserve stroke details as paints are smeared and mixed [Chu et al. 2010]. Together these allow digital painting in a manner much closer to the real-world experience. This not only makes it possible to paint digitally with very little training but also respects the skills artists have acquired in real life. We further combine our painting simulation with the latest touch and pen input technology to make Project Gustav even more enjoyable to use. Together, these features provide a responsive and realistic digital painting experience never before attained.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.4</cat_node>
				<descriptor>Paint systems</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>J.5</cat_node>
				<descriptor>Fine arts</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.6.5</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010469.10010470</concept_id>
				<concept_desc>CCS->Applied computing->Arts and humanities->Fine arts</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341.10010342</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Model development and analysis</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010469.10010471</concept_id>
				<concept_desc>CCS->Applied computing->Arts and humanities->Performing arts</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2264921</person_id>
				<author_profile_id><![CDATA[81100207483]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[William]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Baxter]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Microsoft Research]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264922</person_id>
				<author_profile_id><![CDATA[81100141173]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Nelson]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Chu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Microsoft Research]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264923</person_id>
				<author_profile_id><![CDATA[81100383019]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Naga]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Govindaraju]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Microsoft Research]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1730826</ref_obj_id>
				<ref_obj_pid>1730804</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Baxter, W., and Govindaraju, N. 2010. Simple data-driven modeling of brushes. In <i>Proc. Symposium on Interactive 3D Graphics (I3D 2010)</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383313</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Baxter, W., Scheib, V., Lin, M., and Manocha, D. 2001. Dab: Haptic painting with 3d virtual brushes. <i>Proc. of ACM SIGGRAPH</i>, 461--468.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1809943</ref_obj_id>
				<ref_obj_pid>1809939</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Chu, N., Baxter, W., Wei, L.-Y., and Govindaraju, N. 2010. Detail-preserving paint modeling for 3D brushes. In <i>Proc. Symposium on Non-Photorealistic Animation and Rendering (NPAR 2010)</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Project Gustav: Immersive Digital Painting William Baxter Nelson Chu Naga Govindaraju Microsoft Research 
Microsoft Research Microsoft Research  Figure 1: User interface of our painting system and close-ups 
of our oil and pastel art media models. Figure 2: We capture brush deformations from video sequences, 
and create a variety of brushes using this data. Abstract Project Gustav is a digital painting system 
that enables artists to become immersed in the digital painting experience. It achieves interactivity 
and realism by leveraging the GPU to implement our novel art media modeling and brush simulation algorithms. 
Compared with current digital painting packages, there are two main advances over state-of-the-art. The 
.rst is a highly ef.cient, data-driven 3D deformable brush simulation [Baxter and Govin­daraju 2010]. 
The second is paint modeling algorithms for oil and pastel that preserve stroke details as paints are 
smeared and mixed [Chu et al. 2010]. Together these allow digital painting in a manner much closer to 
the real-world experience. This not only makes it possible to paint digitally with very little training 
but also respects the skills artists have acquired in real life. We further combine our painting simulation 
with the latest touch and pen input technology to make Project Gustav even more enjoyable to use. Together, 
these features provide a responsive and realistic digital painting experi­ence never before attained. 
1 Brush Simulation We create dynamic deformable 3D brushes based on measurements taken of actual brush 
deformations (Fig. 2, top left). We .nd that a small table of such measurments is enough to recreate 
the key deformation characteristics of a brush (Fig. 2, bottom left), and the result is a fast, stable 
and realsitic brush model for use in a digital painting system. To get realistic dynamics, we augment 
the static deformation data with a simpli.ed energy optimization that has a closed form solution, leading 
to an extremely ef.cient simulation technique. We also introduce two new methods for skinning mesh deformations 
based on control spines. Copyright is held by the author / owner(s). SIGGRAPH 2010, Los Angeles, California, 
July 25 29, 2010. ISBN 978-1-4503-0210-4/10/0007 Figure 3: Resolution-matched 2D pickup map preserves 
.ne color detail better than the 3D mapping technique used in previous 3D brush paint systems. 2 Paint 
Model Previous work such as [Baxter et al. 2001] showed the bene.ts of combining 3D brush models with 
a paint media model; however, the way these works mapped paint onto the 3D brush surface and used repeated 
back-and-forth tranfers to implement smearing led to excessive blurring of color detail. By making the 
counter-intuitive choice of using a 2D pickup map with matched resolution (Fig. 3) to implement bidirectional 
transfer of paint, our technique mini­mizes re-sampling artifacts. Furthermore, by using a special snap­shot 
buffer, we are able to prevent the other major source of blur­ring, the repeated resampling. Together 
these techniques allow us to better preserve the .ne streaks and color details that are charac­teristic 
of oil and pastel styles.  References BAXTER, W., AND GOVINDARAJU, N. 2010. Simple data-driven modeling 
of brushes. In Proc. Symposium on Interactive 3D Graphics (I3D 2010). BAXTER, W., SCHEIB, V., LIN, M., 
AND MANOCHA, D. 2001. Dab: Haptic painting with 3d virtual brushes. Proc. of ACM SIGGRAPH, 461 468. CHU, 
N., BAXTER, W., WEI, L.-Y., AND GOVINDARAJU, N. 2010. Detail-preserving paint modeling for 3D brushes. 
In Proc. Symposium on Non-Photorealistic Animation and Render­ing (NPAR 2010).  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2010</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1837081</article_id>
		<sort_key>550</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>42</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Interactively browsing large image collections]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1837026.1837081</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1837081</url>
		<abstract>
			<par><![CDATA[<p>Manually locating an image in a large collection has become infeasible with the recent rapid growth in size of such collections. Nowadays, even private collections easily contain tens of thousands of images; public collections have long passed the billion images mark. Current approaches for finding images in large collections, therefore, try to confine the set of images by returning only those images that correspond to certain properties defined by a query. Such properties can include: keywords, semantic information associated with the images, similarity to an example image, a rough sketch of the desired outlines, or any combination thereof.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>H.2.8</cat_node>
				<descriptor>Image databases</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>H.3.1</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>H.3.3</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10002951.10003317</concept_id>
				<concept_desc>CCS->Information systems->Information retrieval</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002951.10003317.10003318</concept_id>
				<concept_desc>CCS->Information systems->Information retrieval->Document representation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002951.10003227.10003251.10003253</concept_id>
				<concept_desc>CCS->Information systems->Information systems applications->Multimedia information systems->Multimedia databases</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2264924</person_id>
				<author_profile_id><![CDATA[81421598974]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Ronald]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Richter]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[TU Berlin]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264925</person_id>
				<author_profile_id><![CDATA[81336488955]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Mathias]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Eitz]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[TU Berlin]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264926</person_id>
				<author_profile_id><![CDATA[81100235480]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Marc]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Alexa]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[TU Berlin]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1311121</ref_obj_id>
				<ref_obj_pid>1311063</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Fukunaga, K., and Narendra, P. M. 1975. A branch and bound algorithms for computing k-nearest neighbors. <i>IEEE Trans. Computers 24</i>, 7, 750--753.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Heesch, d., and R&#252;ger, S. 2004. NNk networks for content-based image retrieval. <i>Advances in Information Retrieval</i>, 253--266.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>598462</ref_obj_id>
				<ref_obj_pid>598425</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Oliva, A., and Torralba, A. 2001. Modeling the Shape of the Scene: A Holistic Representation of the Spatial Envelope. <i>IJCV 42</i>, 3, 145--175.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Interactively Browsing Large Image Collections Ronald Richter, Mathias Eitz and Marc Alexa TU Berlin 
 Figure 1: Quick approximate nearest-neighbor search (c) is utilized to accelerate NNk search (d) yielding 
interactive image browsing (e). 1 Introduction Manually locating an image in a large collection has become 
in­feasible with the recent rapid growth in size of such collections. Nowadays, even private collections 
easily contain tens of thousands of images; public collections have long passed the billion images mark. 
Current approaches for .nding images in large collections, therefore, try to con.ne the set of images 
by returning only those images that correspond to certain properties de.ned by a query. Such properties 
can include: keywords, semantic information as­sociated with the images, similarity to an example image, 
a rough sketch of the desired outlines, or any combination thereof. However, common to all those methods 
is that they require the user to have an a priori notion of the image to be found. Very often, this is 
not the case and users only have a vague idea of the desired image or simply want to get a general idea 
of the variety of images con­tained in a collection. In this paper, we propose a novel method for interactively 
browsing large image collections, making the user an integral part of the interactive exploration by 
repeatedly exploiting the amazing ability of humans to quickly identify relevant images from a large 
set. The method requires only minimal input effort: users simply point to the image in the current display 
that seems most attractive to them (Fig. 1a)). The system then assembles a representative set of other 
images from the collection that are likely in its perceptual vicinity. The resulting browsing approach 
is even for novice users extremely simple to use and enables an interac­tive exploration of the collection 
as well as target-oriented selection towards a speci.c mental image model. 2 Interactive Browsing The 
only data that is reliably available for any image collection is the data extractable from the images 
themselves. Consequently, we restrict ourselves to content-based methods and make use of ex­isting image 
features such as texture information [Oliva and Tor­ralba 2001], color distributions, and spatial color 
layouts (Fig. 1b)). When browsing images, a notion of image similarity is required. A simple de.nition 
is commonly given by de.ning a distance metric in image feature space. Images are then compared by computing 
distances between their corresponding feature vectors. In the context of browsing, it is desirable to 
combine several differ­ent image features in order to cover multiple aspects of an image at once. However, 
the importance of individual features can dif­fer according to the contents of a particular image as 
well as the intentions of the user. As we cannot predict the user s goal, a sin­gle, .xed linear combination 
of weights is inappropriate for com­bining features. Instead, NNk search has been proposed [Heesch and 
R¨ uger 2004], exploring the complete (quantized) weight-space (Fig. 1d)). The NNk of a query is de.ned 
as that set of images, that are nearest-neighbors to the query with respect to at least one combination 
of weights. NNk networks can be constructed of.ine but the process is quadratic in the number of images 
in the collec­tion and thus quickly becomes infeasible for large collections. In this work, we propose 
an approximation of NNk search that allows applying the underlying idea to theoretically arbitrarily 
large image collections without notable loss of quality. Instead of .nding exact nearest-neighbors, we 
propose to restrict the search to promising candidates using an approximate k-means tree for sublinear 
search­ing [Fukunaga and Narendra 1975]. The resulting system reaches a precision of over 90% with respect 
to the full search but gives instant answers without the need for costly of.ine computation. 3 Results 
and Discussion The resulting browsing interface see Fig. 1e) allows browsing large image collections 
interactively and ef.ciently. In the accom­panying video, we demonstrate an interactive system using 
the pro­posed method on a collection of one million images. Users of the experimental system were able 
to quickly gain an overview of the collection as well as to quickly narrow down onto certain desired 
image categories. Using the proposed approximate NNk search, the system is theoretically scalable to 
web-scale collections, limited only by the amount of main memory available. Given the hierarchi­cal nature 
of the underlying nearest-neighbor search datastructure, we believe that an out-of-core as well as a 
distributed version of the search (subtrees distributed over multiple machines) are natu­ral candidates 
for future work. Additionally, we see potential for a localized version of the browsing interface, where 
users would .x parts of the image that are acceptable, e.g. using a multi-touch in­terface. Browsing 
would subsequently be restricted to those images that contain the .xed local features, exploring only 
the part of the collection that is relevant for the current search. References FUKUNAGA, K., AND NARENDRA, 
P. M. 1975. A branch and bound algorithms for computing k-nearest neighbors. IEEE Trans. Computers 24, 
7, 750 753. HEESCH, D., AND R¨ UGER, S. 2004. NNk networks for content-based image retrieval. Advances 
in Information Retrieval, 253 266. OLIVA, A., AND TORRALBA, A. 2001. Modeling the Shape of the Scene: 
A Holistic Representation of the Spatial Envelope. IJCV 42, 3, 145 175. Copyright is held by the author 
/ owner(s). SIGGRAPH 2010, Los Angeles, California, July 25 29, 2010. ISBN 978-1-4503-0210-4/10/0007 
  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2010</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	</section>
	<section>
		<section_id>1837082</section_id>
		<sort_key>560</sort_key>
		<section_seq_no>14</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Games & Real Time]]></section_title>
		<section_page_from>43</section_page_from>
	<article_rec>
		<article_id>1837083</article_id>
		<sort_key>570</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>43</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[User generated terrain in ModNation Racers]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1837026.1837083</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1837083</url>
		<abstract>
			<par><![CDATA[<p>ModNation Racers is an action packed kart racing game focused around user generated content, developed for the Sony PlayStation 3 console. As such we need a terrain system that is dynamically editable and also has a low storage foot print. In this talk we will give details on the system we implemented to support these goals.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.4</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>K.8.0</cat_node>
				<descriptor>Games</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010476.10011187.10011190</concept_id>
				<concept_desc>CCS->Applied computing->Computers in other domains->Personal computers and PC applications->Computer games</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10011007.10010940.10010941.10010969.10010970</concept_id>
				<concept_desc>CCS->Software and its engineering->Software organization and properties->Contextual software domains->Virtual worlds software->Interactive games</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002951.10003227.10003251.10003258</concept_id>
				<concept_desc>CCS->Information systems->Information systems applications->Multimedia information systems->Massively multiplayer online games</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2264927</person_id>
				<author_profile_id><![CDATA[81335490796]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[James]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Grieve]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[United Front Games]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264928</person_id>
				<author_profile_id><![CDATA[81466643777]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Clint]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hanson]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[United Front Games]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264929</person_id>
				<author_profile_id><![CDATA[81466646761]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[John]]></first_name>
				<middle_name><![CDATA[(Liuguo)]]></middle_name>
				<last_name><![CDATA[Zhang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[United Front Games]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264930</person_id>
				<author_profile_id><![CDATA[81466647888]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Lucas]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Granito]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[United Front Games]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264931</person_id>
				<author_profile_id><![CDATA[81466645004]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Cody]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Snyder]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[United Front Games]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 172.16.1.21_20100120_192048.jpg User Generated Terrain in ModNation RacersJames Grieve United Front 
GamesClint Hanson United Front Games John (Liuguo) Zhang United Front Games Lucas Granito United Front 
Games Cody Snyder United Front Games 1. Introduction ModNation Racers is an action packed kart racing 
game focused around user generated content, developed for the Sony PlayStation 3 console. As such we 
need a terrain system that is dynamically editable and also has a low storage foot print. In this talk 
we will give details on the system we implemented to support these goals. 2. Outline The system is based 
around two core concepts. All terrain parameters are stored in 2D maps accessible by the GPU. Editing 
is facilitated by the application of brushes on the GPU into these maps. Maps are not stored explicitly 
on the disk, rather the sequence of brush applications instead is stored in order to save on disk space. 
In this way we are able to store the terrain for a complete 1km x 1km world in only a few 100 kb. We 
store maps representing the following parameters of the terrain: . Height . Diffuse . Detail Mask 
. Shadow . Physics Type . Ground Cover Density . Geometry Mask  3. Terrain Maps Maps are stored as 
either simple 2D textures or more complex quadtree based textures. For quadtree textures, we maintain 
a small active cache of texture tiles that are regenerated in a view dependent manner as the camera moves 
through the world. Detail are given below of the individual maps in the system: 3.1 Height The height 
map is interpreted in a traditional fashion to generate the base terrain geometry. 3.2 Diffuse/Detail 
Mask/Shadow These maps are used directly by the terrain shaders to control the appearance of the terrain 
when rendered to the screen. The detail mask map is a four channel map used for blending between different 
detail maps. The diffuse map is stored in a quadtree structure that is generated at runtime as the user 
moves through the world. This allows us to support a diffuse texture resolution of 8k x 8k for a manageable 
memory cost. 3.3 Physics This map is a lookup map that defines the surface properties of the underlying 
material. Most often, this map is coupled with the diffuse and detail maps, and all are edited simultaneously. 
 3.4 Ground Cover Density The ground cover density map controls the generation of large amounts of small 
instanced geometry on the terrain surface. The four channels of density control the distribution of four 
separates classes of ground cover geometry, and are essential to creating the illusion of detail at close 
range with little storage overhead 3.5 Geometry Mask The geometry mask map is generated by a stencil 
test operation that performs a CSG operation between clip geometry and the underlying height map. This 
is used to cut tunnels through the terrain, and the mask information is used in the height field collision 
operations to ensure physical consistency. 4. Map Construction The user interacts with the system by 
painting a series of brushes over the surface of the terrain. Each of these brushes has predetermined 
parameters that control which maps it effects and in what manner. In addition to user painted brushes, 
other systems also generate geometry to use in brush operations. For example, the road system used in 
the game deforms sections of geometry by multiple spline curves. The outputs of these deformations are 
used as brushes in the terrain system in order to match terrain height to road surfaces. We are also 
able to attach brushes to any props in the world, enabling them to affect the terrain maps where they 
are placed. This can be used to place ground effects around buildings, to prevent groundcover from covering 
roads and for many other uses. All brush application is done on the GPU, allowing for many thousands 
of brushes to be placed in a small amount of time. This also allows brushes access to the full richness 
of the GPU shader language to control their effect. 5. Conclusion Each of the constituent elements of 
our system are fairly simple. We feel our novel contribution is in the single interface provided by our 
brush system for editing multiple maps, the tight integration of brushes with the rest of the scene geometry 
and the efficient application of brushes on the GPU. These elements allow users of the system to create 
scenes of substantial richness and complexity with very little effort. 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2010</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1837084</article_id>
		<sort_key>580</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>44</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Irradiance rigs]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1837026.1837084</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1837084</url>
		<abstract>
			<par><![CDATA[<p>When precomputed lighting is generated for static scene elements, the incident illumination on dynamic objects must be computed in a manner that is efficient and that faithfully captures the near- and far-field variation of the environment's illumination. Depending on the relative size of dynamic objects, as well as the number of lights in the scene, previous approaches fail to adequately sample the incident lighting and/or fail to scale. We present a principled, error-driven approach for dynamically transitioning between near- and far-field lighting. A more accurate model for sampling near-field lighting for disk sources is introduced, as well as far-field sampling and interpolation schemes tailored to each dynamic object. Lastly, we apply a flexible reflectance model to the computed illumination.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Color, shading, shadowing, and texture</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>K.8.0</cat_node>
				<descriptor>Games</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010476.10011187.10011190</concept_id>
				<concept_desc>CCS->Applied computing->Computers in other domains->Personal computers and PC applications->Computer games</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061.10010063</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures->Computational geometry</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10011007.10010940.10010941.10010969.10010970</concept_id>
				<concept_desc>CCS->Software and its engineering->Software organization and properties->Contextual software domains->Virtual worlds software->Interactive games</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002951.10003227.10003251.10003258</concept_id>
				<concept_desc>CCS->Information systems->Information systems applications->Multimedia information systems->Massively multiplayer online games</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010384</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Texturing</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2264932</person_id>
				<author_profile_id><![CDATA[81423592550]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Hong]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yuan]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[UMass Amherst]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264933</person_id>
				<author_profile_id><![CDATA[81319498527]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Derek]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nowrouzezahrai]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Toronto]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264934</person_id>
				<author_profile_id><![CDATA[81100524617]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Peter-Pike]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sloan]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Disney Interactive Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1185832</ref_obj_id>
				<ref_obj_pid>1185657</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[
Mitchell, J., McTaggart, G., and Green, C. 2006. Shading in Valve's Source Engine. In <i>Advanced Real-Time Rendering in 3D Graphics and Games, SIGGRAPH '06</i>.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[
Sloan, P.-P. 2008. Stupid Spherical Harmonics (SH) Tricks. In <i>GDC '08</i>.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[
Smedberg, N., and Wright, D. 2009. Rendering Techniques in Gears of War 2. In <i>GDC '09</i>.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>74366</ref_obj_id>
				<ref_obj_pid>74333</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[
Wallace, J. R., Elmquist, K. A., and Haines, E. A. 1989. A Ray Tracing Algorithm for Progressive Radiosity. In <i>Proceedings of SIGGRAPH '89</i>.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Irradiance Rigs Hong Yuan Derek Nowrouzezahrai Peter-Pike Sloan UMass Amherst University of Toronto 
Disney Interactive Studios Analytic / Ground Truth Mixed Rig Constant Rig Gradient Rig 3 × 2 × 2 Lattice 
Rig Figure 1: Mixed rigs capture more accurate spatial variation than traditional rigs and nearly match 
ground truth quality. Abstract When precomputed lighting is generated for static scene elements, the 
incident illumination on dynamic objects must be computed in a manner that is ef.cient and that faithfully 
captures the near-and far-.eld variation of the environment s illumination. Depending on the relative 
size of dynamic objects, as well as the number of lights in the scene, previous approaches fail to adequately 
sample the in­cident lighting and/or fail to scale. We present a principled, error­driven approach for 
dynamically transitioning between near-and far-.eld lighting. A more accurate model for sampling near-.eld 
lighting for disk sources is introduced, as well as far-.eld sampling and interpolation schemes tailored 
to each dynamic object. Lastly, we apply a .exible re.ectance model to the computed illumination. 1 Irradiance 
Rigs We are interested in rigs that reconstruct Spherical Harmonic (SH) irradiance coef.cients at points 
p in a bounded volume around a dy­namic object. The spatially-varying irradiance coef.cients li can be 
computed every frame using an abstract model function M(p, c), where c is a vector of model parameters 
for a given object. The simplest type of rig for a dynamic object is a spatially-constant SH function. 
Projecting point or spherical light sources into SH is simple [Sloan 2008], and this rig s only parameter 
is the SH projec­tion of the light sources as observed from the center of the object, lc. Concretely, 
this rig model is li = M(p, lc)= lc. The simplest spatially varying rig augments the centroid-lighting 
rig with a linear model of how lighting changes in space based on its gradient: gx, gy, gz. And so the 
model for this rig is li = M (p, {lc, gx, gy, gz})= lc +pxgx +pygy +pz gz . The gra­dient can be evaluated 
ef.ciently for spherical lights and computed numerically for other types of sources. Another common rig 
.xes a lattice around an object, where light­ing is sampled at each lattice point cj and tri-linearly 
interpolated to reconstruct irradiance on the surface of the object. Given an ar­bitrary reconstruction 
kernel b(p), this model is li = M(p, c)= b j bj (p - cj )cj , where cj is the SH irradiance at point 
cj . This can be evaluated ef.ciently in hardware with volume textures. For objects with different spatial 
scales along each axis, it can often be advantageous to employ a rig with different models for each di­mension. 
In Figure 1, the car s surface irradiance varies at different rates across its length, height and width. 
We apply a three-sample lattice model for the length, a constant model for the height, and a gradient 
model for the width. This mixed rig model s parame­ter vector is composed of only six SH vectors, yet 
generates results comparable to a 8×4×2 = 64-sample lattice rig and outperforms a 12-sample rig. We compare 
to traditional (e.g., constant [Smedberg and Wright 2009]) rigs and a ground-truth analytic evaluation. 
Error driven criteria are used to transition lights between analytic evaluation and evaluation in the 
rig. We have experimented with several other approaches, most notably, coupling radial basis functions 
with linear polynomials, combining functions and gradients in a lattice, as well as a form of hermite 
interpolation; however, all of these approaches performed poorly from a quality-per-unit-performance 
metric. 2 Disk Light Source Model When using an irradiance rig, light sources near the character are 
evaluated directly. Unfortunately, the common approximation for disk light sources [Wallace et al. 1989] 
has extremely high error in this situation. We alternatively tabulate the response to a disk light source 
in SH and use this data to evaluate irradiance at run­time. Two small textures and a simple shader suf.ce 
to perform this computation ef.ciently, and we model the problem in a coordinate frame that eliminates 
three degrees of freedom in the SH response (whereas direct tabulation would require an unwieldily 4D 
table.) 3 Wrap Lighting Wrap re.ectance models are a commonly used approach for con­vincing character 
shading. We generalize Valve s model [Mitchell et al. 2006] while retaining its important properties. 
Our new parameterized model, f(.|a) = ((cos . + a)/(1 + a))1+a, has quadratic per-band SH scaling coef.cients 
 2(1 + a) 4(1 + a) 2(1 + a)3 - 2a + a 2 f =,, . 2+ a (2 + a)(3 + a) (2+ a)(3 + a)(4 + a) This model can 
decay faster than a diffuse BRDF while retaining its .rst two derivatives, and reduces to a clamped cosine 
lobe at a =0.  References MITCHELL, J., MCTAGGART, G., AND GREEN, C. 2006. Shading in Valve s Source 
Engine. In Advanced Real-Time Rendering in 3D Graphics and Games, SIG-GRAPH 06. SLOAN, P.-P. 2008. Stupid 
Spherical Harmonics (SH) Tricks. In GDC 08. SMEDBERG, N., AND WRIGHT, D. 2009. Rendering Techniques in 
Gears of War 2. In GDC 09. WALLACE, J. R., ELMQUIST, K. A., AND HAINES, E. A. 1989. A Ray Tracing Algorithm 
for Progressive Radiosity. In Proceedings of SIGGRAPH 89. Copyright is held by the author / owner(s). 
SIGGRAPH 2010, Los Angeles, California, July 25 29, 2010. ISBN 978-1-4503-0210-4/10/0007  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2010</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1837085</article_id>
		<sort_key>590</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>45</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Practical morphological antialiasing on the GPU]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1837026.1837085</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1837085</url>
		<abstract>
			<par><![CDATA[<p>The subject of antialiasing techniques has been actively explored for the past 40 years. The classical approach involves computing the average of multiple samples for each final sample. Graphics hardware vendors implement various refinements of these algorithms. Computing multiple samples (MSAA) can be very costly depending on the complexity of the shading, or in the case of raytracing. Moreover, image-space techniques like deferred shading are incompatible with hardware implementation of MSAA since the lighting stage is decorrelated from the geometry stage. A filter based approach called Morphological Antialiasing (MLAA) was recently introduced [2009]. This technique does not need multiple samples and can efficiently be implemented on CPU using vector instructions. However, this filter is not linear and requires deep branching and image-wise knowledge which can be very inefficient on graphics hardware. We introduce an efficient adaptation of the MLAA algorithm running flawlessly on medium range GPUs.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[antialiasing]]></kw>
			<kw><![CDATA[gpu]]></kw>
			<kw><![CDATA[real-time]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.1</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2264935</person_id>
				<author_profile_id><![CDATA[81442615071]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Venceslas]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Biri]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Institut Gaspard Monge]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264936</person_id>
				<author_profile_id><![CDATA[81442615185]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Adrien]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Herubel]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Institut Gaspard Monge]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264937</person_id>
				<author_profile_id><![CDATA[81453654247]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Stephane]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Deverly]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Duran Duboi]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[
Hensley, J., Scheuermann, T., Coombe, G., Singh, M., and Lastra, A. 2005. Fast summed-area table generation and its applications. In <i>Computer Graphics Forum</i>, vol. 24, Citeseer, 547--556.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1572789</ref_obj_id>
				<ref_obj_pid>1572769</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[
Iourcha, K., Yang, J., and Pomianowski, A. 2009. A directionally adaptive edge anti-aliasing filter. In <i>Proceedings of the 1st ACM conference on High Performance Graphics</i>, ACM, 127--133.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1572787</ref_obj_id>
				<ref_obj_pid>1572769</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[
Reshetov, A. 2009. Morphological antialiasing. In <i>Proceedings of the 2009 ACM Symposium on High Performance Graphics</i>.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Practical morphological antialiasing on the GPU VenceslasBiri * AdrienHerubel StephaneDeverly InstitutGaspardMonge 
InstitutGaspardMonge DuranDuboi Figure 1: Left. 4 layers used. Middle : Result on a test image. Right 
: Comparison between MSAA 8x and our MLAA Keywords: antialiasing,gpu, real-time 1 Introduction The subject 
of antialiasing techniques has been actively explored for the past 40 years. The classical approach involves 
computing the average of multiple samples for each .nal sample. Graphics hardware vendors implement various 
re.nements of these algo­rithms. Computing multiplesamples(MSAA) canbevery costly depending on the complexity 
of the shading, or in the case of ray­tracing. Moreover, image-space techniques like deferred shading 
are incompatible with hardware implementation of MSAA since the lighting stage is decorrelated from the 
geometry stage. A .l­terbased approach calledMorphologicalAntialiasing(MLAA) was recentlyintroduced[2009]. 
Thistechniquedoes not need multiple samples and can ef.ciently be implemented on CPU using vector instructions. 
However, this .lter is not linear and requires deep branching andimage-wiseknowledge which canbe veryinef.cient 
on graphics hardware. We introduce an ef.cient adaptation of the MLAA algorithm running .awlessly on 
medium range GPUs. 2 MLAA on the GPU Overview In theMLAAalgorithm,L-shaped edges areidenti.ed in the 
.nal image and samples are blended along the formed trian­gles. The value of the blended samples depends 
on the area of the trapeze covering each sample. Line length detection We start by identifying the discontinu­ities 
between the color samples and write them in a discontinuity map where red and green components store 
respectively vertical andhorizontaldiscontinuities. Todetectdiscontinuity we switch to CIEL*a*b* color 
space, and compute a colordifference whichis a practical way to control the quality of the blending while 
avoiding the caveats of luminance-based discontinuity. Then we compute line and column lengths in two 
textures. This algorithm is based on the recursivedoubling technique usedinSATgeneration[2005] and results 
are obtainedin log(width)+ log(height) passes. Each sample of the two resulting textures contains distances 
to the dis­continuity both ways. * e-mail: biri@univ-mlv.fr e-mail: herubela@esiee.fr e-mail: sdeverly@quintaindustries.com 
Computing areas The area computing pass is the most costly since it is not linear. For each sample we 
have to identify its posi­tion in any L-shape formed by a column and a line using distances to discontinuities 
previously computed. This relative position al­lows us to estimate the area a of the sample, in one particular 
di­rection, covered by the resulting trapeze. We use a precomputed areatabletexturewhichisa512x512 .oatingpointtexturewith 
one component. It implies that we only .lter lines with a maximum of 512pixels which is enough for real 
world scenes. Therefore,in the shader, we identify any of the 8 possible L-shape and evaluate the area 
of the trapeze needed toblend sample withits4-neighbours. Blending Inthe .nalpassof thealgorithmweblend 
thevalueof the sample with its 4-neighbours using the a values computed in theprevious stage. 3 Results 
and discussion Ourimplementation adds atotal cost of34ms(3.49ms) tothe ren­dering at resolution1248x1024 
onaNVidiaGeforce8600GT(295 GTX). The GPU version tends to scale very well since the cost at1600x1200 
resolutionisonly 67.5ms(5.54ms) which represents acost98%(65.3%) higherfor144% morepixels. Wecan com­pare 
our results to a standard CPU implementation which runs in 67ms at 1024x768 and in 128ms at 1600x1200 
on a Core2Duo 2.20Ghz. Note that it does not include the costly GPU/CPU/GPU transfersin case of real 
time rendering. Weprovide an open source OpenGL/GLSL implementation of our method at http://igm.univ­mlv.fr/ 
biri/mlaa-gpu/. Further works will consist in handling artifacts introduced by .lter approaches in animation 
using techniques such as temporal coher­ence and autodetermination of thediscontinuity factor. References 
HENSLEY, J., SCHEUERMANN, T., COOMBE, G., SINGH, M., AND LASTRA,A. 2005. Fast summed-area tablegeneration 
and its applications. In Computer Graphics Forum, vol.24,Citeseer, 547 556. IOURCHA, K., YANG, J., AND 
POMIANOWSKI, A. 2009. A di­rectionally adaptive edge anti-aliasing .lter. In Proceedings of the 1st ACM 
conference on High Performance Graphics, ACM, 127 133. RESHETOV, A. 2009. Morphological antialiasing. 
In Proceedings of the 2009 ACM Symposium on High Performance Graphics. Copyright is held by the author 
/ owner(s). SIGGRAPH 2010, Los Angeles, California, July 25 29, 2010. ISBN 978-1-4503-0210-4/10/0007 
 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2010</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1837086</article_id>
		<sort_key>600</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>46</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[Curvature-dependent reflectance function for rendering translucent materials]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1837026.1837086</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1837086</url>
		<abstract>
			<par><![CDATA[<p>Simulating sub-surface scattering is one of the most effective ways for realistically synthesizing translucent materials such as marble, milk and human skin. In previous work, the method developed by Jensen et al. [2002] significantly improved the speed of the simulation. However, the process is still not fast enough to produce realtime rendering. Thus, we have developed a curvature-dependent reflectance function (CDRF) which mimics the presence of a subsurface scattering effect.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Color, shading, shadowing, and texture</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>K.8.0</cat_node>
				<descriptor>Games</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.1</cat_node>
				<descriptor>Reflectance</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010476.10011187.10011190</concept_id>
				<concept_desc>CCS->Applied computing->Computers in other domains->Personal computers and PC applications->Computer games</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10011007.10010940.10010941.10010969.10010970</concept_id>
				<concept_desc>CCS->Software and its engineering->Software organization and properties->Contextual software domains->Virtual worlds software->Interactive games</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002951.10003227.10003251.10003258</concept_id>
				<concept_desc>CCS->Information systems->Information systems applications->Multimedia information systems->Massively multiplayer online games</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372.10010376</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering->Reflectance modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010384</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Texturing</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2264938</person_id>
				<author_profile_id><![CDATA[81331497177]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Hiroyuki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kubo]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Waseda University, JSPS]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264939</person_id>
				<author_profile_id><![CDATA[81100329647]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Yoshinori]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Dobashi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Hokkaido University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264940</person_id>
				<author_profile_id><![CDATA[81100066959]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Shigeo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Morishima]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Waseda University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[
d'Eon, E., Luebke, D., and Enderton, E. 2007. Efficient rendering of human skin. 147--157.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>566619</ref_obj_id>
				<ref_obj_pid>566654</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[
Jensen, H. W., and Buhler, J. 2002. A rapid hierarchical rendering technique for translucent materials. 576--581.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1321304</ref_obj_id>
				<ref_obj_pid>1321261</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[
Kolchin, K. 2007. Curvature-based shading of translucent materials, such as human skin. 239--242.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>882423</ref_obj_id>
				<ref_obj_pid>882404</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[
Mertens, T., Kautz, J., Bekaert, P., Seidelz, H.-P., and Van Reeth, F. 2003. Interactive rendering of translucent deformable objects. 130--140.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Curvature-Dependent Re.ectance Function for Rendering Translucent Materials Hiroyuki Kubo* Yoshinori 
Dobashi Shigeo Morishima Waseda University, JSPS Hokkaido University Waseda University (a) Lambert (Marble) 
(b) Our method (Marble) (c) Our method (Jade) Figure 1: Synthesized images of marble objects (a), (b) 
and Jade (c). 1 Introduction Simulating sub-surface scattering is one of the most effective ways for 
realistically synthesizing translucent materials such as marble, milk and human skin. In previous work, 
the method developed by Jensen et al. [2002] signi.cantly improved the speed of the simula­tion. However, 
the process is still not fast enough to produce real­time rendering. Thus, we have developed a curvature-dependent 
re.ectance function (CDRF) which mimics the presence of a sub­surface scattering effect. The process 
we have developed is faster, simpler and easier to con­trol than that developed by Kolchin[2007]. In 
our approach, we make use of only one parameter .0 that represents the intensity of light scattering 
inside the material. .0 can be obtained not only by curve-.tting simulated sets of data, but can also 
be manually deter­mined by an artist. Furthermore, this is easily implementable on the GPU and doesn 
t require any complicated pre-processing or multi­pass rendering, as is often the case in this area of 
research [Mertens et al. 2003; d Eon et al. 2007]. 2Curvature-Dependent Re.ectance Functions The effects 
of subsurface scattering tend to be more noticeable on smaller, more intricate objects than on simpler, 
.atter ones. This seems to indicate that surface complexity largely determines these effects. For the 
purposes of our research, we decided to use curva­ture to represent surface complexity, combined with 
a simple local illumination model. Prior to rendering, it is necessary for subsurface scattering pro­.les 
to be acquired and interpreted for our algorithm. We rendered several spheres of varying radii to reveal 
the relationship between curvature ' and radiance Ld. The effects of subsurface scattering are simulated 
on each of the spheres using photon mapping. The spheres are illuminated by a directional light from 
the left side of the sphere, as shown in Figure-2-(a). The calculated color of each pixel on the equatorial 
line represents a relation of e (the angle be­tween the light direction and the normal vector) and the 
radiance of the sphere s particular curvature (Figure-2-(b)). We then .t a curve to the obtained data 
using CDRF fr below. fr (e, ')=(Li * g)(e) (1) CDRF is de.ned by the convolution of incident light energy 
Li and a gauss function g (e, .) to provide a bluring effect. Li and g (e, .) are given by Li(e) g (e, 
.) = = max(cos(e), 0) 1v 21.(')2 exp {- e2 2.(')2 } (2) (3) .(') represents the intensity of scattering 
inside the material, and is assumed to be in inverse propotion to radius r. Therefore, .(') . r 0 = .0'. 
To minimize RMS error between the obtained data-set and fr, we acquire the optimal parameter .0 (see 
Figure-2-(c)). e-mail: hkubo@suou.waseda.jp Figure 2: Acquiring a CDRF and the parameter .0 3 Results 
Figures1-(a) and (b) are synthesized images of a marble object, and Figure1-(c) is an example of a jade 
object. To render Figure1­ (b) we acquired the scattering parameter .0 for Figure1-(b) using curve-.tting. 
The results shown in Figure1-(c) were obtained by setting the parameters by hand. We implemented our 
algorithm as a hardware-accelerated real-time renderer with a HLSL pixel shader. 4 Conclusion We have 
developed a method for approximating the effects of sub­surface scattering using a curvature-dependent 
re.ectance function. Since the function is a local illumination model, we are able to syn­thesize realistic 
translucent materials in real-time. Furthermore, our system can be easily used to stylize subsurface 
scattering effects be­cause only one parameter .0 is required. In our future work, we will apply our 
techniques to deformable objects by computing curvature on the GPU.  References D EON, E., LUEBKE, D., 
AND ENDERTON, E. 2007. Ef.cient rendering of human skin. 147 157. JENSEN, H. W., AND BUHLER, J. 2002. 
A rapid hierarchical rendering technique for translucent materials. 576 581. KOLCHIN, K. 2007. Curvature-based 
shading of translucent mate­rials, such as human skin. 239 242. MERTENS, T., KAUTZ, J., BEKAERT, P., 
SEIDELZ, H.-P., AND VAN REETH, F. 2003. Interactive rendering of translucent de­formable objects. 130 
140. Copyright is held by the author / owner(s). SIGGRAPH 2010, Los Angeles, California, July 25 29, 
2010. ISBN 978-1-4503-0210-4/10/0007  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2010</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	</section>
	<section>
		<section_id>1837087</section_id>
		<sort_key>610</sort_key>
		<section_seq_no>15</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Interaction Omelette]]></section_title>
		<section_page_from>47</section_page_from>
	<article_rec>
		<article_id>1837088</article_id>
		<sort_key>620</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>47</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Head-mounted photometric stereo for performance capture]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1837026.1837088</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1837088</url>
		<abstract>
			<par><![CDATA[<p>Head-mounted cameras are an increasingly important tool for capturing an actor's facial performance. Such cameras provide a fixed, unoccluded view of the face. The resulting imagery is useful for observing motion capture dots or as input to existing video analysis techniques. Unfortunately current systems are typically affected by ambient light and generally fail to record subtle 3D shape changes between expressions Artistic interventions is often required to cleanup and map the captured performance onto a virtual character. We have developed a system that augments a head-mounted camera with LED-based photometric stereo. The system allows observation of the face independent of the ambient light and records per-pixel surface normal. Our data can be used to generate dynamic 3D geometry, for facial relighting, or as input of machine learning algorithms to accurately control an animated face.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.4.1</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.4.8</cat_node>
				<descriptor>Photometry</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.8</cat_node>
				<descriptor>Stereo</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.8</cat_node>
				<descriptor>Motion</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010352.10010238</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation->Motion capture</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352.10010380</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation->Motion processing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010238</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->Motion capture</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010225.10010227</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision tasks->Scene understanding</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2264941</person_id>
				<author_profile_id><![CDATA[81100556708]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Andrew]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Jones]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Southern California]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264942</person_id>
				<author_profile_id><![CDATA[81421597317]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Graham]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Fyffe]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Southern California]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264943</person_id>
				<author_profile_id><![CDATA[81440613130]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Xueming]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Southern California]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264944</person_id>
				<author_profile_id><![CDATA[81470641515]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Alex]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ma]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Southern California]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264945</person_id>
				<author_profile_id><![CDATA[81440600975]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Jay]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Busch]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Southern California]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264946</person_id>
				<author_profile_id><![CDATA[81100467115]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Mark]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bolas]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Southern California]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264947</person_id>
				<author_profile_id><![CDATA[81100086933]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>7</seq_no>
				<first_name><![CDATA[Paul]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Debevec]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Southern California]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>378090</ref_obj_id>
				<ref_obj_pid>378040</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[
Cootes, T. F., Edwards, G. J., and Taylor, C. J. 2001. Active appearance models. <i>IEEE Trans. Pattern Anal. Mach. Intell. 23</i>, 6, 681--685.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2383925</ref_obj_id>
				<ref_obj_pid>2383894</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[
Malzbender, T., Wilburn, B., Gelb, D., and Ambrisco, B. 2006. Surface enhancement using real-time photometric stereo and reflectance transformation. In <i>Rendering Techniques 2006: 17th Eurographics Workshop on Rendering</i>, 245--250.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[
Woodham, R. 1980. Photometric method for determining surface orientation from multiple images. <i>Optical Engineering 19</i>, 1, 139--144.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Head-mounted Photometric Stereo for Performance Capture Andrew Jones Graham Fyffe Xueming Yu Alex Ma 
Jay Busch Mark Bolas Paul Debevec University of Southern California Institute for Creative Technologies 
 (a) (b) (c) (d) (e) Figure 1: (a) Lighting conditions (3 directions and ambient only) captured at 120fps 
(b) Recovered surface albedo, independent of ambient light (c) Recovered surface normals (d) Geometry 
recovered by integrating surface normals (e) 3D facial rig driven by albedo and normals. Head-mounted 
cameras are an increasingly important tool for cap­turing an actor s facial performance. Such cameras 
provide a .xed, unoccluded view of the face. The resulting imagery is useful for observing motion capture 
dots or as input to existing video anal­ysis techniques. Unfortunately current systems are typically 
af­fected by ambient light and generally fail to record subtle 3D shape changes between expressions Artistic 
interventions is often required to cleanup and map the captured performance onto a vir­tual character. 
We have developed a system that augments a head­mounted camera with LED-based photometric stereo. The 
system allows observation of the face independent of the ambient light and records per-pixel surface 
normal. Our data can be used to generate dynamic 3D geometry, for facial relighting, or as input of machine 
learning algorithms to accurately control an animated face. We use a Point Grey Grasshopper camera mounted 
20 cm from the face (Fig. 2). A ring of 12 individually controlled LEDs encircles the lens. The LEDs 
repeat a sequence of three illumination patterns, followed by an unlit frame to record and subtract ambient 
light (Fig. 1a); crossed linear polarizers on the lights and lens attenuate spec­ular re.ection from 
the face. The camera and lights run at 120 fps, yielding albedo and normal estimates (Fig. 1b,c) at 30 
fps. ramps across the twelve LEDs, rotated at 0, 120, and 240 degrees. Using the full ring of lights 
provides more even illumination, re­duces shadow artifacts, and emits light from a wider area to increase 
actor comfort; our video shows results using three individual LEDs for comparison. LED .icker can be 
greatly reduced by switching lightings patterns at a faster framerate, while using shorter expo­sure 
time skip extra frames. To further eliminate distraction from the lights, we built a second lighting 
rig using invisible infrared LEDs, leveraging the broad spectral sensitivity of the camera. This provided 
similar results at the expense of some detail in the photo­metric normals due to increased subsurface 
scattering. To correct for subject motion, we compute optical .ow between similar illumination patterns 
to temporally align each set of pat­terns. Our light sources are close to the face, violating the assump­tion 
of distant illumination. To compensate, we compute per-pixel lighting directions relative to a plane 
approximating the face. To estimate 3D performance geometry, we integrate the surface nor­mals using 
Gaussian belief propagation. As expected, the geometry (Fig. 1d) suffers from low-frequency distortions 
yet reveals expres­sive performance detail in 3D. The results can be used as input to a machine learning 
algorithm to drive a facial rig with the perfor­mance after an initial training phase. For an initial 
test, we use an active appearance model [Cootes et al. 2001] to .nd blendshape weights for a given set 
of albedo and normal maps. Our initial re­sults are restricted to phonemes (Fig. 1e) and we are working 
to extend this algorithm to animating the entire face. Initial results show that analysis of normals 
and albedo provides smoother ani­mation than analysis of albedo alone. We are also working to mini­mize 
system weight, which should not be signi.cantly greater than existing rigs as the LEDs weigh just a few 
grams each. References COOTES, T. F., EDWARDS, G. J., AND TAYLOR, C. J. 2001. Active appearance models. 
IEEE Trans. Pattern Anal. Mach. Intell. 23, 6, 681 685. MALZBENDER, T., WILBURN, B., GELB, D., AND AMBRISCO, 
B. 2006. Surface enhancement using real-time photometric stereo and re.ectance transformation. In Rendering 
Techniques 2006: 17th Eurographics Workshop on Rendering, 245 250. WOODHAM, R. 1980. Photometric method 
for determining surface orientation from multiple images. Optical Engineering 19, 1, 139 144. Copyright 
is held by the author / owner(s). SIGGRAPH 2010, Los Angeles, California, July 25 29, 2010. ISBN 978-1-4503-0210-4/10/0007 
  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2010</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1837089</article_id>
		<sort_key>630</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>48</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Dynamic luminance correction for colored surfaces]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1837026.1837089</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1837089</url>
		<abstract>
			<par><![CDATA[<p>Recently, large surrounding displays are widely used for providing immersive virtual environments. Latest trends of the projection system are using our accessible rooms as a surrounding screen, because the traditional special display like a CAVE is quite large and difficult for its maintenance. Therefore, precise correction techniques for projecting images on the walls surrounding us in our daily life are strongly required. Many geometrical and luminance correction techniques were proposed for improving the quality of projected images on the walls. However, those methods have to previously measure the 3-D shapes and reflecting properties of the target walls precisely. And also, simple assumption leads to the degradation of correction quality[Fujii et al. 2005]. This complex limitation is the bottle neck of the widespread use of the immersive projection techniques using the walls.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.5</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Color, shading, shadowing, and texture</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>H.5.2</cat_node>
				<descriptor>Interaction styles (e.g., commands, menus, forms, direct manipulation)</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Virtual reality</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010866</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Virtual reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010384</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Texturing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061.10010063</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures->Computational geometry</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2264948</person_id>
				<author_profile_id><![CDATA[81100135451]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Naoki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hashimoto]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Electro-Communications]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264949</person_id>
				<author_profile_id><![CDATA[81466646064]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Akio]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Watanabe]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Electro-Communications]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1069222</ref_obj_id>
				<ref_obj_pid>1068508</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[
Fujii, K., Grossberg, M. D., and Nayar, S. K. 2005. A projector-camera system with real-time photometric adaptation for dynamic environments. <i>CVPR '05</i>, 1180.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[
Majumder, A., and Gopi, M. 2005. Modeling color properties of tiled displays. <i>Computer Graphics Forum 24</i>, 149--163.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Dynamic Luminance Correction for Colored Surfaces Naoki HASHIMOTO and Akio WATANABE The University of 
Electro-Communications*  Figure 1: Luminance correction for dynamic change of luminance properties with 
a people cut across in front of the projected wall. The correction was dynamically applied according 
to his movement, and .nally in the right edge image, he stopped and completely disappeared. 1 Introduction 
Recently, large surrounding displays are widely used for providing immersive virtual environments. Latest 
trends of the projection sys­tem are using our accessible rooms as a surrounding screen, because the 
traditional special display like a CAVE is quite large and dif.­cult for its maintenance. Therefore, 
precise correction techniques for projecting images on the walls surrounding us in our daily life are 
strongly required. Many geometrical and luminance correction techniques were proposed for improving the 
quality of projected images on the walls. However, those methods have to previously measure the 3-D shapes 
and re.ecting properties of the target walls precisely. And also, simple assumption leads to the degradation 
of correction quality[Fujii et al. 2005]. This complex limitation is the bottle neck of the widespread 
use of the immersive projection techniques using the walls. In our approach, we propose a dynamic luminance 
correction tech­nique for projecting precise images on arbitrary-colored walls. This method requires 
no previous measurement, and within an only frame of projected images, adapts to the dynamic change of 
scene s re.ectance properties, like moving curtains and peoples cut across in front of the walls. 2 
Our Approach Our proposal uses only a projector and a CCD camera. In our basic strategy, we estimate 
the response function of the projec­tor at a point as a reference, and also, we get the luminance-ratio 
(Lmap) of each pixel toward the reference point. The basic shape of the response function(Fresp) is unique 
for the projector, and the difference at each pixel is represented with the luminance ra­tio[Majumder 
and Gopi 2005]. Thus, the luminance projected at this coordinate (s, t) for input i(r, g, b) is given 
by Equation1. Luminance(s, t, i(r, g, b)) = Fresp(i(r, g, b))×Lmap(s, t) (1) In this approach, we estimate 
these two factors (Fresp &#38; Lmap)in each projected frames. At the .rst frame projection, we project 
a completely white image to the target walls, and obtain the re.ected luminance information with the 
RAW image output of the camera. Although, at this point, we do not have enough information to estimate 
the response func­tion, we only obtain the luminance-ratio (Lmap) toward the refer­ence point. *e-mail: 
{naoki@, watanabe@ims.}cs.uec.ac.jp At the second frame projection, we project the input image without 
any correction, and obtain the re.ected luminance information with the same process. Although this re.ected 
luminance is quite impor­tant for estimating the response function, there is a lack of response pairs 
for estimating that functions at each pixel. So in our approach, by using the luminance-ratio map obtained 
at the .rst frame projec­tion, measured luminance at each pixel is normalized so as to be observed at 
the reference point. And then, at the reference point, we can get enough response pairs for estimating 
the response func­tion (Fresp). Based on the assumption of the function s formula as an exponent function, 
that estimation is performed with the least­square method. After the third frame projection, we can apply 
this response func­tion at the reference point for other pixels by scaling with the luminance-ratio map, 
because the shape of the response function is basically same at every pixel. That is to say, we can achieve 
proper luminance correction for following frames. In addition, environ­ment condition may have sudden 
changes of re.ectance properties, with an exception of the constant shape of the response function. So 
we compare the predicted luminance with the estimated response function and observed luminance, and based 
on that difference, we update the luminance-ratio map in order to represent the change of the re.ectance 
properties. Then, our approach can adapt to the dynamic change of the environment condition. Figure1 
shows our luminance correction results for dynamic change of the re.ectance properties with a people 
cut across a projected surface. The clothes of the people include some variety of the re­.ectance properties, 
and it dynamically moves unpredictably. In such situation, our techniques can correct the images immediately, 
and the person himself is soon disappeared, without any prior mea­surements. The CCD camera s spec is 
640x480 pixels and 200 fps. Although the camera is fully high-speed, actual capture speed is about 33 
fps because of the exposure time of the camera and the re­sponse delay of the LCD projector. The estimation 
of the response function is processed with a GPU (GeForce GTX 295), and it can achieve totally 20 fps. 
(Supported by SCOPE, MIC, Japan) References FUJII,K.,GROSSBERG,M. D., AND NAYAR, S. K. 2005. A projector-camera 
system with real-time photometric adaptation for dynamic environments. CVPR 05, 1180. MAJUMDER,A., AND 
GOPI, M. 2005. Modeling color properties of tiled displays. Computer Graphics Forum 24, 149 163. Copyright 
is held by the author / owner(s). SIGGRAPH 2010, Los Angeles, California, July 25 29, 2010. ISBN 978-1-4503-0210-4/10/0007 
  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2010</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1837090</article_id>
		<sort_key>640</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>49</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[A laser-based system for through-the-screen collaboration]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1837026.1837090</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1837090</url>
		<abstract>
			<par><![CDATA[<p>We describe a novel system for supporting remote collaboration over shared media. The system is based on the "ClearBoard" idea of [Ishii and Kobayashi 1992] where the shared media is presented as though on a sheet of glass between local and remote participants.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>H.5.3</cat_node>
				<descriptor>Collaborative computing</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>H.5.2</cat_node>
				<descriptor>Interaction styles (e.g., commands, menus, forms, direct manipulation)</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>H.5.1</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003120.10003121.10003124</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002951.10003227.10003251</concept_id>
				<concept_desc>CCS->Information systems->Information systems applications->Multimedia information systems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003130</concept_id>
				<concept_desc>CCS->Human-centered computing->Collaborative and social computing</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2264950</person_id>
				<author_profile_id><![CDATA[81100187687]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Ian]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Robinson]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[HP Labs, Palo Alto, California]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264951</person_id>
				<author_profile_id><![CDATA[81440605768]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Kar]]></first_name>
				<middle_name><![CDATA[Han]]></middle_name>
				<last_name><![CDATA[Tan]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[HP Labs, Palo Alto, California]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264952</person_id>
				<author_profile_id><![CDATA[81456636895]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Ramin]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Samadani]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[HP Labs, Palo Alto, California]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264953</person_id>
				<author_profile_id><![CDATA[81100518692]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Bruce]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Culbertson]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[HP Labs, Palo Alto, California]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264954</person_id>
				<author_profile_id><![CDATA[81100514731]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[John]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Apostolopoulos]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[HP Labs, Palo Alto, California]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>142977</ref_obj_id>
				<ref_obj_pid>142750</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[
Ishii, H., and Kobayashi, M. 1992. Clearboard: a Seamless Medium for Shared Drawing and Conversation with Eye Contact. In <i>Proceedings of the ACM SIGCHI Conference on Human factors in Computing Systems (CHI)</i>, ACM, 525--532.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1130420</ref_obj_id>
				<ref_obj_pid>1130237</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[
Kuechler, M., and Kunz, A. 2006. Holoport - a Device for Simultaneous Video and Data Conferencing Featuring Gaze Awareness. In <i>Proceedings of the IEEE Conference on Virtual Reality</i>. IEEE Computer Society, 81--88.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[
Tan, K., Robinson, I., Samadani, R., Lee, B., Gelb, D., Vorbau, A., Culbertson, B., and Apostolopoulos, J. 2009. ConnectBoard: A Remote Collaboration System that Supports Gaze-Aware Interaction and Sharing. In <i>Proceedings IEEE Workshop on Multimedia Signal Processing (MMSP)</i>, IEEE.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 A Laser-Based System for Through-the-Screen Collaboration Ian Robinson, Kar Han Tan, Ramin Samadani, 
Bruce Culbertson, John Apostolopoulos HP Labs, Palo Alto, California, U.S.A ian.robinson@hp.com, kar-han.tan@hp.com, 
ramin.samadani@hp.com, bruce.culbertson@hp.com Figure 1: (a) Prototype system with part of enclosure 
removed, (b) captured image showing interference from backscattered projected image, (c) system in use 
with notch filter on camera (inset shows captured image, mirrored, sent to remote user)  1 Introduction 
  We describe a novel system for supporting remote collaboration over shared media. The system is based 
on the ClearBoard idea of [Ishii and Kobayashi 1992] where the shared media is presented as though on 
a sheet of glass between local and remote participants. Conventional videoconferencing solutions do not 
lend themselves well to supporting natural collaboration experiences. For communication to be effective 
in these situations we have to go beyond doing a reasonable job with eye contact, to accurately conveying 
gestures and gaze direction with respect to the shared media: the same non-verbal cues that enhance communication 
in a co-located situation. An ideal way to capture this information, along with a frontal view of the 
user, is by capturing images through the display. The ClearBoard system used 45° half-silvered mirrors 
and polarizing filters to achieve this. Recent work, e.g. HoloPort [Kuechler and Kunz 2006] and ConnectBoard 
[Tan et al. 2009], has focused on using holographic diffusing screens developed for electronic signage 
applications. These screens are naturally transparent except to light from a particular point, at which 
the projector is placed, where the holographic film s encoded optical elements steer the projected light 
out through a designed viewing angle. However, some fraction of the projected image is scattered off 
the back of the screen and is visible to the camera, interfering with the desired light coming from the 
scene beyond the screen. In order to prevent the camera picking up this portion of the projected image, 
various multiplexing schemes have been implemented: temporal in the HoloPort, wavelength division in 
the ConnectBoard. However, all of these methods, including Ishii s original polarization scheme, suffer 
from the usual multiplexing inefficiencies whereby significantly less than half the light available makes 
it to the screen and into the camera lens. 2 Our Approach  Our approach is to exploit wavelength multiplexing 
in conjunction with a laser-based projection display to avoid these issues. The only filter used in the 
system is a notch filter just before the camera, which is used to remove the three laser frequencies 
(red, green and blue). As these notches can be very narrow, roughly 80% or more of the visible spectrum 
is available to the camera. No filters are used with the projector so 100% or its output is used. To 
test this approach we used a Mitsubishi LaserVue L65-A90 as a convenient source of an off-the-shelf laser 
rear-projection system that happened to roughly correspond to the acceptance angles of an available holographic 
diffusing screen (a 55deg HOPS Glass screen from Sax3D). The TV was dismantled, it s screen removed and 
replaced with the holographic screen, and its electronics relocated to allow a camera a clear line of 
sight through the screen from behind. An off-the-shelf triple notch filter was attached to the camera 
to block the laser wavelengths. The resulting system was enclosed to prevent stray light from entering 
the reconfigured rear-projection system. 3 Results  The images above show the camera output before 
and after the notch filters are placed in front of the camera. Note that the backscattered light is no 
longer visible in the second image. The blue-ish disc in the filtered image is an artifact due to the 
off-the-shelf filter s blue notch not being ideally matched with the blue laser wavelength over the range 
of incidence angles at the camera. References ISHII, H., AND KOBAYASHI, M. 1992. Clearboard: a Seamless 
Medium for Shared Drawing and Conversation with Eye Contact. In Proceedings of the ACM SIGCHI Conference 
on Human factors in Computing Systems (CHI), ACM, 525 532. KUECHLER, M., AND KUNZ, A. 2006. Holoport 
- a Device for Simultaneous Video and Data Conferencing Featuring Gaze Awareness. In Proceedings of the 
IEEE Conference on Virtual Reality. IEEE Computer Society, 81 88. TAN, K., ROBINSON, I., SAMADANI, R., 
LEE, B., GELB, D., VORBAU, A., CULBERTSON, B., AND APOSTOLOPOULOS, J. 2009. ConnectBoard: A Remote Collaboration 
System that Supports Gaze-Aware Interaction and Sharing. In Proceedings IEEE Workshop on Multimedia Signal 
Processing (MMSP), IEEE. 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2010</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1837091</article_id>
		<sort_key>650</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>50</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[A spatial workbench for physically-based sound]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1837026.1837091</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1837091</url>
		<abstract>
			<par><![CDATA[<p>In this research, we embed physically-based sound models in a spatial, interactive, multi-modal context to make a workbench for exploring new musical ideas. Instruments made with the workbench make sounds that are physically plausible, and they may be controlled in familiar ways, but they go beyond the physical to do things impossible or difficult to do in the real world. Our system outputs both real-time sound and real-time animation in response to users' actions.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Physically based modeling</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Animation</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>H.5.2</cat_node>
				<descriptor>Interaction styles (e.g., commands, menus, forms, direct manipulation)</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003741.10003742.10003745</concept_id>
				<concept_desc>CCS->Mathematics of computing->Continuous mathematics->Topology->Geometric topology</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352.10010379</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation->Physical simulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2264955</person_id>
				<author_profile_id><![CDATA[81448593411]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Benjamin]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Schroeder]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The Ohio State University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264956</person_id>
				<author_profile_id><![CDATA[81100414668]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Richard]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Parent]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The Ohio State University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264957</person_id>
				<author_profile_id><![CDATA[81100288519]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Marc]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ainger]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The Ohio State University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1264763</ref_obj_id>
				<ref_obj_pid>1264357</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Kaltenbrunner, M., Jorda, S., Geiger, G., and Alonso, M. 2006. The reactable*: A collaborative musical instrument. <i>Enabling Technologies, IEEE International Workshops on</i>, 406--411.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 A Spatial Workbench for Physically-Based Sound Benjamin Schroeder* Richard Parent Marc Ainger The Ohio 
State University  (a) Two strings (b) Rhythmic particle emitters 1 Introduction In this research, we 
embed physically-based sound models in a spa­tial, interactive, multi-modal context to make a workbench 
for ex­ploring new musical ideas. Instruments made with the workbench make sounds that are physically 
plausible, and they may be con­trolled in familiar ways, but they go beyond the physical to do things 
impossible or dif.cult to do in the real world. Our system outputs both real-time sound and real-time 
animation in response to users actions. Figure (a) shows a simple example involving two guitar strings; 
the topmost one has just been plucked by dragging the mouse across it. The user of the system sees how 
the string moves and hears how it sounds at the same time. Plucking at different points and with different 
speeds produces slightly different sounds and images, mimicking the response of a real string. Pressing 
in the middle of a string shortens it in a way analogous to fretting a real guitar string. The second 
string is shorter and so has a higher-pitched sound. Moving the endpoints of the strings changes their 
lengths and thus their pitches. We use simple procedural elements to give the system a richer vo­cabulary 
and to extend its capabilities beyond the strictly physi­cal. Figure (b) shows rhythmic particle emitters, 
each of which acts as a kind of active metronome, sending particles which pluck the strings. This might 
be used to set up a background beat, freeing the user to perform using other parts of the instrument. 
Furthermore, the design of the instrument itself becomes a locus for performance: the user can change 
the instrument as it is operating, varying the pitch of existing strings, introducing new ones, or moving 
things around. The spatial setting suggests further ideas. For example, multi­ple strings might be linked 
together to produce resonance (with a physically-based connection) or sequenced sounds (with a more al­gorithmic 
connection). Emitters might rotate or move over time, or respond to various stimuli. Other models, such 
as bars or plates or even sampled sounds, could be placed in the same space and linked together by either 
explicit connection or spatial proximity. The virtual nature of the system suggests ideas as well. For 
exam­ple, a partial mesh of strings might break apart and re-form in new con.gurations over time, or 
particles might accrete like barnacles on a string, changing its motion and sound. *e-mail: benschroeder@acm.org 
e-mail: parent@cse.ohio-state.edu e-mail: ainger.1@osu.edu (c) In operation on a touch table (d) Particles 
sent by a breath sensor 2 Multi-modal Controllers Multitouch input is a natural choice for a system 
such as the one de­scribed here. Figure (c) shows the system being used on a diffuse­illumination multitouch 
table. In this setting, it is simple to fret and pluck a string at the same time, and multiple people 
may interact with the same space at once. Using a typical rotate gesture on a string endpoint changes 
its tension as if a guitar tuning key were being turned; the same gesture can be used to place emitters 
at dif­ferent angles. We are also exploring ways to incorporate tangible controllers into the basic touch-based 
system, giving input that not only speci.es a place at a particular time but also a separate varying 
input level. Simple examples might involve a bend sensor used as a sort of whammy bar or a plunger setup 
used to blow particles into the space. An intermediate step is to use sensors that are represented virtually 
in the space, rather than using tangible objects. Figure (d) shows the result of using a MIDI-based breath 
sensor to send a .eld of particles from an emitter. 3 Relation to Previous Work There are many ways 
to create synthetic sound. Physically-based sound models are both rich in output and controllable in 
familiar ways. Unlike other approaches, they provide a natural mapping to quantities such as positions, 
velocities, and forces, making them a good choice for an embedding such as the one considered here. A 
physically-based setting for objects, on a touch table or other­wise, is not itself a new idea. Our concern, 
however, is mainly in the use of space and physics as a setting for the use of physically­based sound 
models and in the exploration of musical ideas. Several other systems, including the popular Re­actable 
[Kaltenbrunner et al. 2006], have situated musical objects in spatial relation, and these are certainly 
an inspiration for our work. The objects in these systems are based on signal processing and abstract 
notation, and use some spatial ideas in their conception of rhythm and interconnection. Our research 
uses physically based models and a more concrete, physical notion of space. References KALTENBRUNNER, 
M., JORDA, S., GEIGER, G., AND ALONSO, M. 2006. The reactable*: A collaborative musical instrument. Enabling 
Technologies, IEEE International Workshops on, 406 411. Copyright is held by the author / owner(s). SIGGRAPH 
2010, Los Angeles, California, July 25 29, 2010. ISBN 978-1-4503-0210-4/10/0007  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2010</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	</section>
	<section>
		<section_id>1837092</section_id>
		<sort_key>660</sort_key>
		<section_seq_no>16</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Fur, Feathers and Trees]]></section_title>
		<section_page_from>51</section_page_from>
	<article_rec>
		<article_id>1837093</article_id>
		<sort_key>670</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>51</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Furtility]]></title>
		<subtitle><![CDATA[dynamic grooming for <i>Wolfman</i>]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1837026.1837093</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1837093</url>
		<abstract>
			<par><![CDATA[<p>For <i>Wolfman</i>, MPC was presented with the challenge of fulfilling the director's creative vision of gradual and graphic werewolf transformations, requiring significant enhancements to Furtility, the company's hair/fur system. Originally developed for <i>10,000 BC</i> to create photorealistic mammoths and heavily used since on everything from seaweed to digital double actors, Furtility was extended to handle growing and transforming hair in almost 50 shots for <i>Wolfman</i>, many of which were extreme close-ups.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.4</cat_node>
				<descriptor>Application packages</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Animation</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Color, shading, shadowing, and texture</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010384</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Texturing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2264958</person_id>
				<author_profile_id><![CDATA[81466647316]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Damien]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Fagnou]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MPC]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264959</person_id>
				<author_profile_id><![CDATA[81466647821]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[James]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Leaning]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MPC]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1179902</ref_obj_id>
				<ref_obj_pid>1179849</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Haddon, J., and Griffiths, D. 2006. A system for crowd rendering. In <i>SIGGRAPH '06: ACM SIGGRAPH 2006 Sketches</i>, ACM, New York, NY, USA, 42.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Furtility: Dynamic Grooming for Wolfman DamienFagnou* James Leaning MPC MPC Abstract For Wolfman, 
MPC was presented with the challenge of ful.ll­ing the director s creative vision of gradual and graphic 
werewolf transformations, requiring signi.cant enhancementstoFurtility,the company s hair/fur system. 
Originally developed for 10,000 BC to create photorealistic mammoths and heavily used since on ev­erything 
from seaweed to digital double actors, Furtility was ex­tended to handle growing and transforming hair 
in almost 50 shots for Wolfman, manyof which were extreme close-ups. Figure 1: Final rendered image 
c&#38;#169;2010 Universal Pictures 1 A scriptable and modular fur-system Furtilitywasbuilt upon MPC s3D 
library, Muggins, and its script­ing engine, Giggle, as described in [Haddon and Grif.ths 2006]. Both 
of these allow the software to be easily extended and rapidly customized for speci.c show needs. This 
foundationgave us the freedom to improve the software over the years to meet new chal­lenges without 
having to modify much of its core engine. The Furtility engine itself is solely built on proprietary 
or open­source technologiesandthenexposedvia interfacesforthemain3D packages we are using: Maya and RenderMan. 
We also have the ability to render high quality previews using our in-house OpenGL renderer: MugginsGL. 
Althougha groomisbuiltusingalarge numberof nodes connected in a graph, the artists are presented with 
a simple yet powerful in­terface where theycan layer the deformation of the fur using GOPs (Geometry 
OPerators) that add shape or shading properties to the fur, like length, direction, comb, color etc. 
ForWolfman we addedaverypowerful guide curve GOP thatgives artists the level of precise control needed 
when creating creatures with long hair, or digital doubles where the hair style of an actor needstobefaithfully 
reproduced. *e-mail: damien-f@moving-picture.com e-mail: james-l@moving-picture.com 2 Adding a time 
component to grooms Using Furtility as previously described, our artists were able to create a very convincing 
wolfman/werewolf that in many ways looked more realistic than the prosthetic version. But for MPC the 
main challenge of this .lm project was to create a transforma­tion sequence where the real actor would 
seamlessly evolve into the full CG character. The transformation is slow, very detailed and fullscreen. 
To achieve this we introduced a mix of animatable groom propertiesanddynamictexturesthatwouldgiveartistsaway 
to shape the groom over time. One of the components of the new tool-kit was an improved ver­sion of our 
procedural texture engine with added support for tex­ture types based on animated maps and particle or 
geometry caches. Thisgavethe artistthe abilityto createinputtexturesforthe groom that would evolve based 
on blood .ow, geometry stretch maps and other data generated during the transformation. These maps could 
then be used to drive hair coverage and length, allowing our clean­shavendigital actortogrowafullfaceoffur. 
As a .nal layer of deformation we added a dynamic simulation to the hair using an algorithm that allowed 
a stable distribution of guide curves, even on changing surfaces. This secondary motion, while somewhat 
subtle, added a critical degree of visual realism when layered in with the transitioning hairstyles. 
 Figure 2: Groomed hairstyles for pre-and post-transformation  Summary and Acknowledgements Delivering 
the shots on Wolfman meant signi.cant new develop­ment of our fur system, from the guide curve editor 
to our 3D tex­ture engine. All of these extensions are being reused for upcoming .lm projectsand areexpectedtobea 
permanent .xturein our tool­set. Thanks areowed to Jimmi Gravesen and JonathanWills for their contribution 
to previous versions of Furtility. References HADDON,J., AND GRIFFITHS,D. 2006.A system for crowd rendering. 
In SIGGRAPH 06: ACM SIGGRAPH 2006 Sketches, ACM,NewYork,NY, USA, 42. Copyright is held by the author 
/ owner(s). SIGGRAPH 2010, Los Angeles, California, July 25 29, 2010. ISBN 978-1-4503-0210-4/10/0007 
  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2010</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1837094</article_id>
		<sort_key>680</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>52</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Feathers for mystical creatures]]></title>
		<subtitle><![CDATA[creating pegasus for <i>Clash of the Titans</i>]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1837026.1837094</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1837094</url>
		<abstract>
			<par><![CDATA[<p>One of the major challenges for <i>Clash of the Titans</i> was to develop a highly sophisticated feather system used on several flying characters in over a hundred shots. The system had to reliably provide photo-realistic results over a large range of detail levels from characters in the background to those passing just in front of the camera. The feather system was tightly integrated with MPC's existing pipeline and allowed for a large amount of artistic freedom and control efficiently, while requiring almost no technical background to use.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.6.8</cat_node>
				<descriptor>Animation</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.6.5</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010341.10010349.10011310</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Simulation types and techniques->Simulation by animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341.10010342</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Model development and analysis</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2264960</person_id>
				<author_profile_id><![CDATA[81466647821]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[James]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Leaning]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MPC]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264961</person_id>
				<author_profile_id><![CDATA[81466647316]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Damien]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Fagnou]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MPC]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Feathers for Mystical Creatures: Creating Pegasus for Clash of the Titans James Leaning* DamienFagnou 
MPC MPC  Figure 1: Final rendered image c&#38;#169;2010 Warner Bros. Pictures Abstract One of the 
major challenges for Clash of the Titans was to develop a highly sophisticated feather system used on 
several .ying char­acters in over a hundred shots. The system had to reliably provide photo-realistic 
results over a large range of detail levels from char­acters in the background to those passing just 
in front of the cam­era. The feather system was tightly integrated with MPC s existing pipelineandallowedforalarge 
amountof artistic freedomand con­trol ef.ciently, while requiring almost no technical background to use. 
1 Feather Designing The foundation of the feather system shigh quality had to start with the initial 
design of individual feathers. It was essential that these feathers had to look realistic even very close 
up. To achieve this each feather barb was procedurally created as curves in full un­restricted three 
dimensions. This approach allowed these .uffy feathers to render with a full sense of volume. Fortunately 
there was already an advanced system for creating styled curves in the form of the Furtility fur/hair 
creation system. A feature was added to distribute fur on curves. This system al­lowed the groom artists 
to design feathers with the same powerful and .exible tools used for complex hair grooms. These feather 
templates could be used to create a in.nite set of feathervariationsat render-timeallowingevery feathertobediffer­ent. 
 2 Feather Distribution Additional targets were added to the the fur system that allowed the distribution 
of feather templates on polygon surfaces using ex­isting hair grooming tools. Feather distribution could 
use a variety of methods ranging from highly automated distribution to bespoke individual feather placement 
and sculpting. Tools were created to allow feathers to quickly align to surfaces, twist and stretch using 
control texture maps. These maps were created by the MPC 3D painting system.Weight paintingwas addedtothe 
painting system to allow additional .exibility in style selection. *e-mail: james-l@moving-picture.com 
e-mail: damien-f@moving-picture.com Figure 2: Final rendered image c&#38;#169;2010 Warner Bros. Pictures 
 These feather distributiontoolsallowedthe grooming artisttowork with real-time visual feedback with 
a selection of visualization op­tions using actual feather designs. This previewing would actually match 
the .nal renders. 3 Animation and Dynamics The animators rig used a simple NURBS plane as a substitution 
for feather wings. This surface was then substituted with feathers at render-time, where additional procedural 
secondary animation could be applied for wind effects. For hero shots a technical Animation rig was used 
to allow a pass of dynamics, additional animation and tweaks which allowed the adjustment of proxies 
based on the actual feathers. 4 Rendering Due to the vast amounts of geometric data required to create 
such detailedeffects,all feathers were createdat render-timeina custom highly ef.cient multi-threaded 
PRMan DSO. In this process only blocks of feathers being rendered currently were generated and re­tained 
in memory at any one time, allowing each resulting .nal Pegasus render to contain tens of millions of 
curves. As only a compact set of parameters and maps were saved on disk, theycould be reinterpreted at 
anyLOD (level of detail) at render­time, tailored foreach shots requirements usingasimple slider. Ad­ditional 
automatedLOD control would alter detail on a per feather basis based on the distance from camera and 
amount of motion blur. The feathers used a modi.ed version of the existing advanced hair shader. While 
occlusion was traced against polygonal versions of the feathers generated automatically at render-time. 
 5 Summary The Clash of the Titans feathers presented many new and unique challenges that tested every 
part of our software to the limit. To achieve the high standard of work required in the short time-frame 
itwas essentialtobuildonexisting toolsand pipelinesin ingenious ways,to guaranteethefastdeliveryofhighqualityworkwhilststill 
allowing the artist creative freedom and toolfamiliarity within the their shots. Copyright is held by 
the author / owner(s). SIGGRAPH 2010, Los Angeles, California, July 25 29, 2010. ISBN 978-1-4503-0210-4/10/0007 
  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2010</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1837095</article_id>
		<sort_key>690</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>53</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Art-directing Disney's <i>Tangled</i> procedural trees]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1837026.1837095</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1837095</url>
		<abstract>
			<par><![CDATA[<p>Creating stylized trees with hundreds of thousands of leaves is typically a painstaking task that requires hours of artist time. In Walt Disney's animated feature film <i>Tangled</i>, we faced the challenge of populating dense forests with animated trees on which artists could quickly iterate to meet an art-directed look. We designed a system of authoring trees based around a language of hierarchical curves. Our system lets artists interactively sketch out a base skeleton representation of a tree and grow procedural twigs and leaves out to a canopy shell by tweaking a limited number of parameters.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Curve, surface, solid, and object representations</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor>Line and curve generation</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>D.3.2</cat_node>
				<descriptor>Specialized application languages</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.4</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010401</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Volumetric models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010399</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Parametric curve and surface models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010399</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Parametric curve and surface models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10011007.10011006.10011050.10011023</concept_id>
				<concept_desc>CCS->Software and its engineering->Software notations and tools->Context specific languages->Specialized application languages</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Languages</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2264962</person_id>
				<author_profile_id><![CDATA[81442614845]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Arthur]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Shek]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Walt Disney Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264963</person_id>
				<author_profile_id><![CDATA[81466647309]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Dylan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lacewell]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Walt Disney Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264964</person_id>
				<author_profile_id><![CDATA[81100351513]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Andrew]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Selle]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Walt Disney Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264965</person_id>
				<author_profile_id><![CDATA[81341497366]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Daniel]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Teece]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Walt Disney Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264966</person_id>
				<author_profile_id><![CDATA[81466645160]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Tom]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Thompson]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Walt Disney Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1360643</ref_obj_id>
				<ref_obj_pid>1399504</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Au, O. K.-C., Tai, C.-L., Chu, H.-K., Cohen-Or, D., and Lee, T.- Y. 2008. Skeleton extraction by mesh contraction. In <i>SIGGRAPH '08: ACM SIGGRAPH 2008 papers</i>, ACM, New York, NY, USA, 1--10.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Art-directing Disney s Tangled Procedural Trees Arthur Shek Dylan Lacewell Andrew Selle Daniel Teece 
Tom Thompson Walt Disney Animation Studios  Figure 1: Left to right: visual development art-direction 
target (courtesy Kevin Nelson), curve representation, procedural foliage to canopy shell, look dev element 
render (courtesy Charles Colladay), stylized render from Tangled (sparse leaf variant to support story) 
1 Introduction Creating stylized trees with hundreds of thousands of leaves is typ­ically a painstaking 
task that requires hours of artist time. In Walt Disney s animated feature .lm Tangled, we faced the 
challenge of populating dense forests with animated trees on which artists could quickly iterate to meet 
an art-directed look. We designed a system of authoring trees based around a language of hierarchical 
curves. Our system lets artists interactively sketch out a base skeleton rep­resentation of a tree and 
grow procedural twigs and leaves out to a canopy shell by tweaking a limited number of parameters. 2 
Curves and Geometry We determined that the base representation of trunks, branches and twigs would be 
a system of curves with a notion of hierarchy. This allowed us to write modular pieces of a tree pipeline 
based around a common structure. Curves also gave us the additional ability to quickly setup animation 
rigs or dynamic simulations where needed. To create the desired curve and geometric structure of trees, 
we designed a number of Maya tools. Certain hero trees called for hand-modeled 3D geometry. We authored 
a tool called Skeletor to extract curves from the model via a mesh skeletonization algorithm [Au et al. 
2008]. This allows us to map complex geometry to a net­work of curves. We also needed software that would 
allow artists to quickly generate curves to build up a tree. TreeSketch is a plug-in that enables artists 
to interactively draw a hierarchy of curves (see Fig. 1, 2nd image). From hand-sketched or procedurally 
generated curves, we needed to perform the inverse operation of Skeletor, to automatically build geometry 
from a base curve skeleton. Disney s pipeline necessitated a quad mesh input and we also desired closed, 
manifold and smooth meshes. Our Treeman tool was implemented to meet these speci.cations, while meeting 
the challenging require­ment of robustly handling many close and competing junctions. 3 Procedural Growth 
A standard technique to create procedural branching uses L­systems. We found L-systems require a language 
and grammar that is unintuitive for artists to describe a desired look. The alternative of hardcoding 
certain rules and giving artists sliders to dial in mul­tipliers and random seeds also limits the achievable 
range of looks. To get around these limitations, we wrote an interactive particle marching engine called 
Dendro. The engine instances particles on the base tree curves according to user exposed parameters. 
At each timestep, particles can either branch or march out a certain distance in a particular direction, 
in.uenced by user parameters. The history of the timestepped particles describe points on a curved path 
that result in a procedural branching twig structure. Because each point on a twig has an associated 
lifetime, we can use that information to derive twig width and to instance leaves at desired points. 
Dendro s particle method resulted in a number of additional bene­.ts. Artists could further art-direct 
growth on trees by using canopy shells. These polygonal shells were transformed into voxel regions and 
used to kill marching particles within a threshold distance, let­ting artists clip tree growth loosely 
or tightly to a desired shape. The particle technique also allowed us to include natural tropism effects, 
such as dealing with environmental factors like gravity, growing to­wards light, or around obstacles. 
Artists may dial in wind direction, frequency and amplitude to add procedural animation. Rounding out 
the core featureset of Dendro is the ability for users to control leaf orientation. All parameters are 
generic enough that they can be used to generate variant trees of the same species simply by chang­ing 
the random seed. Dendro features are exposed to artists at the asset creation level via an interactive 
frontend within Maya. 4 Rendering Twigs and leaves were rendered with a Renderman procedural that called 
the Dendro engine. For direct lighting, we used two-sided shaders and deep shadow maps. For indirect 
lighting and occlusion, we baked static precomputed radiance transfer (PRT) textures with a custom raytracer. 
The art direction and the forgiving nature of foliage let us optimize the baking aggressively; we used 
order 3 PRT, simpli.ed leaves to single quads, computed one sample per leaf, and reused the static data 
even during animation. We used stochastic pruning on the leaves and twigs to cull out appropriate level 
of detail for rendering large groves of trees. On top of that, we achieved signi.cant acceleration of 
renders by using a brickmap representation to optimize heavily populated scenes. Together, this suite 
of tools has dramatically increased our studio s ability to generate and iterate on largely procedural, 
yet highly art­directable trees and foliage. Figure 2: Left: Tree element from Tangled, 1.02 million 
leaves (courtesy Larry Wu), Right: Leaf detail References AU, O. K.-C., TAI, C.-L., CHU, H.-K., COHEN-OR, 
D., AND LEE, T.- Y. 2008. Skeleton extraction by mesh contraction. In SIGGRAPH 08: ACM SIGGRAPH 2008 
papers, ACM, New York, NY, USA, 1 10. Copyright is held by the author / owner(s). SIGGRAPH 2010, Los 
Angeles, California, July 25 29, 2010. ISBN 978-1-4503-0210-4/10/0007   
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2010</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	</section>
	<section>
		<section_id>1837096</section_id>
		<sort_key>700</sort_key>
		<section_seq_no>17</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Touchy-Feely]]></section_title>
		<section_page_from>54</section_page_from>
	<article_rec>
		<article_id>1837097</article_id>
		<sort_key>710</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>54</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Content-adaptive parallax barriers for automultiscopic 3D display]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1837026.1837097</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1837097</url>
		<abstract>
			<par><![CDATA[<p>We optimize the performance of automultiscopic barrier-based displays, constructed by stacking a pair of LCD panels. To date, such displays have conventionally employed heuristically-determined parallax barriers, containing a fixed array of slits or pinholes, to provide view-dependent imagery. While recent methods adapt barriers to one or more viewers, we show that both layers can be adapted to the multi-view content as well. The resulting <i>content-adaptive parallax barriers</i> increase display brightness and frame rate. We prove that any 4D light field created by dual-stacked LCDs is the tensor product of two 2D mask functions. Thus, a pair of 1D masks only achieves a rank-1 approximation of a 2D light field. We demonstrate higher-rank approximations using temporal multiplexing.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.3</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>H.5.2</cat_node>
				<descriptor>Screen design (e.g., text, graphics, color)</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003120.10003123</concept_id>
				<concept_desc>CCS->Human-centered computing->Interaction design</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10011666</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Touch screens</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2264967</person_id>
				<author_profile_id><![CDATA[81319495323]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Douglas]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lanman]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Brown University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264968</person_id>
				<author_profile_id><![CDATA[81442601918]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Matthew]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hirsch]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MIT Media Lab]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264969</person_id>
				<author_profile_id><![CDATA[81466646505]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Yunhee]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kim]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MIT Media Lab]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264970</person_id>
				<author_profile_id><![CDATA[81100022847]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Ramesh]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Raskar]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MIT Media Lab]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Kim, Y., Kim, J., Kang, J.-M., Jung, J.-H., Choi, H., and Lee, B. 2007. Point light source integral imaging with improved resolution and viewing angle by the use of electrically movable pinhole array. <i>Optics Express 15</i>, 26, 18253--18267.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Konrad, J., and Halle, M. 2007. 3-D displays and signal processing. <i>IEEE Signal Processing Magazine 24</i>, 6, 97--111.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Lee, D. D., and Seung, H. S. 1999. Learning the parts of objects by non-negative matrix factorization. <i>Nature 401</i>, 788--791.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Content-Adaptive Parallax Barriers for Automultiscopic 3D Display Douglas Lanman* Matthew Hirsch Yunhee 
Kim Ramesh Raskar Brown University MIT Media Lab MIT Media Lab MIT Media Lab  Figure 1: Automultiscopic 
3D display with content-adaptive parallax barriers. (Left) A 4D light .eld, represented as a set of oblique 
projections of a synthetic scene, is displayed using our dual-stacked LCD prototype, with corresponding 
photographs shown in the overlaid region. (Right) A pair of content-adaptive parallax barriers, drawn 
from a rank-15 decomposition of the reshaped 4D light .eld matrix. Such masks allow increased display 
brightness and frame rate, when compared to conventional parallax barriers [Konrad and Halle 2007]. Abstract 
We optimize the performance of automultiscopic barrier-based dis­plays, constructed by stacking a pair 
of LCD panels. To date, such displays have conventionally employed heuristically-determined parallax 
barriers, containing a .xed array of slits or pinholes, to pro­vide view-dependent imagery. While recent 
methods adapt barriers to one or more viewers, we show that both layers can be adapted to the multi-view 
content as well. The resulting content-adaptive par­allax barriers increase display brightness and frame 
rate. We prove that any 4D light .eld created by dual-stacked LCDs is the tensor product of two 2D mask 
functions. Thus, a pair of 1D masks only achieves a rank-1 approximation of a 2D light .eld. We demon­strate 
higher-rank approximations using temporal multiplexing. 1 Content-Adaptive Parallax Barriers We de.ne 
a pair of 2D masks f[i, j] and g[k, l], corresponding to the images displayed on the front and rear LCD 
panels, respectively. A 2D slice of the 4D light .eld is given by the outer product L[i, k]= f[i] . g[k]= 
f[i]g T[k]. (1) Similarly, the complete 4D light .eld is given by the tensor product L[i, j, k, l]= f[i, 
j] . g[k, l]. (2) These expressions imply a .xed mask pair only produces a rank­1 approximation of a 
2D light .eld matrix. To our knowledge, this limitation has not been previously described for dual-layer 
displays. Conventional parallax barriers result in reduced spatial resolution and image brightness. Recently, 
translated barriers have been pro­posed to eliminate spatial resolution loss [Kim et al. 2007]; here, 
a high-speed LCD sequentially displays a series of translated bar­riers. If the complete mask set is 
displayed faster than the .icker fusion threshold, no spatial resolution loss will be perceived. *e-mail: 
dlanman@brown.edu We generalize the concept of temporal multiplexing by considering all possible mask 
pairs. Any sequence of T mask pairs represents (at most) a rank-T decomposition of a 2D light .eld matrix 
as TT TT L[i, k]= ft[i] . gt[k]= ft[i]gt T[k]. (3) t=1 t=1 Thus, time-multiplexed light .eld display 
can be cast as a matrix (or more generally a tensor) approximation problem. Speci.cally, the light .eld 
matrix must be decomposed as L FG, (4) where F and G are Ni×T and T ×Nk, respectively. Since each mask 
must be non-negative, we seek a decomposition such that 1 arg min IL - FGI2 (5) W, for F, G = 0. 2 F,G 
Unlike conventional barriers, we allow a .exible .eld of view tuned to one or more viewers by specifying 
elements of the weight matrix W. General 4D light .elds are handled by reordering them as 2D matrices, 
whereas 2D masks are reordered as vectors. Equation 5 can be solved using non-negative matrix factoriza­tion 
[Lee and Seung 1999], with typical results shown above. In conclusion, we propose the resulting content-adaptive 
parallax barriers as a generalization of conventional barriers, in which both layers are jointly optimized 
depending on the multi-view content.  References KIM, Y., KIM, J., KANG, J.-M., JUNG, J.-H., CHOI, H., 
AND LEE, B. 2007. Pointlightsourceintegralimagingwithimproved resolution and viewing angle by the use 
of electrically movable pinhole array. Optics Express 15, 26, 18253 18267. KONRAD, J., AND HALLE, M. 
2007. 3-D displays and signal processing. IEEE Signal Processing Magazine 24, 6, 97 111. LEE, D.D., AND 
SEUNG,H.S.1999.Learningthepartsofobjects by non-negative matrix factorization. Nature 401, 788 791. Copyright 
is held by the author / owner(s). SIGGRAPH 2010, Los Angeles, California, July 25 29, 2010. ISBN 978-1-4503-0210-4/10/0007 
  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2010</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1837098</article_id>
		<sort_key>720</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>55</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[NiCE formula editor]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1837026.1837098</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1837098</url>
		<abstract>
			<par><![CDATA[<p>Interactive Whiteboards are becoming an increasingly important component of face-to-face meetings, particularly in educational settings. Still, non-interactive surfaces have remained the standard for mathematics-oriented discussions in part due to the lack of robust algorithms for recognizing handwritten mathematical expressions. Thus teachers, for example, are typically burdened with drawing visualizations by hand to reinforce abstract and complex mathematical problems. Our work expands upon the notion of mathematical sketching [1] by supporting both pen and touch interactions on an interactive whiteboard to write expressions, to perform algebraic transformations, and to drive sophisticated 2D and 3D illustrations. By incorporating lightweight touch-based interactions, students can "play" with formulas and explore mathematics from a fundamentally interactive perspective that we believe many will find to be engaging and enjoyable.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>K.3.1</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>J.2</cat_node>
				<descriptor>Mathematics and statistics</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>H.5.2</cat_node>
				<descriptor>Input devices and strategies (e.g., mouse, touchscreen)</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010383</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Image processing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010432.10010442</concept_id>
				<concept_desc>CCS->Applied computing->Physical sciences and engineering->Mathematics and statistics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10011666</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Touch screens</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003125</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010489</concept_id>
				<concept_desc>CCS->Applied computing->Education</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2264971</person_id>
				<author_profile_id><![CDATA[81319495417]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Jakob]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Leitner]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Media Interaction Lab]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264972</person_id>
				<author_profile_id><![CDATA[81442613741]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Christian]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Rendl]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Media Interaction Lab]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264973</person_id>
				<author_profile_id><![CDATA[81442617361]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Florian]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Perteneder]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Media Interaction Lab]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264974</person_id>
				<author_profile_id><![CDATA[81421597934]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Adam]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gokcezade]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Media Interaction Lab]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264975</person_id>
				<author_profile_id><![CDATA[81319500586]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Thomas]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Seifried]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Media Interaction Lab]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264976</person_id>
				<author_profile_id><![CDATA[81100048383]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Michael]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Haller]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Media Interaction Lab]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264977</person_id>
				<author_profile_id><![CDATA[81100099358]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>7</seq_no>
				<first_name><![CDATA[Robert]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Zeleznik]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Brown University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264978</person_id>
				<author_profile_id><![CDATA[81381597937]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>8</seq_no>
				<first_name><![CDATA[Andrew]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bragdon]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Brown University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1015741</ref_obj_id>
				<ref_obj_pid>1186562</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[LaViola, J. and Zeleznik, R. MathPad&#60;sup&#62;2&#60;/sup&#62;: A system for the creation and exploration of mathematical sketches. In Proceedings of SIGGRAPH 2004, pp. 432--440]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[http://pen.cs.brown.edu/starpad.html]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[http://www.anoto.com]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1531371</ref_obj_id>
				<ref_obj_pid>1576246</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Rosenberg, I. and Perlin, K. 2009. The UnMousePad: an interpolating multi-touch force-sensing input pad. In Proceedings of SIGGRAPH 2009.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 NiCE Formula Editor Jakob Leitner, Christian Rendl, Florian Perteneder, Adam Gokcezade, Thomas Seifried, 
Michael Haller Media Interaction Lab mi-lab@fh-hagenberg.at Robert Zeleznik, Andrew Bragdon Brown University, 
Department of Computer Science {bcz, acb}@cs.brown.edu  C:\Users\Florian\Studium\IM\1. Semester\Projekt\presentation\images\write-formula.jpg 
C:\Users\Florian\Studium\IM\1. Semester\Projekt\presentation\images\formula-pinch.jpg C:\Users\Florian\Studium\IM\1. 
Semester\Projekt\presentation\images\2Dviewer-matrixDrag.jpg C:\Users\Florian\Studium\IM\1. Semester\Projekt\presentation\images\graph2D-pinch.jpg 
 Figure 4: Interacting with plug-ins Figure 3: Dropping expressions on opened plug-ins Figure 2: Manipulating 
digital interpretation of expressions Figure 1: Writing mathematical expressions  1. Motivation Interactive 
Whiteboards are becoming an increasingly important component of face-to-face meetings, particularly in 
educational settings. Still, non-interactive surfaces have remained the standard for mathematics-oriented 
discussions in part due to the lack of robust algorithms for recognizing handwritten mathematical expressions. 
Thus teachers, for example, are typically burdened with drawing visualizations by hand to reinforce abstract 
and complex mathematical problems. Our work expands upon the notion of mathematical sketching [1] by 
supporting both pen and touch interactions on an interactive whiteboard to write expressions, to perform 
algebraic transformations, and to drive sophisticated 2D and 3D illustrations. By incorporating lightweight 
touch-based interactions, students can play with formulas and explore mathematics from a fundamentally 
interactive perspective that we believe many will find to be engaging and enjoyable. 2. NiCE Formula 
Editor In this talk, we present the NiCE Formula Editor, which recognizes handwritten formulas to provide 
in situ computation and visualization. Our editor uses a novel hardware setup, suitable for use as a 
desktop or whiteboard that disambiguates pen and multi-touch input. In our interface, the pen writes 
and performs gestures, while touch manipulates. Multiple users can sketch mathematical expressions (e.g. 
formulas, vectors, matrices, etc.) with digital pens on the interactive surface (see Figure 1). These 
expressions are analyzed and interpreted using the StarPad SDK s [2] support for mathematics recognition. 
Pen and touch gestures can be used to manipulate the written expressions. In addition, expressions can 
be selected and moved by touch or pen input. For example, a scribbling gesture can be used to delete 
symbols and a loop gesture can be used to create a graph. Also, the digital interpretation of the written 
expression can be manipulated through touch interaction. For example a user can re-arrange terms in an 
equation or pinch equations to simplify them by combining the two touched terms together (see Figure 
2). Moreover, a written expression can be connected to visualization widgets simply by dragging the expression 
using a fingertip over an already visible widget and then dropping it by lifting the finger. A computer 
graphics teacher, for example, can drop various matrices onto the 2D Matrix Stack Viewer widget to show 
how matrices transform a 2D object, as seen in Figure 3. Due to the diversity of mathematical problems, 
our formula editor supports a plug-in architecture for incorporating customized widgets. This flexible 
plug-in API provides developers with pre-defined interaction modules and access to equation-solving-functions 
which are specially designed to support visualization. 3. Prototype Our prototype implementation is 
fully functional and robustly supports simultaneous multi-pen and pressure-sensitive multi-touch input. 
A scratch resistant foil with printed Anoto [3] pattern on top of a 24 Interpolated Force Sensitive Resistor 
(IFSR) foil [4] is used for combined pen and touch input. Anoto DP-301 pens are used as wireless pen 
input devices. Pressure-sensitive touch input provided by the IFSR foil is used to disambiguate between 
transforming graph windows as a whole and panning their graph contents. Touches with a high pressure 
lead to window-level transformations (i.e. move or scale a widget), while light pressure just affects 
the content (Figure 4). REFERENCES [1] LaViola, J. and Zeleznik, R. MathPad²: A system for the creation 
and exploration of mathematical sketches. In Proceedings of SIGGRAPH 2004, pp. 432-440 [2] http://pen.cs.brown.edu/starpad.html 
[3] http://www.anoto.com [4] Rosenberg, I. and Perlin, K. 2009. The UnMousePad: an interpolating multi-touch 
force-sensing input pad. In Proceedings of SIGGRAPH 2009. 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2010</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1837099</article_id>
		<sort_key>730</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>56</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[3D multitouch]]></title>
		<subtitle><![CDATA[when tactile tables meet immersive visualization technologies]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1837026.1837099</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1837099</url>
		<abstract>
			<par><![CDATA[<p>Multitouch tactile input, while having been in the research labs for quite some time, is just reaching the general public through well-known interfaces like mobile phones or multitouch tables. The technology, when used in the right context in the right way, is known to provide an intuitive manipulation of the synthetic -- mostly 2D -- content that is displayed on its surface.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>H.5.2</cat_node>
				<descriptor>Input devices and strategies (e.g., mouse, touchscreen)</descriptor>
				<type>S</type>
			</primary_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003120.10003121.10003125</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10011666</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Touch screens</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003125</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10011666</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Touch screens</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2264979</person_id>
				<author_profile_id><![CDATA[81381609918]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Jean-Baptiste]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[de la Rivi&#232;re]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Immersion SAS]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264980</person_id>
				<author_profile_id><![CDATA[81381608476]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[C&#233;dric]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kerv&#233;gant]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Immersion SAS]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264981</person_id>
				<author_profile_id><![CDATA[81381606548]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Nicolas]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Dittlo]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Immersion SAS]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264982</person_id>
				<author_profile_id><![CDATA[81466641118]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Mathieu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Courtois]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Immersion SAS]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264983</person_id>
				<author_profile_id><![CDATA[81381610813]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Emmanuel]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Orvain]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Immersion SAS]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 3D Multitouch : When Tactile Tables Meet Immersive Visualization Technologies Jean-BaptistedelaRivi`edricKerv´ere,C´egant,NicolasDittlo,MathieuCourtois,EmmanuelOrvain 
ImmersionSAS 1 Introduction Multitouch tactile input, while having been in the research labs for quite 
some time, is just reaching the general public through well-known interfaces like mobile phones or multitouch 
tables. The technology, when used in the right context in the right way, is known to provide an intuitive 
manipulation of the synthetic mostly2D contentthatisdisplayed on itssurface. Likewise, means to display 
stereoscopic images are known for decades now, but are just beginning to be understood and accepted by 
the general public thanks to very well received movies and all the announcements around the soon to be 
released 3D television. Whenrenderedproperly,stereoscopicdisplays indeed offerahuge sense of depth to 
previously .at images. Furthermore, combining stereorendering withviewpoint trackingde.nitelyprovidesanim­mersive 
visualization of3D models. The combination of the two technologies introduces speci.c con­straints that 
have not yet been dealt with nor extensively studied. Following the reviewers comments, we therefore 
propose to give a talk on our current work that will be demonstrated in the emerg­ingtechonologiessection, 
includingthenew issuesandpreliminary developments. 2 Technological Constraints Whilethe two technologiessharemanyobjectives,whilebothmul­titouch 
tables and immersive visualization are being extensively studied by the research community, they have 
to our knowledge never been combined into a single solution. Such a combination is indeed not that obvious, 
since strengths of a technology become constraintsfor theotherone.They morepreciselyconsist in: single 
viewpoint : multitouch tables are ef.cient for collab­orative work, but this implies that two users will 
face each other, have opposite point of views on the content and their right and left sides will be switched. 
On the contrary, most stereo technologies are single viewpoint only.  Viewingangle: multitouchtablesare 
intendedtobeusedhor­izontally,whilemost stereo technologiesarereadyforvertical visualization requiring 
a narrow viewing angle.  Hand and 3D content collision : the most immersive stereo visualization is 
obtained when negative parallax is used, to make the3D content appear infront and outsideofthedisplay. 
Fingers in contact with the screen ruin the depth perception, since they wronglypass through the3D objects. 
 Parallax:activatingtheheadtrackingimpliesthatthecontent willmoveaccordingto theuser shead,dynamicallychanging 
theactual3Dpoints that areprojectedbeneath the .ngers.  3 Proposal Trying to take the multitouch tables 
from 2D interaction to an ef.­cient and collaborative3Dimmersiveinterface,webuilta two-user multiview 
multitouch table that we has been accepted for demon­stration in theETech.Two-usermultiview,which ensureseach 
user hashisownperspectivecorrectviewpoint on thesame3D model, is obtained by combining active and passive 
stereo technologies and using six dof sensors that help to retrieve the position and orienta­tion of 
each user shead. The focal plane has been set right on the tactile surface. Negative parallax, which 
then leads to the most impressive stereo visualiza­tion with the 3D content coming out of the display, 
is used as long asnohand approachesthe tactilesurface.Whena.nger touches to the surface, the model may 
slowly be lowered to obtain a positive parallax,withthe3D contentlocated rightinsidethe table.Thanks 
tothediffuseinfraredilluminationourmultitouch videoanalysisal­gorithmsrelyon,wearealsoable todetectfeaturesbeyondthesim­ple 
2D contact points and therefore retrieve a hand presence even before its.ngerstouchthesurfaceandbreakthedepthperception. 
While image plane interaction techniques would work quite well forasingleuser,theywouldhardly takeintoaccountthe 
twousers opposite viewpoints and the stereo positive parallax. We therefore chose to experiment with 
shooting virtual rays orthogonal to the tactile surface from each .nger in contact, which currently seems 
to strenghten the .ngers presence within the virtual world and its relationship with3D objects. 4 Talk 
Based on theETech reviewerscomments,wepropose tofocusour talk on the new issues that the combination 
oftactile interaction and immersive visualization introduces. We will brie.y describe the stateoftheart,both 
in termsofimmersivevisualization,multi-view systems, visualization of 3D information on tabletops, tactile 
and multitouch interaction. This will include the studies of Hancock et al. on the 3D perspective rendering 
on tabletops, as well as the work ofShning et al.on tactilestereoscopicvertical screens. The technicalconstraintseach 
ofthem introduceswillbedescribed, leading tothepresentationofthespeci.cissuesonewillfacewhen merging 
together those two worlds. After focusing on the speci.c solution we devised, specifying both its strengths 
and weaknesses, wewoulddescribetheopenquestionsaswellasalternativeways to address our issues, present 
our .rst user feedbacks1 and our short and long termgoals. 5 Conclusion Our ETech demonstration will 
offer attendees a .rst experience with the combination of multitouch tables and immersive visual­ization 
technologies, and is built to help them experiment with the .rst issues it introduces. We herein propose 
a more formal talk that would present all our issues, results and thoughts in a better way than the exchanges 
on our booth, and could therefore com­plement our actual demonstration. We are indeed very con.dent stereoscopic 
visualization will .ndits way within multitouch tables inthefuture,and wewould liketohaveachancetoreducethisde­layby 
sharing our .rst results through aSiggraph2010Talk.Given all the topicssuch anintegrated solutionisraising,thiscouldbeof 
interest and open up new possibilities for both the 3D, interaction and visualization communities. 1This 
work has been partially funded by the European Comission through theV-Cityproject(ICT-231199-VCITY reference). 
Copyright is held by the author / owner(s). SIGGRAPH 2010, Los Angeles, California, July 25 29, 2010. 
ISBN 978-1-4503-0210-4/10/0007  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<article_sponsors>
			<funding_agency>European Comission V-City project</funding_agency>
			<grant_numbers>
				<grant_number>ICT-231199-VCITY</grant_number>
			</grant_numbers>
		</article_sponsors>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2010</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1837100</article_id>
		<sort_key>740</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>57</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[Z-touch]]></title>
		<subtitle><![CDATA[a multi-touch system for detecting spatial gestures near the tabletop]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1837026.1837100</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1837100</url>
		<abstract>
			<par><![CDATA[<p>We introduce Z-touch, a multi-touch table that can detect spatial hand gestures near the tabletop surface. This system uses a combination of a multilayered infrared (IR) laser plane and a high-speed camera. Multilayered IR line laser modules are synchronized with the shutter signal of the high-speed camera. This system allows user interaction on the basis of the posture of the user's fingers near the tabletop surface without the use of special markers or devices on the fingers.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[hand gesture interaction]]></kw>
			<kw><![CDATA[multi-touch]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>H.5.2</cat_node>
				<descriptor>Input devices and strategies (e.g., mouse, touchscreen)</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.4.1</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003125</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10011666</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Touch screens</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2264984</person_id>
				<author_profile_id><![CDATA[81466647551]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Yoshiki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Takeoka]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264985</person_id>
				<author_profile_id><![CDATA[81416607459]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Takashi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Miyaki]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264986</person_id>
				<author_profile_id><![CDATA[81100008564]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Jun]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Rekimoto]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[
Benko, H., and Wilson, A. D. 2009. Depthtouch: Using depthsensing camera to enable freehand interactions on and above the interactive surface. <i>Tech. Report</i>, MSR-TR-2009-23.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Z-touch: A multi-touch system for detecting spatial gestures near the tabletop Yoshiki Takeoka Takashi 
Miyaki Jun Rekimoto The University of Tokyo The University of Tokyo The University of Tokyo yoshikinoko@gmail.com 
Sony Computer Science Laboratory  Figure 1: These two pictures have been captured simultaneously. (a): 
Tabletop of Z-touch. Multilayered line laser modules are placed at the edge of the tabletop panel. (b): 
Captured depth map of the hand near the top panel of Z-touch shown in (a). RGB channels correspond to 
the Laser Plane: R-Highest, G-Middle, B-Lowest (very close to the surface of the tabletop). The image 
at the bottom of (b) is that of the thumb not touching to the surface, so the silhouette is in a pale 
red color. Abstract We introduce Z-touch, a multi-touch table that can detect spatial hand gestures near 
the tabletop surface. This system uses a combi­nation of a multilayered infrared (IR) laser plane and 
a high-speed camera. Multilayered IR line laser modules are synchronized with the shutter signal of the 
high-speed camera. This system allows user interaction on the basis of the posture of the user s .ngers 
near the tabletop surface without the use of special markers or devices on the .ngers. Keywords: multi-touch, 
hand gesture interaction 1 Introduction Since multi-touch systems detect multiple contact points on a 
table­top surface, user interaction is restricted to a 2D planar surface. However, users often forget 
this restriction and interact with the system using 3D gestures near the tabletop. When carrying out 
physical interactions such as manipulating some objects, we simul­taneously control multiple points and 
continuous parameters. This is because the most of the parameters are dependent on how the .ngers/hands 
touch to the objects. In the same context, we believe that it is very important to take into account 
the interaction using the postures of .ngers when designing multi-touch systems. We may use .nger posture 
in applications such as those for controlling 3DCG objects and effects of music. We introduce Z-touch, 
a novel multi-touch table that is capable of detecting posture and position of multiple objects (.ngers) 
near the tabletop. Users can interact with the contents displayed on Z-touch by using their bare hands 
with­out wearing special markers. [Benko and Wilson 2009] introduced the concept of 3D hover gesture 
interaction in front of the projec­tion screen surface. They used a depth sensing camera to detect 3D 
interaction. However, we use a multilayered laser plane synchro­nized with the shutter signal of a high-speed 
camera to obtain a clear image of the depth map near the surface (Figure1(b)). 2 System Design Figure 
2 shows the system architecture of Z-touch. The key com­ponents of Z-touch are multilayered IR line laser 
modules and a high-speed camera. The appearance of each of IR laser plane is Figure 2: System architecture. 
 controlled by synchronization with the shutter signal of high-speed camera. The IR laser is re.ected 
by the objects in the middle of the top panel and is captured by the high-speed camera embedded under 
the surface (Figure1(b)). We used Pointgrey Grasshopper which can capture 8bit gray scale VGA image at 
200fps. Eight sets of triple line laser modules are placed at center of edge and vertex of the top panel 
(Figure1(a)). The IR laser plane is calibrated along the distance between each laser plane (11.5mm) and 
is parallel to the top panel. The lowest laser plane is very close to the surface of the top panel. The 
top panel (control area) is 420 mm square glass panel having a thickness of 6mm. This panel is covered 
by a projection sheet, the DILAD screen sheet, which only diffuses vis­ible light from the rear projector 
to the surface of the top panel but allows re.ected IR light to pass through. 3 Conclusion We introduced 
Z-touch, that combines multi-touch and 3D hand gesture detection near the surface of a tabletop, using 
multilayered IR line lasers synchronized with the shutter signal of a high-speed camera.  References 
BENKO, H., AND WILSON,A.D.2009.Depthtouch:Usingdepth­sensing camera to enable freehand interactions on 
and above the interactive surface. Tech.Report, MSR TR 2009 23. Copyright is held by the author / owner(s). 
SIGGRAPH 2010, Los Angeles, California, July 25 29, 2010. ISBN 978-1-4503-0210-4/10/0007  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2010</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	</section>
</content>
</proceeding>
