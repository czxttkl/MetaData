<?xml version="1.0" encoding="ISO-8859-1"?>
<!DOCTYPE proceeding SYSTEM "proceeding.dtd">
<proceeding ver="6.0" ts="04/10/2010">
<conference_rec>
	<conference_date>
		<start_date>08/05/2012</start_date>
		<end_date>08/09/2012</end_date>
	</conference_date>
	<conference_loc>
		<city><![CDATA[Los Angeles]]></city>
		<state>California</state>
		<country></country>
	</conference_loc>
	<conference_url></conference_url>
</conference_rec>
<series_rec>
	<series_name>
		<series_id>SERIES382</series_id>
		<series_title><![CDATA[International Conference on Computer Graphics and Interactive Techniques]]></series_title>
		<series_vol></series_vol>
	</series_name>
</series_rec>
<proceeding_rec>
	<proc_id>2342896</proc_id>
	<acronym>SIGGRAPH '12</acronym>
	<proc_desc>ACM SIGGRAPH 2012 Posters</proc_desc>
	<conference_number>2012</conference_number>
	<proc_class>conference</proc_class>
	<proc_title></proc_title>
	<proc_subtitle></proc_subtitle>
	<proc_volume_no></proc_volume_no>
	<isbn13>978-1-4503-1682-8</isbn13>
	<issn></issn>
	<eissn></eissn>
	<copyright_year>2012</copyright_year>
	<publication_date>08-05-2012</publication_date>
	<pages>131</pages>
	<plus_pages></plus_pages>
	<price><![CDATA[]]></price>
	<other_source></other_source>
	<abstract>
		<par><![CDATA[<p>Posters are a light-weight, low-tech method for presenting student, in-progress, and late-breaking work. Poster topics range from applications of computer graphics to novel interactive techniques and in-depth research in specific areas. They are displayed throughout the conference for attendees to browse at their leisure, and poster authors meet and discuss their work with attendees during Poster Presentations.</p>]]></par>
	</abstract>
	<publisher>
		<publisher_id>PUB27</publisher_id>
		<publisher_code>ACMNY</publisher_code>
		<publisher_name>ACM</publisher_name>
		<publisher_address>2 Penn Plaza, Suite 701</publisher_address>
		<publisher_city>New York</publisher_city>
		<publisher_state>NY</publisher_state>
		<publisher_country>USA</publisher_country>
		<publisher_zip_code>10121-0701</publisher_zip_code>
		<publisher_contact>Bernard Rous</publisher_contact>
		<publisher_phone>212 869-7440</publisher_phone>
		<publisher_isbn_prefix></publisher_isbn_prefix>
		<publisher_url>www.acm.org/publications</publisher_url>
	</publisher>
	<sponsor_rec>
		<sponsor>
			<sponsor_id>SP932</sponsor_id>
			<sponsor_name>ACM Special Interest Group on Computer Graphics and Interactive Techniques</sponsor_name>
			<sponsor_abbr>SIGGRAPH</sponsor_abbr>
		</sponsor>
	</sponsor_rec>
	<categories>
		<primary_category>
			<cat_node/>
			<descriptor/>
			<type/>
		</primary_category>
	</categories>
</proceeding_rec>
<content>
	<section>
		<section_id>2342897</section_id>
		<sort_key>10</sort_key>
		<section_seq_no>1</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Animation]]></section_title>
		<section_page_from>1</section_page_from>
	<article_rec>
		<article_id>2342898</article_id>
		<sort_key>20</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>1</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[A biologically inspired latent space for gait parameterization]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2342896.2342898</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2342898</url>
		<abstract>
			<par><![CDATA[<p>The problem of character locomotion synthesis is notorious for its high dimensionality and the nonlinear relationship between dimensions. However, many human motion activities lie intrinsically on low dimensional manifolds [Safonova et al. 2004] leading to significant data redundancy. Linear and non--linear methods for dimension reduction have been applied to the problem, but none of the existing approaches for dimensional reduction provide a physically--justified explanation for selected dimensions, instead they use general methods which employ numerical error analysis.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3737186</person_id>
				<author_profile_id><![CDATA[81438594516]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Richard]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Southern]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737187</person_id>
				<author_profile_id><![CDATA[81504684484]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Shihui]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Guo]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737188</person_id>
				<author_profile_id><![CDATA[81504684044]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Fangde]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Liu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737189</person_id>
				<author_profile_id><![CDATA[81504687240]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Jian]]></first_name>
				<middle_name><![CDATA[J.]]></middle_name>
				<last_name><![CDATA[Zhang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Chen, V. F. H. 2007. <i>Passive dynamic walking with knees: A point foot model</i>. Master's thesis, Massachusetts Institute of Technology.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Full, R. J., and Koditschek, D. E. 1999. Templates and anchors: neuromechanical hypotheses of legged locomotion on land. <i>Journal of Experimental Biology 202</i>, 23, 3325--3332.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Matsuoka, K. 1985. Sustained oscillations generated by mutually inhibiting neurons with adaptation. <i>Biological Cybernetics 52</i>, 1, 345--353.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1015754</ref_obj_id>
				<ref_obj_pid>1186562</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Safonova, A., Hodgins, J. K., and Pollard, N. S. 2004. Synthesizing physically realistic human motion in low-dimensional, behavior-specific spaces. In <i>ACM SIGGRAPH 2004 Papers</i>, ACM, New York, NY, USA, SIGGRAPH '04, 514--521.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 A biologically inspired latent space for gait parameterization Richard Southern Shihui Guo Fangde Liu 
Jian J. Zhang   (c) Latent space query (d) Motion query result Figure 1: Our latent space is suitable 
for mapping low dimensional simulations of human gait into a motion database. Introduction The problem 
of character locomotion synthesis is notorious for its high dimensionality and the nonlinear relationship 
between dimen­sions. However, many human motion activities lie intrinsically on low dimensional manifolds 
[Safonova et al. 2004] leading to signif­ icant data redundancy. Linear and non linear methods for dimen­sion 
reduction have been applied to the problem, but none of the ex­isting approaches for dimensional reduction 
provide a physically justi.ed explanation for selected dimensions, instead they use gen­eral methods 
which employ numerical error analysis. [Full and Koditschek 1999] theorize that all biological motion 
is governed by low dimensional templates: the simplest possible model required to exhibit a desired action. 
While the motion we observe may appear complex and non linear, the body is still at­tempting to adhere 
as closely as possible to this low dimensional template. The cyclical behavior of locomotion has been 
observed in both biology and robotics disciplines, and is a guiding principle of dynamic system design 
(see Figure 1b). We propose that this cycle is the template to which human locomotion is attempting to 
adhere, implying that a polar representation of this space will yield a feature rich latent space. Dynamic 
models used to drive energy ef.cient robots do not rely on motion capture data or statistical methods 
to derive motion. These models are physics based and are able to synthesize motion for un­predicted environmental 
conditions. However, due to the numeri­cal complexity in analyzing these systems they are typically very 
simple, seldom incorporating an upper body, and are normally sim­ulated in only the 2D sagital plane 
to remove problems associated with the de stabilizing effect of side swing behavior. We use a dynamic 
model to drive a virtual character by a latent space mapping into a database of captured motion states. 
The map­ping is achieved using standard statistical and point based methods, and results in motion that 
enriches the dynamic walking motion with plausible upper body, ankle and knee animations, and yielding 
motion that is generally smooth and natural looking. Method and Results We de.ne .i to be the angle 
between the vertical and the thigh of leg le i, so the state variable for leg i =1, 2 as yi = .i,..i 
. We de.ne our reduced dimensional mapping space for both legs as y = {r, f} where r and f are the coordinates 
of (y1 - y2) in polar coordinates (see Figure 1a). In Figure 1b it is clear from 5 extracted steps from 
our motion database that gait generally conforms with this cyclical behaviour. A polar representation 
is therefore ideal because of the small vari­ance of r for each step if the data is incomplete, a .xed 
r and an increasing f will still result in a plausible walking motion. Gen­eral methods are less suitable 
because they do not exploit biological meaning. The compass gait model is one of the simplest dynamic 
models for human locomotion. Its energy ef.ciency results in adaptive loco­motion behavior that is natural 
looking. We adapt our mechanical model from [Chen 2007], and the [Matsuoka 1985] neural oscil­ lator 
is used as a controller. The structural stability of the cou­pled dynamic system is enhanced by entrainment: 
the phenomenon whereby two coupled oscillating systems, with different periods when they function independently, 
assume the same period. For a query, nearby motion states must be queried from with the latent space 
and the results blended to form the query result for a particular frame. We have implemented three methods 
to per­form motion querying: K-Nearest Neighbors (fast, no coherence, not smooth), Iterative Closest 
Point (exploits coherence, prone to drift) and Moving Least Squares projection (exploits coherence, smooth, 
slowest). In Figure 1c, steps from 46 individuals are extracted to form the database (data from the publicly 
available CMU motion database). Note the discontinuities in the query data (in red) resulting from knee 
locking and ground impact. Results of the resulting motion by blending the MLS projections of the query 
point onto the n-nearest walking motions are shown in Figure 1d. A standard smoothing .l­ ter was applied 
to correct the discontinuity errors resulting from the passive model. Effectively the ankle, knee and 
upper body motion is generated with our method.  References CHEN, V. F. H. 2007. Passive dynamic walking 
with knees: A point foot model. Master s thesis, Massachusetts Institute of Technology. FULL, R. J., 
AND KODITSCHEK, D. E. 1999. Templates and anchors: neuromechanical hypotheses of legged locomotion on 
land. Journal of Experimental Biology 202, 23, 3325 3332. MATSUOKA, K. 1985. Sustained oscillations generated 
by mutually inhibiting neurons with adaptation. Biological Cybernetics 52, 1, 345 353. SAFONOVA, A., 
HODGINS, J. K., AND POLLARD, N. S. 2004. Synthesizing physically realistic human motion in low­ dimensional, 
behavior-speci.c spaces. In ACM SIGGRAPH 2004 Papers, ACM, New York, NY, USA, SIGGRAPH 04, 514 521. 
Copyright is held by the author / owner(s). SIGGRAPH 2012, Los Angeles, California, August 5 9, 2012. 
ISBN 978-1-4503-1435-0/12/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2342899</article_id>
		<sort_key>30</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>2</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[A stereoscopic representation of impossible rectangle twisted torus figure]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2342896.2342899</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2342899</url>
		<abstract>
			<par><![CDATA[<p>Impossible figure is a figure which looks realizable at first glance, but is not actually realizable as perceived by the human eyes. Optical illusion object is a three dimensional object made to be an impossible figure when viewed from a specific viewpoint. The viewpoint that realizes an optical illusion is called as an illusion viewpoint. An optical illusion object is not perceived as an impossible figure when a viewpoint moves away from an illusion viewpoint.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3737190</person_id>
				<author_profile_id><![CDATA[81504683574]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Kana]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nakatsu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tokyo Denki University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737191</person_id>
				<author_profile_id><![CDATA[81319502327]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Tokiichiro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Takahashi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tokyo Denki University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737192</person_id>
				<author_profile_id><![CDATA[81331499700]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Tomoaki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Moriya]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tokyo Denki University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 A Stereoscopic Representation of Impossible Rectangle Twisted Torus Figure  Kana Nakatsu Tokiichiro 
Takahashi Tomoaki Moriya School of Science and Technology for Future Life, Tokyo Denki University 1. 
Introduction  Impossible figure is a figure which looks realizable at first glance, but is not actually 
realizable as perceived by the human eyes. Optical illusion object is a three dimensional object made 
to be an impossible figure when viewed from a specific viewpoint. The viewpoint that realizes an optical 
illusion is called as an illusion viewpoint. An optical illusion object is not perceived as an impossible 
figure when a viewpoint moves away from an illusion viewpoint. In this paper, we propose a shape modeling 
method and an animating method for an optical illusion object that shapes an impossible rectangle twisted 
torus figure, which is a kind of impossible figures, according to the illusion viewpoint. The proposed 
method realizes an interactive and stereoscopic representation of the impossible figure, which has not 
been dealt with in previous research, moved in a wide range of illusion viewpoints. 2. 3D Shape Model 
for Optical Illusion Object  Three dimensional shape model of twisted torus, which is one of the optical 
illusion objects, consists of multiple quadrangular prisms (prism in short) as shown in Fig.1. Shape 
model fundamentally is a ring shape. This has a shape with a ring with area cut apart to model an impossible 
object as an existent three-dimensional object. We assign numbers to prisms, and the starting point of 
the number is the area cut part. In the example of Fig.1 (a), the shape model consists of four prisms 
in total. We assign number to each prism: 1 to 4. Two prisms fundamentally connecting with each other 
are called 'start prism' and 'end prism' respectively. In the example of Fig.1, prism 1 and 4 are the 
start prism and the end prism respectively. There is a gap between prism 1 and 4, and they are not connected 
(Fig.1 (b)). The plane consisting of the first two prisms is called 'front face'. In Fig.1(a), the front 
face consists of prism 1 and 2. From an illusion viewpoint, the prisms behind the front face is called 
'flexible quadrangular prism' (flexible prism in short). In Fig.1(a), prism 3 and 4 are the flexible 
prisms. We transform the shape model into optical illusion object by stretching the flexible prism 3 
and 4 according to the given viewpoint. Fig1. Optical Illusion Objects 3. Illusion Viewpoint and Projection 
 The sphere containing an optical illusion object is called 'bounding sphere'. The illusion viewpoint 
moves on the upper half of the bounding sphere except the pole. The illusion viewpoint looks at the center 
of the sphere all the time. The illusion viewpoint is given by a latitude .., and longitude .. on the 
bounding hemi-sphere. The illusion viewpoint is expressed as (..,..), where 0=..<p2,0=..<2p. The parallel 
projection is used for the shape model seen from the illusion viewpoint by 3DCG. 4. Modeling of Optical 
Illusion  We stretch the flexible prism of a shape model in the depth and the horizontal direction so 
a shape model can be seen as an optical illusion object when the model is seen from an arbitrary illusion 
viewpoint (..,..). In this chapter, we explain the method to transform a shape model concretely so it 
can be seen as an optical illusion object using Fig.2. As shown in Fig.2(a), the shape model of an optical 
illusion object in Fig.1 is projected on X-Z plane. The illusion viewpoint with the longitude of .., 
that is, the viewpoint (..,0) is called 'initial illusion viewpoint'. We regard that the shape model 
is seen as an optical illusion object when it is seen from the initial illusion point(..,0). Here, when 
the shape model is projected parallel toward the illusion viewpoint, the vertex F(..,0) of starting prism 
and the vertex ..(..,0) of ending prism become the same value in the projection plane, and both vertexes 
look connected. The way for the shape model to be seen as an optical illusion object, when the viewpoint 
moves to (..,..), is to transform the shape model to have vertex ..(..,..) and ..(..,..) from a viewpoint 
(..,..) with the same value. So, as in Fig.2 (b), the ending prism 4 is moved to the prism 4 in the depth 
direction parallel to X axis and Z axis, and stretched in the X axis direction as the way to realize 
the position of the vertex .. as below: ....(.)=..sin.+....(0) (1) ....(.)=-..cos. + ....(0) (2) Next, 
an optical illusion object is complete by stretching the prism 3 connected to the ending prism in the 
depth (Z axis) direction.      Fig2. G(.) Coordinate Change Viewed from X-Y Plane 5. Stereoscopic 
Vision of Impossible Rectangle Twisted Torus Figure  With the method mentioned above, it is possible 
to model an impossible rectangle twisted torus figure. In this chapter, we realize the stereoscopic vision 
of an impossible figure by using the binocular disparity as a difference in longitude of the illusion 
viewpoint. The stereoscopic vision of an impossible figure is realized by modeling optical illusion objects 
with different longitude of illusion viewpoint respectively for the right eye and the left eye to make 
stereoscopic representation of the results from each illusion viewpoint. Fig.3 is a stereoscopic image 
generated by our method. Further, it is possible to move illusion viewpoint interactively, and the interactive 
stereoscopic view is also possible. See the attached movie.    Fig3. Generated Stereoscopic Image 
(For the Parallel Method) 6. Conclusion  We realized the stereoscopic representation of the interactive 
animation of an impossible twisted torus figure. We have implemented an interactive stereoscopic animation 
system of an impossible figure on Nintendo 3DS game system. 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2342900</article_id>
		<sort_key>40</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>3</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[CurveThis]]></title>
		<subtitle><![CDATA[a tool to create controllable massive crawling]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2342896.2342900</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2342900</url>
		<abstract>
			<par><![CDATA[<p>Forming the shape of an animated character from hundreds of thousands of crawling bugs, as shown in the film of <i>The Sorcerer's Apprentice</i>, is a challenging shot for most animators. It is desirable to have the massive motions of the crawling bugs or growing vine generated automatically by procedures to avoid time-consuming key-framing. On the other hand, the motions also need to be controllable to achieve specific artistic effects. In other words, a desirable tool for creating this kind of animation should allow the director to specify desirable visual effects such as heading toward common destinations and following surface constraint, and then the animation should be generated automatically.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3737193</person_id>
				<author_profile_id><![CDATA[81504686718]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Pei-Zhi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Huang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Base FX Studio]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737194</person_id>
				<author_profile_id><![CDATA[81100475460]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Tsai-Yen]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Li]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[National Chengchi University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Dunlop, R. 2006. Sorcerer's Apprentice. Available at http://www.cgsociety.org/index.php/CGSFeatures/CGSFeatureSpecial/sorcerers_apprentice]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>532147</ref_obj_id>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Latombe, J. C. 1991. <i>Robot Motion Planning</i>. Kluwer Academic Publishers.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 CurveThis: A Tool to Create Controllable Massive Crawling Pei-Zhi Huang , Tsai-Yen Li Base FX Studio, 
National Chengchi University Figure 1. The controllable massive crawling effect on the body surface of 
an animated character. Green dots: starting points; Red triangles: goal points; Blue points: current 
endpoints of curves; Red curves: trace of crawling curves. 1. Introduction Forming the shape of an animated 
character from hundreds of thousands of crawling bugs, as shown in the film of The Sorcer­er s Apprentice, 
is a challenging shot for most animators. It is desirable to have the massive motions of the crawling 
bugs or growing vine generated automatically by procedures to avoid time-consuming key-framing. On the 
other hand, the motions also need to be controllable to achieve specific artistic effects. In other words, 
a desirable tool for creating this kind of animation should allow the director to specify desirable visual 
effects such as head­ing toward common destinations and following surface constraint, and then the animation 
should be generated automatically. Several approaches had been proposed to solve the problem. For example, 
one can divide the creeping bugs into layers along the surface of a model to reduce the complexity of 
resolving intersec­tions [1]. The same visual effect may be achieved by composing several layers into 
one. However, due to the speed variation of each vertex on a character model in an animation, the simulation 
along the surface may suffer from sliding or popping artifacts. Another way to avoid these artifacts 
is to perform the simulation in the UV texture space and then transform the simulation results to the 
world space for rendering. Although the method is straight­forward and creates smooth motions, there 
is no easy way to know how to design UV layouts to achieve a specific visual effect.  2. Generating 
Curves on Surfaces Our approach aims at facilitating the creation of these massive bug animations by 
generating goal-directed curves along the surface of a character model and allowing the dispersion of 
the curves to be controlled by the director. This approach consists of three phases: gradient graph creation, 
path planning, and curve generation. Gradient Graph: Topological connectivity is an invariant of a geometric 
model in an animation. A cyclic connectivity graph C can be constructed by explicitly building the neighboring 
relation for each vertex on a mesh. On a uniform mesh, the number of edges between two vertices can be 
used as an approximation of Manhattan or Euclidean disctance on the surface. The connectivity graph can 
then be used as the workspace for finding a feasible path. One can define obstacles, such as protruding 
objects, in the workspace to prevent bugs from crawling over. The gradient field in the workspace can 
then be computed with the wave propagation algorithm (NF1) commonly used in the potential-field based 
motion planners by propagating a gradient field from the goals [2]. Figure 2. Path generation with different 
control parameters. Left: preferring less visited nodes; Middle: with coverage preference; Right: with 
Brownian motion  Path Planning: The gradient field is a directed acyclic graph that can be used to guide 
the search in the path planning phase. Unlike other path planning problems where a shortest path is desired, 
the path along the surface of a model in our tool should not only fol­low the gradient field but also 
satisfy physical constraints (such as linear and angular velocities, etc.) and respect design preferences 
(such as coverage, sibling order, etc.) for artistic control. In Figure 2, we show three examples of 
paths resulting from different con­trol parameters for optimizing certain criteria. Curve Generation: 
A generated path consists of a sequence of vertices on C. We then take these via points as the control 
points to generate smooth B-spline curves on or above the world space. Although the curves could be generated 
with dispersion preference, intersection between them may not be avoided. In case of collision, we displace 
the control points along the vertex normal to resolve the problem. 3. Conclusion We have developed a 
tool, called CurveThis, which can generate goal-directed curves on animated model s surface which satisfy 
physical constraints and provide artistic controls. We use a gradi­ent field to guide the search for 
a collision-free path, which is used in turn to generate a smooth curve above the model s surface. The 
efficiency of the method allows the users to interactively tune the design parameters and preview the 
generated results in real time. References DUNLOP, R. 2006. Sorcerer s Apprentice. Available at http://www.cgsociety.org/index.php/CGSFeatures/CGSFeat 
 ureSpecial/sorcerers_apprentice LATOMBE, J.C. 1991. Robot Motion Planning. Kluwer Academic Publishers. 
Copyright is held by the author / owner(s). SIGGRAPH 2012, Los Angeles, California, August 5 9, 2012. 
ISBN 978-1-4503-1435-0/12/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2342901</article_id>
		<sort_key>50</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>4</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[Easy-to-use authoring system for Noh (Japanese traditional) dance animation]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2342896.2342901</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2342901</url>
		<abstract>
			<par><![CDATA[<p>In this article, we introduce an easy-to-use authoring system for Noh (Japanese traditional) dance animation. This is a joint research between computer animation and Noh research groups.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3737195</person_id>
				<author_profile_id><![CDATA[81335495744]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Masaki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Oshita]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Kyushu Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[oshita@ces.kyutech.ac.jp]]></email_address>
			</au>
			<au>
				<person_id>P3737196</person_id>
				<author_profile_id><![CDATA[81504688156]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Reiko]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yamanaka]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Hosei University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[yamanaka@hosei.ac.jp]]></email_address>
			</au>
			<au>
				<person_id>P3737197</person_id>
				<author_profile_id><![CDATA[81456636516]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Masami]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Iwatsuki]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Hosei University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[iwatsuki@hosei.ac.jp]]></email_address>
			</au>
			<au>
				<person_id>P3737198</person_id>
				<author_profile_id><![CDATA[81504684825]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Yukiko]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nakatsuka]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Hosei University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[yukiko.nakatsuka.58@hosei.ac.jp]]></email_address>
			</au>
			<au>
				<person_id>P3737199</person_id>
				<author_profile_id><![CDATA[81504684377]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Takeshi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Seki]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Hosei University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[takeshi.seki.67@adm.hosei.ac.jp]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Oshita, M. 2008. Smart Motion Synthesis. <i>Computer Graphics Forum (Pacific Graphics 2008)</i>, 27(7), 1909--1918.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Brazell, K. (ed.) 1998. <i>Traditional Japanese Theater</i>. Columbia U. Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Easy-To-Use Authoring System for Noh (Japanese Traditional) Dance Animation Masaki Oshita Reiko Yamanaka 
 Masami Iwatsuki Yukiko Nakatsuka Takeshi Seki Kyushu Institute of Technology Hosei University oshita@ces.kyutech.ac.jp 
{yamanaka@, iwatsuki@, yukiko.nakatsuka.58@, takeshi.seki.67@adm.}hosei.ac.jp  Figure 1: The authoring 
system for Noh dance animation. Figure 2: Comparison between the video from motion capture and the synthesized 
motion. 1 Introduction In this article, we introduce an easy-to-use authoring system for Noh (Japanese 
traditional) dance animation. This is a joint research between computer animation and Noh research groups. 
Noh is a genre of Japanese traditional theater, a kind of musical drama which has been performed for 
600 years (Brazell 1998). Similar to other dance forms, Noh dance can also be divided into small, discrete 
units of motion (shosa). The continuous motions performed on stage are written down in katatsuke, a traditional 
notation, and a main actor called Shite dances along with singing or instrumental music by aligning consecutive 
motion units according to the katatsuke. In theory, if we have a set of data of motion units (shosa), 
we can synthesize Noh dance animation by composing them in sequence. However, it is difficult for researchers, 
learners and teachers of Noh dance to utilize existing animation systems. Moreover, simple connection 
of these units is not enough to synthesize continuous dance animations identical to real performances 
by professionals. We are developing an easy-to-use authoring system for Noh dance animation. We employ 
our smart motion synthesis technique (Oshita 2008) to compose motion units automatically. The goal of 
this research is to provide a tool with which people can learn and enjoy Noh dance. It can also be used 
to visualize old-style performances described in historical documents. This can be very useful to the 
researchers of Noh. Another goal of this research is to identify how professional performers move their 
bodies especially during the intervals between motion units, the content of which is not clearly documented 
but passed from teachers to apprentices tacitly through their training. By analyzing the differences 
between the synthesized motion and the motion captured from a professional performer, we expect to grasp 
their way of moving. Our findings would help further development of motion synthesis technique to realize 
natural connecting motions. We also expect that our results can be extended to other kinds of dance forms 
and even non-dance human motions too. 2 Our System On our prototype system (Figure 1), the user can 
select and put motion units on the timeline. We captured about 50 motion units with the help of a professional 
Noh performer. Based on input motion units arranged on the timeline, our system automatically synthesizes 
a continuous motion of the character. Although recent animation authoring systems (e.g. Motion Builder, 
Maya) have similar functions, to get a continuous and natural- looking motion, animators still need to 
adjust the appropriate blending ranges, additional constraints, and motion speeds, etc. These are difficult 
tasks for novices. The advantage of our system is that it automatically determines appropriate synthesis 
method for each pair of sequential motions based on the constraints between the feet of the character 
and the ground during motion. As the user changes the timings of the motion units on the timeline, the 
system interactively changes the output motion. We also implemented a module for generating walking motion 
based on a given path (Figure 1 right), because the performer sometimes walks along a specific path on 
the stage in Noh dance. 3 Evaluation and Future Work With the help of a professional Noh performer, 
we capture the dance motion and compared them with synthesized motions from by our system (Figure 2). 
We have noticed some differences between them. For example, since our system simply blends the upper 
body poses between two motions, the synthesized animation contains unnecessary motions that are parts 
of the original motions or are added in the blending process. The professional performer pointed out 
the difference of subtle moves of the wrists. As future work, we are going to do further analysis and 
extension of our motion synthesis method. We also demonstrated our prototype system to the professional 
performer. He is impressed with our system very much and commented that our system would be a good tool 
to study Noh dance. We are also going to have many people use our system to get more feedbacks. References 
OSHITA, M. 2008. Smart Motion Synthesis. Computer Graphics Forum (Pacific Graphics 2008), 27(7), 1909-1918. 
BRAZELL, K. (ED.) 1998. Traditional Japanese Theater. Columbia U. Press. Copyright is held by the author 
/ owner(s). SIGGRAPH 2012, Los Angeles, California, August 5 9, 2012. ISBN 978-1-4503-1435-0/12/0008 
  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2342902</article_id>
		<sort_key>60</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>5</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>5</seq_no>
		<title><![CDATA[LabaNOHtation]]></title>
		<subtitle><![CDATA[Laban meets Noh]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2342896.2342902</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2342902</url>
		<abstract>
			<par><![CDATA[<p>Labanotation [Hutchinson, 2005] is a graphical notation scheme for describing human body movement that has been widely accepted for the purpose of recording human movements in the fields of choreography and dance education, mainly in Western dance communities. Labanotation is rich in symbols, and by using the full set of symbol s almost all of our body movements can be described.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3737200</person_id>
				<author_profile_id><![CDATA[81453662674]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Worawat]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Choensawat]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Ritsumeikan University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737201</person_id>
				<author_profile_id><![CDATA[81492649120]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Sachie]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Takahashi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Ritsumeikan University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737202</person_id>
				<author_profile_id><![CDATA[81492646814]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Minako]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nakamura]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Ochanomizu University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737203</person_id>
				<author_profile_id><![CDATA[81100078311]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Kozaburo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hachimura]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Ritsumeikan University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Hutchinson, A. 2005. Labanotation: The System of Analyzing and Recording Movement, Theatre Arts Books.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Choensawat, W., Takahashi, S., Choi, W., Nakamura, M., and Hachimura, K. 2010. Description and Reproduction of Stylized Traditional Dance Body Motion by Using Labanotation. Transactions of the Virtual Reality Society of Japan, 15(3), 379--388.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1089893</ref_obj_id>
				<ref_obj_pid>1089870</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Wilke, L., Calvert, T., Ryman, R., Fox, I. 2005. From dance notation to human animation: The LabanDancer Project. Computer Animation and Virtual Worlds, 16 (3-4), 201--211.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Figure2_siggraph.jpg LabaNOHtation: Laban meets Noh Worawat Choensawat* Ritsumeikan University Sachie 
Takahashi Ritsumeikan University Minako Nakamura Ochanomizu University Kozaburo Hachimura Ritsumeikan 
University (a)     (b)    (c) Figure 1: LabaNOHtation. (a) Editing window. (b), (c) 3D CG animation 
of Noh play generated from the score in (a). 1 Introduction Labanotation [Hutchinson, 2005] is a graphical 
notation scheme for describing human body movement that has been widely accepted for the purpose of recording 
human movements in the fields of choreography and dance education, mainly in Western dance communities. 
Labanotation is rich in symbols, and by using the full set of symbols almost all of our body movements 
can be described. We have been working on a system named LabanEditor [Choensawat et al. 2010], which 
is an interactive system for inputting/editing Labanotation scores and displaying 3D CG character animation 
associated with the scores. LabanDancer [Wilke et al. 2005] is also a system for generating animation 
from Labanotation. However, this is principally for Western dances, and it takes no particular account 
of stylized dance motions of other cultures. Recently, we added a unique feature built in LabanEditor 
for handling highly stylized classical Japanese performances, Noh . 2 Labanotation for Noh Plays When 
describing and representing artistic, traditional dance, it is important to have capabilities for handling 
its unique and characteristic body movements. The question is: how can we realize a method of describing 
peculiar features and nuances of artistic, traditional dance movements while suppressing the complexity 
in the notation score? Noh is the most famous and characteristic Japanese classical performing art that 
takes the form of a musical drama. Noh body movement is peculiarly stylized and is not like ordinary 
human movement. From this characteristic, most people understand that it would be difficult to handle 
this kind of stylized traditional movement with Labanotation. Even though it is possible, a resulting 
notation would become very complicated. However, in fact, Noh shares a characteristic similar to Western 
ballet which is composed of unique movement units, or pas . We investigated a method of composing Noh 
Plays using a combination of movement units of Noh, known as Kata, each of which are described with Labanotation. 
In this case, the Labanotation will be interpreted and represented following the above mentioned Noh 
movement standard. * Current address: School of Science and Technology, Bangkok University 3 LabaNOHtation 
 We developed an interactive user interface for composing Noh plays, named LabaNOHtation , by aligning 
predefined Kata in a time line. The Labanotation score and its motion of Kata were carefully described 
in consultation with a Noh player. After selecting Kata, a user can interactively edit a sequence of 
the selected Kata to an appropriate position on a timeline. Then the system automatically converts the 
successive Kata into the Labanotation score as shown in Figure 1(a). In the reproduction of Noh motion, 
we present a dance-style interpretation module embedded in the Noh player model. The embedded module 
enables a Noh player model to interpret the pattern of Labanotation score and select an appropriate dance 
movement to the pattern by using the learned knowledge of the particular dance. Our dance knowledge 
model is a two-layered associative memory where the first layer is for matching between Labanotation 
symbols and their associative poses, and the second is for assigning a sequence of the poses with a motion. 
Figure 1(b) and (c) show snapshots of animation generated from the score in (a). The evaluation of our 
system was undertaken by three Noh professionals from Kanze Noh School. The subjects could easily compose 
a Noh play from the database of Kata. As a result of the evaluation, we are confident that the system 
has a possibility to be used for enlightenment and succession of Noh. Acknowledgments We would like 
to thank Mr. Toyohiko Sugiura for his generous cooperation and valuable advice on Noh plays. References 
 HUTCHINSON, A. 2005. Labanotation: The System of Analyzing and Recording Movement, Theatre Arts Books. 
 CHOENSAWAT, W., TAKAHASHI, S., CHOI, W., NAKAMURA, M., AND HACHIMURA, K. 2010. Description and Reproduction 
of Stylized Traditional Dance Body Motion by Using Labanotation. Transactions of the Virtual Reality 
Society of Japan, 15(3), 379-388. WILKE, L., CALVERT, T., RYMAN, R., FOX, I. 2005. From dance notation 
to human animation: The LabanDancer Project. Computer Animation and Virtual Worlds, 16 (3-4), 201-211. 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2342903</article_id>
		<sort_key>70</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>6</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>6</seq_no>
		<title><![CDATA[Lace curtain: rendering animation of woven cloth using BRDF/BTDF]]></title>
		<subtitle><![CDATA[estimating physical characteristics from subjective impression]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2342896.2342903</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2342903</url>
		<abstract>
			<par><![CDATA[<p>The need for rendering woven fabrics arises frequently in computer graphics. Woven fabrics have specific appearances such as luster and transparency. To express realistic appearance, however, it is necessary to set various parameters ad hoc by trained users. In our previous study, we proposed one solution for issues of luster and transparency by using a physically-based BTDF model of woven cloth on the basis of the GGX [Nomura et al. 2011]. Additionally, rendering fabric motion properties is also significant to enhance the texture of materials in animations [Faure et al. 2009]. In this study, we propose a new method to render woven fabrics which have both realistic appearance and motion.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3737204</person_id>
				<author_profile_id><![CDATA[81328488825]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Atsushi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ishida]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Kwansei Gakuin University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737205</person_id>
				<author_profile_id><![CDATA[81488671886]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Emi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ishigo]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Kwansei Gakuin University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737206</person_id>
				<author_profile_id><![CDATA[81504688443]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Eriko]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Aiba]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Kwansei Gakuin University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737207</person_id>
				<author_profile_id><![CDATA[81100522455]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Noriko]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nagata]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Kwansei Gakuin University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[nagata@kwansei.ac.jp]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1559762</ref_obj_id>
				<ref_obj_pid>1559755</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Faure, F., Volino, P., and Magnenat-Thalmann, N. 2009. A Simple Approach to Nonlinear Tensile Stiffness for Accurate Cloth Simulation. In <i>ACM Transactions on Graphics(TOG)</i>, vol. 28, 105--116.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2037795</ref_obj_id>
				<ref_obj_pid>2037715</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Nomura, S., Ishida, A., Ishigo, E., Okamoto, T., Mizushima, Y., and Nagata, N. 2011. Lace curtain: Modeling and Rendering Woven Cloth using Microfacet BSDF-production of a catalog of curtain animations-. In <i>ACM SIGGRAPH 2011 posters</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 2 3 4 (a)Light intensity/bend and the (b)Light intensity and the thickness of thickness of warp the 
weft Figure 1: Relationship between optic and motion properties *e-mail:nagata@kwansei.ac.jp 5 (b)Pliable 
and middle weight impression. The middle appearance of (a) and (c). Figure 2: The results of curtain 
animations with different subjective impressions. These images are rendered in the same frame time. 6 
Conclusion The results of the evaluation on subjective impression showed the pos­sibility of estimating 
the dynamic properties by the optic properties more directly. The rendering animation with the new method 
which adopted the evaluations of subjective impressions enhanced both real­istic appearance and motions. 
We are planning to generate a catalog of curtain animations that can express various types of woven fabrics 
under arbitrary light conditions. References FAURE, F., VOLINO, P., AND MAGNENAT-THALMANN, N. 2009. 
A Simple Approach to Nonlinear Tensile Stiffness for Accurate Cloth Simulation. In ACM Transactions on 
Graphics(TOG), vol. 28, 105 116. NOMURA, S., ISHIDA, A., ISHIGO, E., OKAMOTO, T., MIZUSHIMA, Y., AND 
NAGATA, N. 2011. Lace curtain: Modeling and Rendering Woven Cloth using Microfacet BSDF-production of 
a catalog of curtain animations-. In ACM SIGGRAPH 2011 posters. Copyright is held by the author / owner(s). 
SIGGRAPH 2012, Los Angeles, California, August 5 9, 2012. ISBN 978-1-4503-1435-0/12/0008 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2342904</article_id>
		<sort_key>80</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>7</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>7</seq_no>
		<title><![CDATA[Particle-based simulation of snow trampling taking sintering effect into account]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2342896.2342904</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2342904</url>
		<abstract>
			<par><![CDATA[<p>In the real world, solid powders are likely to coalesce firmly with each other owing to melting, which is called <i>sintering</i> and arises in various daily scenes. We herein focus on sintering with snow. In the conventional computer graphics, adhesion of snow to shoes and/or feet when trampling into snow is not explicitly represented without any consideration of the sintering effect, neglect of which in the involved scenes would reduce the overall quality of the work. Procedural particle animation insufficiently represents the behavior of snow with the sintering effect due to the complexity of the phenomena.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3737208</person_id>
				<author_profile_id><![CDATA[81504683927]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Tetsuya]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Takahashi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[takahashi@fj.ics.keio.ac.jp]]></email_address>
			</au>
			<au>
				<person_id>P3737209</person_id>
				<author_profile_id><![CDATA[81100355096]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Issei]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Fujishiro]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[fuji@ics.keio.ac.jp]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[{HB02} Herschel, W. H. and Bulkley, R.: <i>Konsistenzmessungen von Gummi-Benzoll&#246;sungen</i>, Springer Berlin/Heidelberg, 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1073400</ref_obj_id>
				<ref_obj_pid>1073368</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[{CBP05} Clavet, S., Beaudoin, P., and Poulin, P.: "Particle-Based Viscoelastic Fluid Simulation," in <i>Proceedings of the 2005 ACM SIGGRAPH/Eurographics Symposium on Computer Animation</i>, pp. 219--228, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
  TetsuyaTakahashiIssei Fujishiro Keio University Figure 1: Simulated snow trampling with sintering 
effect. 1 Introduction In the real world, solid powders are likely to coalesce .rmly with each other 
owing to melting, which is called sintering and arises in various daily scenes. We herein focus on sintering 
with snow. In the conventional computer graphics, adhesion of snow to shoes and/or feet when trampling 
into snow is not explicitly represented without anyconsideration of the sintering effect, neglect of 
which intheinvolved sceneswould reducetheoverall qualityofthework. Procedural particle animation insuf.ciently 
represents the behav­ior of snow with the sintering effect due to the complexity of the phenomena. Against 
this background, we striveto simulate interactions between snow and objects taking the sintering effect 
into account. We as­sume that both snow and rigid bodies can be represented as an ag­gregation of particles. 
We use the SPH (Smoothed Particle Hy­drodynamics) method to approximate the behavior of snow as a .uid. 
Salient features of our simulation approach are three-fold: (1)behaviorofsnowbymodifyingthe viscositytermoftheNavier-Stokes 
equations; (2) adhesion of snow to dynamic rigid bodies by appending the sintering term to the Navier-Stokes 
equations; and (3) thermal conduction of particles including in.uence from the convection of the external 
air. 2 Our Approach 2.1 Behavior of snow Snow is classi.ed as non-Newtonian .uid, whose behavior cannot 
begovernedbythe originalNavier-Stokes equationsforNewtonian .uid. Thus, we represent the behavior of 
snow by introducing a new viscosity term based on the Herschel-Bulkleymodel [HB02], which has been commonly 
used to provide a generalized model of non-Newtonian .uids in the rheology .eld. 2.2 Sintering effect 
Adhesion of snow to objects can be explained as a sintering effect, which cannot be represented by the 
Navier-Stokes equations mod­i.ed in Section 2.1. Clavet et al. [CBP05] appended an attraction force formulated 
by a quadratic function for each particle to have antigravity disposition, and represented .ows along 
the surface of objects. However, this formulation is not able to represent adhesion to dynamic objects 
because the .uid runs away if forced. Thus, we represent the phenomena by introducing into the Navier-Stokes 
equations, a new sintering term which is formulated by the inter­particle pressure, average of sintering 
coef.cients between parti­cles, and particles properties such as temperature and moisture. As a side 
effect of this, snow particles could intrude excessively into a rigid body when collisions are detected 
between particles as long as we rely on the traditional SPH method.To address this problem, e-mail: takahashi@fj.ics.keio.ac.jp 
e-mail: fuji@ics.keio.ac.jp back. well. 2.3 Thermal conduction method. by the convection of the external 
air. 3 POV-Ray ver. 3.62. Fig. 1 shows key snapshots excerpted from the accompanying video, where we 
used about 131k and 24k particles for snow and a shoe, respectively. From can clearly observe snow com­pressed 
with the weight of the shoe ((b)-(c)); and adhesion of snow to the shoe bottom and scatter of some adhered 
parti­cles into the air ((d)-(f)). Fig.2 shows a particle representation of the simulation result corresponding 
to the scene (e) in Figure 1. An average frame rate of the simulation (without rendering) is 0.4framepersecondonastandardPC(CPU:Corei78602.80GHz, 
RAM: 6.00GB) without using GPU. Additional results are included in the accompanying video. In future 
work, we would improve the adhesion among snow par­ticles taking into account other kinds of physics, 
and attempt to simulate the sintering effect with other sorts of substances such as metal powder, ceramics 
and glass.  References [HB02] Herschel,W. H. and Bulkley, R.: Konsistenzmessungen von Gummi-Benzoll¨osungen, 
Springer Berlin/Heidelberg, 2002. [CBP05] Clavet, S., Beaudoin,P., and Poulin,P.: Particle-Based Viscoelastic 
Fluid Simulation, inProceedings of the 2005 ACM SIGGRAPH/Eurographics Symposium on Computer Animation, 
pp. 219-228, 2005. Copyright is held by the author / owner(s). SIGGRAPH 2012, Los Angeles, California, 
August 5 9, 2012. ISBN 978-1-4503-1435-0/12/0008 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2342905</article_id>
		<sort_key>90</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>8</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>8</seq_no>
		<title><![CDATA[Shape deformation using freeform deformation axis]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2342896.2342905</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2342905</url>
		<abstract>
			<par><![CDATA[<p>2D Shape deforming techniques has been presented for various applications recently. Particularly, gradient domain 2D deformation techniques shown promising result in the context of visually pleasing deformation and interactive performance.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3737210</person_id>
				<author_profile_id><![CDATA[81502734338]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Eisung]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sohn]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Yonsei University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[essohn@mglab.yonsei.ac.kr]]></email_address>
			</au>
			<au>
				<person_id>P3737211</person_id>
				<author_profile_id><![CDATA[81451600047]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Yoon-Chul]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Choy]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Yonsei University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[ycchoy@mglab.yonsei.ac.kr]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1365226</ref_obj_id>
				<ref_obj_pid>1365091</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Guo, H., Fu, X., Chen, F., Yang, H., Wang, Y., and Li, H. 2008. As-rigid-as-possible shape deformation and interpolation. <i>Journal of Visual Communication and Image Representation 19</i>, 4, 245--255.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1165416</ref_obj_id>
				<ref_obj_pid>1165407</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Weng, Y., Xu, W., Wu, Y., Zhou, K., and Guo, B. 2006. 2D shape deformation using nonlinear least squares optimization. <i>The Visual Computer 22</i>, 9 (Sept.), 653--660.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Shape Deformation Using Freeform Deformation Axis Eisung Sohn and Yoon-Chul Choy* Yonsei University 
  (a) (b) (c) (d) (e) (f) Figure 1: An example deformation image of sprout using our freeform deformation 
axis (a) original image and a drawn freeform deformation axis (b) a generated mesh according to FDA s 
thickness value (c) setup of position constraints (d)-(f) shape deformation results 1 Introduction 2D 
Shape deforming techniques has been presented for various ap­plications recently. Particularly, gradient 
domain 2D deformation techniques shown promising result in the context of visually pleas­ing deformation 
and interactive performance. However, since these approaches treat the whole shape as a simple .at shape 
without structural meaning, the semantic shape properties such as thickness or segment length are not 
taken into account. In this work, we propose a 2D shape deformation algorithm that deforms shapes using 
thin, deformable freeform skeletal structure called freeform deformation axis (FDA). Our algorithm falls 
into same category with other approaches which are based on nonlinear least squares optimization[Weng 
et al. 2006] [Guo et al. 2008]. The key difference of our approach is that we do not directly manipulate 
the target shape, but .rst deform the freeform deformation axis and use it as main handle for the desired 
deformation. This concept sep­arates the shape and the manipulation metaphor and enables user to manipulate 
various arbitrary shapes in as-rigid-as possible manner and to change the shape attributes intuitively. 
 (a) (b) (c) Figure 2: FDA s deformations showing area preservation.  2 Our approach The user .rst begin 
with drawing a FDA on a given bitmap image. Then the user draws contour line to assign the thickness 
value to the FDA. The system then calculates the appropriate thickness of both side of the FDA and updates 
the thickness value to the result. Thus, the user determines internal structure of deformable shape for 
representing desired deformation at this initial stage. To calculate the thickness value from the contour 
line, we .rst test the intersection of each edges normal vector of FDA with contour line. Next, the norm 
of the vector, which is from the intersection point to the edges center point, is assigned as the thickness 
value of one side of the FDA. The opposite side s thickness value can also be calculated in the same 
way. Lastly, our system generate the mesh structure based on the user designated FDA. Our algorithm preserves 
two shape constraints during shape defor­mation: line segment lengths and laplacian coordinates, which 
are represented in a non-quadratic energy function. An iterative Gauss-Newton method is used to minimize 
this non-linear energy function. Our algorithm also support ef.cient area preservation based on the length 
of the FDA s segment as shown in Figure 2. Our approach enables the user to easily determine the internal 
de­formable structure and to change shape s attribute dynamically. Also, our approach shows promising 
deformation result and per­formance.  References GUO, H., FU, X., CHEN, F., YANG, H., WANG, Y., AND 
LI, H. 2008. As-rigid-as-possible shape deformation and interpolation. Journal of Visual Communication 
and Image Representation 19, 4, 245 255. WENG, Y., XU, W., WU, Y., ZHOU, K., AND GUO, B. 2006. 2D shape 
deformation using nonlinear least squares optimiza­tion. The Visual Computer 22, 9 (Sept.), 653 660. 
*e-mail:{essohn, ycchoy}@mglab.yonsei.ac.kr Copyright is held by the author / owner(s). SIGGRAPH 2012, 
Los Angeles, California, August 5 9, 2012. ISBN 978-1-4503-1435-0/12/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	</section>
	<section>
		<section_id>2342906</section_id>
		<sort_key>100</sort_key>
		<section_seq_no>2</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Hardware]]></section_title>
		<section_page_from>2</section_page_from>
	<article_rec>
		<article_id>2342907</article_id>
		<sort_key>110</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>9</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[A colloidal display]]></title>
		<subtitle><![CDATA[membrane screen that combines transparency, BRDF and 3D volume]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2342896.2342907</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2342907</url>
		<abstract>
			<par><![CDATA[<p>It is a common knowledge that the surface of soap bubble is a micro membrane. It allows light to pass through and displays the color on its structure. We developed an ultra thin and flexible BRDF screen using the mixture of two colloidal liquids. There have been several researches on dynamic BRDF display[1] in the past. However, our work is different in several points. Our membrane screen can be controlled using ultrasonic vibrations. Membrane can change its transparency and surface states depending on the scales of ultrasonic waves. Based on these facts, we developed several applications of the membranes such as 3D volume screen.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3737212</person_id>
				<author_profile_id><![CDATA[81466641621]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Yoichi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ochiai]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[the University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[ochyai@me.com]]></email_address>
			</au>
			<au>
				<person_id>P3737213</person_id>
				<author_profile_id><![CDATA[81504682972]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Alexis]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Oyama]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Carnegie Mellon University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737214</person_id>
				<author_profile_id><![CDATA[81482649155]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Keisuke]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Toyoshima]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Tsukuba]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[M. Hullin et al., Dynamic Display of BRDFs. In: Proceedings of Eurographics 2011.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 A Colloidal Display:membrane screen that combines transparency, BRDF and 3D volume Yoichi Ochiai* Alexis 
Oyama** Keisuke Toyoshima*** *the University of Tokyo **Carnegie Mellon University ***University of Tsukuba 
 Figure 1: (top-left)controllable transparency, (top-center) sardine s back texture (top-right) wood 
texture on membrane screen (bottom-left) plane based 3D screen (bottom-center) metal texture on screen 
(bottom-right) piercing the object into the screenIt s very dif.cult to take the picture as we see this 
display. The re.ection of projector light is dif.cult to capture in cameras. 1. Introduction It is a 
common knowledge that the surface of soap bubble is a micro membrane. It allows light to passthrough 
and displays the color on its structure. We de­veloped an ultra thin and .exible BRDF screen using themixture 
of two colloidal liquids. There have been severalresearches on dynamic BRDF display[1] in the past. However, 
our work is different in several points. Our membrane screen can be controlled using ultrasonic vi­brations. 
Membrane can change its transparency and surface states depending on the scales of ultrasonic waves. 
Based on these facts, we developed several ap­plications of the membranes such as 3D volume screen. The 
combination of the ultrasonic waves and ultra thin membranes makes more realistic, distinctive, and vivid 
imageries on screen. This system contributes to open up a new path for display engineering with sharp 
imageries, transparency, BRDF and .exibility. 2. Design We developed the .rst prototype by using soap 
and milk. These are colloidal liquids and their moleculeshave different sizes and colors. With ultrasonic 
paramet­ric speakers, we could control their movements (liquidsand particles) on the membranes. If they 
move with in­tensity, the re.ections change while the membrane works as a projector screen. Since we 
can control thedynamics of the wavelengths, the state of the surface can be easily changed.    0-4000Hz 
Ultrasonic modulated by 50kHz Light membrane surface -40° to 40° display transparency transparency With 
the parametric speakers, this system can make thescreens drastically thinner since it does not need anyadditional 
systems or materials on the screen. The thin­ness of the screen is approximately 1 micrometer. Sincewe 
could control the state of the surface, we were able to have several interactions such as piercing a 
.nger through it or enlarging it by using its elasticity and .exi­bility. 3. Application First we developed 
a screen for displaying realisticmaterial. The display s state changes in correspondenceto the images 
the projector shows. Secondly, we developed the plane based 3D screen with three membranes using a single 
projector. (chang­ing frequency 25Hz) The transparency of each mem­brane is controllable by frequency 
of the sound from theparametric speakers. We set the projector and linked it with the transparency of 
each membrane. In addition, we developed polyhedrons made fromthese membranes and displayed several images 
on it. This system shows that this is useful to .exibly display 3D objects. 4. Future Work We introduced 
the .rst prototype of a new kind of dis­play by using colloidal liquids and a method of control­ling 
them by using ultrasonic waves. Due to its thinnessand transparency, the method could be applied to a 
vari­ety of cases in using display technologies. The 3Ds screen and texture screen are exemplary applications 
of this system. Currently our membrane can maintain its surface (screen) for 5 minutes. However, there 
are several solu­tions for this problem and there are rooms for new po­tential colloidal materials for 
the use. REFERENCES [1] M. Hullin et al., Dynamic Display of BRDFs. In: Proceed­ings of Eurographics 
2011. Figure2: ultrasonic &#38; membrane Copyright is held by the author / owner(s). SIGGRAPH 2012, 
Los Angeles, California, August 5 9, 2012. ISBN 978-1-4503-1435-0/12/0008 *email: ochyai@me.com 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	</section>
	<section>
		<section_id>2342908</section_id>
		<sort_key>120</sort_key>
		<section_seq_no>3</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Animation]]></section_title>
		<section_page_from>3</section_page_from>
	<article_rec>
		<article_id>2342909</article_id>
		<sort_key>130</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>10</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Synthesis of a video of performers appearing to play user-specified band music]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2342896.2342909</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2342909</url>
		<abstract>
			<par><![CDATA[<p>We propose a novel method to synthesize a video of multiple performers appearing to play the user-specified band music. The performance video often helps us to enjoy or understand the music. For example, watching the performance of a bass player makes it easier for us to focus on the bass sound, which is an experience different from listening to the music just using speakers or headphones. Our approach is based on the database of performance videos: given the band music, our system chooses appropriate footages from the database, slightly modifies their speeds and timings according to the music, and then concatenates them as the resulting video. In existing research, there are several methods proposed for synchronizing videos to a user-specified music, e.g., to synthesize a dance video [Nakano et al. 2011]. These methods synchronize a video to the music by analyzing its mood based on tempo or chord changes in the music. On the other hand, since we want to synthesize a performance video and require more precise synchronization, we analyze the music extracting the timings of musical notes from the audio signal (Fig.1). We perform best match search using the timings as feature vector, and copy the footage that has a similar set of timings (Fig.1-a and b). We demonstrate that our method enables to create a performance video of a band music, which is a fake but looks interesting.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3737215</person_id>
				<author_profile_id><![CDATA[81504681987]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Tomohiro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yamamoto]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Electro-Communications]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[yamamoto@onailab.com]]></email_address>
			</au>
			<au>
				<person_id>P3737216</person_id>
				<author_profile_id><![CDATA[81100546396]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Makoto]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Okabe]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Electro-Communications and JST Presto]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[m.o@acm.com]]></email_address>
			</au>
			<au>
				<person_id>P3737217</person_id>
				<author_profile_id><![CDATA[81548027593]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Rikio]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Onai]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Electro-Communications]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[onai@onailab.com]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Nakano, T., Murofushi, S., Goto, M., and Morishima, S. 2011. Dancereproducer: An automatic mashup music video generation system by reusing dance video clips on the web. In <i>Proc. of SMC</i>, 183--189.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1776739</ref_obj_id>
				<ref_obj_pid>1776684</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Smaragdis, P., Raj, B., and Shashanka, M. 2007. Supervised and semi-supervised separation of sounds from single-channel mixtures. In <i>Proc. of ICA</i>, 414--421.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Synthesis of a Video of Performers Appearing to Play User-Speci.ed Band Music Tomohiro Yamamoto* Makoto 
Okabe Rikio Onai The University of Electro-Communications * JST PRESTO 1 Introduction We propose a 
novel method to synthesize a video of multiple per­formers appearing to play the user-speci.ed band music. 
The per­formance video often helps us to enjoy or understand the music. For example, watching the performance 
of a bass player makes it easier for us to focus on the bass sound, which is an experience different 
from listening to the music just using speakers or headphones. Our approach is based on the database 
of performance videos: given the band music, our system chooses appropriate footages from the database, 
slightly modi.es their speeds and timings according to the music, and then concatenates them as the resulting 
video. In existing research, there are several methods proposed for synchro­nizing videos to a user-speci.ed 
music, e.g., to synthesize a dance video [Nakano et al. 2011]. These methods synchronize a video to the 
music by analyzing its mood based on tempo or chord changes in the music. On the other hand, since we 
want to synthesize a performance video and require more precise synchronization, we analyze the music 
extracting the timings of musical notes from the audio signal (Fig.1). We perform best match search using 
the tim­ings as feature vector, and copy the footage that has a similar set of timings (Fig.1-a and b). 
We demonstrate that our method enables to create a performance video of a band music, which is a fake 
but looks interesting. Figure 1: The system overview 2 Our Approach We constructed a video database 
for each instrument independently. We asked each performer of viola, bass and drums to play the in­strument 
for one hour. For each video, we analyze its audio track to estimate the timings of the musical notes 
(Fig.2-c). The process starts by applying the short-time Fourier transform (STFT) to the audio signal. 
The spectrogram is usually noisy (Fig.2-a). To smooth it preserving a strong edge corresponding to the 
beginning and end of each musical note, we apply a bilateral .lter. We differentiate the smoothed spectrogram 
horizontally to extract the beginning and end of each musical note. Then, we integrate the spectrogram 
vertically to obtain one-dimensional (1D) signal (Fig.2-b). Finally, we extract the feature vector by 
.nding local maxima of the 1D signal (Fig.2­c). We show the score that the performer actually plays (Fig.2-d). 
The peaks match the timing of the score. The process described above is directly applied to the videos 
of viola and bass. However, in the video of drums, the audio track *e-mail:yamamoto@onailab.com e-mail.m.o.acm.org 
e-mail:onai@onailab.com Figure 2: Extracting the feature vector is mixture of four instruments, snare 
drum, bass drum, cymbals, and hi-hats. To analyze each sound, we apply audio source sep­aration technique 
to the audio signal. We use probabilistic latent component analysis (pLCA) [Smaragdis et al. 2007]. Fig. 
3 shows the result, where the original signal is separated into the four com­ponents. Actually, our drum 
kit has toms, but we distribute their sounds into the component of snare drum. Figure 3: Audio source 
separation The basic idea to synthesize the video is to compare the feature vectors between the input 
music and the video database and select a footage that has a feature vector similar to a part of the 
input mu­sic.We perform this process for each instrument. To increase the search rate, we allow slight 
differences between the feature vectors , which are solved in the rendering process by automatically 
modi­fying the speeds and timings of the footage using time remapping. 3 Results We applied our method 
to two band music, 1) Etupirka of Taro Hakase, and 2) Let It Be of The Beatles. Given input music, we 
apply the same analysis as is used to construct the database and create the feature vector. Since Etupirka 
is originally a wave .le, we applied pLCA to it to separate it into the violin part and the drum part. 
Let It Be is provided on the web with a violin solo part as a wave .le, and the other parts as MIDI .les. 
Given a 30 second audio track as input music, our method takes 30 to 40 seconds to search for footages 
for each instrument The selected footages are summarized with the information of time remapping as a 
script of Adobe After Effects. The user loads it into the software and renders the .nal video. References 
NAKANO, T., MUROFUSHI, S., GOTO, M., AND MORISHIMA, S. 2011. Dancereproducer: An automatic mashup music 
video generation system by reusing dance video clips on the web. In Proc. of SMC, 183 189. SMARAGDIS, 
P., RAJ, B., AND SHASHANKA, M. 2007. Su­pervised and semi-supervised separation of sounds from single­channel 
mixtures. In Proc. of ICA, 414 421. Copyright is held by the author / owner(s). SIGGRAPH 2012, Los Angeles, 
California, August 5 9, 2012. ISBN 978-1-4503-1435-0/12/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	</section>
	<section>
		<section_id>2342910</section_id>
		<sort_key>140</sort_key>
		<section_seq_no>4</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Design]]></section_title>
		<section_page_from>4</section_page_from>
	<article_rec>
		<article_id>2342911</article_id>
		<sort_key>150</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>11</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Turn]]></title>
		<subtitle><![CDATA[a virtual pottery by real spinning wheel]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2342896.2342911</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2342911</url>
		<abstract>
			<par><![CDATA[<p>Is it possible to transfer real-world sculpting expertise to a virtual space? Through digital pottery, the answer is yes. Digital pottery is a collection of systems that makes it convenient to make pottery in 3D space. In this study, we present a natural & tangible user interface system that fluently connects users' real-world sculpting experience to virtual pottery making. Introducing a real spinning wheel, we could extend the physical concept of making pottery into 3D space without the burden of bridging the gaps between real & virtual worlds. As a result, we could also retain its unique characteristics of pottery by preserving its essential mechanism of shape formation: Turn.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3737218</person_id>
				<author_profile_id><![CDATA[81504684716]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Sungmin]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Cho]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Seoul National University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[sungmins@snu.ac.kr]]></email_address>
			</au>
			<au>
				<person_id>P3737219</person_id>
				<author_profile_id><![CDATA[81421594806]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Yunsil]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Heo]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Seoul National University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[yunsil@snu.ac.kr]]></email_address>
			</au>
			<au>
				<person_id>P3737220</person_id>
				<author_profile_id><![CDATA[81421599972]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Hyunwoo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Seoul National University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[savoy@snu.ac.kr]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>261171</ref_obj_id>
				<ref_obj_pid>261135</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Kameyama, K.: Virtual clay modeling system. In: <i>Proceedings of the ACM Symposium on Virtual Reality Software and Technology</i> pp, 227--234 (1997)]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>37422</ref_obj_id>
				<ref_obj_pid>37402</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[W.E.Lorenson and H. E. Cline. Marching cubes: A high resolution 3d surface construction algorithm. <i>Computer Graphics</i>, 21(4):163--169, July 1987]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 C:\Users\Vonham\Desktop\1.jpg http://everyware.kr/portfolio/contents/11_turn/imgs/best6.jpg Turn: A 
Virtual Pottery by Real Spinning Wheel Sungmin Cho*       Yunsil Heo*     &#38; Hyunwoo Bang* 
New Media Lab  Department of Crafts &#38; Design   New Media Lab Mechanical &#38;   School of Arts 
    Mechanical &#38; Aerospace Engineering  Seoul National University    Aerospace Engineering 
Seoul National University     Seoul National University  Figure 1(a)Virtual pottery making (b)Image 
seen by the depth camera and rendition of virtual clay (c)output samples 1. Introduction Is it possible 
to transfer real-world sculpting expertise to a virtual space? Through digital pottery, the answer is 
yes. Digital pottery is a collection of systems that makes it convenient to make pottery in 3D space. 
In this study, we present a natural &#38; tangible user interface system that fluently connects users' 
real-world sculpting experience to virtual pottery making. Introducing a real spinning wheel, we could 
extend the physical concept of making pottery into 3D space without the burden of bridging the gaps between 
real &#38; virtual worlds. As a result, we could also retain its unique characteristics of pottery by 
preserving its essential mechanism of shape formation: Turn. 2. Interaction A wooden rotating wheel is 
placed in front of an LED display panel. When you spin the wheel, an imaginary spinning wheel in the 
virtual world also revolves correspondingly. As you move your other hand over the wheel, you can see 
a digital rendition of clay that smears out from the tip of your fingers like toothpaste on the screen(Figure1-a). 
As we designed this virtual space, you can put, add, subtract &#38; modify clay at anywhere you want 
without considering real world constraints like gravity. This gives you another level of freedom to create 
unique shapes shown on the images above while preserving the tangibility of real world pottery. 3. Implementation 
The Kinect sensor from Microsoft installed on top of the display panel traces your hand in 3D space. 
The position of your hand is calibrated in real time based on the relative position from the top plate 
of the spinning wheel. A wireless mouse is installed upside down beneath the spinning plate sensing the 
speed of rotation with its optical sensor without giving any friction to the moving parts. Raw depth 
information obtained from the Kinect sensor is processed &#38; transformed to construct 3D points cloud 
above the imaginary spinning wheel in the virtual space. A graphical sculpting tool gizmo gives you the 
clue where your hand is at and what you re doing now. The imaginary cylinder above the spinning wheel 
constructs a control volume that limits the span of your gestural inputs where a volumetric rendering 
of processed points cloud is drawn as a sculpted pottery(Figure1-b). This volumetric data is computed 
as iso-surfaces by employing the marching cubes algorithms for 3D arrays. This algorithm allows the final 
3D model to be a free-from surface whose surface continuity is maintained. We used OpenGL with GLSL for 
the 3D visualizations. 4. Conclusion In this study, we propose a digital pottery system that successfully 
reflects real-world gestures to their virtual counterparts. By introducing a tangible user interface 
that extends real-world experiences, "Turn" offers a natural, intuitive and interactive virtual 3D modeling 
method. Acting out the sculpting experience on virtual space, users can easily create organic forms of 
virtual pottery that can hardly be made in real-world circumstances (Figure1-c). In addition, this spinning-wheel 
metaphor can be extended to various design interfaces including architectural modeling, product designs 
and fine art sculptures. Digital pottery is a good example for utilizing our technique but it is not 
the last stop of this interface. Turn opens a new way of modeling method feasible for making intuitive 
virtual 3D modeling environments. It will offer a revolutionary way to extend users expertise of real-world 
sculpting to its virtual counterpart. 5. Reference Kameyama, K.: Virtual clay modeling system. In: Proceedings 
of the ACM Symposium on Virtual Reality Software and Technology pp, 227-234 (1997) W.E.Lorenson and H.E.Cline. 
Marching cubes: A high resolution 3d surface construction algorithm. Computer Graphics, 21(4):163-169,July 
1987 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	</section>
	<section>
		<section_id>2342912</section_id>
		<sort_key>160</sort_key>
		<section_seq_no>5</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Art]]></section_title>
		<section_page_from>5</section_page_from>
	<article_rec>
		<article_id>2342913</article_id>
		<sort_key>170</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>12</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Design ornamentation &#38; fabrication by multi agent system]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2342896.2342913</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2342913</url>
		<abstract>
			<par><![CDATA[<p>From a historical perspective, we were extremely intrigued by the organizational complexity and integrity of traditional ornamentation styles in architecture & furniture design. One of the remarkable styles of interest is Rococo or "Late Baroque", which evolved in France & Italy for architectural structures and furniture d&eacute;&#233;cor. This research project is conceived from the notion of how design computation could be a valid medium to generate ornamentation in design, comparable to the one found in traditional architectural & furniture d&eacute;&#233;cor style. The research work predominantly studies the extent of ornamentation in Rococo Style at the scale of both Architectural Design and Product Design. Rococo was chosen to understand complexity & structural construction of ornamentation & motif formation. The rich grandeur and florid beauty of the motifs and the sculptural tectonics in Rococo is highly appreciated and thus, intent of the project was to develop a self-organized computational framework, which is calibrated to have the potential to form complex designed artifacts & art forms having unique ornate quality. Adding to that, the project strictly adheres to the objective to create a computational methodology or script which can provide series of such designed artifacts, based on designers input parameters and initial conditions.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3737221</person_id>
				<author_profile_id><![CDATA[81504684146]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Subhajit]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Das]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Pennsylvania, Philadelphia, PA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[subhajit.design@gmail.com]]></email_address>
			</au>
			<au>
				<person_id>P3737222</person_id>
				<author_profile_id><![CDATA[81504688415]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Florina]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Dutt]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Pennsylvania, Philadelphia, PA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[scorpio.rina@gmail.com]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>645696</ref_obj_id>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Jaime, S. Sichman, Rosaria, and C., Nigel G., 1998 Multi-Agent Systems and Agent-Based Simulation: <i>First International Workshop, MABS '98</i>, Paris, France, Proceedings (Lecture Notes in Artificial Intelligence) (v. 1534)]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1695886</ref_obj_id>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Wooldridge, M., 2009, An Introduction to Multi Agent Systems - Second Edition, John Wiley &amp; Sons]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 DDesign Orrnamentaation &#38; Faabricationn by Multti Agent SSystem Subhajjit Das University of Pennsylvania 
Philadelphiaa, PA, USA subhajit.designn@gmail.com 11. Introductioon FFrom a historicall perspective, 
wee were extremelly intrigued by thhe oorganizational complexity aand integrity of traditionnal oornamentation 
sttyles in architeccture &#38; furniturre design. One of thhe remarkable sstyles of interesst is Rococo 
orr "Late Baroquee", wwhich evolved inn France &#38; Itally for architectuural structures annd fufurniture 
décor. TThis research prooject is conceiveed from the notioon oof how design coomputation coulld be a 
valid meedium to generaate oornamentation iin design, commparable to the one found in trraditional archittectural 
&#38; furnniture décor styyle. The researcch wwork predominaantly studies the extent of oornamentation 
in RRococo Style aat the scale of both Architecttural Design annd PProduct Design. Rococo was ch osen 
to understaand complexity &#38; sstructural construuction of ornammentation &#38; mottif formation. Thhe 
rich grandeur annd florid beauty of the motifs aand the sculpturral teectonics in Roc oco is highly apppreciated 
and tthus, intent of thhe pproject was to deevelop a self-orgganized computaational frameworrk, wwhich 
is calibrrated to have the potential to form compleex ddesigned artifac ts &#38; art formss having uniquue 
ornate qualitty. AAdding to that, the project striictly adheres too the objective to ccreate a computaational 
methodoology or script wwhich can providde sseries of such designed artifaacts, based on designers inpput 
pparameters and innitial conditions . 22. Computational Framewwork TThe coding for the computationnal 
framework wwas done in javva bbased applet Pro cessing. We we re interested to learn and develoop frfrom 
the self-orrganized interactions/ simulatioon of multi ageent ssystem, where thhe embedded artificial 
intelligennce of each ageent reacts with eac h other based on predefined logistics. The se reactions 
and feeedback proceduure imparted a visual pattern of sself-organized coomplex system, having aestheticc 
qualities simillar too traditional ornnamentation styyle. Research wwork encompasseed ppheromone trail 
formation in AAnt colonies (as exhibited in AAnt CColony optimizzation methodss), by which ants lay dowwn 
ppheromones direecting each othher to resourcess while explorinng thheir environmennt. This trail obbtained 
after laaying pheromonnes ggets stronger &#38; stronger in furthher simulations, essentially baseed oon 
the ants whoo achieve strongger solution sets in the previouus ssimulation. At the designer ss level, 
this wweb of compleex ppheromone traill in 2 or 3 ddimensions is of aesthetic annd oornamental qualiity, 
which couldd be used as a pootential artifact of hhigh dimension ssculptural tectonnics. 33. 3D formati 
on &#38; Digital Fabrication OOnce the system is established annd the coded simmulation produceed ddesired 
results, tthe 2 dimensionnal frameworks was converted to 33Dimensional setup with the addition of zz- 
vector and z Florina Duutt Uniiversity of Pen nnsylvania Philadelphia, PAA, USA scoorpio.rina@gmmail.com 
 coordinnates on each off the agents in alll of their respecctive classes. This tiime their 3 d imensional 
simmulations of reaactions and interacttion resulted innto similar coheerent and controolled design ornameentation 
in 3-dimmension space. EEventually, anothher program code wwas written to form iso-surfaccing between 
thhe 3D point cloud obtained from the 3D ornammentation patternn from the simulattion results. Thee iso 
surfaced mmesh was exporteed as an .obj file andd imported in GGeomagic platfo rm where it waas fine-tuned 
with fuurther mesh optimmization and acccuracy. Fiigure 1. Images depicting the 3dd pattern organiization. 
4. Conclusions The addvanced comput ational coding aand digital fabriication tools allowedd us to producee 
ornate artifactss of desired scalle, precision and prooportion. The prrocedure followwed clearly unravvels 
a novel paradiggm of design enndeavor wherein very complex aand intricate design patterns can bbe computationaally 
studied, annalyzed and reinvennted. The researrch project not only facilitates a genre of ornate digital 
crafts, buut at the same tiime documents tthe complex computtational framewwork adapted too synthesize 
suuch kind of design work. The digiital crafts so prooduced can be successfully appliedd at all the scalle 
of furniture ddécor, building fenestration componnents or ornameental units, sculppture design and so 
on.  Referrences JAIMEE, S. SICHMANN, ROSARIA, AAND C., NIGEEL G. ,1998 Multti-Agent Systemms and Agentt-Based 
Simulaation: First Interrnational Worksshop, MABS '98,, Paris, France, P Proceedings (Leccture Notes 
in Arrtificial Intelligeence) (v. 1534) WOOLLDRIDGE, M., 2009 ,An Inttroduction to MMulti Agent Systeems 
- Second Eddition, John Wil ley &#38; Sons Copyright is held by the author / owner(s). SIGGRAPH 2012, 
Los Angeles, California, August 5 9, 2012. ISBN 978-1-4503-1435-0/12/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2342914</article_id>
		<sort_key>180</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>13</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Door]]></title>
		<subtitle><![CDATA[the evolution of messenger and analogue emotion]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2342896.2342914</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2342914</url>
		<abstract>
			<par><![CDATA[<p><b>Door</b> is an interactive art within an emotional technology, and it has been studied about human communication in modern cities through the symbolic meaning of the door. Today, an instant messenger or a mobile phone is a general way for conversation, and we are familiar with such methods of communication. Digital technology has made life dramatically easier for us to talk to others immediately. Nevertheless, this type of relation with people is not likely to last long because an instant messenger has emphasized the importance of giving information without an emotional exchange. This new type of human network has been made in virtual world with extremely limited human touch. Door is an excellent alternative to overcome the limitations of digital technology.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3737223</person_id>
				<author_profile_id><![CDATA[81504687961]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Kim]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Jaeyoung]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Soongsil University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737224</person_id>
				<author_profile_id><![CDATA[81474698911]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Kang]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Byongsue]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Soongsil University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737225</person_id>
				<author_profile_id><![CDATA[81474694021]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Kim]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Semi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Soongsil University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737226</person_id>
				<author_profile_id><![CDATA[81474697276]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Jo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hwanik]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Soongsil University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737227</person_id>
				<author_profile_id><![CDATA[81504688236]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Choi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bonhwa]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Soongsil University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737228</person_id>
				<author_profile_id><![CDATA[81474704296]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Sung]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Junghwan]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Soongsil University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Michael Margolis "Arduino Cookbook", O'Reilly, U. S. A. 2011, PP.438--450.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Dan O'Sullivan, Tom Igoe, "PHYSICAL COMPUTING" MATERIAL, U. S. A. 2004]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Door : The Evolution of Messenger and Analogue Emotion Kim Jaeyoung, Kang Byongsue, Kim Semi, Jo Hwanik, 
Choi Bonhwa, Sung Junghwan Cross Design Lab, Soongsil University         Figure 1. Demonstration 
of Door 1. Introduction Door is an interactive art within an emotional technology, and it has been 
studied about human communication in modern cities through the symbolic meaning of the door. Today, an 
instant messenger or a mobile phone is a general way for conversation, and we are familiar with such 
methods of communication. Digital technology has made life dramatically easier for us to talk to others 
immediately. Nevertheless, this type of relation with people is not likely to last long because an instant 
messenger has emphasized the importance of giving information without an emotional exchange. This new 
type of human network has been made in virtual world with extremely limited human touch. Door is an excellent 
alternative to overcome the limitations of digital technology. Door is tracing patterns of electronic 
communication in major urban conurbations. This installation bridges the space gaps of virtual and real 
world. We have designed an inventive form with the small door with physical computing and network technology. 
We materialized the method of communication in an architectural way after a careful research for interface 
between human and machines. In this installation, voices were visualized by material media such as the 
door. Architecture as the door has the dual knowledge that links up and blocks out the space. If you 
will open a door, it means that you want to connect the spaces and you are ready to move on to the next 
stage for communication. This work converts from the digital method to the analogue such as knock and 
open the door. Door is created to give people the warm emotion and more humane communication. 2. Technical 
Details A potentiometer is embedded in the upper edge of Door part and a servo motor is established 
in Door part on the bottom. The middle of Door part, a piezo sensor and a RFID reader are inserted in. 
If Door opens, the potentiometer sends a revolved angle value of Door part to other Doors' servo motor 
though Arduino(open-source single-board microcontroller) which are embedded under the bottom part of 
the work. The piezo sensor gets signals when a spectator knocks his/her Door, and then the sensor value 
is sent to another Door and it plays a knocking sound. Finally, there is "Do not disturb" card that has 
RFID chip in it. If the card is hung on one of Doors handle, the RFID reader sends computer a signal 
to have Door temporary dormant. 3. Experience scenario and Conditions          Figure 2. Installation 
conditions If you want to talk to door pal , turns your handle last and turns on the light in the upper 
edge of Door. Now, you are ready to move next step for communication. You have to knock on the door before 
communication and wait. If door pal answers to your knocking, you can open the door. Pal s door is opened 
simultaneously with yours. Now, You can have an everyday conversation. After talk, don t forget to close 
the door. Moreover, you can use Door as a special decoration with romantic lighting and personal speaker. 
 4. Results and Conclusions It is not difficult to find out the metaphor of a door when we actually 
encounter it, which has simple function : Open and Close. Door s interface has noticed an affordance 
within the door. And this project poetically represented user s voices and explored a way to facilitate 
the interaction and communication between people. Door shows the visible, which conceals the invisible 
layers (Electronic signals for human communication). And this installation gives body to user-centered 
experience As invisible layers become visualized, the concept of space can be expanded in our lives. 
 References MICHAEL MARGOLIS ARDUINO COOKBOOK , O'REILLY, U.S.A. 2011, PP.438-450. DAN O'SULLIVAN,TOM 
IGOE, "PHYSICAL COMPUTING" MATERIAL, U.S.A. 2004 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2342915</article_id>
		<sort_key>190</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>14</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Growing documentary]]></title>
		<subtitle><![CDATA[creating a collaborative computer-supported story telling environment]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2342896.2342915</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2342915</url>
		<abstract>
			<par><![CDATA[<p>Technological innovations in the fields of digital video production, distribution as well as broadband network access and speeds have made creating and sharing digital video contents into a simple process that can be performed by anyone, anywhere. Although the process of sharing contents has undergone a great change with respect to traditional distribution models, the contents themselves still tend to follow traditional linear narrative structures, construction and production workflows.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3737229</person_id>
				<author_profile_id><![CDATA[81504687718]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Janak]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bhimani]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[janak@kmd.keio.ac.jp]]></email_address>
			</au>
			<au>
				<person_id>P3737230</person_id>
				<author_profile_id><![CDATA[81504687794]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Annisa]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Mahdia]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737231</person_id>
				<author_profile_id><![CDATA[81504688331]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Ali]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Almahr]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737232</person_id>
				<author_profile_id><![CDATA[81311481862]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Daisuke]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Shirai]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737233</person_id>
				<author_profile_id><![CDATA[81504685933]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Naohisa]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ohta]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Renambot, L. et al. 2004. SAGE: the Scalable Adaptive Graphics Environment. In <i>Proceedings of WACE 2004, (The 4th Workshop on Advanced Collaborative Environments)</i>
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1594953</ref_obj_id>
				<ref_obj_pid>1594943</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Ursu, M. F. et al. 2009. Interactive documentaries: A Golden Age. <i>Comput. Entertain</i>. 7, 3, Article 41 (September 2009)]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Growing Documentary: Creating a Collaborative Computer-Supported Story Telling Environment C:\Users\kmd\Desktop\workflow_prototype 
(1).tif  Janak Bhimani*, Annisa Mahdia, Ali Almahr, Daisuke Shirai, Naohisa Ohta Keio University, Graduate 
School of Media Design Figure 1. Growing Documentary workflow implemented in the production and performance 
of lenses + landscapes Introduction Technological innovations in the fields of digital video production, 
distribution as well as broadband network access and speeds have made creating and sharing digital video 
contents into a simple process that can be performed by anyone, anywhere. Although the process of sharing 
contents has undergone a great change with respect to traditional distribution models, the contents themselves 
still tend to follow traditional linear narrative structures, construction and production workflows. 
 We believe the Growing Documentary concept can support and encourage more people to tell their stories 
together in new and innovative ways. The Growing Documentary is a platform for computer supported cooperative 
work (CSCW) using user-generated content to produce digital video productions that can be remixed, reworked 
and built upon as the story and story tellers change and adapt. Motivation The idea and concept for 
the Growing Documentary came about as a result of the Great East Japan Earthquake and Tsunami which occurred 
on March 11th, 2011. Not only did this event have an impact on Japan and the rest of the world because 
of the unprecedented level of damage caused by both terrestrial and oceanic natural disasters; but also 
because news of the impact was delivered to people with unprecedented expedience and quality via a variety 
of media to people everywhere. Traditional media outlets were utilizing new social communication tools 
to broadcast their contents. People in the most devastated areas who had no electricity or public services 
were using their smart phones or feature phones to receive information and communicate their stories 
to the outside world. Inspiration . First Prototype The first iteration of the Growing Documentary was 
a short documentary about the devastation and aftermath of March 11th from the point of view of three 
amateur and professional photographers from different walks of life who were all affected by the earthquake 
and tsunami. The film, lenses + landscapes , was produced by crowd-sourcing the photographers, translators 
and audio via social networking services (SNS). As the Growing Documentary is a social platform for individual 
and community cinematic expression, 4K (4096x2160) was chosen as the output resolution. Through the use 
of a cloud-based file sharing system, the photographers raw still images were combined with multi-framed 
HD video to render a very high resolution movie clip through the use of community resources. (Figure 
1) Hands-on Non-linear Narrative Collaboration The Growing Documentary is an important step in furthering 
the field of interactive documentaries [Ursu, M. F. et Al.]. The Growing Documentary platform allows 
collaborators to not only interact with one another in the production of a story, but also the elements 
of the story. Moreover, the story itself becomes an interactive experience: ideas, images, videos and 
sounds can be shared by the story tellers as well as the audience. By incorporating SAGE OptIPortables 
[Renambot, L. et al.] into the creative process of the Growing Documentary, digital contents over gigabit 
pipelines around and throughout the world, people, regardless of location, have the potential to collaborate 
at any time in order to produce high-quality digital contents by and for the global community. Implementation 
and Demonstration The flexibility in the narrative structure of the Growing Documentary allows for freedom 
in implementation. After lenses + landscapes was shown at a special screening during the 2011 Tokyo International 
Film Festival, attendees also had a chance to interact with the film via SAGE. Multiple SAGE walls were 
connected in San Diego at CineGrid 2011 and participants had a chance to immerse themselves in the images, 
videos and sounds of lenses + landscapes while interacting in real-time with others in Japan to create 
their own versions of the story. References RENAMBOT, L. ET AL. 2004. SAGE: the Scalable Adaptive Graphics 
Environment. In Proceedings of WACE 2004, (The 4th Workshop on Advanced Collaborative Environments) 
URSU, M. F. ET AL. 2009. INTERACTIVE DOCUMENTARIES: A GOLDEN AGE. COMPUT. ENTERTAIN. 7, 3, ARTICLE 41 
(SEPTEMBER 2009) 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2342916</article_id>
		<sort_key>200</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>15</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[living floccus]]></title>
		<subtitle><![CDATA[floating volumetric pixels using fog rings with stroboscopic effect]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2342896.2342916</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2342916</url>
		<abstract>
			<par><![CDATA[<p>We can find various aesthetic elements in a series of actions of fog and smoke: appearing, expanding, floating and disappearing. This time, we focus on fog as a material, which forms a pixel, and propose a novel volumetric display system named "living floccus". As shown in Figure 1, this display system can show dot images with fog rings generated by air cannons in the air.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3737234</person_id>
				<author_profile_id><![CDATA[81504683350]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Keina]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Konno]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737235</person_id>
				<author_profile_id><![CDATA[81100527572]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Yasuaki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kakehi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[ykakehi@sfc.keio.ac.jp]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1187306</ref_obj_id>
				<ref_obj_pid>1187297</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Rakkolainen, I., DiVerdi, S., Olwal, A., Candussi, N., H&#252;llerer, T., Laitinen, M., Piirto, M., and Palovuori, K. 2005. The interactive fogscreen. In <i>SIGGRAPH 2005 Emerging technologies</i>, ACM.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Schulze, D., 2010. for those who see. http://www.design.udk-berlin.de/DanielSchulze/Diplom.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 living .occus: FloatingVolumetric Pixels Using Fog Rings with Stroboscopic Effect KeinaKonno* Yasuaki 
Kakehi* Keio University Keio University  Figure 1: living .occus. Figure 2: System design. Figure 3: 
Volumetric pixels in the air. 1 Introduction We can .nd various aesthetic elements in a series of actions 
of fog and smoke: appearing, expanding, .oating and disappearing. This time, we focus on fog as a material, 
which forms a pixel, and pro­pose a novel volumetric display system named living .occus . As shown in 
Figure 1, this display system can show dot images with fog rings generatedby air cannonsin the air. Fog 
and smoke are often used for atmospheric effects in the enter­tainment industry such as live theatre, 
concerts, theme parks. Espe­cially in the .eld of media art, projection screens utilizing fog have been 
proposed as a display system that makes use of its features, the motion of air, and .ow [Rakkolainen 
et al. 2005]. On the other hand, several systems (e.g. for those who see [Schulze 2010]) have been proposed 
that construct images utilizing an array of fog rings as pixels. In these previous systems, formed images 
.ow and disappear instantly . In contrast to this, our system can control the positions and patterns 
of volumetric pixels in the air by combining the fog ring array and strobe light projection.  2 living 
.occus Figure2shows the system design of the living .occus. The hard­ware of living .occus consists of 
twoparts: a generation part of fog rings and a strobe light projection part. We describe technical innovations 
provided by our system as fol­lows: Firstly, as mentioned above, in this system we utilize a fog ring 
as a pixel of an image. For generating fog rings, we attached a vibration speaker on the bottom side 
of a cylindrical container. In addition, through a tube attached to the side surface of the con­tainer, 
there is a continuous input of humidi.ed air generated by a commercially available humidi.er. By controlling 
the motion of thespeakersurface,the humidi.edairarepushedoutandafogring .iesoutofasmallholeinthetopsideofthe 
container. Whilethese fog rings .ies away instantly, we applied the stroboscopic effect to makepixels 
remain in the same position. In this system, white LED lights are attached in several points for illuminating 
the fog rings. Thus, in a dark environment, by forming fog rings at regular inter­vals and controlling 
the on/offtiming of the LEDs according to the *e-mail: ykakehi@sfc.keio.ac.jp fog interval,wecanseethefogringsasiftheykeepstillintheair. 
Secondly, we have also developed software for controlling the po­sition and pattern of the volumetric 
pixels. By controlling of the volume, frequencyand phase of the output signal of the vibration speaker, 
we can change the position and interval of the appeared pixels. For example, when the strobe light blinks 
in 4Hz, by con­trolling the on/offaction of the vibration speaker in 20Hz, we can change the pattern 
of every .ve pixels freely. Finally, by designing the arrangement of the fog ring generation units, we 
can develop various types and scales of substantial dis­plays. For example, when we set the units in 
a line, the system can show two dimensional images in the air by operating the units simultaneously. 
Furthermore, we can achieve a three dimensional volumetric display with units in a 2D array setting. 
 3 Applications and FutureWorks We have already implemented prototype systems for displaying 2D and 3D 
pixel patterns. As shown in Figure 1 and Figure 3, this display works as an ambient information board. 
The display can show not only static imagesbut also animationsby changing pix­els patterns sequentially. 
Of course, the image displayed on it is affected from environmentalfactors such as winds. Audiences can 
observe the digital information and analog phenomena simultane­ously. In addition, since these pixels 
made of fog rings have high transparency, we can apply this system for Augmented Reality by overlapping 
the displayed pixel image onto physical objects. In the future, we are going to implement this system 
in various scales and situations. In addition, we also plan to propose inter­actions using gestures and 
tangible objects. References RAKKOLAINEN, I., DIVERDI, S., OLWAL, A., CANDUSSI, N., H¨T., LAITINEN, 
M., PIIRTO, M., AND ULLERER, PALOVUORI, K. 2005. The interactive fogscreen. In SIG-GRAPH 2005 Emerging 
technologies,ACM. SCHULZE,D.,2010. for those who see. http://www.design. udk-berlin.de/DanielSchulze/Diplom. 
Copyright is held by the author / owner(s). SIGGRAPH 2012, Los Angeles, California, August 5 9, 2012. 
ISBN 978-1-4503-1435-0/12/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2342917</article_id>
		<sort_key>210</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>16</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>5</seq_no>
		<title><![CDATA[Micro sized art "the weight of life"]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2342896.2342917</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2342917</url>
		<abstract>
			<par><![CDATA[<p>In some way we can see everything that exists in our surroundings, even though it is too small to see by the naked eye. What about ourselves? How can we notice our own existence?</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3737236</person_id>
				<author_profile_id><![CDATA[81504682630]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Akiko]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sato]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[akisato@iis.u-tokyo.ac.jp]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2342918</article_id>
		<sort_key>220</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>17</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>6</seq_no>
		<title><![CDATA[ViewPaint (vol. 1 <i>The Milkmaid</i> by Johannes Vermeer)]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2342896.2342918</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2342918</url>
		<abstract>
			<par><![CDATA[<p>ViewPaint is a viewing system for paintings. This system was produced to propose a new exhibition method using digital technology to art museums. In developing this system, we have tried to realize the idea that if viewers can explore a painting converted into a three-dimensional space, it is possible to foster understanding and imagination, with viewers considering why artists chose certain compositions, what motifs are depicted, and what the background to the period was.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3737237</person_id>
				<author_profile_id><![CDATA[81504688101]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Kota]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Okukubo]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Toppan Printing Co., Ltd., Tokyo, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737238</person_id>
				<author_profile_id><![CDATA[81504685563]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Katsuhito]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yagi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Toppan Printing Co., Ltd., Tokyo, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737239</person_id>
				<author_profile_id><![CDATA[81504685339]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Koichi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yoshino]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Toppan Printing Co., Ltd., Tokyo, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737240</person_id>
				<author_profile_id><![CDATA[81488664223]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Takafumi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Watanabe]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Toppan Printing Co., Ltd., Tokyo, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737241</person_id>
				<author_profile_id><![CDATA[81504682090]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Megumu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Machida]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Toppan Printing Co., Ltd., Tokyo, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737242</person_id>
				<author_profile_id><![CDATA[81504688551]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Akira]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hizawa]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Toppan Printing Co., Ltd., Tokyo, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737243</person_id>
				<author_profile_id><![CDATA[81504686692]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>7</seq_no>
				<first_name><![CDATA[Eiichi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nagai]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Toppan Printing Co., Ltd., Tokyo, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737244</person_id>
				<author_profile_id><![CDATA[81504687596]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>8</seq_no>
				<first_name><![CDATA[Yoriko]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kobayashi-Sato]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Mejiro University, Tokyo, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Yoriko Kobayashi-Sato, <i>The Milkmaid: The birth of the painter Vermeer</i>, Tokyo, 2007]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Philip Steadman, <i>Vermeer's Camera</i>, Oxford, 2001]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[A. Criminisi, <i>Visual Metrology from Single and Multiple Uncalibrated Images</i>, Oxford, 1999]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 ViewPaint (Vol.1 The Milkmaid by Johannes Vermeer) Kota Okukubo, Katsuhito Yagi, Eiichi Nagai Yoriko 
Kobayashi­Sato Koichi Yoshino, Takafumi Watanabe, Prometech Software, Inc. Mejiro University Megumu Machida, 
Akira Hizawa TOKYO, JAPAN TOKYO, JAPAN TOPPAN PRINTING CO., LTD. TOKYO, JAPAN  1. Introduction ViewPaint 
is a viewing system for paintings. This system was produced to propose a new exhibition method using 
digital technology to art museums. In developing this system, we have tried to realize the idea that 
if viewers can explore a painting converted into a three­dimensional space, it is possible to foster 
understanding and imagination, with viewers considering why artists chose certain compositions, what 
motifs are depicted, and what the background to the period was. ViewPaint enables a painting originally 
on a flat surface to be experienced as a three­dimensional space. This system shows representations of 
assets produced based on archives of paintings without losing the unique style of the artist. In addition, 
the generated CG images are rendered and displayed in real­time in response to the movement of the viewer 
detected by a sensor, and viewers can see elements of paintings in an interactive manner, seemingly going 
inside the paintings themselves. The first piece to which we have applied ViewPaint is The Milkmaid by 
Johannes Vermeer, a Dutch master of 17th century (Figure 1). We call it ViewPaint vol.I. 2. Our Approach 
2.1. The Milkmaid converted into a three­dimensional space The room depicted in The Milkmaid has been 
converted into a three dimensional space. To execute the operation, we have traced back the assumed applying 
process of geometrical perspective used by Vermeer, referred to various contemporary Dutch genre paintings, 
furniture and architectural details, and to the standard size of a piece of Delft tile of the time (13x13cm), 
which offers us to infer the sizes of each represented detail. Numerous CG simulations also help us greatly 
[1], [2], [3] (Figure 2). 2.2. Vermeer s style is preserved regardless of the viewing angle Due to the 
development of an original real­time CG rendering program, whatever angle the space is viewed from, the 
monitor always displays CG images that seem as if they were painted by Vermeer (Figure 3). 2.3. An interactive 
experience in which you go beyond the frame. By combining the 3D space of the painting with a sensor 
that detects a person, the CG image is displayed in response to the movement of the viewer standing in 
front of the device. This means that the viewer can have an interactive experience and feel as if they 
were beyond the frame and actually visited the room painted by Vermeer.  3. Conclusions By offering 
the viewer an interactive virtual experience to be inside the painted space, ViewPaint will arouses the 
viewer s interest in appreciating the painting. It is planned to add a function to our system to provide 
the viewer with information about the painting technique of the artist and the subject of the painting 
(Figure 4). We also expect to produce ViewPaint images of the other Vermeer s works and to contribute 
to make painting and composing techniques of Vermeer Figure4. Demonstration Exhibited at Fukuoka Asian 
Art Museum. (March 2012) References [1] Yoriko Kobayashi­Sato, The Milkmaid: The birth of the painter 
Vermeer, Tokyo, 2007 [2] Philip Steadman, Vermeer's Camera, Oxford, 2001 [3] A. Criminisi, Visual Metrology 
from Single and Multiple Uncalibrated Images, Oxford, 1999 &#38;#169;Toppan Printing Co., Ltd. Original 
photo data (Het melkmeisje [The Milkmaid] by Johannes Vermeer) : &#38;#169;Rijksmuseum Amsterdam. Purchased 
with the support of the Vereniging Rembrandt. Copyright is held by the author / owner(s). SIGGRAPH 2012, 
Los Angeles, California, August 5 9, 2012. ISBN 978-1-4503-1435-0/12/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	</section>
	<section>
		<section_id>2342919</section_id>
		<sort_key>230</sort_key>
		<section_seq_no>6</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Color]]></section_title>
		<section_page_from>6</section_page_from>
	<article_rec>
		<article_id>2342920</article_id>
		<sort_key>240</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>18</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[A stereo nine-band camera for accurate color and spectrum reproduction]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2342896.2342920</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2342920</url>
		<abstract>
			<par><![CDATA[<p>In the digital archiving for cultural heritage preservation, in the medical field, and in some industrial fields, high-fidelity color reproduction is very important. Multiband imaging technology is a solution for accurate color reproduction. Although several types of multiband camera systems have been developed, all of them are multi-shot systems and they cannot take images of moving objects. Tsuchida et al. [2010] have developed a one-shot stereo six-band camera system using two commercial digital cameras. This system is very reasonable in terms of cost, but the distance between the cameras causes self occlusion disparity problems when the object has 3D shape. In addition, the number of color channels of camera system should be increased for further improvement of the accuracy of estimated spectral reflectance. In this paper, we propose a stereo nine-band camera consisting of nine monochrome cameras with nine different interference filters. A nine-band image is generated from nine stereo images captured by this system. Estimated spectral reflectance and reproduced color images reproduced in experiments are shown.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3737245</person_id>
				<author_profile_id><![CDATA[81466642662]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Masaru]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tsuchida]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[NTT Communication Science Laboratories, NTT Corporation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737246</person_id>
				<author_profile_id><![CDATA[81100599057]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Takahito]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kawanishi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[NTT Communication Science Laboratories, NTT Corporation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737247</person_id>
				<author_profile_id><![CDATA[81100494551]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Kunio]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kashino]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[NTT Communication Science Laboratories, NTT Corporation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737248</person_id>
				<author_profile_id><![CDATA[81100230754]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Junji]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yamato]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[NTT Communication Science Laboratories, NTT Corporation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1836917</ref_obj_id>
				<ref_obj_pid>1836845</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Tsuchida, M., et al., 2010. A stereo one-shot multi-band camera system for accurate color reproduction. <i>In proceedings of Siggraph, Poster, ACM</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Tsuchida, M., et al., 2011. Evaluating Color Reproduction Accuracy of Stereo One-shot Six-band Camera System. <i>In proceedings of 19</i>&#60;sup&#62;<i>th</i>&#60;/sup&#62; <i>Color and Imaging Conference (CIC19)</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Takita, H., et al., 2003. High-accuracy image registration based on phase-only correlation. <i>IEICE Transaction of Fundamentals, Vol. E86-A, no.8, 1925--1934</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Pratt, W. K., et al., 1976. Spectral estimation techniques for the spectral calibration of a color image scanner. <i>Applied Optics, OSA, 15, 73--75</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Hashimoto, M., 2008. Two-Shot type 6-band still image capturing system using Commercial Digital Camera and Custom Color Filter. <i>In proceedings of Fourth European Conference on Colour in Graphics, Imaging, and Vision (CGIV2008)</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 A stereo nine-band camera for accurate color and spectrum reproduction Masaru Tsuchida, Takahito Kawanishi, 
Kunio Kashino, Junji Yamato NTT Communication Science Laboratories, NTT Corporation 1. Introduction 
In the digital archiving for cultural heritage preservation, in the medical field, and in some industrial 
fields, high-fidelity color reproduction is very important. Multiband imaging technology is a solution 
for accurate color reproduction. Although several types of multiband camera systems have been developed, 
all of them are multi-shot systems and they cannot take images of moving objects. Tsuchida et al. [2010] 
have developed a one-shot stereo six-band camera system using two commercial digital cameras. This system 
is very reasonable in terms of cost, but the distance between the cameras causes self occlusion disparity 
problems when the object has 3D shape. In addition, the number of color channels of camera system should 
be increased for further improvement of the accuracy of estimated spectral reflectance. In this paper, 
we propose a stereo nine-band camera consisting of nine monochrome cameras with nine different interference 
filters. A nine-band image is generated from nine stereo images captured by this system. Estimated spectral 
reflectance and reproduced color images reproduced in experiments are shown. 2. Camera system Figure.1 
shows our camera system and its spectral sensitivities. Visible wave length is divided into nine with 
little overlap, and the eighth and ninth bands cover infrared. The distance between the optical axis 
of each camera is 55 mm. Captured image size is 2-M pixels (1600 x 1200) and the bit-depth of signal 
is 10 bits. Each camera is connected to a PC with IEEE 1394b cable. The frame rate of image transfer 
is 15 fps at maximum for a full-size image, but 30 fps can be achieved when image size is reduced to 
1024 x 768 pixels (XGA). All cameras are synchronized and images of moving objects can be taken as still- 
or moving images. 3. Generating nine-band image from stereo image In order to generate a nine-band image 
from the stereo image captured with our system, eight images have to be aligned into a reference image. 
Let the image of the center camera be the reference image. The image generation in this paper is based 
on an image transformation technique. In the future, 3D depth information estimated from stereo images 
will also be used, which will make the quality of the image generated in each band much better. When 
an image-set of 2D object like a painting or tapestry is taken, a projective transformation is useful 
for aligning the position of each channel image. When 3D objects are taken, it is better to use nonlinear 
transformation. In this work, thin-plate spline approximation (TPS) was applied for this purpose [Tsuchida 
2011]. To calculate transformation parameters, corresponding points between the two images are detected 
by using the phase-only correlation (POC) method [Takita 2003]. POC is a scale- and rotation-invariant 
pattern detection method that uses phase information. The resultant nine monochrome images are combined 
into a nine-band image. The nine-band image is converted into a spectral reflectance image through Wiener 
estimation [Pratt 1976], and, the spectral reflectance image is converted into a RGB image by using the 
measured illumination spectrum and monitor characteristics. 4. Experimental results and summary First, 
we confirmed the accuracy of the estimated spectral reflectance by comparing it with reflectance measured 
with a spectrometer. As a target object, we used Macbeth ColorCheckerTM. Figures.2 and 3 show a part 
of the measured and estimated spectral reflectance. Although some errors are found in the short-wavelength 
domain between 380 to 420 nm, spectral reflectance seems to be well estimated using the proposed camera 
system. These errors were caused by a lack of camera sensitivity in this wavelength domain. Next, images 
of 2D and 3D objects were taken with this system. Each channel image of captured nine-band image of 2D 
object (woodblock print) is shown in Fig.4. Final images after image transformations and color reproduction 
of the images of the 2D and 3D objects are also shown in Fig. 4. We compared the color of the real object 
and that of image displayed on a LCD monitor by eye and spectrometer, and confirmed that the object color 
is well reproduced. The resultant image was compared with the image obtained with two-shot six-band system 
[Hashimoto 2008], and it was confirmed that both results have the same color quality as the real object. 
  References TSUCHIDA, M., et al., 2010. A stereo one-shot multi-band camera system for accurate color 
reproduction. In proceedings of Siggraph, Poster, ACM. Copyright is held by the author / owner(s). SIGGRAPH 
2012, Los Angeles, California, August 5 9, 2012. ISBN 978-1-4503-1435-0/12/0008   380 480 580 680 
780 Wavelength [nm] Figure 1 Stereo nine-band camera and its spectral sensitivity. 1 1 0.9 0.9 0.8 0.8 
0.7 0.7 0.6 0.6 0.5 0.5 0.4 0.4 0.3 0.3 0.2 0.2 0.1 0.1 0 0 380 480 580 680 780 380 480 580 680 780 
  Wavelength [nm] Wavelength [nm] Figure 2. Measured spectral reflectance of color chart 1 1 0.9 0.9 
0.8 0.8 0.7 0.7 0.6 0.6 0.5 0.5 0.4 0.4 0.3 0.3 0.2 0.2 0.1 0.1 0 0 380 480 580 680 780 380 480 580 
680 780  Wavelength [nm] Wavelength [nm] Figure 3. Estimated spectral reflectance of color chart  
 Figure 4. Captured nine-band images and color reproduction results (2D and 3D objects). TSUCHIDA, M., 
et al., 2011. Evaluating Color Reproduction Accuracy ofStereo One-shot Six-band Camera System. In proceedings 
of 19th Color and Imaging Conference (CIC19). TAKITA, H., et al., 2003. High-accuracy image registration 
based onphase-only correlation. IEICE Transaction of Fundamentals, Vol. E86-A, no.8, 1925-1934. PRATT, 
W. K., et al., 1976. Spectral estimation techniques for the spectral calibration of a color image scanner. 
Applied Optics, OSA, 15, 73 75. HASHIMOTO, M., 2008. Two-Shot type 6-band still image capturing system 
using Commercial Digital Camera and Custom Color Filter. In proceedings of Fourth European Conference 
on Colour in Graphics, Imaging, and Vision (CGIV2008). 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2342921</article_id>
		<sort_key>250</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>19</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[High-definition and multispectral capturing for digital archiving of large 3D woven cultural artifacts]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2342896.2342921</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2342921</url>
		<abstract>
			<par><![CDATA[<p>This paper describes a 3D measurement system with wheel-rail, a capturing system with multi-band camera, and a 3D modeling of large woven cultural artifacts, and show a high-resolution 3D model with multi-band image.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[3D scanning]]></kw>
			<kw><![CDATA[Gion festival]]></kw>
			<kw><![CDATA[computer graphics]]></kw>
			<kw><![CDATA[digital archive]]></kw>
			<kw><![CDATA[multi-band imaging]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3737249</person_id>
				<author_profile_id><![CDATA[81464673753]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Wataru]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wakita]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Ritsumeikan University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[wakita@cv.ci.ritsumei.ac.jp]]></email_address>
			</au>
			<au>
				<person_id>P3737250</person_id>
				<author_profile_id><![CDATA[81466642662]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Masaru]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tsuchida]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[NTT Corporation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737251</person_id>
				<author_profile_id><![CDATA[81504685703]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Shiro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tanaka]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Ritsumeikan University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737252</person_id>
				<author_profile_id><![CDATA[81100599057]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Takahito]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kawanishi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[NTT Corporation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737253</person_id>
				<author_profile_id><![CDATA[81502671774]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Kunio]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kashino]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[NTT Corporation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737254</person_id>
				<author_profile_id><![CDATA[81100230754]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Junji]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yamato]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[NTT Corporation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737255</person_id>
				<author_profile_id><![CDATA[81100135138]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>7</seq_no>
				<first_name><![CDATA[Hiromi]]></first_name>
				<middle_name><![CDATA[T.]]></middle_name>
				<last_name><![CDATA[Tanaka]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Ritsumeikan University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1900183</ref_obj_id>
				<ref_obj_pid>1900179</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Tanaka, H. T., Hachimura, K., Yano, K., Tanaka, S., Furukawa, K., Nishiura, T., Tsutida, M., Choi, W., and Wakita, W. 2010. Multimodal digital archiving and reproduction of the world cultural heritage "gion festival in kyoto". In <i>The ACM SIGGRAPH Sponsored 9th International Conference on VRCAI</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Wataru Wakita* Masaru Tsuchida Ritsumeikan University NTT Corporation Kunio Kashino NTT Corporation 
Abstract This paper describes a 3D measurement system with wheel-rail, a capturing system with multi-band 
camera, and a 3D modeling of large woven cultural artifacts, and show a high-resolution 3D model with 
multi-band image. Keywords: Digital Archive, Computer Graphics, 3D Scanning, Multi-band Imaging, Gion 
Festival 1 Introduction Recently, research on digital museums received increased attention. In the digital 
museum project [Tanaka et al. 2010], we are working on the digital archiving of the intangible cultural 
heritage Gion Festival in Kyoto , focused on the culture of Kyoto, and developed a multi-modal VR exhibition 
system. After the Gion Festival of 2011, the repair work of the Fune-hoko storage was scheduled. Therefore, 
taking this opportunity, we cre­ated a 3D measurement system for the large woven cultural artifacts with 
the laser range scanner, and measured all 3D woven cultural artifacts. Then we captured high-resolution 
images with a two-shot type 6-band image capturing system, and modeled woven cultural artifacts in 3D. 
 2 Capturing System with Multi-band Imaging Camera In order to reproduce accurate color in digital photography, 
multi­band imaging technology is a practical solution. Then, we con­structed ultra high-de.nition six-band 
digital camera system and used for archiving some tapestries and ornaments of Fune-hoko. This system 
can take more than 100 M pixel images and we archived that the resolution is almost 0.02 mm/pixel. This 
sys­tem consists of a consumer 35 mm-format digital camera and a custom interference .lter. Attached 
in front of the camera lens, a customized .lter cuts off the left sides, i.e., the short wavelength domain, 
of the peaks of both the blue and red in original spectral sensitivity. It also cut off the green s right 
side, i.e., the long wave­length domain. Sliding the .lter horizontally by hand, one can cap­ture two 
images with and without .lter alternately. First, an image is taken with the .lter in front of the lens. 
Then, another image is taken without the .lter by sliding the .lter hori­zontally by hand. Combining 
the two RGB color images, a six-band image is synthesized.  3 3D Scanning System of Large 3D Woven Cultural 
Artifacts Many of the Fune-hoko hangings are extremely large, approxi­mately 1 m×3 m, and so measurement 
must be conducted in sec­ *e-mail: wakita@cv.ci.ritsumei.ac.jp  Figure 1: Comparative results. Multi-band 
Image(.rst image), 3D Surface (second image), 3D Model with Multi-band Image(third and fourth images). 
tions. The .nal process of combining the data from measure­ments of individual sections is therefore 
time-consuming. Choos­ing the textile cultural artifacts with particularly strong three­dimensionality 
among the Fune-hoko hangings as our subjects, we have constructed a scanning system that takes into account 
the pro­cess of measurement and ease of combined processing. In order to simplify the process of integration 
processing after measurement as much as possible, we laid out a rail so that a wheeled platform could 
run along it from left to right, and set up the laser-range scanner on another wheeled platform on top 
of the .rst; we then conducted the measurements without moving the target object, by moving the scanner 
at set intervals and recording the necessary shots. We analyzed the measurement data, and modeled by 
the denoising, the interpolating of the lost part, the alignment, and the integration of the overlapped 
point. Figure 1 shows a comparative result of a multi-band image and a 3D model with multi-band image. 
Third and fourth images are rendering results which estimated through the use of a multi-band image, 
directional light source, and normal vector of 3D surface. 4 Conclusions and Future Work This paper 
described a 3D measurement system with wheel-rail, a capturing system with multi-band camera, and a 3D 
modeling of large woven cultural artifacts. In future work, we plan to develop a real-time point-based 
visuo-haptic exhibition system and multi­.nger elastic interaction.  Acknowledgement This research has 
been conducted partly by the support of the Dig­ital Museum Project in the Ministry of Education, Sports, 
Science and Technology, Japan. We would like to thank the Gion-Matsuri Fune-hoko Preservation Society, 
a generous collaborator of this project. References TANAKA, H. T., HACHIMURA, K., YANO, K., TANAKA, 
S., FU-RUKAWA, K., NISHIURA, T., TSUTIDA, M., CHOI, W., AND WAKITA, W. 2010. Multimodal digital archiving 
and reproduc­tion of the world cultural heritage gion festival in kyoto . In The ACM SIGGRAPH Sponsored 
9th International Conference on VRCAI. Copyright is held by the author / owner(s). SIGGRAPH 2012, Los 
Angeles, California, August 5 9, 2012. ISBN 978-1-4503-1435-0/12/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2342922</article_id>
		<sort_key>260</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>20</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Optimized color LUT transformations by means of analysis of image memorable and subject important colors]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2342896.2342922</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2342922</url>
		<abstract>
			<par><![CDATA[<p>In this work we are investigating ICC color management workflow in color imaging system and proposing improvement to standard ICC-based color management scheme. We are created optimal look-up tables of ICC profiles by using spline approximation and neural network for direct transformation RGB &#8594; Lab and regression models for inverse transformation Lab &#8594; RGB and integrated criterion (optimization criterion). Our algorithm implements corresponding color reproduction to accurately predict memorable and scene-important colors. Mean errors was reduced by 21% as compared to standard color profiles for particular output device and type of paper.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[LUT-tables]]></kw>
			<kw><![CDATA[color image processing]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3737256</person_id>
				<author_profile_id><![CDATA[81504688106]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Natalia]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gurieva]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Guanajuato, Mexico]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737257</person_id>
				<author_profile_id><![CDATA[81351604060]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Igor]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Guryev]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Guanajuato, Mexico]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737258</person_id>
				<author_profile_id><![CDATA[81504683722]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Francisco]]></first_name>
				<middle_name><![CDATA[Javier Montecillo]]></middle_name>
				<last_name><![CDATA[Puente]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Guanajuato, Mexico]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737259</person_id>
				<author_profile_id><![CDATA[81504682988]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Reynaldo]]></first_name>
				<middle_name><![CDATA[Thompson]]></middle_name>
				<last_name><![CDATA[Lopez]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Guanajuato, Mexico]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Phil Green (Ed.), Michael Kriss. Color Management: Understanding and Using ICC Profiles, Wiley-IS&T Series in Imaging Science and Technology, 2010, 314p.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Specification ICC.1:2010 (Profile version 4.3.0.0) Image technology colour management --- Architecture, profile format, and data structure. International Color Consortium. http://www.color.org/specification/ICC1v43_2010-12.pdf]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Optimized Color LUT Transformations by means of Analysis of Image Memorable and Subject Important Colors 
Natalia Gurieva, Igor Guryev, Francisco Javier Montecillo Puente, Reynaldo Thompson Lopez Department 
of Digital Arts and Management, Division of Engineering, University of Guanajuato, Mexico  Figure 1: 
Result of transformations a) from workspace Adobe RGB . Lab . RGB-output profile Epson for printer Stylus 
Pro 4880, Premium Glossy Photo Paper; b) from workspace Adobe RGB . Lab . our RGB-output profile that 
takes into account memorable and subject important colors; c) color difference map Abstract In this 
work we are investigating ICC color management workflow in color imaging system and proposing improvement 
to standard ICC-based color management scheme. We are created optimal look-up tables of ICC profiles 
by using spline approximation and neural network for direct transformation RGB . Lab and regression models 
for inverse transformation Lab . RGB and integrated criterion (optimization criterion). Our algorithm 
implements corresponding color reproduction to accurately predict memorable and scene-important colors. 
Mean errors was reduced by 21% as compared to standard color profiles for particular output device and 
type of paper. Keywords: LUT-tables, color image processing. 1 Introduction Color management helps to 
achieve the same appearance on all of input, visualization and output devices. In color management systems 
there are a lot of transformations of digital workflow on the way from the input image to the output 
image. The basic way the ICC profiles are typically used to achieve color reproduction is by combining 
a source profile with a destination profile to enable input device data to be transformed to required 
one to give the required color at the output. Insufficient knowledge in some questions of color management 
in computer systems is related to the complexity of the mathematical description of all the factors that 
affect the final perception of color information by human visual system. Therefore, the development of 
the algorithms of color transformations in the digital system is needed for accurate processing based 
upon image requirements. 2 Our Approach We propose improvements to standard ICC-based color management 
scheme by using spline approximation and neural network models (Generalized Regression Neural Network) 
to create look-up tables for direct transformation RGB . Lab and regression models for inverse transformation 
Lab . RGB of ICC profiles. The source data for comparative analysis of the methods for creating LUT-tables 
is the test chart which color coordinates has been established using spectrometric measurements. For 
a process of creation color LUT it is developed integrated criterion (optimization criterion) which takes 
into account: average color difference .E between measured and calculated color coordinates in the device-independent 
color space Lab; the maximum color difference of all the colors of the sample .m..; color difference 
.E. in areas of a memorable colors (colors of human skin, greenery and colors of the sky); average color 
difference .E. of the achromatic colors because the human eye is most sensitive to color shifts in shades 
of gray; color difference .Eo of sample of the colors on the gamut boundary. The sample consists of highly 
saturated colors that are often used to create a brand identity, as well as tints of blacks that are 
responsible for the details in the deep shadows. With this criterion we can estimate the accuracy of 
transformation by using various methods of approximation of the color data. The final value of the criterion 
calculated from the ratio value of each component. Our algorithm realizes corresponding color reproduction 
to accurate predict memorable and scene-important colors of the images. References 1. Phil Green (Ed.), 
Michael Kriss. Color Management: Understanding and Using ICC Profiles, Wiley-IS&#38;T Series in Imaging 
Science and Technology, 2010, 314p. 2. Specification ICC.1:2010 (Profile version 4.3.0.0) Image technology 
colour management Architecture, profile format, and data structure. International Color Consortium. 
http://www.color.org/specification/ICC1v43_2010-12.pdf 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2342923</article_id>
		<sort_key>270</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>21</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[Use of periodic shift and color combinations to enhance illusory motion]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2342896.2342923</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2342923</url>
		<abstract>
			<par><![CDATA[<p>A new illusory motion which is so strong that almost everyone can easily perceive it has been created by using two techniques. One is to shift the image periodically, and the other is to create a still image which causes the illusion by blending only two primary colors such as red and blue.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3737260</person_id>
				<author_profile_id><![CDATA[81365598536]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Kazuhisa]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yanaka]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Kanagawa Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[yanaka@ic.kanagawa-it.ac.jp]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1360661</ref_obj_id>
				<ref_obj_pid>1399504</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Chi Ming-Te et al., 2008. Self-Animating Images: Illusory Motion Using Repeated Asymmetric Patterns. ACM SIGGRAPH 2008 paper.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Kitaoka, A., 2004, 'Rollers'. http://www.psy.ritsumei.ac.jp/~akitaoka/rollers.gif]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Kitaoka, A., 2008. Optimized Fraser-Wilcox Illusion, http://www.psy.ritsumei.ac.jp/~akitaoka/nisshin2008ws.html]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Yanaka et al., 2011. Automatic Shake to Enhance Fraser-Wilcox Illusions. Proc.VISAPP 2011. pp. 405--408.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Use of Periodic Shift and Color Combinations to Enhance Illusory Motion Kazuhisa Yanaka Kanagawa Institute 
of Technology   Figure 1: Illusory motion of simple color stripe. Abstract A new illusory motion 
which is so strong that almost everyone can easily perceive it has been created by using two techniques. 
One is to shift the image periodically, and the other is to create a still image which causes the illusion 
by blending only two primary colors such as red and blue. 1 Motivation The basis of movies and computer 
animations is apparent motion, which is perceived when a series of still images is shown successively 
for a short duration. On the other hand, illusory motion is both unique and interesting because, in spite 
of the fact that the image is perfectly stationary, the cognitive effect makes it appear to be moving. 
Frazer and Wilcox reported this type of illusion in 1979. Faubert and Herbert called a similar effect 
the peripheral drift illusion (PDI) in 1999 because it occurs in the visual periphery. Chi et al. [2008] 
presented a method for generating such a self-animating image from a copy of a piece of art by, for example, 
van Gogh. Kitaoka published various beautiful original PDIs including famous rotating snakes . Kitaoka 
[2008] also classified this kind of illusion into several types based on the arrangement of gray levels 
and colors. Among them the Type V illusion, in which the color combination of red and blue is essential, 
is one of the strongest illusions. However, even in this case, the illusory motion was so faint that 
not everyone perceived it. There seems to be another reason: viewers must not stare at it as the illusion 
is a PDI. The viewer needs to glance but not everyone can do this. Therefore, we propose a very simple 
method to make the illusion strong enough for almost everyone to perceive it [Yanaka et al., 2011]. The 
method was to shake a still image, electronically or mechanically, at about five to ten Hz. However, 
the method was not very effective for types other than the Type V. Another issue is that too many parameters 
might be related because the motion is not digital but analog if it is continuously shaken. 2 Method 
We used simple cyclic switching between two images, one of which is the original and the other is a horizontally 
shifted image of it. No intermediate positions were allowed. In addition, we used only a small number 
of colors without using gradation. Even on this condition strong illusory motion can be created. e-mail: 
yanaka@ic.kanagawa-it.ac.jp Figure 2: Illusory motion based on Kitaoka s Rollers .  3 Experiments A 
simple case in which a stripe consisting of three colors of red, magenta, and violet-blue was used as 
shown in Fig.1 (a). A horizontally shifted image of Fig. 1 (a) to a half cycle is shown in Fig. 1 (b). 
When the two still images above were displayed in turn and the duration was about 100 ~ 500 ms each, 
a strong illusion that stripes are moving to the left is obtained as shown in Fig.1 (c). Although green 
can be used instead of red, the illusion becomes a little weak. An illusory motion based on Rollers [Kitaoka, 
2004] is shown in Fig. 2. His original image can be observed without color because only the gradation 
of brightness is needed. However, using opposing colors is known to strengthen the effect of the illusion. 
In his original version, two almost complementary colors are used. Namely, there are several cyan ellipsoidal 
patterns on a yellow background. Although this color combination is suitable for a conventional illusory 
motion in which it looks as if three rollers are slowly rotating, little enhancement is expected when 
the image is switched periodically using our way. We changed the color combination as shown in Fig. 2. 
Here, only two primary colors of red and blue are blended to produce four colors. No green component 
is included. When a horizontally shifted image is produced and the image and the original image was displayed 
alternatively at a frequency of 1 ~ 5 Hz, a very strong illusory motion in which rollers are perceived 
to be rotating was obtained. 4 Conclusion We created a new illusion in which strong unidirectional motion 
is perceived in spite of the fact that only two images, the original and the shifted one, are displayed 
alternatively. Once the detailed mechanism is clarified, we will apply it to computer animation.  References 
Chi Ming-Te et al., 2008. Self-Animating Images: Illusory Motion Using Repeated Asymmetric Patterns. 
ACM SIGGRAPH 2008 paper. Kitaoka, A., 2004, Rollers . http://www.psy.ritsumei.ac.jp/~akitaoka/rollers.gif 
Kitaoka, A., 2008. Optimized Fraser-Wilcox Illusion, http://www.psy.ritsumei.ac.jp/~akitaoka/nisshin2008ws.html 
Yanaka et al., 2011. Automatic Shake to Enhance Fraser-Wilcox Illusions. Proc.VISAPP 2011. pp. 405-408. 
 Copyright is held by the author / owner(s). SIGGRAPH 2012, Los Angeles, California, August 5 9, 2012. 
ISBN 978-1-4503-1435-0/12/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	</section>
	<section>
		<section_id>2342924</section_id>
		<sort_key>280</sort_key>
		<section_seq_no>7</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Design]]></section_title>
		<section_page_from>7</section_page_from>
	<article_rec>
		<article_id>2342925</article_id>
		<sort_key>290</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>22</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Graphic narratives]]></title>
		<subtitle><![CDATA[generative book covers]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2342896.2342925</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2342925</url>
		<abstract>
			<par><![CDATA[<p>Designing an individual book cover is substantially different from designing the covers for a book collection. In the latter case, the task of the designer is to establish visual and perceptual unity to a set of literary works that may be heterogeneous in their motivations, goals and literary contexts. The introduction of programming in the creative process of graphic design empowers the designer, freeing he from the constraints of predefined computational tools, and promoting creative freedom in the construction of visual metaphors. Resulting from the intersection of programming with the creative process of design, the work "graphic narratives" explores new creative possibilities in the design of covers for book collections.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3737261</person_id>
				<author_profile_id><![CDATA[81488667706]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[L&#237;gia]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Duro]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Coimbra]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[ligia@student.dei.uc.pt]]></email_address>
			</au>
			<au>
				<person_id>P3737262</person_id>
				<author_profile_id><![CDATA[81100379916]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Penousal]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Machado]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Coimbra]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[machado@dei.uc.pt]]></email_address>
			</au>
			<au>
				<person_id>P3737263</person_id>
				<author_profile_id><![CDATA[81504683049]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Artur]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Rebelo]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Coimbra]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[arturr@dei.uc.pt]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>78223</ref_obj_id>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Tufte, Edward R., 1990. <i>Envisioning Information</i>. Cheshire, CT: Graphics Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Maeda, J., 2004. <i>Creative Code</i>. Thames &amp; Hudson.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Graphic Narratives: Generative Book Covers Lígia Duro1, Penousal Machado2 and Artur Rebelo3 CISUC, Department 
of Informatics Engineering, University of Coimbra Antonin Artaud O Teatro e o seu Duplo Oscar Wilde Um 
Guia para a Vida Moderna Boris Vian Boris Vian por Boris Vian Luis Buñuel O meu Último Suspiro Montesquieu 
Elogio da Sinceridade a O TEATRO UM GUIA BORIS O MEU ELOGIO E O SEU PARA VIAN ÚLTIMO DA DUPLO A VIDA 
POR SINCERIDADE SUSPIRO MODERNA BORIS VIAN ANTONIN OSCAR BORIS LUIS MONTESQUIEU ARTAUD WILDE VIAN BUÑUEL 
b Figure 1a and b.Two different graphic lines for the same collection. From the punctuation marks (a 
) and the number and length of chapters ( b ). 1 Introduction Designing an individual book cover is substantially 
different from designing the covers for a book collection. In the latter case, the task of the designer 
is to establish visual and perceptual unity to a set of literary works that may be heterogeneous in their 
motivations, goals and literary contexts.The introduction of programming in the creative process of graphic 
design empowers the designer, freeing he from the constraints of predefined computational tools, and 
promoting creative freedom in the construction of visual metaphors. Resulting from the intersection of 
programming with the creative process of design, the work graphic narratives explores new creative possibilities 
in the design of covers for book collections. 2 Approach and results Designing covers for a collection 
requires harmonizing the need of creating a visual unity with the individualization of each cover. Generative 
processes tend to implicitly define a visual language of akin, yet different, shapes. As such it is considered 
that they may be relevant in this context. Striving to get away from the classical approach to cover 
design, simplification of the book s contents and reinterpretation, we explore the individualization 
of each cover through the analysis of the shape of the text, instead of the analysis of its content. 
This rationalistic approach, inspired by the literary movement of the 50s Concretism , more concerned 
with writing in a rational way than in an emotional one, allowed us to attain the desired levels of unity 
and individuality. __________________________ 1 ligia@student.dei.uc.pt 2 machado@dei.uc.pt 3 arturr@dei.uc.pt 
The ultimate goal is not to visually communicate the form of the texts in a quickly and easily interpretable 
way, as would be desirable in the discipline of information visualization, but rather the creation of 
graphical shapes that abstractly characterize the form of each text. Two different results for the same 
collection of books (various authors and epochs) are presented. In the first case . taking into account 
that punctuation marks directly contribute to the construction of the rhythm of the text a pattern is 
created through the direct mapping of the marks and the rotation of each element (Fig.1a ). Figure 2 
Detail of the graphic line of figure 1a . In the second case, the high level structure of the text comes 
into play: the number of rectangles equals the number of chapters and the area of each rectangle is proportional 
to the length of the corresponding chapter (Fig.1b). By using these elements, punctuation marks and chapter 
divisions, that bestow explicit structure to the text, we were able to create visual artifacts that assist 
the designer in the construction of elegant covers for collections. Other factors, such as the usage 
of different punctuation marks, paragraph frequency, the average length of sentences, its variance, could 
also have been explored. References TufTe, edward r., 1990. Envisioning Information. Cheshire, CT: Graphics 
Press. Maeda, J., 2004. Creative Code.Thames &#38; Hudson. Copyright is held by the author / owner(s). 
SIGGRAPH 2012, Los Angeles, California, August 5 9, 2012. ISBN 978-1-4503-1435-0/12/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2342926</article_id>
		<sort_key>300</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>23</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[SketchGraph]]></title>
		<subtitle><![CDATA[gestural data input for mobile tablet devices]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2342896.2342926</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2342926</url>
		<abstract>
			<par><![CDATA[<p>As tablets become ever more powerful and popular, people want to use them broadly, including for business applications like spreadsheet data graphing. Tablets are better suited to informal exploration through sketching, however, than to inputting data into a spreadsheet. It would be much more appealing, and suitable to the medium, to sketch a graph as if you were drawing on a napkin. We describe an early prototype to support a gestural, graphical interface for inputting and updating graph data that is as easy as drawing a few strokes. With it, users can focus on exploring their domain, rather than on the mechanics of data entry.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3737264</person_id>
				<author_profile_id><![CDATA[81100508902]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Jacquelyn]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Martino]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[IBM Research, Skyline Drive, Hawthorne, NY]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[jmartino@us.ibm.com]]></email_address>
			</au>
			<au>
				<person_id>P3737265</person_id>
				<author_profile_id><![CDATA[81100317041]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Paul]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Matchen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[IBM Research, Skyline Drive, Hawthorne, NY]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[matchen@us.ibm.com]]></email_address>
			</au>
			<au>
				<person_id>P3737266</person_id>
				<author_profile_id><![CDATA[81100333974]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Harold]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ossher]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[IBM Research, Skyline Drive, Hawthorne, NY]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[ossher@us.ibm.com]]></email_address>
			</au>
			<au>
				<person_id>P3737267</person_id>
				<author_profile_id><![CDATA[81100257325]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Rachel]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bellamy]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[IBM Research, Skyline Drive, Hawthorne, NY]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[rachel@us.ibm.com]]></email_address>
			</au>
			<au>
				<person_id>P3737268</person_id>
				<author_profile_id><![CDATA[81100027041]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Cal]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Swart]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[IBM Research, Skyline Drive, Hawthorne, NY]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[cals@us.ibm.com]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1985909</ref_obj_id>
				<ref_obj_pid>1985793</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Bellamy, R. K. E., et al., Sketching tools for ideation: NIER track. In Proceedings of ICSE. 2011, 808--811.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Csikszentmihalyi, M. 1990. Flow: The Psychology of Optimal Experience, New York: Harper and Row.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1281521</ref_obj_id>
				<ref_obj_pid>1281500</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[LaViola, J. J. Jr.. 2007. Mathematical sketching. In ACM SIGGRAPH 2007 courses (SIGGRAPH '07). ACM, New York, NY, USA, Article 10]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Tversky, B. et al. (2003). Sketches for design and design of sketches. In Ugo Lindemann (Ed), Human behavior in design: Individuals, teams, tools. Pp. 79--86. Berlin: Springer.D.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 SketchGraph: Gestural Data Input for Mobile Tablet Devices Jacquelyn Martino, Paul Matchen, Harold Ossher, 
Rachel Bellamy, Cal Swart IBM Research 19 Skyline Drive Hawthorne, NY 10532 {jmartino,matchen,ossher,rachel,cals}@us.ibm.com 
 Figure 1. User draws a recognized graph gesture (left), strokes curves and labels (middle), gestures 
to get the underlying data table (right). 1 Introduction As tablets become ever more powerful and popular, 
people want to use them broadly, including for business applications like spreadsheet data graphing. 
Tablets are better suited to informal exploration through sketching, however, than to inputting data 
into a spreadsheet. It would be much more appealing, and suitable to the medium, to sketch a graph as 
if you were drawing on a napkin. We describe an early prototype to support a gestural, graphical interface 
for inputting and updating graph data that is as easy as drawing a few strokes. With it, users can focus 
on explor­ing their domain, rather than on the mechanics of data entry. 2 Exposition In our hand-drawn 
computing project [Bellamy et al. 2011] we investigate user interfaces (UI) for natural, hand-drawn problem 
solving using digital tablets. Sketching, and the visual thinking it enables [Tversky et al. 2003], are 
powerful tools for problem solv­ing, especially during the early, exploratory stages. Its effective­ness 
largely depends on enabling users to stay in the flow [Csikszentmihalyi 1990], focused on their free-flowing, 
prob­lem-solving activities without being distracted by the medium. At the same time, however, we have 
design goals similar to LaViola [2007] in that we want to leverage the power of digital media, building 
models implied by sketch marks as appropriate and ele­vating the paper plane to a dynamic computational 
space. This has the potential to replace the familiar disconnect between early exploration and later, 
detailed problem solving, design, and mod­eling with a smooth transition eased by shared information. 
Our talk describes early research in the area of numeric explora­tion on tablets. For many domains, problem 
solving involves playing with data to understand relationships and trends. A flowy way to work is to 
sketch graphs, but the graphs remain entirely informal, with no actual data behind them. Alternatively, 
one can enter data into a table or spreadsheet, and produce graphs from them. This has the opposite problem: 
entering data breaks the flow when one s goal is to produce a graph that shows a de­sired relationship 
or trend. We want the best of both worlds. SketchGraph allows the user to sketch a graph on a tablet, 
using a few simple gestures (Fig. 1). The strokes making up the axes are recognized as such, and subsequent 
strokes within the upper-right quadrant are then interpreted as curves. A collection of points on each 
stroke is computed and tabulated automatically. A smooth curve is drawn through these points, with the 
points highlighted. The graph can be made up of many curves, each of which is interpreted as distinct 
data series, distinguished by color. The user can label the axes as well as the graph as a whole. The 
user can manipulate the curves, moving the identified points around at will, with quick, automatic data 
interpolation and under­lying data table updates. Dragging a curve from its starting point and off the 
edge of the graph deletes it. A C gesture clears the entire graph area. The user can thus manipulate 
graphs on the tablet in a flowy way, even more so than on a whiteboard (which does not support point 
movement with automatic interpolation). In addition, the user can, at any time, make a down stroke to 
see the underlying table of numbers. The data series for each curve is shown in the same color as the 
curve itself. The user can now go back and forth, examining and changing either the numbers in the table 
or the positions of points on the graph, at will. As the user changes one representation, the system 
automatically updates the other, so both the visual and numeric effects are manifest. We envision the 
user staying in the flow throughout their entire interaction with numerical data, supported through gestures 
such as circling to select, and pinching and stretching to shrink and expand axes. Our sketch like gestural 
interactions are in contrast with OmniGraphSketcher s (www.omnigroup.com) palette approach to drawing 
and manipulating graphs. In their case, a traditional WIMP like interface breaks user flow by demanding 
additional interaction overhead. Our prototype is an HTML5 application, initially targeting the iPad. 
We use a number of underlying technologies to enable our UI design investigations. The $1 Unistroke Recognizer 
recognizes strokes as a graph or as individual characters (http://depts. washing­ton.edu/aimgroup/proj/dollar/). 
To extend the recognized template to multiple strokes, we added a timer to trigger the recognizer after 
a delay in drawing a stroke sequence and pieced the strokes to­gether into a single stroke for recognition. 
KineticJS provides an object and event model that allows interactivity on an otherwise static canvas 
element (http://www.kineticjs.com/). To determine Bézier control points for pleasing curves through a 
set of points we refer to Rob Spenser (http://scaledinnovation.com/). References BELLAMY, R.K.E., et 
al., Sketching tools for ideation: NIER track. In Proceedings of ICSE. 2011, 808-811. CSIKSZENTMIHALYI, 
M. 1990. Flow: The Psychology of Optimal Experi­ence, New York: Harper and Row. LAVIOLA, J. J. Jr.. 2007. 
Mathematical sketching. In ACM SIGGRAPH 2007 courses (SIGGRAPH '07). ACM, New York, NY, USA,Article 10 
TVERSKY, B. et al. (2003). Sketches for design and design of sketches. In Ugo Lindemann (Ed), Human behavior 
in design: Individuals, teams, tools. Pp. 79-86. Berlin: Springer.D. Copyright is held by the author 
/ owner(s). SIGGRAPH 2012, Los Angeles, California, August 5 9, 2012. ISBN 978-1-4503-1435-0/12/0008 
  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2342927</article_id>
		<sort_key>310</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>24</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Sketching knots]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2342896.2342927</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2342927</url>
		<abstract>
			<par><![CDATA[<p>Physical knots have always fascinated people. For instance, Ashley Book of Knots contains some 7000 illustrations covering over 2000 knots. Physical knots usually associated with sailing, but they are also used in weaving, knitting, braiding and crocheting. Mathematical knots are introduced to formally classify the physical knots [Turner and de Griend 1996]. The significant difference with mathematical and physical knots is that in mathematical knots threads are closed curves. This property helps to formulate the problem more precisely and several knot polynomials such as Alexander and Jones polynomials are introduced to categorize the knots. In decorative arts, the most well-known knot form is Celtic knots, which are also good examples that shows how difficult to draw for humans knots. To draw Celtic knots and to obtain cyclic plain weaving on surfaces mesh based methods are introduced [Kaplan and Cohen 2003; Akleman et al. 2009]. Except mesh based methods, we do not know any other way to provide people to design knots. There exist software such as Mathematica's knot theory package or Knotplot, but these software does not provide free form design of knots.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3737269</person_id>
				<author_profile_id><![CDATA[81502805158]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Ozgur]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gonen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Texas A&M University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737270</person_id>
				<author_profile_id><![CDATA[81100124987]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Ergun]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Akleman]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Texas A&M University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1531384</ref_obj_id>
				<ref_obj_pid>1576246</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Akleman, E., Chen, J., Xing, Q., and Gross, J. L. 2009. Cyclic plain-weaving on polygonal mesh surfaces with graph rotation systems. In <i>ACM SIGGRAPH 2009 papers</i>, SIGGRAPH '09, 78:1--78:8.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>882406</ref_obj_id>
				<ref_obj_pid>882404</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Kaplan, M., and Cohen, E. 2003. Computer generated celtic design. In <i>Proc. of 14th Eurographics Workshop on Rendering</i>, 9--16.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Turner, J., and de Griend, P. V. 1996. <i>History and Science of Knots</i>. World Scientific Publishing, Singapore.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Texas A&#38;M University Texas A&#38;M University Sketch view Side view Sketch view Side view with 
sketching plane Figure 1: Examples of knots that are created using our sketch based interface. Physical 
knots have always fascinated people. For instance, Ashley Book of Knots contains some 7000 illustrations 
cover­ing over 2000 knots. Physical knots usually associated with sailing, but they are also used in 
weaving, knitting, braid­ing and crocheting. Mathematical knots are introduced to formally classify the 
physical knots [Turner and de Griend 1996]. The signi.cant di.erence with mathematical and physical knots 
is that in mathematical knots threads are closed curves. This property helps to formulate the problem 
more precisely and several knot polynomials such as Alexan­der and Jones polynomials are introduced to 
categorize the knots. In decorative arts, the most well-known knot form is Celtic knots, which are also 
good examples that shows how di.cult to draw for humans knots. To draw Celtic knots and to obtain cyclic 
plain weaving on surfaces mesh based methods are introduced [Kaplan and Cohen 2003; Akleman et al. 2009]. 
Except mesh based methods, we do not know any other way to provide people to design knots. There exist 
software such as Mathematica s knot theory package or Knotplot, but these software does not provide free 
form design of knots. We present an unexpectedly easy to use interface to create knots by using sketch 
based modeling. In our interface the only thing the users need to do for creating knots and links is 
to draw a set of curves. These curves serves the medial axis of the knots to be constructed. To construct 
knots we .rst estimate of the depth z value for every point on the medial axis curve. The depth estimation 
turns 2D medial axis curve into a 3D medial axis. We then extrude a polygon along the 3D medial axis 
curve to obtain the tread that forms the physical knot. If the medial axis consists of closed curves, 
the result is a mathematical knot. The key part of our knot construction is the depth estimation. Our 
depth estimation is developed based on following observations: 1. The threads are drawn under orthographic 
transforma­tion and they do not self intersect. 2. The threads cross other threads (or themselves) by 
alternatingly going over and under.  The .rst observation is natural and expected. The second one is 
not always desired but it signi.cantly simpli.es the in­terface and reduces the amount of intersection. 
For instance, The mathematical knots that exhibit this pattern is called alternating knots in mathematical 
knot theory. Since most prime knots are also alternating, our system automatically can create these knots 
from sketches. Alternating links such as well-known Borromean link can also be created automat­ically 
from sketches. Many well-known physical knots can be created using al­ternating pattern. It is interesting 
to note that even if the knot is not alternating, it is still possible to obtain a de­sired knot by moving 
the treads closer. In this case, system counts two threads as one and it is possible to obtain desired 
knot automatically. There are also many objects around us from spaghetti to snakes whose visual appearance 
can be im­proved introducing some alternating in crossings. Weaving or knitting can also be obtained 
using such an approach. One issue is that a particular knot cannot be created by alternat­ing pattern. 
To resolve those cases we provide an interface option to the user for reverse the order in a crossing 
by a mouse click. References Akleman, E., Chen, J., Xing, Q., and Gross, J. L. 2009. Cyclic plain-weaving 
on polygonal mesh surfaces with graph rotation systems. In ACM SIGGRAPH 2009 papers, SIGGRAPH 09, 78:1 
78:8. Kaplan, M., and Cohen, E. 2003. Computer generated celtic design. In Proc. of 14th Eurographics 
Workshop on Rendering, 9 16. Turner, J., and de Griend, P. V. 1996. History and Science of Knots. World 
Scienti.c Publishing, Singapore. Copyright is held by the author / owner(s). SIGGRAPH 2012, Los Angeles, 
California, August 5 9, 2012. ISBN 978-1-4503-1435-0/12/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2342928</article_id>
		<sort_key>320</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>25</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[tamable looper]]></title>
		<subtitle><![CDATA[creature-like expressions and interactions by movement and deformation of clusters of sphere magnets]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2342896.2342928</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2342928</url>
		<abstract>
			<par><![CDATA[<p>Researches using unique materials to move or deform objects are now popular. There are generally two approaches to accomplish this achievement. By implanting actuators, such as bio-metals, directly into the object is a common solution. In this way, actuators can disturb the materials behaviors or characteristics and ruin some performances. On the other hand, researches that enable to move or deform objects without implanting any actuators directly into the material also have been accomplished. In this approach, magnetic force is appropriate and useful for moving or deforming objects.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3737271</person_id>
				<author_profile_id><![CDATA[81504682944]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Michinari]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kono]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[t08524mk@sfc.keio.ac.jp]]></email_address>
			</au>
			<au>
				<person_id>P3737272</person_id>
				<author_profile_id><![CDATA[81100527572]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Yasuaki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kakehi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio Universtiy]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[ykakehi@sfc.keio.ac.jp]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>572011</ref_obj_id>
				<ref_obj_pid>571985</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Pangro, G., Maynes-Aminzade, D., and Ishii, H. 2002. The Actuated Workbench: Computer-Controlled Actuation in Table-top Tangible Interfaces. In <i>Proceedings of UIST</i>, ACM, 181--190.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2047239</ref_obj_id>
				<ref_obj_pid>2047196</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Lee, J., Post, R., and Ishii, H. 2011. ZeroN: Mid-Air Tangible Interface Enabled by Computer Controlled Magnetic Levitaition. In <i>Proceedings of UIST 2011</i>, ACM, 327--336.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Kodama, S., and Takeno, M. 2001. Protrude, Flow. In <i>SIGGRAPH 2001 Art Gallery</i>, ACM.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 tamable looper: Creature-like Expressions and Interactions by Movement and Deformation of Clusters of 
Sphere Magnets MichinariKono* Yasuaki Kakehi* Keio University Keio Universtiy  Figure 1: tamable looper 
Figure 2: System Design Figure 3: Interaction 1 Introduction Researches using unique materials to move 
or deform objects are now popular. There are generally two approaches to accomplish this achievement. 
By implanting actuators, such as bio-metals, di­rectly into the object is a common solution. In this 
way, actuators can disturb the materials behaviors or characteristics and ruin some performances. On 
the other hand, researches that enable to move or deform objects without implanting anyactuators directly 
intothe material also have been accomplished. In this approach, magnetic force is appropriate and useful 
for moving or deforming objects. Previous researchessuchasthe ActuatedWorkbench[Pangroetal. 2002], use 
magnetic force to move objects onaworkbench surface. ZeroN [Lee et al. 2011] can levitate and move objects 
in a three dimensional space. Moreover, ferro.uid is a popular material and artworkssuchas Protrude,Flow[Kodamaetal.2001]havebeenac­complished. 
Our system, tamable looper (see Figure 1), enables not only to move and deform shapes of multiple objectsbut 
to control postures and ways of how it moves. Furthermore, interactions and engaging creature-like expressions 
are included, which can inspire users to interact naturally. The system is consisted of electromag­nets 
and clusters of sphere shaped neodymium magnets.  2 tamable looper The tamable looper proposes two technical 
innovations as follows. Firstly, we managed a peculiar algorithm, controlling electromag­nets to move 
and deform the clusters of the sphere shaped magnets (the looper,see Figure 2). This enables our system 
to represent .ex­ible and cubic expressions. The system consists of 100 electromag­nets arranged in a 
10 x 10 grid, which are independently control­lable and designed to drive bi-directionally. Utilizing 
the magnetic force and direction driven from the electromagnets, attraction and repulsion occur to the 
looper and the patterns of the force gener­ates various actions. The force of attraction and repulsion 
occurred from the electromagnets are switched rapidly and the algorithm to express such actions is essential 
and delicate. Secondly,we proposetourge usersto interact naturally,byimplant­ing creature-like movements, 
imitating a looper . We use the Mi­crosoft kinect device to capture and analyze the user sgestures. Ac­ 
*e-mail: {t08524mk, ykakehi}@sfc.keio.ac.jp cording to the gesture represented, the looper will act 
relatively(see Figure3). No additional devices are required, thus the manipulation is intuitive and simple. 
 3 Applications and Future Works The tamable looper provides various actions such as moving like a looper, 
standing up, wheeling, .ipping and jumping. Each ac­tion is linked to the user s interactions and is 
capable to manipulate with intuitive gestures. Combinations of actions mentioned above are practical 
to develop unique applications. The system can be utilized as a tabletop miniature theater. Since the 
system does not recommend to have a .at or parallel surface, depending on how we arrange the electromagnets, 
the looper may move onabumpy, sphere or round surface. Actions changes depending on relations of the 
positions between the creatures or gestures responded from the user. Engaging and charming behavior will 
certainly entertain users and have potentiality to be an enjoyable creature. Inthe future, tamable looper 
canbe utilizedasa substantial interac­tive display, by designing images with plural loopers. Establishing 
algorithms to control plural loopers at the same time and to avoid interfering each other are necessary. 
Furthermore, we plan to model adeveloped device on larger scale and to provide anyattained infor­mation 
by patterns and actions produced by plurally located loop­ers. Therefore, we plan to update for the increase 
of interaction techniques and moving and deforming patterns. References PANGRO,G.,MAYNES-AMINZADE,D., 
ANDISHII,H. 2002. The ActuatedWorkbench: Computer-Controlled ActuationinTable­topTangible Interfaces.In 
Proceedings of UIST,ACM, 181 190. LEE,J.,POST,R., AND ISHII,H. 2011. ZeroN: Mid-AirTangible Interface 
EnabledbyComputer Controlled MagneticLevitaition. In Proceedings of UIST 2011,ACM, 327 336. KODAMA, S., 
AND TAKENO, M. 2001. Protrude, Flow. In SIG-GRAPH 2001 Art Gallery,ACM. Copyright is held by the author 
/ owner(s). SIGGRAPH 2012, Los Angeles, California, August 5 9, 2012. ISBN 978-1-4503-1435-0/12/0008 
  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2342929</article_id>
		<sort_key>330</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>26</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>5</seq_no>
		<title><![CDATA[Transparent]]></title>
		<subtitle><![CDATA[brain computer interface and social architecture]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2342896.2342929</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2342929</url>
		<abstract>
			<par><![CDATA[<p>Transparent is an office window that varies its opacity in order to help a user maintain focus at work. By changing opacity, the window blocks distractions in the user's environment while subtly signaling the person's availability to others. The user's focus is determined via a neuroheadset that passively measures her brain activity through electroencephalography (EEG); the focus of the user is then algorithmically determined and wirelessly communicated to a smart glass module that changes transparency accordingly. Transparent explores opportunities to merge brain computer interfaces (BCI) with smart architecture to improve productivity at work.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3737273</person_id>
				<author_profile_id><![CDATA[81500661904]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Arlene]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ducao]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MIT media cab, Cambridge, MA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[arlduc@media.mit.edu]]></email_address>
			</au>
			<au>
				<person_id>P3737274</person_id>
				<author_profile_id><![CDATA[81479648755]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Tiffany]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tseng]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MIT media cab, Cambridge, MA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[ttseng@media.mit.edu]]></email_address>
			</au>
			<au>
				<person_id>P3737275</person_id>
				<author_profile_id><![CDATA[81548023185]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Anette]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[von Kapri]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MIT media cab, Cambridge, MA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[kapri@media.mit.edu]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Buxton, B. 1999. The future and emerging potential. <i>Human Input to Computer Systems: Theories, Techniques, and Technologies</i>..]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[NeuroSky, 2012. Neurosky: Dry sensor technology.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1855009</ref_obj_id>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Tan, D. S., and Nijholt, A. 2010. <i>Brain-Computer Interaction: Applying our Minds to Human-Computer Interaction</i>. Springer-Verlag.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Transparent: Brain Computer Interface and Social Architecture Arlene Ducao, Tiffany Tseng, Anette von 
Kapri* MIT Media Lab, Cambridge, MA  Figure 1: (Left) A person is sitting at her desk in the of.ce without 
Transparent. Distractions are visible through transparent windows. (Right) The Transparent system measures 
the brain activity and changes the window opacity according to the user s level of focus. Abstract Transparent 
is an of.ce window that varies its opacity in order to help a user maintain focus at work. By changing 
opacity, the win­dow blocks distractions in the user s environment while subtly sig­naling the person 
s availability to others. The user s focus is deter­mined via a neuroheadset that passively measures 
her brain activ­ity through electroencephalography (EEG); the focus of the user is then algorithmically 
determined and wirelessly communicated to a smart glass module that changes transparency accordingly. 
Trans­parent explores opportunities to merge brain computer interfaces (BCI) with smart architecture 
to improve productivity at work. 1 Introduction and Motivation Workers can .nd it dif.cult to concentrate 
in busy of.ce environ­ments, particularly spaces with open .oor plans that may encour­age social interaction 
at the cost of privacy (Figure 1a). This is especially the case at the MIT Media Lab s recently erected 
2009 complex in which every of.ce has at least one glass wall. The glass limits researchers privacy and 
makes it easy for them to become distracted by outside activity. A system that conveys a person s availability 
to others while blocking outside distractions could help improve the user s focus and productivity. To 
address this design problem, we have developed Transparent, a system that passively measures a user s 
level of focus by way of a neuroheadset. The user s focus is then communicated to others in the work 
environment through a changing opacity window; as the user becomes focused, the window becomes more opaque, 
helping to block out distractions while signaling to others that the user is occupied (Figure 1b). The 
window becomes more transparent as the user s level of focus decreases, communicating that the user is 
more available to be interrupted. 2 Related Work The main contribution of this work is the application 
of BCI for social architecture. BCI, particularly EEG interfaces, have tradi­tionally been used in the 
health sector [Tan and Nijholt 2010] but have recently moved into the consumer market with the develop­ment 
of low-cost, plug-and-play EEG devices. Consumer products are often packaged with software for helping 
users reduce stress or games in which users control virtual or physical objects with their *e-mail: arlduc, 
ttseng, kapri@media.mit.edu mind. However, these interfaces are designed to help users monitor their 
own level of focus without communicating these levels to oth­ers. A classic example of a social application 
for communicating availability is Bill Buxton s Door Mouse in which a physical of.ce door is coupled 
with a virtual icon that displays a user s availabil­ity to others online [Buxton 1999]; however, with 
this interface, the user is actively manipulating the icon by opening or closing her door. Transparent 
occupies the unique space of being a passive and social application; the EEG interface passively monitors 
the user s attention level in order to socially communicate the user s availabil­ity and help the user 
minimize unwanted distractions. 3 Design and Future Work Transparent currently uses EEG input from a 
single user through a NeuroSkys ThinkGear ASIC Module[NeuroSky 2012]. The head­ set outputs a combination 
of alpha and gamma brain wave signals and attention and medication levels, which are algorithmically 
pro­cessed to determine the user s level of focus. The user s level of focus is wirelessly transmitted 
to a custom controller for an elec­trochromic smart glass tile, which consequently changes opacity anywhere 
between fully opaque and fully transparent. Transparent s mapping of brain data to window opacity provides 
a subtle aid for users trying to prioritize social interaction and indi­vidual work. Furthermore, lower 
light levels can help users relax if they are stressed, so increasing the opacity of the window both 
acts as a signal to others and also helps adjust the lighting of the room to improve a person s emotional 
state. As Transparent is developed, usability tests will be conducted to examine the tradeoffs between 
passive and active control and opti­mize for utility and comfort, both for the user and for other people 
in the work environment. Additionally, since EEG signals tend to be rather noisy, a statistical analysis 
of how well the signals map to the mental state needs to be performed.  References BUXTON, B. 1999. 
The future and emerging potential. Human In­put to Computer Systems: Theories, Techniques, and Technolo­gies.. 
NEUROSKY, 2012. Neurosky: Dry sensor technology. TAN, D. S., AND NIJHOLT, A. 2010. Brain-Computer Interaction: 
Applying our Minds to Human-Computer Interaction. Springer-Verlag. Copyright is held by the author / 
owner(s). SIGGRAPH 2012, Los Angeles, California, August 5 9, 2012. ISBN 978-1-4503-1435-0/12/0008 
 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	</section>
	<section>
		<section_id>2342930</section_id>
		<sort_key>340</sort_key>
		<section_seq_no>8</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Art]]></section_title>
		<section_page_from>8</section_page_from>
	<article_rec>
		<article_id>2342931</article_id>
		<sort_key>350</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>27</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[The flying : Kinect art using OpenNI and learning system]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2342896.2342931</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2342931</url>
		<abstract>
			<par><![CDATA[<p>Humans have the desire to fly. Out of the desire, they invented the airplane through many processes and today a large number of airplanes are flying to move people from place to place. Every person may feel the desire to fly when they see a bird flying high. Such a feeling and desire were expressed in a our digital art work that adopted reinforcement learning concept.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3737276</person_id>
				<author_profile_id><![CDATA[81375606832]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Ok-Hue]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Cho]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Chung-Ang University, Seoul, South Korea]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[pluszzang@hotmail.com]]></email_address>
			</au>
			<au>
				<person_id>P3737277</person_id>
				<author_profile_id><![CDATA[81100384671]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Won-Hyung]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lee]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Chung-Ang University, Seoul, South Korea]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[whlee@cau.ac.kr]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[http://code.google.com/p/simple-openni/]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Untitled-8 The Flying : Kinect art using OpenNI Untitled-2 and Learning System ..2 ddd1 Ok-Hue Cho 
GSAIM CT / GAME Lab Chung-Ang University Seoul, South Korea pluszzang@hotmail.com Won-Hyung Lee GSAIM 
CT / GAME Lab Chung-Ang University Seoul, South Korea whlee@cau.ac.kr 1. Introduction Humans have the 
desire to fly. Out of the desire, they invented the airplane through many processes and today a large 
number of airplanes are flying to move people from place to place. Every person may feel the desire to 
fly when they see a bird flying high. Such a feeling and desire were expressed in a our digital art work 
that adopted reinforcement learning concept. Bird is the only animal that can fly among vertebrates. 
The three most important motions for a bird s flying are flapping, twisting and folding. Flapping is 
moving both wings up and down, twisting is a motion that twists the two wings in the opposite directions, 
and folding is a motion that folds the two wings up to the body and then unfolds them. Through our art 
work, spectators experience such an instinctive system of birds indirectly. 2. Implementation    
   Figure 3 Skeleton tracking (shoulder, elbow, hand, torso)   Figure 1. System of The Flying Making 
This work was implemented based on reinforcement learning system. The spectator is faced with the work 
without any prior knowledge of the work. The first image that the spectator sees is a low land. It is 
the view of land that the young bird sees from above on a tree. Because the image displays fine movements 
in response even to the spectator s little movements, so by intuition the spectator realizes the fact 
that the image moves according to his/her motion. Stimulated by the image and phrase, the spectator makes 
a flapping motion like a bird instinctively or out of curiosity. From that moment, learning is started. 
Then the screen sways up and down gradually, giving the feeling as if a young bird is trying to flutter. 
We used Kinect to recognize gesture of spectator using Simple-openni library in Processing. After a certain 
amount of motion, the spectator is presented with the next-stage image as a reward. Because the process 
depends on the measured volume of the spectator s movement, the length of time until the spectator sees 
the next-stage image varies among spectators. Once the spectator moves over to the next stage, the horizon 
on the image goes down a little bit. This means that the screen shows more of the sky. If the spectator 
does not move the image does not play either. All shown images are from the first-person viewpoint seen 
with the bird s eye. If the spectator makes bigger motions, he/she can move to the next stage earlier. 
This is not one dimensional interaction but staged flying training. This experiment is not a  one-dimensional 
digital art but a digital art for experiencing a reinforce- ment learning system directly by number of 
stages. We used 6 Figure 2 Result as motion ( The image is played only when spectator is moving ) skeletons 
for our work. Spectator s shoulder, elbow, hand, torso are main coordinates. Spectator can see calibrated 
6 skeletons in screen and recognize their gesture by themselves. When the spectator stops the motion 
or disappears from the screen, the screen shows the first image(ground) again. After they learned all 
the three important movements for bird's flying with continuous motion(movem- ent), the art work finished 
by showing the flying image in the sky.  4. Conclusions This work has its meaning in analyzing the 
results of the application of reinforcement learning as a game implementation technology in the implementation 
of a digital art work. From the experiment were obtained results as follows. First, when reinforcement 
learning was applied, the activeness of the spectator s involvement is enhanced in the interaction of 
digital art. Second, it increases the spectator s immersion in the work and the length of viewing. These 
results suggest that when reinforcement learning, a game implementation technology, is applied to the 
implementation of digital art it enhances the spectator s involvement. References http://code.google.com/p/simple-openni/ 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	</section>
	<section>
		<section_id>2342932</section_id>
		<sort_key>360</sort_key>
		<section_seq_no>9</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Design]]></section_title>
		<section_page_from>9</section_page_from>
	<article_rec>
		<article_id>2342933</article_id>
		<sort_key>370</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>28</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Typeface styling with ramp responses]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2342896.2342933</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2342933</url>
		<abstract>
			<par><![CDATA[<p>Linear filters are standard tools of the artist in image, video and audio processing. This work demonstrates that they can also be used for <i>vector</i> graphics, in particular for typeface design. Serifs and other features of a typeface can be created and edited; gross effects are possible, too. In the software prototype, the user can freely define the shapes that the &#9492; type and the &#9496; type of convex axis-aligned rectangular corners should have after filtering. Corners of the types &#9488; and &#9484; will have the corresponding shapes, rotated by 180&#176; degrees. The two target shapes can be edited as cubic B&#233;zier paths. The result on a given text in a selected typeface is computed as vector graphics and displayed at an interactive rate.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3737278</person_id>
				<author_profile_id><![CDATA[81408599742]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[J&#246;rn]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Loviscach]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Fachhochschule Bielefeld (University of Applied Sciences), Bielefeld, Germany]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[joern.loviscach@fh-bielefeld.de]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>138775</ref_obj_id>
				<ref_obj_pid>138774</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Mokhtarian, F., and Mackworth, A. K. 1992. A theory of multiscale, curvature-based shape representation for planar curves. <i>IEEE TPAMI 14</i>, 8 (Aug), 789--805.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Typeface Styling with Ramp Responses J¨orn Loviscach* Fachhochschule Bielefeld (University of Applied 
Sciences) Bielefeld, Germany 1 Introduction Linear .lters are standard tools of the artist in image, 
video and audio processing. This work demonstrates that they can also be used for vector graphics, in 
particular for typeface design. Serifs and other features of a typeface can be created and edited; gross 
effects are possible, too. In the software prototype, the user can freely de.ne the shapes that the type 
and the type of convex axis-aligned rectangular corners should have after .ltering. Corners of the types 
and will have the corresponding shapes, rotated by 180. degrees. The two target shapes can be edited 
as cubic B´ezier paths. The result on a given text in a selected typeface is computed as vector graphics 
and displayed at an interactive rate. In contrast to other recent approaches, the technique does not 
em­ploy any high-level recognition of serifs or other features. Hence, it is fast and robust and produces 
consistent results for any shape that may occur in the font .le. 2 Technique Filtering planar curves 
by convolution is a tool long known in shape analysis [Mokhtarian and Mackworth 1992], in particular 
as a pre­ processing step for pattern recognition. A curve is given by a map­ping r : [0,stotal) . R2 
of arc length measured from an arbitrary reference point on the curve to two-dimensional position. Here, 
stotal stands for the complete arc length of the curve. A curve spec­i.ed in this manner can be .ltered 
by convolution with a 2 × 2­matrix-valued function H: 8 s .H(u) r(s - u) du, (1) -8 where the mapping 
r is interpreted as periodic with period stotal. The resulting curve will typically not be parameterized 
by arc length, but it is easy to check that it is closed, irrespective of the choice of H. Hence, the 
2 × 2 element functions of the matrix H can be employed freely for design. Note that standard shape anal­ysis 
does not use an arbitrary matrix function H, but employs a scalar Gaussian instead. The relation of the 
four component functions of H to the visual re­sult is not straightforward. Hence, the user interface 
cannot simply show these functions for editing. Interestingly, however, it turns out that specifying 
the target shape for the corners and completely de.nes these four functions and that any target shape 
for these cor­ners can actually be achieved. Key to this is the following observation: Let s . (x1(s),y1(s)) 
be the coordinates of the target shape for a -type corner and let s . (x2(s),y2(s)) be the coordinates 
of the target shape for a ­type corner, both corners centered at a value s0 of the arc length. (As opposed 
to the original curve, these target curves do not need to be parameterized by arc length.) Then a computation 
using Eq. 1 shows the following relations between the second derivatives of the coordinate functions 
and the elements of the matrix H: d2 x1(s) x2(s)1 -1 = H(s - s0) ds2y1(s) y2(s)11 *e-mail: joern.loviscach@fh-bielefeld.de 
 Figure 1: The software prototype offers interactive controls for the target shapes of corners, demonstrated 
with two different settings applied to Times New Roman and Verdana Bold, respectively. This equation 
is uniquely solvable in terms of the matrix H. As the x and y components of the r mappings of axis-aligned 
corners are ramp functions, adjusting the corner shapes can be considered to be the design of ramp responses. 
 3 Implementation The software prototype computes the .ltered version of the r map­pings for all contours 
of all glyphs of the entered text string in the selected font .le. To this end, each contour is discretized 
into a polygon with hundreds or thousands of sides of constant length. While the user drags the control 
points of target shapes of the cor­ners, the four functions of the matrix H are recomputed by nu­merical 
differentiation and applied to the shapes. For acceleration, the convolution process makes use of multi-threading 
by process­ing several glyphs in parallel. It turned out not to be necessary to speed up this process 
further, for instance by employing an FFT. A slider controls the ratio of the arc length of the corner 
input .elds to the arc length of the glyphs. This enables adjusting the overall size scale of the generated 
or affected features. The software prototype is a quick tool for type effects and for sketching new typeface 
designs based on existing fonts. The result can be exported as an SVG vector graphics .le. On export, 
the high­resolution polygonal shapes resulting from convolution can option­ally be converted to a representation 
through cubic B´ezier paths, with the obvious bene.ts for later editing, .le size and geometric quality. 
With this .nal step, the method promises to be a helpful addition to graphics software and to font editing 
software. References MOKHTARIAN, F., AND MACKWORTH, A. K. 1992. A theory of multiscale, curvature-based 
shape representation for planar curves. IEEE TPAMI 14, 8 (Aug), 789 805. Copyright is held by the author 
/ owner(s). SIGGRAPH 2012, Los Angeles, California, August 5 9, 2012. ISBN 978-1-4503-1435-0/12/0008 
  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2342934</article_id>
		<sort_key>380</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>29</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Vignette]]></title>
		<subtitle><![CDATA[a style preserving sketching tool for pen-and-ink illustration with texture synthesis]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2342896.2342934</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2342934</url>
		<abstract>
			<par><![CDATA[<p>Pen-and-ink illustrations take significant amounts of skill, artistry, and patience to create. Digital tools are widely used to accelerate the process; but they provide less artistic freedom and cannot easily capture illustrators' personal style. Furthermore, these tools disrupt the traditional illustration workflow, because they are tedious and draw attention to dialog boxes and away from the illustration itself. We present Vignette, an interactive tool that facilitates texture creation in pen-and-ink illustrations in a natural way with built in texture synthesis capabilities.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3737279</person_id>
				<author_profile_id><![CDATA[81484645237]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Rubaiat]]></first_name>
				<middle_name><![CDATA[Habib]]></middle_name>
				<last_name><![CDATA[Kazi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[JST ERATO, IGARASHI Design Interface Project and National University of Singapore]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[rubaiat@comp.nus.edu.sg]]></email_address>
			</au>
			<au>
				<person_id>P3737280</person_id>
				<author_profile_id><![CDATA[81100444444]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Takeo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Igarashi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[JST ERATO, IGARASHI Design Interface Project and University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[takeo@acm.org]]></email_address>
			</au>
			<au>
				<person_id>P3737281</person_id>
				<author_profile_id><![CDATA[81416592901]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Shengdong]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Zhao]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[National University of Singapore]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[zhaosd@comp.nus.edu.sg]]></email_address>
			</au>
			<au>
				<person_id>P3737282</person_id>
				<author_profile_id><![CDATA[81416597118]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Richard]]></first_name>
				<middle_name><![CDATA[C.]]></middle_name>
				<last_name><![CDATA[Davis]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Singapore Management University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[rcdavis@smu.edu.sg]]></email_address>
			</au>
			<au>
				<person_id>P3737283</person_id>
				<author_profile_id><![CDATA[81335498203]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Kenshi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Takayama]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[kenshi84@acm.org]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>2208302</ref_obj_id>
				<ref_obj_pid>2207676</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Kazi, R. H., Igarashi, T., Zhao, S., and Davis, R. C. 2012. Vignette: Interactive Texture Design and Manipulation with Freeform Gestures for Pen-and-ink Illustration. In <i>CHI (Proceedings of CHI 12)</i>, ACM.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Vignette: A Style Preserving Sketching Tool for Pen-and-Ink Illustrationwith Texture Synthesis Rubaiat 
Habib Kazi1,2, Takeo Igarashi1,3, Shengdong Zhao2, Richard C. Davis4, Kenshi Takayama3 1JST ERATO, IGARASHI 
Design Interface Project2CS, National University of Singapore 3Information Systems, University of Tokyo 
4SIS, Singapore Management University{rubaiat, zhaosd}@comp.nus.edu.sg, {takeo, kenshi84}@acm.org, rcdavis@smu.edu.sg 
 1. Introduction Pen-and-ink illustrations take significant amounts of skill, artistry, and patience 
to create. Digital tools are widely used to acceleratethe process; but they provide less artistic freedom 
and cannoteasily capture illustrators personal style. Furthermore, these tools disrupt the traditional 
illustration workflow, because they are tedious and draw attention to dialog boxes and away from the 
illustration itself. We present Vignette, an interactive tool that facilitates texture creation in pen-and-ink 
illustrations in a natural way with built in texture synthesis capabilities. 2. Vignette: Workflow and 
Interaction Unlike existing systems, Vignette preserves illustrators workflow and style: users draw a 
fraction of a texture and use gestures toautomatically fill regions with the texture with built in texturesynthesis 
capabilities. Our system supports both 1D and 2D synthesis, along with stitching. Our system also has 
interactiverefinement and editing capabilities to provide a higher level texture control, which helps 
artists achieve their desired vision.The interface and interaction design of Vignette was guided bythe 
analysis of traditional pen-and-ink illustration workflow andresulting artifacts [1]. The following steps 
illustrate the typicaldrawing workflow in Vignette:Step-1: The user draws a small fragment of the target 
texture. Step-2: The user then selects a texture filling tool (Brush,Continuous hatching, or Flood fill) 
and gestures to specify howthe texture should be filled in (Fig 1(a), 2(a) and 3(a)). The example strokes 
are automatically collected into a patch, while the direction and curvature of the gesture specify the 
reference orientation of this patch. The system then generates the rest of thetexture from the example 
patch to fill up the region (Fig. 1(b),2(b), 3(b)) using the underlying texture synthesis techniques. 
Thisgesture (red) unifies the definition and application of exampletexture, creating a fluid interaction 
for texture creation from examples.Step-3: After generating the textures, using the higher-levelartistic 
controls, the user interactively refines the textures (Fig.3(c)), tones, perspective view (Fig. 2(c)), 
sweep (Fig. 1(c)) andorientation of the texture to achieve desired results. 3. Conclusion Vignette is 
a practical tool that speeds up creating illustrationwhile preserving natural workflow capturing artists 
personal style. Our evaluation with Vignette shows that even first-time users cancreate complex and expressive 
illustrations within minutes. References KAZI, R.H., IGARASHI, T., ZHAO, S., AND DAVIS, R.C. 2012. Vignette: 
Interactive Texture Design and Manipulation withFreeform Gestures for Pen-and-Ink Illustration. In CHI 
(Proceedings of CHI 12), ACM.   Copyright is held by the author / owner(s). SIGGRAPH 2012, Los Angeles, 
California, August 5 9, 2012. ISBN 978-1-4503-1435-0/12/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2342935</article_id>
		<sort_key>390</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>30</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[When cheesecake craving unplugs the pleasure button]]></title>
		<subtitle><![CDATA[understanding aesthetics and quality of experience in a computer generated graphics]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2342896.2342935</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2342935</url>
		<abstract>
			<par><![CDATA[<p>Some studies claim that the creation and experience of art is an innate capacity that evolved to meet various needs of human survival and reproduction. Steven Pinker for instance argues that an aesthetic stimuli is comparable to strawberry cheesecake which are "mega doses of agreeable stimuli" and are made for the instant purpose of "pressing our pleasure buttons". However, an experience of art is more than just the by-products of innate pleasure circuits; there are factors that draw an individual to them. In our previous study we observed that implicit experiences (IE) lead to a positive evaluation of Quality of Experience (QoE) [Mansilla et al., 2011]. On the other hand, it is important to recognize that aesthetic experience is in part modulated by human nature. For instance, during craving, strong mental thoughts involving vivid associations of imagery (of the craved item) associated with pleasurable sensations and eventually leading to distress due to prolonged deficit, are most evident.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3737284</person_id>
				<author_profile_id><![CDATA[81453655292]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Wendy]]></first_name>
				<middle_name><![CDATA[Ann]]></middle_name>
				<last_name><![CDATA[Mansilla]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Norwegian University of Science and Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737285</person_id>
				<author_profile_id><![CDATA[81472648115]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Jordi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Puig]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Norwegian University of Science and Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737286</person_id>
				<author_profile_id><![CDATA[81100272386]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Andrew]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Perkis]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Norwegian University of Science and Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737287</person_id>
				<author_profile_id><![CDATA[81100284035]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Touradj]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ebrahimi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Norwegian University of Science and Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Kemps, E., et al. (2008). Food cravings consume limited cognitive resources. <i>In Jrnl. of Expm. Psychology: Applied</i>, 14(3), 247--254.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1386177</ref_obj_id>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Kortum, P. (Ed.) (2008) <i>HCI Beyond the GUI: Design for Haptic, Speech, Olfactory and Other Nontraditional Interfaces</i>. Morgan Kaufmann.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2072320</ref_obj_id>
				<ref_obj_pid>2072298</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Mansilla W. A., Perkis A., &amp; Ebrahimi, T. (2011). Implicit experiences as a determinant of perceptual quality and aesthetic appreciation. <i>In ACM Multimedia 2011</i>, 153--162.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 When Cheesecake Craving Unplugs the Pleasure Button: Understanding Aesthetics and Quality of Experience 
in a Computer Generated Graphics Wendy Ann Mansilla, Jordi Puig, Andrew Perkis and Touradj Ebrahimi 
Centre for Quantifiable Quality of Service in Comm. Systems (Q2S),1 Norwegian University of Science and 
Technology 1. Introduction Some studies claim that the creation and experience of art is an innate capacity 
that evolved to meet various needs of human survival and reproduction. Steven Pinker for instance argues 
thatan aesthetic stimuli is comparable to strawberry cheesecake which are mega doses of agreeable stimuli" 
and are made for the instant purpose of "pressing our pleasure buttons". However, an experience of art 
is more than just the by-products of innatepleasure circuits; there are factors that draw an individual 
to them. In our previous study we observed that implicit experiences (IE) lead to a positive evaluation 
of Quality of Experience (QoE) [Mansilla et al., 2011]. On the other hand, it is important to recognize 
that aesthetic experience is in part modulated by human nature. For instance, during craving, strong 
mental thoughtsinvolving vivid associations of imagery (of the craved item) associated with pleasurable 
sensations and eventually leading to distress due to prolonged deficit, are most evident. Ensuring quality 
in a technology-driven medium no longer reliessolely on Quality of Service (QoS) conventions and as a 
result,research on multimedia quality has shifted its focus to QoE. QoE is an effective measure of a 
surface-level multimedia quality butmost studies on QoE fail to consider the implicit factors that affectaesthetic 
experiences [Authors et al., 2011]. In the contemporary visual arts, quality expectations are governed 
by experiences and are no longer based on the traditional realistic rendering or surfacelevel features 
(where highest resolution usually wins). Thus, wedefine Quality of Aesthetic Experience (QAE) as pleasurable 
or positive experiences producing a vivid (heightened) yet biased experience (considers how Alexander 
Baumgarten rationalizesaesthetics in his Reflections on Poetry) modulated by consciousand implicit mental 
concepts. Although several literatures in thehuman-computer field (see [Kortum, 2008]) have recognized 
theimportance of multi-sensory stimulus, there s still a lack of recognition on how the understanding 
of QAE can contribute in optimizing user s experience in a computer-generated graphics. Itis important 
to recognize that many users appreciate computer graphics (e.g. in games) based on its ability to maintain 
a pleasurable experience. Identifying the above surface factors thataffects QAE may shed light on how 
we can further enhance thedesign and assess QAE expected for a certain computer graphicsapplication. 
One implicit factor that may affect QAE is the intrusive human nature (e.g. food craving). In the next 
sections, we describe our initial empirical research on the relationship between food craving and QAE. 
Finally we discuss our futureapproach considering intrusive mental concepts and IE in our investigations 
of the QAE. 2. Exposition Food cravings (FC) are normative to both men and women. Craving is a strong 
motivational state, an intense desire or urge, in which an individual is compelled to seek and ingest 
a particular substance (see [Kemps et al., 2008]). Contrary to hunger, FC tend to be specific. In the 
absence of hunger, we crave a very specific chocolate form or flavor. Over the past decades, several 
studies have demonstrated attentional biases for craving-related stimuliduring craving episodes. This 
tendency to selectively attend and derive pleasure to personally relevant cues (over neutral ones) in 
the form of mental imagery consumes cognitive resources leading to an inferred behavior [ibid, 2008]. 
Fortunately, these intrusivemental thoughts can be modulated by IE. In one experiment, cravers who watched 
flickering patterns on a monitor reported adecrease in the visual vividness of their craved item [ibid, 
2008]. The ability of the intrusive mental concepts and IE to modulatehuman judgment and vivid experiences 
need to be examined by amulti-disciplinary approach in terms of context and QoE. 3. Preliminary Results 
and Future Work Our initial experiment examines the occurrence and modulating effect of FC on QAE when 
engaged with a video game simultaneously running an audio-visual narration. Participants who reported 
FC episodes (measured using the standard FC Inventory scale) when engaged with digital media were recruited. 
Participants (equal distribution of gender with the mean age of 32 years old, healthy and did not ingest 
any food or heavy drinks atleast 4.5 hours before the test) were asked to pick the red applesrandomly 
appearing in a video game. 14 Participants who played a game narrating no food cues performed significantly 
better (picked a mean of 26 out of 35 apples) than those 14 others who played a game showing varieties 
of foodstuff (mean = 23). 18 previous participants played the video game (given in randomtype) approximately 
an hour after taking a meal performed higher compared to the previous conditions (mean = 30; see [Author, 
2012]). The results suggest that intrusive mental concepts (i.e. FC) can significantly interfere with 
the assessment on QAE in acomputer-generated medium. The findings in the current study served as a basis 
of a research protocol for an art installation called Candy, (see, http://q2s.ntnu.no/~wendyann/candy) 
which will be used in our further studies. Inspired by the positive results of the current study, for 
our future work, we have also performed series of experiments using the standard quality impairment measure. 
The results are promising suggesting the high range impact of the intrusive mental concepts on aesthetic 
experiences. The result of the current study and our future studies will serve as a basis towards the 
formulation of a QAE paradigm.  References KEMPS, E., et al. (2008). Food cravings consume limited cognitive 
resources. In Jrnl. of Expm. Psychology: Applied, 14(3), 247-254. KORTUM, P. (Ed.) (2008) HCI Beyond 
the GUI: Design for Haptic, Speech, Olfactory and Other Nontraditional Interfaces. Morgan Kaufmann. MANSILLA 
W. A., PERKIS A., &#38; EBRAHIMI, T. (2011). Implicit experiences as a determinant of perceptual quality 
and aesthetic appreciation. In ACM Multimedia 2011, 153-162. This research has been funded by Centre 
for Quantifiable Quality in Comm. Sys., appointed by Research Council of Norway. Copyright is held by 
the author / owner(s). SIGGRAPH 2012, Los Angeles, California, August 5 9, 2012. ISBN 978-1-4503-1435-0/12/0008 
  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	</section>
	<section>
		<section_id>2342936</section_id>
		<sort_key>400</sort_key>
		<section_seq_no>10</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Games]]></section_title>
		<section_page_from>10</section_page_from>
	<article_rec>
		<article_id>2342937</article_id>
		<sort_key>410</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>31</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[A simulation game for line memorization]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2342896.2342937</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2342937</url>
		<abstract>
			<par><![CDATA[<p>Line memorization is a key aspect of the performing arts. Ideally, actors will practice their lines together, as this most closely emulates the actual performance, and is the best way to learn lines. However, circumstances often arise the force actors to learn lines on their own. This is a much more ineffective way of memorizing lines, and existing techniques to help with this usually involve memorization techniques, or significant extra work on the actor's part. This project aims to solve this problem by utilizing game technology to assist with line memorization. While games are traditionally a medium for entertainment, there is a rising trend in "serious games", or games which are used to help their players in the real world [Tarja et al. 2007]. This game falls under that category, and is (as far as we know) the first game to specifically address the problem of line memorization.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[education]]></kw>
			<kw><![CDATA[serious games]]></kw>
			<kw><![CDATA[simulation games]]></kw>
			<kw><![CDATA[training]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.2.1</cat_node>
				<descriptor>Games</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Virtual reality</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010866</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Virtual reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010476.10011187.10011190</concept_id>
				<concept_desc>CCS->Applied computing->Computers in other domains->Personal computers and PC applications->Computer games</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010070.10010099</concept_id>
				<concept_desc>CCS->Theory of computation->Theory and algorithms for application domains->Algorithmic game theory and mechanism design</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10011007.10010940.10010941.10010969.10010970</concept_id>
				<concept_desc>CCS->Software and its engineering->Software organization and properties->Contextual software domains->Virtual worlds software->Interactive games</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002951.10003227.10003251.10003258</concept_id>
				<concept_desc>CCS->Information systems->Information systems applications->Multimedia information systems->Massively multiplayer online games</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P3737288</person_id>
				<author_profile_id><![CDATA[81504688717]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Aidan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hanly]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Waterloo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[ahanly@uwaterloo.ca]]></email_address>
			</au>
			<au>
				<person_id>P3737289</person_id>
				<author_profile_id><![CDATA[81504687558]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[James]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Swidersky]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Waterloo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[jnswider@uwaterloo.ca]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Games, E., 2012. Unreal development kit. "http://udk.com/".]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Tarja, S., Johannesson, M., and Backlund, P. 2007. Serious games an overview. Tech. Rep. HS-IKI-TR-07-001, School of Humanities and Informatics - University of Sk&#246;vde, Sweden.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 A Simulation Game for Line Memorization Aidan Hanly and James Swidersky ahanly@uwaterloo.ca, jnswider@uwaterloo.ca 
University of Waterloo Figure 1: Two Screenshots of the Game CR Categories: I.2.1 [Arti.cial Intelligence]: 
Applications and Expert Systems Games I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism 
Virtual Reality; Keywords: serious games, simulation games, education, training 1 Introduction Line memorization 
is a key aspect of the performing arts. Ideally, actors will practice their lines together, as this most 
closely em­ulates the actual performance, and is the best way to learn lines. However, circumstances 
often arise the force actors to learn lines on their own. This is a much more ineffective way of memoriz­ing 
lines, and existing techniques to help with this usually involve memorization techniques, or signi.cant 
extra work on the actor s part. This project aims to solve this problem by utilizing game technology 
to assist with line memorization. While games are tra­ditionally a medium for entertainment, there is 
a rising trend in se­rious games , or games which are used to help their players in the real world [Tarja 
et al. 2007]. This game falls under that category, and is (as far as we know) the .rst game to speci.cally 
address the problem of line memorization. 2 Approach When the game begins, the player selects a character 
and scene to practice. The game loads the model of the set for the scene, and provides non-player characters 
(NPCs) to stand in for all other characters. The player starts the scene, and the NPCs start speaking. 
When the player needs to say a line, they input it via text or speech. This input is compared word-for-word 
to the stored script. If the player s line was correct, the NPCs and scene proceeds as normal. If the 
line was incorrect, the Director NPC intervenes. This NPC will point out the player s error, then provide 
a hint as to the correct line. The player can also manually call the director for help, via a button 
in the game. As the player requests more and more help from the director, the director will get angrier, 
and his responses will change to re.ect this mood shift. Once the scene is complete, the player is given 
a summary of his/her performance. This sum­mary shows what percentage of the lines the player got correct, 
and how many times the director had to provide assistance to the player. If there were too many incorrect 
lines, or the director had to provide too much assistance, the player fails the simulation. The game 
is implemented via Epic Games Unreal Development Kit [Games 2012], with custom code to handle the NPC 
dialogue and actions. Rather than have the NPCs use .xed, predetermined lines, they have been designed 
to act like chat robots, in that they alter their responses according to the player s input. This approach 
is an effective aid in line memorization due to the en­gaging experience it creates. By implementing 
line memorization in a game format, it ceases to be a task requiring completion, and instead becomes 
an enjoyable and replay-able activity. Memoriza­tion is a byproduct of the experience, rather than the 
end goal. The engagement is enhanced by the speech and responses of the NPCs (as opposed to having the 
player respond solely to text prompts). 3 Effectiveness and Future Work While the effectiveness of the 
game has not been tested, it is some­thing we hope to do in the near future. Test subjects would be pro­vided 
a script and role, and would attempt to learn their lines using the game. They would then be asked to 
recite their lines, and the results would be compared against a control group who did not use the game. 
Such tests would also allow us to get user feedback on the successes and failures of the game. There 
are a number of different sources of future work to be done on this game. Currently, the available practice 
scenes are hard-coded into the game. Ideally, the user would be able to upload whatever script they desire 
and, in addition to selecting a scene, would also select which set to practice the scene in. We would 
also like to improve the quality of the feedback the player receives, giving spe­ci.c advice as to lines 
which need improvement, and how best to improve them. Lastly, we would like to further increase player 
im­mersion by improving the quality of the character models and sets, and by using a motion capture device 
(like Microsoft s Kinect) to let the player act out the scenes, instead of just saying their lines. 
References GAMES, E., 2012. Unreal development kit. "http://udk. com/". TARJA, S., JOHANNESSON, M., AND 
BACKLUND, P. 2007. Seri­ous games an overview. Tech. Rep. HS-IKI-TR-07-001, School of Humanities and 
Informatics -University of Sk¨ovde, Sweden. Copyright is held by the author / owner(s). SIGGRAPH 2012, 
Los Angeles, California, August 5 9, 2012. ISBN 978-1-4503-1435-0/12/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2342938</article_id>
		<sort_key>420</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>32</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Lifelike interactive characters with behavior trees for social territorial intelligence]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2342896.2342938</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2342938</url>
		<abstract>
			<par><![CDATA[<p>State-of-the-art technology allows for photo-realistic graphics but this is not always enough. The gaming industry is slowly evolving the art of story telling but no matter how compelling the graphics or thrilling a story, awkward character behavior often breaks player immersion. In previous seminal work [Pedica and H. Vilhj&#225;lmsson 2010] we showed how the social theories of human territoriality and face-to-face interaction can serve as a solid base to model reactions expected by users when interacting with virtual characters.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3737290</person_id>
				<author_profile_id><![CDATA[81384592940]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Claudio]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Pedica]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Reykjavik University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[claudio@iiim.is]]></email_address>
			</au>
			<au>
				<person_id>P3737291</person_id>
				<author_profile_id><![CDATA[81488665852]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Hannes]]></first_name>
				<middle_name><![CDATA[H&#246;gni]]></middle_name>
				<last_name><![CDATA[Vilhj&#225;lmsson]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Reykjavik University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[hannes@ru.is]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1836582</ref_obj_id>
				<ref_obj_pid>1836576</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Pedica, C., and H. Vilhj&#225;lmsson, H. 2010. Spontaneous avatar behavior for human territoriality. <i>Applied Artificial Intelligence 24</i>, 6, 575--593.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Lifelike Interactive Characters with Behavior Trees for Social Territorial Intelligence Claudio Pedica*ogni 
Vilhj´ , Hannes H¨almsson Icelandic Institute for Intelligent Machines and CADIA, Reykjavik University 
 Figure 1: The social group dynamics simulation in four steps. (a) Someone walking by is noticed when 
stepping inside the territory. Through common attention, somebody looks back after watching a member 
looking at the passerby. (b) The passerby is engaged and invited to join. Other members notice the salutation. 
(c) The newcomer is welcomed with a short glance. (d) The group opens up to make room for the new member 
that .nally joins. State-of-the-art technology allows for photo-realistic graphics but this is not always 
enough. The gaming industry is slowly evolving the art of story telling but no matter how compelling 
the graphics or thrilling a story, awkward character behavior often breaks player immersion. In previous 
seminal work [Pedica and H. Vilhj´ almsson 2010] we showed how the social theories of human territoriality 
and face-to-face interaction can serve as a solid base to model reactions expected by users when interacting 
with virtual characters. quest demands a certain action such as look there , move here , play an animation 
, etc, without actually implementing it. Some BTs implementations stop the decision logic after an action 
has been selected while in ours multiple decision branches can run si­multaneously, each leading to a 
different action request. After gen­eration, action requests are gathered into groups and each group 
blended through an arbitration strategy which resolves potential con.icts. The result is a set of .nal 
combined requests forming the attributes of what we call a motivation. A motivation models the psychological 
drive to react and results in a, compound collec­tion of motion requirements to be issued to the actuation 
layer for action execution.  Figure 2: The architecture in a nutshell. (a) Behaviors generate different 
types of action requests to control different bodily parts. (b) Action requests are gathered in groups 
of same type, combined, and packed into a motivation. (c) The motivation is sent to an actu­ation interface 
for action rendition. We have now integrated our reactive approach for social territori­ality with Behavior 
Trees (BTs), an emerging game A.I. technique that is fast becoming a standard in the industry. This integration 
led to a variant of BTs where multiple branches can run simul­taneously and blend. A middle-layer of 
custom-made arbitration strategies performs the blending before actuation, resembling com­mand fusion 
architectures. We also gave behavior nodes a priority. High priority behavior branches can subsume lower 
priority ones to respond immediately to critical contingencies, akin to subsump­tion architectures. The 
resulting behavior achieves responsiveness, smoothness and continuity of motion when the decision logic 
si­multaneously controls where to look, where to stand, how to orient the body and what animation to 
play. In our variant of BTs, the leaf nodes generate action requests. A re­ *e-mail:claudio@iiim.is e-mail:hannes@ru.is 
Figure 3: Territoriality in an interactive scenario. The user con­trols the red character that can join 
or leave groups. Generally, different branches of the whole tree may pursue con.ict­ing goals. What if 
a branch demands keeping proper body posture and position to show awareness of the ongoing social interaction 
while another wants to keep looking at something really important happening elsewhere? How can priorities 
be handled? We resolve this with behavior tree subsumption. Every behavior node has a pri­ority and, 
if given an action request, it will suppress the execution of lower priority nodes. Subsumption helped 
organizing complex BTs in horizontal layers of goals at different levels of abstraction. Using this variant 
of BTs, our territorial behavior for conversation group dynamics [2010] became a parallel composition 
of nodes to keep personal distance, equality, cohesion and common attention. The resulting social animation 
system achieves new levels of realism of behavior as well as being easier to extend and reuse. References 
PEDICA, C., AND H. VILHJ ALMSSON´ , H. 2010. Spontaneous avatar behavior for human territoriality. Applied 
Arti.cial Intel­ligence 24, 6, 575 593. Copyright is held by the author / owner(s). SIGGRAPH 2012, Los 
Angeles, California, August 5 9, 2012. ISBN 978-1-4503-1435-0/12/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2342939</article_id>
		<sort_key>430</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>33</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Texture-size-independent address translation for virtual texturing]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2342896.2342939</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2342939</url>
		<abstract>
			<par><![CDATA[<p>Virtual texturing (VT) is a promising technique to increase texture resolution and uniqueness in real-time applications such as GIS and games. VT manages large texture data sets by splitting the texture data into smaller tiles and assigning a unique address to every tile. Only the visible subset of these tiles is then kept in graphics memory. Any newly-visible tiles are loaded on-demand from disc. Several recent games such as id Software's Rage and Splash Damage's Brink have used VT to manage their texture data at run-time.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3737292</person_id>
				<author_profile_id><![CDATA[81309505302]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Charles-Frederik]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hollemeersch]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Ghent University - IBBT]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[charles.hollemeersch@elis.ugent.be]]></email_address>
			</au>
			<au>
				<person_id>P3737293</person_id>
				<author_profile_id><![CDATA[81331501981]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Bart]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Pieters]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Ghent University - IBBT]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737294</person_id>
				<author_profile_id><![CDATA[81474694185]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Aljosha]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Demeulemeester]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Ghent University - IBBT]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737295</person_id>
				<author_profile_id><![CDATA[81413597099]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Peter]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lambert]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Ghent University - IBBT]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737296</person_id>
				<author_profile_id><![CDATA[81319502984]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Rik]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Van de Walle]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Ghent University - IBBT]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Hollemeersch, C., Pieters, B., Lambert, P., and Van de Walle, R. 2010. Accelerating virtual texturing using cuda. In <i>Gpu Pro: Advanced Rendering Techniques</i>. ch. 10.2, 623--641.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Sugden, B., and Iwanicki, M. 2011. Mega meshes: Modelling, rendering and lighting a world made of 100 billion polygons. In <i>Game Developers Conference 2011</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[van Waveren, J.-P. 2009. id tech 5 challenges. In <i>SIGGRAPH 2009 Beyond Programmable Shading Course Notes</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Texture-size-independent address translation for virtual texturing Charles-Frederik Hollemeersch* Bart 
Pieters Aljosha Demeulemeester Peter Lambert Rik Van de Walle Ghent University -IBBT ELIS -Multimedia 
Lab 1 Introduction Virtual texturing (VT) is a promising technique to increase texture resolution and 
uniqueness in real-time applications such as GIS and games. VT manages large texture data sets by splitting 
the texture data into smaller tiles and assigning a unique address to every tile. Only the visible subset 
of these tiles is then kept in graphics mem­ory. Any newly-visible tiles are loaded on-demand from disc. 
Sev­eral recent games such as id Software s Rage and Splash Damage s Brink have used VT to manage their 
texture data at run-time. We .rst brie.y describe how the render is traditionally implemented for VT 
[van Waveren 2009] [Sugden and Iwanicki 2011] [Holle­ meersch et al. 2010]. When sampling the VT in the 
shader, we .rst calculate the tile address based on the texture coordinates. This tile address is then 
used to do a look up into the translation lookup table (TLT). The TLT is itself a texture that contains 
a texel for every tile in the virtual address space. The contents of the TLT s texels give us the location 
of the tile in a second texture containing the cur­rently loaded tiles. The .nal sample color value is 
then determined by a lookup into this cache texture. One of the main promises of virtual texturing is 
that the run-time re­source demands are no longer relative to the whole dataset size but instead become 
relative to the screen size. While currently exist­ing practical implementations largely ful.ll this 
promise, the TLT is one key step which is still dependent on the texture dataset size. In all the aforementioned 
implementations, a single entry is needed in the TLT per virtual tile address. When using very large 
virtual address spaces this step quickly becomes a bottleneck since both generation and memory use of 
the TLT become prohibitively ex­pensive. For example for a 106 ×106 texture with 128×128 pages, the TLT 
alone already uses 341 megabytes of data. This is in the order of the cache size needed for HD screen 
resolutions and could thus be better spend on caching more data. In this work, we present a new approach 
to eliminate the TLT. In­stead of having a data structure that contains an entry for every vir­tual page, 
we propose a system that allows ef.ciently querying if a page is available in the cache. The rest of 
the virtual texturing pro­cess, streaming, caching, and the .nal look-up in the cache tile are largely 
unmodi.ed. We will show that our method is suf.ciently fast for real-time use on current-generation graphics 
hardware. 2 The Cache Query Tree There are several ways to ef.ciently implement querying if an item 
is available in a list. Caches and balanced search trees are two com­mon ways to approach this problem. 
However, when implemented on a GPU, such systems reduce ef.ciency due to lots of diverging branches on 
the massively-parallel architecture of the GPU. Since we know the maximum number of items up front (i.e. 
the number of tiles that .t in the cache) we can easily use a sorted *charles.hollemeersch@elis.ugent.be 
The research activities that have been described in this work were funded by Ghent University, the Interdisciplinary 
Institute for Broadband Technology (IBBT), the Institute for the Promotion of Innovation by Sci­ence 
and Technology in Flanders (IWT), the Fund for Scienti.c Research-Flanders (FWOFlanders), and the European 
Union. list instead. This list can then ef.ciently be searched with a binary search in a .xed and constant 
number of steps. This search can also easily be implemented without diverging code paths in the pixel 
shader threads. In fact, our system can be implemented without any branching at all making it amenable 
to platforms which do not sup­port dynamic branching such as the WebGL standard or embedded hardware. 
Practically, our system consists of two data structures. The .rst is a search table that contains the 
tile addresses of the tiles present in the cache. This list is sorted by increasing tile addresses. The 
second data structure is a list where the i th element contains the cache address of the i th element 
in the search table. When T is the opaque address of the tile to .nd and N is the number of cache items 
we can .nd the item as follows: C = unavailable Min =1 Max = N for i := 1 to log(N) step 1 do Mid =(Min 
+ Max)/2 id = tex1d(searchT ab, Mid) if id == T then C = tex1d(searchT ab, Mid) .; if T >id then Min 
:= Mid + 1; else Max := Mid - 1; . od Note that in practice these two tables are stored as a N×2 texture. 
Since log(N), is a constant the loop can easily be unrolled and im­plemented without branching, using 
only a single texture read. Its up to the application to handle the situation where a page is not available 
in the cache. For example, in our application we currently do another search for a lower resolution version 
of the data on the next mipmap level. Note that only about 1% of the pixels need to follow this path 
since unavailable tiles will be streamed in by the cache manager. 3 Results We have implemented the 
proposed method in our virtual texturing system. Our updated system now supports almost arbitrary texture 
address spaces. Most importantly, run-time performance and mem­ory use are only dependent on the screen 
resolution. We did not notice any signi.cant performance degradation compared to our traditional VT implementation 
since our system bottleneck is not the VT shader. References HOLLEMEERSCH, C., PIETERS, B., LAMBERT, 
P., AND VAN DE WALLE, R. 2010. Accelerating virtual texturing using cuda. In Gpu Pro: Advanced Rendering 
Techniques. ch. 10.2, 623 641. SUGDEN, B., AND IWANICKI,M.2011.Megameshes:Modelling, rendering and lighting 
a world made of 100 billion polygons. In Game Developers Conference 2011. VAN WAVEREN, J.-P. 2009. id 
tech 5 challenges. In SIGGRAPH 2009 Beyond Programmable Shading Course Notes. Copyright is held by the 
author / owner(s). SIGGRAPH 2012, Los Angeles, California, August 5 9, 2012. ISBN 978-1-4503-1435-0/12/0008 
  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	</section>
	<section>
		<section_id>2342940</section_id>
		<sort_key>440</sort_key>
		<section_seq_no>11</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Hardware]]></section_title>
		<section_page_from>11</section_page_from>
	<article_rec>
		<article_id>2342941</article_id>
		<sort_key>450</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>34</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[8D display]]></title>
		<subtitle><![CDATA[a relightable glasses-free 3D display]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2342896.2342941</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2342941</url>
		<abstract>
			<par><![CDATA[<p>Imagine a display that behaves like a window. Glancing through it, viewers perceive a virtual 3D scene with correct parallax, without the need to wear glasses or track the user. Light that passes through the display correctly illuminates the virtual scene. While researchers have considered such displays, or prototyped subsets of these capabilities, we contribute a new, interactive, relightable, glasses-free 3D display. By simultaneously capturing a 4D light field, and displaying a 4D light field, we are able to realistically modulate the incident light on rendered content. We present our optical design, and GPU pipeline. Beyond mimicking the physical appearance of objects under natural lighting, an 8D display can create arbitrary directional illumination patterns and record their interaction with physical objects. Our hardware points the way towards novel 3D interfaces, in which users interact with digital content using light widgets, physical objects, and gesture.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3737297</person_id>
				<author_profile_id><![CDATA[81442601918]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Matthew]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hirsch]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MIT Media Lab]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737298</person_id>
				<author_profile_id><![CDATA[81504684647]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Shahram]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Izadi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Microsoft Research, UK]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737299</person_id>
				<author_profile_id><![CDATA[81435601065]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Henry]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Holtzman]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MIT Media Lab]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737300</person_id>
				<author_profile_id><![CDATA[81100022847]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Ramesh]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Raskar]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MIT Media Lab]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1360657</ref_obj_id>
				<ref_obj_pid>1360612</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Fuchs, M., Raskar, R., Seidel, H.-P., and Lensch, H. P. A. 2008. Towards passive 6d reflectance field displays. <i>ACM Trans. Graph. 27</i>, 3 (Aug.), 58:1--58:8.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1618505</ref_obj_id>
				<ref_obj_pid>1618452</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Hirsch, M., Lanman, D., Holtzman, H., and Raskar, R. 2009. Bidi screen: a thin, depth-sensing lcd for 3d interaction using light fields. <i>ACM Trans. Graph. 28</i>, 5 (Dec.), 159:1--159:9.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Hullin, M. B., Lensch, H. P. A., Raskar, R., Seidel, H.-P., and Ihrke, I. 2011. Dynamic display of brdfs. <i>Computer Graphics Forum 30</i>, 2, 475--483.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1027414</ref_obj_id>
				<ref_obj_pid>1027411</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Nayar, S. K., Belhumeur, P. N., and Boult, T. E. 2004. Lighting sensitive display. <i>ACM Trans. Graph. 23</i>, 4 (Oct.), 963--979.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 8D Display: A Relightable Glasses-Free 3D Display Matthew Hirsch1 Shahram Izadi2 Henry Holtzman1 Ramesh 
Raskar1 1 MIT Media Lab 2 Microsoft Research, UK Figure 1: (Left) Photograph of light .eld view under 
different incident illumination (Left, Inset). (Center) Captured and displayed light .elds. (Right) Our 
projector-camera-based 8D Display prototype is ammenable to implementation using Sensor-In-Pixel LCD 
hardware. Abstract Imagine a display that behaves like a window. Glancing through it, viewers perceive 
a virtual 3D scene with correct paral­lax, without the need to wear glasses or track the user. Light 
that passes through the display correctly illuminates the virtual scene. While researchers have considered 
such displays, or prototyped subsets of these capabilities, we contribute a new, interactive, re­lightable, 
glasses-free 3D display. By simultaneously capturing a 4D light .eld, and displaying a 4D light .eld, 
we are able to realis­tically modulate the incident light on rendered content. We present our optical 
design, and GPU pipeline. Beyond mimicking the phys­ical appearance of objects under natural lighting, 
an 8D display can create arbitrary directional illumination patterns and record their interaction with 
physical objects. Our hardware points the way to­wards novel 3D interfaces, in which users interact with 
digital con­tent using light widgets, physical objects, and gesture. Introduction Our 8D display prototype 
is a general purpose light transducer. With limitations, it is capable of mapping 4D light­ing between 
the physical environment in front of the display and the digital environment behind it. Nayar et al. 
[2004] create a lighting sensitive display, though it cannot accurately map shad­ows and specularities. 
BRDF displays can simulate .at surfaces with a particular Bi-Directional Re.ectance Distribution Function 
[Hullin et al. 2011]. 6D displays that demonstrate 4D relighting of 2D images have been shown in both 
active [Hirsch et al. 2009] and passive [Fuchs et al. 2008] modes. Our work contributes a real­time 8D 
display, composed of off-the-shelf optical elements and a new GPU rendering and capture pipeline, to 
make simultaneous real-time 4D lighting and 4D capture feasible for the .rst time. 8-Dimensional Rendering 
To achieve real-time rendering and decoding of light .elds we have developed a new 8D GPU pipeline. We 
implement diffuse lighting in our prototype, which can be ef­.ciently mapped to projective texture lookups. 
We implement a simpli.ed version of the rendering equation, neglecting BRDFs. Z '' ' Lo(x,.)= Li(x,.)(-.· 
n)d.(1) O where Li is the measured incident light .eld, Lo the displayed light .eld, and .' the incoming 
lighting direction. Though in our model local regions are invariant in outgoing light direction, ., each 
light .eld view is generated with a view matrix corresponding to a virtual skewed orthographic camera 
viewing the scene from .. To capture a 4D light .eld we deinterlace images recorded from the back of 
the lens array in our GPU pipeline. Optics and Implementation A hexagonal lens sheet (Fresnel Tech. #360), 
is placed atop a high-resolution sensor and display panel, composed of a 2048 × 2048 Point Gray Gazelle 
camera and 1080P Optoma projector, respectively. We achieve a 150mm, 325dpi display by modifying our 
projector lens. The camera and projector are focused on a diffuser placed behind the lens sheet. A beam 
splitter system is used to overlap the projector and camera views (Figure 1). We prevent cross-talk between 
the camera and projector by multiplexing through crossed linear polarizers. The projector-camera system 
introduces unwanted calibration complex­ities, which are addressed in supplementary material. In all 
cases, an angle limiting material, such as privacy .lm, can be included in the optical system to prevent 
angular aliasing. Results and Discussion The included video demonstrates our display simultaneously showing 
realistic horizontal and vertical parallax, without tracking or glasses, as well as relighting effects. 
A 3D model can be illuminated and shadowed by various light sources. Abundant interactive possibilities 
include: using an off­the-shelf light source as 6DoF input controller, direct manipula­tion of physical 
lights to cause relighting of 3D scenes, accu­rately mimicking surfaces with exotic BRDFs, and applying 
non­realistic physics to real lighting sources. Furthermore, our design is amenable to implementation 
on emerging Sensor-In-Pixel LCD technology, detailed in our supplement. Placing an appropriately designed 
lens sheet on such a panel will achieve an optically analo­gous system, paving the way for thin 8D displays. 
References FUCHS, M., RASKAR, R., SEIDEL, H.-P., AND LENSCH, H. P. A. 2008. Towards passive 6d re.ectance 
.eld displays. ACM Trans. Graph. 27, 3 (Aug.), 58:1 58:8. HIRSCH, M., LANMAN, D., HOLTZMAN, H., AND RASKAR, 
R. 2009. Bidi screen: a thin, depth-sensing lcd for 3d interaction using light .elds. ACM Trans. Graph. 
28, 5 (Dec.), 159:1 159:9. HULLIN, M. B., LENSCH, H. P. A., RASKAR, R., SEIDEL, H.-P., AND IHRKE, I. 
2011. Dynamic display of brdfs. Computer Graphics Forum 30, 2, 475 483. NAYAR, S. K., BELHUMEUR, P. N., 
AND BOULT, T. E. 2004. Lighting sensitive display. ACM Trans. Graph. 23, 4 (Oct.), 963 979. Copyright 
is held by the author / owner(s). SIGGRAPH 2012, Los Angeles, California, August 5 9, 2012. ISBN 978-1-4503-1435-0/12/0008 
  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2342942</article_id>
		<sort_key>460</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>35</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[A cell phone based platform for facial performance capture]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2342896.2342942</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2342942</url>
		<abstract>
			<par><![CDATA[<p>We propose a new head-mounted camera system based on stereo cell phone cameras. These cameras have the advantage of being extremely small, light-weight, and programmable. We provide step-by-step details on how to recreate this apparatus and also how to apply this data to multiple applications in facial tracking and reconstruction. Our system is based on the LG Thrill, a 3D enabled cell phone that provides two synchronized stereo cameras in a tiny 4.2 gram module. We use two phones for a total of four cameras. However we do not want to mount the entire phone at the end of the helmet arm. Instead we designed a custom umbilical cord that allows the camera module to function at a large distance from the phone itself.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3737301</person_id>
				<author_profile_id><![CDATA[81504688287]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Svetlana]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Akim]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[USC, Institute for Creative Technologies]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737302</person_id>
				<author_profile_id><![CDATA[81100556708]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Andrew]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Jones]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[USC, Institute for Creative Technologies]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[jones@ict.usc.edu]]></email_address>
			</au>
			<au>
				<person_id>P3737303</person_id>
				<author_profile_id><![CDATA[81440613130]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Xueming]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[USC, Institute for Creative Technologies]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737304</person_id>
				<author_profile_id><![CDATA[81440600975]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Jay]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Busch]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[USC, Institute for Creative Technologies]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737305</person_id>
				<author_profile_id><![CDATA[81421597317]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Graham]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Fyffe]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[USC, Institute for Creative Technologies]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737306</person_id>
				<author_profile_id><![CDATA[81504686804]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Paul]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Graham]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[USC, Institute for Creative Technologies]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737307</person_id>
				<author_profile_id><![CDATA[81100086933]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>7</seq_no>
				<first_name><![CDATA[Paul]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Debevec]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[USC, Institute for Creative Technologies]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1778766</ref_obj_id>
				<ref_obj_pid>1778765</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Adams, A., Talvala, E.-V., Park, S. H., Jacobs, D. E., Ajdin, B., Gelfand, N., Dolson, J., Vaquero, D., Baek, J., Tico, M., Lensch, H. P. A., Matusik, W., Pulli, K., Horowitz, M., and Levoy, M. 2010. The frankencamera: An experimental platform for computational photography. <i>ACM Transactions on Graphics 29</i>, 4 (July), 29:1--29:12.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1778777</ref_obj_id>
				<ref_obj_pid>1778765</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Beeler, T., Bickel, B., Beardsley, P., Sumner, B., and Gross, M. 2010. High-quality single-shot capture of facial geometry. <i>ACM Transactions on Graphics 29</i>, 4 (July), 40:1--40:9.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 A Cell Phone Based Platform for Facial Performance Capture Svetlana Akim Andrew Jones Xueming Yu Jay 
Busch Graham Fyffe Paul Graham Paul Debevec USC, Institute for Creative Technologies jones@ict.usc.edu 
 Figure 1: (left) Photograph of apparatus and calibration object. (right) Photographs from our head-mounted 
cameras We propose a new head-mounted camera system based on stereo cell phone cameras. These cameras 
have the advantage of being ex­tremely small, light-weight, and programmable. We provide step­by-step 
details on how to recreate this apparatus and also how to apply this data to multiple applications in 
facial tracking and recon­struction. Our system is based on the LG Thrill, a 3D enabled cell phone that 
provides two synchronized stereo cameras in a tiny 4.2 gram module. We use two phones for a total of 
four cameras. However we do not want to mount the entire phone at the end of the helmet arm. Instead 
we designed a custom umbilical cord that allows the camera module to function at a large distance from 
the phone itself. The LG Thrill s dual 5MP camera module is plugged into the mother board through a 40-pin 
connector, of which 7 pins are used to supply power of different voltage, 10 pins are paired into 5 differ­ential 
signals to transmit high speed digital data and clock, whereas others are dedicated for low speed control 
signals or ground. Power channels and high speed digital signals must be handled with care when an extension 
cable is designed to connect the camera module from a certain distance away from the smart phone body. 
 Figure 2: (Left) Extension PCB layout (Right) PCB schematics In a high-speed environment, the digital 
circuit alternates between high and low logic at a very high rate. In order for the device to change 
state, a large output current must always be available from the power line in a very short period of 
time. This current is typ­ically provided by a bypsss capacitor, placed between the power line and ground. 
When the camera module is extended away from the mother board, the distance from the the bypass capacitors 
also increases. The longer the extension cable is, the more it will limit the surging current from the 
bypass capacitors. In order for the extended camera to work even at a long distance away from the mother 
board, we place new bypass capacitors as close as possi­ble to the camera module. Secondly, in digital 
circuitry, everything happens in discrete increments of time. Any signi.cant delay be­tween clock and 
data, or between the two polarized channels of a differential pairs is mostly like to cause error. Especially 
in the high-speed environment, since the signal travel as a very high rate, even a micrometer difference 
in the circuit trace will result in de­lay. In designing the circuit board it is critical to tune each 
trace so that they match up to a common length, even after additional ca­pacitors have been added (Fig 
2). Extension cable and circuit board designed with considerations of the above two factors work as far 
as 60cm long, and may worker for longer distances though this has not been tested yet. Without additional 
capacitors it is only possible to extend the camera 10cm. In keeping with a point and shoot philosophy 
cell phone are typ­ically designed to automate exposure, focus, color balance, and stereo convergence. 
We developed a custom camera application that uses the LG Real3D SDK to lock the convergence and Android 
SDK to set focus and color balance. In the future, lower level hard­ware control may be possible as with 
the Frankencamera SDK for Nokia phones [Adams et al. 2010]. Many 3D computer vision algorithms require 
accurate camera cal­ibration. We developed a new single-shot calibration process using a 6 cylinder covered 
with a 2cm grid of black and white square (Figure 1). The cylinder s checkerboard corners can be detected 
quickly and automatically. Unlike techniques that rely on planar or spherical [Beeler et al. 2010] calibration 
objects, a cylinder pro­ vides points at multiple depths and more closely approximates the shape of a 
human face. We are currently working on using this multiview data for stereo reconstruction and facial 
tracking. The cell phone also opens up the possibility of previewing video over the network and onboard 
image processing. References ADAMS, A., TALVALA, E.-V., PARK, S. H., JACOBS, D. E., AJDIN, B., GELFAND, 
N., DOLSON, J., VAQUERO, D., BAEK, J., TICO, M., LENSCH, H. P. A., MATUSIK, W., PULLI, K., HOROWITZ, 
M., AND LEVOY, M. 2010. The frankencamera: An experimental platform for computational photography. ACM 
Transactions on Graphics 29, 4 (July), 29:1 29:12. BEELER, T., BICKEL, B., BEARDSLEY, P., SUMNER, B., 
AND GROSS, M. 2010. High-quality single-shot capture of facial geometry. ACM Transactions on Graphics 
29, 4 (July), 40:1 40:9. Copyright is held by the author / owner(s). SIGGRAPH 2012, Los Angeles, California, 
August 5 9, 2012. ISBN 978-1-4503-1435-0/12/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	</section>
	<section>
		<section_id>2342943</section_id>
		<sort_key>470</sort_key>
		<section_seq_no>12</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Animation]]></section_title>
		<section_page_from>12</section_page_from>
	<article_rec>
		<article_id>2342944</article_id>
		<sort_key>480</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>36</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Stop-motion cameras in the network]]></title>
		<subtitle><![CDATA[connected multi-cameras for the collaboration work in stop-motion]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2342896.2342944</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2342944</url>
		<abstract>
			<par><![CDATA[<p>There are several systems and researches for the stop-motion cameras[1]. We focused on the collaboration work and new expression in the stop-motion movie. We developed four system modules for the stop-motion cameras which are connected each other on the network. It enables users to capture, to share and to adjust images simultaneously between several cameras on the same network(figure5).</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3737308</person_id>
				<author_profile_id><![CDATA[81466641621]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Yoichi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ochiai]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[the University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[ochyai@me.com]]></email_address>
			</au>
			<au>
				<person_id>P3737309</person_id>
				<author_profile_id><![CDATA[81482649155]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Keisuke]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Toyoshima]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Tsukuba]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[toyoc@me.com]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Stephen A. Kallis, Jr., Computer Animation Techniques In: Journal of the SMPTE, 1971]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[USPTO, trademark serial number #78285661]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Stop-motion cameras in the network: connected multi-cameras for the collaboration work in stop-motion 
 Yoichi Ochiai* and Keisuke Toyoshima***the University of Tokyo *University of Tsukuba Figure 1 Figure 
2 Figure 3 Figure 4 Figure 1, Figure2: 3D gif animation on single image (separated to two images) Figure 
3: use scene Figure 4: our system s interfaces 1. Introduction There are several systems and researches 
for the stop­motion cameras[1]. We focused on the collaboration work and new expression in the stop-motion 
movie. We developed four system modules for the stop-motion cameras which are connected each other on 
the network . It enables users to capture, to share and to adjust imagessimultaneously between several 
cameras on the samenetwork(.gure5). Then using the onionskin and exchanging the imagesbetween different 
cameras supports users to take 3Danimation and multi-angle stop-motion movies easily. 2. Implementation 
We developed four system modules on the iOS and itworks on several hardware based on the iOS. First, 
we developed the 3D onionskin system for multi­cameras. If our system captures the image, it send to 
theother cameras in the same network. If the user pressed the onionskin button(.gure4 up), the images 
start blink­ing and system shows camera view and the image thattaken in the other cameras alternately. 
This function makes 3D gif animation from these two images(.gure1). cameras are connected and images 
are captured simultaneously Figure 5: (left) 3D gif animation on single image. This system supports users 
to take the 3D photography easily. Secondly, we developed the multi-shutter system. If our system captures 
the image, it transmits capturing signal to the other cameras and they capture the imagesimultaneously. 
This system enables users to make the3D gif animation and bullet time images like the Matrix movie[2] 
easily. In addition, we developed time lapsecapturing system in this application. With the time lapsesystem 
and multi shutter system, users can take the .xed 3D movie using two cameras. Third we developed multi 
spirit level system(.gure4). Each camera has a mark in the center of the camera view that shows the difference 
of the angle between itself and the other system. This system supports users to set the same angle easily. 
Fourth, this system has the animation system with multi-gallery. This system supports to make a stop­motion 
movie by the images from several galleries of several cameras. These four systems are merged in one application 
and user could use system to make the new expression in stop-motion movie.  3. Future Work Stop-motion 
movie s metaphor is used in a lot of areassuch as life log(time lapse) and 3D viewer(rotation) in the 
web. Using this system, users can make the thesecontents easily. Adding to that, collaboration in the 
work would has a chance to give new expression in stop­motion movies. For the next work of this concept, 
we aregoing to develop the location detecting system for each cameras and automatically correct the camera 
angle sys­tem on the location system. These system enable users to edit the stop motion movie with the 
data of angle, posi­tion and camera in collaboration. REFERENCES [1] Stephen A. Kallis, Jr., Computer 
Animation Techniques In: Journal of the SMPTE, 1971 [2] USPTO, trademark serial number #78285661  Copyright 
is held by the author / owner(s). *email: ochyai@me.comSIGGRAPH 2012, Los Angeles, California, August 
5 9, 2012.**email: toyoc@me.com ISBN 978-1-4503-1435-0/12/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	</section>
	<section>
		<section_id>2342945</section_id>
		<sort_key>490</sort_key>
		<section_seq_no>13</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Hardware]]></section_title>
		<section_page_from>13</section_page_from>
	<article_rec>
		<article_id>2342946</article_id>
		<sort_key>500</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>37</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[CoDAC]]></title>
		<subtitle><![CDATA[compressive depth acquisition using a single time-resolved sensor]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2342896.2342946</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2342946</url>
		<abstract>
			<par><![CDATA[<p>CoDAC is a method for compressive acquisition of scene depth with high spatial and range resolution using a single, omnidirectional, time-resolved photodetector and no scanning components [Kirmani et al.]. Light detection and ranging (LIDAR) systems use time of flight (TOF) in combination with raster scanning of the scene to form depth maps, and TOF cameras instead make TOF measurements in parallel by using an array of sensors. Moreover, existing depth sensing technologies do not use the high compressibility of scene depth to reduce acquisition costs. Here, we present a framework for compressive depth map acquisition using neither raster scanning by the illumination source nor an array of sensors.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3737310</person_id>
				<author_profile_id><![CDATA[81466648528]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Andrea]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Cola&#231;o]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Massachusetts Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[acolaco@mit.edu]]></email_address>
			</au>
			<au>
				<person_id>P3737311</person_id>
				<author_profile_id><![CDATA[81488667605]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Ahmed]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kirmani]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Massachusetts Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737312</person_id>
				<author_profile_id><![CDATA[81542012156]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Franco]]></first_name>
				<middle_name><![CDATA[N. C.]]></middle_name>
				<last_name><![CDATA[Wong]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Massachusetts Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737313</person_id>
				<author_profile_id><![CDATA[81504684911]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Vivek]]></first_name>
				<middle_name><![CDATA[K.]]></middle_name>
				<last_name><![CDATA[Goyal]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Massachusetts Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Kirmani, A., Cola&#231;o, A., Wong, F., and Goyal, V. Exploiting sparsity in time-of-flight range acquisition using a single time-resolved sensor. <i>OSA Opt. Express</i> (Oct 2011).]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 CoDAC: Compressive Depth Acquisition using a Single Time-resolved Sensor Andrea Colac¸o *, Ahmed Kirmani, 
Franco N. C. Wong, Vivek K Goyal Massachusetts Institute of Technology Figure 1: CoDAC depth sensing 
architecture uses a spatial light modulator (SLM) to spatially pattern a temporally-modulated light source. 
In step 1, time-resolved measurements from a single omnidirectional sensor are processed using parametric 
signal processing to recover scene depth information. We estimate the three distinct depths in this fronto-parallel 
scene with sub-centimeter resolution even with pulse width of 3ns (rise-time 0.7 ns). Spatial resolution 
was recovered through step 2 of the algorithm using only 205 SLM patterns. This corresponds to 5% of 
the total number of 4096 pixels in the depth map. 1 Introduction CoDAC is a method for compressive acquisition 
of scene depth with high spatial and range resolution using a single, omnidi­rectional, time-resolved 
photodetector and no scanning compo­nents [Kirmani et al. ]. Light detection and ranging (LIDAR) sys­ 
tems use time of .ight (TOF) in combination with raster scanning of the scene to form depth maps, and 
TOF cameras instead make TOF measurements in parallel by using an array of sensors. Moreover, existing 
depth sensing technologies do not use the high compress­ibility of scene depth to reduce acquisition 
costs. Here, we present a framework for compressive depth map acquisition using neither raster scanning 
by the illumination source nor an array of sensors. Our depth map reconstruction relies on parametric 
signal modeling of the impulse response of piecewise planar scenes. We use para­metric deconvolution 
to achieve much .ner depth resolution than dictated by the illumination pulse width and the detector 
bandwidth. Spatial resolution in our framework is rooted in patterned illumina­tion followed by decoupling 
the inverse problems of range estima­tion and spatial resolution recovery during computational process­ing. 
Spatial resolution equal to that of the spatial light modulator (SLM) is achieved despite using fewer 
SLM patterns than the num­ber of pixels in the SLM. Proof-of-concept experiments have veri­.ed the validity 
of our modeling, algorithms and improved spatial and range resolution over existing methods. CoDAC enables 
depth acquisition in a compact form factor, with signi.cantly-reduced hardware cost and complexity as 
compared to state-of-the-art LI-DAR systems and TOF cameras. 2 Modeling and Algorithmic Framework CoDAC 
has a two-step reconstruction procedure. Step 1, uses omnidirectional illumination or no spatial patterning, 
i.e., a fully­transparent SLM con.guration. Under the assumption that the scene is approximately piecewise 
planar, we have shown that the continuous-time light intensity signal at the single photodetector is 
well approximated by a piecewise linear signal [Kirmani et al. ]. For fronto-parallel scenes, this parametric 
signal is simply a series of short square pulses as shown in Figure 1. Estimation of the un­ derlying 
parametric signal P (t) implies recovery of the range or *email:acolaco@mit.edu depth content present 
in the scene. The use of a parametric sig­nal modeling and recovery framework enables us to achieve high 
depth resolution relative to the speed of the time sampling and the photodetector bandwidth. After depth 
identi.cation in step 1, the remaining problem is to .nd correspondences between spatial loca­tions and 
depths to form the depth map. Step 2, uses several pseudorandom binary patterns on the SLM. For each 
pattern, the received time-resolved signal is processed to yield amplitude data. Again for fronto-parallel 
scenes, the ampli­tude of each peak in the parametric signal is equal to the fraction of the scene at 
a particular depth that is illuminated by the pro­jected pattern. Since we assume that the scene is approximately 
piecewise planar, this translates to the Laplacian of the depth map being approximately sparse. We introduce 
a convex optimization problem [Kirmani et al. ] that .nds the depth map consistent with the measurements 
that approximately minimizes the number of nonzero entries in the Laplacian of the depth map. Solving 
this optimization problem yields the desired depth map. 3 Hardware Implementation We conducted proof-of-concept 
experiments to demonstrate the range and spatial resolution capabilities of the CoDAC framework. We used 
a pulsed laser source at 780nm and 70mW average power to illuminate the SLM with 64 × 64-pixel resolution. 
The re.ected light was collected at a Si PIN photodiode with rise-time of 0.7ns. The recovered depth 
maps for a fronto-parallel scene is shown in Figure 1 and for piecewise planar scenes is shown below. 
  References KIRMANI, A., COLAC¸ O, A., WONG, F., AND GOYAL, V. Ex­ploiting sparsity in time-of-.ight 
range acquisition using a single time-resolved sensor. OSA Opt. Express (Oct 2011). Copyright is held 
by the author / owner(s). SIGGRAPH 2012, Los Angeles, California, August 5 9, 2012. ISBN 978-1-4503-1435-0/12/0008 
 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2342947</article_id>
		<sort_key>510</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>38</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Development of a portable anisotropic reflectance measurement system for modeling and rendering of bidirectional texture functions]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2342896.2342947</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2342947</url>
		<abstract>
			<par><![CDATA[<p>Recently, Computer Graphics (CG) productions need the novel method to represent realistic CG objects. It is very difficult for their creators to set parameters of the reflection models that express correctly surface reflection of real objects.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3737314</person_id>
				<author_profile_id><![CDATA[81460644684]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Yuki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Takeda]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Dai Nippon Printing Co., Ltd.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[takeda-y13@mail.dnp.co.jp]]></email_address>
			</au>
			<au>
				<person_id>P3737315</person_id>
				<author_profile_id><![CDATA[81504685897]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Jiro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hara]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Ritsumeikan University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737316</person_id>
				<author_profile_id><![CDATA[81464673753]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Wataru]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wakita]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Ritsumeikan University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737317</person_id>
				<author_profile_id><![CDATA[81319500817]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Yoshiyuki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sakaguchi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Ritsumeikan University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737318</person_id>
				<author_profile_id><![CDATA[81100135138]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Hiromi]]></first_name>
				<middle_name><![CDATA[T.]]></middle_name>
				<last_name><![CDATA[Tanaka]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Ritsumeikan University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>2383917</ref_obj_id>
				<ref_obj_pid>2383894</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Einarsson, P., Chabert, C.-F., Jones, A., Ma, W.-C., Lamond, B., Hawkins, T., Bolas, M., Sylwan, S., and Debevec, P. 2006. Relighting human locomotion with flowed reflectance fields. In <i>In Rendering Techniques 2006: 17th Eurographics Workshop on Rendering</i>, 183--194.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Development of a Portable Anisotropic Re.ectance Measurement System for Modeling and Rendering of Bidirectional 
Texture Functions Yuki Takeda* , Jiro Hara, Wataru Wakita, Yoshiyuki Sakaguchi, and Hiromi T. Tanaka 
Dai Nippon Printing Co., Ltd. 1. Measuring          Ritsumeikan University Multi-illuminated 
HDR Images The Parameter Maps Simulated Result of Anisotropic Reflection Model Figure 1: Our Modeling 
and Rendering Process of Bidirectional Texture Functions using the PARMS. 1 Introduction Recently, Computer 
Graphics (CG) productions need the novel method to represent realistic CG objects. It is very dif.cult 
for their creators to set parameters of the re.ection models that express correctly surface re.ection 
of real objects. Einarsson et al. proposed the image-based relighting method us­ing Light Stage 6 that 
can render person under variable viewpoint and illumination [Einarsson et al. 2006]. But their system 
design depends on the size of a target, so the size of measurement system must be in proportion to the 
size of a target. In this paper, we propose a Portable Anisotropic Re.ectance Mea­surement System (PARMS) 
that has no limits to the shape and the size of targets. We also con.gure a modeling and rendering process 
of the Bidirectional Texture Function (BTF) described by anisotropic re.ection models using the PARMS. 
We show the result on the re.ection simulation using the BTFs that are generated from multi-illuminated 
High Dynamic Range (HDR) images acquired by the PARMS. 2 Our Approach Figure 1 shows our modeling and 
rendering process of BTF using the PARMS. At .rst, the multi-illuminated HDR images of a tar­get are 
acquired by the PARMS. Second, we generate the BTF that is described by the anisotropic re.ection models 
generated from multi-illuminated HDR images in each pixel. At last, we imple­ment the image-based lighting 
method using generated BTF for the re.ection simulation of a target. For measuring, we develop the PARMS 
that is constructed of a digital camera, a tripod, the arm of semi-circular arc with equally spaced 9 
LED lights, a stepping motor, an absolute encoder, the ro­tation controller board of the arm, and a PC 
that controls the cam­era and the board. We can acquire multi-illuminated HDR images by rotating arm, 
turning on the lights in order, and taking multiple shots at different exposure settings. Since the PARMS 
is portable system and is able to measure around the center space of the arc, we can measure separately 
the surface re.ection of large objects from any angle. This is the reason that the PARMS has no limits 
to the shape and the size of targets. * e-mail:takeda-y13@mail.dnp.co.jp In modeling process, the parameters 
of anisotropic re.ection model are estimated in Levenberg-Marquardt optimization using multi­illuminated 
HDR images in each pixel. This parameters is the set of specular re.ectance, anisotropic variance, diffuse 
re.ectance, nor­mal direction and tangent direction in each pixel. In rendering process, we adopt the 
image-based lighting method for BTF rendering of targets. For real-time rendering, the omni direc­tional 
image is approximated with a .nite number of light sources using importance sampling algorithm. The contributions 
of our work are the following: 1. Introducing the PARMS that can measure the anisotropic re­.ection of 
arbitrary 3D objects because of its portability. 2. Providing modeling and rendering process of BTF 
using the PARMS.  In our experiment, we set the red carpet as a target. Multi­illuminated HDR images 
were generated from 684 images acquired by rotating arm in each 10 degrees, turning on 9 lights in order, 
and taking 4 shots at different exposure settings. The parameters of anisotropic re.ection model were 
automatically determined from multi-illuminated HDR images in each pixel. We simulated the re­.ection 
of red carpet using the BTF generated from the anisotropic re.ection models in doors. In conclusion, 
we proposed the PARMS that has no limits to the shape and the size of targets. We con.gured the modeling 
and ren­dering process of BTF described by the anisotropic re.ection mod­els using the PARMS. We showed 
the result on the re.ection sim­ulation using the BTFs that are generated from multi-illuminated HDR 
images acquired by the PARMS. In future work, we try to develop the image-based 3D reconstruc­tion algorithm 
based on the images acquired by the PARMS. References EINARSSON, P., CHABERT, C.-F., JONES, A., MA, 
W.-C., LA-MOND, B., HAWKINS, T., BOLAS, M., SYLWAN, S., AND DE-BEVEC, P. 2006. Relighting human locomotion 
with .owed re.ectance .elds. In In Rendering Techniques 2006: 17th Euro­graphics Workshop on Rendering, 
183 194. Copyright is held by the author / owner(s). SIGGRAPH 2012, Los Angeles, California, August 5 
 9, 2012. ISBN 978-1-4503-1435-0/12/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2342948</article_id>
		<sort_key>520</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>39</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[GaussSketch]]></title>
		<subtitle><![CDATA[add-on magnetic sensing for natural sketching on smartphones]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2342896.2342948</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2342948</url>
		<abstract>
			<par><![CDATA[<p>This work presents <i>GaussSketch</i>, a retrofit stylus sensing extension to enable the detection of stylus tilt and pressure for natural sketch simulation on smartphones by utilizing magnetism. Attaching the compatible sensor grid on the back of a conventional smartphone can (1) sense the stylus' tilt degrees and pressure values, (2) discriminate the touch events generated by a finger or the stylus, and (3) detect where the stylus hovers upon the screen.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3737319</person_id>
				<author_profile_id><![CDATA[81321494894]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Rong-Hao]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Liang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[National Taiwan University and Academia Sinica]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737320</person_id>
				<author_profile_id><![CDATA[81416600159]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Kai-Yin]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Cheng]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[National Taiwan University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737321</person_id>
				<author_profile_id><![CDATA[81100108039]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Bing-Yu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Chen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[National Taiwan University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737322</person_id>
				<author_profile_id><![CDATA[81100353194]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[De-Nian]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Academia Sinica]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1531371</ref_obj_id>
				<ref_obj_pid>1531326</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Rosenberg, I., and Perlin, K. 2009. The unmousepad: an interpolating multi-touch force-sensing input pad. <i>ACM TOG 28</i>, 3, 65:1--65:9.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 GaussSketch: Add-On Magnetic Sensing for Natural Sketching on Smartphones  Rong-Hao Liang*Kai-Yin 
Cheng* Bing-Yu Chen* De-Nian Yang * National Taiwan University Academia Sinica Figure 1: (a) The principle 
of GaussSketch. (b) A magnetic stylus with the other side as the eraser. (c) A thin form-factor sensor 
augments magnetic .eld sensing on the multitouch display. (d) A comparison of fast sketching without 
(left) and with (right) GaussSketch. Abstract This work presents GaussSketch, a retro.t stylus sensing 
extension to enable the detection of stylus tilt and pressure for natural sketch simulation on smartphones 
by utilizing magnetism. Attaching the compatible sensor grid on the back of a conventional smartphone 
can (1) sense the stylus tilt degrees and pressure values, (2) dis­criminate the touch events generated 
by a .nger or the stylus, and (3) detect where the stylus hovers upon the screen. 1 INTRODUCTION Sketching 
on readily available smartphones can let users easily pre­serve ideas or notes for further inspiration 
or reminding. Neverthe­less, .ngers are too imprecise for sketching, and therefore some commercial products 
have started providing a the stylus mecha­nism. Recent smartphone touchscreen can be divided into resistive­based 
(e.g., [Rosenberg and Perlin 2009]) or capacitive-based (e.g., iPhone1), but neither can detect the advanced 
features of a stylus, such as tilt angle or pressure, and simulate sketching in a more nat­ural manner. 
Although commercial available touchscreens tightly­integrated with sophisticated electromagnetic resonance 
(EMR) sensors2 can enable the sensing of the aforementioned features. It is hard to attach this as an 
add-on module to enhance the common smartphones, because the electromagnetic .eld will be blocked by 
the shielding materials inside most of the smartphones. 2 DESIGN To let general smartphones easily enable 
the stylus sketching fea­ture, GaussSketch (Figure 1(a)) has been developed by utilizing the penetrability 
of the directional magnetism. A prototype stylus (Figure 1(b)) is made of stacked 8mm-diameter and 30mm-height 
cylindrical neodymium magnets with a conductive rubber tip torn down from an Elecom3 iPhone stylus. To 
detect the position and status of the stylus, a 60(W ) × 80(H)-diameter and 2mm-thin sensor board is 
made of 12 × 16 = 192 Winson4 WSH138 analog Hall sensors in a grid manner. All sensor data are transferred 
to a 1http://www.apple.com/iphone/ 2http://www.wacom.com/ 3http://www.elecom.co.jp/ 4http://www.winson.com.tw/ 
 PC through a Teensy5 micro-controller and upsampled by bi-cubic interpolation from 12 × 16 to 360 × 
480 (163dpi) to reconstruct the shape of magnetic .eld consistently over 60fps. An unmod­i.ed iPod touch6 
and a sensor grid attached on a plastic case for external sensing are used for prototyping (Figure 1(c)). 
By calculating the centroid of the magnetic .eld shape as shown in Figure 2(a), we can obtain O =(Ox,Oy), 
which represents the position of the peak magnitude, and M as the magnetic .eld. With the incorporation 
of the actual touch point P =(Px,Py) of the stylus on the screen, the following features can be enabled: 
 Figure 2: (a) Overview of stylus sensing. (b) Tilt sensing. (c) Pres­sure sensing. (d) Implicit mode 
switching for erasing. Tilt Sensing (Figure 2(b)): The relative position between O and P O changes during 
tilting the stylus. Hence, dO= PO and d = ||OP ||can be mapped as the tilted direction and angle, respectively. 
Pressure Sensing (Figure 2(c)): While a user stresses on the tip, the rubber will be deformed and the 
distance between the embed­ded magnets and the sensor board is shortened. Hence, the sensed values M 
will become higher. Discriminating magnetic stylus from .ngers (Figure 2(d)): If M is above a prede.ned 
threshold, P will be regarded as a magnetic sty­lus event rather than the .nger or conventional stylus 
touch event. Hover sensing: While the system senses the magnetic .eld without sensing the touch point 
P within a prede.ned diameter r from the calculated centroid O, then O can be treated as the hover point. 
 References ROSENBERG, I., AND PERLIN, K. 2009. The unmousepad: an interpolating multi-touch force-sensing 
input pad. ACM TOG 28, 3, 65:1 65:9. 5http://www.pjrc.com/teensy/ 6http://www.apple.com/ipodtouch/ Copyright 
is held by the author / owner(s). SIGGRAPH 2012, Los Angeles, California, August 5 9, 2012. ISBN 978-1-4503-1435-0/12/0008 
  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2342949</article_id>
		<sort_key>530</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>40</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[GeigerCam]]></title>
		<subtitle><![CDATA[measuring radioactivity with webcams]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2342896.2342949</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2342949</url>
		<abstract>
			<par><![CDATA[<p>Measuring radioactivity is almost exclusively a professional task in the realms of science, industry and defense, but recent events spur the interest in low-cost consumer detection devices. We show that by using image processing techniques, a current, only slightly modified, off-the-shelf HD webcam can be used to measure &alpha;, &#946; as well as &#947; radiation. In contrast to dedicated measurement devices such as Geiger counters, our framework can classify the type of radiation and can differentiate between various kinds of radioactive materials. By optically insulating the camera's imaging sensor, recordings at extreme exposure and gain values are possible, and the partly very faint signals caused by the particle impacts are separated from the thermal and device background noise and analyzed in real-time.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3737323</person_id>
				<author_profile_id><![CDATA[81503679503]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Thomas]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Auzinger]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Vienna University of Technology, Austria]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737324</person_id>
				<author_profile_id><![CDATA[81456635182]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Ralf]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Habel]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Vienna University of Technology, Austria]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737325</person_id>
				<author_profile_id><![CDATA[81504687487]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Andreas]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Musilek]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Vienna University of Technology, Austria]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737326</person_id>
				<author_profile_id><![CDATA[81504687608]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Dieter]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hainz]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Vienna University of Technology, Austria]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737327</person_id>
				<author_profile_id><![CDATA[81100084933]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Michael]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wimmer]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Vienna University of Technology, Austria]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 GeigerCam: Measuring Radioactivity with Webcams Thomas Auzinger1, Ralf Habel1, Andreas Musilek2, Dieter 
Hainz2, Michael Wimmer1 1 Institute of Computer Graphics 2 Institute of Atomic and Subatomic Physics 
Vienna University of Technology, Austria Figure 1: With a modi.ed off-the-shelf HD webcam (a) we record 
impacts of various kinds of nuclear radiation (b) and obtain a measure of the energy spectrum of the 
source. (c) shows the characteristic spectrum of the a emitter Americium-241. 1 Abstract Measuring radioactivity 
is almost exclusively a professional task in the realms of science, industry and defense, but recent 
events spur the interest in low-cost consumer detection devices. We show that by using image processing 
techniques, a current, only slightly modi.ed, off-the-shelf HD webcam can be used to measure a, ß as 
well as . radiation. In contrast to dedicated measurement de­vices such as Geiger counters, our framework 
can classify the type of radiation and can differentiate between various kinds of radioac­tive materials. 
By optically insulating the camera s imaging sensor, recordings at extreme exposure and gain values are 
possible, and the partly very faint signals caused by the particle impacts are sep­arated from the thermal 
and device background noise and analyzed in real-time. 2 Introduction The measurement of ionizing radiation 
has a long history with many academic and commercial applications in nuclear safety, defense, medicine, 
biology, material sciences, amongst others. Even entry­level devices start in the $200-300 price range, 
which is too ex­pensive for casual applications or mass deployment. While Geiger counters have a higher 
sensitivity than our approach, they lack the capability to differentiate different types of radioactivity 
or to measure the radiation s energy, features which require signi.cantly more expensive equipment and 
constraints our framework does not suffer from. The rapid development in image sensor technology and 
their mass applications resulted in very cheap megapixel sen­sors, with a physical pixel size of 2-3 
µm. Due to the resulting high sensitivity it is possible to record even low energy nuclear radia­tion 
with consumer HD webcams, with their sensor only shielded against visible light. Our approach results 
in a simple device that can be operated on any computer and costs only $20-30, the price of the webcam 
and the modi.cation. 3 Our Approach One of the main challenges in using a webcam (in our case a Log­itech 
C270) to produce reliable measurements of ionizing radiation is the separation of the particle impact 
signal from the optical, ther­mal and device background noise without excessively damping of the signal. 
To achieve this, we remove both the lens and the IR .lter, which would otherwise absorb all alpha particles 
and beta particles to a large degree. The fully exposed sensor is then sealed with an 8µm thin aluminum 
foil, removing any optical in.uences and re­placing the IR .lter. The camera is set to the longest exposure 
time possible and to a very high gain to detect even faint signals. Dur­ing measurements, GPU assisted 
real-time image processing of the direct video feed is used to treat the remaining noise by tracking 
the noise spectrum per pixel, incorporating not only spatial but also temporal variations due to temperature 
changes and spontaneous emissions. A con.dence value per pixel based on event probabili­ties is calculated 
to identify potentially hit pixels. Finally, we use morphological clustering to group pixels into particle 
impact events and analyze their energies. We measured the emissions of a broad sample of radioactive 
sub­stances. As a emitters, we chose 241Am, 238Pu, 239Pu and 233U, ß emitters measured are 14C, 36Cl, 
90Sr90Y. . emitters are 60Co, 137Cs and Plutonium-Beryllium (P uBe) as a neutron source. Fur­thermore, 
we measured naturally available radioactive sources Tho­rium (232Th), pitchblende (238U) and tritium 
(3H). 4 Results and Future Work Preliminary measurements show reliable detection rates for a and ß radiation 
and their energies. Compared to a professional very sen­sitive radiation detector, a Berthold LB 124 
Scintillator, we achieve a relative count rate for a particles of 1.10 × 10-2 ± 0.17. The ß detection 
rate can be measured with high precision (error < 5%) but depends on the material measured. This indicates 
that the sen­sitivity of the sensor varies with the radiation s energy. Reliable . detection requires 
higher radiation levels than ß due to the smaller effective cross section. We were not able to detect 
neutrons, despite a signi.cant emission rate of the PuBe source. For more robust results, the ef.ciency 
of different sensors and intra­model differences need to be explored. Furthermore, an exact energy calibration 
allows the identi.cation of radioactive materi­als, both natural and synthetic. We see the .nal applications 
of a cheap radioactivity sensor both in high-level radiation environ­ments where inevitable sensor destruction 
would incur negligible costs, and as consumer hardware, either as a do-it-yourself modi.­cation kit or 
as a mass-produced consumer product. Copyright is held by the author / owner(s). SIGGRAPH 2012, Los Angeles, 
California, August 5 9, 2012. ISBN 978-1-4503-1435-0/12/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2342950</article_id>
		<sort_key>540</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>41</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>5</seq_no>
		<title><![CDATA[Ikimo]]></title>
		<subtitle><![CDATA[open entry-level robotics platform]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2342896.2342950</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2342950</url>
		<abstract>
			<par><![CDATA[<p>Ikimo is a low-cost entry level robotics ecosystem build under Open Source Hardware license developed initally as part of research project in Keio University. The project was motivated by the lack of financially accessible robots on the market that would be able to support very wide scope of research projects, that vary in complexity and demands on the technology - including the needs for possible modifications both to software and hardware. Ikimo also functions as cheap robotics learning platform. It integrates physical designs, hardware and software and thus provides set of tools for various robotics applications, allowing modification based on users' demands, as everything is shared under open source licenses. Ikimo was devised as an alternative to other robotics system used in research projects, such as Roomba, that do not (or do only partially) allow the modifications based on the application needs.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3737328</person_id>
				<author_profile_id><![CDATA[81504685861]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Charith]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Fernando]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[InMojo, Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[charith@inmojo.com]]></email_address>
			</au>
			<au>
				<person_id>P3737329</person_id>
				<author_profile_id><![CDATA[81444608441]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Jan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Rod]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[InMojo, Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[jan@inmojo.com]]></email_address>
			</au>
			<au>
				<person_id>P3737330</person_id>
				<author_profile_id><![CDATA[81504688455]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[David]]></first_name>
				<middle_name><![CDATA[Siren]]></middle_name>
				<last_name><![CDATA[Eisner]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[InMojo, Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[dseisner@inmojo.com]]></email_address>
			</au>
			<au>
				<person_id>P3737331</person_id>
				<author_profile_id><![CDATA[81504684487]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Mauricio]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Cordero]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[InMojo, Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[mauricio@inmojo.com]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Ikimo: Open Entry-level Robotics Platform Charith Fernando* Jan Rod David Siren Eisner Mauricio Cordero§ 
InMojo, Inc. InMojo, Inc. InMojo, Inc. InMojo, Inc. 1 Introduction Ikimo is a low-cost entry level robotics 
ecosystem build under Open Source Hardware license developed initally as part of research project in 
Keio University. The project was motivated by the lack of .nancially accessible robots on the market 
that would be able to support very wide scope of research projects, that vary in complexity and demands 
on the technology -including the needs for possible modi.cations both to software and hardware. Ikimo 
also functions as cheap robotics learning platform. It integrates physical designs, hardware and software 
and thus provides set of tools for various robotics applications, allowing modi.cation based on users 
demands, as everything is shared under open source licenses. Ikimo was devised as an alternative to other 
robotics system used in research projects, such as Roomba, that do not (or do only partially) allow the 
modi.cations based on the application needs. Figure 1: Ikimo robot, buggy version. 2 System Description 
The Ikimo ecosystem composes of three main parts: the brain , a micro controller board that implements 
various functionality; the robot chassis, based around cheap accessible materials and laser-cutting friendly 
designs and .nally software API that allows multiple levels of programming to meet various demands of 
target communities of researchers, educators and designers that are using Ikimo robots for their projects. 
The robot brain is based on Atmel 328P chip preloaded with Arduino stack. Using Arduino, rather than 
custom built software signi.cantly simpli.es the learning curve of the whole platform. Furthermore, the 
board contains four DC motor controllers, four servo motor outputs and six analog sensor breakouts. The 
communication is facilitated by Bluetooth using standard serial protocol. Boards can be stacked to allow 
access to more inputs and outputs, in which case they communicate over I2C protocol. *e-mail: charith@inmojo.com 
e-mail: jan@inmojo.com e-mail: dseisner@inmojo.com §e-mail: mauricio@inmojo.com  Figure 2: Ikimo system 
and its parts. Ikimo system has two types of software APIs. The .rst one, .rmware API works on the system 
level of Arduino and allows experienced users of this platform to easily program the robots without a 
need for detailed knowledge of DC motors controls or readings of analog sensors. The .rmware API allows 
to control directly single motors and read analog values of sensors providing great .exibility to con.gure 
various movement patterns including steering and rotating the robot. The higher level API provides access 
to simple set of commands that with which user can control the whole robot. These commands, sent to the 
robot wirelessly over Bluetooth, are optimized for going forwards, backwards, rotating by a given number 
of degrees and preprogrammed steering. This API simply allows anyone to interact with the robot without 
the knowledge of Arduino programming, using any other environment, such as Processing, Java or C++. 
3 Demonstration Scenarios We would like to demonstrate some semi-autonomous as well as autonomous operations 
of the robots, including: interfacing with video game controllers (Wiimote) with obstacle avoidance; 
AR-based applications, such as draw-a-path-to-follow, or a vector analysis based movement of objects 
applied in physical games based around moving blocks in space. The demo is focusing on demonstrating 
hands-on all the abilities and various con.gurations achievable with Ikimo robot platform. We also propose 
to do a workshop focused on building the robot from scratch and basic interaction with the robot. 4 
Conclusion Ikimo provides an interesting alternative to existing robotics plat­form. Due to the complete 
openness of the system in hardware design, physical design and software, it presents an ideal entry­level 
robotics platform for simple research projects as well as for teaching robotics in higher educational 
level institutions (from high schools to universities). Ikimo was a part of multiple research projects 
and publications and thus is tested in real applications. Copyright is held by the author / owner(s). 
SIGGRAPH 2012, Los Angeles, California, August 5 9, 2012. ISBN 978-1-4503-1435-0/12/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2342951</article_id>
		<sort_key>550</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>42</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>6</seq_no>
		<title><![CDATA[Improved linear light source material reflectance scanning]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2342896.2342951</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2342951</url>
		<abstract>
			<par><![CDATA[<p>We improve the resolution, accuracy, and efficiency of Linear Light Source (LLS) Reflectometry with several acquisition setup and data processing improvements, allowing spatially-varying reflectance parameters of complex materials to be recorded with unprecedented accuracy and efficiency.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3737332</person_id>
				<author_profile_id><![CDATA[81100332743]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Jan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Meseth]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[RTT AG]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737333</person_id>
				<author_profile_id><![CDATA[81504684464]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Shawn]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hempel]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[RTT AG]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737334</person_id>
				<author_profile_id><![CDATA[81320496420]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Andrea]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Weidlich]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[RTT AG]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737335</person_id>
				<author_profile_id><![CDATA[81504688224]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Lynn]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Fyffe]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[RTT AG]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737336</person_id>
				<author_profile_id><![CDATA[81421597317]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Graham]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Fyffe]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[RTT AG]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737337</person_id>
				<author_profile_id><![CDATA[81504683342]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Craig]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Miller]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[RTT AG]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737338</person_id>
				<author_profile_id><![CDATA[81504683540]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>7</seq_no>
				<first_name><![CDATA[Paul]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Carroll]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[RTT AG]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737339</person_id>
				<author_profile_id><![CDATA[81504685651]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>8</seq_no>
				<first_name><![CDATA[Paul]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Debevec]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[RTT AG and USC ICT]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>882342</ref_obj_id>
				<ref_obj_pid>1201775</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Gardner, A., Tchou, C., Hawkins, T., and Debevec, P. 2003. Linear light source reflectometry. In <i>ACM SIGGRAPH 2003 Papers</i>, ACM, New York, NY, USA, SIGGRAPH '03, 749--758.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1964940</ref_obj_id>
				<ref_obj_pid>1964921</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Ren, P., Wang, J., Snyder, J., Tong, X., and Guo, B. 2011. Pocket reflectometry. In <i>ACM SIGGRAPH 2011 papers</i>, ACM, New York, NY, USA, SIGGRAPH '11, 45:1--45:10.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Improved Linear Light Source Material Re.ectance Scanning Jan Meseth1 Shawn Hempel1 Andrea Weidlich1 
Lynn Fyffe1 Graham Fyffe1 Craig Miller1 Paul Carroll1 Paul Debevec1,2 1RTT AG 2USC ICT Figure 1: Left 
to Right: (a) Scanner (b) Materials being scanned (c) Anisotropic specular parameters (d) Rendering with 
measured materials. Abstract We improve the resolution, accuracy, and ef.ciency of Linear Light Source 
(LLS) Re.ectometry with several acquisi­tion setup and data processing improvements, allowing spatially­varying 
re.ectance parameters of complex materials to be recorded with unprecedented accuracy and ef.ciency. 
Introduction Measuring the re.ectance of spatially-varying ma­terials is a topic of great interest in 
the graphics industry; however, no commercially available instrument measures spatially varying re.ectance 
parameters for a wide range of materials in a way which is high resolution, ef.cient, and accurate. The 
basic problem is that moving a light to thousands of incident angles is extremely time consuming, and 
still typically fails to record highly specular mate­rials accurately. This problem can be alleviated 
using linear light sources [Gardner et al. 2003]: instead of moving a point light source to thousands 
of positions, a linear light source (LLS) is passed across the sample at a few different orientations 
and per-pixel re.ectance is inferred from the resulting re.ectance traces using a re.ectance model. Our 
new measurement setup (Fig. 1a) yields high-quality results for anything ranging from Lambertian to sharp 
specular materials. Unlike [Ren et al. 2011], which aims at quick measurements using a mobile device, 
the quality of measurements does not depend on the existence of a reference material database or the 
skill of the operator. In addition, our device also handles anisotropic materials successfully. Scanner 
Design A signi.cant limitation of previous LLS ap­proaches is that the camera observes the sample at 
an oblique angle of approximately 45. . This keeps the light source from occluding sample points when 
the light is directly above, and separates the position of the diffuse and specular peaks so that they 
can be mod­eled independently. This limits resolution due to foreshortening and since the sample will 
likely extend beyond a high-resolution imag­ing system s limited depth of .eld. We instead place the 
camera 1m directly above the material, observ­ing the 30cm × 20cm sample region with an 11 megapixel 
Prosilica GE4000C camera with a 105mm lens. As expected, the LLS lights 10cm above the sample occlude 
the sample just as the LLS passes above it, obscuring important normal re.ectance angles. We solve this 
by illuminating the sample from these lighting angles via a sec­ond, virtual LLS, constrcuted by aiming 
an LLS into the underside of a strip of half-silvered glass mounted at approximately 45.. The light re.ects 
down to the sample and back up through the glass to the camera, yielding a light whose illumination is 
visible even though the lamp itself is unseen. Although the angular coverage of the virtual LLS is limited, 
it more than covers the angles blocked by the direct LLS, so all incident angles are covered. With the 
camera looking straight down, the peaks of the specular and diffuse lobes become largely coincident, 
which makes diffuse and specular re.ections challenging to separate and model inde­pendently. We place 
a strip of polarizing .lter gel over each LLS, and a .lter wheel in front of the camera so that polarization 
dif­ference imaging can separate the diffuse and specular components from two passes of each light source. 
In total, the scanner has four linear light sources: a pair of virtual and direct LLS s to scan in both 
the horizontal and vertical directions. The camera records at 5fps and moves 3mm (the width of each LED 
light strip) per captured image, completing a scan in 20 minutes. Data Processing As the lights pass 
over the sample, each sample point produces a re.ectance trace of pixel values. We composite the unoccluded 
regions from the direct and virtual LLS traces to­gether, and compute a specular-only trace by subtracting 
the cross­polarized diffuse-only trace from the parallel-polarized trace. We use Knuth s online algorithm 
to ef.ciently compute the sum, mean, and standard deviation of the re.ectance lobe in each trace, yielding 
estimates of the albedo, surface normal, and specular roughness of each material point for both the diffuse 
and specular components. For rendering, these parameters are used to drive the Ashikhmin-Shirley re.ectance 
model. Results Fig. 1 shows several brushed and stamped metal sam­ples being scanned, their recovered 
anisotropic re.ectance parame­ter maps, and a real-time rendering of a car interior using several of 
the materials. Other examples appear in the supplemental material. Acknowledgements We thank Ludwig Fuchs, 
Peter Roehner, Jeroen Snepvangers, Muybridge Digital Imaging, Happy Digital, Marco Tan, Danny Tierry, 
Andrea Gunschera, and Conny Denk for their contributions and support. Contact: Jan.Meseth@rtt.ag References 
GARDNER, A., TCHOU, C., HAWKINS, T., AND DEBEVEC, P. 2003. Linear light source re.ectometry. In ACM SIGGRAPH 
2003 Papers, ACM, New York, NY, USA, SIGGRAPH 03, 749 758. REN, P., WANG, J., SNYDER, J., TONG, X., AND 
GUO, B. 2011. Pocket re.ectometry. In ACM SIGGRAPH 2011 papers, ACM, New York, NY, USA, SIGGRAPH 11, 
45:1 45:10. Copyright is held by the author / owner(s). SIGGRAPH 2012, Los Angeles, California, August 
5 9, 2012. ISBN 978-1-4503-1435-0/12/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2342952</article_id>
		<sort_key>560</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>43</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>7</seq_no>
		<title><![CDATA[Printing 3D light field with 1D halftone screening]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2342896.2342952</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2342952</url>
		<abstract>
			<par><![CDATA[<p>Lenticular printing is an old technique, which enables stereoscopic 3D by recording pictures taken by two or more cameras onto one medium. It is often awkward in its transition between insufficient images due to sparse sampling from the light space. In this paper, we propose a method for making 3D print which reproduces light space represented as a light field by introducing an ideal halftone screening.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3737340</person_id>
				<author_profile_id><![CDATA[81504682117]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Hideki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yamazaki]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Dai Nippon Printing Co., Ltd.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737341</person_id>
				<author_profile_id><![CDATA[81504682194]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Yasuhiro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Takaki]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tokyo University of Agriculture and Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>237199</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[M. Levoy, and P. Hanrahan, "Light Field Rendering", <i>SIGGRAPH '96, pp. 31--42, 1996</i>
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Y. Yoshikawa and Y. Takaki, "Depth tolerance of simple interpolation methods for 3D camera used for natural 3D display", <i>Proc. SPIE, vol.5243, pp. 165--171, 2003</i>
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[S. Sakamoto and Y. Takaki, "Three-Dimensional Print Using One-Dimensional Screen Technique", <i>Jpn. J. Appl. Phys., vol.47, no.7, pp. 5486--5492, 2008</i>
]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Printing 3D Light Field with 1D Halftone Screening Hideki Yamazaki / Yasuhiro Takaki Dai Nippon Printing 
Co., Ltd. / Tokyo University of Agriculture and Technology 1. Introduction Lenticular printing is an 
old technique, which enables stereoscopic 3D by recording pictures taken by two or more cameras onto 
one medium. It is often awkward in its transition between insufficient images due to sparse sampling 
from the light space. In this paper, we propose a method for making 3D print which reproduces light space 
represented as a light field[1] by introducing an ideal halftone screening. 2. 1D screening for printing 
light field We print three-dimensional (3D) light field by lenticular printing. Each 2D slice S..(x, 
y) in the 3D light field represents an image viewed from the azimuth of .. The rays are collected from 
all slices and printed together behind the lenticular lens pixel by pixel as illustrated in Fig.1. Required 
azimuth.. for each sub-pixel located at distance w from the optical axis is given as arctan(w / f ), 
where f denotes the focal length of the lenticular lens.  (a) (b) Fig. 1. Lenticular printing Numbers 
of images captured from horizontally different positions are required to create 3D light field[2]. We 
take pictures with a camera moving on the straight rail. As images are in perspective, we collect rays 
passing through the focal plane in the specific direction to make a slice of the light field. Fig.2 shows 
an example of captured images and representative slices created from them. Note that objects are projected 
in parallel horizontally, while remaining perspective vertically. (a) captured image (b) . = 0º (c) 
. = 11º Fig. 2. Creating 3D light field We need to employ halftone screening for printing light field 
in full color. One-dimensional (1D) AM screening[3] is proposed for lenticular printing which controls 
tone by series of small binary dots vertically connected instead of 2D dots used in general printing 
as shown in Fig.3(b). Alternatively, 1D-FM screening represents tone by density of isolated dots as shown 
in Fig.3(c). They make possible to include maximum number of slices in a pitch of lenticular lens, and 
make the smoothest transition in parallax with specific resolution in plate making. 1D-AM screening, 
however, locates halftone dots densely around particular lines that are undesirably noticeable on 3D 
print. On the other hand, 1D-FM screening disperses halftone dots over the region, while solitary dots 
make image impression noisy.  (a) (b) (c) (d) (e) Fig. 3. 1D screening : (a) an array of colors to be 
printed, (b) 1D-AM, (c) 1D-FM, (d) displaced 1D-AM, (e) our 1D-AM screening.  To solve the problem 
on 1D-AM screening, we displace each series of dots by which dots are located around lines inclined independently 
each of the four primary colors as shown in Fig.3(d). Although this arrangement reduces partiality of 
dots, a new problem known as moiré is expected due to regular arrangement as often seen in usual color 
printing. The second strategy on improving screening is to diffuse regularity by dividing each series 
of dots into a few fragments with maintaining probability of dots and spaces as shown in Fig.3(e). Furthermore, 
dividing into independent number of fragments to each of the four primary colors will gain dispersion. 
3. Result Fig. 4. Printed 3D light field Fig.4 shows three different views of a trial product. We inserted 
48 parallax images into a lenticular cell (50lpi) with 0.6 degrees of interval in azimuth using a 2400dpi 
resolution plate making system, so a quite smooth motion parallax was reproduced. Fig.5 shows magnified 
images of a 3D print with each of proposed 1D screenings. While horizontal lines appear conspicuously 
with 1D-AM, the image becomes gritty-textured with 1D-FM. Displacement on 1D-AM then makes noticeable 
moiré. Finally, all artifacts are disappeared with our 1D-AM and the image is reproduced smoothly. (a) 
(b) (c) (d) Fig. 5. Comparison of 1D Screening (magnified) : (a) 1D-AM, (b) 1D-FM, (c) displaced 1D-AM, 
(d) our 1D-AM screening. References [1] M. Levoy, and P. Hanrahan, "Light Field Rendering", SIGGRAPH 
96, pp. 31-42, 1996 [2] Y. Yoshikawa and Y. Takaki, "Depth tolerance of simple interpolation methods 
for 3D camera used for natural 3D display", Proc. SPIE, vol.5243, pp. 165-171, 2003 [3] S. Sakamoto and 
Y. Takaki, "Three-Dimensional Print Using One-Dimensional Screen Technique", Jpn. J. Appl. Phys., vol.47, 
no.7, pp. 5486-5492, 2008 Copyright is held by the author / owner(s). SIGGRAPH 2012, Los Angeles, California, 
August 5 9, 2012. ISBN 978-1-4503-1435-0/12/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2342953</article_id>
		<sort_key>570</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>44</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>8</seq_no>
		<title><![CDATA[SGRT]]></title>
		<subtitle><![CDATA[a scalable mobile GPU architecture based on ray tracing]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2342896.2342953</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2342953</url>
		<abstract>
			<par><![CDATA[<p>Recently, with the increasing demand for photorealistic graphics and the rapid advances in desktop CPUs/GPUs, real-time raytracing has attracted considerable attention. Unfortunately, raytracing in the current mobile environment is difficult because of inadequate computing power, memory bandwidth, and flexibility in mobile GPUs. In this work, we present a novel mobile GPU architecture called the SGRT (Samsung reconfigurable GPU based on RayTracing) by enhancing our previous works with the following features: 1) a fast compact hardware engine that accelerates a traversal and intersection operation, 2) a flexible reconfigurable processor that supports software ray generation and shading, and 3) a parallelization framework that achieves scalable performance. Unlike our previous work, the current architecture is designed for both static and dynamic scenes with a smaller area. Experimental results show that the SGRT can be a versatile graphics solution, as it supports compatible performance compared to desktop GPU raytracers. To the best of our knowledge, the SGRT is the first mobile GPU based on full Whitted raytracing.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3737342</person_id>
				<author_profile_id><![CDATA[81440599617]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Won-Jong]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lee]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[SAIT, Samsung Electronics, Korea]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[joe.w.lee@samsung.com]]></email_address>
			</au>
			<au>
				<person_id>P3737343</person_id>
				<author_profile_id><![CDATA[81485641922]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Shi-Hwa]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lee]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[SAIT, Samsung Electronics, Korea]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737344</person_id>
				<author_profile_id><![CDATA[81474665022]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Jae-Ho]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nah]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Yonsei University, Korea]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737345</person_id>
				<author_profile_id><![CDATA[81406599725]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Jin-Woo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kim]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Yonsei University, Korea]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737346</person_id>
				<author_profile_id><![CDATA[81485647592]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Youngsam]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Shin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[SAIT, Samsung Electronics, Korea]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737347</person_id>
				<author_profile_id><![CDATA[81496658721]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Jaedon]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lee]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[SAIT, Samsung Electronics, Korea]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737348</person_id>
				<author_profile_id><![CDATA[81440603283]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>7</seq_no>
				<first_name><![CDATA[Seok-Yoon]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Jung]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[SAIT, Samsung Electronics, Korea]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>2024194</ref_obj_id>
				<ref_obj_pid>2070752</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Nah, J.-H., et al. 2011. T&I Engine: Traversal and Intersection Engine for Hardware Accelerated Ray Tracing. In <i>ACM Transaction on Graphics (Proceedings of SIGGRAPH ASIA 2011)</i>, 30, 6, 160:1--10.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Lee, W.-J., et al. 2011. A Scalable GPU Architecture based on Dynamically Embedded Reconfigurable Processor. In <i>High Performance Graphics 2011, Posters</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 SGRT: A Scalable Mobile GPU Architecture based on Ray Tracing Won-Jong Lee* Shi-Hwa Lee* Jae-Ho Nah 
 Jin-Woo Kim  Youngsam Shin* Jaedon Lee* Seok-Yoon Jung* *SAIT, Samsung Electronics, Korea Yonsei 
University, Korea Figure 1: (a) Our system architecture including the SGRT cores and host processor. 
(b) Rendered images by the SGRT simulator: Ferrari (left, 210K trian­gles, 1 light source) and Fairy 
(right, 170K triangles, 2 light sources). The SGRT (4-core) is predicted to render at 67.83fps (Ferrari) 
and at 87.82fps (Fairy). 1. Introduction Recently, with the increasing demand for photorealistic graphics 
and the rapid advances in desktop CPUs/GPUs, real-time raytrac­ing has attracted considerable attention. 
Unfortunately, raytracing in the current mobile environment is difficult because of inade­quate computing 
power, memory bandwidth, and flexibility in mobile GPUs. In this work, we present a novel mobile GPU 
archi­tecture called the SGRT (Samsung reconfigurable GPU based on RayTracing) by enhancing our previous 
works with the following features: 1) a fast compact hardware engine that accelerates a traversal and 
intersection operation, 2) a flexible reconfigurable processor that supports software ray generation 
and shading, and 3) a parallelization framework that achieves scalable performance. Unlike our previous 
work, the current architecture is designed for both static and dynamic scenes with a smaller area. Experimental 
results show that the SGRT can be a versatile graphics solution, as it supports compatible performance 
compared to desktop GPU raytracers. To the best of our knowledge, the SGRT is the first mobile GPU based 
on full Whitted raytracing. 2. SGRT Core Architecture Dedicated Hardware for Traversal and Intersection: 
The lack of computational power (<68GFLOPS) and memory bandwidth (<6.4GBPS) of current mobile GPUs motivated 
us to design a dedicated hardware for traversal and intersection, which are com­putation-intensive operations 
in raytracing. Our hardware, called the T&#38;I engine, is based on our previous work [Nah et al. 2011]. 
However, unlike our previous work, the new T&#38;I engine is de­signed for handling dynamic scenes with 
a bounding volume hier­archy (BVH). Moreover, the T&#38;I engine has a smaller area (3.89 mm2 per core, 
65nm), because BVH is an object hierarchy, which negates the need for LIST units to manage primitives. 
High per­formance features like the MIMD architecture for incoherent rays and a ray accumulation unit 
for latency hiding are directly reused. We can selectively utilize a specific BVH between the variants 
(e.g. Full SAH, Binned, SBVH, and LBVH) that are supported by the T&#38;I engine. The T&#38;I engine 
also has other outstanding fea­tures such as ray-AABB intersection units and a compact node layout. A 
full paper version will be announced in the near future. Reconfigurable Processor for Shading: We utilize 
a proprietary low-power DSP core developed in our previous work [Lee et al. 2011]; it is called the SRP 
(Samsung Reconfigurable Processor). The SRP is very flexible for supporting full programmability (standard 
C language); thus, various shaders (e.g. material and illumination) can be easily implemented. Unlike 
the conventional mobile GPU, the VLIW engine of the SRP can fully support con­trol-flow such as branch, 
which make recursive raytracing possi­ble. In addition, the SRP is capable of highly parallel data pro­ 
*e-mail: joe.w.lee@samsung.com cessing. The coarse-grained reconfigurable array (CGRA) of the SRP makes 
full use of the software pipeline technique to allow loop acceleration. Therefore, the ray packet stream 
processing can be done in ray generation and shading kernels, which maximizes the utilization of the 
functional units. Furthermore, the use of the SRP s reconfigurable feature might enable hybrid rendering 
that combines the OpenGL|ES rasterizer and raytracing. Parallelization Framework: For scalable performance, 
we built a parallelization framework based on the Samsung Micro Kernel (SMK), a real-time operating system 
for embedded system. The SMK supports multi-tasking by systematic scheduling in the task queues, and 
it allows developers to create and use tasks easily. We define an individual task for each SGRT core 
that is responsible for different pixels (or pixel tiles), then the scheduler can distrib­ute the next 
tasks to the idle SGRT core first, which results in dynamic load balancing. According to preliminary 
experiments, we could determine the performance scalability: 3.8x speedup on 4 SGRT cores compared to 
a single core. 3. Results Figure 1(a) shows the overall system architecture including the SGRT cores 
and host CPUs. Our architecture is based on an asyn­chronous BVH that is a combination of the rebuild 
(CPU), the refit (H/W) and the rendering (SGRT). The validity of the SGRT is verified and its performance 
is evaluated during cycle accurate simulation. The Ferrari and Fairy scene has been thoroughly test­ed 
(Figure 1(b)). Table 1 lists the performance results of raytrac­ing performed by the SGRT (4 cores), 
including shadow, reflec­tion and refraction with WVGA (800x480) resolution at 1GHz clock speed. We achieve 
around 170M RPS (T&#38;I engine), 255M RPS (SRP) and 87.82 fps (Fairy), which may be equivalent to the 
performance of recent desktop GPU ray tracers (200-300M RPS). We are now implementing the T&#38;I engine 
at the RTL level, and we will release the complete product supporting fully dynamic scenes in the future. 
Table 1. Performance results of the SGRT architecture Scene # of tri. # of ray T&#38;I Engine (usage 
&#38; cache hit ratio) SRP FPS Pipe TRV $ IST $ MRPS MRPS Fairy 170K 1.7M 87.27 93.83 96.53 171.32 255.72 
87.82 Ferrari 210K 1.5M 79.75 92.56 92.92 122.48 319.56 67.83  References NAH, J.-H., ET AL. 2011. T&#38;I 
Engine: Traversal and Intersection Engine for Hardware Accelerated Ray Tracing. In ACM Transaction on 
Graphics (Proceedings of SIGGRAPH ASIA 2011), 30, 6,160:1-10. LEE, W.-J., ET AL. 2011. A Scalable GPU 
Architecture based on Dynami­cally Embedded Reconfigurable Processor. In High Performance Graphics 2011, 
Posters. Copyright is held by the author / owner(s). SIGGRAPH 2012, Los Angeles, California, August 5 
 9, 2012. ISBN 978-1-4503-1435-0/12/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2342954</article_id>
		<sort_key>580</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>45</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>9</seq_no>
		<title><![CDATA[Towards a transparent, flexible, scalable, and disposable image sensor]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2342896.2342954</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2342954</url>
		<abstract>
			<par><![CDATA[<p>Conventional optoelectronic techniques have forced image sensors to a planar shape. We present our first attempts towards a transparent, flexible, scalable, and disposable sensor that samples the light-transport of a two-dimensional light-field within a thin-film luminescent concentrator for reconstructing an image being focussed on the concentrator film.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[image reconstruction]]></kw>
			<kw><![CDATA[image sensor]]></kw>
			<kw><![CDATA[light fields]]></kw>
			<kw><![CDATA[light transport]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.4.1</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P3737349</person_id>
				<author_profile_id><![CDATA[81504688325]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Alexander]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Koppelhuber]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Johannes Kepler University Linz, Austria]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[alexander.koppelhuber@jku.at]]></email_address>
			</au>
			<au>
				<person_id>P3737350</person_id>
				<author_profile_id><![CDATA[81504687784]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Darko]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lukic]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Johannes Kepler University Linz, Austria]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[darko.lukic@jku.at]]></email_address>
			</au>
			<au>
				<person_id>P3737351</person_id>
				<author_profile_id><![CDATA[81100622976]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Oliver]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bimber]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Johannes Kepler University Linz, Austria]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[oliver.bimber@jku.at]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Towards A Transparent, Flexible, Scalable, and Disposable Image Sensor Alexander Koppelhuber, Darko 
Lukic, and Oliver Bimber* Institute of Computer Graphics, Johannes Kepler University Linz, Austria  
Figure 1: Thin-.lm luminescent concentrator (LC) principle (a) and example (b). The .lm s edges are cut 
into triangular aperture slits (c,d,e) and photodiodes (four line cameras) are placed on the surfaces 
of these slits. This measures the light-transport of a two-dimensional light .eld within the LC .lm (c) 
that can be used to reconstruct the image focussed at the LC surface. Currently, only low resolution 
images have been reconstructed physically (e.g., ying-yang symbol, 9x9 pixels, f: ground truth, g: reconstruction), 
due to the limited sensitivity of the applied photodiodes. In simulations, higher resolutions have been 
possible (e.g., 64x64 pixels, h: ground truth, i: reconstruction). Abstract Conventional optoelectronic 
techniques have forced image sensors to a planar shape. We present our .rst attempts towards a transpar­ent, 
.exible, scalable, and disposable sensor that samples the light­transport of a two-dimensional light-.eld 
within a thin-.lm lumi­nescent concentrator for reconstructing an image being focussed on the concentrator 
.lm. CR Categories: I.4.1 [IMAGE PROCESSING AND COM-PUTER VISION]: Digitization and Image Capture; Keywords: 
light .elds, light transport, image reconstruction, im­age sensor 1 Introduction Thin-.lm luminescent 
concentrators (LC) are polymer foils com­prising optically active molecules (doped with a .uorescent 
dye). They can be less than one millimeter thick, bendable and transpar­ent. The foils are low cost (less 
than 10 EUR per square meter) and can be manufactured in almost arbitrary sizes. They are normally used 
for increasing the ef.ciency of solar cells with respect to sup­porting larger incident angles. Waveguides 
based on an LC forward *e-mail: {alexander.koppelhuber,darko.lukic,oliver.bimber}@jku.at the emitted 
light towards the edges of the LC by total internal re­.ection at an attenuation (transport loss) that 
is proportional to the travel distance. Photodiodes glued to the LC surface create an inter­face with 
higher optical density than air or the polymer of the LC. This causes light to be decoupled from the 
LC at the positions of the photodiodes. 2 Our Approach and Current Results The correlation of the transport 
losses between discrete entrance points (i.e., pixels p) on the LC surface with many photodiodes (d) 
placed at the boundary of the LC surface can be represented with s = Tp, where T is the light-transport 
matrix that can be calibrated. In principle, an image focussed on the LC can be reconstructed T -1 with 
the inverse light transport (p = s), or with .ltered back­projection. However, since each photodiode 
measures the integral of all pixel contributions, the light-transport matrix would be dense with a high 
condition number, and image reconstruction becomes very unstable (in particular in the presence of sensor 
noise). A to­mographic reconstruction would be seriously undersampled. For solving this problem, we cut 
the LC edges into triangular aper­ture slits and place the photodiodes appropriately on the surfaces 
of these slits. Re.ective paint at the backside of the slits causes a higher decoupling ef.ciency. With 
this, we are recording the transport of a two-dimensional light­.eld within the LC .lm using multiple 
1D slit-cameras surrounding the imaging area. In this case, the light-transport matrix becomes sparse, 
its condition number is reduced, and more positional and directional samples are available for a tomographic 
reconstruction. Figure 1 illustrates this principle and our current results. Multiple stacked LC layers 
with different wavelength responses can enable the reconstruction of color images. With multiple (sub­pixel-shifted) 
light-transport matrices, images can be reconstructed in a higher resolution. Copyright is held by the 
author / owner(s). SIGGRAPH 2012, Los Angeles, California, August 5 9, 2012. ISBN 978-1-4503-1435-0/12/0008 
  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	</section>
	<section>
		<section_id>2342955</section_id>
		<sort_key>590</sort_key>
		<section_seq_no>14</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Image and video processing]]></section_title>
		<section_page_from>14</section_page_from>
	<article_rec>
		<article_id>2342956</article_id>
		<sort_key>600</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>46</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[2D denoising factor for high dynamic range imaging]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2342896.2342956</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2342956</url>
		<abstract>
			<par><![CDATA[<p>The presence of noise in a high dynamic range (HDR) synthesis poses a serious degradation to the HDR image especially when the input images are captured at low light condition or with high sensitivity settings. Thus, a two-dimensional (2D) denoising factor is proposed to assign higher weight to a pixel with less noise based on both pixel luminance and image exposure. This pure temporal denoising factor is controlled by two key coefficients and can preserves edge and fine detail without blurring artifact. In addition, both memory and computation time are significantly reduced compare to other denoising methods.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3737352</person_id>
				<author_profile_id><![CDATA[81474692949]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Zijian]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Zhu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Institute for Infocomm Research]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[zhuzj@i2r.a-star.edu.sg]]></email_address>
			</au>
			<au>
				<person_id>P3737353</person_id>
				<author_profile_id><![CDATA[81423596048]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Zhengguo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Li]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Institute for Infocomm Research]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[ezgli@i2r.a-star.edu.sg]]></email_address>
			</au>
			<au>
				<person_id>P3737354</person_id>
				<author_profile_id><![CDATA[81100244820]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Susanto]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Rahardja]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Institute for Infocomm Research]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[rsusanto@i2r.a-star.edu.sg]]></email_address>
			</au>
			<au>
				<person_id>P3737355</person_id>
				<author_profile_id><![CDATA[81502722137]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Pasi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Fr&#228;nti]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Eastern Finland]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[franti@cs.joensuu.fi]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>258884</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Debevec, P. E. and Malik, J. 1997. Rendering high dynamic range radiance maps from photographs. In <i>Proceedings of SIGGRAPH 1997</i>, 369--378.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Mitsunaga T. and Nayar S. K. 1999. Radiometric Self Calibration. <i>IEEE Conf. on Computer Vision and Pattern Recognition 1999</i>, 374--380.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1287925</ref_obj_id>
				<ref_obj_pid>1287835</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Akyuz A. O. and Reinhard E. 2007. Noise Reduction in High Dynamic Range Imaging. <i>Journal of Visual Communication and Image Representation 2007</i>, 18(5), 366--376.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Yao W., Li Z. G., Rahardja S., Yao S. S., and Zheng J. H. 2010. Half-quadratic Regularization Based De-noising for High Dynamic Range Image Ssynthesis. <i>IEEE Int. Conf. on Acoustics Speech and Signal Processing 2010</i>, 18(5), 1370--1373.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 2D Denoising Factor for High Dynamic Range Imaging Zijian Zhu, Zhengguo Li, and Susanto Rahardja* Pasi 
Fr¨ anti Institute for Infocomm Research University of Eastern Finland Figure 1: HDR image generated 
by using (a) Debevec and Malik 1997, (b) Yao et al. 2010, (c) Akyuz and Reinhard 2007, and (d) the proposed 
method which not only generates better result than method (c) but also uses only 1/5 of its processing 
time. 1 Introduction The presence of noise in a high dynamic range (HDR) synthesis poses a serious degradation 
to the HDR image especially when the input images are captured at low light condition or with high sen­sitivity 
settings. Thus, a two-dimensional (2D) denoising factor is proposed to assign higher weight to a pixel 
with less noise based on both pixel luminance and image exposure. This pure temporal denoising factor 
is controlled by two key coef.cients and can pre­serves edge and .ne detail without blurring artifact. 
In addition, both memory and computation time are signi.cantly reduced com­pare to other denoising methods. 
 2 Our Approach The .rst dimension of the proposed denoising factor assigns a high weight to a pixel 
with a large luminance. Given an assumption that the noise distribution is independent of the measurement 
pixel value z, [Mitsunaga and Nayar 1999] argued that a luminance based weighting function (MN weight) 
of . = f(z)/f'(z) will achieve the best signal to noise ratio, where f(z) is the radiometric response 
function. [Akyuz and Reinhard 2007] modi.ed the MN weight by replacing pixel value with luminance value. 
A broad hat function h(z)=1 - (2z/255 - 1)12 was used to restrict the saturated pix­els which may cause 
color cast. Since the radiometric response function is usually monotonic increasing, we approximate the 
lu­minance based weight by a controllable hat function and a Hermite interpolation. Thus, we can signi.cantly 
reduce processing time on response function recovery by de.ning a new weighting factor as 1 -| z - 1|a 
, 0 = z<ß ß .(z)=1 - 3( 255-z , (1) )2 + 2( 255-z )3 ,ß = z< 255 255-ß 255-ß where two key coef.cients 
are the denoising strength coef.cient a and the saturation control coef.cient ß. It can be seen the smaller 
the value of a, the hat function will be steeper and this will result in better denoising effect. A large 
a gives high weights to small value (luminance) pixels, which keeps noise in the synthesized HDR im­age. 
We choose a =2 in the experiments. And test also shows that a = 12 generates similar result as [Akyuz 
and Reinhard 2007]. The saturation control coef.cient limits the near saturated pixels to avoid color 
cast due to gamut limitations (an empirical value ß = 200 is used here). The second dimension of the 
proposed denoising factor is based on exposure time. More photons reach the camera sensor with a longer 
exposure time (.t), which results in a more accurate reading. Thus, the proposed 2D weighting factor 
is designed to multiply the ge­ometrically normalized exposure times with the luminance based *e-mail: 
{zhuzj, ezgli, rsusanto}@i2r.a-star.edu.sg e-mail: franti@cs.joensuu..  denoising factor as  P W (z, 
.tj )=.tj / P.tp · .(z), (2) p=1 where j denotes the jth image in the total P input images. The ge­ometrical 
normalization avoids overwhelming big weights caused by some very large exposure time. Then, the objective 
function, N P (3) O ={W (z, .tj )[Zij ) - ln Ei - ln .tj]}2 i=1j=1z=Zmax-1 ii +.[W (z, max(.t1, ..., 
.tP ))g(z)]2 , z=Zmin+1 is used to calculate camera response function (g) and synthesize the clean HDR 
image. The proposed denoising method is veri.ed by comparing it with three HDR synthesis methods. The 
noise is signi.cantly reduced as compared to [Debevec and Malik 1997] with the same process­ing time. 
No blur artifact is generated as [Yao et al. 2010] and they are achieved due to spatial averaging. Comparing 
with [Akyuz and Reinhard 2007], the quality of the proposed is about the same. However, the proposed 
is achieved with only 1/5 of the processing time, since no intermediate steps for approximating response 
func­tion is required. References DEBEVEC, P. E. AND MALIK, J. 1997. Rendering high dynamic range radiance 
maps from photographs. In Proceedings of SIGGRAPH 1997, 369 378. MITSUNAGA T. AND NAYAR S.K. 1999. Radiometric 
Self Calibration. IEEE Conf. on Computer Vision and Pattern Recognition 1999, 374 380. AKYUZ A.O. AND 
REINHARD E. 2007. Noise Reduction in High Dynamic Range Imaging. Journal of Visual Communication and 
Image Representation 2007, 18(5), 366 376. YAO W., LI Z.G., RAHARDJA S., YAO S.S., AND ZHENG J.H. 2010. 
Half­quadratic Regularization Based De-noising for High Dynamic Range Image Ssyn­thesis. IEEE Int. Conf. 
on Acoustics Speech and Signal Processing 2010, 18(5), 1370 1373. Copyright is held by the author / owner(s). 
SIGGRAPH 2012, Los Angeles, California, August 5 9, 2012. ISBN 978-1-4503-1435-0/12/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2342957</article_id>
		<sort_key>610</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>47</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Automatic music video generating system by remixing existing contents in video hosting service based on hidden Markov model]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2342896.2342957</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2342957</url>
		<abstract>
			<par><![CDATA[<p>User-generated music video clip called <i>MAD movie</i>, which is a derivative (mixture or combination) of some original video clips, are gaining popularity on the web and a lot of them have been uploaded and are available on video hosting web services. Such a MAD music video clip consists of audio signals and video frames taken from other original video clips. In a MAD video clip, good music-to-image synchronization with respect to rhythm, impression, and context is important. Although it is easy to enjoy watching MAD videos, it is not easy to generate them. It is because a creator needs high-performance video editing software and spends a lot of time for editing video. Additionally, a creator is required video editing skill. DanceReProducer (Nakano et al [2011]) is a dance video authoring system that can automatically generate dance video appropriate to music by reusing existing dance video sequences. It trains correspondence relationship between music and video. However, DanceReProducer cannot train video sequence information because it only trains one-bar correspondence relationship. So we improved DanceReProducer to consider video sequence information by using Markov chain of latent variable and Forward Viterbi algorithm.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3737356</person_id>
				<author_profile_id><![CDATA[81504683299]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Hayato]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ohya]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Waseda University, Shinjuku, Tokyo, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[hayato-o@ruri.waseda.jp]]></email_address>
			</au>
			<au>
				<person_id>P3737357</person_id>
				<author_profile_id><![CDATA[81504685727]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Shigeo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Morishima]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Waseda University, Shinjuku, Tokyo, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[shigeo@waseda.jp]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[T. Nakano, S. Murofushi, M. Goto, and S. Morishima. 2011. DanceReProducer: an Automatic Mashup Music Video Generation System by Reusing Dance Video Clips on the Web. <i>Proceedings of the SMC 2011</i>, pp.183--189, July 2011.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Automatic Music Video Generating System by Remixing Existing Contents in Video Hosting Service Based 
on Hidden Markov Model  Hayato Ohya Waseda University Shinjuku, Tokyo, Japan hayato-o@ruri.waseda.jp 
 Shigeo Morishima Waseda University Shinjuku, Tokyo, Japan shigeo@waseda.jp 1. Introduction User-generated 
music video clip called MAD movie, which is a derivative (mixture or combination) of some original video 
clips, are gaining popularity on the web and a lot of them have been uploaded and are available on video 
hosting web services. Such a MAD music video clip consists of audio signals and video frames taken from 
other original video clips. In a MAD video clip, good music-to-image synchronization with respect to 
rhythm, impression, and context is important. Although it is easy to enjoy watching MAD videos, it is 
not easy to generate them. It is because a creator needs high-performance video editing software and 
spends a lot of time for editing video. Additionally, a creator is required video editing skill. DanceReProducer 
(Nakano et al [2011]) is a dance video authoring system that can automatically generate dance video appropriate 
to music by reusing existing dance video sequences. It trains correspondence relationship between music 
and video. However, DanceReProducer cannot train video sequence information because it only trains one-bar 
correspondence relationship. So we improved DanceReProducer to consider video sequence information by 
using Markov chain of latent variable and Forward Viterbi algorithm. 2. System Overview Figure 1 shows 
our system s flow chart. Our system consists of three parts. The first is database construction part. 
The second is model training part. The last one is video generation part. In the database construction 
part, music feature and video feature are extracted from existing video contents. Then each feature is 
collected per one-bar. In the model training part, each feature is clustered for Hidden Markov Model 
(HMM) and HMM parameters are trained by Markov chain. In the video generation part, music feature and 
tempo are extracted from music we want to attach video on (input music) and feature is collected by bar-level 
feature. Then decide video sequence by Forward Viterbi algorithm and using HMM parameter calculated from 
previous part. Considering state of previous bar video feature in addition to current bar music feature 
in estimating one-bar feature of video enables video generation training video sequence. 3. Training 
by Markov Chain Training model of music and video feature is Markov chain of latent variable. Automaton 
is separated per one-bar, observation X is bar-level music feature and latent variable Z is bar-level 
video feature. Additionally, state Y is the cluster of bar-level video feature that is clustered by k-means 
clustering. Initial state probability and state transition probability  from i to j are calculated 
      Figure 1. System s flow chart.     Figure 2. Percentage of all video time of each assessment 
time. 4. Subjective Assessment Experiment We generated each four videos from four input music by Dance-ReProducer 
and our method. Number of videos in database is 90. Then we prepared 12 videos in total in addition to 
four user-generated videos that are generated by using the same music as input music. View count of the 
user-generated videos is over 50,000. This number can be considered as high-quality video. The examinees 
who are 15 men in 20s watched 12 videos and assessed synchronization-level between music and video. Synchronization-level 
is four step values, in order of increasing level; not synchronize , a little synchronize , synchronize 
, very synchronize . Assessment data has time information and its assessment. Figure 2 shows percentage 
of all video time of each assessment time in the experiment. 5. Result and Conclusions Our method enabled 
video generation considering not only music feature but also video sequence information. At the result 
of subjective assessment experiment, comparing our method with DanceReProducer, it turns out that synchronize 
has increased and a little synchronize has decreased. This verified that considering video sequence information 
increases synchronization between music and video. Remaining issues, such as feature extraction in detail, 
will be topics in our future work. References T. NAKANO, S. MUROFUSHI, M. GOTO, AND S. MORISHIMA. 2011. 
DanceReProducer: an Automatic Mashup Music Video Generation System by Reusing Dance Video Clips on the 
Web. Proceedings of the SMC 2011, pp.183 189, July 2011. 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2342958</article_id>
		<sort_key>620</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>48</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Calligraphic cutting]]></title>
		<subtitle><![CDATA[extreme image resizing with cuts in continuous domain]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2342896.2342958</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2342958</url>
		<abstract>
			<par><![CDATA[<p>Seam carving [Avidan and Shamir 2007; Rubinstein et al. 2009] revolutionized the way we think about image resizing by demonstrating that it is possible to obtain significant changes in image sizes with changes in proximity relationships, which we call topological properties of an image. Seam carving can change the size of an image by progressively carving out (or carving in) seams, which are monotonically connected paths of low-energy pixels crossing an image from top to bottom, or from left to right. Unfortunately, it quickly became obvious that seam carving creates geometric discontinuities once low-energy regions start to diminish. As a result, improvements and alternative approaches have been suggested to minimize discontinuities.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3737358</person_id>
				<author_profile_id><![CDATA[81504685409]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Youyou]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Texas A&M University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737359</person_id>
				<author_profile_id><![CDATA[81100124987]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Ergun]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Akleman]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Texas A&M University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1276390</ref_obj_id>
				<ref_obj_pid>1276377</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Avidan, S., and Shamir, A. 2007. Seam carving for content-aware image resizing. <i>ACM Trans. Graph. 26</i> (July), 10.1--10.8.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1531329</ref_obj_id>
				<ref_obj_pid>1531326</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Rubinstein, M., Shamir, A., and Avidan, S. 2009. Multi-operator media retargeting. <i>ACM Trans. Graph 28</i> (July), 23:1--23:11.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Calligraphic Cutting: Extreme Image Resizing with Cuts in Continuous Domain Youyou Wang Ergun Akleman 
Department of Computer Science Visualization Department Texas A&#38;M University Texas A&#38;M University 
 Original Image 33% scaling Seam carving (SC) Improved SC Our method Figure 1: Comparison of our method 
with scaling and seam carving after resizing an image with no low-energy regions. Scaling does not cause 
discontinuities but does change curvatures and slopes. Seam carving (SC) causes severe discontinuities 
under extreme resizing. Improved seam carving (ISC) does not eliminate most of these discontinuities. 
Our calligraphic cutting (CC) can resize the image by preserving geometric properties of the original 
image without causing severe discontinuities. The method removes high-energy regions e.ectively by changing 
topological properties. Note, for instance, the pointed arch above the main entrance door, which moved 
down to keep its shape and revealed the background part of the facade automatically. Seam carving [Avidan 
and Shamir 2007; Rubinstein et al. 2009] revolutionized the way we think about image resiz­ing by demonstrating 
that it is possible to obtain signi.cant changes in image sizes with changes in proximity relation­ships, 
which we call topological properties of an image. Seam carving can change the size of an image by progressively 
carving out (or carving in) seams, which are monotonically connected paths of low-energy pixels crossing 
an image from top to bottom, or from left to right. Unfortunately, it quickly became obvious that seam 
carving creates geometric discon­tinuities once low-energy regions start to diminish. As a result, improvements 
and alternative approaches have been suggested to minimize discontinuities. In this work, we show that 
by reformulating the concept of seam carving in continuous domain, it is theoretically pos­sible to eliminate 
most of the distortions and discontinu­ities caused by seam carving. Our theoretical approach does not 
require any user interaction, any importance or saliency map, or any complicated energy function. We 
simply use the original energy function of seam carving. The paper has three theoretical contributions: 
(1) generalization of seams to calligraphic cuts, (2) identi.cation of the conditions to preserve tangent 
continuity, and (3) cutting through mini­mum and maximum points to resize shapes. The calligraphic cuts 
are monotonically connected paths like seams. Nevertheless, calligraphic cuts have one signi.cant advantage 
over seams. The discretized versions of calli­graphic cuts can include a set of pixels that form horizon­tal 
lines for top-to-bottom cuts or vertical lines for left-to­right cuts. Therefore, they can follow the 
contours of shapes and can e.ectively reach and remove all possible low-energy regions. Seams do not 
have this ability and eventually re­move high-energy regions even when there exist low-energy regions. 
Thus, discontinuities caused by seam carving can be avoided by calligraphic cutting. Our method also 
preserve the original G1 i.e. tangent continuities. We show that avoiding only low-energy regions is 
necessary but not su.cient since doing so can only pre­serve G0 continuity. G1 continuity preservation 
requires an additional condition: the calligraphic cuts must be perpen­dicular to the gradient everywhere 
except in minimum and maximum points. The minimum and maximum points in high-energy regions usually have 
very high curvature, and they already look G1 discontinuous in original images the most human-made objects, 
such as roofs of the buildings, usually have such discontinuous maximum points. The cuts that pass through 
such discontinuous points actually pre­serve the visual structure of the original shape. Therefore, exploiting 
this property, we can continue to apply calli­graphic cuts even long after all low-energy regions are 
elim­inated. References Avidan, S., and Shamir, A. 2007. Seam carving for content-aware image resizing. 
ACM Trans. Graph. 26 (July), 10.1 10.8. Rubinstein, M., Shamir, A., and Avidan, S. 2009. Multi­ operator 
media retargeting. ACM Trans. Graph 28 (July), 23:1 23:11. Copyright is held by the author / owner(s). 
SIGGRAPH 2012, Los Angeles, California, August 5 9, 2012. ISBN 978-1-4503-1435-0/12/0008 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2342959</article_id>
		<sort_key>630</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>49</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[Compressive light field photography]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2342896.2342959</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2342959</url>
		<abstract>
			<par><![CDATA[<p>Light field cameras, e.g. [Lytro 2012; Veeraraghavan et al. 2007], have ushered a new direction in photography allowing consumers to synthesize photographs with novel viewpoints or varying focus after the actual recording. Unfortunately, current light field camera designs impose a fixed trade-off between spatial and angular resolution --- spatial resolution is reduced to capture angular light variation on the sensor. We introduce a principled computational framework and a new camera design to acquire and reconstruct light fields at full spatial and angular resolution from a single exposure. Our framework introduces a high-dimensional sparse basis for light fields learned from millions of light fields patches. The same optimization procedure also allows for the synthesis of optimal mask patterns that are mounted at a slight offset in front of the sensor and optically attenuate the light field before it is recorded by the sensor. Finally, a weighted compressive sensing-style reconstruction is performed to recover the light field. We demonstrate, in theory and with simulations, how our compressive approach to light field photography outperforms state-of-art techniques.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3737360</person_id>
				<author_profile_id><![CDATA[81504688520]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Kshitij]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Marwah]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MIT Media Lab]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737361</person_id>
				<author_profile_id><![CDATA[81335499586]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Gordon]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wetzstein]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MIT Media Lab]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737362</person_id>
				<author_profile_id><![CDATA[81300333501]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Ashok]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Veeraraghavan]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Rice University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737363</person_id>
				<author_profile_id><![CDATA[81100022847]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Ramesh]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Raskar]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MIT Media Lab]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[E. Candes and Y. Eldar and D. Needell and P. Randall. 20010. Compressed sensing with coherent and redundant dictionaries. <i>Harmonic Analysis</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Lytro, I., 2012. Lytro Light Field Camera.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1553463</ref_obj_id>
				<ref_obj_pid>1553374</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Mairal, J., Bach, F., Ponce, J., and Sapiro, G. 2009. Online dictionary learning for sparse coding. <i>International Conference on Machine Learning</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1276463</ref_obj_id>
				<ref_obj_pid>1275808</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Veeraraghavan, A., Raskar, R., Agarwal, A., Mohan, A., and Tumblin, J. 2007. Dappled photography: Mask enhanced cameras for heterodyned light fields and coded aperture refocusing. <i>ACM Siggraph</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Compressive Light Field Photography Kshitij Marwah1 Gordon Wetzstein1 Ashok Veeraraghavan2 Ramesh Raskar1 
1MIT Media Lab 2Rice University Figure 1: Compressive light .eld acquisition using joint optical light 
coding and compressive computational reconstruction. Using machine learning techniques, we extract the 
essence of natural light .elds from archives and store them in overcomplete dictionaries (left). The 
same optimization procedure also generates optimal attenuation mask patterns that are physically mounted, 
at a slight offset, in front of a camera sensor (center left). These masks create a dappled pseudo-random 
pattern in captured photographs (center). From a precomputed dictionary and a single sensor image, we 
then reconstruct a full-resolution light .eld that contains slightly different viewpoints of a scene 
(right) and allows a photograph to be refocused after the fact (see supplement). Introduction and Overview 
Light .eld cameras, e.g. [Lytro 2012; Veeraraghavan et al. 2007], have ushered a new direction in photography 
allowing consumers to synthesize photographs with novel viewpoints or varying focus after the actual 
recording. Un­fortunately, current light .eld camera designs impose a .xed trade­off between spatial 
and angular resolution spatial resolution is reduced to capture angular light variation on the sensor. 
We in­troduce a principled computational framework and a new camera design to acquire and reconstruct 
light .elds at full spatial and an­gular resolution from a single exposure. Our framework introduces 
a high-dimensional sparse basis for light .elds learned from mil­lions of light .elds patches. The same 
optimization procedure also allows for the synthesis of optimal mask patterns that are mounted at a slight 
offset in front of the sensor and optically attenuate the light .eld before it is recorded by the sensor. 
Finally, a weighted compressive sensing-style reconstruction is performed to recover the light .eld. 
We demonstrate, in theory and with simulations, how our compressive approach to light .eld photography 
outper­forms state-of-art techniques. Sparse Coding of Light Fields Light .elds can be thought of as 
a collection of slightly different perspectives of the same scene that vary over the size of the camera 
aperture. As these viewpoints have a very narrow baseline, they contain a large amount of redundancy. 
This redundancy can be computationally exploited using a variety of different bases commonly used for 
redundant or sparse coding, including data-independent bases such as Fourier transforms, con­tourlets, 
bandelets, and Hadamard codes as well as data-dependent functions such as principal components or overcomplete 
dictionar­ies. In the signal processing literature, learned overcomplete dic­tionaries are commonly considered 
the best approach for compres­sive sensing [Mairal et al. 2009]. We consider millions of light .eld patches 
from various light .eld archives that each encode the essence of depth variations and occlusions in natural 
scenes. This basis is about 100× overcomplete in which each light .eld patch is represented as a linear 
combination of a few dictionary elements. Compressive Reconstruction By placing a mask on the sen­sor 
we optically modulate the light rays incident on the sensor; the mask creates a dappled pattern in the 
sensor image. The pattern of modulation is learned simultaneously with the dictionary to be the most 
incoherent physically probable sensing matrix for capture. To reconstructing the light .eld back we employ 
a weighted L1­minimization (see [E. Candes and Y. Eldar and D. Needell and P. Randall 20010] for more 
details) on the dappled sensor image as: arg min a 1 (1) a s.t. Isensor - FM Da 2 = , where FM is the 
measurement matrix modeling the attenuation mask and angular integration of the sensor, Isensor is the 
captured sensor image, D is the overcomplete dictionary, and a is the un­known vector with sparse coef.cients. 
The light .eld is represented as the combination of a sparse set of coef.cients and corresponding elements 
in the dictionary Da. Results and Discussion We learned an overcomplete dictionary for patch size 16 
× 16 in space and 5 × 5 viewpoints from synthetic light .elds rendered with a raytracer and also reparameterized 
light .elds from the Stanford Light Field Archive. Our dictionary con­tains about 100,000 elements with 
each patch being represented in no more than ten coef.cients. We tested our reconstruction algo­rithm 
using a new light .eld that was not part of the training dataset, which is simulated to be captured with 
the learned mask pattern in front of the sensor (Fig. 1). The sensor image is then used to recover the 
light .eld, which includes multiple viewpoints and can be used to refocus a photograph, at a higher resolution 
than conventional methods, after it is captured (see supplement). Our approach has the potential to signi.cantly 
improve the resolu­tion of next-generation computational cameras, which are already emerging on the consumer 
market. References E. CANDES AND Y. ELDAR AND D. NEEDELL AND P. RANDALL. 20010. Com­pressed sensing with 
coherent and redundant dictionaries. Harmonic Analysis. LYTRO, I., 2012. Lytro Light Field Camera. MAIRAL, 
J., BACH, F., PONCE, J., AND SAPIRO, G. 2009. Online dictionary learn­ing for sparse coding. International 
Conference on Machine Learning. VEERARAGHAVAN, A., RASKAR, R., AGARWAL, A., MOHAN, A., AND TUMBLIN, J. 
2007. Dappled photography: Mask enhanced cameras for heterodyned light .elds and coded aperture refocusing. 
ACM Siggraph. Copyright is held by the author / owner(s). SIGGRAPH 2012, Los Angeles, California, August 
5 9, 2012. ISBN 978-1-4503-1435-0/12/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2342960</article_id>
		<sort_key>640</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>50</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>5</seq_no>
		<title><![CDATA[Computational cellphone microscopy]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2342896.2342960</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2342960</url>
		<abstract>
			<par><![CDATA[<p>Within the last few years, cellphone subscriptions have widely spread and now cover even the remotest parts of the planet. Adequate access to healthcare, however, is not widely available, especially in developing countries. We propose a new approach to converting cellphones into low-cost scientific devices for microscopy. Cellphone microscopes have the potential to revolutionize health-related screening and analysis for a variety of applications, including blood and water tests. Our optical system is more flexible than previously proposed mobile microscopes and allows for wide field of view panoramic imaging, the acquisition of parallax, and coded background illumination, which optically enhances the contrast of transparent and refractive specimens.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3737364</person_id>
				<author_profile_id><![CDATA[81504683951]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Aydin]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Arpa]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MIT Media Lab]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737365</person_id>
				<author_profile_id><![CDATA[81335499586]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Gordon]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wetzstein]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MIT Media Lab]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737366</person_id>
				<author_profile_id><![CDATA[81319495323]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Douglas]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lanman]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MIT Media Lab]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737367</person_id>
				<author_profile_id><![CDATA[81548005482]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Ramesh]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Raskar]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Breslauer, D. N., Maamari, R. N., Switz, N. A., Lam, W. A., and Fletcher, D. A. 2009. Mobile Phone Based Clinical Microscopy for Global Health Applications. <i>PLoS ONE 4</i>, 7 (07), e6320.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Smith, Z. J., Chu, K., Espenson, A. R., Rahimzadeh, M., Gryshuk, A., Molinaro, M., Dwyre, D. M., Lane, S., Matthews, D., and Wachsmann-Hogiu, S. 2011. Cell-phone-based platform for biomedical device development and education applications. <i>PLoS ONE 6</i>, 3 (03), e17150.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Tseng, D., Mudanyali, O., Oztoprak, C., Isikman, S. O., Sencan, I., Yaglidere, O., and Ozcan, A. 2010. Lensfree Microscopy on a Cellphone. <i>Lab Chip 10</i>, 1787--1792.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Zhu, H., Yaglidere, O., Su, T.-W., Tseng, D., and Ozcan, A. 2011. Cost-effective and compact wide-field fluorescent imaging on a cell-phone. <i>Lab Chip 11</i>, 315--322.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Computational Cellphone Microscopy Aydin Arpa Gordon Wetzstein Douglas Lanman Ramesh Raskar MIT Media 
Lab  Figure 1: (Left) Illustration of our cellphone microscope in the .eld. The data captured by this 
versatile and low-cost platform can either be analyzed directly on the phone or remotely, for instance 
by a medical doctor in a hospital. (Center) Our prototype consists of a standard cellphone camera, a 
secondary lens on a mount that is directly placed on the microscopic sample, and background illumination, 
for instance provided by an LED or a secondary cellphone display. Optical magni.cation is achieved by 
mounting a small lenslet at its focal length to the sample. (Right) Microscopic images showing stained 
apple cells, sea salt crystals, and onion cells. Abstract Within the last few years, cellphone subscriptions 
have widely spread and now cover even the remotest parts of the planet. Adequate access to healthcare, 
however, is not widely available, especially in developing countries. We propose a new approach to converting 
cellphones into low-cost scienti.c devices for mi­croscopy. Cellphone microscopes have the potential 
to revolution­ize health-related screening and analysis for a variety of applica­tions, including blood 
and water tests. Our optical system is more .exible than previously proposed mobile microscopes and allows 
for wide .eld of view panoramic imaging, the acquisition of paral­lax, and coded background illumination, 
which optically enhances the contrast of transparent and refractive specimens. Overview Today, an estimated 
six billion cellphone subscriptions exist worldwide with about 70% of those in developing countries (www.itu.int/ict/statistics). 
However, developing countries often suffer from a lack of access to adequate healthcare, which is party 
due to the cost and training associated with high-tech scienti.c in­struments required for medical analysis. 
We present a low-cost portable microscope that uses a cellphone camera and a simple, sec­ondary lens 
that is placed on top of the specimen. As illustrated in Figure 1, our device can be used in the .eld, 
for instance to analyze water sources for potential contamination, and can either directly process the 
captured data or transmit it wirelessly for remote pro­cessing. Cellphone microscopes provide a unique 
opportunity to make disease diagnosis and healthcare accessible to everyone, even in remote and undeveloped 
parts of the world. Based on their optical setup, cellphone microscopes can be cate­gorized into three 
methodologies: on-chip analysis, off-chip clip­on methodology, and on-lens approaches. The .rst category, 
on­chip analysis , requires major, intrusive modi.cations to cellphone hardware [Tseng et al. 2010]. 
Furthermore, associated holographic imaging requires standard photographs to be reconstructed from captured 
fringe patterns. The second approach, off-chip clip-on , requires additional hardware attachments to 
be mounted on the cell­phone [Breslauer et al. 2009; Zhu et al. 2011]. Due to the varying dimensions 
of different cellphone models, however, a clip-on at­tachment usually only works with a speci.c model 
and also .xes the relative viewpoint of the specimen. The third methodology of cellphone microscopy can 
be described as an on-lens approach [Smith et al. 2011], where a refractive optical element is directly 
attached to the camera lens. In this work, we introduce practical, low-cost, single lens off-chip computational 
microscopy using cellphone cameras. Our approach is unique in its optical design: a single lens is placed, 
separated by its focal length, on a microscopic sample and directly imaged from a detached camera phone, 
which allows different viewpoints of the sample to be recorded. We further demonstrate that an additional 
cell phone display can be used to provide structured background il­lumination, which optically enhances 
the contrast of the observed specimen. The focus of this paper is to make .eld microscopy prac­tical 
and cost-effective at the same time. Results and Discussion As seen in Figure 1 (right), we cap­ tured 
a variety of different microscopic specimens with our system. In contrast to other cellphone microscopes, 
our optical lens is not rigidly attached to the cellphone. Therefore, our approach facili­tates wide 
.eld-of-view panoramas of microscopic samples to be captured (see supplemental video). We employ computational 
il­lumination, speci.cally a Schlieren imaging setup, in order to am­plify the contrast of refractive 
transparent specimen. In summary, we have presented a new approach for low-cost cellphone-based microscopy. 
Our setup uniquely combines the characteristics of be­ing cost-effective, non-intrusive, .exible, requiring 
minimal post­processing, and allowing computational illumination. References BRESLAUER, D. N., MAAMARI, 
R. N., SWITZ, N. A., LAM, W. A., AND FLETCHER, D. A. 2009. Mobile Phone Based Clinical Microscopy for 
Global Health Applications. PLoS ONE 4, 7 (07), e6320. SMITH, Z. J., CHU, K., ESPENSON, A. R., RAHIMZADEH, 
M., GRYSHUK, A., MOLINARO, M., DWYRE, D. M., LANE, S., MATTHEWS, D., AND WACHSMANN-HOGIU, S. 2011. Cell-phone-based 
platform for biomedical de­vice development and education applications. PLoS ONE 6, 3 (03), e17150. TSENG, 
D., MUDANYALI, O., OZTOPRAK, C., ISIKMAN, S. O., SENCAN, I., YAGLIDERE, O., AND OZCAN, A. 2010. Lensfree 
Microscopy on a Cellphone. Lab Chip 10, 1787 1792. ZHU, H., YAGLIDERE, O., SU, T.-W., TSENG, D., AND 
OZCAN, A. 2011. Cost­effective and compact wide-.eld .uorescent imaging on a cell-phone. Lab Chip 11, 
315 322. Copyright is held by the author / owner(s). SIGGRAPH 2012, Los Angeles, California, August 5 
 9, 2012. ISBN 978-1-4503-1435-0/12/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2342961</article_id>
		<sort_key>650</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>51</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>6</seq_no>
		<title><![CDATA[Computational retinal imaging via binocular coupling and indirect illumination]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2342896.2342961</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2342961</url>
		<abstract>
			<par><![CDATA[<p>The retina is a complex light-sensitive tissue that is an essential part of the human visual system. It is unique, as it can be optically observable with non-invasive methods through the eye's transparent elements. This has inspired a long history of retinal imaging devices for examination of optical function [Van Trigt 1852; Yates 2011] and for diagnosis of many of the diseases that manifest in the retinal tissue, such as diabetic retinophathy, hypertension, HIV/AIDS related retinitis, and age-related macular degeneration. These conditions are some of leading causes of blindness, especially in the developing world, but can often be prevented if screened and diagnosed in early stages.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3737368</person_id>
				<author_profile_id><![CDATA[81487645512]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Everett]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lawson]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MIT Media Lab]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737369</person_id>
				<author_profile_id><![CDATA[81361605701]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Jason]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Boggess]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MIT Media Lab]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737370</person_id>
				<author_profile_id><![CDATA[81504685987]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Siddharth]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Khullar]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MIT Media Lab]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737371</person_id>
				<author_profile_id><![CDATA[81309497296]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Alex]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Olwal]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MIT Media Lab]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737372</person_id>
				<author_profile_id><![CDATA[81335499586]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Gordon]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wetzstein]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MIT Media Lab]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737373</person_id>
				<author_profile_id><![CDATA[81100022847]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Ramesh]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Raskar]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MIT Media Lab]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Abr&#224;moff, M., Garvin, M., and Sonka, M. 2010. Retinal imaging and image analysis. <i>Biomedical Engineering, IEEE Reviews in 3</i>, 169--208.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Keeler, C. 1997. 150 years since babbage's ophthalmoscope. <i>Archives of ophthalmology 115</i>, 11, 1456.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Van Trigt, A. 1852. De oogspiegel. <i>Nederlandisch Lancet, third series, Utrecht 1853</i>, 417--509.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Yates, Paul Andrew; Tran, K., 2011. Hand-held portable fundus camera for screening photography, March.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Computational Retinal Imaging via Binocular Coupling and Indirect Illumination Everett Lawson Jason 
Boggess Siddharth Khullar Alex Olwal Gordon Wetzstein Ramesh Raskar MIT Media Lab  Figure 1: How can 
we capture images of the human retina using a standalone, hand-held and user-interactive device? Our 
system comprises a unique co-design of a low-cost CMOS camera, programmable stimulus control, and indirect 
diffusive illumination to create an interactive and portable alternative to conventional retinal (fundus) 
imaging devices. Using this imaging platform, we add the dimension of user­interaction to the .eld of 
wireless health and tele-medicine by enabling unsupervised and ef.cient retinal imaging for measuring 
longitudinally modulating parameters within the human retina. Introduction and Overview The retina is 
a complex light­sensitive tissue that is an essential part of the human visual sys­tem. It is unique, 
as it can be optically observable with non-invasive methods through the eye s transparent elements. This 
has inspired a long history of retinal imaging devices for examination of op­tical function [Van Trigt 
1852; Yates 2011] and for diagnosis of many of the diseases that manifest in the retinal tissue, such 
as di­abetic retinophathy, hypertension, HIV/AIDS related retinitis, and age-related macular degeneration. 
These conditions are some of leading causes of blindness, especially in the developing world, but can 
often be prevented if screened and diagnosed in early stages. Unfortunately, the majority of retinal 
imaging devices employ high quality optical elements for illumination and observation and re­quire precise 
alignment with the eye [Keeler 1997; Abr` amoff et al. 2010]. Available devices are bulky, expensive, 
and virtually not ac­ cessible in developing countries. We present an inexpensive, self­directed, interactive 
device to capture and visualize images of the retina (Figure 1). For this purpose, we exploit a combination 
of two novel ideas to overcome the alignment and illumination require­ments of traditional devices. First, 
we use indirect diffuse illumi­nation in a small form factor close to the eye. This avoids the chal­lenges 
of direct pupillary illumination via focused beams. Second, we put the user in the loop and exploit the 
natural biological cou­pling of binocular vision. We use a close-up display on one eye to help the user 
self-align the pupil. In addition, we lock the gaze and focus of the same eye for convenient imaging 
of the other eye. Approach Our prototypes use ultrabright 1W dichromatic white LEDs with a luminous ef.cacy 
of 120lm/W mounted near the user s temple, to indirectly illuminate the retina through the tis­sue. The 
image of the illuminated retina is projected out through the eye, where it is captured by a camera for 
a real-time live video feed. In our standalone prototypes, which .t in a pair of modi­.ed glasses, we 
provide the user, or an observer, with a live view of the retina. To capture wide .eld-of-view retinal 
panoramas, our other prototypes use a tethered camera and software for real-time processing and image 
stitching. Our software exploits binocular coupling to gaze-lock the eyes, such that the rotation of 
the test eye can be computationally controlled through stimulus patterns shown to the display eye (see 
Fig. 1). Potential Impact and Discussion Many of the leading diseases, on a global scale, of both the 
eye and the body manifest on and within the substructures of the retinal lining. As an optical system, 
the human eye allows for direct screening using non-invasive imag­ing technology. Unfortunately, todays 
retinal imaging devices are bulky, expensive, and require highly trained ophthalmologists and specialized 
capture conditions. Our system is compact, low-cost, requires no moving parts, and utilizes a novel combination 
of indi­rect diffuse illumination through the side of the eye coupled with, a camera, and a computationally 
controlled display, which requires no special training, can be a self-examination, and enables retinal 
imaging into new form of graphic renderings. With our work, we demonstrate the ability to use computational 
imaging and interactive techniques for low-cost retinal imaging via binocular coupling and indirect illumination. 
Although these are only the .rst steps toward a robust diagnostic tool, the positive feedback we have 
received from ophthamalogists indicates great promise and many future opportunities to map graphics research 
to this important health task. We hope our work will inspire further graphics and user interaction research 
for modeling human anatomy with low-cost devices that will eventually impact global health in remote 
parts of the world. References ABR ` AMOFF, M., GARVIN, M., AND SONKA, M. 2010. Retinal imaging and image 
analysis. Biomedical Engineering, IEEE Reviews in 3, 169 208. KEELER, C. 1997. 150 years since babbage 
s ophthalmoscope. Archives of ophthal­mology 115, 11, 1456. VAN TRIGT, A. 1852. De oogspiegel. Nederlandisch 
Lancet, third series, Utrecht 1853, 417 509. YATES, PAUL ANDREW; TRAN, K., 2011. Hand-held portable fundus 
camera for screening photography, March. Copyright is held by the author / owner(s). SIGGRAPH 2012, Los 
Angeles, California, August 5 9, 2012. ISBN 978-1-4503-1435-0/12/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2342962</article_id>
		<sort_key>660</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>52</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>7</seq_no>
		<title><![CDATA[CosmicAI]]></title>
		<subtitle><![CDATA[generating sky backgrounds through content-based search and flexible composition]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2342896.2342962</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2342962</url>
		<abstract>
			<par><![CDATA[<p>In film and game production, sky images are frequently changed to fit a given situation. <i>SkyFinder</i> [Tao et al. 2009] and <i>A System for Editing Sky Images Using an Image Database</i> (<i>Ono2011</i> for short hereafter) [Ono et al. 2011] allow the users to generate sky images efficiently without three dimensional computer graphics software such as <i>Vue</i> and <i>Terragen. SkyFinder</i> users specify related attributes (category, layout, horizon height, sun position, and richness) so as to easily search sky images downloaded from Flickr.com. <i>Ono2011</i> lets the users to take two steps. The first step is to generate a background with atmospherically-distributed sky color through an intuitive user interface. The second step is to compose clouds using a collection of real photographs downloaded from Flicker.com. The detailed user actions are to specify related features (sky color, cloud shape, amount of edges, and sun position); to select clouds in the search results by a paint-like operation; and to compose them into the original background. It looks like the quality of sky images generated by these two systems becomes unstable, because many public photographs are unsuitable for composition (See Fig. 2(a)).</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3737374</person_id>
				<author_profile_id><![CDATA[81504683379]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Takanobu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Mitani]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[mitani@fj.ics.keio.ac.jp]]></email_address>
			</au>
			<au>
				<person_id>P3737375</person_id>
				<author_profile_id><![CDATA[81100355096]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Issei]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Fujishiro]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[fuji@ics.keio.ac.jp]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>2077426</ref_obj_id>
				<ref_obj_pid>2077378</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Ono, A., Dobashi, Y., and Yamamoto, T. 2011. A System for Editing Sky Images Using an Image Database. In <i>Proceedings of SIGGRAPH Asia 2011 Sketches</i>, Article 38.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1531374</ref_obj_id>
				<ref_obj_pid>1531326</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Tao, L., Yuan, L., and Sun, J. 2009. Attribute-based sky image search. <i>ACM Transactions on Graphics 28</i>, 3, Article 68.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 CosmicAI: Generating Sky Backgrounds Through Content-Based Search and Flexible Composition   Takanobu 
MITANI and Issei FUJISHIRO Keio University  Figure 1. (a) 1-Click search (with 1 attribute). (b) Active 
composition (sketch-based search and automatic arrangement). (c) Attribute- and sketch-based search. 
(d) Background generation from panorama pictures. (e) Adjustment of size, brightness, and thickness. 
1 Introduction In film and game production, sky images are frequently changed to fit a given situation. 
SkyFinder [Tao et al. 2009] and A System for Editing Sky Images Using an Image Database (Ono2011 for 
short hereafter) [Ono et al. 2011] allow the users to generate sky images efficiently without three dimensional 
computer graphics software such as Vue and Terragen. SkyFinder users specify related attributes (category, 
layout, horizon height, sun position, and richness) so as to easily search sky images downloaded from 
Flickr.com. Ono2011 lets the users to take two steps. The first step is to generate a background with 
atmospherically-distributed sky color through an intuitive user interface. The second step is to compose 
clouds using a collection of real photographs downloaded from Flicker.com. The detailed user actions 
are to specify related features (sky color, cloud shape, amount of edges, and sun position); to select 
clouds in the search results by a paint-like operation; and to compose them into the original background. 
It looks like the quality of sky images generated by these two systems becomes unstable, because many 
public photographs are unsuitable for composition (See Fig. 2(a)). In this study, we have developed 
a system called CosmicAI (COntent-based Search and Myriad Image Composition with Acquired Information 
for sky background generation) (The official web site address: http://cosmicai.com/ ), which utilizes 
a collection of proprietary sky part images including backgrounds, clouds and the moon, with the following 
six advantages in comparison with the use of public image collections: Ad1. Fine-grained image parts 
purely related to sky can be treated. Ad2. Prior part photographing can be optimized. Ad3. Image resolution 
can be highly and steadily maintained. Ad4. The elevation angle of each image s center is known. Ad5. 
The focal length can be confined. Ad6. The developer holds copyright of all pictures.  Figure 2. (a) 
Ono2011 v.s. CosmicAI: The difference in usage of photographs. (b) The summary of relationships among 
advantages, functions, and benefits to use CosmicAI. 2 Our Approach CosmicAI allows the users to replace 
sky background images efficiently through searching for sky parts by combining the following attributes. 
Note that the priority of them can be adjusted. ·Backgrounds: Type, chromatic dispersion, location of 
the sun, and elevation angle. ·Clouds: Type, size, height, elevation angle, average color, and chromatic 
dispersion. ·The moon: Lunar age, brightness, and elevation angle. The users are allowed to control the 
trade-off between work speed and detailed setups (See Fig. 1(a), (b), (c)). CosmicAI provides the users 
with various functions: F1. To acquire backgrounds with various configurations through the use of panoramas 
(See Fig. 1(d)); F2. To use arbitrary pictures which the users own as a background and to arrange clouds 
in the picture; F3. To generate a long sequence of time-varying images to export to other available animation 
generators; F4. To select cutout or depth-aware arrangement which transforms clouds in an optically-correct 
manner; and F5. To adjust the size, brightness, and thickness of clouds and the moon (See Fig. 1(e)). 
 We can find five benefits to use CosmicAI, which are not seen when using Ono2011 (See Fig. 2(b)): B1. 
There is no need to specify the domain of clouds. B2. The users can freely move clouds and the moon. 
B3. They can generate sky images with a sense of reality including clouds whose size and elevation angle 
are correct. B4. They can generate desirable sky images. B5. They can use the system without regard for 
copyright. References ONO, A., DOBASHI, Y., AND YAMAMOTO, T. 2011. A System for Editing Sky Images 
Using an Image Database. In Proceedings of SIGGRAPH Asia 2011 Sketches, Article 38. e-mail: {mitani@fj., 
fuji@}ics.keio.ac.jp TAO, L., YUAN, L., AND SUN, J. 2009. Attribute-based sky image search. ACM Transactions 
on Graphics 28, 3, Article 68. 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2342963</article_id>
		<sort_key>670</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>53</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>8</seq_no>
		<title><![CDATA[Effective global prediction for dense light-field compression by using synthesized multi-focus images]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2342896.2342963</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2342963</url>
		<abstract>
			<par><![CDATA[<p>Light-Field Rendering is a promising technique generating 3-D images from multi-view images captured by dense camera arrays or lens arrays [Isaksen et al. 2000]. However, Light-Field generally consists of 4-D enormous data, that are not suitable for storing or transmitting without effective compression [Magnor and Girod 2000]. We previously derived a method of reconstructing 4-D Light-Field directly from 3-D information composed of multi-focus images without any scene estimation [Kodama et al. 2006]. On the other hand, it is easy to synthesize multi-focus images from Light-Field. Therefore, we can achieve conversion between 4-D Light-Field and 3-D multi-focus images without significant degradation. Recently, researchers in computational photography also study such interesting properties of Light-Field [Levin and Durand 2010]. In this work, based on the conversion, we propose novel global prediction for dense Light-Field compression via synthesized multi-focus images as effective representation of 3-D scenes like Figure 1.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3737376</person_id>
				<author_profile_id><![CDATA[81504687038]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Takashi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sakamoto]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tokyo University of Science and National Institute of Informatics]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[sakamoto@isl.ee.kagu.tus.ac.jp; sakamoto@nii.ac.jp]]></email_address>
			</au>
			<au>
				<person_id>P3737377</person_id>
				<author_profile_id><![CDATA[81344493272]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Kazuya]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[kodama]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[National Institute of Informatics]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[kazuya@nii.ac.jp]]></email_address>
			</au>
			<au>
				<person_id>P3737378</person_id>
				<author_profile_id><![CDATA[81100489471]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Takayuki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hamamoto]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tokyo University of Science]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[hamamoto@isl.ee.kagu.tus.ac.jp]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>344929</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Isaksen, A., McMillan, L., and Gortler, S. 2000. Dynamically Reparameterized Light Fields. In <i>Proc. ACM SIGGRAPH</i>, 297--306.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Kodama, K., Mo, H., and Kubota, A. 2006. Free Viewpoint, Iris and Focus Image Generation by Using a Three-Dimensional Filtering based on Fresquency Analysis of Blurs. In <i>Proc. IEEE ICASSP</i>, vol. II, 625--628.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Kodama, K., Izawa, I., and Kubota, A. 2010. Robust Reconstruction of Arbitrarily Deformed Bokeh from Ordinary Multiple Differently Focused Images. In <i>Proc. IEEE ICIP</i>, 3989--3992.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Levin, A., and Durand, F. 2010. Linear View Synthesis Using a Dimensionality Gap Light Field Prior. In <i>Proc. IEEE CVPR</i>, 1831--1838.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2323083</ref_obj_id>
				<ref_obj_pid>2322507</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Magnor, M., and Girod, B. 2000. Data Compression for Light Field Rendering. <i>IEEE Trans. CSVT 10(3)</i>, 338--343.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Ota, M., Fukushima, N., Yendo, T., Tanimoto, M., and Fujii, T. 2009. Rectification of Pure Translation 2D Camera Array. In <i>Proc. IWAIT, 0044</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Ou, X., Hamamoto, T., Kubota, A., and Kodama, K. 2008. Efficient free viewpoint image acquisition from multiple differently focused images. In <i>Proc. SPIE VCIP</i>, vol. 6822-73, 1--8.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Effective Global Prediction for Dense Light-Field Compression by Using Synthesized Multi-focus Images 
Takashi SAKAMOTO* KazuyaKODAMA Takayuki HAMAMOTO* *Tokyo University of Science National Institute of 
Informatics Figure 1: Light-Field compression using multi-focus images. 1 Introduction Light-Field Renderingisa 
promising technique generating3-Dim­ages from multi-view images captured by dense camera arrays or lens 
arrays[Isaksen et al. 2000]. However, Light-Field generally consists of 4-D enormous data, that are not 
suitable for storing or transmitting without effective compression[Magnor and Girod 2000]. We previously 
derived a method of reconstructing 4-D Light-Field directly from 3-D information composed of multi-focus 
images without any scene estimation[Kodama et al. 2006]. On the other hand, it is easy to synthesize 
multi-focus images from Light-Field. Therefore, we can achieve conversion between 4-D Light-Field and 
3-D multi-focus images without signi.cant degradation. Recently,researchers in computational photographyalso 
study such interesting properties of Light-Field[Levin and Durand 2010]. In thiswork, based on the conversion, 
we propose novel global predic­tion for dense Light-Field compression via synthesized multi-focus images 
aseffective representationof 3-D scenes like Figure1. 2 Our Approach We synthesize multi-focus imagesg(x, 
y, z) with a virtual lens by merging shifted multi-view images using constant weight function s p(s, 
t) as follows: g(x, y, z)= s,t p(s, t)as,t(x + sz, y + tz), where a viewpoint is denoted by (s, t) and 
as,t(x, y) is the corre­sponding image. Then, we apply DCT-based coding to synthesized multi-focus images. 
The compressed 3-D multi-focus images en­able us to predict 4-D Light-Field globally. Actually, g(x, 
y, z) is combined with 3-D scene f(x, y, z) by using a convolution of a 3-D blurring .lter h(x, y, z) 
= (1/z2)p(x/z, y/z)[Kodama et al. 2006]. Based on the relation, Light-Field can be approximately re­constructed 
by a simple combination of dimension reduction and a 2-D .ltering[Ou et al. 2008;Kodama et al. 2010]. 
We showexperimental results using real images[Ota et al. 2009]. 21 × 21 multi-view images are taken from 
equally-spaced view­points, and each image has 512×512 pixels. Disparity between adjacent imagesis smaller 
than about 2.0pixels. Figure2(a) shows quality of the reconstructed multi-viewimages from the synthesized 
64 multi-focus images compressed at 0.027 bpp. It indicates how the reconstruction quality changes by 
(s, t), where PSNR is eval­uated for 384 × 384 pixels in the center of the image. We obtain *e-mail: 
{sakamoto, hamamoto}@isl.ee.kagu.tus.ac.jp e-mail: {sakamoto, kazuya}@nii.ac.jp (a) Reconstruction quality 
(b) Comparison Figure 2: Evaluation of prediction. 30.35 dB at almost all the viewpoints. In Figure2(b), 
our pro­posed global prediction using multi-focus images is compared with the conventional local disparity-compensation[Magnor 
and Girod 2000]by averages of PSNR. Our prediction is robust, especially, at very low bit-rate. Effective 
Light-Field compression including residuals based on the proposed prediction is expected in future. 
References ISAKSEN,A.,MCMILLAN,L., AND GORTLER,S. 2000. Dynam­ically Reparameterized Light Fields. In 
Proc. ACM SIGGRAPH, 297 306. KODAMA,K.,MO,H., AND KUBOTA,A. 2006. Free Viewpoint, Iris and Focus Image 
Generationby Usinga Three-Dimensional Filtering based on FresquencyAnalysis of Blurs. In Proc. IEEE ICASSP, 
vol. II, 625 628. KODAMA,K.,IZAWA,I., AND KUBOTA,A. 2010. Robust Recon­struction of Arbitrarily Deformed 
Bokeh from Ordinary Multiple DifferentlyFocused Images. In Proc. IEEE ICIP, 3989 3992. LEVIN,A., AND 
DURAND,F. 2010. Linear View Synthesis Using a Dimensionality Gap Light Field Prior. In Proc. IEEE CVPR, 
1831 1838. MAGNOR,M., ANDGIROD,B. 2000. Data Compression for Light Field Rendering. IEEE Trans. CSVT 
10(3), 338 343. OTA, M., FUKUSHIMA, N., YENDO, T., TANIMOTO, M., AND FUJII, T. 2009. Recti.cation of 
Pure Translation 2D Camera Array. In Proc. IWAIT, 0044. OU,X.,HAMAMOTO,T.,KUBOTA,A., AND KODAMA,K. 2008. 
Ef.cient free viewpoint image acquisition from multiple differ­ently focused images. In Proc. SPIE VCIP, 
vol. 6822-73, 1 8. Copyright is held by the author / owner(s). SIGGRAPH 2012, Los Angeles, California, 
August 5 9, 2012. ISBN 978-1-4503-1435-0/12/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2342964</article_id>
		<sort_key>680</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>54</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>9</seq_no>
		<title><![CDATA[Estimating diffusion parameters from polarized spherical gradient illumination]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2342896.2342964</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2342964</url>
		<abstract>
			<par><![CDATA[<p>Accurately modeling and reproducing the appearance of real-world materials is crucial for the production of photoreal imagery of digital scenes and subjects. The appearance of many common materials is the result of subsurface light transport that gives rise to the characteristic "soft" appearance and the unique coloring of such materials. Jensen et al. [2001] introduced the dipole-diffusion approximation to efficiently model isotropic sub-surface light transport. The scattering parameters needed to drive the dipole-diffusion approximation are typically estimated by illuminating a homogeneous surface patch with a collimated beam of light, or in the case of spatially varying translucent materials with a dense set of structured light patterns. A disadvantage of most existing techniques is that acquisition time is traded off with spatial density of the scattering parameters.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3737379</person_id>
				<author_profile_id><![CDATA[81504682479]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Yufeng]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Zhu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[USC Institute for Creative Technologies]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737380</person_id>
				<author_profile_id><![CDATA[81100016992]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Pieter]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Peers]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The College of William & Mary]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737381</person_id>
				<author_profile_id><![CDATA[81504685651]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Paul]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Debevec]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[USC Institute for Creative Technologies]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737382</person_id>
				<author_profile_id><![CDATA[81385599077]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Abhijeet]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ghosh]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[USC Institute for Creative Technologies]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Ishimaru, A. 1978. <i>Wave Propagation and Scattering in Random Media</i>. Academic Press, New York.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383319</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Jensen, H. W., Marschner, S. R., Levoy, M., and Hanrahan, P. 2001. A practical model for subsurface light transport. In <i>Proceedings of ACM SIGGRAPH 2001</i>, 511--518.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2383873</ref_obj_id>
				<ref_obj_pid>2383847</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Ma, W.-C., Hawkins, T., Peers, P., Chabert, C.-F., Weiss, M., and Debevec, P. 2007. Rapid acquisition of specular and diffuse normal maps from polarized spherical gradient illumination. In <i>Rendering Techniques</i>, 183--194.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Estimating Diffusion Parameters from Polarized Spherical Gradient Illumination Yufeng Zhu Pieter Peers* 
Paul Debevec Abhijeet Ghosh USC Institute for Creative Technologies The College of William &#38; Mary* 
 (a) Diff. albedo (b) Red normals (c) Green normals (d) Blue normals (e) Translucency (f) Rendering 
Figure 1: Estimating spatially varying diffusion parameters from polarized spherical gradient illumination. 
RGB translucency parameters (e) inferred from diffuse albedo (a) and RGB diffuse normals (b-d). Introduction. 
Accurately modeling and reproducing the appear­ance of real-world materials is crucial for the production 
of pho­toreal imagery of digital scenes and subjects. The appearance of many common materials is the 
result of subsurface light transport that gives rise to the characteristic soft appearance and the unique 
coloring of such materials. Jensen et al. [2001] introduced the dipole-diffusion approximation to ef.ciently 
model isotropic sub­surface light transport. The scattering parameters needed to drive the dipole-diffusion 
approximation are typically estimated by illu­minating a homogeneous surface patch with a collimated 
beam of light, or in the case of spatially varying translucent materials with a dense set of structured 
light patterns. A disadvantage of most existing techniques is that acquisition time is traded off with 
spatial density of the scattering parameters. Polarized Spherical Gradients. Recently, Ma et al. [2007] 
pro­posed a technique to obtain high quality estimates of diffuse and specular albedo and photometric 
normal maps from just eight pho­tograph under four different polarized spherical gradient lighting conditions. 
In addition, Ma et al. also proposed a hybrid normal rendering technique that approximates the soft appearance 
of sub­surface scattering with local shading using measured RGB diffuse normals. This suggest a connection 
between spherical gradient il­lumination and subsurface scattering. In this work, we aim to formalize 
this apparent connection between subsurface scattering parameters and observations under spherical gradient 
illumination of translucent materials based on radiative transfer theory [Ishimaru 1978]. In particular, 
we show that dense per-surface-point scattering parameters can be directly obtained from observations 
under spherical gradient illumination (cross­polarized to discard specular re.ections), without resorting 
to any explicit .tting of observed scattering pro.les. Background. Light transport in highly scattering 
translucent mate­rials can be well approximated by diffusion theory [Ishimaru 1978; Jensen et al. 2001]. 
According to radiative transfer theory, diffu­sion can be accurately approximated by a two-term spherical 
har­ monic expansion of radiance: L(x, .) = 1 4p f(x) + 3 4p . · E (x), (1) where f(c) is the scalar 
.uence and E (x) is the vector irradi­ance. Substituiting Equation 1 in the radiative transfer equation 
and assuming semi-in.nite material, leads to the well-known dif­fuse BSSRF [Jensen et al. 2001]: (En 
· .f(xo)) Rd (r)= -D , (2) dFi(xi) where r = ||xo - xi||, and D = 1/3s:is the diffusion constant. t 
(a) Diff. albedo (b) Translucency (c) Rendering Figure 2: Spatially varying diffusion parameters of 
material samples estimated us­ing spherical gradient illumination. Top-row: Red wax. Bottom-row: Polished 
marble. Diffusion from Gradients. Relating Equations (1) and (2) yields a mechanism for estimating scattering 
parameters of dipole diffusion from observations of the 1st -order spherical gradients. Speci.cally, 
a BRDF approximation of Equation 2 relates the observed diffuse albedo Rd (cross-polarized 0th order 
spherical statistics) to the nor­mal aligned component of the estimated diffuse normal En (cross­polarized 
1st order spherical statistics [Ma et al. 2007]) via the diffusion constant D. Assuming that the surface 
normal is along the +Z direction, this leads to the following compact relation between the diffusion 
constant and polarized spherical gradients: Rd D . . (3) |Enz| Conclusion. Equation 3 provides a mechanism 
for directly ob­taining dense spatially-varying diffusion parameters from just four observations of translucent 
materials under polarized spherical gra­dient illumination. References ISHIMARU, A. 1978. Wave Propagation 
and Scattering in Random Media. Academic Press, New York. JENSEN, H. W., MARSCHNER, S. R., LEVOY, M., 
AND HANRA-HAN, P. 2001. A practical model for subsurface light transport. In Proceedings of ACM SIGGRAPH 
2001, 511 518. MA, W.-C., HAWKINS, T., PEERS, P., CHABERT, C.-F., WEISS, M., AND DEBEVEC, P. 2007. Rapid 
acquisition of specular and diffuse normal maps from polarized spherical gradient illumina­tion. In Rendering 
Techniques, 183 194. Copyright is held by the author / owner(s). SIGGRAPH 2012, Los Angeles, California, 
August 5 9, 2012. ISBN 978-1-4503-1435-0/12/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2342965</article_id>
		<sort_key>690</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>55</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>10</seq_no>
		<title><![CDATA[Estimating specular normals from spherical Stokes reflectance fields]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2342896.2342965</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2342965</url>
		<abstract>
			<par><![CDATA[<p>Despite being at the focal point of intense research in both computer graphics as well as in computer vision, accurately reproducing the shape and appearance of real-world scenes remains a challenging problem, especially under uncontrolled conditions. One cue that has been used to separate diffuse and specular reflectance is polarization. Recent work in computer graphics has explored polarization of incident illumination in conjunction with spherical gradient illumination to infer high quality diffuse-specular separation of both albedo as well as photometric normal information [Ma et al. 2007]. Ghosh et al. [2010] improved upon this by removing the view-dependence of the polarization scheme of Ma et al. by analyzing the Stokes reflectance field under incident circularly polarized spherical gradient illumination, and recover more detailed specular reflectance information including index of refraction as well as specular roughness.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3737383</person_id>
				<author_profile_id><![CDATA[81384613270]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Giuseppe]]></first_name>
				<middle_name><![CDATA[Claudio]]></middle_name>
				<last_name><![CDATA[Guarnera]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[USC Institute for Creative Technologies]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737384</person_id>
				<author_profile_id><![CDATA[81100016992]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Pieter]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Peers]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The College of William & Mary]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737385</person_id>
				<author_profile_id><![CDATA[81504685651]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Paul]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Debevec]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[USC Institute for Creative Technologies]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737386</person_id>
				<author_profile_id><![CDATA[81385599077]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Abhijeet]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ghosh]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[USC Institute for Creative Technologies]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1866163</ref_obj_id>
				<ref_obj_pid>1882261</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Ghosh, A., Chen, T., Peers, P., Wilson, C. A., and Debevec, P. 2010. Circularly polarized spherical illumination reflectometry. <i>ACM Trans. Graph. 29</i> (December), 162:1--162:12.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2383873</ref_obj_id>
				<ref_obj_pid>2383847</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Ma, W.-C., Hawkins, T., Peers, P., Chabert, C.-F., Weiss, M., and Debevec, P. 2007. Rapid acquisition of specular and diffuse normal maps from polarized spherical gradient illumination. In <i>Rendering Techniques</i>, 183--194.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Estimating Specular Normals from Spherical Stokes Re.ectance Fields Giuseppe Claudio Guarnera Pieter 
Peers* Paul Debevec Abhijeet Ghosh USC Institute for Creative Technologies The College of William &#38; 
Mary*  (a) Subject (b) Specular normals (c) Circular Stokes (C.S.) (d) Normals from C.S. (e) Unpol. 
Stokes (U.S.) (f) Normals from U.S. Figure 1: Estimating specular normals from Stokes parameters of incident 
spherical illumination. Specular normals inferred from circularly polarized illumination (c-d), and unpolarized 
illumination (e-f), compared to measurement with polarized spherical gradients (b). Top-row: Plastic 
orange. Bottom-row: Marble statue. Introduction. Despite being at the focal point of intense research 
in both computer graphics as well as in computer vision, accurately reproducing the shape and appearance 
of real-world scenes remains a challenging problem, especially under uncontrolled conditions. One cue 
that has been used to separate diffuse and specular re­.ectance is polarization. Recent work in computer 
graphics has explored polarization of incident illumination in conjunction with spherical gradient illumination 
to infer high quality diffuse-specular separation of both albedo as well as photometric normal informa­tion 
[Ma et al. 2007]. Ghosh et al. [2010] improved upon this by removing the view-dependence of the polarization 
scheme of Ma et al. by analyzing the Stokes re.ectance .eld under incident cir­cularly polarized spherical 
gradient illumination, and recover more detailed specular re.ectance information including index of refrac­tion 
as well as specular roughness. Contribution. In this work we analyze the view-independent sym­metric 
Stokes re.ectance .eld under constant incident spherical illumination that is either circularly polarized 
(Fig. 1,c-d), or un­ polarized (Fig. 1, e-f). We demonstrate that both types of incident lighting can 
be used to reliably estimate specular normals, and show how this theory can be applied to normal estimation 
under uncon­trolled outdoor illumination. Normal Computation. We capture four photographs of the target 
object under uniform spherical illumination (either circu­larly polarized or unpolarized) with different 
polarizers in front of the camera as in Ghosh et al. [2010] (i.e., three linear polar­ izers (P0, P45, 
P90) and a (left) circular polarizer (P.)), and com­pute the four Stokes parameters of re.ected light 
(s0, s1, s2, s3) per pixel. Without loss of generality, we assume that the camera is looking down the 
-Z axis. It can be shown that in this case, h s¯3 = s3/ s21 + s22 + s2 relates to . = arccos(n · Z), 
where n is 3 the specular normal. Furthermore, the normalized linear compo­ h nents (s;1, s;2), with 
si;= si/ s21 + s22, i .{1, 2}, relates to the angle f = arccos(n · X). However, the mapping from (s;1, 
s2;) to f suffers from a rotational ambiguity: f and f + p map to the same (s;1, s;2). We propose two 
solutions to solve for this ambiguity: 1. An additional measurement under a gradient illumination (a) 
Plastic orange (b) Unpol. Stokes (U.S.) (c) Normals from U.S. Figure 2: Estimated specular normals from 
Stokes parameters of diffuse outdoor illumination. orthogonal to the camera viewing axis (i.e., X or 
Y gradi­ent) breaks the ambiguity. However, this reduces the view­independence of the normal computations. 
 2. For convex objects, we can grow the normals in from the silhouette, assuming that the normals at 
the silhouette are or­thogonal to silhouette edge and the view direction. Outdoor Illumination. We note 
that instead of using constant illumination, we can also observe the subject under a Y gradient. In this 
case, the incident lighting already provides the additional cue to break the rotational ambiguity for 
computation of f. Interestingly, outdoor illumination on a cloudy day, is approximately similar to unpolarized 
Y-gradient illumination. We employ this observation to compute the normals under such outdoor lighting 
condition in Figure 2. References GHOSH, A., CHEN, T., PEERS, P., WILSON, C. A., AND DE-BEVEC, P. 2010. 
Circularly polarized spherical illumination re­.ectometry. ACM Trans. Graph. 29 (December), 162:1 162:12. 
MA, W.-C., HAWKINS, T., PEERS, P., CHABERT, C.-F., WEISS, M., AND DEBEVEC, P. 2007. Rapid acquisition 
of specular and diffuse normal maps from polarized spherical gradient illumina­tion. In Rendering Techniques, 
183 194. Copyright is held by the author / owner(s). SIGGRAPH 2012, Los Angeles, California, August 5 
 9, 2012. ISBN 978-1-4503-1435-0/12/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2342966</article_id>
		<sort_key>700</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>56</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>11</seq_no>
		<title><![CDATA[Focus tracking for cinematography]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2342896.2342966</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2342966</url>
		<abstract>
			<par><![CDATA[<p>Cinematographers primarily use manual control for focusing their cameras because existing autofocus techniques used in photography can't be directly applied to video or motion-pictures and don't provide sufficient artistic control. Adjusting focus therefore remains a key challenge, which limits the possibilities of executing certain shots. For instance, the amount of depth of field used in shots with a moving camera or subject is heavily influenced by how precise focus can be controlled. This work presents a simple method to overcome some of these challenges by tracking the focus with off the shelf sensory equipment and state of the art 3D point cloud processing techniques. The method integrates well with the current workflow of camera operators and their first assistants and even gives them more flexibility than a manually controlled follow focus. To evaluate the feasibility, a fully functional prototype was built and tested with professional camera operators.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[cinematography]]></kw>
			<kw><![CDATA[focus tracking]]></kw>
			<kw><![CDATA[kinect]]></kw>
			<kw><![CDATA[point cloud]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.8</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.4.9</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010225</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision tasks</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010507</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Graphics recognition and interpretation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P3737387</person_id>
				<author_profile_id><![CDATA[81504688210]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Aurel]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wildfellner]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Johannes Kepler University Linz]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[aurel@rantanplan.org]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Focus Tracking for Cinematography Aurel Wildfellner* Johannes Kepler University Linz  Figure 1: Left: 
A DSLR video rig with an integrated Kinect. The Kinect is used as an external range.nder and for 6DOF 
tracking. Center: Continuous focus on a small walnut in a dolly shot (detail from a video). Right: The 
camera moving very close to the nut and maintaining focus automatically. Abstract Cinematographers primarily 
use manual control for focusing their cameras because existing autofocus techniques used in photography 
can t be directly applied to video or motion-pictures and don t pro­vide suf.cient artistic control. 
Adjusting focus therefore remains a key challenge, which limits the possibilities of executing certain 
shots. For instance, the amount of depth of .eld used in shots with a moving camera or subject is heavily 
in.uenced by how precise focus can be controlled. This work presents a simple method to overcome some 
of these challenges by tracking the focus with off the shelf sensory equipment and state of the art 3D 
point cloud pro­cessing techniques. The method integrates well with the current work.ow of camera operators 
and their .rst assistants and even gives them more .exibility than a manually controlled follow fo­cus. 
To evaluate the feasibility, a fully functional prototype was built and tested with professional camera 
operators. CR Categories: I.3.8 [Computer Graphics]: Applications [I.4.9]: Image Processing and Computer 
Vision Applications; Keywords: focus tracking, point cloud, kinect, cinematography 1 Method A Microsoft 
Kinect sensor is integrated into a DSLR video rig in a way that it is rigidly mounted in respect to the 
camera. After intrin­sic and extrinsic calibration the captured depth image and resulting point cloud 
is registered with the image of the DSLR camera. This way, the Kinect can serve as an external active 
range.nder with a *e-mail:aurel@rantanplan.org very high number of meter points. As with a conventional 
autofo­cus the camera can be pointed at a certain part in the scene, the dis­tance is measured and the 
focus is then adjusted accordingly. This is also used to select points in the scene, which are then tracked 
and continuously kept in focus while moving or panning the camera. Rather than tracking single points, 
the pose of the camera itself is estimated. This is done by real-time Iterative Closest Point (ICP) point 
cloud registration on a GPGPU. This way several static points in the scene can be selected and tracked 
simultaneously, even if they are temporarily outside of the .eld of view. The Kinect itself is not able 
to measure depth in close proximity (approximately half a meter), but because of global registration 
of the camera, points can be tracked when moving even closer to the lens. This makes the system feasible 
for continues autofocus in the macro range. Certain strategies have been implemented to select and interpolate 
between tracked points for controlling focus. The work.ow for the system is to .rst select a strategy 
and choose relevant focus points in the scene while setting up the shot. The lens is then controlled 
fully automatically during the actual recording phase. These simple steps enable precise, fast and complex 
focusing scenarios. A key bene.t is the seamless integration into existing, well known camera setups. 
The method works almost intuitively and no com­plex interaction is needed, as the prototype is literally 
controlled by a single button. Another bene.t is the possibility of operating in low light situations 
due to the active projection of infrared light used by the Kinect sensor. 2 Conclusion The method turned 
out to be feasible and especially useful for pan­ning and tracking shots with a low depth of .eld, where 
precise manual focus control would have been impossible or only achiev­able by complex motion-control. 
It is also obvious that such focus tracking could be highly useful for Steadicam operators, but this 
is yet to be tested. Current limiting factors are the time consum­ing calibration process after modifying 
the DSLR camera system and that the physical scale of the scene is bounded to a few meters. In the future, 
the latter could be solved by improving and extending the ICP and reconstruction implementation to work 
with large scale environments. Copyright is held by the author / owner(s). SIGGRAPH 2012, Los Angeles, 
California, August 5 9, 2012. ISBN 978-1-4503-1435-0/12/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2342967</article_id>
		<sort_key>710</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>57</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>12</seq_no>
		<title><![CDATA[Guided tone mapping]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2342896.2342967</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2342967</url>
		<abstract>
			<par><![CDATA[<p>In this paper, we propose a simple but effective tone mapping operator, which can achieve impressive results with low complexity in both the formulation and computation. The key point of our operator is to calculate a scalar matrix by which the input HDR image can be mapped into a displayable LDR image. From the graphical perspective, we can regard the scalar matrix as a guided image, as shown in Figure 1(b). Experimental results show that our algorithm can achieve real-time perceptually pleasing result.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[guided image]]></kw>
			<kw><![CDATA[high dynamic range]]></kw>
			<kw><![CDATA[tone mapping]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3737388</person_id>
				<author_profile_id><![CDATA[81504685269]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Huxiang]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[National Laboratory of Pattern Recognition]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[hxgu@nlpr.ia.ac.cn]]></email_address>
			</au>
			<au>
				<person_id>P3737389</person_id>
				<author_profile_id><![CDATA[81498645611]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Ying]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[National Laboratory of Pattern Recognition]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[ywang@nlpr.ia.ac.cn]]></email_address>
			</au>
			<au>
				<person_id>P3737390</person_id>
				<author_profile_id><![CDATA[81414596083]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Gaofeng]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Meng]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[National Laboratory of Pattern Recognition]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[gfmeng@nlpr.ia.ac.cn]]></email_address>
			</au>
			<au>
				<person_id>P3737391</person_id>
				<author_profile_id><![CDATA[81361594848]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Shiming]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Xiang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[National Laboratory of Pattern Recognition]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[smxiang@nlpr.ia.ac.cn]]></email_address>
			</au>
			<au>
				<person_id>P3737392</person_id>
				<author_profile_id><![CDATA[81504686000]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Chunhong]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Pan]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[National Laboratory of Pattern Recognition]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[chpan@nlpr.ia.ac.cn]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1964963</ref_obj_id>
				<ref_obj_pid>2010324</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Paris, S., Hasinoff, S. W., and Kautz, J. 2011. Local laplacian filters: edge-aware image processing with a laplacian pyramid. <i>ACM Trans. Graph. 30</i>, 68:1--68:12.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Stockham, T. G., J. 1972. Image processing in the context of a visual model. <i>Proceedings of the IEEE. 60</i>, 7, 828--842.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Guided Tone Mapping Huxiang Gu,YingWang, Gaofeng Meng, Shiming Xiang, ChunhongPan National LaboratoryofPattern 
Recognition*  (a) (b) (c) (d) Figure 1: (a) the input high dynamic range (HDR) image. (b) the guided 
image constructed by our approach. Mathematically, the guided image can be regarded as a scalar matrix 
which map the HDR luminance channel Ih into low dynamic range (LDR) luminance channel Il simply by Il 
= P · Ih. The scalar matrix P is calculated per pixel with local mean and variance in the luminance channel 
of the input HDR image. Bright regions in the guided image indicate that the same areas of the input 
HDR image should be enhanced, otherwise should be compressed. (c) our enhanced result in details()1 =0.6,)2 
=0.3). (d) our natural result()1 =0.6,)2 =0.1). Abstract In this paper, we proposea simplebuteffective 
tone mapping oper­ator, which can achieve impressive results with low complexity in both the formulation 
and computation. Thekeypoint of our oper­ator is to calculate a scalar matrix by which the input HDR 
image can be mapped into a displayable LDR image. From the graphical perspective, we can regard the scalar 
matrix as a guided image, as showninFigure1(b). Experimental resultsshowthatour algorithm can achieve 
real-time perceptually pleasing result. Keywords: tone mapping, high dynamic range, guided image 1 Introduction 
The last three decades have witnessed the prosperity in tone map­ping algorithm. Mathematically, given 
a HDR luminance channel Ih, the target of tone mapping is to .nd a mapping function f(·) so that we can 
get the LDR luminance channel Il by Il = f(Ih). In literature, the tone mapping operators can be roughly 
classi.ed into two categories: global operators and local operators. The mapping functions f(·) in global 
techniques, such as linear function,gamma function, histogram based function and so on, have different 
for­mulations but share one similarity that it is spatially invariant in the whole image. Global operators 
are usually simple andfast,but theyalwaysfailin balancingunveiling visual contentsand preserv­ing details. 
Therefore, the recent literature focus on local operators in which the mapping function f(·) is locally 
invariant. For local operators,thekeypointis transformedtohowto de.nealocal mea­surement rather than 
.nd a suitable mapping function because, in each local patch, linear mapping function is usually good 
enough to obtain a satisfactory result. As a result, different local measure­ments based on neighborhoods, 
layers after decomposition, areas aftersegmentationhaveenjoyedaboomindifferent local operators in the 
last ten years. Local operators can effectively compress the high dynamic range while maintain or enhance 
the details. How­ever, most of them have a high complexity in formulation or com­putation.Typically,Paris 
[Pariset al. 2011]decompose the input *e-mail: {hxgu, ywang, gfmeng, smxiang, chpan}@nlpr.ia.ac.cn HDR 
image into different parts and layers by standard Laplacian pyramids and then the same function is used 
in each layer or part to achieve edge-aware tone mapping. The local Laplacian meth­ods can produce impressive 
high-quality results, especially in de­tails enhancement. Unfortunately, their algorithm suffers from 
high complexity. 2 Our Approach As analyzed above, if we still focus on how to .nd an explicit map­ping 
function or a new local measurement, it is not easy to avoid the problem of balancing the complexity 
and the effect. Actually, if the mapping function f(·) can be expressed as a formulation of dot product, 
namely f(Ih)= P · Ih, this problem might be solved. Under this circumstance, the fundamental task of 
tone mapping is to .nd a suitable scalar matrix. Fortunately, inspired by the obser­vationof Stockham 
[Stockham 1972], we .nd sucha proper scalar matrix which can be calculated as: Pi =1 . (1) 1 2 max(uii 
,k0) Where ui and i denote the mean and variance of the local patch centering in pixel i, while k0 =0.001 
isa constantvaluetokeep the denominator from zero. Note that our operator has not only a simple formulationbut 
alsoalow computation complexity because there are only local mean and variance computation and dot prod­uct 
involved in our algorithm. Besides, our results can be adapted to speci.c requirements by toning up the 
two parameters. To be more speci.c, parameter )1 controls the total brightness of the out­put LDR image 
while parameter )2 in.uences the details manipu­lation,asshownin Figure1(c),(d). Moreexperimental results 
are illustrated in the supplementary.  References PARIS,S.,HASINOFF,S.W., AND KAUTZ,J. 2011. Local lapla­cian 
.lters: edge-aware image processing witha laplacianpyra­mid. ACM Trans. Graph. 30, 68:1 68:12. STOCKHAM,T.G.,J. 
1972. Image processinginthe contextofa visual model. Proceedings of the IEEE. 60, 7, 828 842. Copyright 
is held by the author / owner(s). SIGGRAPH 2012, Los Angeles, California, August 5 9, 2012. ISBN 978-1-4503-1435-0/12/0008 
  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2342968</article_id>
		<sort_key>720</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>58</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>13</seq_no>
		<title><![CDATA[Hair motion capturing from multiple view videos]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2342896.2342968</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2342968</url>
		<abstract>
			<par><![CDATA[<p>To create a realistic virtual human, hair animation is an indispensable factor. Many hair simulation methods have been proposed so far, but simulating realistic hair motion such as considering an effect of turbulent flow and friction among enormous amount of hairs is still one of the challenging phenomena. Thus, to reproduce hair motion which includes these desirable features, capturing real hair motion has advantages compared with simulated one. Ishikawa et al. [2007] used a motion capture system and tracked some reflective makers placed on some strands. They successfully reproduce hair motions including an effect of turbulent flow, but since they put on some markers which have weight and only captured sparse strands which mean friction among others is ignored, it is still far from real hair motion.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3737393</person_id>
				<author_profile_id><![CDATA[81504688282]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Tsukasa]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Fukusato]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Waseda University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[tsukasa@moegi.waseda.jp]]></email_address>
			</au>
			<au>
				<person_id>P3737394</person_id>
				<author_profile_id><![CDATA[81488654875]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Naoya]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Iwamoto]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Waseda University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737395</person_id>
				<author_profile_id><![CDATA[81466642917]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Shoji]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kunitomo]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Waseda University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737396</person_id>
				<author_profile_id><![CDATA[81537598956]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Hirofumi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Suda]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Waseda University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737397</person_id>
				<author_profile_id><![CDATA[81504685727]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Shigeo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Morishima]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Waseda University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[shigeo@waseda.jp]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1280806</ref_obj_id>
				<ref_obj_pid>1280720</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Takahito Ishikawa et al, "Hair motion reconstruction using motion capture system", In ACM SIGGRAPH 2007, posters.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Linjie Luo et al. "Dynamic hair Capture" Technical Report TR-907-11, Princetion University, August 2011.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Hair Motion Capturing from Multiple View Videos i e-mail tsukasa@moegi.waseda.jp ii e-mail shigeo@waseda.jp 
 C:\Users\Tsukasa Fukusato\Desktop\.11.png C:\Users\Tsukasa Fukusato\Desktop\.3.png C:\Users\Tsukasa 
Fukusato\Desktop\.4.png Tsukasa Fukusato i Naoya Iwamoto Shoji Kunitomo Hirofumi Suda Shigeo Morishima 
ii Waseda University 1. Introduction To create a realistic virtual human, hair animation is an indispensable 
factor. Many hair simulation methods have been proposed so far, but simulating realistic hair motion 
such as considering an effect of turbulent flow and friction among enormous amount of hairs is still 
one of the challenging phenomena. Thus, to reproduce hair motion which includes these desirable features, 
capturing real hair motion has advantages compared with simulated one. Ishikawa et al. [2007] used a 
motion capture system and tracked some reflective makers placed on some strands. They successfully reproduce 
hair motions including an effect of turbulent flow, but since they put on some markers which have weight 
and only captured sparse strands which mean friction among others is ignored, it is still far from real 
hair motion. So our goal is to overcome those defects and capture more realistic hair motion. To achieve 
the goal, we assume that hair is a model of several hair-bundles and so we decided to use some hair extensions 
which are dyed in horizontal strips. We capture the center of gravity of colored areas and then estimate 
those 3D coordinates. We also applied them to existing hair model. By our results, more realistic hair 
animation reflecting hair-to-hair friction is achieved than previous technique. 2. Hair Extension and 
Acquisition To escape overlapping of colored hair extensions, each strand has to be located as sparse 
as possible. And also to identify each strand easily and accurately, color combination of each strand 
has to be different. So we choose eleven positions and colors of hair strands to represent overall hair 
motion to be captured by eight cameras located in four directions around head. An example of hair bundle 
is shown in Figure 1(a). Figure 1 (b) shows the acquisition setup. Note that external and internal camera 
parameters are calibrated using Zhang method. 3. Color detection and Parallel Stereo We captured hair 
motion when wind is given. First of all, image coordinates of center of colored areas are detected and 
then 3D coordinates are estimated. We first process color detection every video sequences and define 
a center of colored areas. To make correspondence automatically, colored areas in initial frame are labeled 
by hand. After second frame, they are labeled based on nearest colored areas of the previous frame and 
so all of the areas in all video frames are able to capture automatically.     (a) Hair Extension 
(b) Acquisition Setup Figure 1. hair extension and acquisition set-up           (a):Reference 
motion (b): Result of our hair motion capture (c): Applying hair model Figure 2. hair motion result 
based on camera capture  Next, the 3D coordinates of colored areas are estimated by parallel stereo 
vision which is based on detected 2D coordinates. 4. Hair Model We design hair model for a reed-shaped 
hair strand. This reed-shaped polygon structure is generally used hair mapping textures onto this kind 
of structure. This model actually has been used to design a realistic human character s hair in movies 
or games. Hair motion we captured is applied to the chosen the hair strand. 5. Results In this section, 
we demonstrate an animation obtained by applying our approach. The reference motion is showed in Figure 
2(a). The result of computing hair motion is achieved in Figure 2(b). Figure 1(d) represents the result 
of applying hair motion to a hair model. This method shows that our hair motion capture consist with 
the reference motion. 6. Conclusions and Future Work In this paper, we have proposed a method of capturing 
hair motion using hair extensions instead of markers. Consequently, we successfully capture and reconstruct 
more natural hair motion, since no weight is put on the hair and also friction among hairs is considered. 
As future work, we need to capture hair motion in a more natural situation, for example, when human walk. 
Furthermore, if we position more hair extensions and capture motion inside of hair, we could create richer 
hair animation. Reference TAKAHITO ISHIKAWA et al, Hair motion reconstruction using motion capture system 
, In ACM SIGGRAPH 2007, posters. LINJIE LUO et al. Dynamic hair Capture Technical Report TR-907-11, Princetion 
University, August 2011. 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2342969</article_id>
		<sort_key>730</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>59</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>14</seq_no>
		<title><![CDATA[High detail marker based 3D reconstruction by enforcing multiview constraints]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2342896.2342969</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2342969</url>
		<abstract>
			<par><![CDATA[<p>We present a 3D reconstruction method enabling high resolution marker-based capturing of deforming surfaces. In contrast to previous work, we allow all markers to look exactly the same and do not rely on temporal tracking. This implies considerable advantages: markers can be smaller and are easier to apply due to omitted identification; long-range motions normally confusing temporal tracking algorithms become feasible. However, the correct matching of markers between camera views is highly ambiguous in such a scenario. To solve this problem we propose an optimization framework that considers multiview conflicts and local smoothness of the captured surface. An iterative relaxation method based on graph matching is adopted to obtain a consistent, smooth reconstruction for all stereo pairs of a multi-camera system simultanously. Preliminary experiments show excellent and robust results.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[3D reconstruction]]></kw>
			<kw><![CDATA[motion capture]]></kw>
			<kw><![CDATA[multiview stereo]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3737398</person_id>
				<author_profile_id><![CDATA[81479644753]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Thomas]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Neumann]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[HTW Dresden]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[tneumann@htw-dresden.de]]></email_address>
			</au>
			<au>
				<person_id>P3737399</person_id>
				<author_profile_id><![CDATA[81100403252]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Markus]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wacker]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[HTW Dresden]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[wacker@htw-dresden.de]]></email_address>
			</au>
			<au>
				<person_id>P3737400</person_id>
				<author_profile_id><![CDATA[81320496204]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Kiran]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Varanasi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MPI Informatik, Saarbr&#252;cken]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[varanasi@mpi-inf.mpg.de]]></email_address>
			</au>
			<au>
				<person_id>P3737401</person_id>
				<author_profile_id><![CDATA[81331505042]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Christian]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Theobalt]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MPI Informatik, Saarbr&#252;cken]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[theobalt@mpi-inf.mpg.de]]></email_address>
			</au>
			<au>
				<person_id>P3737402</person_id>
				<author_profile_id><![CDATA[81100477906]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Marcus]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Magnor]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[TU Braunschweig]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[magnor@cg.cs.tu-bs.de]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1888189</ref_obj_id>
				<ref_obj_pid>1888150</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Cho, M., Lee, J., and Lee, K. M. 2010. Reweighted random walks for graph matching. In <i>Proc. ECCV</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1141970</ref_obj_id>
				<ref_obj_pid>1179352</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Park, S. I., and Hodgins, J. K. 2006. Capturing and animating skin deformation in human motion. In <i>Proc. SIGGRAPH</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1276420</ref_obj_id>
				<ref_obj_pid>1275808</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[White, R., Crane, K., and Forsyth, D. 2007. Capturing and animating occluded cloth. In <i>Proc. SIGGRAPH</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 High Detail Marker based 3D Reconstruction by Enforcing Multiview Constraints Thomas Neumann, Markus 
Wacker* Kiran Varanasi, Christian Theobalt Marcus Magnor HTW Dresden MPI Informatik, Saarbr¨TU Braunschweig 
ucken  Figure 1: 3D Reconstruction with over 5.000 markers. 1) Camera images; 2) Closeup of detected 
markers; 3) 3D reconstruction from separate stereo matches without our multiview constraints; 4 and 5) 
Reconstruction by enforcing multiview constraints using our method Abstract We present a 3D reconstruction 
method enabling high resolution marker-based capturing of deforming surfaces. In contrast to pre­vious 
work, we allow all markers to look exactly the same and do not rely on temporal tracking. This implies 
considerable advan­tages: markers can be smaller and are easier to apply due to omit­ted identi.cation; 
long-range motions normally confusing temporal tracking algorithms become feasible. However, the correct 
match­ing of markers between camera views is highly ambiguous in such a scenario. To solve this problem 
we propose an optimization frame­work that considers multiview con.icts and local smoothness of the captured 
surface. An iterative relaxation method based on graph matching is adopted to obtain a consistent, smooth 
reconstruction for all stereo pairs of a multi-camera system simultanously. Prelim­inary experiments 
show excellent and robust results. Keywords: 3D reconstruction, multiview stereo, motion capture Figure 
2: 3 camera views with extracted 2D markers. Correspon­dences between markers are ambigous (e.g. both 
m1,3 and m1,4 might be correct matches for p1). Considering one-to-one con­straints, multiview con.icts, 
and local smoothness our algorithm removes such ambiguities and .nds a consistent 3D reconstruction. 
*e-mail: {tneumann,wacker}@htw-dresden.de e-mail: {varanasi,theobalt}@mpi-inf.mpg.de e-mail:magnor@cg.cs.tu-bs.de 
1 Our Approach Given the 2D projections of markers found in multiple camera im­ages, we seek the 3D reconstruction 
of the markers. To this end, correspondences between projections of markers have to be found. For thousands 
of identical markers, matching is highly ambiguous even when respecting epipolar constraints. Finding 
the one-to-one matching between two cameras can be cast as a graph matching problem and solved using 
approximate methods, which in theory produces locally smooth correspondence .elds [Cho et al. 2010]. 
In a multi-camera setting, solving several stereo reconstructions sepa­rately yields the 3D reconstruction. 
However, we observed that this produces poor results. We propose to consider interactions between previously 
separately handled stereo pairs, in the form of multiview constraints. For ex­ample, in Fig. 2 the matches 
m1,2 and m1,3 belonging to separate pairs of cameras agree on the same triangulated 3D position. In con­trast, 
m1,2 and m1,4 correspond to distinct triangulated 3D points. The latter case implies a con.ict between 
said matches since p1 can only be the projection of one of those separate 3D points. In the it­erative 
graph matching procedure, we replace the bi-stochastic nor­malization (concerning only one-to-one matching 
constraints) with normalization concerning multi-view constraints and solve for all stereo matches simultaneously. 
The result is a smooth and consis­tent 3D reconstruction, Fig. 1 (image 4 &#38; 5). The proposed algorithm 
has high potential for further improve­ments both in reconstruction quality and runtime performance, 
two issues we would like to tackle in the near future. Being able to eas­ily place thousands of markers 
without taking special care of marker identi.cation and layout could ease acquisition in interesting 
appli­cation scenarios, such as the capturing of skin and muscle deforma­tion [Park and Hodgins 2006], 
capturing of garments [White et al. 2007], and marker-based reconstruction of facial animations.  References 
CHO, M., LEE, J., AND LEE, K. M. 2010. Reweighted random walks for graph matching. In Proc. ECCV. PARK, 
S. I., AND HODGINS, J. K. 2006. Capturing and animating skin deformation in human motion. In Proc. SIGGRAPH. 
WHITE, R., CRANE, K., AND FORSYTH, D. 2007. Capturing and animating occluded cloth. In Proc. SIGGRAPH. 
Copyright is held by the author / owner(s). SIGGRAPH 2012, Los Angeles, California, August 5 9, 2012. 
ISBN 978-1-4503-1435-0/12/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2342970</article_id>
		<sort_key>740</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>60</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>15</seq_no>
		<title><![CDATA[Measurement-based synthesis of facial microgeometry]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2342896.2342970</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2342970</url>
		<abstract>
			<par><![CDATA[<p>Current scanning techniques record facial <i>mesostructure</i> with sub-millimeter precision showing pores, wrinkles, and creases. However, surface roughness continues to shape specular reflection at the level of <i>microstructure</i>: micron scale structures. Here, we present an approach to increase the resolution of mesostructure-level facial scans using microstructure examples digitized about the face. We digitize the skin patches using polarized gradient illumination and 10 &mu;m resolution macro photography, and observe point-source reflectance measurements to characterize the specular reflectance lobe at this smaller scale. We then perform constrained texture synthesis to create appropriate surface microstructure per facial region, blending the regions to cover the whole entire face. We show that renderings of microstructure-augmented facial models preserve the original scanned mesostructure and exhibit surface reflections which are qualitatively more consistent with real photographs.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3737403</person_id>
				<author_profile_id><![CDATA[81504686804]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Paul]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Graham]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[USC Institute for Creative Technologies]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737404</person_id>
				<author_profile_id><![CDATA[81442617187]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Borom]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tunwattanapong]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[USC Institute for Creative Technologies]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737405</person_id>
				<author_profile_id><![CDATA[81440600975]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Jay]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Busch]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[USC Institute for Creative Technologies]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737406</person_id>
				<author_profile_id><![CDATA[81440613130]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Xueming]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[USC Institute for Creative Technologies]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737407</person_id>
				<author_profile_id><![CDATA[81100556708]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Andrew]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Jones]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[USC Institute for Creative Technologies]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737408</person_id>
				<author_profile_id><![CDATA[81504685651]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Paul]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Debevec]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[USC Institute for Creative Technologies]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737409</person_id>
				<author_profile_id><![CDATA[81385599077]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>7</seq_no>
				<first_name><![CDATA[Abhijeet]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ghosh]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[USC Institute for Creative Technologies]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Graham, P., Tunwattanapong, B., Busch, J., Yu, X., Jones, A., Debevec, P., and Ghosh, A. 2012. Measurement-based synthesis of facial microgeometry. Tech. Rep. ICT-TR-01-2012, USC-ICT.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383295</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Hertzmann, A., Jacobs, C. E., Oliver, N., Curless, B., and Salesin, D. H. 2001. Image analogies. In <i>Proceedings of the 28th annual conference on Computer graphics and interactive techniques</i>, ACM, New York, NY, USA, SIGGRAPH '01, 327--340.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Measurement-Based Synthesis of Facial Microgeometry Paul Graham Borom Tunwattanapong Jay Busch Xueming 
Yu Andrew Jones Paul Debevec Abhijeet Ghosh USC Institute for Creative Technologies  (a) Rendering from 
original facial scan (b) Rendering using synthesized microstructure (c) Comparison photograph Figure 
1: (a) Scanned mesostructure with 4K displacement map. (b) Synthesized microstructure with 16K displacement 
map. (c) Real photograph under .ash illumination. 1 Introduction Current scanning techniques record facial 
mesostructure with sub­millimeter precision showing pores, wrinkles, and creases. How­ever, surface roughness 
continues to shape specular re.ection at the level of microstructure: micron scale structures. Here, 
we present an approach to increase the resolution of mesostructure-level fa­cial scans using microstructure 
examples digitized about the face. We digitize the skin patches using polarized gradient illumination 
and 10 µm resolution macro photography, and observe point-source re.ectance measurements to characterize 
the specular re.ectance lobe at this smaller scale. We then perform constrained texture synthesis to 
create appropriate surface microstructure per facial re­gion, blending the regions to cover the whole 
entire face. We show that renderings of microstructure-augmented facial models preserve the original 
scanned mesostructure and exhibit surface re.ections which are qualitatively more consistent with real 
photographs. 2 Recording Skin Microstructure (a) (b) Figure 2: Microgeometry acquisition setups (a) 
Twelve-light hemisphere capturing a patch on the cheek. (b) LED Sphere with camera inside, capturing 
the nose tip. We record the microstructure of skin patches under polarized gradi­ent illumination using 
either of two systems. For both, we stabilize the skin patch relative to the camera by placing the subject 
s skin against a 24mm × 16mm aperture in a thin metal plate. Our small capture system (Fig. 2(a)) is 
a 12-light dome, where each light can produce both linear polarization conditions. The difference be­tween 
images acquired under parallel-and cross polarization isolate surface re.ectance and attenuate the blur 
of subsurface scattering. For BRDF .tting, we additionally acquire a single-light polariza­tion difference 
image. For smooth or oily skin patches, twelve light positions can yield separated specular highlights, 
biasing surface normal measurement. For higher angular resolution, we can alter­nately acquire microstructure 
with the macro camera and aperture frame inside the same 2.5m-diameter polarized LED sphere used for 
facial scanning (Fig. 2(b)). 3 Facial Microstructure Synthesis AA B B Figure 3: We add microstructural 
detail to a scanned facial region B using the analogous relationship between an exemplar microstructure 
patch A1 and a blurred version of it A which matches the mesostructural detail of B. We segment the face 
into seperate regions (forehead, nose, temple, cheek, and chin) and use the surface mesostructure evident 
in the full facial scan to guide the texture synthesis process for each facial region. We then merge 
the synthesized facial regions into a full 16K map of the microstructure. To do this, we derive displacement 
maps of both surface meso-(A, B) and microstructure (A ) from the measured specular normal maps (Fig. 
1, inset) and then syn­ thesize displacement maps with microstructure (B ) for the entire face using 
constrained texture synthesis based on Image Analogies [Hertzmann et al. 2001]. Finally, we render with 
BRDFs measured during microgeometry acquisition; greater detail is provided in the supplemental technical 
report [Graham et al. 2012]. References GRAHAM, P., TUNWATTANAPONG, B., BUSCH, J., YU, X., JONES, A., 
DEBEVEC, P., AND GHOSH, A. 2012. Measurement-based synthesis of facial microgeometry. Tech. Rep. ICT-TR-01-2012, 
USC-ICT. HERTZMANN, A., JACOBS, C. E., OLIVER, N., CURLESS, B., AND SALESIN, D. H. 2001. Image analogies. 
In Proceedings of the 28th annual conference on Computer graphics and inter­active techniques, ACM, New 
York, NY, USA, SIGGRAPH 01, 327 340. Copyright is held by the author / owner(s). SIGGRAPH 2012, Los Angeles, 
California, August 5 9, 2012. ISBN 978-1-4503-1435-0/12/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2342971</article_id>
		<sort_key>750</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>61</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>16</seq_no>
		<title><![CDATA[Panorama light-field imaging]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2342896.2342971</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2342971</url>
		<abstract>
			<par><![CDATA[<p>We present a first approach towards panorama light-field imaging. By converting overlapping sub-light-fields into individual focal stacks, computing a panoramic focal stack from them, and converting the panoramic focal stack back into a panoramic light field, we avoid the demand for a precise reconstruction of scene depth.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[computational photography]]></kw>
			<kw><![CDATA[light fields]]></kw>
			<kw><![CDATA[panorama]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.4.1</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P3737410</person_id>
				<author_profile_id><![CDATA[81503672096]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Clemens]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Birklbauer]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Johannes Kepler University Linz, Austria]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[clemens.birklbauer@jku.at]]></email_address>
			</au>
			<au>
				<person_id>P3737411</person_id>
				<author_profile_id><![CDATA[81100622976]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Oliver]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bimber]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Johannes Kepler University Linz, Austria]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[oliver.bimber@jku.at]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Levin, A., and Durand, F. 2010. Linear view synthesis using a dimensionality gap light field prior. In <i>Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on</i>, 1831--1838.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Panorama Light-Field Imaging Clemens Birklbauer and Oliver Bimber* Institute of Computer Graphics, Johannes 
Kepler University Linz, Austria  Figure 1: Images rendered from a 22 MPixels (spatial resolution: 17885x1275 
pixels) panoramic light .eld at different focus settings (far: top, near: center), and close-ups in native 
resolution (bottom). We thank the Raytrix GmbH for capturing and providing the raw data. Abstract We 
present a .rst approach towards panorama light-.eld imag­ing. By converting overlapping sub-light-.elds 
into individual focal stacks, computing a panoramic focal stack from them, and convert­ing the panoramic 
focal stack back into a panoramic light .eld, we avoid the demand for a precise reconstruction of scene 
depth. CR Categories: I.4.1 [IMAGE PROCESSING AND COMPUTER VISION]: Digitization and Image Cap-ture;I.3.3COMPUTER 
GRAPHICSPicture/Image Generation Keywords: light .elds, computational photography, panorama Links: DL 
PDF 1 Introduction With increasing resolution of imaging sensors, light-.eld photog­raphy is now becoming 
increasingly practical, and .rst light-.eld cameras are already commercially available (e.g., Lytro, 
Raytrix, and others). Applying common digital image processing techniques to light-.elds, however, is 
in many cases not straight forward. The reason for this is, that the outcome must not only be spatially 
con­sistent, but also directionally consistent. Otherwise, refocussing and perspective changes will cause 
strong image artifacts. Panorama imaging techniques, for example, are an integral part of digital photography 
 often being supported by camera hard­ware today. We present a .rst approach towards the construction 
*e-mail: {clemens.birklbauer,oliver.bimber}@jku.at of panoramic light-.elds (i.e., large .eld-of-view 
light-.elds com­puted from overlapping sub-light-.eld recordings). 2 Our Approach We capture overlapping 
sub-light-.elds of a scene during a cylindri­cal or spherical camera motion and convert each sub-light 
.eld into a focal stack using synthetic aperture reconstruction. For light-.eld cameras that directly 
deliver a focal stack, this step is not neces­sary. Next, we compute an all-in-focus image for each focal 
stack by extracting and composing the highest-frequency image content throughout all focal stack slices. 
The registration and blending pa­rameters are then computed for the resulting (overlapping) all-in­focus 
images. For this, we apply conventional panorama stitch­ing techniques, such as SURF feature extraction, 
pairwise feature matching and RANSAC outlier detection, bundle adjustment, wave correction, exposure 
compensation, and the computation of blend­ing seams. The registration and blending parameters that were 
de­rived for the all-in-focus images are now applied to all correspond­ing slices of the focal stacks. 
We use a four dimensional (three rotation and focal length) motion model for registration and multi­band 
blending for composition. The result is a registered and seam­lessly blended panoramic focal stack that 
can be converted to a light .eld with linear view synthesis, as described in [Levin and Durand 2010]. 
Since we chose an intermediate focal stack representation, common image panorama techniques can be applied 
for robustly computing a panoramic light-.eld without a precise reconstruction of scene depth. However, 
the focal stack of a scene covers no more than a 3D subset of the full 4D light .eld. This limits our 
current ap­proach to Lambertian scenes with modest depth discontinuities. In future, we will investigate 
panorama light-.eld imaging techniques that are applicable directly to 4D ray-space.  References LEVIN, 
A., AND DURAND, F. 2010. Linear view synthesis using a dimensionality gap light .eld prior. In Computer 
Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on, 1831 1838. Copyright is held by the author 
/ owner(s). SIGGRAPH 2012, Los Angeles, California, August 5 9, 2012. ISBN 978-1-4503-1435-0/12/0008 
  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2342972</article_id>
		<sort_key>760</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>62</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>17</seq_no>
		<title><![CDATA[Patching of moving objects for ghosting-free HDR synthesis]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2342896.2342972</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2342972</url>
		<abstract>
			<par><![CDATA[<p>It is very challenging to synthesize a ghosting-free high dynamic range (HDR) image using differently exposed low dynamic range (LDR) images with moving objects. To achieve this, an anti-ghosting scheme is required to detect and patch occluded motion regions. In this poster, a new optimization problem is formulated on patching motion regions along a differently exposed image sequence, which optimizes consistencies in both spatial and temporal domain. To solve this optimization problem, a hybrid patching scheme including pixel-level intensity mapping function and block-based template matching is proposed.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3737412</person_id>
				<author_profile_id><![CDATA[81464647082]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Jinghong]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Zheng]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Institute for Infocomm Research, Fusionopolis Way, Singapore]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[jzheng@i2r.a-star.edu.sg]]></email_address>
			</au>
			<au>
				<person_id>P3737413</person_id>
				<author_profile_id><![CDATA[81423596048]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Zhengguo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Li]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Institute for Infocomm Research, Fusionopolis Way, Singapore]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[ezgli@i2r.a-star.edu.sg]]></email_address>
			</au>
			<au>
				<person_id>P3737414</person_id>
				<author_profile_id><![CDATA[81474692949]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Zijian]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Zhu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Institute for Infocomm Research, Fusionopolis Way, Singapore]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[zhuzj@i2r.a-star.edu.sg]]></email_address>
			</au>
			<au>
				<person_id>P3737415</person_id>
				<author_profile_id><![CDATA[81323497843]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Shiqian]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Institute for Infocomm Research, Fusionopolis Way, Singapore]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[shiqian@i2r.a-star.edu.sg]]></email_address>
			</au>
			<au>
				<person_id>P3737416</person_id>
				<author_profile_id><![CDATA[81100244820]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Susanto]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Rahardja]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Institute for Infocomm Research, Fusionopolis Way, Singapore]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[rsusanto@i2r.a-star.edu.sg]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1819606</ref_obj_id>
				<ref_obj_pid>1819298</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Li, Z. G., Rahardja, S., Zhu, Z. J., Xie, S. L., and Wu, S. Q. 2010. Movement detection for the synthesis of high dynamic range images. In <i>IEEE Int. Conf. Image Processing</i>, 3133--3136.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>882269</ref_obj_id>
				<ref_obj_pid>1201775</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Perez, P., Gangnet, M., and Blake, A. 2003. Poisson image editing. In <i>ACM SIGGRAPH</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1187163</ref_obj_id>
				<ref_obj_pid>1187112</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Jacobs, K., Ward, G., and Loscos, C. 2005. Automatic HDRI generation of dynamic environments. In <i>ACM SIGGRAPH</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Patching of Moving Objects for Ghosting-free HDR Synthesis Jinghong Zheng, Zhengguo Li, Zijian Zhu, 
Shiqian Wu and Susanto Rahardja * Signal Processing Department, Institute for Infocomm Research, 1 Fusionopolis 
Way, Singapore 138632  Figure 1: (a)-(c) A set of differently exposed images with moving objects. The 
images produced by anti-ghosting schemes of (d) [Jacobs et al. 2005]; (e) Photoshop CS5; (f) [Li et al. 
2010]; (g) our hybrid patching scheme. Part of patched areas is highlighted by red box. 1 Introduction 
It is very challenging to synthesize a ghosting-free high dynamic range (HDR) image using differently 
exposed low dynamic range (LDR) images with moving objects. To achieve this, an anti­ghosting scheme 
is required to detect and patch occluded motion regions. In this poster, a new optimization problem is 
formulated on patching motion regions along a differently exposed image se­quence, which optimizes consistencies 
in both spatial and tempo­ral domain. To solve this optimization problem, a hybrid patching scheme including 
pixel-level intensity mapping function and block­based template matching is proposed . 2 Approach To 
compose a complete anti-ghosting scheme, the proposed patch­ing scheme is integrated with a detection 
module presented in [Liet al. 2010]. Suppose an image to be corrected is Zk0+1 and its reference image 
is Zk0 . The non-consistent pixels are detected by[Li et al. 2010] and labelled in a binary map. The 
goal of patching is to reconstruct new pixels Z k0+1(p) for all non-consistent pixels ¯ in a set Ck0+1 
by solving the following optimization problem: arg min .k0+1,k0 (Z k0+1(p)) - Zk0 (p) 2, (1) Z k0+1(p) 
p. ¯ Ck0+1 subject to Zk0+1(p)|. ¯ = Zk0+1(p)|. ¯ , (2) Ck0+1 Ck0+1 [ .. .. where V. = .x , .y ] is 
the gradient operator, and .k0+1,k0 is an intensity mapping function (IMF) from Zk0+1 to Zk0 [Li et al. 
¯ 2010]. .C¯k0+1 is the boundary of Ck0+1. Solution of the new problem (1) is very challenging, as it 
includesa non-linear function .k0+1,k0 . Instead of directly solving the newproblem (1), it is converted 
into an optimization problem that wasde.ned in [Perez et al. 2003] as arg min Z k0+1(p) - V (p) 2, 
subject to (2), (3) Z k0+1(p) p. ¯ Ck0+1 where V is a guidance .eld on detail information of reconstructed 
motion regions. Then a vector .eld V and initial value Z k0 0+1 is ¯ built up for all p . Ck0+1 by using 
a hybrid patching scheme, which exploits the spatial redundancy of Zk0+1 and the temporal correlation 
between Zk0+1 and Zk0 . *e-mail:{jzheng, ezgli, zhuzj, shiqian, rsusanto}@i2r.a-star.edu.sg The hybrid 
patching scheme includes two correction methods based on pixel-level IMF and block-level template matching 
respectively. For the motion regions where collocated areas in Zk0 are well ex­posed, IMF based correction 
scheme is applied. An auxiliary pixel Z k0 0+1(p) is computed as .k0,k0+1(Zk0 (p)) which will be used 
to compute vector .led V . For the remaining parts of motion re­gions that IMF based correction is not 
applicable, block-level tem­plate matching based correction scheme is applied to compute vec­tor .eld 
V and initial value Z k0 0+1. Let Bk0+1,i denote a block at position i in Zk0+1, containing non-consistent 
pixels. A match­ing search, spanning a searching window, is conducted to .nd a best-match block. Then 
the pixels of best-match block are used to replace the non-consistent pixels in Bk0+1,i. A new matching 
cri­terion is proposed for searching best-match block, which includes two parts: one measures the spatial 
similarity of consistent pixels in Bk0+1,i and their counterparts of candidate block in Zk0+1; and the 
other measures the similarity of collocated blocks in Zk0 . After template matching correction, auxiliary 
pixels can be obtained as Z k0 0+1(p) s, which are critical for convergence speed. The vector .eld V 
can then be computed. To include all available information, the gradients from both Zk0 and Zk0+1 are 
adopted to compose V . ¯ For a pixel Zk0+1(p)(p . Ck0+1), if its collocated surrounding pixels in Zk0 
are well exposed, then the gradients VZk0 (p) is con­sidered as reliable, and V (p) is computed as V.k0,k0+1(Zk0 
(p)). Otherwise, the gradient VZ k0 0+1(p) is adopted. It is worth noting that Z k0 0+1 and V are selected 
such that the problem (3) is equiva­lent to the problem (1). With Z k0 0+1 and V , the motion regions 
can be reconstructed by solving problem (3). We compare the proposed scheme with anti-ghosting approaches 
of [Jacobs et al. 2005], [Li et al. 2010] and Photoshop CS5. In [Jacobs et al. 2005], each motion region 
is presented by one image contain­ing the least saturation in that particular area. Phothshop CS5, [Li 
et al. 2010] and proposed scheme synchronize motion regions ac­cording to a pre-selected input image. 
The ghosting effects have been signi.cantly alleviated by proposed scheme. References LI, Z. G., RAHARDJA, 
S., ZHU, Z. J., XIE, S. L., AND WU, S. Q. 2010. Movement detection for the synthesis of high dynamic 
range images. In IEEE Int. Conf. Image Processing, 3133-3136. PEREZ,P., GANGNET, M., AND BLAKE, A. 2003. 
Poisson image editing. In ACM SIGGRAPH. JACOBS,K., WARD, G., AND LOSCOS, C. 2005. Automatic HDRI generation 
of dynamic environments. In ACM SIGGRAPH. Copyright is held by the author / owner(s). SIGGRAPH 2012, 
Los Angeles, California, August 5 9, 2012. ISBN 978-1-4503-1435-0/12/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2342973</article_id>
		<sort_key>770</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>63</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>18</seq_no>
		<title><![CDATA[Perceptually-optimized content remapping for automultiscopic displays]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2342896.2342973</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2342973</url>
		<abstract>
			<par><![CDATA[<p>3D content and display technology is now widely available. However, available displays range from large-screen cinematic projection systems to hand-held devices and from screens supporting glasses-free 3D modes to glasses-bound systems. Counterintuitively, the produced content is usually only generated for a single display configuration, making labor-intense, manual post-processing of the data necessary. Recently, several content remapping techniques for stereo content have been proposed (see e.g. [Lang et al. 2010]). We present a perceptually-driven optimization framework for automatic light field remapping, specially designed for <i>automultiscopic</i> displays. It takes into account both the limitations of the target display as well as those of a human observer, and poses the problem as a non-linear least squares optimization. Our model includes depth of field limitations and the contrast sensitivity function, as well as sensitivities to binocular disparity and motion parallax in the perception of depth. Additionally, the model can be adapted for more common stereo remapping.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3737417</person_id>
				<author_profile_id><![CDATA[81448601683]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Belen]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Masia]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Universidad de Zaragoza and MIT Media Lab]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737418</person_id>
				<author_profile_id><![CDATA[81335499586]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Gordon]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wetzstein]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MIT Media Lab]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737419</person_id>
				<author_profile_id><![CDATA[81504688294]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Carlos]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Aliaga]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Universidad de Zaragoza]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737420</person_id>
				<author_profile_id><![CDATA[81100022847]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Ramesh]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Raskar]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MIT Media Lab]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737421</person_id>
				<author_profile_id><![CDATA[81100022708]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Diego]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gutierrez]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Universidad de Zaragoza]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Bradshaw, M. F., Hibbard, P. B., Parton, A. D., Rose, D., and Langley, K. 2006. Surface orientation, modulation frequency and the detection and perception of depth defined by binocular disparity and motion parallax. <i>Vision Research 46</i> (September), 2636--2644.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1964991</ref_obj_id>
				<ref_obj_pid>2010324</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Didyk, P., Ritschel, T., Eisemann, E., Myszkowski, K., and Seidel, H.-P. 2011. A perceptual model for disparity. <i>ACM Trans. Graph. 30</i>, 96:1--96:10.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1778812</ref_obj_id>
				<ref_obj_pid>1778765</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Lang, M., Hornung, A., Wang, O., Poulakos, S., Smolic, A., and Gross, M. 2010. Nonlinear disparity mapping for stereoscopic 3d. <i>ACM Trans. Graph. 29</i> (July), 75:1--75:10.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1964935</ref_obj_id>
				<ref_obj_pid>2010324</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Mantiuk, R., Kim, K. J., Rempel, A. G., and Heidrich, W. 2011. HDR-VDP-2: A calibrated visual metric for visibility and quality predictions in all luminance conditions. <i>ACM Trans. Graph. 30</i>, 40:1--40:13.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2185576</ref_obj_id>
				<ref_obj_pid>2185520</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Wetzstein, G., Lanman, D., Hirsch, M., and Raskar, R. 2012. Tensor Displays: Compressive Light Field Display using Multilayer Displays with Directional Backlighting. <i>ACM Trans. Graph. 31</i>, 1--11.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Perceptually-Optimized Content Remapping for Automultiscopic Displays Belen Masia1,2 Gordon Wetzstein2 
Carlos Aliaga1 Ramesh Raskar2 Diego Gutierrez1 1Universidad de Zaragoza 2MIT Media Lab Figure 1: Light 
.eld retargeting for automultiscopic 3D displays. Just like cameras, all automultiscopic 3D displays 
exhibit a limited depth of .eld that blurs virtual objects extruding from the physical display enclosure 
(top right). Using a perceptually-driven optimization framework, we retarget an input light .eld (left 
column) compressing the depth of the scene (center column) in such a way that the perceived 3D appearance 
is preserved but we also retain sharp details of the observed images (bottom right). Introduction 3D 
content and display technology is now widely for per-pixel depth d: available. However, available displays 
range from large-screen cin­ arg min (µDOF I.CSF (.S (Lorig) - .S (fb (Lorig,d)))Iematic projection systems 
to hand-held devices and from screens d 22 supporting glasses-free 3D modes to glasses-bound systems. 
Coun­ + µBD I.BD (.L (f. (Dorig)) - .L (f. (d)))I terintuitively, the produced content is usually only 
generated 22 + µMP I.MP (.L (f. (Dorig)) - .L (f. (d)))I 22 for a single display con.guration, making 
labor-intense, manual post-processing of the data necessary. Recently, several content ) (1) where µDOF 
, µBD and µMP are the weights given to each of the remapping techniques for stereo content have been 
proposed (see e.g. [Lang et al. 2010]). We present a perceptually-driven opti­mization framework for 
automatic light .eld remapping, specially designed for automultiscopic displays. It takes into account 
both the limitations of the target display as well as those of a human observer, and poses the problem 
as a non-linear least squares op­timization. Our model includes depth of .eld limitations and the contrast 
sensitivity function, as well as sensitivities to binocular dis­parity and motion parallax in the perception 
of depth. Additionally, the model can be adapted for more common stereo remapping. Disparity Remapping 
Glasses-free automultiscopic displays share the vergence-accommodation mismatch of stereoscopic dis­plays. 
Their main limitation, however, is the reduced depth of .eld (similar to cameras with large apertures) 
[Wetzstein et al. 2012]; hence, 3D objects extruding from the physical display enclosure ap­pear blurred 
(see Fig. 1, top right). To preserve the 3D appearance of the scene but also sharp image details (Fig. 
1, bottom right), our objective function includes a term that accounts for the perceived blur using a 
model for contrast sensitivity [Mantiuk et al. 2011]. The most important depth cues provided by automultiscopic 
dis­plays not supported by conventional 2D displays are binocular dis­parity and motion parallax. Our 
function includes two terms which aim at preserving the original depthmap. These terms are based on published 
perceptual .ndings, and take into account our ability to discriminate depth from these two cues. Sensitivity 
to both cues de­pends on the spatial frequency of the signal, and thus a muti-scale pyramid decomposition 
is needed. For binocular disparity our func­tion incorporates threshold detection and discrimination 
data from Didyk et al. [2011]. For motion parallax, detection thresholds come from experiments by Bradshaw 
and colleagues [2006]. Our objective function has the following form, which we optimize terms. fb (L, 
d) models the blurring of the original luminance im­age Lorig in a depth-dependent manner, and Dorig 
represents the original depthmap, converted into vergence values with operator f. (·). The multi-scale 
decompositions into frequency levels are given by .S (·) and .L (·). Finally, .CSF , .BD and .MP are 
the weights accounting for sensitivity to the different aspects involved, explained above. For further 
details on our objective function and explanatory .gures, please refer to the supplementary material. 
Discussion As opposed to previous disparity remapping ap­proaches, which focused on stereo displays, 
our framework auto­matically retargets an input light .eld for a given automultiscopic display. Our model 
is the .rst to take into account both the speci.c limitations of such displays, as well as the characteristics 
of the hu­man visual system. As shown in the supplementary material, it can easily be adapted for stereo 
content as well. References BRADSHAW, M. F., HIBBARD, P. B., PARTON, A. D., ROSE, D., AND LANGLEY, K. 
2006. Surface orientation, modulation frequency and the detection and perception of depth de.ned by binocular 
disparity and motion parallax. Vision Research 46 (September), 2636 2644. DIDYK, P., RITSCHEL, T., EISEMANN, 
E., MYSZKOWSKI, K., AND SEIDEL, H.-P. 2011. A perceptual model for disparity. ACM Trans. Graph. 30, 96:1 
96:10. LANG, M., HORNUNG, A., WANG, O., POULAKOS, S., SMOLIC, A., AND GROSS, M. 2010. Nonlinear disparity 
mapping for stereoscopic 3d. ACM Trans. Graph. 29 (July), 75:1 75:10. MANTIUK, R., KIM, K. J., REMPEL, 
A. G., AND HEIDRICH, W. 2011. HDR-VDP­ 2: A calibrated visual metric for visibility and quality predictions 
in all luminance conditions. ACM Trans. Graph. 30, 40:1 40:13. WETZSTEIN, G., LANMAN, D., HIRSCH, M., 
AND RASKAR, R. 2012. Tensor Dis­ plays: Compressive Light Field Display using Multilayer Displays with 
Directional Backlighting. ACM Trans. Graph. 31, 1 11. Copyright is held by the author / owner(s). SIGGRAPH 
2012, Los Angeles, California, August 5 9, 2012. ISBN 978-1-4503-1435-0/12/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2342974</article_id>
		<sort_key>780</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>64</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>19</seq_no>
		<title><![CDATA[Physical simulation for real-time image/video retargeting]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2342896.2342974</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2342974</url>
		<abstract>
			<par><![CDATA[<p>Retargeting methods present content on arbitrary aspect ratio media displays limiting distortions in relevant objects. This is done by means of applying non-homogeneous resizing operators across the whole media, constraining it to fit into the required size. Several succesful systems have been proposed to achieve image retargeting, while video retargeting is still challenging due to time consistency and computational complexity requirements. Two important contributes were proposed: non-homogeneous retargeting [Wolf et al. 2007] and improved seam-carving [Rubinstein et al. 2008]. The first one claims to achieve real-time performance but considers spatial coordinates separately. The latter do preserve media structure, but sometimes introduces artifacts and does not provide real-time performance. In addition it is not designed for streaming purposes. We show how a physical simulation solves the retargeting problem and the relative issues, such as time consistence, reporting how this is feasible in real-time. Moreover, at each step the system considers only two consequent frames, being able to deal with streaming media.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3737422</person_id>
				<author_profile_id><![CDATA[81375621442]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Roberto]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gallea]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[DICGIM - Universit&#225; degli studi di Palermo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[roberto.gallea@unipa.it]]></email_address>
			</au>
			<au>
				<person_id>P3737423</person_id>
				<author_profile_id><![CDATA[81100227835]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Edoardo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ardizzone]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[DICGIM - Universit&#225; degli studi di Palermo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737424</person_id>
				<author_profile_id><![CDATA[81100509827]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Roberto]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Pirrone]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[DICGIM - Universit&#225; degli studi di Palermo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>2192179</ref_obj_id>
				<ref_obj_pid>2191740</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Cheng, M.-M., Zhang, G.-X., Mitra, N. J., Huang, X., and Hu, S.-M. 2011. Global contrast based salient region detection. In <i>IEEE CVPR</i>, 409--416.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1360615</ref_obj_id>
				<ref_obj_pid>1360612</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Rubinstein, M., Shamir, A., and Avidan, S. 2008. Improved seam carving for video retargeting. <i>ACM Transactions on Graphics (SIGGRAPH) 27</i>, 3, 1--9.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Wolf, L., Guttmann, M., and Cohen-Or, D. 2007. Non-homogeneous content-driven video-retargeting. In <i>Proceedings of the Eleventh IEEE International Conference on Computer Vision (ICCV-07)</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Physical simulation for real-time image/video retargeting Roberto Gallea, Edoardo Ardizzone, Roberto 
Pirrone. DICGIM -Universita degli studi di Palermo*  Figure 1: We retarget media by considering them 
as viscous objects whose local stiffness is related to local visual saliency. Object particles are modeled 
as mass-points connected by springs. Stiffer springs correspond to high-content regions which are preserved, 
while deformations are spread across pliable ones. Time consistency for videos is assured using strong 
connections between correspondent nodes in consequent frames planes. The whole method is implemented 
as an ODE system implemented on a GPU-based lattice providing real-time performances. 1 Introduction 
Retargeting methods present content on arbitrary aspect ratio me­dia displays limiting distortions in 
relevant objects. This is done by means of applying non-homogeneous resizing operators across the whole 
media, constraining it to .t into the required size. Several succesful systems have been proposed to 
achieve image retargeting, while video retargeting is still challenging due to time consistency and computational 
complexity requirements. Two important con­tributes were proposed: non-homogeneous retargeting [Wolf 
et al. 2007] and improved seam-carving [Rubinstein et al. 2008]. The .rst one claims to achieve real-time 
performance but considers spa­tial coordinates separately. The latter do preserve media structure, but 
sometimes introduces artifacts and does not provide real-time performance. In addition it is not designed 
for streaming purposes. We show how a physical simulation solves the retargeting problem and the relative 
issues, such as time consistence, reporting how this is feasible in real-time. Moreover, at each step 
the system consid­ers only two consequent frames, being able to deal with streaming media. 2 Proposed 
model and implementation In a nutshell, the proposed method models the media as a viscous physical object 
(on an xyt coordinate system) composed by par­ticles. Relations between particles are modelled as springs 
with variable stiffness. By stretching or compressing the media, elas­tic forces arise and it gets deformed 
properly until an equilibrium is reached. The result is a retargeted media. Content preservation is realized 
by relating content saliency proportionally to local stiff­ness. Just few constraints are required: 1) 
After the deformation, * e-mail: roberto.gallea@unipa.it pixels are forced to remain on the original 
frame, 2) Frames border pixels in original media should remain border pixels in the retar­geted version 
too (optional). Saliency is extracted using a contrast­based method [Cheng et al. 2011]. For the retargeting 
purpose, a grid consisting of 16x16 nodes is used. However, 8x8 or 12x12 nodes are often suf.cient. In 
order to retarget streaming media, when considering the i-th frame, the result of the (i-1)-th is taken 
as a starting point and high­stiffness springs are connected between two corresponding nodes of the two 
frames. This constrains any deformation taking place af­ter the normal dynamic of individual frames to 
be similar between neighboring frames. The method is implemented as a parallel ODE system, solved ex­plicitly 
using a 4th order Runge-Kutta integration method, taking advantage of gpGPU parallelism. References 
CHENG, M.-M., ZHANG, G.-X., MITRA, N. J., HUANG, X., AND HU, S.-M. 2011. Global contrast based salient 
region detection. In IEEE CVPR, 409 416. RUBINSTEIN, M., SHAMIR, A., AND AVIDAN, S. 2008. Im­proved seam 
carving for video retargeting. ACM Transactions on Graphics (SIGGRAPH) 27, 3, 1 9. WOLF, L., GUTTMANN, 
M., AND COHEN-OR, D. 2007. Non­homogeneous content-driven video-retargeting. In Proceedings of the Eleventh 
IEEE International Conference on Computer Vi­sion (ICCV-07). Copyright is held by the author / owner(s). 
SIGGRAPH 2012, Los Angeles, California, August 5 9, 2012. ISBN 978-1-4503-1435-0/12/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2342975</article_id>
		<sort_key>790</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>65</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>20</seq_no>
		<title><![CDATA[Real-time HDR video reconstruction for multi-sensor systems]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2342896.2342975</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2342975</url>
		<abstract>
			<par><![CDATA[<p>HDR video is an emerging field of technology, with a few camera systems currently in existence [Myszkowski et al. 2008], Multi-sensor systems [Tocci et al. 2011] have recently proved to be particularly promising due to superior robustness against temporal artifacts, correct motion blur, and high light efficiency. Previous HDR reconstruction methods for multi-sensor systems have assumed pixel perfect alignment of the physical sensors. This is, however, very difficult to achieve in practice. It may even be the case that reflections in beam splitters make it impossible to match the arrangement of the Bayer filters between sensors. We therefor present a novel reconstruction method specifically designed to handle the case of non-negligible misalignments between the sensors. Furthermore, while previous reconstruction techniques have considered HDR assembly, debayering and denoising as separate problems, our method is capable of simultaneous HDR assembly, debayering and smoothing of the data (denoising). The method is also general in that it allows reconstruction to an arbitrary output resolution and mapping. The algorithm is implemented in CUDA, and shows video speed performance for an experimental HDR video platform consisting of four 2336x1756 pixels high quality CCD sensors imaging the scene trough a common optical system. ND-filters of different densities are placed in front of the sensors to capture a dynamic range of 24 <i>f</i>-stops.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3737425</person_id>
				<author_profile_id><![CDATA[81488647582]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Joel]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kronander]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Link&#246;ping University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[joel.kronander@liu.se]]></email_address>
			</au>
			<au>
				<person_id>P3737426</person_id>
				<author_profile_id><![CDATA[81320490266]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Stefan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gustavson]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Link&#246;ping University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[stefan.gustavson@liu.se]]></email_address>
			</au>
			<au>
				<person_id>P3737427</person_id>
				<author_profile_id><![CDATA[81100120690]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Jonas]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Unger]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Link&#246;ping University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[jonas.unger@liu.se]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1481300</ref_obj_id>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Myszkowski, K., Mantiuk, R., and Krawczyk, G. 2008. <i>High Dynamic Range Video</i>. Morgan &amp; Claypool.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1964936</ref_obj_id>
				<ref_obj_pid>2010324</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Tocci, M. D., Kiser, C., Tocci, N., and Sen, P. 2011. A Versatile HDR Video Production System. <i>ACM Transactions on Graphics (TOG) (Proceedings of SIGGRAPH 2011) 30</i>, 4.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Real-time HDR video reconstruction for multi-sensor systems Joel Kronander, Stefan Gustavson, Jonas 
Unger * Link¨ oping University Figure 1: (a) Sketch of a typical multi-sensor HDR capturing system. 
(b) Assuming a known transformation sensor images can be warped to a common reference coordinate system, 
note that due to non-perfect pixel alignment, transformed sensor pixels will generally be irregularly 
distributed in the reference grid. The reconstruction at a point zj uses measured sensor pixels from 
all sensors inside the support of a .nite window function with radius w. (c) Shows a tonemapped HDR video 
frame reconstructed at an resolution of 2400x1700 using our method. 1 Introduction HDR video is an emerging 
.eld of technology, with a few camera systems currently in existence [Myszkowski et al. 2008]. Multi­sensor 
systems [Tocci et al. 2011] have recently proved to be par­ ticularly promising due to superior robustness 
against temporal ar­tifacts, correct motion blur, and high light ef.ciency. Previous HDR reconstruction 
methods for multi-sensor systems have as­sumed pixel perfect alignment of the physical sensors. This 
is, however, very dif.cult to achieve in practice. It may even be the case that re.ections in beam splitters 
make it impossible to match the arrangement of the Bayer .lters between sensors. We therefor present 
a novel reconstruction method speci.cally designed to han­dle the case of non-negligible misalignments 
between the sensors. Furthermore, while previous reconstruction techniques have con­sidered HDR assembly, 
debayering and denoising as separate prob­lems, our method is capable of simultaneous HDR assembly, de­bayering 
and smoothing of the data (denoising). The method is also general in that it allows reconstruction to 
an arbitrary output reso­lution and mapping. The algorithm is implemented in CUDA, and shows video speed 
performance for an experimental HDR video platform consisting of four 2336x1756 pixels high quality CCD 
sensors imaging the scene trough a common optical system. ND­.lters of different densities are placed 
in front of the sensors to cap­ture a dynamic range of 24 f -stops. 2 Reconstruction We treat the reconstruction 
of each HDR video frame, F , as a sep­arate problem. Using N sensors, the input data for each frame con­sists 
of a set of raw images, Is(i, j) s =1...N, each with a possibly varying exposure time, ts and ND-.lter 
coef.cient, ns. We assume that there exists an af.ne transform, Ts relating each image to a vir­tual 
reference coordinate system, in which we seek to reconstruct the HDR output image. In practice we .nd 
the transforms, Ts, for each sensor matching the detected corners of a chessboard calibra­tion target 
(implemented in OpenCV 2.1), for our system, we obtain a maximum reprojection error of 0.1 pixels across 
the image. As a .rst step in the reconstruction, we perform shading correction *e-mail: {joel.kronander, 
stefan.gustavson, jonas.unger}@liu.se for all sensor images, linearizing the sensor output to a common 
scale of reference. We then map all sensor images (pixel measure­ments) to the output coordinate system. 
This generally produces an irregular distribution of red, green and blue pixel samples, ys,i,c captured 
from sensors s =1..N , using a lexicographical ordering index i, see Figure 1(a). We now seek to reconstruct 
the output image in a regular grid, with spacings corresponding to the desired output resolution. For 
each output pixel location, zj , we compute a locally weighted average of nearby pixel measurements, 
ys,c,i, for each color channel c = R, G, B. w(j, s, i, c)ys,i,c z j,c =( (1) w(j, s, i, c) si We compute 
the weights based on two criteria, w(j, s, i, c)= wg(i, j) · wr(j, s, i, c). The .rst factor wg(j, i) 
is a windowing function of .nite support, giving higher weights to spatially nearby samples, in our implementation 
we use a Gaussian function with the euclidean distance as argument. The second factor is a radio­metric 
weight, wr(j, s, i, c). This weight is set according to a linear function of the raw digital input value 
with a small offset from the blacklevel level and saturation point. Our method thus automati­cally removes 
saturated pixels before reconstruction/debayering by setting their radiometric weight to zero. Figure 
1(c) shows a reconstructed HDR frame from our experimen­tal HDR video platform. The CUDA implementation, 
running on an NVidia 580, performs simultaneous debayering, .ltering and HDR reconstruction on the four 
4Mpixel input Bayer pattern im­ages at 26 fps sustained rate with an output resolution of 1168x876 pixels. 
Compared to methods considering the HDR reconstruction and debayering in separate steps, our method offers 
comparable quality and provides more .exibility in the choice of output res­olution and mapping. References 
MYSZKOWSKI, K., MANTIUK, R., AND KRAWCZYK, G. 2008. High Dynamic Range Video. Morgan &#38; Claypool. 
TOCCI, M. D., KISER, C., TOCCI, N., AND SEN, P. 2011. A Versatile HDR Video Production System. ACM Transactions 
on Graphics (TOG) (Proceedings of SIGGRAPH 2011) 30, 4. Copyright is held by the author / owner(s). SIGGRAPH 
2012, Los Angeles, California, August 5 9, 2012. ISBN 978-1-4503-1435-0/12/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2342976</article_id>
		<sort_key>800</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>66</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>21</seq_no>
		<title><![CDATA[Turning photographs into abstract expressionist paintings]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2342896.2342976</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2342976</url>
		<abstract>
			<par><![CDATA[<p>During the recent history of painting there has been several movements who tried to obtain an illusion of flattened 3D space. Cubists employed multi-perspective views to flatten the 3D space [Meadows and Akleman 2000]. Impressionists and abstract expressionists, on the other hand, flattened 3D space with layers of paints to make objects fuzzy. A particular paint layering technique is impasto, which is introduced by impressionist artists such as Claude Monet or Vincent VanGogh [Schaefer et al. 2008]. Another paint layering technique is drip painting, which is introduced by abstract expressionist Jackson Pollock [Taylor 1999]. Abstract expressionism became popularized throughout the 20th century with artists such as Robert Jay Wolff, Franz Kline, Willem de Kooning, Larry Rivers, and Robert Motherwell [Ross 1990]. Unlike Pollock, most abstract expressionist painters uses impasto to obtain flatten effect and further transform images from flattened 3D representational form to abstract. Many of the abstract expressionists painted layers upon layers of paint until they were satisfied with a result. For instance, in <i>Woman</i> by Willem de Kooning, we may make sense of a set of eyes and a mouth, but it is really through the name of the painting that we associate a human form with the image. This abstraction is obtained by applying paint in consistent impastos which thin out to the canvas in a few places while rising elsewhere to heavy ridges.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3737428</person_id>
				<author_profile_id><![CDATA[81504685409]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Youyou]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Texas A&M University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737429</person_id>
				<author_profile_id><![CDATA[81100124987]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Ergun]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Akleman]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Texas A&M University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>850409</ref_obj_id>
				<ref_obj_pid>518910</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Meadows, S., and Akleman, E., 2000. Abstract digital paintings created with painting camera technique. Proceedings of D'ART 2000/Information Visualization.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Ross, C., 1990. Abstract expressionism: Creators and critics. Abrams Books, New York.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Schaefer, I., von Saint-George, C., and Lewerentz, K., 2008. Painting light: The hidden techniques of the impressionists. Skira Rizzoli, New York.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Taylor, R. P., 1999. Fractal analysis of pollock's drip paintings. Nature, June 1999, pp. 399--422.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Turning Photographs into Abstract Expressionist Paintings Youyou Wang Ergun Akleman Department of Computer 
Science Visualization Department Texas A&#38;M University Texas A&#38;M University  (a) Original (b) 
Result (c) Detail Figure 1: Two examples of paintings obtained from photographs using successive application 
of Fretwork operator. This operation makes image .atter, but increase local variances as shown in close-up 
images. During the recent history of painting there has been several movements who tried to obtain an 
illusion of .attened 3D space. Cubists employed multi-perspective views to .atten the 3D space [Meadows 
and Akleman 2000]. Impressionists and abstract expressionists, on the other hand, .attened 3D space with 
layers of paints to make objects fuzzy. A par­ticular paint layering technique is impasto, which is intro­duced 
by impressionist artists such as Claude Monet or Vin­cent VanGogh [Schaefer et al. 2008]. Another paint 
layering technique is drip painting, which is introduced by abstract expressionist Jackson Pollock [Taylor 
1999]. Abstract ex­pressionism became popularized throughout the 20th cen­tury with artists such as Robert 
Jay Wol., Franz Kline, Willem de Kooning, Larry Rivers, and Robert Motherwell [Ross 1990]. Unlike Pollock, 
most abstract expressionist painters uses impasto to obtain .atten e.ect and further transform images 
from .attened 3D representational form to abstract. Many of the abstract expressionists painted layers 
upon layers of paint until they were satis.ed with a result. For instance, in Woman by Willem de Kooning, 
we may make sense of a set of eyes and a mouth, but it is re­ally through the name of the painting that 
we associate a human form with the image. This abstraction is obtained by applying paint in consistent 
impastos which thin out to the canvas in a few places while rising elsewhere to heavy ridges. We present 
an approach to convert photographs into ab­stract paintings in a conceptually similar way to expression­ist 
painting. The basic concept of our approach is simply keeping foreground high frequency regions while 
replacing foreground low frequency regions with background high fre­quency regions. To construct such 
images, we introduce an operator that creates holes in images by making low fre­quency regions transparent 
such that when combined with a background image, low transparency regions will be replaced by the corresponding 
parts of background image. This op­eration, when applied only once, does not guarantee the re­moval of 
all low frequency regions since background image may also introduce some low frequency regions. However, 
if the operation is applied repeatedly using randomly trans­formed versions of background image, low 
frequency regions diminishes after a few iteration and the resulting images con­sist of mainly high frequency 
components. By changing the hole sizes of the operator, we provide a consistent transition from .attened 
3D representations to abstract ones. Small hole sizes results .attened 3D repre­sentations that resemble 
paintings such as Kooning s or a Monet s. On the other hand, the large hole sizes causes the forms in 
original photograph becomes unrecognizable resulting images that resemble abstract paintings such as 
Pollack s. References Meadows, S., and Akleman, E., 2000. Abstract digital paintings created with painting 
camera technique. Pro­ ceedings of D ART 2000 / Information Visualization. Ross, C., 1990. Abstract expressionism 
: Creators and crit­ics. Abrams Books, New York. Schaefer, I., von Saint-George, C., and Lewerentz, K., 
2008. Painting light : The hidden techniques of the impressionists. Skira Rizzoli, New York. Taylor, 
R. P., 1999. Fractal analysis of pollock s drip paintings. Nature, June 1999, pp. 399-422. Copyright 
is held by the author / owner(s). SIGGRAPH 2012, Los Angeles, California, August 5 9, 2012. ISBN 978-1-4503-1435-0/12/0008 
 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2342977</article_id>
		<sort_key>810</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>67</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>22</seq_no>
		<title><![CDATA[Use of CUDA streams for block-based MPEG motion estimation on the GPU]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2342896.2342977</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2342977</url>
		<abstract>
			<par><![CDATA[<p>The H.264 standard of MPEG-4 includes motion estimation that takes about 91% of encoding time. Luckily, the problem of block-based motion estimation is highly parallel. Motion vectors are calculated by determining block displacement within an area, typically 32 x 32 pixels, in a known reference frame. We enhance the GPU-based Sum of Absolute Difference (SAD) calculations of motion estimation using CUDA streams to hide memory latency by means of different overlapping techniques. A novel implementation strategy is explored that takes advantage of the amount of shared memory available in GPU devices of compute capability 2.x.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[GPU computing]]></kw>
			<kw><![CDATA[MPEG-4]]></kw>
			<kw><![CDATA[motion estimation]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.1</cat_node>
				<descriptor>Graphics processors</descriptor>
				<type>S</type>
			</primary_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010389</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Graphics processors</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010389</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Graphics processors</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P3737430</person_id>
				<author_profile_id><![CDATA[81461658099]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Mai]]></first_name>
				<middle_name><![CDATA[H.]]></middle_name>
				<last_name><![CDATA[El-Shehaly]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Virginia Tech]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737431</person_id>
				<author_profile_id><![CDATA[81100637283]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Denis]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gra&#269;anin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Virginia Tech]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737432</person_id>
				<author_profile_id><![CDATA[81314493445]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Hicham]]></first_name>
				<middle_name><![CDATA[G.]]></middle_name>
				<last_name><![CDATA[Elmongui]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Alexandria University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Chen, W., and Hang, H. 2008. H.264/AVC motion estimation implmentation on compute unified device architecture (CUDA). In <i>Proceedings of the 2008 IEEE International Conference on Multimedia and Expo</i>, IEEE, 697--700.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Harris, M. 2007. Optimizing parallel reduction in CUDA. CUDA SDK white paper, NVidia.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	</section>
	<section>
		<section_id>2342978</section_id>
		<sort_key>820</sort_key>
		<section_seq_no>15</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Interaction]]></section_title>
		<section_page_from>15</section_page_from>
	<article_rec>
		<article_id>2342979</article_id>
		<sort_key>830</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>68</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Active touch sensing of being-pulled illusion for pedestrian route navigation]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2342896.2342979</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2342979</url>
		<abstract>
			<par><![CDATA[<p>Locomotion is a vital activity for human beings. When walking through unfamiliar places such as a large convention center, we usually rely on visual information from a map or a compass. In contrast, kinesthetic cues are intuitive for all users to indicate a certain direction such as lead-by-hand navigation or a guide dog for people with visual impairment. However, mobile devices have not provided a stable pulling or pushing force feedback because both the user and device must be physically connected to an external ground to provide it. Over the last several years, we have designed and developed several prototypes to generate asymmetric oscillation [Amemiya et al. 2006], and succeeded in creating a sensation of being pulled without grounding by using a sensory illusion produced by the asymmetric oscillation [Amemiya and Sugiyama 2010].</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3737433</person_id>
				<author_profile_id><![CDATA[81100595703]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Tomohiro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Amemiya]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[NTT Communication Science Laboratories]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[amemiya.tomohiro@lab.ntt.co.jp]]></email_address>
			</au>
			<au>
				<person_id>P3737434</person_id>
				<author_profile_id><![CDATA[81504682085]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Hiroaki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gomi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[NTT Communication Science Laboratories]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1857923</ref_obj_id>
				<ref_obj_pid>1857920</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Amemiya, T., and Sugiyama, H. 2010. Orienting kinesthetically: A haptic handheld wayfinder for people with visual impairments. <i>ACM Trans. Access. Comput. 3</i>, 2, 6:1--6:23.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1179160</ref_obj_id>
				<ref_obj_pid>1179133</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Amemiya, T., Ando, H., and Maeda, T. 2006. Perceptual attraction force: the sixth force. In <i>SIGGRAPH 2006 Emerging technologies</i>, ACM Press, 26.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Active Touch Sensing of Being-Pulled Illusion for Pedestrian Route Navigation Tomohiro Amemiya* Hiroaki 
Gomi NTT Communication Science Laboratories Figure 1: (a) Proposed novel three-DoF force feedback display. 
(b) Superimposed walking picture every 3 sec. 1 Introduction Locomotion is a vital activity for human 
beings. When walking through unfamiliar places such as a large convention center, we usu­ally rely on 
visual information from a map or a compass. In con­trast, kinesthetic cues are intuitive for all users 
to indicate a certain direction such as lead-by-hand navigation or a guide dog for people with visual 
impairment. However, mobile devices have not pro­vided a stable pulling or pushing force feedback because 
both the user and device must be physically connected to an external ground to provide it. Over the last 
several years, we have designed and developed several prototypes to generate asymmetric oscillation [Amemiya 
et al. 2006], and succeeded in creating a sensation of be­ing pulled without grounding by using a sensory 
illusion produced by the asymmetric oscillation [Amemiya and Sugiyama 2010]. In this work, we propose 
an intuitive pedestrian navigation system with our novel prototype of a force display. The force display 
is the smallest and lightest ever with three degrees of freedom (DoF) and implements a navigation system 
that tracks the position and orientation of the user. The tracking system helps the user walk along a 
path sequentially from point to point and understand the directional cue for navigation by actively moving 
the hand. 2 Force Feedback Technique Our approach to creating a sensation of being pulled exploits the 
characteristics of human perception, using different acceleration patterns for the two directions to 
create a perceived force imbalance. A brief and strong force is generated in a desired direction, while 
a weaker one is generated over a longer period of time in the reverse direction. Although the average 
magnitudes of the two forces are the same, reducing the magnitude of the longer and weaker force to below 
a sensory threshold makes the holders feel as if they are being pulled to the desired direction. Changing 
the desired direc­tion by a rotational mechanism allows the holders to feel an omni­directional force 
sensation in the azimuth plane. This approach with rotation mechanism provides precise angular resolution 
of force di­rection. However, it takes a short time for rotating a unit to change the pulling direction 
largely. To reduce its time, we decided to stack two units, which allows the system to switch from one 
unit to the *e-mail: amemiya.tomohiro@lab.ntt.co.jp other when the rotation angle is large. This mechanism 
allows the system to cover the all direction in a shorter time. In addition, the new prototype can create 
not only a translational but also a rotational force (or torque) sensation around the yaw-axis by the 
effect of a momentum wheel with the rotational mechanism. Three DoF planar force feedbacks (thrust forces 
in the xy-plane and rotational force around the yaw) provide a haptic cue suf.cient for pedestrian navigation. 
Furthermore, the position and orientation tracking system allows user to actively sense the updating 
force direction by moving their hand or walking around so as to maximize information gain. These exploratory 
activities maximize the information gain using spatial and temporal information changes. 3 Implementation 
The system consists of a .126-mm force display [Fig. 1(a)], a shoulder bag (containing a battery, an 
ultra-mobile PC, and a con­trol device), a tracking server, and a motion capture system [Fig. 1(b)]. 
The force display comprises two slider-crank units to gen­erate the asymmetric force pattern and the 
rotational mechanism to rotate the slider-crank units and present a torque sensation. Re.ec­tive markers 
are attached on the force display. Their position and orientation are recorded using motion a capture 
system and calcu­lated by a server. Then, the server sends a command to change the force direction. Using 
all possible via points and connection paths prede.ned, our system automatically .nds the shortest path 
includ­ing the via-locations using a dynamic programing search technique. The direction of force is updated 
and presented so as to help users walk along the shortest path. In our system, participants select some 
destinations (nodes) on a tablet computer and then are instructed to walk in the direction they feel 
they are being pulled. References AMEMIYA, T., AND SUGIYAMA, H. 2010. Orienting kinesthet­ically: A 
haptic handheld way.nder for people with visual im­pairments. ACM Trans. Access. Comput. 3, 2, 6:1 6:23. 
AMEMIYA, T., ANDO, H., AND MAEDA, T. 2006. Perceptual attraction force: the sixth force. In SIGGRAPH 
2006 Emerging technologies, ACM Press, 26. Copyright is held by the author / owner(s). SIGGRAPH 2012, 
Los Angeles, California, August 5 9, 2012. ISBN 978-1-4503-1435-0/12/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2342980</article_id>
		<sort_key>840</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>69</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Direct, spatial, and dexterous interaction with see-through 3D desktop]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2342896.2342980</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2342980</url>
		<abstract>
			<par><![CDATA[<p>Despite advances in 3D sensing and display technologies, our interactions with desktop interfaces have remained stagnant from the form that evolved under 2D I/O modalities. HoloBook enables users to manage windows with their hands in a 3D physical space. HoloBook is a term for the entire ensemble of necessary components for realizing this spatial operating environment: visualizations of the desktop rendered on a see-through display, technologies and physical design for emulating collocated 3D input and output space.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3737435</person_id>
				<author_profile_id><![CDATA[81460646887]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Jinha]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lee]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MIT Media Lab and Microsoft Applied Sciences Group]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[jinhalee@media.mit.edu]]></email_address>
			</au>
			<au>
				<person_id>P3737436</person_id>
				<author_profile_id><![CDATA[81452607629]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Cati]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Boulanger]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Microsoft Applied Sciences Group]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[catib@microsoft.com]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1166292</ref_obj_id>
				<ref_obj_pid>1166253</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Andrew D. Wilson. 2006. Robust computer vision-based detection of pinching for one and two-handed gesture input. UIST '06. ACM, New York, NY, USA, 255--258.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[HoloDesk: http://www.youtube.com/watch?v=JHL5tJ9ja_w]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Direct, Spatial, and Dexterous Interaction with See-through 3D Desktop Jinha Lee MIT Media Lab Cati 
Boulanger Microsoft Applied Sciences GroupMicrosoft Applied Sciences Group jinhalee@media.mit.edu catib@microsoft.com 
 seamless transitions between 2D and 3D input. A combination of a transparent display and 3D gesture 
detection algorithmcontributes to 3D interaction technologies collocating 3D input space and emulated 
3D rendering without tethering or encumbering users with wearable devices. By renderingperspective corrected 
3D scenes on a transparent display based on a user s head position, we achieved an illusion that virtual 
windows are floating in the 3D space behind the screen. 1. Abstract Despite advances in 3D sensing and 
display technologies, our interactions with desktop interfaces have remained stagnant from the form that 
evolved under 2D I/O modalities. HoloBook enables users to manage windows with their hands in a 3D physical 
space. HoloBook is a term for the entire ensemble of necessary components for realizing this spatial 
operating environment: visualizations of the desktop rendered on a see-through display, technologies 
and physical design for emulating collocated 3D input and output space. 2. Interaction Holobook looks 
similar to a conventional laptop computer except that the screen is connected to a leading edge of the 
2D input panel unlike in a conventional computer where the screen is connected to the far-edge. Users 
can casually open up the HoloBook and type on the keyboard or use a trackpad as in traditional 2D operating 
environment. Windows or files are perceived to be placed in a 3D space behind the screen. The user can 
lift up his or her hands to reach the displayed windows and move them in this space. Here are examples 
of interactions with HoloBook. Virtual Cabinet: Interaction with stack of Windows Multiple aggregated 
windows are represented as layers stacked in a 3D volume. The windows pop up as a user selects or pull 
one of them. This enables a better access to the desktop content. When the user wants to pull up one 
of the layers from the cabinet , he/she reaches the specific layer with his/her finger. Sliding Door: 
Interaction behind a layer The main active window can vertically slide down as the user try to reach 
the space behind it. This metaphor of sliding door can be applied in many other occasions when there 
is the need to see what is hidden by the top layer. 3. Technologies and Design To enable our ideal scenario 
of interaction, we developedsupporting technology and designed our prototype to emulate a collocated 
3D I/O space without user instrumentation, with We employed a depth-sensing camera to detect the position 
of auser s face. We built our algorithms on top of Microsoft Kinect SDK to estimate the position of the 
user s eye. Another depth­sensing camera sees the 3D position and pose of a user s fingertips. Following 
Wilson s techniques [Wilson. 06], the systemdetects the position of both finger tips by image segmentation 
and estimates pinch gesture through connected components analysis. 4. Conclusion We propose the HoloBook 
demo a novel spatial operatingenvironments for direct dexterous interaction with 3D desktop. The specific 
contributions of our work are as follows. Visualization: windows and Icons represented in a physical 
3D space. Various opacity change/physical motion are used to emulate 3D space.  Unencumbered 3D I/O: 
The technologies enable unencumbered direct interaction with 3D virtual objects, close to touching a 
holographic display with bare hands.  Design of I/O space for seamless mode shift: Holobook s novel 
design collocates the 2D input space and 3D I/O space. It allows users to transition between conventional 
input modalities and 3D I/O.  References ANDREW D. WILSON. 2006. ROBUST COMPUTER VISION-BASED DETECTION 
OF PINCHING FOR ONE AND TWO-HANDED GESTURE INPUT. UIST '06. ACM, NEW YORK, NY, USA, 255-258. HOLODESK: 
HTTP://WWW.YOUTUBE.COM/WATCH?V=JHL5TJ9JA_W Copyright is held by the author / owner(s). SIGGRAPH 2012, 
Los Angeles, California, August 5 9, 2012. ISBN 978-1-4503-1435-0/12/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2342981</article_id>
		<sort_key>850</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>70</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Drum On]]></title>
		<subtitle><![CDATA[interactive personal instrument learning system]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2342896.2342981</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2342981</url>
		<abstract>
			<par><![CDATA[<p>Drum On is a prototype system to enhance personal instrument practice that may conventionally create boredom and limitation. The MIDI signal generating from the electrical drum is handled by a computer and each animating image interacts with this signal. The images displayed on the drum by a projector are animated to directly give rhythm cues for the drum player. The animations subsequently react with proper hit timing.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3737437</person_id>
				<author_profile_id><![CDATA[81504683482]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Jaehyuck]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bae]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Seoul National University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[jaehyuck.bae@gmail.com]]></email_address>
			</au>
			<au>
				<person_id>P3737438</person_id>
				<author_profile_id><![CDATA[81504684130]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Byungjoo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lee]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Seoul National University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[bjlee@snu.ac.kr]]></email_address>
			</au>
			<au>
				<person_id>P3737439</person_id>
				<author_profile_id><![CDATA[81504684716]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Sungmin]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Cho]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Seoul National University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[sungmins@snu.ac.kr]]></email_address>
			</au>
			<au>
				<person_id>P3737440</person_id>
				<author_profile_id><![CDATA[81421594806]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Yunsil]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Heo]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Seoul National University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[yunsil@snu.ac.kr]]></email_address>
			</au>
			<au>
				<person_id>P3737441</person_id>
				<author_profile_id><![CDATA[81421599972]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Hyunwoo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Seoul National University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[savoy@snu.ac.kr]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Drum On : Interactive Personal Instrument Learning System   Jaehyuck Bae*, Byungjoo Lee*, Sungmin 
Cho*, Yunsil Heo** and Hyunwoo Bang* *New Media Lab., School of Mechanical &#38; Aerospace Engineering, 
Seoul National University **Department of Crafts &#38; Design, School of Arts, Seoul National University 
   C:\Users\jaehyuck\Desktop\siggraph2012\..4.JPG C:\Users\jaehyuck\Desktop\siggraph2012\..3.JPG 
    Figure 1.left:before hitting, right:hitting moment    Figure 2.install and performing situation 
 1. Introduction Drum On is a prototype system to enhance personal instrument practice that may conventionally 
create boredom and limitation. The MIDI signal generating from the electrical drum is handled by a computer 
and each animating image interacts with this signal. The images displayed on the drum by a projector 
are animated to directly give rhythm cues for the drum player. The animations subsequently react with 
proper hit timing. 2. Setup To detect exact hit signal, we use an electrical drum. A short throw projector 
is positioned at a proper height to cover all of the drum kits projecting with the object mapping method 
on the drum. The animating effect of playing on the drum kit informs the hit timing to the users and 
the MIDI signal generated by playing is transmitted to the computer. Sequentially, the computer can detect 
correct hit timing from this signal and the player is given feedback through the interacting animation. 
To play smooth animation for eight drum kits, all the image effects are generated by the GPU process. 
3. Interaction The system has three modes of play and practice. Beginner mode: practice basic drum bit 
sequences categorized by three steps (easy, medium, hard). Practice mode: practice with background music. 
Free mode: play the drum freely with interactive images on the drum. With the first two modes for educational 
purposes, the user would become familiar with basic drum beat hitting for the drum kits when the moving 
animation is close to the red circular area at the center point. After finishing one play loop, the player 
can check his performance score. One problem is that the kick drum is located at the bottom where it 
cannot be projected by the projector. As a result, the animation for the kick drum projects on the player 
s knee, which plays the kick drum. The user still gets visual feedback directly from the animating images. 
Practice mode with background music is implemented by BMS (Be Music Source) to generate correct hitting 
timing with background music. It also makes the users feel as if they are performing one piece of music. 
In free mode, the player is able to play drums with various interactive visual effects on the drum for 
entertainment value. 4. Conclusion Drum On provides not only effective practice but also affordable playing 
enjoyment. Generally, most rhythmical instrument practices are based on paper note or independent display. 
This leads to diffused sight and interruption in the direct connection between instrument and note, which 
consequently results in incorrect playing posture as well as boredom. This system provides exact drum 
bit directly while the drummer's eyes are fixed on the drum kit. The player can practice efficiently 
while maintaining the right posture while playing. In addition, the gaming factor and interactive animation 
relieves the tediousness of personal drum practice. 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2342982</article_id>
		<sort_key>860</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>71</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[Image based smartphone interaction with large high resolution displays]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2342896.2342982</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2342982</url>
		<abstract>
			<par><![CDATA[<p>We investigate the practicality of using smartphones to interact with large high resolution displays. We do not intend to find the spatial location of the phone relative to the display, but we want to identify the object a user wants to interact with through image recognition. The interaction with the object itself is done with the smartphone.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3737442</person_id>
				<author_profile_id><![CDATA[81504685636]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Lynn]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nguyen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of California at San Diego, La Jolla, CA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[lkn001@ucsd.edu]]></email_address>
			</au>
			<au>
				<person_id>P3737443</person_id>
				<author_profile_id><![CDATA[81100047808]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[J&#252;rgen]]></first_name>
				<middle_name><![CDATA[P.]]></middle_name>
				<last_name><![CDATA[Schulze]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of California at San Diego, La Jolla, CA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[jschulze@ucsd.edu]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1731503</ref_obj_id>
				<ref_obj_pid>1731477</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[S. Jeon, J. Hwang, G. J. Kim, and M. Billinghurst. Interaction with large ubiquitous displays using camera-equipped mobile phones. Personal Ubiquitous Computing, 14(2):83--94, 2010]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1622205</ref_obj_id>
				<ref_obj_pid>1622176</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[D. C. McCallum, P. Irani. ARC-Pad: Absolute+Relative Cursor Positioning for Large Displays with a Mobile Touchscreen. In UIST'09, pp. 153--156, 2009]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Image Based Smartphone Interaction with Large High Resolution Displays Lynn Nguyen, J¨urgen P. Schulze* 
University of California at San Diego, La Jolla, CA 1 Introduction We investigate the practicality of 
using smartphones to interact with large high resolution displays. We do not intend to .nd the spatial 
location of the phone relative to the display, but we want to identify the object a user wants to interact 
with through image recognition. The interaction with the object itself is done with the smartphone. Large 
high-resolution displays, often implemented by tiling a wall with large LCD displays, are getting increasingly 
popular. While they are great for viewing high resolution data, they can be dif.cult to interact with 
because of their size. Common methods of interac­tion include using a desktop control station with a 
standard mouse, a gyroscopic presentation mouse, or even a 3D tracking system. Unfortunately, these methods 
are not without their shortcomings. Using a standard or gyromouse is bothersome because the pixel resolution 
of the mouse is lower than that of the display wall. And because the cursor position of the mouse is 
independent of the lo­cation of the user it is easy to lose track of the cursor. The tracking system 
is able to address these issues but can be quite expensive. Another issue with many of the current navigation 
methods is that they are designed for a single user. Many previously developed smartphone-wall interaction 
methods assume the user wants a precise location on the display [McCal­ lum09; Jeon10]. However, this 
level of granularity is not required of all applications. Often, the data on the display consists of 
mul­tiple objects. We can think of interaction with the display in two parts: at a coarse grain level 
where the user needs to determine the object to interact with, and at .ne grain level where the interaction 
is localized to the object, and executed on the smartphone. Our prototype implements a scenario in which 
a set of images is being viewed on a display wall, and users can annotate the images with their phones. 
The coarse grain interaction is determining which im­age to focus on while the .ne grain interaction 
is the note taking on the selected image. 2 System Description Our prototype uses an HTC Aria Android 
phone to select one of multiple photographs on Calit2 s AESOP wall, a 4 by 4 array of 46 monitors. We 
use a computer vision algorithm to match what the phone sees to one of the displayed photographs to determine 
which image the user selected. The system uses a client-server model communicating via TCP/IP. The server 
is out.tted with a database to store user comments on images. The smartphone continuously sends 768 × 
432 pixel image frames (query images) of what its camera sees at a rate of two frames per second to the 
server. The phone does not do feature extraction since that is more ef.cient on the server side. The 
only processing the client does is converting the query image to JPEG and scaling it to a size the server 
requests. The server receives query images from the client and extracts features with the SURF algorithm 
from the OpenCV library. Once these features are extracted, a matching im­age is determined using a nearest 
neighbor algorithm provided by the FLANN library. When running the client application on the phone, the 
user initially *e-mail: lkn001@ucsd.edu, jschulze@ucsd.edu Figure 1: User selecting an image on a large 
display with a phone. sees the image from the camera, superimposed with some of the EXIF metadata of 
the selected photograph. Additionally, the image on the wall will be highlighted, to give the user veri.cation 
that the correct image has been recognized, see Figure 1. In our prototype images turn from grayscale 
to color when highlighted (but the im­age recognition works just as well with color images). On a .nger 
tap, a menu will appear at the bottom of the screen, which offers further information about the image: 
view a description of the im­age, see on a map the location where the image was taken, as well as view 
or make comments on the image selected. We extracted the features from the photographs using the SURF 
al­gorithm, whose rotation-and scale-invariance proved to be crucial for this application in which the 
phone is hand held at odd angles. In order to obtain the actual matched image, we compare the query image 
against every displayed photograph. The comparison is a nearest neighbor algorithm provided by FLANN. 
For each photo­graph, a match percentage is determined by the number of matched descriptors in the query 
image over the total number of descriptors in the query image. The highest match percentage obtained 
deter­mines the selected photograph. In order to reduce the probability of false matches, this percentage 
must be higher than an empirically determined threshold. There are many bene.ts to using a smartphone 
as an interaction de­vice for a large display. The main advantage of our system over previous work is 
that it does not rely on additional infrastructure such as a tracking system or visual markers to operate. 
The many features of the phone allow multiple users to select objects on the display in an intuitive 
manner while offering many more functions than just selecting. One thing to note is that in our current 
imple­mentation, duplicate or very similar images that the user wants to treat as distinct entities will 
not be treated correctly by the system. A possible solution would be to use neighboring image information, 
or motion of the device. References S. Jeon, J. Hwang, G. J. Kim, and M. Billinghurst. Interaction with 
large ubiquitous displays using camera-equipped mobile phones. Personal Ubiquitous Computing, 14(2):83-94, 
2010 D. C. McCallum, P. Irani. ARC-Pad: Absolute+Relative Cursor Positioning for Large Displays with 
a Mobile Touchscreen. In UIST 09, pp. 153-156, 2009 Copyright is held by the author / owner(s). SIGGRAPH 
2012, Los Angeles, California, August 5 9, 2012. ISBN 978-1-4503-1435-0/12/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2342983</article_id>
		<sort_key>870</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>72</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>5</seq_no>
		<title><![CDATA[mimicat]]></title>
		<subtitle><![CDATA[face input interface supporting animatronics costume performer's facial expression]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2342896.2342983</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2342983</url>
		<abstract>
			<par><![CDATA[<p>Today a character costume can be seen in many places, such as amusement facilities, sport stadium and so on. They perform comical and funny body action for us. In general, the performers can't control their costume's facial expression. We developed "mimicat" that can synchronizing performer's facial action and costume's one. A character costume performer can do more comical action by using mimicat. At first our motivation is combining animatronics and face and expression recognition.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3737444</person_id>
				<author_profile_id><![CDATA[81504688027]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Rika]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Shoji]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tokyo Metropolitan University, Hino, Tokyo, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[sh233221@gmail.com]]></email_address>
			</au>
			<au>
				<person_id>P3737445</person_id>
				<author_profile_id><![CDATA[81504687300]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Toshiki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yoshiike]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tokyo Metropolitan University, Hino, Tokyo, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737446</person_id>
				<author_profile_id><![CDATA[81488673349]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Yuya]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kikukawa]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tokyo Metropolitan University, Hino, Tokyo, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737447</person_id>
				<author_profile_id><![CDATA[81504682922]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Tadahiro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nishikawa]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tokyo Metropolitan University, Hino, Tokyo, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737448</person_id>
				<author_profile_id><![CDATA[81504687915]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Taigetsu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Saori]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tokyo Metropolitan University, Hino, Tokyo, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737449</person_id>
				<author_profile_id><![CDATA[81504687427]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Suketomo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ayaka]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tokyo Metropolitan University, Hino, Tokyo, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737450</person_id>
				<author_profile_id><![CDATA[81319488008]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>7</seq_no>
				<first_name><![CDATA[Tetsuaki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Baba]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tokyo Metropolitan University, Hino, Tokyo, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737451</person_id>
				<author_profile_id><![CDATA[81504682817]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>8</seq_no>
				<first_name><![CDATA[Kumiko]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kushiyama]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tokyo Metropolitan University, Hino, Tokyo, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Paul Ekman, E. L 1987. What Face Reveaks: Basic and Applied Studies of Spontaneous Expression Using the Facial Action Coding System (FACS), pp. 413--425]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1938021</ref_obj_id>
				<ref_obj_pid>1937966</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[J. Saragin, S. Lucey and J. Cohn, "Deformable Model Fitting by Regularized Landmark Mean-Shifts", International Journal of Computer Vision (IJCV), 2010]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 mimicat : Face input interface supportinganimatronics costume performer`s facial expression Rika Shoji, 
Toshiki Yoshiike, Yuya Kikukawa, Tadahiro Nishikawa, Taigetsu Saori, Suketomo Ayaka Tetsuaki Baba, Kumiko 
Kushiyama Graduate School of System Design, Tokyo Metropolitan University Hino, Tokyo, JAPAN sh233221@gmail.com 
 1. Introduction Today a character costume can be seen in many places, such as amusement facilities, 
sport stadium and so on. They performcomical and funny body action for us. In general, the performerscan 
t control their costume s facial expression. We developed mimicat that can synchronizing performer`s 
facial action and costume`s one. A character costume performer can do more comical action by using mimicat. 
At first our motivation is combining animatronics and face and expression recognition. Facial Action 
Coding System (FACS) by Ekman [1] is well known in the field. His process requires a complex three­dimensional 
measurement of the facial muscles to measure behavior. We tried to detect user s facial action in a dark 
placelike the inside of the costume. At the same time we intended contactless sensing. In this research, 
we compared and inspectedtwo methods to detect facial actions. First method is multi pointedphoto-interrupters 
on mask. Second method is using an infrared camera.  2. Recognition of Facial action Firstly, we tried 
to use multi pointed photo-interrupters to captureuser s facial action. nine photo-interrupters (RPR-220) 
on themask made of ABS resin (Acrylonitrile-Butadiene-Styrene resin)(Fig1) detect each facial part action 
on eyes, eyebrows, a cheek and a mouth without skin contact. The value is obtained by serial communication 
from photo-interrupter via I/O board (Arduino)and was set the threshold to detect facial action by calibrating. 
Further, we measured the distance between mask and face toadjust sensitivity and consider placement of 
photo-interrupter. Sixvoluntary subjects (3 female, 3 male) who were students of graduate school of System 
Design, Tokyo Metropolitan University participated in this measuring. We determined that it is desirable 
to detect by 20mm or less from result that the minimum5.12mm to the maximum of 22.15mm. Thus, we designed 
theelectronic circuit so that the highest sensitivity of photo­interrupter when the distance between 
them and detected objectwas 0mm~20mm. In this method responses are quick and systemcost is low. On the 
other hand wearing our mask seemed to be  burden for users. Figure 1. Left : Multi pointed photo-interrupters 
on the mask. Right : Image from an infrared camera in mimicat. Secondly, we tried to use an infrared 
camera to capture user sfacial action. We used fitting deformable model [2] (Fig1) developed by J. Saragin 
et al. for face tracking. The infrared Copyright is held by the author / owner(s). SIGGRAPH 2012, Los 
Angeles, California, August 5 9, 2012. ISBN 978-1-4503-1435-0/12/0008  camera enabled this system to 
detect facial expression even in a dark place. In this system users do not have to put on the mask forsensing. 
Comparatively speaking, this system is easier to use than older one and able to detect comprehensive 
facial expression. 3. Animatronics costume We made original electro magnetic actuators and installed 
to inthe device. These actuators moves the costume s eyes and a mouse. When a user opens or closes his 
eyes or mouth, thecostume s facial parts move similarly(Fig2). These actuators can move quietly and quickly 
using two opposite directions electromagnetic force. Furthermore, short-range radio modules(XBee-S1) 
is mounted in this device. Using this actuators can be controlled wirelessly. Figure 2. System of mimicat 
 Figure 3. The playing of mimicat  4. Future Works In this paper we developed a system that support 
character costume performers by combining animatronics and face andexpression recognition. Using an infrared 
camera and imageprocessing we could detect user s facial expression in a costume.On the other hand our 
actuators are relatively too few and simple.So for the future work we will install other actuators and 
try toexpress more minute facial expression, such as an air actuator toexpress the puff of user s cheek. 
 References [1]Paul Ekman, E.L 1987. What Face Reveaks : Basic and Applied Studies of Spontaneous Expression 
Using the FacialAction Coding System (FACS), pp. 413-425 [2] J. Saragin, S. Lucey and J. Cohn, Deformable 
Model Fittingby Regularized Landmark Mean-Shifts , International Journalof Computer Vision (IJCV), 2010 
 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2342984</article_id>
		<sort_key>880</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>73</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>6</seq_no>
		<title><![CDATA[NanoAR]]></title>
		<subtitle><![CDATA[mobile AR application with microscopic interaction]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2342896.2342984</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2342984</url>
		<abstract>
			<par><![CDATA[<p>We propose a novel mobile AR (Augmented Reality) application that uses a microscopic interaction prototype called NanoAR. A user can view the AR content using a USB or an iPhone microscope via NanoAR. The objects that the user can view using this application are related to the magnification rate; in other words, the application resembles a microscope in the real world. This indicates that a user can view two different objects on the same spot of the paper.</p> <p>In our previous project <i>MicroAR</i>, we used the metaphor of familiar actions in order to view small objects in the real world, such as the use of a magnifying glass. This makes the AR tool more natural, leading users to concentrate on the AR world more deeply and easily. NanoAR is the developed prototype of this concept.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3737452</person_id>
				<author_profile_id><![CDATA[81492651742]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Shintaro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kitazawa]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[IDMI, National University of Singapore National University of Singapore, Heng Mui Keng Terrace, Singapore]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[woof.design675@gmail.com]]></email_address>
			</au>
			<au>
				<person_id>P3737453</person_id>
				<author_profile_id><![CDATA[81365592127]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Koh]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sueda]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[IDMI, National University of Singapore National University of Singapore, Heng Mui Keng Terrace, Singapore]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[apochang.jp@gmail.com]]></email_address>
			</au>
			<au>
				<person_id>P3737454</person_id>
				<author_profile_id><![CDATA[81100369572]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Henry]]></first_name>
				<middle_name><![CDATA[Been-Lirn]]></middle_name>
				<last_name><![CDATA[Duh]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[IDMI, National University of Singapore National University of Singapore, Heng Mui Keng Terrace, Singapore]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[duhbl@acm.org]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[E. Cobb, "The ecology of imagination in childhood," Daedalus, vol.88, pp. 537--548, 1959.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[E. H. Gombrich, Meditations On a Hobby Horse and Other Essays On the Theory of Art: Phaidon Press, 1994]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[J. Piaget, Play, dreams, and imitation in childhood. London: W. Heinemann, 1951.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2073381</ref_obj_id>
				<ref_obj_pid>2073370</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Koh Sueda, Jian Gu, Shintaro Kitazawa, and Henry Been-Lirn Duh (2011). "Micro AR for education: using metaphors for familiar actions," in proceeding of SIGGRAPH ASIA 2011 Emerging Technologies. 978-1-4503-1136-6]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2196323</ref_obj_id>
				<ref_obj_pid>2195921</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[H. Uchiyama, H. Saito, (2011). "Random Dot Markers," IEEE Virtual Reality Conference. ISSN1087-8270]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 NanoAR: Mobile AR Application with Microscopic Interaction Shintaro Kitazawa Koh Sueda Henry Been-Lirn 
Duhwoof.design675@gmail.com apochang.jp@gmail.com duhbl@acm.org Mime lab, IDMI, National University of 
SingaporeNational University of Singapore, I-Cube building Level 2, 21 Heng Mui Keng Terrace, Singapore 
119613 1. Abstract We propose a novel mobile AR (Augmented Reality) application that uses a microscopic 
interaction prototype called NanoAR. A user canview the AR content using a USB or an iPhone microscope 
viaNanoAR. The objects that the user can view using this application arerelated to the magnification 
rate; in other words, the applicationresembles a microscope in the real world. This indicates that a 
user can view two different objects on the same spot of the paper.In our previous project MicroAR, we 
used the metaphor of familiaractions in order to view small objects in the real world, such as the useof 
a magnifying glass. This makes the AR tool more natural, leadingusers to concentrate on the AR world 
more deeply and easily. NanoARis the developed prototype of this concept.  Figure1.Conceptualmodeloflayered 
 structuralrandomdotmarkerandthecontents 2. Description of Our Work Interaction:We propose a novel mobile 
AR (Augmented Reality)application from the viewpoint of interactivity. The concept of NanoAR is similar 
to that of MicroAR[4]. In MicroAR, we applied the metaphorof familiar actions to the AR application. 
Originally, AR technologyallowed us to view objects or information that we were unable to view with the 
naked eye or without imagination; however,conventional AR applications focus only on visualizations of 
theobjects or information. By applying the meataphor of familiar actions in MicroAR, We added extra value 
to AR application, which can ignite the imagination of users. As human beings, wehave the ability to 
imagine invisible things and liken one object toanother (e.g., children use a stick as a horse while 
playing)[2].Since childhood, we are able to enjoy more immersive and creative play of this kind by using 
our imaging ability. IOurimaging ability plays an important role in the concept of our ARapplication. 
A user can liken NanoAR to a real microscope.NanoAR further ignites a user s imagination by changable 
ARcontent, and it can be used to perform tangible actions by varyingthe magnification rate. Figure 2. 
The scene showing the use of NanoAR Hardware: NanoAR can be implemented by using certain hardwaresuch 
as a USB or an iPhone microscope, paper media, and a random dotmarker[4]. In MicroAR, a traditional black/white 
marker can be viewedby the naked eye. However, some users were of the opinion that it wasvery easy to 
detect the content. As a solution to this problem, we implemented Random Dot Markers[5]. The implementation 
of a random dot marker provided two advantages to our application. (1) Thedots made by using the marker 
can be nearly invisible to the naked eye. (2) The dots can be layered (see Fig1). Owing to advantage 
(1), thesystem can be similar to a natural feature detection AR system. Moreover, it can function in 
low performance devices such as asmartphone and a tablet. Advantage (2) enables microscopic interactionusing 
the NanoAR. We performed some experiments for improving thedetection of extremely small dots made using 
the random dot marker. From the results, we found that the position of the dot should correspond to the 
nozzles of the printer. Otherwise, the shape of the dotscollapses and results in blurring (see Fig3). 
We designed an experimental grid that is divided into 300 dpi in order to determine theposition of the 
dots. After the experiment, the square area of the random dot markers was smaller than 3 mm × 3 mm. As 
a result, most users did not notice the position of the markers. NanoAR will surprise users andencourage 
their desire of finding something in the paper media. Figure 3. The grid and its result 3. Future Work 
Currently, our program can display only 2D content. As future work on this technical topic, we intend 
to build a new program that is enabled todisplay 3DCG, which would further impress the users. We also 
intendto make practical application of NanoAR. NanoAR can be utilized fordiscovery learning, e-learning, 
and mobile learning. Reference [1] E. Cobb, The ecology of imagination in childhood, Daedalus, vol.88, 
pp. 537-548, 1959. [2] E. H. Gombrich, Meditations On a Hobby Horse and OtherEssays On the Theory of 
Art: Phaidon Press, 1994 [3] J. Piaget, Play, dreams, and imitation in childhood. London: W.Heinemann, 
1951. [4] Koh Sueda, Jian Gu, Shintaro Kitazawa, and Henry Been-LirnDuh (2011). Micro AR for education: 
using metaphors forfamiliar actions, in proceeding of SIGGRAPH ASIA 2011Emerging Technologies. 978-1-4503-1136-6 
[5] H. Uchiyama, H. Saito, (2011). Random Dot Markers, IEEEVirtual Reality Conference. ISSN1087-8270 
Copyright is held by the author / owner(s). SIGGRAPH 2012, Los Angeles, California, August 5 9, 2012. 
ISBN 978-1-4503-1435-0/12/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2342985</article_id>
		<sort_key>890</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>74</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>7</seq_no>
		<title><![CDATA[NeonDough]]></title>
		<subtitle><![CDATA[crafting with interactive lighted clay]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2342896.2342985</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2342985</url>
		<abstract>
			<par><![CDATA[<p>From childhood, we often play with clay to learn and enjoy formative design. In clay crafting, we believe that color arrangement on the clay is one of the important elements as well as forming shapes. However, since the colors of normal clay are static and it is not easy to change them in contrast to its flexible form. Toward a resolution of this issue, we propose a novel clay interface named NeonDough which is clay crafting with glowing clay. So far, there have been various studies aiming to support creative activities of children using educational toys that contain electronic circuits (e.g. [Raffle et al. 2004]). As one of typical interfaces which focused on clay material, Illuminating Clay [Piper et al. 2002] uses a laser scanner to detect the shape of the clay and projects images onto it according to the input. In this approach, sensors and display devices need to be located at the exterior of the clay. On the other hand, Squishy Circuits [Johnson and Thomas 2010] is a related work of our research that utilizes the electric character of conductive dough. While Squishy Circuits mainly utilized the conductive dough as alternated wires in electronic circuits, our NeonDough works as both input and output tools themselves.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3737455</person_id>
				<author_profile_id><![CDATA[81442615273]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Junichi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yamaoka]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[yamajun@sfc.keio.ac.jp]]></email_address>
			</au>
			<au>
				<person_id>P3737456</person_id>
				<author_profile_id><![CDATA[81100527572]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Yasuaki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kakehi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[ykakehi@sfc.keio.ac.jp]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1754109</ref_obj_id>
				<ref_obj_pid>1753846</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Johnson, S., and Thomas, A. P. 2010. Squishy circuits: A tangible medium for electronics education. In <i>CHI EA '10 Proceedings of the 28th of the international conference extended abstracts on Human factors in computing systems</i>, ACM.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>503439</ref_obj_id>
				<ref_obj_pid>503376</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Piper, B., Ratti, C., and Ishii, H. 2002. Illuminating clay: a 3-d tangible interface for landscape analysis. In <i>CHI '02 Proceedings of the SIGCHI conference on Human factors in computing systems: Changing our world, changing ourselves</i>, ACM.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>985774</ref_obj_id>
				<ref_obj_pid>985692</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Raffle, H. S., Parkes, A. J., and Ishii, H. 2004. Topobo: a constructive assembly system with kinetic memory. In <i>CHI '04 Proceedings of the SIGCHI conference on Human factors in computing systems</i>, ACM.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 NeonDough: Crafting with Interactive Lighted Clay JunichiYamaoka* Yasuaki Kakehi* Keio University Keio 
University  Figure 1: NeonDough Figure 2: System Design Figure 3: Examples of Crafts 1 Introduction 
From childhood, we often play with clay to learn and enjoyforma­tive design. In clay crafting, we believe 
that color arrangement on the clay is one of the important elements as well as forming shapes. However,since 
the colors of normal clay are static and it is not easy tochangethemin contrasttoits.exibleform.Towarda 
resolution of this issue, we propose a novel clay interface named NeonDough which is clay crafting with 
glowing clay. Sofar, there have been various studies aiming to support creative activities of children 
us­ing educational toys that contain electronic circuits (e.g. [Raf.e et al. 2004]). As one of typical 
interfaces which focused on clay material, Illuminating Clay [Piper et al. 2002] uses a laser scan­ner 
to detect the shape of the clay and projects images onto it ac­cording to the input. In this approach, 
sensors and display devices need to be located at the exterior of the clay. On the other hand, SquishyCircuits 
[Johnson and Thomas 2010] is a related work of our research that utilizes the electric character of conductivedough. 
While SquishyCircuits mainly utilized the conductive dough as al­ternated wires in electronic circuits, 
our NeonDough works as both input and output tools themselves.  2 NeonDough NeonDough is conductive 
clay which contains a module that con­sists of electronic circuits and a full-color LED. As the clay 
mate­rial, we use .our clay that is made of salt and water and has high electro conductivity. In addition, 
the .our clay is also suitable for this interface since it diffuses the light of LEDs effectively. Each 
module is connected to a hub device in the shape of a clay board that can measure resistance values between 
every two modules. By sensing the resistance values, this system can detect and esti­mate various states 
of clay with these modules. Firstly, as shown in Figure 2, when the user combines two pieces of clay, 
the system can detect the connection since an electric current initiates between these modules electrodes. 
Secondly, according to the change of resistance, this system can estimate the distance between modules 
when they are pushed together. When the user stretches apart the clay, the resistance value gets higher 
gradually in proportion to the distance and thinness of the clay between these modules. The LEDs installed 
in each module can change their brightness and *e-mail: {yamajun, ykakehi}@sfc.keio.ac.jp colors dynamically 
and interactively according to these states infor­mation of the clay. In addition, note that cables are 
removable after the clay is formed since batteries are included in the modules. 3 Applications and Future 
Works As a typical application of NeonDough, we have developed an ap­plication that enables users program 
the light s glowing pattern and colors with simple ways such as combining different colored glow­ing 
pieces of clay or stretching them. More concretely, for instance, by pressing pieces of red and green 
clay together, it becomes an or­ange color; while stretching pieces apart changes the color gradu­ally, 
eventually returning to their original color by separation. Fur­thermore, for supporting practical creations, 
we implementedalock function which maintains the clay s blended color after separation, and a blinking 
function which adds a blinking pattern to the LEDs. Crafts madeby participantsofworkshops using NeonDough. 
As another approach, we developed several interactive applications by attaching various output devices 
to the module in addition to the LEDs. For example, by putting vibration speakers, users can play music 
with connecting, merging, stretching apart pieces of clay. By attaching a servo motor in the modules, 
users can partially move clay characters such asa hand or footin the sameway.We believe that these diverse 
outputs and inputs of NeonDough make much more novel representations achievable in the future. References 
JOHNSON, S., AND THOMAS, A. P. 2010. Squishy circuits: A tangible medium for electronics education. In 
CHI EA 10 Pro­ceedings of the 28th of the international conference extended ab­stracts on Human factors 
in computing systems,ACM. PIPER,B.,RATTI,C., AND ISHII,H. 2002. Illuminating clay:a3­dtangible interface 
for landscape analysis. InCHI 02 Proceed­ings of the SIGCHI conference on Human factors in computing 
systems: Changing our world, changing ourselves,ACM. RAFFLE,H.S.,PARKES,A.J., AND ISHII,H. 2004.Topobo: 
a constructive assembly system with kinetic memory. In CHI 04 Proceedings of the SIGCHI conference on 
Human factors in computing systems,ACM. Copyright is held by the author / owner(s). SIGGRAPH 2012, Los 
Angeles, California, August 5 9, 2012. ISBN 978-1-4503-1435-0/12/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2342986</article_id>
		<sort_key>900</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>75</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>8</seq_no>
		<title><![CDATA[Neu]]></title>
		<subtitle><![CDATA[how brain activity can change an animated scene]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2342896.2342986</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2342986</url>
		<abstract>
			<par><![CDATA[<p>In the last years the advances in animated cinema and videogames have suggested the possibility of creating virtual worlds with an high quality real-time rendering. Thus it is reasonable to think that all the properties in a scene could be modified in real-time.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[animation]]></kw>
			<kw><![CDATA[brain-computer interfaces]]></kw>
			<kw><![CDATA[games]]></kw>
			<kw><![CDATA[interaction]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>H.5.2</cat_node>
				<descriptor>User-centered design</descriptor>
				<type>S</type>
			</primary_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003120.10003123.10010860.10010859</concept_id>
				<concept_desc>CCS->Human-centered computing->Interaction design->Interaction design process and methods->User centered design</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003123.10010860.10010859</concept_id>
				<concept_desc>CCS->Human-centered computing->Interaction design->Interaction design process and methods->User centered design</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P3737457</person_id>
				<author_profile_id><![CDATA[81492654211]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Marco]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Marchesi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Bologna, Italy]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[marchesi@neu-project.com]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1477102</ref_obj_id>
				<ref_obj_pid>1477046</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[L&#233;cuyer, A., Lotte, F., Reilly, R. B., Leeb, R., Hirose, M., and Slater, M. 2008. Brain-computer interfaces, virtual reality, and videogames. <i>Computer 41</i> (October), 66--72.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2073385</ref_obj_id>
				<ref_obj_pid>2073370</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Marchesi, M., Farella, E., Ricc&#242;, B., and Guidazzoli, A. 2011. Mobie: a movie brain interactive editor. In <i>SIGGRAPH Asia 2011 Emerging Technologies</i>, ACM, New York, NY, USA, SA '11, 16:1--16:1.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Neu: how brain activity can change an animated scene Marco Marchesi* University of Bologna, Italy CR 
Categories: H.5.2 [User Interfaces]: Input devices and strategies User-centered design; Keywords: Brain-Computer 
Interfaces, interaction, animation, games 1 Introduction In the last years the advances in animated cinema 
and videogames have suggested the possibility of creating virtual worlds with an high quality real-time 
rendering. Thus it is reasonable to think that all the properties in a scene could be modi.ed in real-time. 
In such a scenery a new generation of controllers have been intro­duced for creating more engaging interactive 
experiences. Brain-Computer Interfaces seemtobeavalid solutiontobe applied in the entertainment and educational 
.elds, not only for people af­.ictedby de.citsin attention or motor control,buteven for healthy people[L´ 
ecuyer et al. 2008]. In this paper we introduce Neu, a tool that acquires brain signals from a commercial 
BCI and uses them to in.uence properties of a game or, more generally, an interactive real-time animated 
scene. Neu is the evolution of Mobie, an editor/player for changing nar­ration in interactive movies 
using the signals acquired by a single EEG channel BCI[Marchesi et al. 2011]. Speci.cally, attention 
and relaxation levels was useful for creating avocabulary of8brain states that can be connected to anypart 
of the movie and reveal the amount and the variation slope of the two measures.We willkeep thisvocabularyinNeu, 
as the simplest set we canwork with.Anew complexvocabulary willbe designed for applications with EEG 
multichannel BCIs. 2 Neu approach If we consider a story and a 3d world where characters act to mo­dify 
it, in mathematical terms the attention (or the relaxation) level ofa user canbe seenasa .eld, ableto 
transforma virtual space,the objects, the characters and the properties included in it. Thus anyshader, 
light source, the rendering quality as well may de­pend on attention and relaxation levels. Complex properties 
like character s emotions are parameterizable too. For example, we could imagine a story where the user 
meets characters that react differently to his brain states. In a First-Person Shooter game the enemies 
could become smarter and more aggressive if the player s attention goes down, in order to involve him 
in thegame. In this sense we can say there is a brain in.uence in it. Simplifying, anyproperty p of anyobject 
is a temporal function of the coordinates and the brain states of the user, -D- D p(t)= g(X, D,A,M; t) 
-D- D with X the user s position, D the view direction, A and M the attention and meditation states calculated 
by the levels (scalar values)1. *e-mail: marchesi@neu-project.com 1Actually g should be a recursive function. 
Considering the example above,agame character should learn from his past behavior (and mistakes) and 
change it consequently. With Neu the concept of controller is reconsidered. BCIs in games are still viewed 
mainly as substitutes of classic directional controllers and the user is usually encouraged to imagine 
the movements to do or improve the attention level to go forward or backward. Unfortunately this traditional 
approach works slowly and it needs a lot of training to reach an acceptable control. Part of research 
of Neu istoexplorewhatarethe propertiesthatitwouldbe interesting to modify,keepinganhigh interactive 
userexperience. Neu worksin3steps:a) creationand populationofthe scene with objects and characters rendered 
with a 3d engine; b) setup of the properties that the brain activity can in.uence through a BCI; c) acquisition 
of the brain signals and playing. Figure 1: In Neu any user can be sketched as a box, oriented to the 
z-axis, that moves within a 3D environment and focuses the atten­tion to a speci.c direction, most affecting 
the behavior of objects that are being observed or the ambient sounds, forexample. Relax­ation states 
can in.uence mainly global variables of the environ­ment, like light emitters or soundtracks. 3 Conclusions 
Ashort overview of Neu has been made. As a standalone tool, it provides a quick graphical solution for 
editing virtual reality envi­ronments where the objects can be in.uenced by the brain activity. Future 
efforts will be focused on transforming Neu in a API, to add functionality to other softwares that creategames 
or movies. Other multichannel BCIs will be considered, and we will investi­gate howthe additional information 
acquired from the brain activity could improve anynarrative multimedia experience. References ´M.,AND 
SLATER,M. 2008. Brain-computer interfaces, virtual reality, and videogames. Computer 41 (October), 66 
72. LECUYER, A., LOTTE, F., REILLY, R. B., LEEB, R., HIROSE, MARCHESI, M., FARELLA, E., RICCO`, B., AND 
GUIDAZZOLI, A. 2011. Mobie: amovie brain interactiveeditor. InSIGGRAPH Asia 2011 EmergingTechnologies,ACM,NewYork,NY, 
USA, SA 11, 16:1 16:1. Copyright is held by the author / owner(s). SIGGRAPH 2012, Los Angeles, California, 
August 5 9, 2012. ISBN 978-1-4503-1435-0/12/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2342987</article_id>
		<sort_key>910</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>76</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>9</seq_no>
		<title><![CDATA[New interactive visualisation of multiscale biomedical data]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2342896.2342987</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2342987</url>
		<abstract>
			<par><![CDATA[<p>The consideration of systemic processes is becoming a common trend in the biomedical domain. However, exemplary problems analysis and review of the state of the art make evident that there is a shortage of appropriate tools for exploring data defined across a broad range of spatial and/or temporal scales. Multiscale visualization is not a new issue, and it has been investigated in other scientific contexts of which the most relevant effort has been undertaken in geographical data visualization, like in Google Earth [1]. While these approaches are extremely effective within their context, not all of solutions can be generalized to other domains. This is particularly true for the biomedical area, where datasets are usually of higher dimension and contain a greater variety of field data and data types. Based on these considerations, the development of a new open-source software library, called MSVTK, has started. The library aims at providing software components general enough to be potentially used in any biomedical software project with multiscale visualization issues.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3737458</person_id>
				<author_profile_id><![CDATA[81100301101]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Debora]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Testi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[BioComputing Competence Centre, SCS srl, Italy]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737459</person_id>
				<author_profile_id><![CDATA[81504688708]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Daniele]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Giunchi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[BioComputing Competence Centre, SCS srl, Italy]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737460</person_id>
				<author_profile_id><![CDATA[81100545987]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Gordon]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Clapworthy]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Bedfordshire, UK]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737461</person_id>
				<author_profile_id><![CDATA[81504683628]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Stephen]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Aylward]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Kitware Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737462</person_id>
				<author_profile_id><![CDATA[81436601600]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Xavier]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Planes]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Universitat Pompeu Fabra, Spain]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737463</person_id>
				<author_profile_id><![CDATA[81504685901]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Richard]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Christie]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Auckland, New Zealand]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[http://www.google.com/earth/index.html;]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1982916</ref_obj_id>
				<ref_obj_pid>1982696</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[M Viceconti et al, Multimodal fusion of biomedical data at different temporal and dimensional scales, <i>Comp. Mtds and Progs Biomed</i>, 102(3):227--237, 2010;]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[http://www.vtk.org;]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[http://www.vtk.Org/doc/release/5.0/html/a01209.html;]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[http://www.openmaf.org;]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[http://www.gimias.org]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 NEWINTERACTIVEVISUALISATIONOF  MULTISCALEBIOMEDICALDATA DeboraTesti1DanieleGiunchi1GordonClapworthy2StephenAylward4XavierPlanes3RichardChristie5 
1BioComputingCompetenceCentre,SCsrl,Italy,2UniversityofBedfordshire,UK,3UniversitatPompeu  Fabra,Spain,4Kitware 
 Inc,USA,5UniversityofAuckland,NewZealand  Example of multiscale bone data interactive visualisation 
proposed by the MSVTK approach: on the left, femur CT scan represented as iso-surface, in the middle, 
microCT scan of trabecular bone in an orthoslice view, on the right nano CT of a trabecular into a slice 
view. The user can move from one scale to the other by clicking on the visual cues, which show the presence 
of lower scale data.  INTRODUCTION Theconsiderationofsystemicprocessesisbecomingacommontrendinthebiomedicaldomain.However,exemplaryproblemsanalysisandreviewofthe 
 state  ofthe  artmake  evidentthatthereisashortage  ofappropriate  toolsforexploring  data  defined 
 acrossabroad  rangeofspatialand/ortemporalscales.Multiscalevisualizationisnotanewissue,andithasbeeninvestigatedinotherscientificcontextsofwhichthemostrelevant 
 effort  has  beenundertakeningeographicaldatavisualization,likeinGoogleEarth[1].Whiletheseapproaches 
 are  extremelyeffective  withintheircontext,notallofsolutionscanbegeneralizedtootherdomains.This  is 
 particularly  trueforthebiomedicalarea,  wheredatasetsareusuallyofhigherdimension  and  contain  agreatervarietyoffield 
 dataand  datatypes.Based  ontheseconsiderations,thedevelopmentofanewopen-­-sourcesoftwarelibrary,calledMSVTK,has 
 started.Thelibraryaimsatprovidingsoftwarecomponentsgeneralenoughtobe  potentiallyusedinanybiomedicalsoftware 
 projectwith  multiscalevisualization  issues.  OURAPPROACH Analysisoftheavailabletoolsshowed  thatthezoom-­-based 
 approach,previouslyinvestigatedduring  the  LHDL  EC-­-fundedproject[2]hasproven  to  beveryeffectiveasin 
 manyotherdomains.With  thisapproach  visualcuesareprovided  forthepositionsoflowerscaledatawithrespecttothewholescene,leadingtoanintuitiveinterfacefordatanavigation. 
 TheMSVTKthusstartsfromthisinteractionparadigm,but  usesplaceholdersnotonlyfortherepresentation  oflower-­-scaledata,butalso 
 forhyperlinkstoprovideextrainformationsuchasdocumentation,etc.Atthe  same  time,MSVTK  givesthe  possibilityofconveyingmeaningfulinformation 
 abouttherepresenteddatathroughtheicon/placeholder  shapesandcolors,withtheaimtooptimizetheuserexperience.TheMSVTK 
 libraryisbeingimplementedasanextensionofVTK  (VisualizationToolKit)  [3]anddesignedtobeasgeneralaspossible.Forwhatconcernstheclick-­-and-­-zoominteractionparadigm,MSVTKreliesand 
 extendsvtkButtonswhichareusedtoprovidethevisual  cuetotheuserforthedatanavigation[4].  Also  extension 
 to  basicVTKfunctionalitiesto  betterdealwith  thetime-­-varying  dataand/orinformationis  being  added. 
 PROTOTYPES TheMSVTK  componentsarebeingusedinthedevelopmentofprototypes,whichallowchecking,onthecollectedexemplaryproblems,theefficacyoftheproposed 
 approach.In  orderto  verify  its  generality,differentprototypes  arebeing  developedintegratingtheMSVTKlibraryinotherframeworkslikeMAF[5],GIMIAS[6],andVTK 
 itself.TheprototypesaredealingwithdifferentaspectsofmultiscaledataindependentlyforclaritywhileMSVTKwillallow 
 toaddressallofthemtogether.AfirstdemonstratorteststheMSVTKvtkButtonswhendealingwithspatialmultiscaledata 
 frombone  imaging.The  userisallowedtonavigate  3Dimagesdatasets(CT  scanatorganlevel,microCTscan,and 
 CTnanoscan)withdifferentvisualizationmodalities(asinFigure).  Thedemonstrator  includesalsothemanagementforlargesizedata.Asecond 
 applicationdealswith  amultiscalecardiologicalexample  withheterogeneousdata  type,sparse  data,andtime 
 scaleissues.Sparsepoints  onthe3DheartsurfaceareregisteredtoECGdata,whicharevaryingover  acertaintimeframe. 
 Therepresentationof  timevaryingdataisobtainedby animating  the  visualization,andeachframe  display 
 the  value  ofeach  parameteratagiven  time.Athird  demonstratorisaddressingtheinteractivevisualizationinhomogeneousspatialscale:theresolutionandthelargenumberofcomponentswith 
 alotofpoints/cellsmakesthestandardrenderingandtheinteractionvery  slow.ThemainMSVTKideaistodynamicallyload,use,andmanagedifferentresolutionswheninteractingandrenderingthesceneandcomponents.MSVTKwillallowloadingalowresolutionoftheentire 
 data  bydefault,andthenhave  everycomponentdynamicand  clickableto  increase/decreaseitsresolution. 
 CONCLUSIONS MSVTKistryingtocovertheimportantaspectoftheinteractive  visualization,whichis  stillmissing 
 intheavailablesoftwareframeworkswhendealingwithmultiscalebiomedicaldata.  Theinteractionapproachhasbeendefinedbasedonanumberofexemplaryproblems,whichhave 
 beencollectedworldwide,analyzed,  andmadepublic(www.msv-­-project.eu).  TheMSVTKlibraryisbeingtestedwiththeimplementationofanumberofprototypes,which 
 aimatdemonstratingtheefficacyoftheproposed  solution  in  differentbiomedicalcontexts.Byend  of2012,MSVTK 
 librarywill  bereleasedinopen-­-sourceandmadeavailable  tothe  biomedicalcommunityatlarge. ACKNOWLEDGMENTS 
ThisworkhasbeenpartiallyfundedbytheEuropeanCommission  within  theMSVproject(FP7-­-IST-­-248032). REFERENCES 
[1]  http://www.google.com/earth/index.html;[2]  MVicecontietal,  Multimodalfusionofbiomedicaldataatdifferenttemporalanddimensionalscales,Comp.MtdsanProgsBiomed,102(3):227-­-237,2010; 
 [3]  http://www.vtk.org;[4]http://www.vtk.org/doc/release/5.0/html/a01209.html;[5]http://www.openmaf.org;[5] 
 http://www.gimias.org Copyright is held by the author / owner(s). SIGGRAPH 2012, Los Angeles, California, 
August 5 9, 2012. ISBN 978-1-4503-1435-0/12/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2342988</article_id>
		<sort_key>920</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>77</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>10</seq_no>
		<title><![CDATA[Perifoveal display]]></title>
		<subtitle><![CDATA[combining foveal with peripheral vision in one visualization]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2342896.2342988</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2342988</url>
		<abstract>
			<par><![CDATA[<p>The Perifoveal Display (cf. Figure 1) is a visualization display for complex, real-time, dynamic data such as stock market data or control room data. The system takes advantage of the unique properties of the human perceptive system which is capable of perceiving a high degree of detail in the foveal area, but is only perceptive to movement and black and white in the peripheral area. The Perifoveal Display varies how data is visualized based on the user's viewing direction. Data in the center of the user's focus are displayed in a lot of detail using color. Important changes in the data which fall into the periphery are highlighted by movement and change in brightness. As such the system is able to attract the user's focus towards data in the periphery that are in need of attention.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3737464</person_id>
				<author_profile_id><![CDATA[81548023184]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Valentin]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Heun]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MIT Media Lab, Cambridge, MA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[heun@media.mit.edu]]></email_address>
			</au>
			<au>
				<person_id>P3737465</person_id>
				<author_profile_id><![CDATA[81548023185]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Anette]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[von Kapri]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MIT Media Lab, Cambridge, MA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[kapri@media.mit.edu]]></email_address>
			</au>
			<au>
				<person_id>P3737466</person_id>
				<author_profile_id><![CDATA[81502706870]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Pattie]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Maes]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MIT Media Lab, Cambridge, MA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[pattie@media.mit.edu]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>636799</ref_obj_id>
				<ref_obj_pid>636772</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Baudisch, P., DeCarlo, D., Duchowski, A. T., and Geisler, W. S. 2003. Focusing on the essential: considering attention in display design. <i>Commun. ACM 46</i>, 3, 60--66.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2039988</ref_obj_id>
				<ref_obj_pid>2039976</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Fanelli, G., Weise, T., Gall, J., and Gool, L. V. 2011. Real time head pose estimation from consumer depth cameras. In <i>DAGM'11</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>286652</ref_obj_id>
				<ref_obj_pid>286498</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Ishii, H., Wisneski, C., Brave, S., Dahley, A., Gorbet, M., Ullmer, B., and Yarin, P. 1998. ambientROOM: integrating ambient media with architectural space. In <i>CHI</i>, ACM, 173--174.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Perifoveal Display: Combining Foveal with Peripheral Vision in one Visualization Valentin Heun, Anette 
von Kapri, Pattie Maes* MIT Media Lab, Cambridge, MA  Figure 1: A user is standing in front of the Perifoveal 
Display. Two depth cameras track the user s head direction. At the focal point more detailed information 
is shown. The data becomes increasingly abstracted the further away it is from the user s focus. In the 
periphery changes in brightness and movement indicate changes in the data and these can be perceived 
by the user. Abstract The Perifoveal Display (cf. Figure 1) is a visualization display for complex, real-time, 
dynamic data such as stock market data or control room data. The system takes advantage of the unique 
properties of the human perceptive system which is capable of per­ceiving a high degree of detail in 
the foveal area, but is only per­ceptive to movement and black and white in the peripheral area. The 
Perifoveal Display varies how data is visualized based on the user s viewing direction. Data in the center 
of the user s focus are displayed in a lot of detail using color. Important changes in the data which 
fall into the periphery are highlighted by movement and change in brightness. As such the system is able 
to attract the user s focus towards data in the periphery that are in need of attention. 1 Introduction 
and Motivation The human eye has a relative small focus area the foveal vision and a huge range of out 
of focus space the peripheral vision. Within the line of sight the focus we can see complex shapes, sharp 
contours, colors, and we can read text. Images that fall into the periphery are blurry but we can detect 
sudden changes and fast movements. This helps us to react to much more information than we are focusing 
on and consciously aware of. The Perifoveal Dis­play takes advantage of these human perception characteristics 
for visualizations of complex data such as stock markets, network traf­.c, and hospital. The motivation 
is to help users keep track of large amounts of data and help them react more ef.ciently to sensitive 
data changes. Previous work on attention in display design [Baud­ isch et al. 2003] prede.nes a detailed 
area on the display to which a user can drag content for more information. We combine this zoom­ing capability 
with an ambient display such as the ambientROOM [Ishii et al. 1998] in which a user can keep track of 
information in her periphery through color, movement or sound. 2 Implementation and Conclusions The 
Perifoveal Display system consists of four monitors that are ar­ranged around the user and two consumer 
depth cameras. The sys­tem could easily be expanded to a larger number of displays such *{heun, kapri, 
pattie}@media.mit.edu as a video wall. Each depth camera tracks the user s head and head direction in 
front of its two monitors. The space is calibrated so that the monitors are de.ned in relation to the 
camera. The head direc­tion estimation is performed by the algorithm presented in [Fanelli et al. 2011]. 
This results in a head pose and direction estimation in the camera space. Ray-plane intersections lead 
to a pixel position on the monitors which is used as focal point. In our prototype we visualize stock 
market data. The individual stocks are aligned in a grid on all four screens. In the focal area each 
individual stock is color-coded based on its current trend. Red meaning the stock falls, green meaning 
the stock goes up. Using a quad tree structure data points are summarized the more distant they are from 
the focal point. The color fades away until there is only grayscale information. If a data point changes 
in the periphery the corresponding rectangular area changes its brightness (white­the stock goes up, 
black-the stock goes down). Additionally, an animation of a circle moves around the changed data point 
to attract the user s attention. We performed an informal user study in which we asked subjects to keep 
track of changes in the data and to assess their cognitive load using the Perifoveal Display as well 
as visualizations with less peripheral highlighting. The evaluation suggests that there is an ad­vantage 
using the Perifoveal Display for tasks where a user has to keep track of a high amount of real-time data. 
Our results show that a user perceives a lot of stress when using the full detail ev­erywhere display, 
whereas the Perifoveal Display guides the user to data changes which gives a safe and more relaxed feeling. 
 References BAUDISCH, P., DECARLO, D., DUCHOWSKI, A. T., AND GEISLER, W. S. 2003. Focusing on the essential: 
considering attention in display design. Commun. ACM 46, 3, 60 66. FANELLI, G., WEISE, T., GALL, J., 
AND GOOL, L. V. 2011. Real time head pose estimation from consumer depth cameras. In DAGM 11. ISHII, 
H., WISNESKI, C., BRAVE, S., DAHLEY, A., GORBET, M., ULLMER, B., AND YARIN, P. 1998. ambientROOM: inte­grating 
ambient media with architectural space. In CHI, ACM, 173 174. Copyright is held by the author / owner(s). 
SIGGRAPH 2012, Los Angeles, California, August 5 9, 2012. ISBN 978-1-4503-1435-0/12/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2342989</article_id>
		<sort_key>930</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>78</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>11</seq_no>
		<title><![CDATA[SAION]]></title>
		<subtitle><![CDATA[selective audio image reproduction system using multiple hyper directional loudspeakers]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2342896.2342989</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2342989</url>
		<abstract>
			<par><![CDATA[<p>In museums, auditory information provides a greater understanding of exhibits to visitors. In these days, there are museum guidance systems presenting auditory information to viewers at particular positions such as hyper directional loudspeakers on ceilings above exhibits or mobile audio devices. However, sounds are presented from the ceiling or the devices, not from exhibits. Hence auditory information and exhibits are seldom associated together spatially.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3737467</person_id>
				<author_profile_id><![CDATA[81550827256]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Shun]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nagao]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737468</person_id>
				<author_profile_id><![CDATA[81100095332]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Takeshi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Naemura]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>2037807</ref_obj_id>
				<ref_obj_pid>2037715</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Nakagaki. K and Kakehi.Y: SonalShooter: A Spatial Augmented Reality System Using Handheld Directional Speaker with Camera, ACM SIGGRAPH2011, Posters]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 SAION - Selective Audio Image Reproduction System  Using Multiple Hyper Directional Loudspeakers 
Shun Nagao The University of Tokyo Takeshi Naemura The University of Tokyo     Figure 1: SAION 
         Figure 2(a): Overhead view        Figure 2(b): Right view 1. Introduction In 
museums, auditory information provides a greater understanding of exhibits to visitors. In these days, 
there are museum guidance systems presenting auditory information to viewers at particular positions 
such as hyper directional loudspeakers on ceilings above exhibits or mobile audio devices. However, sounds 
are presented from the ceiling or the devices, not from exhibits. Hence auditory information and exhibits 
are seldom associated together spatially. There have been a few studies focusing on spatial combination 
of auditory information and exhibits. Sonal Shooter [Nakagaki et al., 2011] is the hand-held device that 
can present auditory information from real exhibits. A viewer holds the device containing a hyper directional 
loudspeaker and aims exhibits to reflect sound on exhibits. However, the system presents sound from only 
one direction and holding device is a burden of the viewer. In this paper we propose SAION (Selective 
Audio Image reproductiON), the system which can present auditory information from multiple directions 
to local positions using multiple hyper directional loudspeakers. Figure 1 is the overview of the system. 
Since the system presents directional sounds with spatial consistency, viewers can experience sound field 
around exhibits with spatial understanding and select sound fields just by changing their positions without 
any devices. In addition, selecting auditory information around exhibits appropriately, the system can 
reconstruct sophisticated sound field. 2. System Overview The core design concept of SAION is that 
audio images are put on real objects and viewers can change the sounds by walking around exhibits. As 
shown in Figure 2(a), we set two exhibits and two enclosures across a glass plate. Hyper Directional 
Loudspeakers (HDLs) reflect sounds on a glass plate from axisymmetric position of exhibits. The design 
can give audio images on exhibits without setting any loudspeakers around exhibits. By presenting directional 
sounds from multiple directions, the system can overlay multiple audio images on one exhibit. Figure 
2(b) shows the example of the right view of the system and the right enclosure. Each enclosure contains 
two HDLs vertically inclined at varying angles. The viewer standing at a point distant from the exhibit 
can hear the sound from HDL B inclined at low angle and cannot hear the sound from HDL C inclined at 
high angle because the sound from HDL C is absorbed by acoustic absorbent attached on the ceiling. On 
the other hand, the viewer standing at a point near the exhibit can hear the sound from HDL C and cannot 
hear the sound from HDL B because the sound is absorbed by stomach area of the viewer. Therefore, the 
viewers can hear different sounds from the same exhibit just coming closer to the exhibit or backing 
away from the exhibit. 3. Implementation First, we set two exhibits inside a room the wall of which 
is a large glass plate and set two operators around each exhibit. Second, we attach microphones to the 
operators and collect the voices of the operators talking about abstract of exhibits. The system presents 
the sounds to viewers outside the room at positions A and B in Figure 2(a) in real time. Only viewers 
at particular positions can hear voices about abstracts of the exhibits as if the voices pass through 
a glass plate. In addition, we recorded detail system sounds of exhibits previously and the system presents 
the sounds to viewers at position C in figure 2(a). The viewers standing at C can hear the detail sound 
of exhibit 2 from the right-hand side and the detail sound of exhibit 1 from the left-hand side separately. 
Therefore, viewers standing at a point distant from the exhibits can hear abstract information and viewers 
standing at a point near the exhibits can hear detail information. It should be added that short people 
such as children can hear abstract information at a point near the exhibits. The fact suggests that the 
system can present different sounds to short people and tall people individually. We held a demonstration 
of the system. During the demonstration, appropriate sounds selected from the noisy room were reconstructed 
outside the room with spatial consistency. As a result, every viewer could rapidly understand which exhibits 
emitted sounds. Hearing reconstructed sound field, viewers became interested in exhibits and got in the 
room. References Nakagaki. K and Kakehi.Y: SonalShooter: A Spatial Augmented Reality System Using Handheld 
Directional Speaker with Camera, ACM SIGGRAPH2011, Posters 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2342990</article_id>
		<sort_key>940</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>79</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>12</seq_no>
		<title><![CDATA[Network for sciences, engineering, arts and design]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2342896.2342990</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2342990</url>
		<abstract>
			<par><![CDATA[<p>Innovations emerging from the intersection of the sciences, engineering, arts and design are transforming our economy, culture, and learning contexts. This transformation is emerging through development of products, methods, and questions that are fundamentally hybrid, such as software developed for human play, hardware designed for aesthetic elegance, or the plethora of scientific and cultural information requiring new means of interpretation and expression in order to enable greater understanding of complex dynamics.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3737469</person_id>
				<author_profile_id><![CDATA[81504686507]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Carol]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[LaFayette]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Texas A&M University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[lurleen@viz.tamu.edu]]></email_address>
			</au>
			<au>
				<person_id>P3737471</person_id>
				<author_profile_id><![CDATA[81100270405]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Thanassis]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Rikakis]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Arizona State University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[thanassis.rikakis@asu.edu]]></email_address>
			</au>
			<au>
				<person_id>P3737472</person_id>
				<author_profile_id><![CDATA[81100290741]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Donna]]></first_name>
				<middle_name><![CDATA[J.]]></middle_name>
				<last_name><![CDATA[Cox]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Illinois Urbana Champaign]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[cox@ncsa.uiuc.edu]]></email_address>
			</au>
			<au>
				<person_id>P3737473</person_id>
				<author_profile_id><![CDATA[81504684961]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Gunalan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nadarajan]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Michigan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[guna@umich.edu]]></email_address>
			</au>
			<au>
				<person_id>P3737474</person_id>
				<author_profile_id><![CDATA[81100637079]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Carol]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Strohecker]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of North Carolina Center for Design Innovation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[cs@centerfordesigninnovation.org]]></email_address>
			</au>
			<au>
				<person_id>P3737475</person_id>
				<author_profile_id><![CDATA[81504687264]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Pamela]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Jennings]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The School of the Art Instutute of Chicago]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[pjennings@saic.edu]]></email_address>
			</au>
			<au>
				<person_id>P3737476</person_id>
				<author_profile_id><![CDATA[81100554401]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>7</seq_no>
				<first_name><![CDATA[Noah]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wardrip-Fruin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of California, Santa Cruz]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737477</person_id>
				<author_profile_id><![CDATA[81100439791]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>8</seq_no>
				<first_name><![CDATA[Roger]]></first_name>
				<middle_name><![CDATA[F.]]></middle_name>
				<last_name><![CDATA[Malina]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Texas, Dallas]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737478</person_id>
				<author_profile_id><![CDATA[81100013007]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>9</seq_no>
				<first_name><![CDATA[Sheldon]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Brown]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of California, San Diego]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737470</person_id>
				<author_profile_id><![CDATA[81361606943]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>10</seq_no>
				<first_name><![CDATA[Alicia]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gibb]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[BugLabs, New York]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Network for Sciences, Engineering, Arts and Design *Carol LaFayette, MFA, Texas A&#38;M University; 
? Thanassis Rikakis, Ph.D., Arizona State University; Donna J. Cox, Ph.D., University of Illinois Urbana 
Champaign; § Gunalan Nadarajan, University of Michigan; ¶ Carol Strohecker, Ph.D., University of North 
Carolina Center for Design Innovation; ?Pamela Jennings, Ph.D., The School of the Art Instutute of Chicago. 
Contributors: Noah Wardrip-Fruin, Ph.D., University of California, Santa Cruz; Roger F. Malina, Ph.D., 
University of Texas, Dallas; Sheldon Brown, University of California, San Diego; Alicia Gibb, BugLabs, 
New York  Top 75 words in meetings about a network, from Alexandria to Baltimore (2010-2011), via wordle.com 
 1. Introduction Innovations emerging from the intersection of the sciences, engineering, arts and design 
are transforming our economy, culture, and learning contexts. This transformation is emerging through 
development of products, methods, and questions that are fundamentally hybrid, such as software developed 
for human play, hardware designed for aesthetic elegance, or the plethora of scientific and cultural 
information requiring new means of interpretation and expression in order to enable greater understanding 
of complex dynamics. As our world undergoes rapid change, we need new ways to create and engage knowledge, 
drawing from multiple disciplines as we seek to understand the ever-increasing complexity. By working 
together, interdisciplinary collaborators can provide insights into dilemmas that elude understanding 
through any singular inquiry. Global economic interests are at stake: we anticipate that most types of 
employment that will come to dominate our economies in twenty years are being spawned now. New forms 
of partnership among political, academic and civil sectors of society are required if we are to bring 
about the needed changes intelligently and humanely. Innovation stemming from interdisciplinary creativity 
is a major contributor to the development of new, sustainable economies and harmonious, cooperating societies. 
2. Impact The National Science Foundation Computer and Information Science and Engineering (CISE) Information 
&#38; Intelligent Systems (IIS) program sponsored five workshops in 2010-2011, bringing together artists 
and scientists from across the United States, to address needs of the burgeoning community of groups 
and individuals engaged in transdisciplinary practice. This effort resulted in the genesis of a new network 
focusing on advocacy and dissemination of innovative methods for connecting and supporting a distributed 
community across academia, non-profit organizations, civil society, industry, and funding entities. The 
network facilitates research community development; collaboration and project matchmaking; expertise 
referrals; large-scale collaborative teaching; forums to share best practices in STE[A]M learning; and 
philanthropic opportunities for funding organizations. The growing interdisciplinary community continues 
to face challenges in its efforts to self-organize among constraints imposed by academic systems and 
historical biases; the community continues to seek a dynamic and synergizing research and outreach exchange. 
We recognize an urgent need for a paradigm shift that can overcome such biases and fully address, in 
an integrated manner, the documentation needs of the science-art community. Therefore, the SEAD network 
is undertaking the development of a dissemination portal (XSEAD) that will provide a centralized view 
of this emergent field; fast dissemination of multimodal research outcomes; extensive databases of prior 
and current research, an informed record of science-art curricula; support structures for science-art 
careers; and evidence of societal impact of interdisciplinary integration. The network addresses fundamental 
challenges including the need to align academic pedagogies with 21st-century thinking skills; to promote 
diversity of perspectives, approaches, and people in the creative economy; and to benchmark best practices 
that create critical thinkers and leaders for the ever-changing job market. We are providing a platform 
to generate and disseminate public dialogue about the intellectual, cultural, and economic potential 
of creative intersections of art, science and technology. For more information: sead.viz.tamu.edu This 
material is based upon work supported by the National Science Foundation under Grant No. 1142510, Collaborative 
Research: EAGER: Network for Science, Engineering, Arts and Design (NSEAD) IIS, Human Centered Computing; 
and Grant 1141631 Collaborative Research: EAGER: virtual exchange for Science, Engineering, Arts and 
Design (XSEAD) IIS, Human Centered Computing. Any opinions, findings, and conclusions or recommendations 
expressed in this material are those of the authors and do not necessarily reflect the views of the National 
Science Foundation. e-mail: * lurleen@viz.tamu.edu; ? thanassis.rikakis@asu.edu ; cox@ncsa.uiuc.edu; 
§ guna@umich.edu; ¶cs@centerfordesigninnovation.org; ? pjennings@saic.edu  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2342991</article_id>
		<sort_key>950</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>80</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>13</seq_no>
		<title><![CDATA[Stuffed toys alive!]]></title>
		<subtitle><![CDATA[cuddly robots from fantasy world]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2342896.2342991</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2342991</url>
		<abstract>
			<par><![CDATA[<p>Stuffed toys live with all ages and hold them in some physical and mental aspects. With the soft feel and cute characters, stuffed toys play with them, sleep together and listen to their complaints. These roles of stuffed toys show that people imagine stuffed toys are inter-active creature. Indeed, there are many stories and movies in which stuffed toys work as living characters. However, stuffed toys in real world are just dolls and they cannot move and react.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[haptic interaction]]></kw>
			<kw><![CDATA[human friendly robot]]></kw>
			<kw><![CDATA[stuffed toy]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3737479</person_id>
				<author_profile_id><![CDATA[81504687256]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Yohei]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yamashita]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tokyo Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[yamashita@hi.pi.titech.ac.jp]]></email_address>
			</au>
			<au>
				<person_id>P3737480</person_id>
				<author_profile_id><![CDATA[81333489343]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Tatsuya]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ishikawa]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Electro-Communications]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[ishikawa@haselab.net]]></email_address>
			</au>
			<au>
				<person_id>P3737481</person_id>
				<author_profile_id><![CDATA[81316489667]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Hironori]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Mitake]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tokyo Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[mitake@hi.pi.titech.ac.jp]]></email_address>
			</au>
			<au>
				<person_id>P3737482</person_id>
				<author_profile_id><![CDATA[81485652285]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Yutaka]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Takase]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tokyo Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[takase@hi.pi.titech.ac.jp]]></email_address>
			</au>
			<au>
				<person_id>P3737483</person_id>
				<author_profile_id><![CDATA[81313481766]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Fumihiro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kato]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tokyo Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[fumihiro.k@hi.pi.titech.ac.jp]]></email_address>
			</au>
			<au>
				<person_id>P3737484</person_id>
				<author_profile_id><![CDATA[81421595917]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Ikumi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Susa]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tokyo Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[susa@hi.pi.titech.ac.jp]]></email_address>
			</au>
			<au>
				<person_id>P3737485</person_id>
				<author_profile_id><![CDATA[81481647072]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>7</seq_no>
				<first_name><![CDATA[Shoichi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hasegawa]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tokyo Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[hase@hi.pi.titech.ac.jp]]></email_address>
			</au>
			<au>
				<person_id>P3737486</person_id>
				<author_profile_id><![CDATA[81100567066]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>8</seq_no>
				<first_name><![CDATA[Makoto]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sato]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tokyo Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[msato@hi.pi.titech.ac.jp]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Shibata, T., and Tanie, K. 2001. Physical and affective interaction between human and mental commit robot. In <i>Proc. of ICRA 2001</i>, vol. 3, 2572--2577 vol.3.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1179149</ref_obj_id>
				<ref_obj_pid>1179133</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Stiehl, W. D., Breazeal, C., Han, K.-H., Lieberman, J., Lalla, L., Maymin, A., Salinas, J., Fuentes, D., Toscano, R., Tong, C. H., and Kishore, A. 2006. The huggable: a therapeutic robotic companion for relational, affective touch. In <i>ACM SIGGRAPH 2006 Emerging technologies</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Stuffed Toys Alive! : Cuddly Robots from Fantasy World Yohei Yamashita * Tatsuya Ishikawa Hironori 
Mitake* Yutaka Takase* Fumihiro Kato* Ikumi Susa* Shoichi Hasegawa* Makoto Sato* {yamashita, mitake, 
takase, fumihiro.k, susa, msato, hase}@hi.pi.titech.ac.jp* ishikawa@haselab.net Tokyo Institute of Technology* 
University of Electro-Communications Keywords: human friendly robot, stuffed toy, haptic interaction 
1 Introduction Stuffed toys live with all ages and hold them in some physical and mental aspects. With 
the soft feel and cute characters, stuffed toys play with them, sleep together and listen to their complaints. 
These roles of stuffed toys show that people imagine stuffed toys are inter­active creature. Indeed, 
there are many stories and movies in which stuffed toys work as living characters. However, stuffed toys 
in real world are just dolls and they cannot move and react. Animatronics, pets and social robots with 
skin of stuffed toys [Shibata and Tanie 2001][Stiehl et al. 2006] realize soft motion of creatures in 
our imagination world. However, they don t have soft feelings. It is known that transitional object for 
infants begins from soft object. The soft feel will be a cause for familiarity of stuffed toys. Thus, 
we propose a mechanism for stuffed toy type robots ( cuddly robots ) which keep the soft feel of stuffed 
toys. 2 Innovation The soft feel both on mechanics and motion in the proposed robot is realized by three 
innovations; Driving mechanism retaining nature of stuffed toys The arms, legs and a head are soft to 
bone. They consist of strings, cotton wool and cloth, and have no hard materials. The sack of cloth is 
in.ated by the pressure from the internal cotton wool. The string is connected to the end of the sack 
and be sewed to the opening of the sack. Then the other side of the string is connected to a small winch 
placed in the center of the body where all hard mechanism is placed. The arms and legs have a good soft 
feel because they don t give sensation of any hard obstacles. In addition, they are robust to bending. 
When bending, the driving mechanism makes natural soft curves on arms and legs and gives soft impression. 
These shapes give natural impression as stuffed toys because they are same to the shapes of the curves 
which are generated when we animate them by taking their hands and feet. Motion control for the soft 
mechanism Proposed mechanism has repeat accuracy enough to create a map between the hand position and 
lengths of strings. Reaching motions to any target positions are available and various interactions can 
be realized. The smooth acceleration and motions conscious on the elasticity of the cotton wool gives 
impression of natural motion of stuffed toys. Force sensor with driving strings With force sensors embedded 
in the winches, the driving strings work as tension sensor. They sense forces on arms, legs and a head 
and realize reactions to user s interaction. In addition, the tension sensors are accurate enough to 
realize impedance control. Interac­tions such as handshakes are realized by controlling target position 
of the motion based on the sensed force. Figure 1: An example of touching interaction and appearance 
and inside of the cuddly robot. The cuddly robot is soft to the bone.  3 Exposition When the stuffed 
bear (cuddly robot) notices the user approaching to him, he stretches his hand to attract attention of 
the user. Then, the user takes his hand and he moves his hand to correspond to the motion of the user. 
If the user puts and leaves him, he will writhe and struggle to get the user s favor until the user takes 
him up in the arms. Finally, the user holds him and he feels a relief and moves his hands up and down. 
 4 Vision Recently, alternate reality and gami.cation, which embed story­telling and entertainments into 
our daily world, are getting public attention. We had proposed mixed reality installations such as Ko­bito 
or Haptic Ring which realize interaction with characters in the real world. However, getting a real body 
is ideal for haptic in­teraction for characters. Stuffed toys get into our daily lives. They have both 
bodies of real substances and their own story and fantasy world. Cuddly robot, which inherits these characteristics 
of stuffed toys, can be a new medium which entertains us in our daily lives and gives a sense of unity 
to our daily world and the fantasy world. Imagine the future, the stuffed toys from the fantasy world 
live with us and enchant our daily lives. References SHIBATA, T., AND TANIE, K. 2001. Physical and affective 
interac­tion between human and mental commit robot. In Proc. of ICRA 2001, vol. 3, 2572 2577 vol.3. 
STIEHL, W. D., BREAZEAL, C., HAN, K.-H., LIEBERMAN, J., LALLA, L., MAYMIN, A., SALINAS, J., FUENTES, 
D., TOSCANO, R., TONG, C. H., AND KISHORE, A. 2006. The huggable: a therapeutic robotic companion for 
relational, affec­tive touch. In ACM SIGGRAPH 2006 Emerging technologies. Copyright is held by the author 
/ owner(s). SIGGRAPH 2012, Los Angeles, California, August 5 9, 2012. ISBN 978-1-4503-1435-0/12/0008 
  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2342992</article_id>
		<sort_key>960</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>81</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>14</seq_no>
		<title><![CDATA[Technoculture of handcraft]]></title>
		<subtitle><![CDATA[fine gesture recognition for <i>haute couture</i> skills preservation and transfer in Italy]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2342896.2342992</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2342992</url>
		<abstract>
			<par><![CDATA[<p>In many different fields specialized artisans have become difficult to find, as their knowledge and their practical abilities are anything but easy to teach and transfer with training models that are no longer based on traditional master-apprentice relationships. This situation is particularly exacerbated in all those high-end <i>haute couture</i> companies that have built their glory on their style, as well as on their highly specialized craftsmen capable of turning leather, wool and other materials into inestimably valued shoes, bags and clothes. Today, alternative solutions can be found as technology can be put to good use to encode, and thus preserve, all this expertise, providing digital means of passing it on to new generations. Modern technologies have already been experimented with manual craftsmanship, for example, in the context of knitting [Rosner and Ryokai, 2009], although focusing most on its amusement and social aspects rather than on its knowledge encoding ones. Tracking techniques could support, instead, a system capable of digitizing the hand gestures performed by an artisan while handcrafting. However, such proposals very often require users to wear specific garments, not always suiting the scenario, as artisans reluctantly bear the use of invasive modern technologies. We show that noninvasive technologies can be exploited to encode and thus preserve artisanal knowledge by presenting a system based on a set of fine gesture recognition algorithms, as derived by [Roccetti et al., 2010], that require no peculiar attire as they solely utilize a frontal webcam, positioned at a close distance from a handcrafter. We witness the viability of using such system for tracking a handcrafter in two important phases of shoe making: hammering and sewing a shoe.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3737487</person_id>
				<author_profile_id><![CDATA[81326490801]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Gustavo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Marfia]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Bologna, Italy]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737488</person_id>
				<author_profile_id><![CDATA[81100141776]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Marco]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Roccetti]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Bologna, Italy]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[marco.roccetti@unibo.it]]></email_address>
			</au>
			<au>
				<person_id>P3737489</person_id>
				<author_profile_id><![CDATA[81504684187]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Giovanni]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Matteucci]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Bologna, Italy]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737490</person_id>
				<author_profile_id><![CDATA[81504687602]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Andrea]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Marcomini]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Bologna, Italy]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1921148</ref_obj_id>
				<ref_obj_pid>1921141</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Roccetti, M., Marfia, G. and Zanichelli, M., 2010. The art and craft of making the tortellino: playing with a digital gesture recognizer for preparing pasta culinary recipes. <i>ACM Comp. Enter</i>. 8, 4.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1640264</ref_obj_id>
				<ref_obj_pid>1640233</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Rosner, D. K. and Ryokai, K., 2009. Reflections on craft: probing the creative process of everyday knitters. In <i>Proc. 7th ACM Conf. on Creativity and Cognition</i>, Berkeley, 195--204.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Technoculture of Handcraft: Fine Gesture Recognition for Haute Couture Skills Preservation and Transfer 
in Italy Gustavo Marfia, Marco Roccetti, Giovanni Matteucci and Andrea Marcomini * University of Bologna, 
Italy  position of a hand, and (c) system performance with hammering. position of a hand, and (c) system 
performance with sewing. 1. Introduction plus (ii) recording the sequence of positions that have been 
reached by the hand within that action. Our fine-grained gestureIn many different fields specialized 
artisans have become difficultrecognition system can support the encoding of such two actions to find, 
as their knowledge and their practical abilities are by following the position of an artisan s hand (Figs. 
1.b and 2.b).anything but easy to teach and transfer with training models thatWe anticipate our accuracy 
results representing in Figs. 1.c andare no longer based on traditional master-apprentice relationships.2.c 
the frames captured by a frontal camera with our system This situation is particularly exacerbated in 
all those high-endcapable of correctly tracking the positions of a hand (i.e., hits)haute couture companies 
that have built their glory on their style,during a single execution of the two types of action. Technically, 
as well as on their highly specialized craftsmen capable of turningwe capture a frame with the camera, 
subtract it from the leather, wool and other materials into inestimably valued shoes,previously captured 
one and apply a Gaussian filter to the bags and clothes. Today, alternative solutions can be found as 
difference, thus identifying the macro area where changestechnology can be put to good use to encode, 
and thus preserve,occurred. Given a macro area partitioned into a grid of squares,all this expertise, 
providing digital means of passing it on to newthe topmost square, surrounded by adjacent ones that also 
fall generations. Modern technologies have already been within the macro area, is chosen to individuate 
the current experimented with manual craftsmanship, for example, in the position of the hand (Figs. 1.b 
and 2.b). As such methodologycontext of knitting [Rosner and Ryokai, 2009], although focusing may fail 
returning discontinuities between subsequent frames, ourmost on its amusement and social aspects rather 
than on itssystem embodies a final step with a Kalman filter, whose role isknowledge encoding ones. Tracking 
techniques could support, that of keeping the trajectory along which a hand is tracked as instead, a 
system capable of digitizing the hand gestures consistent. This method is sound as the experiments carried 
outperformed by an artisan while handcrafting. However, such with an Italian shoe brand reveal. We got 
an almost perfect score proposals very often require users to wear specific garments, not(hand correctly 
detected) in all situations, with a supporting framealways suiting the scenario, as artisans reluctantly 
bear the use ofrate as high as 30 fps and a single execution of a invasive modern technologies. We show 
that noninvasive hammering/sewing action (from top to bottom) as long as 500 mstechnologies can be exploited 
to encode and thus preserveon average (Table I). This way, the correctness of the gesturesartisanal knowledge 
by presenting a system based on a set of fineperformed by less skilled individuals can be assessed, based 
on agesture recognition algorithms, as derived by [Roccetti et al.,comparison with the available data 
[Roccetti et al., 2010]. 2010], that require no peculiar attire as they solely utilize a frontalwebcam, 
positioned at a close distance from a handcrafter. We Distance of cam Hammering (Hit %) Sewing (Hit %) 
50 cm 99% 98% 120 cm 97% 96% Table I. Accuracy Results. witness the viability of using such system for 
tracking a handcrafter in two important phases of shoe making: hammeringand sewing a shoe. 2. Our Approach 
References Hammering and sewing (Figs. 1.a and 2.a) are two fundamentalROCCETTI, M., MARFIA, G. AND ZANICHELLI, 
M., 2010. The art and processes at the base of footwear crafting. With the hammeringcraft of making the 
tortellino: playing with a digital gesture phase, an artisan forges a shoe s shape, while with the sewing 
onerecognizer for preparing pasta culinary recipes. ACM Comp. it assembles its parts. Both require much 
attention: mistakes onEnter. 8, 4. the former can loosen up leather either excessively or too mildly, 
whereas wrongdoings in the latter can compromise the beauty ofROSNER, D.K. AND RYOKAI, K., 2009. Reflections 
on craft: the artifact. In summary, encoding such actions consists in: (i)probing the creative process 
of everyday knitters. In Proc. 7th precisely estimating the number of times a given action ACM Conf. 
on Creativity and Cognition, Berkeley, 195-204. (hammering or sewing) is performed along with its relative 
speed,   *corresponding author s e-mail: marco.roccetti@unibo.it Copyright is held by the author / 
owner(s). SIGGRAPH 2012, Los Angeles, California, August 5 9, 2012. ISBN 978-1-4503-1435-0/12/0008 
 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2342993</article_id>
		<sort_key>970</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>82</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>15</seq_no>
		<title><![CDATA[The telematic dinner party]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2342896.2342993</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2342993</url>
		<abstract>
			<par><![CDATA[<p>Meals have traditionally been a site for togetherness. We explore the opportunities to design a technology platform that supports remote guests in experiencing togetherness and playfulness within the practices of a traditional dinner party. Through both visual, aural channels and remote agency, the guests shared a holistic telematic dining experience comparable to a traditional co-presence dinner. Based on the findings, we propose that one must consider the social structure and cultural background of users to inform the design of technological intervention that supports a sense of togetherness.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3737491</person_id>
				<author_profile_id><![CDATA[81503681039]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Pollie]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Barden]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Queen Mary University of London, London, UK]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[p.barden@eecs.qmul.ac.uk]]></email_address>
			</au>
			<au>
				<person_id>P3737492</person_id>
				<author_profile_id><![CDATA[81502812584]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Rob]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Comber]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Newcastle University, Newcastle, UK]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[robert.comber@newcastle.ac.uk]]></email_address>
			</au>
			<au>
				<person_id>P3737493</person_id>
				<author_profile_id><![CDATA[81100483855]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Nick]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bryan-Kinns]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Queen Mary University of London, London, UK]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[nick.bryan-kinns@eecs.qmul.ac.uk]]></email_address>
			</au>
			<au>
				<person_id>P3737494</person_id>
				<author_profile_id><![CDATA[81330498936]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Tony]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Stockman]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Queen Mary University of London, London, UK]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[tony.stockman@eecs.qmul.ac.uk]]></email_address>
			</au>
			<au>
				<person_id>P3737495</person_id>
				<author_profile_id><![CDATA[81100295667]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Patrick]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Olivier]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Newcastle University, Newcastle, UK]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[patrick.olivier@newcastle.ac.uk]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1357130</ref_obj_id>
				<ref_obj_pid>1357054</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Grimes, A., and Harper, R. 208. Celebratory technology: new directions for food research in hci. In <i>Proceedings of the twenty-sixth annual SIGCHI conference on Human factors in computing systems</i>, ACM Press, CHI '08, ACM, 467--476.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>62273</ref_obj_id>
				<ref_obj_pid>62266</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Grudin, J. 1988. Why cscw applications fail: problems in the design and evaluation of organizational interfaces. In <i>Proceedings of the 1988 ACM conference on Computer-supported cooperative work</i>, ACM Press, CSCW '88, ACM, 85--938.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Mann, J., and Teran, M. 2001. <i>Experiments in Connected Social Spaces. LiveForm:Telekinetics</i>. http://www.lftk.org/tiki/tiki-index.php.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 The Telematic Dinner Party Pollie Barden1, Rob Comber2, Nick Bryan-Kinns1, Tony Stockman1, Patrick Olivier2 
* 1Queen Mary University of London, London, UK 2Newcastle University, Newcastle, UK  Figure 1: (a) Observed 
dinner party. (b) London/Barcelona pilot study. (c) TDP: guests teasing each other with networked turntables. 
Abstract Meals have traditionally been a site for togetherness. We explore the opportunities to design 
a technology platform that supports re­mote guests in experiencing togetherness and playfulness within 
the practices of a traditional dinner party. Through both visual, aural channels and remote agency, the 
guests shared a holistic telematic dining experience comparable to a traditional co-presence dinner. 
Based on the .ndings, we propose that one must consider the social structure and cultural background 
of users to inform the design of technological intervention that supports a sense of togetherness. 1 
Introduction There is an increasing desire to remain connected when physically distant. Computer-mediated 
communication (CMC) and telematic (technology systems that connect people) art practices explored this 
desire. While CMC historically has focused on workplace and tasks [Grudin 1988], recently, there is a 
growing exploration around food practices [Grimes and Harper 208]. Similarly, the telematic art practice 
typically manifests as a performance, even when using the dinner party format [Mann and Teran 2001]. 
For most CMC and telematic art explorations around food, the outcomes are often a cel­ebration of the 
technological feats rather than a means for a shared a dining experience. 2 Our Approach The .nal design 
for the Telematic Dinner Party was informed by observing three traditional dinner parties and conducting 
an pilot study. The traditional dinner parties identi.ed three prominent in­teractions : 1) toasting 
2) food sharing and 3) coordinated passing of food and drink. The pilot study connected remote guests 
in London, United King­dom and Barcelona, Spain. The study highlighted three main con­siderations: 1) 
dif.culty in sustaining connections, both technically and socially, 2) a single audio channel degraded 
communication but prompted creativity and 3) guests a desire to engaged in shared activities, such as 
toasting. The Telematic Dinner Party (TDP) builds on the results of the pilot study. Each guest had their 
own localized presence: 1) visu­ally through tabletop projects of their place setting, hands/arms and 
*e-mail:p.barden, nick.bryan-kinns, tony.stockman @eecs.qmul.ac.uk e-mail:robert.comber, patrick.olivier 
@newcastle.ac.uk 2) aurally through an dedicated audio channel via a lavalier micro­phone mapped to a 
corresponding speaker in the remote space. We identi.ed turntables as a device designed for the dining 
table and could be utilized to provide physical remote agency. A set of two networked turntables (Lazy 
Susans) were developed and pro­grammed to coordinate their locations. When a guest manually ro­tated 
one turntable, the other turntable would rotate to match the new position, with the last one moved being 
dominate. Across the four TDP dinners, the turntables and audio provided the most support in connecting 
the guests. In regards to the localize audio, guests reported when talking to a remote guest they momen­tarily 
forgot it was a speaker in the chair. The guests engaged in teasing by turning the turntables to keep 
the remote guests from ob­taining food or through touching the tabletop video projections of their remote 
guests food. In all the TDPs, the guests interacted with each other through both the networked turntables 
and the tabletop projections. Overall, the guests reported they felt they shared a meal and got to know 
their respective remote guests. In the Telematic Dinner Party, we demonstrated an implementation of a 
technological intervention that, for the most part, supported a cohesive dining experience comprised 
of remotely located guests. The guests had the agency to extended their experience through playful interactions 
with the networked turntables and the tabletop projects. This playfulness was a mode of mutual engagement, 
not outside the behaviour that may occur at a co-presence dinner. While further investigation is required, 
our observations of the TDPs and guest feedback indicate one must consider the social structure and cultural 
background of users to inform the design of a technological intervention intended to promote togetherness. 
 References GRIMES, A., AND HARPER, R. 208. Celebratory technology: new directions for food research 
in hci. In Proceedings of the twenty­sixth annual SIGCHI conference on Human factors in computing systems, 
ACM Press, CHI 08, ACM, 467 476. GRUDIN, J. 1988. Why cscw applications fail: problems in the de­sign 
and evaluationof organizational interfaces. In Proceedings of the 1988 ACM conference on Computer-supported 
coopera­tive work, ACM Press, CSCW 88, ACM, 85 938. MANN, J., AND TERAN, M. 2001. Experiments in Connected 
So­cial Spaces. LiveForm:Telekinetics. http://www.lftk.org/tiki/tiki­index.php. Copyright is held by 
the author / owner(s). SIGGRAPH 2012, Los Angeles, California, August 5 9, 2012. ISBN 978-1-4503-1435-0/12/0008 
  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2342994</article_id>
		<sort_key>980</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>83</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>16</seq_no>
		<title><![CDATA[Video retrieval based on user-specified deformation]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2342896.2342994</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2342994</url>
		<abstract>
			<par><![CDATA[<p>Let's assume that we are now watching the scene of Fig. 1-a, where a red racing car is running from left to right. At the same time, we feel like watching another scene like Fig. 1-b, where a similar car is running toward us. To find such a desired scene, we usually push the forward and rewind buttons or move the play bar. If we cannot find the scene in the currently watching video, we have to go to a video sharing service like YouTube to further search for it. However, these are tedious tasks. Goldman et al. propose a method to navigate a single video [2008], but we want to find a scene not only in a single video but also a different video from the currently watching one.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3737496</person_id>
				<author_profile_id><![CDATA[81504687598]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Yuta]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kawate]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Electro-Communications]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737497</person_id>
				<author_profile_id><![CDATA[81100546396]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Makoto]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Okabe]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Electro-Communications and JST Presto]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737498</person_id>
				<author_profile_id><![CDATA[81100253292]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Rikio]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Onai]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Electro-Communications]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737499</person_id>
				<author_profile_id><![CDATA[81504684260]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Hiromi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hirano]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Rakuten, Inc. Rakuten Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737500</person_id>
				<author_profile_id><![CDATA[81504688602]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Masahiro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sanjo]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Rakuten, Inc. Rakuten Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1449719</ref_obj_id>
				<ref_obj_pid>1449715</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[D. B. Goldman, C. Gonterman, B. Curless, D. Salesin, and S. M. Seitz. 2008. Video object annotation, navigation, and composition. In <i>Proc. of UIST</i>, 3--12.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1409496</ref_obj_id>
				<ref_obj_pid>1409491</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[P. Sand and S. Teller. 2008. Particle Video: Long-range motion estimation using point trajectories. <i>IJCV</i>, 80, 1, pp 72--91.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Video Retrieval based on User-Specified Deformation ,* Yuta KawateMakoto OkabeRikio OnaiHiromi HiranoMasahiro 
Sanjo * The University of Electro-Communications JST PRESTO Rakuten, Inc. Rakuten Institute of Technology 
1. Introduction Let's assume that we are now watching the scene of Fig. 1-a, where a red racing car 
is running from left to right. At the same time, we feel like watching another scene like Fig. 1-b, where 
a similar car is running toward us. To find such a desired scene, we usually push the forward and rewind 
buttons or move the play bar. If we cannot find the scene in the currently watching video, we have to 
go to a video sharing service like YouTube to further search for it. However, these are tedious tasks. 
Goldman et al. propose a method to navigate a single video [2008], but we want to find a scene not only 
in a single video but also a different video Figure 1. Our motivation. We propose a novel user interaction 
for quickly and easily playing the desired scene. Our idea is based on the observation that the car in 
the desired scene (Fig. 1-b) is the rotated version of the car in the currently watching scene (Fig. 
1-a). We then allow the user to specify this deformation. In Fig. 2-a, the user draws the two green arrows 
intending to move the front and back spoilers to tips of the arrows. Then, our system immediately retrieves 
the desired scene, where the location of each spoiler corresponds to the tip of the arrow (Fig. 2-b). 
 Figure 2. The proposed user interaction.  2. Our Approach It is difficult to directly find the desired 
scene (Fig. 2-b), using the currently watching scene (Fig. 2-a) as a query of a standard image or video 
retrieval technique. For example, we extract SIFT features in each frame of the video and compute their 
matching between the currently watching scene and the other frames. As shown in Fig. 3, we successfully 
find many consistent matches with a similar looking scene: it is left side right but the posture of the 
car is similar. However, there is no consistent match between the cars in the right case. However, it 
is interesting to note that the bottom two frames in Fig. 3 exist in the same video sequence: the bottom-left 
frame comes several seconds after the bottom-right frame, i.e., the car is coming toward us and then 
turning to the left. We can find the desired scene by rewinding the video from the frame of Fig. 3-left. 
During the rewinding process, we track the front and back spoilers, since they are specified by the user 
as tails of the arrows. We stop to rewind the video, when each tracker comes near to the tip of the corresponding 
arrow. Then, the desired scene has come.  Figure 3. Video retrieval using SIFT matching. Left: We find 
many matches. Right: We cannot find any consistent match. In conclusion, our algorithm consists of two 
processes: first, we find a frame that looks similar to the query frame using SIFT matching, and second, 
we automatically forward and rewind the video from the similar looking frame to find the user-desired 
scene. SIFT matching is efficiently performed using kd-tree algorithm. To efficiently find the desired 
scene by forwarding and rewinding the video, we use Particle Video algorithm [P. Sand et al. 2008] for 
motion tracking. 3. Results We experiment our method on two videos downloaded from YouTube: one is a 
video of racing cars, and the other is of running horses. The duration of them are 3,718 and 4,450 frames, 
i.e., several minutes long. We spend about four seconds in average for retrieval process using an Intel 
i7 2.8GHz processor. The input and retrieved scenes are shown in Fig. 4. Figure 4. Retrieval results. 
 References D. B. GOLDMAN, C. GONTERMAN, B. CURLESS, D. SALESIN, AND S. M. SEITZ. 2008. Video object 
annotation, navigation, and composition. In Proc. of UIST, 3-12.  P. SAND AND S. TELLER. 2008. Particle 
Video: Long-range motion estimation using point trajectories. IJCV, 80, 1, pp 72-91. Copyright is held 
by the author / owner(s). SIGGRAPH 2012, Los Angeles, California, August 5 9, 2012. ISBN 978-1-4503-1435-0/12/0008 
  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2342995</article_id>
		<sort_key>990</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>84</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>17</seq_no>
		<title><![CDATA[World eco-tope]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2342896.2342995</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2342995</url>
		<abstract>
			<par><![CDATA[<p>To feel the vicissitude of earth more intuitively, our biotope that represents the earth shows the weather of countries on the globe. There are no need to utilize expressions of letters, weather symbols, or voice of weather forecaster. You can see the weather of any locations including the ones you do not even know of, located at the remotest part of the world. And our biotope enables the experience of earth now and nature through the changing of weathers.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3737501</person_id>
				<author_profile_id><![CDATA[81501645817]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Ayumi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kato]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[kato@ics.t.u-tokyo.ac.jp]]></email_address>
			</au>
			<au>
				<person_id>P3737502</person_id>
				<author_profile_id><![CDATA[81488658584]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Masahiko]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Watanabe]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[masahiko@ics.t.u-tokyo.ac.jp]]></email_address>
			</au>
			<au>
				<person_id>P3737503</person_id>
				<author_profile_id><![CDATA[81363593374]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Yusuke]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Fukazawa]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[fukazawayuu@nttdocomo.co.jp]]></email_address>
			</au>
			<au>
				<person_id>P3737504</person_id>
				<author_profile_id><![CDATA[81100566036]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Tomomasa]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sato]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[tomomasasato@jcom.home.ne.jp]]></email_address>
			</au>
			<au>
				<person_id>P3737505</person_id>
				<author_profile_id><![CDATA[81339518618]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Taketoshi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Mori]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[tmoriics-tky@umin.u-tokyo.ac.jp]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>2019354</ref_obj_id>
				<ref_obj_pid>2019342</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[David Bowen, "Tele-present wind," Leonardo, Vol. 44, No. 4, pp. 358--359, 2011.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 world eco-tope   Ayumi Kato1 Masahiko Watanabe1 Yusuke Fukazawa2 Tomomasa Sato3 Taketoshi Mori4 
Graduate School of Interdisciplinary Information Studies, The University of Tokyo Graduate School of 
Information Science and Technology, The University of Tokyo School of Engineering The University of Tokyo 
Graduate School of Information Science and Technology, The University of Tokyo Graduate School of Medicine 
The University of Tokyo  http://a3.sphotos.ak.fbcdn.net/hphotos-ak-snc7/374944_300623776639237_100000748521139_960746_1555217988_n.jpg 
  C:\Users\kato\Pictures\...\_DSC3117.JPG C:\Users\kato\Documents\....\....\1322460979512.jpg C:\Users\kato\Dropbox\ayumi_research\SIGGRAPH\..\LA_cap.PNG 
 1. Introduction1  1: {kato , masahiko} @ics.t.u - tokyo.ac.jp , 2: fukazawayuu@nttdocomo.co.jp 3: tomomasasato@jcom.home.ne.jp 
, 4: tmoriics-tky@umin.u-tokyo.ac.jp To feel the vicissitude of earth more intuitively, our biotope that 
represents the earth shows the weather of countries on the globe. There are no need to utilize expressions 
of letters, weather symbols, or voice of weather forecaster. You can see the weather of any locations 
including the ones you do not even know of, located at the remotest part of the world. And our biotope 
enables the experience of earth now and nature through the changing of weathers. We emphasis on intuitive 
to design world eco tope, which is direct touchable manipulation of designating the location by globe 
and comprehensive of the weather by biotope. In the following, we describe architecture and detailed 
explanation of each module. There are some related works that reproduce experience of remote place. For 
the representative of those past works, Tele-present wind reproduces the wind status of distant beach 
by moving plant. However, it only reproduces wind status of a specific place [1]. Fig.1 shows the outlook 
of our world eco-tope.  2. The world eco-tope system  World eco-tope is a system that enable viewer 
to specify the location of interest using globe, and acquire weather information of the specified location 
from the internet, then the weather is reproduced in the biotope by controlling the strength of rain 
and wind or the lightness of sun or moon. The system consists of following four modules(Fig.2). Input: 
Location designation module To make viewer designate the desired location to see the weather intuitively, 
we make use of the terrestrial globe. The globe is the analogue device but is easy for all the viewers 
including children and elderly to designate the location(Fig.3). We embed circle shaped variable resistor 
and line shaped one to the bottom and bow of the globe respectively. Line shaped and circle shaped one 
can measure the longitude and latitude respectively (Fig.4 and 5). Acquired longitude and latitude are 
input to the Arduino.        Input: Weather acquisition module This module acquires the weather 
at the designated location by using the Google Weather WebAPI. The API accepts the input of both longitude/latitude 
and city name. However, as the response coverage of longitude/latitude is small, we convert longitude/latitude 
to the closest city name, and use it as input to the API. There are 35 types of weather for the response. 
Output: Weather reproduction module This module reproduces the weather acquired in the biotope. The biotope 
has three devices that can reproduce rain, wind and light (sun or moon). We map each of 35 types of weather 
to the three parameters; strength of rain, wind, and lightness of the sun or moon. The example is shown 
in the table below. Rain Wind Lightness CLOUDY 0.0 0.3 0.3 THUNDERSTORMS 1.0 1.0 0.1 WINDY 0.0 0.95 
0.5  Rain device consists of shower head and pomp. Fig.6 shows the scene the rain is falling using the 
device. Wind device consists of motor and propeller. Light device consists of three colored LED, which 
we change the lightness and color according to the weather acquired from above mapping table and the 
time respectively. As for color, if the designated location is night time, the LED works as moon by coloring 
yellow, and if the designated location is day time, the LED works as sun. In addition to above output 
modules, we plant real plant in the content of biotope so as to reflect globe itself, the flow of the 
nature, which the plant glows and water circulate according to the change of weather.       Output: 
Related information display module This module displays related information such as Google street view 
and Flicker pictures taken at the designated location (Fig.7). This function helps viewer understand 
far distant country easily. 3. Conclusion  We realized World eco-tope that viewer can experience the 
feeling as if the viewer is in the remote country by reproducing the weather of the location of interest 
in the biotope. In future, we enlarge the size so that people enjoy more the world weather inside, and 
increase the number of output devices so as to reproduce the weather more precisely such as snow and 
fog module. References [1] David Bowen, Tele-present wind, Leonardo, Vol. 44, No. 4, pp. 358 359, 2011. 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	</section>
	<section>
		<section_id>2342996</section_id>
		<sort_key>1000</sort_key>
		<section_seq_no>16</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Modeling]]></section_title>
		<section_page_from>16</section_page_from>
	<article_rec>
		<article_id>2342997</article_id>
		<sort_key>1010</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>85</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[3D human head geometry estimation from a speech]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2342896.2342997</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2342997</url>
		<abstract>
			<par><![CDATA[<p>We can visualize acquaintances' appearance by just hearing their voice if we have met them in past few years. Thus, it would appear that some relationships exist in between voice and appearance. If 3D head geometry could be estimated from a voice, we can realize some applications (e.g, avatar generation, character modeling for video game, etc.). Previously, although many researchers have been reported about a relationship between acoustic features of a voice and its corresponding dynamical visual features including lip, tongue, and jaw movements or vocal articulation during a speech, however, there have been few reports about a relationship between acoustic features and static 3D head geometry. In this paper, we focus on estimating 3D head geometry from a voice. Acoustic features vary depending on a speech context and its intonation. Therefore we restrict a context to Japanese 5 vowels. Under this assumption, to estimate 3D head geometry, we use a Feedforward Neural Network (FNN) trained by using a correspondence between an individual acoustic features extracted from a Japanese vowel and 3D head geometry generated based on a 3D range scan. The performance of our method is shown by both closed and open tests. As a result, we found that 3D head geometry which is acoustically similar to an input voice could be estimated under the limited condition.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3737506</person_id>
				<author_profile_id><![CDATA[81319496663]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Akinobu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Maejima]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Waseda University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[akinobu@mlab.phys.waseda.ac.jp]]></email_address>
			</au>
			<au>
				<person_id>P3737507</person_id>
				<author_profile_id><![CDATA[81504685727]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Shigeo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Morishima]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Waseda University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[shigeo@waseda.jp]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Amberg, B., Romdhani, S., and Vetter, T. 2007. Optimal step nonrigid icp algorithms for surface registration. In <i>IEEE International Conference on Computer Vision and Pattern Recognition</i>, 1--8.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Reynolds, D. A., and Rose, R. C. 1995. Robust text-independent speaker identification using gaussian mixture speaker models. In <i>IEEE Trans. Acoust. Speech and Audio Processing</i>, vol. 3, 72--83.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 3D Human Head Geometry Estimation from a Speech Akinobu Maejima* Shigeo Morishima Waseda University 
Waseda University 1 Introduction We can visualize acquaintances appearance by just hearing their voice 
if we have met them in past few years. Thus, it would appear that some relationships exist in between 
voice and appearance. If 3D head geometry could be estimated from a voice, we can real­ize some applications 
(e.g, avatar generation, character modeling for video game, etc.). Previously, although many researchers 
have been reported about a relationship between acoustic features of a voice and its corresponding dynamical 
visual features including lip, tongue, and jaw movements or vocal articulation during a speech, however, 
there have been few reports about a relationship between acoustic features and static 3D head geometry. 
In this paper, we focus on estimating 3D head geometry from a voice. Acoustic fea­tures vary depending 
on a speech context and its intonation. There­fore we restrict a context to Japanese 5 vowels. Under 
this assump­tion, to estimate 3D head geometry, we use a Feedforward Neural Network (FNN) trained by 
using a correspondence between an in­dividual acoustic features extracted from a Japanese vowel and 3D 
head geometry generated based on a 3D range scan. The perfor­mance of our method is shown by both closed 
and open tests. As a result, we found that 3D head geometry which is acoustically simi­lar to an input 
voice could be estimated under the limited condition. 2 Feature Extraction We constructed a database 
which contains 70 individuals range scans (50 males/20 females) with neutral faces and 5 Japanese vowel 
speeches for each individual. For a geometric feature, 3D head models are semi-automatically generated 
from each range scan in the DB based on the Radial basis functions and the non-rigid ICP algorithm [Amberg 
et al. 2007] with a template head model consisted of 1621 vertices and 3174 triangle polygons. We then 
apply Principal Component Analysis (PCA) to 3D coordinates of all generated head models to reduce its 
dimension (from 1621 to 90) while holding 95% variance. Here, these principal components for a 3D head 
model are referred to as Geometric Feature (GF). For an acoustic feature, we extract 13 Mel-frequency 
Cepstral Co­ef.cients (MFCCs), their delta and delta-delta coef.cients from a vowel speech at 10 msec 
intervals. This is because these coef.­cients are widely utilized in speech recognition task [Reynolds 
and Rose 1995] and can represent speaker s characteristics. We then combined these coef.cients into a 
vector and refer to the 39 dimen­sional vector as an Acoustic Feature (AF). To reduce the dimension of 
AF, PCA is also applied to all subjects AFs for a vowel in the database while holding 95% variance. Finally, 
we can obtain 19 dimensional acoustic features for each individual at 10 msec inter­vals.  3 Mapping 
between 3D Head Geometry and Acoustic Features using Neural Network To represent a mapping from an AF 
to a GF, we use a FNN which has 1 input, 2 hidden and 1 output layers, 19, 180, 180 and 90 neu­rons with 
Sigmoid function in each layer. We set the hyper param­ *e-mail: akinobu@mlab.phys.waseda.ac.jp e-mail:shigeo@waseda.jp 
 Figure 1: The overview of 3D head geometry estimation Table 1: The estimation accuracy Closed test 
10HCV test SSVS test RMS (mm) 0.53 6.69 4.18  eter a and ß of the sigmoid function to 0.475 and 1.0 
respectively based on the 10-hold Cross validation test. To train a FNN for each vowel speech, we use 
pairs of AFs and GFs for all subjects s vowel speeches in the database. MFCCs represent acoustic characteristics 
of vocal tract (i.e the shape of mouth cavity). Thus, this mapping means the correspondence between a 
shape of mouth cavity and the 3D geometry of a head. 4 Results and Discussions To verify the performance, 
we performed Closed and 10-Hold Cross Validation test (10HCV) using 70 individuals GFs and AFs for Japanese 
vowel a . The Root Mean Squared error between ground truth and the estimate 3D geometries for each test 
is shown in Table 1. As for the open test, we also evaluate for Same Sub­jects Vowel Speeches recorded 
at the different timing from closed tests ones (SSVS). The estimated 3D geometries are rendered in the 
supplemental materials. These results suggest a possibility that we can estimate plausible 3D head geometry 
from a speech under the limited condition when same subjects speak same vowels. Also, we found that 3D 
head geometry which is acoustically similar to an input voice could be estimated. Currently, the geometry 
estimation is sometimes unstable due to the variation of individual acoustic features. We therefore need 
to look for more robust and text inde­pendent acoustic features representing an individual. As a future 
work, we plan to enlarge the database and to perform subjective evaluations to verify the performance 
of our method in more detail. References AMBERG, B., ROMDHANI, S., AND VETTER, T. 2007. Optimal step 
nonrigid icp algorithms for surface registration. In IEEE In­ternational Conference on Computer Vision 
and Pattern Recog­nition, 1 8. REYNOLDS, D. A., AND ROSE, R. C. 1995. Robust text-independent speaker 
identi.cation using gaussian mixture speaker models. In IEEE Trans. Acoust. Speech and Audio Pro­cessing, 
vol. 3, 72 83. Copyright is held by the author / owner(s). SIGGRAPH 2012, Los Angeles, California, August 
5 9, 2012. ISBN 978-1-4503-1435-0/12/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2342998</article_id>
		<sort_key>1020</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>86</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[A resolution reduction method for multi-resolution terrain maps]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2342896.2342998</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2342998</url>
		<abstract>
			<par><![CDATA[<p>Raster images such as raster terrain maps are commonly used in computer graphics. For rapid processing such as rendering and rapid feature extraction, rapid resolution reduction methods are required that keep the quality of huge images. This study deals with the resolution reduction methods.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[graphics data structures and data types]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.6</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P3737508</person_id>
				<author_profile_id><![CDATA[81504688229]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Goro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Akagi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Kobe University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737509</person_id>
				<author_profile_id><![CDATA[81430609625]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Koichi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Anada]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Waseda University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737510</person_id>
				<author_profile_id><![CDATA[81474704719]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Shinji]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Koka]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Nihon University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737511</person_id>
				<author_profile_id><![CDATA[81548031978]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Yasunori]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nakayama]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Nihon University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737512</person_id>
				<author_profile_id><![CDATA[81474695168]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Kenshi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nomaki]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Nihon University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737513</person_id>
				<author_profile_id><![CDATA[81100619059]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Takeo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yaku]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Nihon University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[yaku.takeo@nihon-u.ac.jp]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Akagi, G., Motohashi, T., Nomaki, K., Tsuchida, K., and Yaku, T., Octal Graph Representation for Multi-Resolution 3D Landform Maps, <i>SIAM Conf. GD</i> 05, <i>CP</i>14, Oct. 2005. http://meetings.siam.org/sess/dsp_talk.cfm?p=17773.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Finkel, R. A., and Bentley, J. L., Quad trees: A Data Structure for Retrieval on Composite Keys, <i>Acta Inf</i>. 4:1--9, 1974.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Kirishima, T., Motohashi, T., Tsuchida, K., and Yaku, T., Table Processing based on Attribute Graphs, <i>Proc. 6th IASTED Internat. Conf. Soft. Engin. &amp; Appl</i>., 317--322, 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Kozminsky, K., and Kinnen, E., Rectangular Duals of Planar Graphs, <i>Networks</i> 15, 145--157, 1985.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Shindo, Y., Anada, K., and Yaku, T., Representation of Geometric Objects with Octgrids, <i>Submitted</i>, 2012.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 A Resolution Reduction Method for Multi-resolution Terrain Maps Goro Akagi1 Koichi Anada2 Shinji Koka3 
Yasunori Nakayama3 Kenshi Nomaki3 Takeo Yaku3* 1Kobe University 2Waseda University 3Nihon University 
  Figure 1. A landform rectangular dissection (heights in meters)(left), its corresponding landform 
octgrid (center), and a relation between threshold values (height defferences) for cell unification (x-axis 
in meter) and cell numbers (y-axis) with the octgrid and quadtree methods for a terrain map of Riva del 
Garda in Italy (made from NASA SRTM-3). CR Categories: I.3.6 [Methodology and Techniques ] Keywords: 
Graphics data structures and data types 1 Introduction Raster images such as raster terrain maps are 
commonly used in computer graphics. For rapid processing such as rendering and rapid feature extraction, 
rapid resolution reduction methods are required that keep the quality of huge images. This study deals 
with the resolution reduction methods. Quadtree models are well-known [Finkel and Bentley, 1972] as data 
structures for multi-resolution images. But quadtree model representation abilities are insufficient 
and they have lack of flexibility. On the other hand, rectangular dual models [cf. Kozminsky and Kinnen, 
1985] can represent arbitrary multi-resolution raster terrain maps, but they require a large amount of 
computation time to reduce resolutions. In this study, we provide another resolution reduction method 
of huge raster maps for rapid processing, and show experimental results. 2 Algorithms Raster maps are 
represented by landform rectangular dissections as in Figure 1 (left). We modify an octgrid (e.g. [Kirishima 
et. al, 2002] and introduce an octal grid graph called a landform octgrid [Akagi et al., 2005] as shown 
in Figure 1 (right). In landform octgrids, nodes are linked by edges if their corresponding cells are 
nearest and have the ruled line in common. The degrees of the nodes are bounded by 8. We introduce an 
algorithm that unifies cells vx, vy in a landform octgrid GD with respect to the given threshold value 
l and provides the output landform octgrid GE. Algorithm LandformUnifyCell(GD, vx, vy, l, GE) Next, we 
introduce a resolution reduction algorithm for multi-resolution 3D terrain maps. We call the processing 
method based on the algorithm the landform octgrid method . Algorithm ResolutionReduction(GD, l, GE) 
This algorithm (1) unify nodes along with the Hilbert curve by LandformUnifyCell; then (2) unify nodes 
horizontally and vertically by LandformUnifyCell; * e-mail: yaku.takeo@nihon-u.ac.jp This method derives 
resolution reduction for black and white images without threshold values [Shindo et al., 2012]. 3 Experimental 
Results We compare the landform octgrids with rectangular duals and quadtrees. Property 1. The octgrid 
method provides 3D terrain maps with equivalently less cells to the rectangular dual method in O(N)×O(n+m) 
time, while the rectangular dual method provides ones in O(N)×O(N) time, where N is the number of cells 
and n and m are the number of rows and columns, respectively. Figure 1 (Right) shows the relation between 
the number of cells and the threshold values with respect to the quadtree method and to the landform 
octgrid method. Then we have Property 2. The octgrid methods possibly reduce by half the number of cells 
to the quadtree method. 4 Conclusion We introduced an octal grid graph model called landform octgrid 
for 3D terrain maps. And we proposed two algorithms to reduce their resolution. We compare landform octgrid 
with rectangular duals and quadtrees and claimed that landform octgrid has some advantages. We would 
like to thank to Prof. Kensei Tsuchida of Toyo University. References AKAGI, G., MOTOHASHI, T., NOMAKI, 
K., TSUCHIDA, K., AND YAKU, T., Octal Graph Representation for Multi-Resolution 3D Landform Maps, SIAM 
Conf. GD 05, CP14, Oct. 2005. http://meetings.siam.org/sess/dsp_talk.cfm?p=17773. FINKEL, R. A., AND 
BENTLEY, J. L., Quad trees: A Data Structure for Retrieval on Composite Keys, Acta Inf. 4:1-9, 1974. 
KIRISHIMA, T., MOTOHASHI, T., TSUCHIDA, K., AND YAKU, T., Table Processing based on Attribute Graphs, 
Proc. 6th IASTED Internat. Conf. Soft. Engin. &#38; Appl., 317-322, 2002. KOZMINSKY, K., AND KINNEN, 
E., Rectangular Duals of Planar Graphs, Networks 15, 145-157, 1985. SHINDO, Y., ANADA, K., AND YAKU, 
T., Representation of Geometric Objects with Octgrids, Submitted, 2012. Copyright is held by the author 
/ owner(s). SIGGRAPH 2012, Los Angeles, California, August 5 9, 2012. ISBN 978-1-4503-1435-0/12/0008 
  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2342999</article_id>
		<sort_key>1030</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>87</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Analysis and synthesis of realistic eye movement in face-to-face communication]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2342896.2342999</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2342999</url>
		<abstract>
			<par><![CDATA[<p>During face-to-face conversation, human eyes always rotate irregularly. Therefore, it is essential to synthesize such complex eye movements for the achievement of realistic human facial animation. In the conversation, there are two kinds of eye movements, Saccades and Fixation Eye Movements (FEMs). Saccade is a relatively large scale motion in eye movements compared with FEMs' smaller one. Gu et al [2007] mainly focused on saccades and suggested probability models based on their measurements. In their research, they treated FEMs as a part of saccades, even though saccades and FEMs are totally different kinds of eye movements. Thus their approximation is insufficient and brings unnatural appearance especially in conversations. Moreover, they avoided to provide an approximation of blinks' motions.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3737514</person_id>
				<author_profile_id><![CDATA[81504685722]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Tomoyori]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Iwao]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Waseda University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737515</person_id>
				<author_profile_id><![CDATA[81492655713]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Daisuke]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Mima]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Waseda University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737516</person_id>
				<author_profile_id><![CDATA[81331497177]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Hiroyuki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kubo]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Waseda University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737517</person_id>
				<author_profile_id><![CDATA[81319496663]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Akinobu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Maejima]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Waseda University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737518</person_id>
				<author_profile_id><![CDATA[81504685727]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Shigeo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Morishima]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Waseda University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[E.Gu, S. P. Lee, J. B. Badler, and N. I. Badler, "EyeMovementss, Saccades, and Multiparty conversations", Data-Driven 3D Facial animation, pp79--97, December 11, 2007]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Analysis and Synthesis of Realistic Eye Movement in Face-to-face Communication   Tomoyori Iwao*  
Daisuke Mima  Hiroyuki Kubo  Akinobu Maejima  Shigeo Morishima Waseda University 1. Introduction 
During face-to-face conversation, human eyes always rotate irregularly. Therefore, it is essential to 
synthesize such complex eye movements for the achievement of realistic human facial animation. In the 
conversation, there are two kinds of eye movements, Saccades and Fixation Eye Movements (FEMs). Saccade 
is a relatively large scale motion in eye movements compared with FEMs smaller one. Gu et al [2007] mainly 
focused on saccades and suggested probability models based on their measurements. In their research, 
they treated FEMs as a part of saccades, even though saccades and FEMs are totally different kinds of 
eye movements. Thus their approximation is insufficient and brings unnatural appearance especially in 
conversations. Moreover, they avoided to provide an approximation of blinks motions. We therefore aim 
to analyze saccades, FEMs and blinks efficiently and synthesize these movements using those probability 
models. First, we measure eye movements including FEMs and blinks in actual conversations. Second, we 
separate the acquired eye movements into two different movements, saccades and FEMs. Then, we approximate 
these eye movements and blinks using probability models respectively. Finally, based on the probability 
models, we synthesize cyclic eye movements between saccades and FEMs and add blinks to the resulting 
eye movements. As a result, we can automatically generate realistic eye movements in face-to-face communication. 
 2. Measurement of Eye movements and blinks To acquire reliable eye movements, it is necessary to control 
experimental environment strictly. We measured 24 subjects eye movements in various actual conversations 
using the NAC EMR-9 eye tracker and video camera. The eye tracker can measure eye movements by comparing 
the corneal reflection of the light source relative to the location of the pupil center. Primarily, we 
requested the subjects to be seated at 1.5 meters for having usual conversations between two subjects, 
one is speaker and the other is listener. Then we measured the eye movements and blinks of subjects during 
conversations. Additionally, to acquire subjects FEMs precisely, we put a gaze point on a wall at 1.0 
meters and asked subjects for focusing the point. In both two measurements, we requested the subjects 
to stop their head motions for obtaining eye s rotations without a neck motion. 3. Analysis and Synthesis 
 Eye movements in conversations include two different motions, saccades and FEMs. Initially, based on 
our measurements, we defined saccades as eye movements more than 2 degrees of rotation angle of eyeballs. 
Conversely, we defined FEMs as motions less than 2 degrees. Next, we averaged out results of eye movements 
measurement and approximated it using probability models with the combination of Exponential, Gaussian 
and Trigonometric Functions. As for saccades, first, we separate eye movements in conversations by speakers 
and listeners. Second, we approximated rotating angle (magnitude), direction angle, transient and continuous 
time by probability models. In regard to FEMs, as shown in Figure.1, we were able to observe that there 
were two types velocities; fast and slow. Therefore, we first divided FEM s velocities into fast and 
slow setting a threshold (0.05deg/frame). Then, we approximated these two types of velocities using probability 
models respectively. In addition, we approximated time interval of large peaks and continuous time using 
probability models. Finally as for blinks, we acquired the upper eyelid s moving velocity, then, approximated 
blinks time and interval of blinks using probability models.      Figure1. Variation of velocity 
of   Figure3. Result of synthesis FEMs per 1/60 second      Figure2. Rotating angle of speakers 
(left) and listeners (right) 4. Result s and Discussions The approximation of obtained eye s motion 
as a probability function is shown in figure 2, and figure 3 is an example of synthesized eye s motion 
according to the function. Figure2 indicates that large eye movements of speakers will occur more often 
than of listeners. In this way, our method can generate differences of eye movements about different 
situations, gender and so on. Moreover, our method can analyze eye movements associated with contents 
of conversations using voice data in video camera. More details of our probability-based eye s motions 
are described in supplemental materials. In our research, we separate eye movements into Saccades and 
FEMs, and propose probability functions of each kind of eye movements. Additionally, we also approximate 
the motion of blinks in conversations using probability models and the models to CG character. Besides, 
we analyze eye movements of speakers and listeners respectively. However, we analyze eye movements after 
averaging out 24 subjects eye movements , so we couldn t express individuality of eye movements. More 
detailed Analysis will further increase the realism of eye movements. References E.Gu,S.P.Lee,J.B.Badler,andN.I.Badler, 
EyeMovementss,Saccades,and Multiparty conversations , Data-Driven 3D Facial animation,pp79-97,December 
11, 2007 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2343000</article_id>
		<sort_key>1040</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>88</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[Base mesh construction using global parametrization]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2342896.2343000</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2343000</url>
		<abstract>
			<par><![CDATA[<p>Base mesh construction from a dense-polygon mesh is often used to reduce the complexity of geometry processing problems. In the base or control mesh, each face corresponds to a region on the original surface and is used to encode its geometry. This encoding can involve a different representation of the surface, e.g. using displacement field and subdivision surfaces [Lee et al. 2000], or can be a more direct representation, e.g. through charts [Sander et al. 2003]. In the former example, the control mesh is constructed using edge-collapse simplification, and in the latter by an iterative seed-placement and chart-growth optimization process. Although both methods strive to optimize this construction, the first implies a sequence of local operations, lacking a global strategy, and the second iterates over greedy choices, which may not converge to a global solution.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3737519</person_id>
				<author_profile_id><![CDATA[81496689041]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Francisco]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ganacim]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[VISGRAF Lab, IMPA, Rio de Janeiro, Brazil]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737520</person_id>
				<author_profile_id><![CDATA[81421597426]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Andr&#233;]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Maximo]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[VISGRAF Lab, IMPA, Rio de Janeiro, Brazil]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737521</person_id>
				<author_profile_id><![CDATA[81442611891]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Luiz]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Velho]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[VISGRAF Lab, IMPA, Rio de Janeiro, Brazil]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>344829</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Lee, A., Moreton, H., and Hoppe, H. 2000. Displaced Subdivision Surfaces. In <i>Proceedings of ACM SIGGRAPH '00</i>, 85--94.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>882390</ref_obj_id>
				<ref_obj_pid>882370</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Sander, P. V., Wood, Z. J., Gortler, S. J., Snyder, J., and Hoppe, H. 2003. Multi-Chart Geometry Images. In <i>Proceedings of Eurographics/ACM SIGGRAPH SGP '03</i>, 146--155.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Base Mesh Construction using Global Parametrization FranciscoGanacim Andr´eMaximo LuizVelho VISGRAFLab,IMPA,RiodeJaneiro,Brazil 
 (a) (b) (c) (d) (e) Figure 1: The original mesh (a) is cut using seams (b) containing cone singularities 
(purple points); the mesh is parametrized (c) with connected boundaries (e.g. red and green regions); 
the base mesh (e) is generated using the metric distortion on the parametrization (d). Introduction and 
Motivation Base mesh construction from a dense-polygon mesh is often used to reduce the complexity of 
ge­ometryprocessingproblems.In thebaseorcontrol mesh,eachface corresponds to a region on the original 
surface and is used to en­code itsgeometry. Thisencoding caninvolveadifferentrepresen­tation of the surface, 
e.g. using displacement .eld and subdivision surfaces[Lee et al.2000], or canbe a moredirect representation, 
e.g. through charts[Sanderet al.2003].In theformerexample, the control mesh is constructed using edge-collapse 
simpli.cation, and in the latter by an iterative seed-placement and chart-growth opti­mization process. 
Although both methods strive to optimize this construction, the .rst implies a sequence of local operations, 
lack­ing a global strategy, and the second iterates over greedy choices, which may not converge toaglobal 
solution. Inthiswork,wepresent analgorithm toconstructbasemeshesthat enablesaglobal analysisof theproblem.First,wecomputeanon­optimalglobalparametrizationof 
theoriginal mesh(seeFigure 1), allowing distortions and using seams and cone singularities. Then, we 
map the boundaries of the parametrization pairwise, excluding theconepointsandpossibleboundariesof theoriginal 
surface.Fi­nally, we place a few vertices on top of the global parametrization and computea2DDelaunay 
triangulation,yielding ourbasemesh. Theanalysisof where toplacethebasemesh verticesisdoneonce, without 
iterations,and withglobalknowledgeofthesurface. Our Approach Global parametrization is often done as 
the .nal goal e.g. in texture mapping not as the initial step to build an auxiliary data structure. 
In order to construct our base mesh, we initiallycomputeaglobalparametrizationofthesurface tothen tri­angulatetheparametrizedplanardomain 
toobtainabasemesh and associated atlas structure. The method is as follows: .rst, we cut thesurfaceopen 
intoasimplyconnecteddomainwithboundaryby carefullyplacing seams into themesh(cf.Figure 1(b)). Next,we 
use a modi.ed angle-based .attening algorithm with boundary re­strictions,wheremesh verticeslying onboundary 
seamsare treated as internal verticesexceptforafewverticesmarked asconesingu­larities. Note that all 
vertices that are not located at cone singular­ities must respect the planarity constraint. As a consequence, 
the total curvatureof thesurface isdistributed at theconepoints. Whileperforming theseam-cutstep,wecreatenewverticeson 
the seampointing to itsoriginal vertex.Thissimplepointerstrategy is used to identify vertices on the 
parametrization that are the same on the original mesh. The edges on seams are also identi.ed by the 
pointers on its vertices. An important remark is that although vertices can be replicated several times, 
e.g. in seams bifurcations, theedgesaremappedpairwiseon theparametrizationborder(cf. Figure 1(c)). This 
mapping is only possible because of a special subset of unconnected vertices on the seams called cone 
singular­ities, that is, vertices allowed to have angle distortion. Although these vertices do not respect 
the planarity (or angle-consistency) constraint, the edges around them respect the length-consistency 
constraint and, thus, thescale ispreserved acrossboundaries. The last step of our algorithm is to create 
and place a small num­berofbaseverticesovertheglobalparametrizationand triangulate them to construct 
a new coarser mesh. We compute the length dis­tortion ratiofor all edges andplace the vertices on the 
least stretched areas(cf.Figure 1(d)).Theangulardistortionisusedtoavoidplac­ingbaseverticeson top of(ornear 
to) conesingularities.The trian­gulation is a Delaunay triangulation on the 2D parameter domain and uses 
the correspondence across boundaries stored on edges. The .nal base mesh is obtained by getting the base 
vertices from theparametrizationback to3D maintainingthe triangulation. Future Directions Our approach 
creates only base meshes, but it has the potential to create meshes at any resolution. An interest­ing 
future direction is to investigate how these meshes interact and de.neasetofproductionrules todescribemulti-resolutionmeshes. 
Acknowledgements Weacknowledgethegrantprovidedby the BrazilianagencyCNPq and theAIM@SHAPE repository. 
References LEE, A., MORETON, H., AND HOPPE, H. 2000. DisplacedSubdi­visionSurfaces. In Proceedings of 
ACM SIGGRAPH 00,85 94. SANDER, P. V., WOOD, Z. J., GORTLER, S. J., SNYDER, J., AND HOPPE, H. 2003. Multi-Chart 
Geometry Images. In Proceed­ings of Eurographics/ACM SIGGRAPH SGP 03,146 155. Copyright is held by the 
author / owner(s). SIGGRAPH 2012, Los Angeles, California, August 5 9, 2012. ISBN 978-1-4503-1435-0/12/0008 
  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2343001</article_id>
		<sort_key>1050</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>89</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>5</seq_no>
		<title><![CDATA[De-aging high-resolution 3D facial models by example-driven mesh deformation]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2342896.2343001</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2343001</url>
		<abstract>
			<par><![CDATA[<p>Human face conveys significant characteristics of a person such as age traits and facial attractiveness. The absolute aesthetic value for human faces is still unclear; but meanwhile, facial attractiveness has been considered as a cross-culture criteria. By applying de-aging procedure on 3D model geometry can be used to enhance facial attractiveness due to improved shape analysis. We conclude our observation on aging progress into four rules: 1. drooping of eyelids, 2. lack of elasticity cause hollow cheeks, 3. slack on middle face and 4. wrinkles. In this paper we present a framework to beautify (enhance facial attractiveness) realistic facial model by mesh deformation method based on data-driven approach. For an elder input face, we extract the edges connected among feature points on the face mesh. The edge lengths is used as a feature vector, and an age score is associated with it. We search for local minimum to obtain a related young vector and consequently maintain the similarity between the elder and young face. Once the young length vector has been determined, we can embed it to our new feature locations and then deform the model to a younger shape. The effectiveness of our work is to rejuvenate elder face by altering determined the outline of a facial model.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3737522</person_id>
				<author_profile_id><![CDATA[81492649582]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Hong-Shang]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[National Taiwan University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737523</person_id>
				<author_profile_id><![CDATA[81504682930]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Shih-Yen]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hwang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[National Taiwan University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737524</person_id>
				<author_profile_id><![CDATA[81442602940]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Che-Hua]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yeh]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[National Taiwan University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737525</person_id>
				<author_profile_id><![CDATA[81100319756]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Ming]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ouhyoung]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[National Taiwan University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1360637</ref_obj_id>
				<ref_obj_pid>1399504</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Leyvand, T., Cohen-Or, D., Dror, G., and Lischinski, D. 2008. Data-driven enhancement of facial attractiveness. In <i>ACM SIGGRAPH 2008 papers</i>, 38:1--38:9.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1409074</ref_obj_id>
				<ref_obj_pid>1457515</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Ma, W.-C., Jones, A., Chiang, J.-Y., Hawkins, T., Frederiksen, S., Peers, P., Vukovic, M., Ouhyoung, M., and Debevec, P. 2008. Facial performance synthesis using deformation-driven polynomial displacement maps. In <i>ACM SIGGRAPH Asia 2008 papers</i>, 121:1--121:10.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1632784</ref_obj_id>
				<ref_obj_pid>1632701</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Paysan, P., Knothe, R., Amberg, B., Romdhani, S., and Vetter, T. 2009. A 3d face model for pose and illumination invariant face recognition. In <i>Proceedings of AVSS 2009</i>, 296--301.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 De-aging High-Resolution 3D Facial Models by Example-Driven Mesh Deformation Hong-Shang Lin, Shih-Yen 
Hwang, Che-Hua Yeh, and Ming Ouhyoung National Taiwan University  (a) (b) (c) (d) (e) Figure 1: The 
proposed approach for de-aging: (a) The input high-resolution 3D face model (b) 70 Farkas feature points 
(red points) and 434 edges (blue lines) generated by Delaunay triangulation (c) New locations (blue points) 
for the 70 Farkas feature points after optimization (red points for original location) (d) The result 
after deformation and texture smoothness (e) Final results after applying vertex averaging. 1 Introduction 
Human face conveys signi.cant characteristics of a person such as age traits and facial attractiveness. 
The absolute aesthetic value for human faces is still unclear; but meanwhile, facial attractiveness has 
been considered as a cross-culture criteria. By applying de-aging procedure on 3D model geometry can 
be used to enhance facial at­tractiveness due to improved shape analysis. We conclude our ob­servation 
on aging progress into four rules: 1. drooping of eyelids, 2. lack of elasticity cause hollow cheeks, 
3. slack on middle face and 4. wrinkles. In this paper we present a framework to beautify (enhance facial 
attractiveness) realistic facial model by mesh defor­mation method based on data-driven approach. For 
an elder input face, we extract the edges connected among feature points on the face mesh. The edge lengths 
is used as a feature vector, and an age score is associated with it. We search for local minimum to obtain 
a related young vector and consequently maintain the similarity be­tween the elder and young face. Once 
the young length vector has been determined, we can embed it to our new feature locations and then deform 
the model to a younger shape. The effectiveness of our work is to rejuvenate elder face by altering determined 
the outline of a facial model. 2 Our approach We extend the framework proposed in [Leyvand et al. 2008] 
to beauti.cation and de-aging for 3D face models. The .rst step is to build the de-aging editor. We adopt 
the Basel Face Model [Paysan et al. 2009] to generate the training dataset, and 200 face models are generated 
with uniform distribution in the age attribute. Each model is labelled with 70 feature points according 
to Farkas fea­ture points, and then 434 edges are connected between these fea­ture points through Delaunay 
triangulation. Figure 1(b) illustrates the labelled Farkas feature points and the edges. The 434 edge 
lengths are further reduced to 35 by the principal component anal­ysis (PCA), hence each 3D face model 
can be represented with a 35-dimensional feature vector. To train the regression model, a list of labels 
or scores are required, and we normalize the age attribute of the 200 models to ranks 1 to 5 in a discrete 
manner. Finally, the 35-dimensional feature vectors and their corresponding ranks are used as training 
samples to construct a support vector regression (SVR) model. Let v denote the feature vector extracted 
from an input 3D face 1 model. To de-age the model, we have to generate a new vector v 1 where Fage(v) 
>Fage(v). The function Fage is the regression 1 model obtained from the previous step. vcan be derived 
by mini­ 1 mizing the function -Fage(v) with standard no-derivatives Direc­tion Set Method. Then Levenberg-Marquardt 
algorithm is utilized to search the new location of the 70 Farkas feature points based on 1 the derived 
v. Figure 1(c) shows the original location (red points) and the new location (blue points). Finally, 
we apply thin plate spline warping to all vertices in order to warp the 70 Farkas feature points to new 
locations. As a result, we apply smoothing to the face texture map and apply vertex averaging to the 
face mesh. 3 Results Figure 1(a) shows an input example, which is captured through the lightstage system 
[Ma et al. 2008]. Figure 1(e) is the .nal results processed by the proposed approach. References LEYVAND, 
T., COHEN-OR, D., DROR, G., AND LISCHINSKI, D. 2008. Data-driven enhancement of facial attractiveness. 
In ACM SIGGRAPH 2008 papers, 38:1 38:9. MA, W.-C., JONES, A., CHIANG, J.-Y., HAWKINS, T., FRED-ERIKSEN, 
S., PEERS, P., VUKOVIC, M., OUHYOUNG, M., AND DEBEVEC, P. 2008. Facial performance synthesis us­ing deformation-driven 
polynomial displacement maps. In ACM SIGGRAPH Asia 2008 papers, 121:1 121:10. PAYSAN, P., KNOTHE, R., 
AMBERG, B., ROMDHANI, S., AND VETTER, T. 2009. A 3d face model for pose and illumination invariant face 
recognition. In Proceedings of AVSS 2009, 296 301. Copyright is held by the author / owner(s). SIGGRAPH 
2012, Los Angeles, California, August 5 9, 2012. ISBN 978-1-4503-1435-0/12/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2343002</article_id>
		<sort_key>1060</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>90</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>6</seq_no>
		<title><![CDATA[Facial aging simulator considering geometry and patch-tiled texture]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2342896.2343002</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2343002</url>
		<abstract>
			<par><![CDATA[<p>People can estimate an approximate age of others by looking at their faces. This is because faces have certain elements by which people can judge a person's age. If computers can extract and manipulate such information, wide variety of applications for entertainment and security purpose would be expected.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3737526</person_id>
				<author_profile_id><![CDATA[81504687524]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Yusuke]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tazoe]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Waseda University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[the-brave-w@ruri.waseda.jp]]></email_address>
			</au>
			<au>
				<person_id>P3737527</person_id>
				<author_profile_id><![CDATA[81466647938]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Hiroaki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gohara]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Waseda University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737528</person_id>
				<author_profile_id><![CDATA[81319496663]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Akinobu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Maejima]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Waseda University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737529</person_id>
				<author_profile_id><![CDATA[81504685727]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Shigeo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Morishima]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Waseda University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[K. Scherbaum et al, "Prediction of Individual Non-Linear Aging Trajectories of Faces" <i>Proc. Ann. Conf. European Assoc. Computer Graphics</i>, Vol. 26, No. 3, pp. 285--294, 2007]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1531363</ref_obj_id>
				<ref_obj_pid>1531326</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[U.Mohammed et al, "Visio-lization: Generating Novel Facial Images" <i>ACM Transactions on Graphics (Proceedings SIGGRAPH 2009)</i>, 28(3), No. 4, pp. 57(1)--57(8), 2009]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Facial aging simulator considering geometry and patch-tiled texture Yusuke TAZOE* Hiroaki GOHARA Akinobu 
MAEJIMA Waseda University Waseda University Waseda University (a) Aging Face Model (male) (b) Aging Face 
Model (female) Figure 1: Synthesized Aging Face Models (a), (b). 1 Introduction People can estimate 
an approximate age of others by looking attheir faces. This is because faces have certain elements by 
whichpeople can judge a person s age. If computers can extract and ma­nipulate such information, wide 
variety of applications for enter­tainment and security purpose would be expected. Recently, considerable 
amount of research has been conducted toquantitatively describe the changes in facial aging. Scherbaum 
etal. [2004] have developed a method for synthesizing 3D agingfaces based on 3D Morphable Model (3DMM). 
They modeled anindividual aging trajectory on the model parameter domain for agiven face using non-linear 
Support Vector Regression. However,it is dif.cult to represent wrinkles and .ecks on an aged face be­cause 
the proper alignment between textures used for constructingthe 3DMM is needed. In this paper, we propose 
a method to synthesize face aging or re­juvenation based on geometry modi.cation and patch-tiling basedtexture 
synthesis. The advantage of our method is to describe de­tailed aging skin properties (.ne wrinkles and 
.ecks) on a synthe­sized face while still preserving individuality, because our methodhas no texture 
blending such as 3DMM in texture synthesis. More­over, unlike Scherbaum s approach, our method represents 
a geo­metrical aging effect by scaling an accurate 3D range scan-basedface model according to the ratio 
of each facial organ s scale. Theexperimental result as shown in Figure 1 represents the effective­ness 
of our method for face aging and rejuvenation.  2 Geometry aging First of all, we constructed an age-speci.c 
3D face database whichconsists of 3D face models (239 males/218 females) with their agelabels. As preprocessing, 
for all 3D face models in the database, wecalculate width and height of each facial organ including eyes, 
eyebrows, nose, mouth and facial contour using pre-de.ned landmarkvertices on the 3D face models. We 
then calculate linear regressionswhich can map an age to width or height for each facial organ.Likewise, 
we also calculated linear regressions which can map anage to a depth value for each pre-de.ned landmark 
vertex on facial organs. At this time, we separated the database into two classes;under 18 and more than 
18, because the geometrical changes shiftwith a change in cranial bone growing at this age. Given an 
individual 3D face model with actual age, geometry ag­ing is performed by following procedures. First, 
the width of anindividual 3D face model is calculated by similar way as the pre­processing. Second, the 
width corresponding to the actual and thedesired age is estimated by linear regressions, then the x-scaling 
ra­tio between the estimated and the actual age s width is calculated *e-mail: the-brave-w@ruri.waseda.jp 
 for each facial organ. At this time, y and z scaling ratio are also cal­culated by same manner as mentioned 
above. Finally, each vertexof the input face model is scaled according to each facial organ sscaling 
ratio. (a) generate patch (b) patch database Figure 2: Texture aging method 3 Texture aging Texture 
aging is performed by the tile-based texture synthesis [Mo­hammed et al. 2009] as following procedures. 
As a preprocessing,all faces of the same age category in the database are geometricallyaligned along 
the average face of the age and then normalize tex­tures are generated. Then, age-speci.c texture patch 
database isconstructed from normalized textures for each age group. Given aninput texture, a base texture 
is .rstly generated by mapping the in­put face texture to the average face model of the desired age. 
Next,the base texture is converted from the RGB to the HSV color sys­tem then divided into patches (Figure 
2). After that, utilizing thebase texture as a global constraint to conserve the relative locationof 
face parts, texture patches of the desired age are placed onto thebase texture by same manner of the 
tile-based texture synthesis. Asa result, the input texture is transformed to the desired age s texture. 
 4 Results And Conclusion Finally, face aging or rejuvenation can be performed by integratingboth results 
of geometry and texture aging. Figure1-(a) and (b)show the results of face aging and rejuvenation in 
male and femalecase. From this result, we found that our method can represent thevariation of the skin 
texture such as wrinkles due to the aging. Thus,we conclude that the proposed method is effective for 
simulatingface aging and rejuvenation. As a future work, we plan to modelthe individual aging changes. 
 References K. Scherbaum et al, Prediction of Individual Non-Linear AgingTrajectories of Faces Proc. 
Ann. Conf. European Assoc. Com­puter Graphics, Vol. 26, No. 3, pp. 285-294, 2007 U.Mohammed et al, Visio-lization: 
Generating Novel Fa­cial Images ACM Transactions on Graphics (Proceedings SIG-GRAPH 2009) , 28(3), No. 
4, pp. 57(1)-57(8), 2009 Copyright is held by the author / owner(s). SIGGRAPH 2012, Los Angeles, California, 
August 5 9, 2012. ISBN 978-1-4503-1435-0/12/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2343003</article_id>
		<sort_key>1070</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>91</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>7</seq_no>
		<title><![CDATA[Fast-automatic 3D face generation using a single video camera]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2342896.2343003</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2343003</url>
		<abstract>
			<par><![CDATA[<p>Reconstructing a 3D face model only with a single camera without attaching landmarks and pattern projecting on a face is still a challenging task in computer vision and computer graphics. FaceGen [2011] is able to reconstruct 3D shape and texture from 2D single image based on 3D morphable model. Since the users have to determine of the facial feature points manually, the method does not work automatically. Moreover, it requires a couple of minutes to generate a 3D face model. Thus, the computational cost is not enough cheap for practical usage.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3737530</person_id>
				<author_profile_id><![CDATA[81492649451]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Tomoya]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hara]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Waseda University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[wap.0921@akane.waseda.jp]]></email_address>
			</au>
			<au>
				<person_id>P3737531</person_id>
				<author_profile_id><![CDATA[81331497177]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Hiroyuki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kubo]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Waseda University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737532</person_id>
				<author_profile_id><![CDATA[81319496663]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Akinobu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Maejima]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Waseda University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737533</person_id>
				<author_profile_id><![CDATA[81504685727]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Shigeo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Morishima]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Waseda University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[shigeo@waseda.jp]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1905411</ref_obj_id>
				<ref_obj_pid>1904935</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Choi, j., et.al.. 2010. 3D Face Reconstruction Using A Single or Multiple Views, <i>IEEE International Conference on Pattern Recognition 2010, pp.3959--3962</i>
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Facegen. 2011, http://www.facegen.com]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2073323</ref_obj_id>
				<ref_obj_pid>2073304</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Hara, t., et.al.. 2011. Automatic 3D Face Generation from Video with Sparse Point Constraint and Dense Deformable Model, <i>ACM SIGGRAPH ASIA 2011, Poster, no.18</i>
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Irie, a., et.al., 2011. Improvements to Facial Contour Detection by Hierarchical Fitting and Regression, <i>The First Asian Conference on Pattern Recognition, pp.273--277</i>
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Maejima, a., and Morishima, s. 2008. Fast Plausible 3D Face Generation from a Single Photograph, <i>ACM SIGGRAPH ASIA 2008, Poster</i>, maejima.pdf]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Fast-Automatic 3D Face Generation Using a Single Video Camera C:\Users\tomoPRC\Documents\Dropbox\Works\......\..\_s...9.png 
 Tomoya Hara*   Hiroyuki Kubo Akinobu Maejima   Shigeo Morishima Waseda University  1 Introduction 
Reconstructing a 3D face model only with a single camera without attaching landmarks and pattern projecting 
on a face is still a challenging task in computer vision and computer graphics. FaceGen [2011] is able 
to reconstruct 3D shape and texture from 2D single image based on 3D morphable model. Since the users 
have to determine of the facial feature points manually, the method does not work automatically. Moreover, 
it requires a couple of minutes to generate a 3D face model. Thus, the computational cost is not enough 
cheap for practical usage. Choi et al. [2010] proposed a hybrid approach that combines Structure-from-Motion 
(SfM) and generic-model deformation to estimate face geometry efficiently. This method can automatically 
reconstruct a 3D face model from multi-view images. They compute a 3D face model along with the modification 
of generic face, thus, the reconstructed face does not reflect individual shape especially the shape 
of nose and unevenness of the cheeks. In this paper, we propose a fast-automatic 3D face reconstruction 
method based on Structure-from-Motion and Deformable Face Model (DFM). Considering both 2D frontal face 
image constraint, 3D geometric constraint, and likelihood constraint, we are able to reconstruct subject 
s face model accurately, robustly, and automatically. Using our method, it is possible to create a 3D 
face model in 5.8 [sec] by only shaking own head freely in front of a single video camera. 2 System 
Overview Figure 1 shows the overview of our proposal method which is composed of 6 steps as follows. 
(a) Image Sequence Acquisition: We capture an image sequence in which a subject rotates the head in front 
of a camera freely and gradually. Note that, unlike Choi s method which allows only horizontal movement, 
ours does not constrain the subject how to rotate own head. Therefore we can obtain information of the 
subject s face observed from multi-view directions. (b) Feature Detection: We detect 87 feature points 
from the frontal face image in the sequence used for 2D constraint. We also detect 28 feature points 
from all frames in the sequence used for 3D constraint based on the method proposed by Irie et al. [2011]. 
(c) Factorization: We estimate 3D position of 28 feature points transition acquired from an image sequence 
using SfM algorithm. (d) Deformable Face Model: The generic 3D face model cannot represent individual 
facial parts geometry. Therefore, we construct the Deformable Face Model(DFM) by calculating Principal 
Components(PCs ) for vertices of each face model in the database which includes 1153 male/female, young/elderly 
3D face models. The DFM is able to generate a variety of 3D facial geometry by controlling each magnitude 
of Principal Component. p (e) Optimal Deformation: We formulate the cost function as equation (1) for 
3D facial shape estimation. This function is defined as a summation of the 2D-fitness term, the 3D-fitness 
term, and the likelihood term (the details are stated in supplemental materials). )(pE  (1)  * e-mail: 
wap.0921@akane.waseda.jp e-mail: shigeo@waseda.jp     Figure 1: Overview of Our Method  The 2D-fitness 
term represents the sum of the Euclid distance between the vertex of the frontal facial feature point 
and its corresponding vertex of the DFM. This constraint controls the shape of the face contour and organs 
appropriately. The 3D-fitness term represents the sum of the Euclid distance between the vertex of the 
estimated point by SfM and its corresponding vertex of the DFM. This constraint reflects depth characteristics 
such as height of nose. The likelihood term computes the face likelihood of currently estimated based 
on the method proposed by Maejima et al. [2008]. This constraint maintains a human-like face structure 
not to generate an unnatural face. And the coefficients decide contributions for each term. We solve 
the optimal PCs from this function by BFGS minimization method. ...,, (f) 3D Face Generation: We can 
acquire a final 3D face model by deforming the DFM based on estimated optimal , and mapping the texture 
of frontal face image to the deformed DFM. 3 Experimental Result and Conclusion For our experiments, 
we used an Intel Xeon E3-1245(3.30GHz) machine and an image sequence which are acquired with a single 
consumer video camera which can record with 29.97 fps and resolution of 600*600 pixels. Experimental 
results show that our method can generate a 3D face model with 2.1 [mm] estimation error in 5.8 [sec] 
after capturing an image sequence. In consequence, we can fast-automatic generate a 3D face model, and 
succeed to represent more plausible facial geometry even compared with our previous poster [Hara et al. 
2011] by re-formulating the cost function (the comparison results are shown in supplemental materials). 
As a future work, it is necessary to extend our database by collecting more various types of face models 
in order to generate 3D faces with variation of ethnicity. References CHOI, J., et.al.. 2010. 3D Face 
Reconstruction Using A Single or Multiple Views, IEEE International Conference on Pattern Recognition 
2010, pp.3959-3962 FACEGEN. 2011, http://www.facegen.com HARA, T., et.al.. 2011. Automatic 3D Face Generation 
from Video with Sparse Point Constraint and Dense Deformable Model, ACM SIGGRAPH ASIA 2011, Poster, no.18 
IRIE, A., et.al., 2011. Improvements to Facial Contour Detection by Hierarchical Fitting and Regression, 
The First Asian Conference on Pattern Recognition, pp.273-277 MAEJIMA, A., and MORISHIMA, S. 2008. Fast 
Plausible 3D Face Generation from a Single Photograph, ACM SIGGRAPH ASIA 2008, Poster, maejima.pdf 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2343004</article_id>
		<sort_key>1080</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>92</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>8</seq_no>
		<title><![CDATA[How to draw illustrative figures?]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2342896.2343004</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2343004</url>
		<abstract>
			<par><![CDATA[<p>Simplified illustrations are effective for medical purposes in ways: they can quickly convey stories to patients, and avoid unnecessary graphic depiction. Despite their simplicity, such illustrations are not trivial to improvise. In this talk, we propose a new system that automatically transforms medical images into friendly 2D illustrations via an optimization problem.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3737534</person_id>
				<author_profile_id><![CDATA[81504685385]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Yuki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Morimoto]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[ISIT]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[yukim128@gmail.com]]></email_address>
			</au>
			<au>
				<person_id>P3737535</person_id>
				<author_profile_id><![CDATA[81504685628]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Daisaku]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Arita]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[ISIT]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Eigensatz, M., Sumner, R. W., and Pauly, M. 2008. Curvature-domain shape processing. <i>Comput. Graph. Forum 27</i>, 2, 241--250.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2024167</ref_obj_id>
				<ref_obj_pid>2070781</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Fu, H., Zhou, S., Liu, L., and Mitra, N. J. 2011. Animated construction of line drawings. <i>ACM Trans. Graph. 30</i>, 6, 133.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Millet, L. J., Stewart, M. E., Sweedler, J. V., Nuzzo, R. G., and Gillette, M. U. 2007. Microfluidic devices for culturing primary mammalian neurons at low densities. <i>Lab Chip 7</i>, 8, 987--994.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Ohtake, Y., Belyaev, A. G., and Seidel, H.-P. 2002. Mesh smoothing by adaptive and anisotropic gaussian filter applied to mesh normals. In <i>VMV</i>, 203--210.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 How to Draw Illustrative Figures? YukiMorimoto* DaisakuArita ISIT ISIT  Figure 1: (a) Input image [Millet 
et al. 07]. (b) Contours extracted from input image. (c) Constrained Delaunay triangulation and detection 
of collinearity (red). (d) Our result. (e) Comparison of collinearity, roundness and sharpness (yellow). 
(f) Comparison of parallelism. 1 Introduction Simpli.ed illustrations are effective for medical purposes 
in ways: they can quickly convey stories to patients, and avoid unnecessary graphic depiction. Despite 
their simplicity, such illustrations are not trivial to improvise. In this talk, we propose a new system 
that automatically transforms medical images into friendly 2D illustra­tions via an optimizationproblem. 
We observe thatmanyillustrations are simpli.ed or exaggerated: they have smooth lines with partial parallelism, 
collinearity, and similarity. Taking account these principles of design, we partic­ularly focus on preserving 
parallelism, collinearity, and similarity of contours, when performing noise removal. Our optimization 
.nds the contours that best match the suggested design principles by solving a nonlinear least-squares 
problem. As shown in Figure 1,ourmethodisabletoenhance somegeometricfeatures. 2 Our Method Feature-Preserving 
Weighted Smoothing. We resample contour lines in such a way that the points are equally spaced on the 
origi­nal contour lines preserving sharp points. We perform constrained Delaunay triangulation over the 
contour lines for the discretization andthedetection of collinearpointindices, e.g. i,i+1,j,j+1by Fu 
s collinearity criteria [Fu et al. 2011]. Then we perform .lter­ingoperations overthe curvaturesde.nedin 
[Eigensatz et al.2008] anddistances at eachpoint. Wede.ne thedistance as an average of lengthsassociated 
withDelaunay edges exceptfortheneighboring points of contours. Our .ltering operations diffuse the curvature 
and the distance value by weighting [Ohtake et al. 2002]. This results in stair-like approximation: the 
operation enhances the ge­ometric features while removing noises and clustering similar fea­tures. Optimization. 
Wede.ne smoothing energy Esm =(.- . ' )ds, .O distance energy Ed =(d - d ' )ds, collinearity energy Ec 
= .O. O(.c - .c' )ds, similarity energy Esi =(.u - .u' )ds, and re­ O maining energies such as position 
and points intervals of contours Ep,Ein de.nedin the method ofEigensatz et al.[2008], where the contours 
O . IR1 , ds denotes a line element, . denotes the cur­vature of contourpoints, .c denotes the curvature 
calculatedby us­ing collinearpoint index, .u denotes the unsigned curvature value, and primes denote 
the .ltered instances through the con.gurations *e-mail: yukim128@gmail.com of these parameters, respectively. 
The total energy is de.ned by E = ksmEsm +kdEd +kcEc +ksiEsi +kpEp +kinEin, where the parameters ksm,kd,kc,ksi,kp,kin 
are the coef.cient that are controlled by user s preferences. We employed the Levenburg-Marqurdtalgorithmto 
minimizethe energy E (See [Eigensatz etal. 2008]fordetails). 3 Results and Conclusion We show an example 
results in Figure 1 generated by solving our minimizationproblem. We usedOpenCV2.1 to extract the contour 
lines from a photograph. From the comparison of the red colored edge angles in Figure 1, we are able 
to see that the collinearity optimization makes these more collinear. The signed curvature op­timization 
removes noise from the contours while enhancing the sharpness and roundness. The distance optimization 
also improves parallelism. Oneof thedrawbacks of ourmethodisthat .nding theparame­ters for collinearity 
detection, smoothing, and optimization require careful adjustments. In our experiments, wefound that 
collinearity detection is too sensitive because the applied criterion is not spe­ci.c for our purpose. 
Smoothing parameters are also sensitive to the detection of a feature point which is sharp, parallel, 
or simi­lar in curvature, distance, or unsigned curvature optimization. We expect to semi-automatically 
assign these parameters by evaluat­ing weighted smoothing or by .nding constrains, or by using user interfaces. 
The samplinginterval of thepoints also affects the opti­mization sensitively. In this talk, we proposed 
an optimization algorithm that im­proves illustrations using the proposed design principles. We showed 
that ouroptimization algorithmincreasesthequality ofin­put vectorimages. References EIGENSATZ, M., SUMNER, 
R. W., AND PAULY, M. 2008. Curvature-domain shapeprocessing. Comput. Graph. Forum 27, 2,241 250. FU, 
H., ZHOU, S., LIU, L., AND MITRA, N. J. 2011. Animated construction oflinedrawings. ACM Trans. Graph. 
30,6,133. MILLET, L. J., STEWART, M. E., SWEEDLER, J. V., NUZZO, R. G., AND GILLETTE, M. U. 2007. Micro.uidic 
devices for culturing primary mammalian neurons at low densities. Lab Chip 7,8,987 994. OHTAKE,Y., BELYAEV,A. 
G., AND SEIDEL,H.-P. 2002. Mesh smoothing by adaptive and anisotropic gaussian .lter applied to mesh 
normals. In VMV,203 210. Copyright is held by the author / owner(s). SIGGRAPH 2012, Los Angeles, California, 
August 5 9, 2012. ISBN 978-1-4503-1435-0/12/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2343005</article_id>
		<sort_key>1090</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>93</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>9</seq_no>
		<title><![CDATA[Improving registration using active shape models and depth]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2342896.2343005</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2343005</url>
		<abstract>
			<par><![CDATA[<p>The problem of registering multiple images taken of the same scene is particularly difficult in domains where different sensors are used to collect the information. Registration techniques are used to combine the independent information into a single image for further processing or for additional insight into the data. In video, sequential images are usually highly correlated, making registration simpler; the exception is when a new scene enters the viewfinder or when extreme camera movement results in large differences between sequential images. In these cases, far less information is available for registration. Using multiple sensors trained on the same scene is helpful; however, the precise relationship between the sensors must be obtained to compare the data spatially as well as temporally. In these cases, landmarks are often useful.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3737536</person_id>
				<author_profile_id><![CDATA[81504688093]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Colin]]></first_name>
				<middle_name><![CDATA[P.]]></middle_name>
				<last_name><![CDATA[Bellmore]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Rochester Institute of Technology, Rochester New York]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[cpb8010@rit.edu]]></email_address>
			</au>
			<au>
				<person_id>P3737537</person_id>
				<author_profile_id><![CDATA[81100288268]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Roxanne]]></first_name>
				<middle_name><![CDATA[L.]]></middle_name>
				<last_name><![CDATA[Canosa]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Rochester Institute of Technology, Rochester New York]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Cootes, T., and Taylor, C. 1992. Active shape model -- smart snakes. In <i>Proceedings of the British Machine Vision Conference</i>, 266275, 1829--1841.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Improving Registration Using Active Shape Models and Depth Figure 1: Far left: candidate registration 
points RGB image points shown in green and depth image points shown in red. Left middle: transparent 
overlay of RGB and depth images with depth window border. Center: active shape model .tted to RGB image. 
Right middle: active shape model .tted to depth image. Far right: registration results after deformation 
 RGB points in green and depth points in red. 1 Introduction The problem of registering multiple images 
taken of the same scene is particularly dif.cult in domains where different sensors are used to collect 
the information. Registration techniques are used to com­bine the independent information into a single 
image for further processing or for additional insight into the data. In video, sequen­tial images are 
usually highly correlated, making registration sim­pler; the exception is when a new scene enters the 
view.nder or when extreme camera movement results in large differences be­tween sequential images. In 
these cases, far less information is available for registration. Using multiple sensors trained on the 
same scene is helpful; however, the precise relationship between the sensors must be obtained to compare 
the data spatially as well as temporally. In these cases, landmarks are often useful. Registering RGB 
webcam images with depth images is dif.cult due to the absence of detectable landmarks in the depth channel. 
Usu­ally, an additional motion or shading model is imposed to correlate depth pixels to color pixels, 
taking advantage of the fact that an object s depth and its color are spatially well-registered in the 
real world. In addition, registering a depth sensor with an RGB we­bcam requires .nding and modeling 
both cameras parameters as well as characterizing the noise from each sensor. We propose us­ing a non-rigid 
deformation technique based on active shape models (ASMs) [Cootes and Taylor 1992] to circumvent the 
need for ex­ plicitly modeling camera parameters. An ASM is a statistical shape model that can provide 
information for overcoming the dif.culties inherent in registering image patches obtained from color 
and depth sensors. The ASM is used in the absence of real landmarks and works by assesing curvature and 
learning discrimintating points on a per-channel basis. This technique was applied to real-time video 
and tested on ASMs trained from low-resolution facial images gen­erated from an inexpensive commercially 
available RGB+D sensor. The results show that the deformation technique improves registra­tion under 
speci.c conditions. 2 Our Approach Our system used an active shape model for registering images orig­inating 
from separate depth and color channels. This required that *e-mail:cpb8010@rit.edu object points located 
in the world map to the corresponding pixels in each image. To achieve this, we .rst geometrically aligned 
the depth and color sensors and then spatially recti.ed the two image streams. Additionally, the depth 
data was .ltered to reduce noise and then the face was .tted to both channels for the purpose of locating 
control points for the ASM. The recti.ed and non-rigidly deformed depth map was then overlaid onto the 
RGB image, pro­ducing a registered 4-channel image. The overall approach consisted of .ve processing 
stages: (1) local­ization of the face using a motion-based depth silhoutte, (2) pro­jective viewpoint 
transformation to model the relationship between the RGB and depth images, (3) motion-restricted median 
.ltering to improve the accuracy of the depth measurements, (4) active shape model training and .tting, 
and (5) deformation of the ASM con­trol points. For the .nal deformation step, local image registration 
was required because certain areas of the image needed different amounts of deformation. Since the non-linear 
nature of the de­formation can cause computational ef.ciency problems, a locally weighted mean .eld smoothed 
by a Gaussian allowed the model to spend processing resources on high areas of deformation while not 
affecting previously well-registered areas. The results after deformation were analyzed for validity 
by mea­suring the distance between the transformed points and the con­trol points of the ASM. The results 
were evaluated by comparing our 14-point ASM model to previously published 58-point and 68­point models 
under conditions of varying lighting and facial struc­tures. Evaluation metrics included mean .tting 
time, mean errors, and standard deviation of errors. Results showed that our model has lower mean .tting 
time as compared to state-of-the-art 58-point and 68-point models (41 msec versus 428 msec and 544 msec 
re­spectively) and lower mean error (2 pixels versus 7 pixels and 8 pixels respectively), although the 
lower resolution of our test im­ages compared to the other models test images contributed to our smaller 
average error. Overall, this study showed that ASMs can be successfully used for registration of color 
and depth images. References COOTES, T., AND TAYLOR, C. 1992. Active shape model smart snakes. In Proceedings 
of the British Machine Vision Confer­ence, 266275, 1829 1841. Copyright is held by the author / owner(s). 
SIGGRAPH 2012, Los Angeles, California, August 5 9, 2012. ISBN 978-1-4503-1435-0/12/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2343006</article_id>
		<sort_key>1100</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>94</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>10</seq_no>
		<title><![CDATA[Iterative cage-based registration for dynamic shape capture]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2342896.2343006</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2343006</url>
		<abstract>
			<par><![CDATA[<p>Recent advances in low-cost dynamic scanning turn the cross-parametrization of non-rigid animatable surface into a vision-oriented ill-posed problem. In contrast with [Li et al. 2012], we propose a novel detail-preserving registration approach with resolution-independent control. Furthermore, our skin-detached surface registration avoids patch-based segmentation or affine fitting to maintain the local plasticity, as required in [Budd and Hilton 2010]. In particular, we leverage the problem of highly non-rigid spacetime registration by employing an elasto-plastic coarse cage. Thus, we perform scalable handle-aware harmonic shape registration, relying on the high-level of shape abstraction offered by the space-based paradigm. To the best of our knowledge, our technique is the first to investigate handle-aware elastic overlapping-rigidities for registering life-like dynamic shapes in full-body clothing.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3737538</person_id>
				<author_profile_id><![CDATA[81472654403]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Yann]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Savoye]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[INRIA Bordeaux University, France]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[ysavoye@siggraph.org]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1932892</ref_obj_id>
				<ref_obj_pid>1932686</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Budd, C., and Hilton, A. 2010. Temporal alignment of 3d video sequences using shape and appearance. In <i>CVMP'10: 9th Conference on Visual Media Production</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1964973</ref_obj_id>
				<ref_obj_pid>2010324</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Jacobson, A., Baran, I., Popovi&#263;, J., and Sorkine, O. 2011. Bounded biharmonic weights for real-time deformation. <i>ACM Trans. Graph. 30</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2077343</ref_obj_id>
				<ref_obj_pid>2077341</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Li, H., Luo, L., Vlasic, D., Peers, P., Popovi&#263;, J., Pauly, M., and Rusinkiewicz, S. 2012. Temporally coherent completion of dynamic shapes. <i>ACM Trans. Graph. 31</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1360696</ref_obj_id>
				<ref_obj_pid>1360612</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Vlasic, D., Baran, I., Matusik, W., and Popovi&#263;, J. 2008. Articulated mesh animation from multi-view silhouettes. <i>ACM Trans. Graph. 27</i> (August), 97:1--97:9.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Iterative Cage-based Registration for Dynamic Shape Capture Yann Savoye*  f   (a)(b)(c)(d)(e)(f)(g)(h) 
 Figure 1: Life-like Dynamic Registration of Clothed Organic Shape. A generic humanoid-type cage is 
enveloping a laser-scanned template mesh roughly, to perform scalable f registration of whole-body scanned 
models (a). We aim at deforming the source template (in yellow color) toward a target point cloud (in 
red color) (b). The skin-detached registration procedure pulls the cage-based geometry toward corresponding 
target locations (c). After a su.cient number of decade iterations, the next target point cloud is reconstructed 
(d), and . the overall iterative procedure is repeated (e). Normal-guided pairwise correspondences (in 
green color) are pruned, satisfying a criteria based on smoothed normals di.erence (f). Consequently, 
handle-aware overlapping harmonic-rigidities (g) are well-suited to register the non-rigid edge-length 
deviation (h) of the surface with controllable .exibility. 1 Introduction Iterative Elasto-Plastic Optimization. 
For each point cloud, our cage-handle curve registration process alternates successively Recent advances 
in low-cost dynamic scanning turn the cross­between deformation optimization and correspondences. At 
eachparametrization of non-rigid animatable surface into a vision­intra-frame iteration t+, we initialize 
the cage geometry with loca­oriented ill-posed problem. In contrast with [Li et al. 2012], tion ct obtained 
at the previous iteration t. Then, we update L (·),we propose a novel detail-preserving registration 
approach with the corresponding d and we infer S. Finally, driven by the cor­resolution-independent control. 
Furthermore, our skin-detached respondences propagation in the subspace, a new cage pose ct+ issurface 
registration avoids patch-based segmentation or a.ne .t­estimated by solving the following variational 
objective function: mm t+ Lt qk - . ting to maintain the local plasticity, as required in [Budd and 
Hilton 2 f 2 2010]. In particular, we leverage the problem of highly non-rigid t+ - dt j argmin a · 
+ ß · .k c wk j · c · . .. . jj spacetime registration by employing an elasto-plastic coarse cage. 2 
 {ct+ ,··· ,ct+ m } 1 j=1 sk .St j=1 2 Thus, we perform scalable handle-aware harmonic shape registra­ 
tion, relying on the high-level of shape abstraction o.ered by the space-based paradigm. To the best 
of our knowledge, our technique is the .rst to investigate handle-aware elastic overlapping-rigidities 
for registering life-like dynamic shapes in full-body clothing. 2 Handle-Aware Detached Registration 
Non-Rigid Registration Setup. We propose to evolve the .xed connectivity F ,o.ered by the given dense 
template mesh M = {V, F ), by registering roughly-and-temporally a given set P = {P0, ··· , Pl} of l 
unstructured time-varying point clouds. We assume no prior knowledge about the temporal matching. The 
ge­ometry of the template mesh is written by V. Let us denote O . R3 the bounded domain included by m 
control cage-handles envelop­ing the static template mesh. We designate by cj the current location of 
the jth cage handle in the global coordinates system. This cage polytope structure is augmented by the 
Laplace-Beltrami Operator L (·) with non-uniform cotangent weights, allowing scalable tem­plate mesh 
registration. Associated di.erential cage coordinates d encode each cage-handle relatively to its neighborhood 
in the cage connectivity. Finally, the geodesic-aware relationship between the volumetric subspace and 
the static template is encapsulated by a bi-harmonic rigging process, computed once at the default pose. 
Normal-Guided Pairwise Correspondences. We adopt a sim­ilar fuzzy-yet-robust geometric strategy than 
[Budd and Hilton 2010] to infer a minimal set of compatible feature correspon­dences S = {sk :(k, qk,.k)}, 
updated at each intra-frame iteration. A unique target location qk . R3 is obtained for the kth current 
tem­plate vertex by averaging candidates in the current point cloud of P. After the outliers pruning, 
each correspondence is weighted by .k de.ned as the dot product of smoothed pairwise normals. *ysavoye@siggraph.org 
 where wk j : O . R is the biharmonic weight for a given cage han­dle j with respect to the kth template 
vertex, as proposed in [Jacob­son et al. 2011]. Consequently, the registered template geometry is generated 
by a cage-based warping .eld with low-distortion. Weight-Control Update Rules. The data-term weight-control 
is initialized at ß = 0.01 and increases along iterations by following an exponential growth rule to 
promote the constraint-guided bend­ing energy. The weight-control a enforcing the shape-prior is set-up 
to 1 and slightly decreases to relax the deformation sti.ness prior. 3 Conclusions Our new approach 
is a .rst step toward the automatic template­based registration of highly non-rigid dynamic shape using 
low­dimensional space-based encoding. We train the e.ectiveness of our algorithm by aligning several 
real-world datasets of [Vlasic et al. 2008]. The main advantage of our iterative optimization re­mains 
in the simultaneous cross-reconstruction of dynamic shape, and skin-detached registration of reusable 
temporal curves express­ing the clothed-body deformations. In brief, we proposed a new system that registers 
shape variations while preserves the life­likeness of captured data, and acquires reusable consistent 
surface parameters. We expect to pursue our on-going e.orts to perform better .ne-tuned non-rigid alignment 
for large organic motion. References Budd, C., and Hilton, A. 2010. Temporal alignment of 3d video sequences 
using shape and appearance. In CVMP 10: 9th Conference on Visual Media Production. Jacobson, A., Baran, 
I., Popovic´, J., and Sorkine, O. 2011. Bounded biharmonic weights for real-time deformation. ACM Trans. 
Graph. 30. Li, H., Luo, L., Vlasic, D., Peers, P., Popovic´, J., Pauly, M., and Rusinkiewicz, S. 2012. 
Temporally coherent completion of dynamic shapes. ACM Trans. Graph. 31. Vlasic, D., Baran, I., Matusik, 
W., and Popovic´, J. 2008. Articulated mesh animation from multi-view silhouettes. ACM Trans. Graph. 
27 (August), 97:1 97:9. Copyright is held by the author / owner(s). SIGGRAPH 2012, Los Angeles, California, 
August 5 9, 2012. ISBN 978-1-4503-1435-0/12/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2343007</article_id>
		<sort_key>1110</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>95</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>11</seq_no>
		<title><![CDATA[LiveTree]]></title>
		<subtitle><![CDATA[realistic tree growth simulation tool]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2342896.2343007</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2343007</url>
		<abstract>
			<par><![CDATA[<p>Natural 3D tree modeling and growth simulation as realistic as possible have been an important goal in a variety of areas such as computer vision and graphics. Hence, the creation of realistic looking 3D tree growth models is one of the most complicated and challenging tasks because of its inherent geometric complexity. As a consequence, computer representations of tree growth processes need considerable efforts to achieve high level of realism. Moreover, when describing the growth processes for a given whole tree model in most of commercial tools, only the scaling factor of overall tree size is considered so that the tree growth simulation has a limitation of having a simple isometric growth, that is the tree's shape is consistent through the entire growth processes. In reality, meanwhile, every parts of a tree grow at relative rates.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3737539</person_id>
				<author_profile_id><![CDATA[81447598630]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Jaehwan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kim]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Electronics and Telecommunications Research Institute, Republic of Korea]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[jh.kim@etri.re.kr]]></email_address>
			</au>
			<au>
				<person_id>P3737540</person_id>
				<author_profile_id><![CDATA[81100058221]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Il]]></first_name>
				<middle_name><![CDATA[Kwon]]></middle_name>
				<last_name><![CDATA[Jeong]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Electronics and Telecommunications Research Institute, Republic of Korea]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[jik@etri.re.kr]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1263423</ref_obj_id>
				<ref_obj_pid>1263141</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Bai, X., L. L. J., and Liu, W. Y. 2007. Skeleton pruning by contour partitioning with discrete curve evolution. <i>IEEE Trans. Pattern Analysis and Machine Intelligence 297</i>, 3, 449--462.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Kim, J., Shim, K.-H., and Choi, S. 2007. Soft geodesic kernel k-means. In <i>Proc. IEEE Int'l Conf. Acoustics, Speech, and Signal Processing</i>, vol. 2, 429--432.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1340234</ref_obj_id>
				<ref_obj_pid>1340087</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Levin, A., Lischinski, D., and Weiss, Y. 2008. A closed form solution to natural image matting. <i>IEEE Trans. Pattern Analysis and Machine Intelligence 30</i>, 2 (Feb.), 228--242.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 (a) (b) (c) Figure 1: (a) LiveTree tool screenshots, (b) three different tree growth models according 
to the growth processes: allometric growth at the left without considering the scaling attribute; isometric 
growth at the middle; our proposed allometric growth at the right, and (c) .rst a few magni.ed resultsof 
isometric(darkblue)&#38;ours(darkbrown)growth modelsin(b). 1 Introduction Natural 3D tree modeling and 
growth simulation as realistic as pos­siblehavebeenan importantgoalinavarietyof areassuchas com­puter 
vision and graphics. Hence, the creation of realistic looking 3D tree growth models is one of the most 
complicated and challeng­ing tasks because of its inherent geometric complexity. As a con­sequence, computer 
representations of tree growth processes need considerable efforts to achieve high level of realism. 
Moreover, when describingthegrowth processesforagiven whole tree model in mostof commercial tools,onlythe 
scalingfactorofoverall tree sizeis consideredsothatthe treegrowth simulationhasa limitation of having 
a simple isometric growth, that is the tree s shape is con­sistent through the entire growth processes. 
In reality, meanwhile, every parts of a tree grow at relative rates. Inthisnote,we presentanovel, easy-to-generate 
solutionthatisca­pable of creating realistic-looking 3D tree models and growth pro­cesses from one given 
tree image. We are not only able to create a variety of similar models sharing the main trunk extracted 
from the given tree image through digital image matting [Levin et al. 2008] andskeleton-based abstraction[BaiandLiu2007]of 
branches,but also generate all ages of young trees from the constructed mature treeby incorporating geodesickernel[Kimetal.2007]intoa 
tree­growing mechanism, in order to preserve topologically stable paths for the visible branches manifold, 
under pre-de.ned assumptions: (a) the new branch is the farthest away from a tree root on the branches 
structure(i.e., branching orders); (b) branch which is thin­ner than others around the branch is more 
likely to be a branch of recent growth(i.e., branch thicknesses); (c) the global shape of tree is preserved 
through the entire growth processes(i.e., tree sizes). 2 Our approach Once the visible branches consisting 
of the trunk with twigs have been extracted from the digital image matting procedure, we next buildatree 
and its allometric growth models about thegiven visible *e-mail: {jh.kim, jik}@etri.re.kr. This work 
was supported by the Ministry of Culture, Sports, and Tourism (MCST) and the Korea Creative Content Agency 
(KOCCA) in the Culture Technology (CT) Research&#38;Development Program 2011 [211A5020021098502001] branches. 
We construct the tree skeleton as plausible as possible. However, for a complex tree branch consisting 
of lots of deforma­tions and noise lead to redundant or shorten skeleton branches. To overcomesuchan 
instability,weemployaskeletonpruning method proposedby[BaiandLiu2007].Themainideaofthe currentskele­ton 
pruning method is to perform a topology preserving skeleton pruning based on a contour partition into 
curve segments, which makesit possibletoextracttheexactskeletonwiththeglobaltopol­ogy. After getting 
the tree branch skeleton, we convert the skeleton into an undirected acyclic graph. Then, with the graph, 
we de.ne a measurement of the i node s age T (i) which is in inverse propor­tion to the growth order, 
and a scaling control value of the whole tree size M(j), based on the addressed assumptions as following: 
T (i .=+ EDT (xi)= r) wp..(xi) - .(xr).-2 = wp(Kii - 2Kir + Krr)-1 + EDT (xi), M(j ..T (j), s.t. = r)0 
< M(·) = 1, j is an index of the latest branching node, where we consider a nonlinear transform .(xt) 
andkernel K as a geodesickernel matrix. xr is a tree root node and wp is a weight value. We employ a 
euclidean distance transform, EDT (xi), asa thickness measurement. As shown in Fig.1, our system is able 
to generate complex and realistic-looking tree growth models having distinct branching structures and 
sizes,but preserving their approx­imate shape through the entire growth processes, in a short time. Our 
system also provides .exible user control for editing the out­put tree model by means of sketching(e.g., 
adding and deleting) branches on the intermediate result, alpha matte. References BAI,X.,L.L.J., AND 
LIU,W.Y. 2007. Skeleton pruningby contour partitioning with discrete curve evolution. IEEE Trans. Pattern 
Analysis and Machine Intelli­ gence 297, 3, 449 462. KIM,J.,SHIM,K.-H., AND CHOI,S. 2007. Soft geodesickernel 
k-means. In Proc. IEEE Int l Conf. Acoustics, Speech, and Signal Processing, vol. 2, 429 432. LEVIN,A.,LISCHINSKI,D., 
AND WEISS,Y. 2008.Aclosed form solutionto natural image matting. IEEE Trans. Pattern Analysis and Machine 
Intelligence 30,2(Feb.), 228 242. Copyright is held by the author / owner(s). SIGGRAPH 2012, Los Angeles, 
California, August 5 9, 2012. ISBN 978-1-4503-1435-0/12/0008 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2343008</article_id>
		<sort_key>1120</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>96</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>12</seq_no>
		<title><![CDATA[Non-rigid shape correspondence and description using geodesic field estimate distribution]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2342896.2343008</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2343008</url>
		<abstract>
			<par><![CDATA[<p>Non-rigid shape description and analysis is an unsolved problem in computer graphics. Shape analysis is a fast evolving research field due to the wide availability of 3D shape databases. Widely studied methods for this family of problems include the Gromov Hausdorff distance [1], Bag-of-Features [2] and diffusion geometry [3]. The limitations of the Euclidian distance measure in the context of isometric deformation have made <i>geodesic distance</i> a de-facto standard for describing a metric space for non-rigid shape analysis. In this work, we propose a novel geodesic field space-based approach to describe and analyze non-rigid shapes from a point correspondence perspective.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3737541</person_id>
				<author_profile_id><![CDATA[81504683764]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Austin]]></first_name>
				<middle_name><![CDATA[T.]]></middle_name>
				<last_name><![CDATA[New]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Georgia, Athens, Georgia]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737542</person_id>
				<author_profile_id><![CDATA[81487646647]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Anirban]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Mukhopadhyay]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Georgia, Athens, Georgia]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737543</person_id>
				<author_profile_id><![CDATA[81100065618]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Hamid]]></first_name>
				<middle_name><![CDATA[R.]]></middle_name>
				<last_name><![CDATA[Arabnia]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Georgia, Athens, Georgia]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737544</person_id>
				<author_profile_id><![CDATA[81100563420]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Suchendra]]></first_name>
				<middle_name><![CDATA[M.]]></middle_name>
				<last_name><![CDATA[Bhandarkar]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Georgia, Athens, Georgia]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1113436</ref_obj_id>
				<ref_obj_pid>1113434</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Memoli F., Sapiro G.: A theoretical and computational framework for isometry invariant recognition of point cloud data. <i>Found. Comput. Math. 5</i>, 3 (2005), 313--347.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Ovsjanikov, M., Bronstein, A. M., Bronstein, M. M., and Guibas, L. J. 2009. Shape Google: a computer vision approach to invariant shape retrieval. In Proc. NORDIA.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1735621</ref_obj_id>
				<ref_obj_pid>1735603</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Sun, J., Ovsjanikov, M., and Guibas, L. J. 2009. A concise and provably informative multi-scale signature based on heat diffusion. In Proc. SGP.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Bhattacharyya, A. (1943). "On a measure of divergence between two statistical populations defined by their probability distributions". Bulletin of the Calcutta Mathematical Society &#60;b&#62;35:&#60;/b&#62; 99--109.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[http://tosca.cs.technion.ac.il/book/shrec.html]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 1 Introduction: Non-rigid shape description and analysis is an unsolved problem in computer graphics. 
Shape analysis is a fast evolving research field due to the wide availability of 3D shape databases. 
Widely studied methods for this family of problems include the Gromov Hausdorff distance [1], Bag-of-Features 
[2] and diffusion geometry [3]. The limitations of the Euclidian distance measure in the context of isometric 
deformation have made geodesic distance a de-facto standard for describing a metric space for non-rigid 
shape analysis. In this work, we propose a novel geodesic field space-based approach to describe and 
analyze non-rigid shapes from a point correspondence perspective. A novel estimate of the geodesic field 
around a point is proposed as follows: Non-rigid Shape Correspondence and Description Using Geodesic 
Field Estimate Distribution Austin T. New, Anirban Mukhopadhyay, Hamid R. Arabnia, Suchendra M. Bhandarkar 
Department of Computer Science, The University of Georgia, Athens, Georgia 30602-7404, USA  Definition: 
For any point f on the surface S, the Geodesic Field Estimate (GFE) at f is the probability that the 
geodesic between points x and y will pass through point f. More formally, GFE(f) = P (f . Geod(x,y)) 
where, Geod(x,y) is the geodesic between points x and y and points f, x, y . S. For a given point, the 
GFE distribution of a local subsurface containing the point is computed as shown in Figure 1(a) and compared 
to the GFE distribution at other points using the Bhattacharyya coefficient measure [4]. The computation 
and comparison of the GFE distribution is performed starting from a coarser scale and proceeding to a 
finer scale (where the scale is defined by the area of the subsurface patch) to narrow down the search 
for corresponding points. 2 Methodology: The proposed method seeks to establish correspondence between 
surface features in a scale-, rotation-, and isometry-invariant manner. By definition the GFE is an isometry-invariant 
valuation of surface vertices, but this valuation must be generalized to achieve scale and rotation invariance. 
Scale invariance is achieved by normalizing the Geodesic length by the longest surface geodesic whereas 
rotation invariance is achieved by taking a sub-surface ring and representing it as a GFE distribution. 
This generalized valuation, resulting from replacing a vertex by the GFE distribution, may then be used 
as a basis for comparing vertices using the Bhattacharya coefficient measure. The search for correspondence 
was based on the refinement of candidates. A starting scale (0.05 of the longest surface geodesic) is 
picked, incremented (0.05 of the longest surface geodesic) and Bhattacharya coefficient is compared until 
the highest ranked candidate is found. 3 Conclusion and Future Work: Geodesic distance - the most common 
metric for non-rigid shape description, is used to generate a Geodesic Field Estimate (GFE) for surface 
points. The GFE distribution around each surface point is used to obtain a sparse point correspondence 
via the Bhattacharyya coefficient measure. The work is performed on TOSCA dataset [5]. As shown in Figure 
1 (c) and (d), the results are promising with very high accuracy without considering symmetry (which 
would increase accuracy of hands and legs). In future, we propose to formalize the notion of geodesic 
scale space in the context of shape description which could then be used to establish partial shape correspondence, 
detect symmetry and search a large database of 3D shapes. 4 References 1. Memoli F., Sapiro G.: A theoretical 
and computational framework for isometry invariant recognition of point cloud data. Found. Comput. Math. 
5, 3 (2005), 313 347. 2. Ovsjanikov, M., Bronstein, A. M., Bronstein, M. M., AND Guibas, L. J. 2009. 
Shape Google: a computer vision approach to invariant shape retrieval. In Proc. NORDIA. 3. Sun, J., Ovsjanikov, 
M., AND Guibas, L. J. 2009. A concise and provably informative multi-scale signature based on heat diffusion. 
In Proc. SGP. 4. Bhattacharyya, A. (1943). "On a measure of divergence between two statistical populations 
defined by their probability distributions". Bulletin of the Calcutta Mathematical Society 35: 99 109. 
5. http://tosca.cs.technion.ac.il/book/shrec.html 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2343009</article_id>
		<sort_key>1130</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>97</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>13</seq_no>
		<title><![CDATA[Simplification of hexahedral mesh]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2342896.2343009</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2343009</url>
		<abstract>
			<par><![CDATA[<p>Mesh simplification is an important and difficult problem in pretreatment and post-processing of finite element analysis (FEA). Simplification of tetrahedral mesh has been studied deeply in the past two decades. However, although hexahedral mesh is widely used in FEA, there is almost no related work about its simplification method.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3737545</person_id>
				<author_profile_id><![CDATA[81504682591]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Pan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Chao]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Zhejiang University, Hangzhou, China]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[20051005@zju.edu.cn]]></email_address>
			</au>
			<au>
				<person_id>P3737546</person_id>
				<author_profile_id><![CDATA[81442619246]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Liu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Zhenyu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Zhejiang University, Hangzhou, China]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[liuzy@zju.edu.cn]]></email_address>
			</au>
			<au>
				<person_id>P3737547</person_id>
				<author_profile_id><![CDATA[81504687529]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Tan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Jianrong]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Zhejiang University, Hangzhou, China]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[tjr@cad.zju.edu.cn]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1394234</ref_obj_id>
				<ref_obj_pid>1394232</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Shepherd, J. F., and Johnson, C. R. 2008. Hexahedral Mesh Generation Constraints. <i>Engineering with Computers, 24</i>, 3, 195--213.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Staten, M. L., Shepherd, J. F., and Shimada, K. 2008. Mesh Matching-Creating Conforming Interfaces between Hexahedral Meshes. <i>Proceedings of the 17th International Meshing Roundtable</i>, Part7, 467--484.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1409101</ref_obj_id>
				<ref_obj_pid>1409060</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Daniels, J., Silva, C. T., Shepherd, J., and Cohen, E. 2008. Quadrilateral Mesh Simplification. <i>ACM Transactions on Graphics, 27</i>, 5, Article 148.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Simplification of Hexahedral Mesh Pan Chao, Liu Zhenyu, Tan Jianrong State Key Lab CAD &#38; CG, Zhejiang 
University, Hangzhou 310027, China*  (a) (b) (c) (d) Figure 1: (a) (b) Input cell set for a hexahedral 
mesh model, (c) (d) output simplified model. 1 Introduction Mesh simplification is an important and difficult 
problem in pretreatment and post-processing of finite element analysis (FEA). Simplification of tetrahedral 
mesh has been studied deeply in the past two decades. However, although hexahedral mesh is widely used 
in FEA, there is almost no related work about its simplification method. Shepherd, Staten and their partners 
has proposed a new method to structure new hexahedral cells through inserting conforming interfaces among 
the non-conforming interfaces into the original model[Shepherd and Johnson, 2008][Staten, Shepherd and 
Shimada, 2008]. Contrary to the algorithm mentioned above, we can imagine that if we could extract the 
conforming interfaces form the original model and remove them, the model would be simplified. Based on 
the idea, a simplification method defined over hexahedral meshes is presented in this work. 2 Our Approach 
The overall strategy is making all the vertices in a conforming interface merged with the neighbor vertices 
on the same side of the conforming interface. Before simplification, as some complicated model could 
not be subdivided to a pure hexahedral mesh model, we must perform pretreatment for the original model. 
If there is small number of other-type mesh cells, just take them away from the original model, treat 
them individually and take the remaining space as cavity of the original model. Besides, if there are 
adjacent-hexahedral-cell couples which do not shall the same quadrilateral chip, just take the interface 
between the two hexahedral cells as a part of the surface of the original model. Given a pure hexahedral 
mesh model, we first build the adjacent relationship between one node with the nodes around it. In order 
to store the relationship, we create a list called six adjacent-node list . There are two kinds of different 
node in the list: some of them can be merged with the list s owner when removing a conforming interface 
from the original model without changing the model s topological structure while others not. The former 
must come up with one another node, so we call them node couples and the other nodes are named by extra 
adjacent nodes . The number of node couples that a node possesses is defined as degree and it figures 
out the biggest number of conforming interfaces which pass through the node. *e-mail: {20051005, liuzy}@zju.edu.cn 
tjr@cad.zju.edu.cn The next step is extracting conforming interfaces from the original model by seeking 
the nodes in the same conforming interface with the six adjacent-node lists. A very interesting finding 
is that the nodes in a conforming interface s list can be traversed with a leftmost tree. Finally, we 
erase the nodes in the same conforming interface and merge the coupled cells on both sides of the conforming 
interface. As a result, the conforming interface is removed. It's worth noting that no vertex in non-conforming 
interfaces can be merged with the vertices in conforming interfaces so as to maintain the shape of the 
model. In order to prove that our algorithm is able to work well, two examples are given and one of them 
is very simple but the other one is quite complex. Two factors is found that would limit the performance 
of the algorithm. For one thing, most part of the time is consumed in constructing the adjacent relations 
of nodes. For another thing, if the preprocessing algorithm could not work, simplification could not 
work as well. If we consider the hexahedral mesh as a cylinder with a quadrilateral-grid cross-section 
and make the quadrilateral-grid cross-section simplified by edge-collapse[Daniels, Silva, Shepherd and 
Cohen, 2008], we can see that the main difference between our algorithm and the idea based on edge-collapse 
is that there is no non-conforming interface could be removed or added to the original mode in our algorithm. 
 References SHEPHERD, J. F., AND JOHNSON, C. R. 2008. Hexahedral Mesh Generation Constraints. Engineering 
with Computers, 24, 3, 195-213. STATEN, M. L., SHEPHERD, J. F., AND SHIMADA, K. 2008. Mesh Matching-Creating 
Conforming Interfaces between Hexahedral Meshes. Proceedings of the 17th International Meshing Roundtable, 
Part7, 467-484. DANIELS, J., SILVA, C. T., SHEPHERD, J., AND COHEN, E. 2008. Quadrilateral Mesh Simplification. 
ACM Transactions on Graphics, 27, 5, Article 148. Copyright is held by the author / owner(s). SIGGRAPH 
2012, Los Angeles, California, August 5 9, 2012. ISBN 978-1-4503-1435-0/12/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	</section>
	<section>
		<section_id>2343010</section_id>
		<sort_key>1140</sort_key>
		<section_seq_no>17</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Rendering]]></section_title>
		<section_page_from>17</section_page_from>
	<article_rec>
		<article_id>2343011</article_id>
		<sort_key>1150</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>98</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[A collision detection method for high resolution objects using tessellation unit on GPU]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2342896.2343011</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2343011</url>
		<abstract>
			<par><![CDATA[<p>Currently, collision detection in virtual worlds is an important research topic, since it can be applied to many categories of simulations. The current trend of research is focused on the development of Bounding Volume Hierarchies (BVHS), and how to accelerate collision detection with speed, accuracy in a way that can be applied to many types of objects such as soft and rigid bodies. As a result many previous works have focused on developing BVHS to update its bound efficiently, data structures with filters that can effectively prune the objects or areas of objects that are not going to collide as well as developing methods to detect collisions in parallel on a GPU [Min, Dinesh et al. 2011]. However, the major problems of this type of research are limitations on the object to be used, which must not have a high resolution of millions of polygons, due to limited memory on the GPU, which is not enough to retain the BVHS data structure of the whole model. Thus, if only the area of collision has a high resolution, there can be enough memory to process this kind of object. Furthermore, a Tessellation Unit on the GPU, which can tessellate the object in areas or at factors that the user wants, is currently available, but it is only used to represent the high quality images, and there is no work focusing on using it for general purpose work. Therefore, this research proposes a method to detect collisions that can be used on objects with high resolution up to millions of polygons via integration with the Tessellation Unit by tessellating objects to high resolution only in the collision areas, which can be determined in parallel using a grid in GPU. Thus, the experimental results show that this method can reduce the amount of memory usage on the GPU, but still provide accurate results in collision detection.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3737548</person_id>
				<author_profile_id><![CDATA[81440621862]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Thiti]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Rungcharoenpaisal]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Chulalongkorn University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[myths.bass@gmail.com]]></email_address>
			</au>
			<au>
				<person_id>P3737549</person_id>
				<author_profile_id><![CDATA[81321493390]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Pizzanu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kanongchaiyos]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Chulalongkorn University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[pizzanu@chula.ac.th]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1944756</ref_obj_id>
				<ref_obj_pid>1944745</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Min, T., M. Dinesh, et al. (2011). Collision-streams: fast GPU-based collision detection for deformable models. <u>Symposium on Interactive 3D Graphics and Games</u>. San Francisco, California, ACM]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 A Colllision DDetectio n Methood for HHigh Resolutionn Objeccts usinng TTessellaation Unnit on GGPU 
Thiti Rungcharoennpaisal1 and P izzanu Kanonngchaiyos2 Departmennt of Computeer Engineeringg, Chulalongkkorn 
Universitty  Figuree 1: 200M polygoons in wireframe (left), tessellatedd polygons in poteential collision 
arreas(center), colliided polygons in green (right) Introductiion Currently, colllision detectionn in 
virtual woorlds is an impportant research topic,, since it can be applied too many catego ries of simulations. 
TThe current treend of researchh is focused on the development o f Bounding Voolume Hierarch ies (BVHS), 
annd how to accelerate coollision detectioon with speed, aaccuracy in a wway that can be appliedd to 
many typees of objects suuch as soft an nd rigid bodies. As a result many previous workks have focussed 
on developing BVVHS to update its bound effic iently, data struuctures with filters th hat can effectivvely 
prune thee objects or arreas of objects that aree not going to coollide as well aas developing mmethods 
to detect collisiions in parallel on a GPU [Minn, Dinesh et al. 2011]. However, the major probleems of 
this tyype of resear ch are limitations on the object to b e used, which must not have a high resolution 
of mmillions of polyygons, due to li mited memory on the GPU, which iss not enough too retain the BVVHS 
data struccture of the whole moddel. Thus, if onnly the area off collision has a high resolution, therre 
can be enouugh memory too process this kkind of object. Furtherrmore, a Tesselllation Unit on the GPU, 
whiich can tessellate the oobject in areas or at factors thhat the user waants, is currently availaable, 
but it is onnly used to reprresent the high quality images, and thhere is no worrk focusing onn using 
it for ggeneral purpose work.. Therefore, thhis research prroposes a methhod to detect collisionns that 
can be uused on objectss with high res olution up to millions of polygons vvia integration with the Tesseellation 
Unit by tessellaating objects too high resolutionn only in the coollision areas, which c an be determinned 
in parallel using a grid inn GPU. Thus, the expeerimental resultts show that th is method can reduce 
the amount off memory usaage on the GPPU, but still pprovide accurate resultss in collision deetection. 
 Proposedd Method In this researc h, the overvieww of our methhod using the GGPU is divided into thrree 
steps as folllows. 1) Grid Pruniing: At this staage, two coarsee triangular pollygonal objects that havve 
potential colllisions and a pposition buffer dderived from the simul ator simulatingg deformable obbjects 
are the innputs to be pruned usinng a grid-basedd method for ffinding only pootential collision areas 
as shown in Figgure 2(left). 1e-mail:myths.baass@gmail.com 2e-mail:pizzanu@@chula.ac.th 2)) Tessellation: 
At this stage, objects will bee tessellated witth the displacement maap from the simmulator in the prresence 
of a colllision onnly. The informmation is stored in the GPU memory in thee next sttep as shown inn Figure 
2(centeer). 3)) Collision dettection: At thiss stage, the poteential collision areas arre assigned to 
the hash tablle which is ussed to find paiirs of trriangular polyggons with a chhance of a col lision. 
The po lygon pairs will be testted to find conttact points, befofore forwarding them too the simulatorr 
to simulate annd display resuults of simulatioon, as shhown in Figure 1(right). Figure 2: Grid p runing 
using commpute shader (leeft), Tessellation using diirectx11 graphicss pipeline (centeer), Collision 
de tection using coompute shhader(right) TThe experimenntal results shhow that the proposed colllision 
detection methood for high-resoolution tessellaated object cann give thhe exact collisiion in high speeed. 
A sample of the experimmental reesult of 200 miillion triangularr polygons in hhigh resolution mesh obbjects 
shows thhe querying timme of our collisiion detection mmethod inn 50 ms in an aaverage. Our meethod presentedd 
is to show thee way too use the Tesseellation unit in tthe general purrpose. This appproach caan also 
be moodified using oothers data strructures and caan be opptimized for thee improvement further. RReference 
MMin, T., M. Dinnesh, et al. (20011). Collision -streams: fast GPU­based collission detection ffor deformable 
models. Sympoosium on Interacttive 3D Graphhics and Gammes. San Fran cisco, California, ACM Copyright 
is held by the author / owner(s). SIGGRAPH 2012, Los Angeles, California, August 5 9, 2012. ISBN 978-1-4503-1435-0/12/0008 
  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2343012</article_id>
		<sort_key>1160</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>99</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Acquiring perceptually diffuse shading from general objects in actual scenes]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2342896.2343012</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2343012</url>
		<abstract>
			<par><![CDATA[<p>Recently, augmented reality has started to be used in interactively simulating clothes, furnitures, and so on in actual scenes. In such applications, the shading of synthesized CG objects should be matched to those scenes to achieve a sense of reality. Although many techniques have been proposed to match the shading of CG objects to actual scenes, typically they require the use of light probes such as mirrored spheres [Debevec 1998]. However, such light probes are not always available in interactive simulation situations.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3737550</person_id>
				<author_profile_id><![CDATA[81504684161]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Yasuhiro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yao]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[NTT Cyber Space Laboratories, Nippon Telegraph and Telephone Corporation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[yao.yasuhiro@lab.ntt.co.jp]]></email_address>
			</au>
			<au>
				<person_id>P3737551</person_id>
				<author_profile_id><![CDATA[81504682747]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Harumi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kawamura]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[NTT Cyber Space Laboratories, Nippon Telegraph and Telephone Corporation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[kawamura.harumi@lab.ntt.co.jp]]></email_address>
			</au>
			<au>
				<person_id>P3737552</person_id>
				<author_profile_id><![CDATA[81100576829]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Akira]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kojima]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[NTT Cyber Space Laboratories, Nippon Telegraph and Telephone Corporation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[kojima.akira@lab.ntt.co.jp]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>280864</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Debevec, P. 1998. Rendering synthetic objects into real scenes: bridging traditional and image-based graphics with global illumination and high dynamic range photography. In <i>ACM SIGGRAPH 1998</i>, 189--198.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Motoyoshi, I., Nishida, S., Sharan, L., and Adelson, E. H. 2007. Image statistics and the perception of surface qualities. <i>Nature 447</i>, 7141 (may), 206--209.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Figure 1: 1 Introduction Recently, augmented reality has started to be used in interactively simulating 
clothes, furnitures, and so on in actual scenes. In such applications, the shading of synthesized CG 
objects should be matched to those scenes to achieve a sense of reality. Although many techniques have 
been proposed to match the shading of CG objects to actual scenes, typically they require the use of 
light probes such as mirrored spheres [Debevec 1998]. However, such light probes are not always available 
in interactive simulation situ­ations. In this paper, we introduce a method to acquire perceptually diffuse 
shading matched to actual scenes by referring to general objects in the scene. Our method does not require 
speci.c light probes and utilizes only a commercial RGB + depth sensor which is increas­ingly being used 
in interactive simulations. 2 Our Approach Our method is based on our discovery that we can acquire 
perceptu­ally diffuse shading by approximating Phong shading as a weighted sum of the .rst nine spherical 
harmonics (sphs). Motoyoshi et al. showed that humans perceive a surface as diffuse if the skewness of 
its luminance histogram is negative. We use skewness as a measure of perception of surface re.ection 
in this research. The aforementioned discovery was made through a CG experiment we conducted. In the 
experiment, we .rst approximated the lumi­nance of shading rendered by Phong re.ection model as a weighted 
sum of the .rst nine sphs on the basis of their surface normals. We then calculated the skewness of the 
resultant shading s luminance histogram. The results showed that skewness become negative re­gardless 
of the specular re.ection shininess. An example is shown in Figure 2, where the resultant shading shows 
negative skewness and is very similar to diffuse shading. This suggests that we can derive perceptually 
diffuse shading by obtaining the luminance and surface normals of actual objects, assuming the objects 
shading is based on the Phong re.ection model. *e-mail: yao.yasuhiro@lab.ntt.co.jp e-mail: kawamura.harumi@lab.ntt.co.jp 
e-mail: kojima.akira@lab.ntt.co.jp Figure 2: The middle result is derived by referring to Phong shad­ing 
(left). It shows negative skewness and is very similar to cor­rectly rendered diffuse shading (right). 
On the basis of our discovery, we implemented our method in the form of a system that uses simple instructions 
to derive perceptu­ally diffuse shading by referring to general objects in actual scenes. The system 
comprises a PC and a commercial RGB + depth sensor. It .rst uses the sensor to capture the scene and 
calculates the lu­minance values and corresponding surface normals. The user then selects a reference 
object (merely by clicking it in the image) and the shading is derived automatically. Segmentation thresholds 
for identifying objects can be adjusted interactively. Figure 1 shows ex­amples of synthesized CG objects 
rendered with derived shading. Although reference objects currently need to have half omnidirec­tional 
surface normals, our method can reference many types of ob­jects, including the human body. This implies 
that our method can be integrated into augmented reality simulations involving humans in the scenes. 
 References DEBEVEC, P. 1998. Rendering synthetic objects into real scenes: bridging traditional and 
image-based graphics with global illu­mination and high dynamic range photography. In ACM SIG-GRAPH 1998, 
189 198. MOTOYOSHI, I., NISHIDA, S., SHARAN, L., AND ADELSON, E. H. 2007. Image statistics and the perception 
of surface qual­ities. Nature 447, 7141 (may), 206 209. Copyright is held by the author / owner(s). SIGGRAPH 
2012, Los Angeles, California, August 5 9, 2012. ISBN 978-1-4503-1435-0/12/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2343013</article_id>
		<sort_key>1170</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>100</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Acquiring shell textures from a single image for realistic fur rendering]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2342896.2343013</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2343013</url>
		<abstract>
			<par><![CDATA[<p>To synthesize a realistic appearance of mammals, it is necessary to express disorderly lie of hairs. "Shell texturing method", proposed by Lengyel [2001], is possible to synthesize realistic fur appearance over arbitrary surfaces in real-time. Prior to rendering, it is necessary to prepare several shell textures as a pre-process. However, acquiring appropriate shell textures is a complicated and time consuming work. In this paper, we present a novel method to acquire shell textures only from a single input picture of an actual animal fur. Since every shell textures are automatically computed by a pixel shader in run-time, it is not necessary any complicated pre-computation. Furthermore, conventional shell texturing method employs typically 16 textures which require huge graphics memory. Because our method requires only a single texture, we realize a significant reduction in memory usage for practical purpose.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3737553</person_id>
				<author_profile_id><![CDATA[81504686871]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Hiroaki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ukaji]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Waseda University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737554</person_id>
				<author_profile_id><![CDATA[81504687323]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Takahiro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kosaka]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Waseda University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737555</person_id>
				<author_profile_id><![CDATA[81466646558]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Tomohito]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hattori]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Waseda University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737556</person_id>
				<author_profile_id><![CDATA[81331497177]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Hiroyuki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kubo]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Waseda University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737557</person_id>
				<author_profile_id><![CDATA[81100066959]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Shigeo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Morishima]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Waseda University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>364407</ref_obj_id>
				<ref_obj_pid>364338</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[J. Lengyel, E. Praun, A. Finkelstein, and H. Hoppe. Real-time fur over arbitrary surfaces. In Proc. I3D'01. pp.227--232. 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Acquiring Shell Textures from a Single Image for Realistic Fur Rendering Hiroaki Ukaji  Takahiro Kosaka 
 Tomohito Hattori  Hiroyuki Kubo  Shigeo Morishima Waseda University (a)input image       
 (b)layered shell textures          (c)synthesized furry model Figure 1. Workflow of our method 
and result image  1. Introduction To synthesize a realistic appearance of mammals, it is necessary to 
express disorderly lie of hairs. Shell texturing method , proposed by Lengyel [2001], is possible to 
synthesize realistic fur appearance over arbitrary surfaces in real-time. Prior to rendering, it is necessary 
to prepare several shell textures as a pre-process. However, acquiring appropriate shell textures is 
a complicated and time consuming work. In this paper, we present a novel method to acquire shell textures 
only from a single input picture of an actual animal fur. Since every shell textures are automatically 
computed by a pixel shader in run-time, it is not necessary any complicated pre-computation. Furthermore, 
conventional shell texturing method employs typically 16 textures which require huge graphics memory. 
Because our method requires only a single texture, we realize a significant reduction in memory usage 
for practical purpose. 2. Our Method Using only a single photograph of an animal fur, we are able to 
generate several fur textures automatically. From an upper view of an actual animal fur (fig.1 (a)), 
brighter region tend to be outside of the hair, and darker region likely to be inside. According to our 
observation, the depth of each pixel on a fur image highly depends on its brightness. Then, to generate 
the shell textures, we decided to approximate the distance from the surface as a function of each pixel 
s brightness. If the fur could be assumed to be a scattering medium, the light which entered perpendicularly 
from the surface of fur arrives at the bottom of fur with exponential decreasing. Therefore, by referring 
to the brightness of the input image, it is possible to estimate the depth of the fur. When we create 
the histogram of the brightness of the input picture, the brightness of the depth h from the surface 
is: (1) And attenuation coefficient is: (2) and  are the maximum and minimum of the histogram, and 
 is the thickness of the fur. So, if we generate N textures for rendering, we draw points which brightness 
is more than p(n-1) and less than p(n) to the nth texture (n=1,2, ,N), which exists in the depth  · 
 from surface, and the brightness p(n) is given by the following: (3) These processes can be performed 
collectively by GPU. So we can draw realistic fur seen in the photographed input image by using the textures 
generated automatically. In addition, it is certainly difficult to estimate the thickness of fur from 
a single image. However, we can adjust intervals between shell textures, so we can get several appearance 
of fur and choose appropriate output image. 3. Results Figure 1 shows an input image (a), layered shell 
textures (b), and furry model (c) using our method. The frame rate for rendering these models are, 1,648.3 
fps for the teapot (2,082 vtx.) and 138.0 fps for the bunny (34,832 vtx.) at 256 x 256 pixels. These 
are obtained from 2.66GHz Intel(R) Core(TM)2 Duo with NVIDIA GeForce GTX 285. Furthermore, compared with 
conventional shell texturing which requires 4,096 KB of memory, our method uses only 256 KB. As a result, 
we are able to synthesize realistic image of fur on arbitrary 3D objects from a single input image. 
4. Conclusions In this paper, we propose a method which generates shell textures automatically from a 
single image, and synthesize an animal fur appearance on arbitrary 3D objects. Using shell textures which 
are computed by our method, it is possible to render in real-time as well as conventional method. This 
work suggests some tasks for future researches, such as corresponding to more complicated fur which has 
two colors like zebra. References J. Lengyel, E. Praun, A. Finkelstein, and H. Hoppe. Real-time fur 
over arbitrary surfaces. In Proc. I3D 01. pp.227-232. 2001. 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2343014</article_id>
		<sort_key>1180</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>101</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[Coarse irradiance estimation using curvilinear skeleton]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2342896.2343014</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2343014</url>
		<abstract>
			<par><![CDATA[<p>Light flux can take complex paths to reach the observer, especially in large and sophisticated scenes. In such situations, global illumination algorithms are used: irradiance caching, path tracing and radiosity allow to precisely compute the irradiance at any point of the scene, but remain quite slow. Our present work aims to evaluate, very coarsely, the irradiance of any point of a scene, allowing to give global illumination algorithms some information about the main direction and intensity of the light flux. We use a coarse representation of the scene using a skeleton of its voids (where the light propagates). We present two heuristics to compute a coarse radiance estimation and the main light stream orientation. Our work gives the possibility to define many other heuristics.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[global illumination]]></kw>
			<kw><![CDATA[irradiance]]></kw>
			<kw><![CDATA[skeletonization]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3737558</person_id>
				<author_profile_id><![CDATA[81504686390]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Laurent]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[No&#235;l]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[UPE MLV, LIGM]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[lnoel@etudiant.univ-mlv.fr]]></email_address>
			</au>
			<au>
				<person_id>P3737559</person_id>
				<author_profile_id><![CDATA[81453662621]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[John]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Chaussard]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[UPE MLV, LIGM]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[chaussaj@esiee.fr]]></email_address>
			</au>
			<au>
				<person_id>P3737560</person_id>
				<author_profile_id><![CDATA[81442615071]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Venceslas]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Biri]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[UPE MLV, LIGM]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1375728</ref_obj_id>
				<ref_obj_pid>1375714</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Eisemann, E., and D&#233;coret, X. 2008. Single-pass GPU Solid Voxelization and Applications. In <i>GI '08: Proceedings of Graphics Interface 2008</i>, vol. 322, 73--80.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Coarse Irradiance Estimation using Curvilinear Skeleton Laurent No¨el* John Chaussard Venceslas Biri 
UPE MLV, LIGM UPE MLV, LIGM UPE MLV, LIGM  Figure 1: Left. Ground truth image. Center : backward irradiance 
estimation and the skeleton, red points are used for comparison. Right : several screenshots showing 
direction, a different point of view in the scene and an exemple with multiple lights Keywords: global 
illumination, irradiance, skeletonization 1 Introduction Point A B C D E F G Reference 1.05 0.23 0.10 
0.08 0.008 0.01 4e-3 backward 1.17 0.26 0.14 0.17 0.036 0.04 0.03 forward 1.17 0.22 0.10 0.01 0.005 9e-3 
0.01  Light .ux can take complex paths to reach the observer, especially in large and sophisticated 
scenes. In such situations, global illumi­nation algorithms are used : irradiance caching, path tracing 
and radiosity allow to precisely compute the irradiance at any point of the scene, but remain quite slow. 
Our present work aims to evalu­ate, very coarsely, the irradiance of any point of a scene, allowing to 
give global illumination algorithms some information about the main direction and intensity of the light 
.ux. We use a coarse repre­sentation of the scene using a skeleton of its voids (where the light propagates). 
We present two heuristics to compute a coarse radi­ance estimation and the main light stream orientation. 
Our work gives the possibility to de.ne many other heuristics. 2 Coarse irradiance estimation Step 1 
-Voxelization We compute the voxelization of the scene using any real-time voxelization algorithm such 
as [Eisemann and D´ecoret 2008]. After the skeletonization step (next step), we com­pute the closest 
skeleton node of each scene s voxel. Step 2 -Skeletonization We compute the curvilinear skeleton using 
a novel approach based on cubical complexes. The skeleton is automatically .ltered using the distance 
map and the opening function of the discrete scene, and is forced to pass by the scene s lights and camera. 
The result is a one-dimensional pruned skeleton with the same visual aspect than the scene, which can 
be seen as a graph. We then compute the shortest path, along the skeleton, between any skeleton s node 
and any scene s light. Step 3 -Heuristics for irradiance computation We present two simple heuristics 
to estimate both radiance and main light stream orientation : a backward and a forward estimation. The 
backward estimation starts from a node nc, and seeks the farthest visible node ne along the shortest 
path, on the skeleton, towards light l. We then compute the distance dall = d(l, ne)+ d(nc,ne) to obtain 
the .­nal irradiance B = L0/(p * d2 being the radiance of the all), L0 source. The forward estimation 
dispatches the .ux of light along *e-mail: lnoel@etudiant.univ-mlv.fr e-mail: chaussaj@esiee.fr Table 
1: Irradiance (W/m2) computed in several scene points the skeleton, using a breadth .rst search starting 
from the skeleton node corresponding to the light. The .ux arriving at a node is mul­tiplied by a factor 
depending on the relative size of the maximal balls between adjacent nodes. This .ux is divided by the 
square of the total distance along the skeleton to give the .nal irradiance. 3 Results and discussion 
We implemented a simple openGL visualization of our heuristics as shown in .gure 1. Our evaluation (step 
3) took an average of 3.5s per light for the backward heuristic and 812ms for the forward one (using 
a 2.26GHz P8400 with 3GB RAM). The voxelization costs 123MB and the skeleton is only 1.56MB. We will 
provide an open source OpenGL/GLSL implementation of both heuristics. We compare our estimation with 
a traditional path tracer (we use Embree from Intel) in various points, directly lighted or not. Both 
heuristics give a coarse estimation of radiance since the re.ectiv­ity of surface is considered as homogeneous, 
and more importantly, any node can perform a diffuse re.ection in any direction even if its border is 
empty. However, results show that our evaluation diverge only in very dark areas mainly due to the last 
mentionned limita­tion. We believe that such information can be valuable especially for global illumination 
algorithms. Note that backward heuristic can be used as an upper bound to irradiance estimation. The 
esti­mation, on each skeleton node, of an average oriented re.ectivity will improve the correctness of 
our method. Our work is also de­signed to accept new heuristics. References EISEMANN, E., AND DECORET´ 
, X. 2008. Single-pass GPU Solid Voxelization and Applications. In GI 08: Proceedings of Graphics Interface 
2008, vol. 322, 73 80. Copyright is held by the author / owner(s). SIGGRAPH 2012, Los Angeles, California, 
August 5 9, 2012. ISBN 978-1-4503-1435-0/12/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2343015</article_id>
		<sort_key>1190</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>102</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>5</seq_no>
		<title><![CDATA[Consistent stylization of stereoscopic 3D images]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2342896.2343015</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2343015</url>
		<abstract>
			<par><![CDATA[<p>The application of stylization filters to photographs is common, Instagram being a popular recent example. These image manipulation applications work great for 2D images. However, stereoscopic 3D cameras are increasingly available to consumers (Nintendo 3DS, Fuji W3 3D, HTC Evo 3D). How will users apply these same stylizations to stereoscopic images?</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[non-photorealistic rendering]]></kw>
			<kw><![CDATA[stereoscopic 3D]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.2.10</cat_node>
				<descriptor>3D/stereo scene analysis</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor>Display algorithms</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>J.5</cat_node>
				<descriptor>Fine arts</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010469.10010471</concept_id>
				<concept_desc>CCS->Applied computing->Arts and humanities->Performing arts</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010469.10010470</concept_id>
				<concept_desc>CCS->Applied computing->Arts and humanities->Fine arts</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P3737561</person_id>
				<author_profile_id><![CDATA[81466647327]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Lesley]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Northam]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[lanortha@yahoo.ca]]></email_address>
			</au>
			<au>
				<person_id>P3737562</person_id>
				<author_profile_id><![CDATA[81100185028]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Paul]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Asente]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[asente@adobe.com]]></email_address>
			</au>
			<au>
				<person_id>P3737563</person_id>
				<author_profile_id><![CDATA[81100155157]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Craig]]></first_name>
				<middle_name><![CDATA[S.]]></middle_name>
				<last_name><![CDATA[Kaplan]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[csk@uwaterloo.ca]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>280951</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Hertzmann, A. 1998. Painterly rendering with curved brush strokes of multiple sizes. In <i>Proceedings of the 25th annual conference on Computer graphics and interactive techniques</i>, ACM, New York, NY, USA, SIGGRAPH '98, 453--460.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Northam, L., Asente, P., and Kaplan, C. S., 2012. Consistent stylization and painterly rendering of stereoscopic 3D images.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2030462</ref_obj_id>
				<ref_obj_pid>2030441</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Richardt, C., &#346;wirski, L., Davies, I., and Dodgson, N. A. 2011. Predicting stereoscopic viewing comfort using a coherence-based computational model. In <i>Proceedings of Computational Aesthetics (CAe)</i>, D. Cunningham and T. Isenberg, Eds.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Stavrakis, E., and Gelautz, M. 2005. Stereoscopic painting with varying levels of detail. In <i>In Proceedings of SPIE - Stereoscopic Displays and Virtual Reality Systems XII</i>, 5564.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Consistent Stylization of Stereoscopic 3D Images LesleyNortham:, Paul Asente , Craig S. Kaplan Figure 
1: The right view, left view, and anaglyph of a stereoscopic image stylized using our methods.1 CR Categories: 
I.2.10 [Arti.cial Intelligence]:Vision and Scene Understanding 3D/stereo scene analysis; I.3.3 [Computer 
Graph­ics]: Picture/Image Generation Display algorithms; J.5 [Com­puter Applications]: Arts and Humanities 
Fine Arts; Keywords: non-photorealistic rendering, stereoscopic 3D 1 Introduction The application of 
stylization .lters to photographs is common, In­stagram beingapopular recentexample. These image manipulation 
applications work great for 2D images. However, stereoscopic 3D cameras are increasingly available to 
consumers (Nintendo 3DS, Fuji W3 3D, HTC Evo 3D). How will users apply these same styl­izations to stereoscopic 
images? Astereoscopic 3D photograph is composed of left and right views. One method to stylize a 3D photo 
is to apply a given .lter inde­pendently to its left and right views. However, as pointed out by Richardt 
et al. [2011], this method introduces inconsistencies be­tween the images which cause ocular pain. Previous 
work has adapted speci.c stylization algorithms, such as painterly rendering, to stereoscopic 3D images[Stavrakis 
and Gelautz 2005]. However, there does not exist to the best of our knowledge a more general method to 
stylize stereoscopic 3D im­ages. Additionally, while many existing methods for stereoscopic 3D stylization 
reduce inconsistencies between left and right views, they do not eliminate them. We present a method 
for consistent stylization of stereoscopic 3D images that works with many.lters from Photoshop, and even 
painterly rendering algorithms such as Hertzmann s[1998]. 2 Method Given a stereoscopic 3D image composed 
of left and right views and a corresponding pair of pre-computed left and right disparity maps our method 
works as follows. First, use the disparity maps to decompose the left and right views into layers Ld 
and Rd, where d is a disparity value. Then, for each disparity value d, merge Ld and Rd to create a merged 
view Md. Note that there are no occluded *e-mail: lanortha@yahoo.ca e-mail:asente@adobe.com e-mail:csk@uwaterloo.ca 
pixels in Md. Next, apply a stylization .lter to each merged view Md and split each image back into left 
and right views L ' d and Rd' . Finally, the stylized left and right views are composited into the .nal 
left and right views of a stereoscopic 3D image. 3 Results We tested our consistent stylization approach 
using a variety of Adobe Photoshop .lters. Many stylization .lters, such as rough canvas, mosaic, angled 
strokes, and under-painting work well within our method, producing comfortable stylized stereoscopic 
3D images. However, .lters such as sumi-e and watercolor, which use global image properties, do not work 
well in general. Speci.cally, we observed that when .lters use global statistics like colour, lu­minosity, 
or contrast, our .nal stylized image was consistent but exhibited visible layering. With small modi.cations 
to the styliza­tion algorithm, or to the merged views Md, the layering can be reduced or eliminated for 
non-stroke based .lters. More involved stylization algorithms, such as stroke-based painterly rendering 
al­gorithms, may require further adjustments to reduce early stroke termination and reduce sharp edges 
between Md layers[Northam et al. 2012]. We conducted a user study to evaluate our algorithm. 85% of par­ticipants 
found our results more comfortable to view than those of other stereoscopic stylization approaches. References 
HERTZMANN, A. 1998. Painterly rendering with curved brush strokes of multiple sizes. In Proceedings of 
the 25th annual con­ ference on Computer graphics and interactive techniques,ACM, NewYork,NY, USA, SIGGRAPH 
98, 453 460. NORTHAM,L.,ASENTE,P., AND KAPLAN,C.S., 2012. Consis­tent stylization and painterly rendering 
of stereoscopic 3D im­ages. ´RICHARDT, C., SWIRSKI, L., DAVIES, I., AND DODGSON, N. A. 2011. Predicting 
stereoscopic viewing comfort using a coherence-based computational model. In Proceedings of Com­ putational 
Aesthetics (CAe), D. Cunningham and T. Isenberg, Eds. STAVRAKIS, E., AND GELAUTZ, M. 2005. Stereoscopic 
paint­ing with varying levels of detail. In In Proceedings of SPIE -Stereoscopic Displays and Virtual 
Reality Systems XII, 5564. Copyright is held by the author / owner(s). SIGGRAPH 2012, Los Angeles, California, 
August 5 9, 2012. ISBN 978-1-4503-1435-0/12/0008 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2343016</article_id>
		<sort_key>1200</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>103</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>6</seq_no>
		<title><![CDATA[Distance aware ray tracing for curves]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2342896.2343016</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2343016</url>
		<abstract>
			<par><![CDATA[<p>The "curves" primitive provides a way to define ribbon-like objects such as fur and hair. Ray tracing thin primitives is a difficult problem. It is possible to compute ray/curve intersections without tessellation [Nakamaru and Ohno 2002], but it requires many samples to reduce aliasing. On the other hand, GPU/rasterization based methods incorporate the concept of an A-buffer for correctly accumulating fragments' colors according to their coverages [Yu et al. 2012].</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3737564</person_id>
				<author_profile_id><![CDATA[81430652065]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Koji]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nakamaru]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Light Transport Entertainment Research]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[nakamaru@lighttransport.com]]></email_address>
			</au>
			<au>
				<person_id>P3737565</person_id>
				<author_profile_id><![CDATA[81504684075]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Toru]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Matsuoka]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Light Transport Entertainment Research]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[toru@lighttransport.com]]></email_address>
			</au>
			<au>
				<person_id>P3737566</person_id>
				<author_profile_id><![CDATA[81504686949]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Masahiro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Fujita]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Light Transport Entertainment Research]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[syoyo@lighttransport.com]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>311555</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Igehy, H. 1999. Tracing ray differentials. In <i>Proceedings of the 26th annual conference on Computer graphics and interactive techniques</i>, ACM Press/Addison-Wesley Publishing Co., New York, NY, USA, SIGGRAPH '99, 179--186.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Nakamaru, K., and Ohno, Y. 2002. Ray tracing for curves primitive. In <i>Journal of WSCG (WSCG '2002 Proceedings)</i>, V. Skala, Ed., vol. 10, 311--316. ISSN 1213-6980.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1141913</ref_obj_id>
				<ref_obj_pid>1179352</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Wald, I., Ize, T., Kensler, A., Knoll, A., and Parker, S. G. 2006. Ray tracing animated scenes using coherent grid traversal. In <i>ACM SIGGRAPH 2006 Papers</i>, ACM, New York, NY, USA, SIGGRAPH '06, 485--493.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2159635</ref_obj_id>
				<ref_obj_pid>2159616</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Yu, X., Yang, J. C., Hensley, J., Harada, T., and Yu, J. 2012. A framework for rendering complex scattering effects on hair. In <i>Proceedings of the ACM SIGGRAPH Symposium on Interactive 3D Graphics and Games</i>, ACM, New York, NY, USA, I3D '12, 111--118.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Distance Aware Ray Tracing for Curves Koji Nakamaru,Toru Matsuoka, Masahiro Fujita LightTransport Entertainment 
Research*  (a) (b) (c) (d) Figure 1: (a) Our approachwith one sample per pixel, 5.83 min. (b)8×8samples 
per pixel, 11.35 min. (c) 16×16 samples per pixel, 47.42 min. Our approach achieves a smooth result while 
others are noisy regardless of many samples. (d) Re.ection/refraction is also supported. 1 Introduction 
The curves primitive provides a way to de.ne ribbon-like ob­jects such as fur and hair. Ray tracing thin 
primitives is a dif.cult problem. It is possible to compute ray/curve intersections without tessellation[Nakamaru 
and Ohno 2002],butit requires many sam­ples to reduce aliasing. On the other hand, GPU/rasterization 
based methods incorporate the concept of an A-buffer for correctly accu­mulating fragments colors according 
to their coverages[Yu et al. 2012]. In this work, we propose distance-aware ray tracing for curves, and 
combine it with the concept of an A-buffer for smooth rendering with low samples. Distance-aware means 
that the method can support not only actual intersectionsbut also distances to curves. We use ray differentials[Igehy1999]and 
distances to approximate curves coverages and accumulate their colors and opacities in the correct order. 
 2 Our Approach The original ray-curves intersection algorithm utilizes a ribbon width as a margin for 
culling sub-curves generated in subdivision. We instead use a largest ray differential that can be easily 
deter­mined by computing ray differentials at near/far z positions of the curve. After the curve is fully 
subdivided, we compute the distance to the survived sub-curve and corresponding ray differentials. In 
our current implementation, the coverage is then approximated as follows: // uw: half the width of the 
ribbon. // d: distance to the ribbon s central axis. // r: ray extent s radius based on ray differentials. 
d0 =d -uw; d1 = d + uw; if (d0 >= 0) // outside coverage = (min(d1 / r, 1.0) -min(d0 / r, 1.0)) * 0.5; 
else // inside coverage = (min(d1 / r, 1.0) + min(-d0 / r, 1.0)) * 0.5; where coverages larger than 
zero are returned with other informa­tion and they are added to the (pseudo) intersection list. For rendering 
manycurves, we have to take into account ray differ­entials in traversing an acceleration structure. 
We currently utilize *e-mail:{nakamaru, toru, syoyo}@lighttransport.com a uniform grid and traverse 
it in a manner similar to the slice-based packet traversal[Wald et al. 2006]. By incrementally computing 
ray differentials (projected onto the plane perpendicular to the ma­jor traversal axis), we determine 
the current ray s extent and cor­responding cells. The grid traversal allows for nearly front-to-back 
checking order, however it doesn tguarantee that the intersection list is sorted. Sort­ing the list for 
every traversal step is too expensive, so we sort it once every .ve traversal steps. The .nal contribution 
of each in­tersection is then determined while accumulating opacity as well. Weterminatethetraversalandavoid 
subsequent intersection testsif opacityfalls below some small threshold. Similarly, we alsoavoid tracing 
re.ection/refraction/shadow raysifthe .nal contributionis suf.ciently small. These adaptive methods greatly 
reduce traversal steps and intersection tests. Figure1shows several results.Weachieveasmooth renderingeven 
with one sample per pixel. Although the cost of each sample is large, our approach outperforms traditional 
methods when the scene contains many.ne details. References IGEHY,H. 1999.Tracing ray differentials. 
In Proceedings of the 26th annual conference on Computer graphics and interactive techniques, ACM Press/Addison-Wesley 
Publishing Co., New York, NY, USA, SIGGRAPH 99, 179 186. NAKAMARU, K., AND OHNO, Y. 2002. Ray tracing 
for curves primitive. In Journal of WSCG (WSCG 2002 Proceedings), V. Skala, Ed., vol. 10, 311 316. ISSN 
1213-6980. WALD, I., IZE, T., KENSLER, A., KNOLL, A., AND PARKER, S. G. 2006. Ray tracing animated scenes 
using coherent grid traversal. In ACM SIGGRAPH 2006Papers,ACM, NewYork, NY, USA, SIGGRAPH 06, 485 493. 
 YU, X., YANG, J. C., HENSLEY, J., HARADA, T., AND YU, J. 2012.Aframeworkfor rendering complex scatteringeffects 
on hair. In Proceedingsof theACM SIGGRAPH Symposium on In­teractive 3D Graphics and Games,ACM,NewYork,NY, 
USA, I3D 12, 111 118. Copyright is held by the author / owner(s). SIGGRAPH 2012, Los Angeles, California, 
August 5 9, 2012. ISBN 978-1-4503-1435-0/12/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2343017</article_id>
		<sort_key>1210</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>104</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>7</seq_no>
		<title><![CDATA[Efficient terrain &#38; ocean rendering for a real size planet]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2342896.2343017</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2343017</url>
		<abstract>
			<par><![CDATA[<p>Full-scale planetary rendering is a broad and active area of research. Its uses range from games to astronomy and simulation applications. Many problems arise when attempting to render both high altitude and close-up scenes of a full-scale planet, especially when the planet has an ocean surface and even more if water surface displacement and refractions in shallow water are required. Some of the most common problems to overcome are: Performance (especially for close-up scenes of shallow water), Z-fighting (between the ocean and the terrain surface in high altitude scenes) and a seamless transition of the shorelines from high to low altitude.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3737567</person_id>
				<author_profile_id><![CDATA[81504683043]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Silviu]]></first_name>
				<middle_name><![CDATA[S.]]></middle_name>
				<last_name><![CDATA[Andrei]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Glen Ellyn]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[silviu.andrei@gmail.com]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Efficient Terrain &#38; Ocean Rendering for a Real Size Planet Silviu S. Andrei Glen Ellyn, USA silviu.andrei@gmail.com 
 (a) Refraction map (b) Terrain &#38; Ocean (c) Final image 1. Introduction Full-scale planetary rendering 
is a broad and active area of research. Its uses range from games to astronomy and simulation applications. 
Many problems arise when attempting to render both high altitude and close-up scenes of a full-scale 
planet, especially when the planet has an ocean surface and even more if water surface displacement and 
refractions in shallow water are required. Some of the most common problems to overcome are: Performance 
(especially for close-up scenes of shallow water), Z­fighting (between the ocean and the terrain surface 
in high altitude scenes) and a seamless transition of the shorelines from high to low altitude. In this 
work I will present a visibility culling and LOD method which is targeted to solve or minimize all of 
the above mentioned problems. 2. The method As a prerequisite I should mention that the terrain in my 
implementation is maintained in a quad tree structure where each node has 29x29 vertices and a 256x256 
terrain height-map. The same nodes are used for rendering both the terrain and the ocean surface which 
is displaced by a water height-map. For the water refractions, the scene is rendered in a traditional, 
two pass way. The first pass is used to render the underwater scene in a refraction texture and the second 
pass is used to render the terrain and ocean surface. The main problem when rendering the shorelines 
from high altitude is that they will be defined by the intersection of the ocean surface polygons and 
the terrain polygons which have a very coarse level of tessellation at high altitudes. Also, the popping 
effect when the nodes are split is very visible across the shorelines because of the same reason. In 
order to solve this issue, the ocean is rendered differently when viewed from high altitude. Instead 
of rendering 2 surfaces, only the terrain surface is rendered, but all vertices that lie below the sea 
level, are raised to the sea level and all pixels that (according to the terrain height­map) lie below 
the sea level are rendered using the water material ID. This way, there will be no z-fighting and the 
shorelines will be defined by the node s height-map, not by the tessellation level of the terrain nodes. 
Also, a transition from high altitude to ground-view rendering is required for frames which contain both 
pixels rendered using the space-view technique and pixels rendered using the ground-view technique. This 
transition is achieved by clipping out space-view pixels that are further away from the camera than a 
given threshold and clipping in ground­view pixels that are closer than the threshold. The space-view 
rendering method also ignores ray bending due to refraction, but from high altitude, the difference is 
negligible and therefore the border between the two rendering methods is not visible. A significant gain 
in speed is obtained by culling away geometry that does not need to be rendered. For example, for the 
refraction map, any geometry below the maximum water visibility depth or above the maximum wave height 
can be clipped away; for the terrain surface, any geometry below the minimum wave height can be clipped 
away and for the ocean surface, any geometry above the maximum wave height can also be clipped away. 
These geometries can be culled in 2 stages. The first stage rejects entire nodes on the CPU side based 
on the minimum and maximum vertex height of the node and the second stage rejects entire triangles in 
the geometry shader based on the triangle s vertices. To accelerate rendering even more, the terrain 
and ocean nodes are rendered using the same draw call. The geometry shader will later decide if it needs 
to emit a triangle for the ocean surface, terrain surface, both or let the pixel shader decide based 
on the node s terrain height-map. When the geometry shader emits a triangle it attaches a material ID 
to the vertex output structure to let the pixel shader know which rendering method to use. Pseudo-code 
of the geometry shader: if (triangle max distance to camera > distance to camera threshold) { Set MaterialID 
to unspecified for all 3 vertices; Set each vertex height to sea level if it is below sea level; Emit 
triangle; } if (triangle min distance to camera < distance to camera threshold + delta) { if (triangle 
max height > min wave height) { Set MaterialID to terrain for all 3 vertices; Emit triangle; } if (triangle 
min height < max wave height) { Set each vertex height to: sea level + water height-map value; Set MaterialID 
to water for all 3 vertices; Emit triangle; } } delta = number chosen to create a small overlap between 
the high altitude rendering and the low altitude rendering method Copyright is held by the author / owner(s). 
SIGGRAPH 2012, Los Angeles, California, August 5 9, 2012. ISBN 978-1-4503-1435-0/12/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2343018</article_id>
		<sort_key>1220</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>105</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>8</seq_no>
		<title><![CDATA[Fast multi-image-based photon tracing with grid-based gathering]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2342896.2343018</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2343018</url>
		<abstract>
			<par><![CDATA[<p>We developed a real-time solution for approximate global illumination, where the temporal cost in complex scenes is dramatically reduced. Our approach initially traces photon-rays with multiple cube-maps, and then gathers the irradiance of photons using uniform grids filled with low-order spherical harmonics. Numerous global illumination effects can be rendered efficiently in our framework.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[photon tracing and real-time global illumination]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P3737568</person_id>
				<author_profile_id><![CDATA[81539448556]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Yun]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Fei]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tsinghua University, P. R. China]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[fyun@acm.org]]></email_address>
			</au>
			<au>
				<person_id>P3737569</person_id>
				<author_profile_id><![CDATA[81450595008]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Bin]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tsinghua University, P. R. China]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[wangbins@tsinghua.edu.cn]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1730821</ref_obj_id>
				<ref_obj_pid>1730804</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Kaplanyan, A., and Dachsbacher, C. 2010. Cascaded light propagation volumes for real-time indirect illumination. In <i>Proceedings of the I3D '10</i>, 99--107.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Szirmay-Kalos, L., Asz&#243;di, B., Laz&#225;nyi, I., and Premecz, M. 2005. Approximate ray-tracing on the gpu with distance impostors. <i>Comput. Graph. Forum 24</i>, 3, 695--704.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Yao, C., Wang, B., Chan, B., Yong, J., and Paul, J.-C. 2010. Multi-image based photon tracing for interactive global illumination of dynamic scenes. <i>Comput. Graph. Forum 29</i>, 4, 1315--1324.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Fast Multi-image-based Photon Tracing with Grid-based Gathering Yun Fei* School of Software, Tsinghua 
University, P. R. China Abstract We developed a real-time solution for approximate global illumi­nation, 
where the temporal cost in complex scenes is dramatically reduced. Our approach initially traces photon-rays 
with multiple cube-maps, and then gathers the irradiance of photons using uni­form grids .lled with low-order 
spherical harmonics. Numerous global illumination effects can be rendered ef.ciently in our frame­work. 
CR Categories: I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism Real-time Global Illumination; 
Keywords: Photon tracing and Real-time global illumination 1 Problem Formation Ray-tracing and light-gathering 
are the two key steps in photon mapping. Recent method focusing on multi-image-based photon mapping [Yao 
et al. 2010] works well in simple scenes, but has se­ rious limitations in complex ones due to: a) they 
intersect a photon­ray iteratively with distance imposters (cube maps), where, how­ever, the initialization 
can be far from accurate, which leads the it­erative re.nement to be failed (Figure 1); b) they intersect 
a photon­ray with all the distance imposters in the scene, which is costly and unnecessary; c) the method 
for radiance estimation called splatting, is not ef.cient when there are large amount of photons, since 
splat­ting millions of photons will need the rasterization of billions of pixels. 2 Technical Approach 
In the ray-tracing phase, to initialize a more accurately, we intro­duce a cascaded partition and ray-casting 
scheme. The searching space of the intersection point is uniformly divided. We then cast a ray in each 
subdivided space and minimize an energy across the searching space. The intersection with minimal energy 
will be cho­sen as the initialized point. This approach has been proven to be much more accurate than 
previous methods. We then .nd an ex­act solution from this initialization following the work proposed 
by Szirmay-Kalos et al. [2005]. Only two cube maps are accessed in our method by each photon-ray, which 
makes our performance in the ray-tracing phase an order of magnitude higher than the existing methods. 
In the radiance gathering phase, the radiance of each photon is rep­resented with a spherical harmonics 
(SH) vector, which is later in­jected and accumulated into two types of uniform grids covering the scene. 
One grid called the light-gathering volume, or LGV, is a 3D grid used to gather smoothed (low-frequency) 
light such as diffuse lighting. Since gathering higher frequency details such as caustics needs a higher 
resolution grid, which is more ef.cient to be in 2D rather than in 3D, we propose a high-resolution 2D 
grid to gather the more detailed light, called screen-space LGV (SSLGV). After injecting the radiance 
of photons into the two grids, we propagate *e-mail: fyun@acm.org e-mail: wangbins@tsinghua.edu.cn Bin 
Wang School of Software, Tsinghua University, P. R. China Figure 1: Without pre-computation, our technique 
renders dy­namic global illumination effects in a complex scene in real-time (19 ~ 56Hz on a GTX480). 
the radiance in the grids following the light propagation framework proposed by Kaplanyan et al. [2010], 
which is actually equivalent to radiance gathering. Finally we use the gathered radiance in the grids 
to illuminate the scene. 3 Limitations for Future Works The artifacts pervasive in existing lattice-based 
methods are tempo­ral discontinuity and light bleeding. Besides, the rendering quality of some .attened 
middle -frequency caustics is still very photon­dependent where tracing billions of photons is still 
unavailable in real-time applications.  References KAPLANYAN, A., AND DACHSBACHER, C. 2010. Cascaded 
light propagation volumes for real-time indirect illumination. In Pro­ceedings of the I3D 10, 99 107. 
SZIRMAY-KALOS, L., ASZ ´ B., LAZ ´ I., AND PRE- ODI, ANYI, MECZ, M. 2005. Approximate ray-tracing on 
the gpu with dis­tance impostors. Comput. Graph. Forum 24, 3, 695 704. YAO, C., WANG, B., CHAN, B., YONG, 
J., AND PAUL, J.-C. 2010. Multi-image based photon tracing for interactive global illumination of dynamic 
scenes. Comput. Graph. Forum 29, 4, 1315 1324. Copyright is held by the author / owner(s). SIGGRAPH 2012, 
Los Angeles, California, August 5 9, 2012. ISBN 978-1-4503-1435-0/12/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2343019</article_id>
		<sort_key>1230</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>106</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>9</seq_no>
		<title><![CDATA[Interactive generation of (paleontological) scientific illustrations from 3D-models]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2342896.2343019</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2343019</url>
		<abstract>
			<par><![CDATA[<p>Scientific illustrations play an important role in the research of natural sciences and are complex drawings that are manually created. The desired images usually combine several drawing techniques such as outlines, distinctive types of stippling or an abstract shaded surface in a single image, as is exemplified in figure 2. Each of these elements aim to focus the viewers attention to certain details like e.g. shape, curvature or surface structure. As to our knowledge no tool exists with which similar images can be produced and the creation of such an illustration is a tedious and time-consuming task, we examined paleontological scientific illustrations together with researchers and artists from <i>Senckenberg Research Institute and Natural History Museum</i> with the intention to create an application to render scientific illustrations from a 3D-model.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3737570</person_id>
				<author_profile_id><![CDATA[81461644546]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Sebastian]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sch&#228;fer]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Goethe-Universit&#228;t Frankfurt]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737571</person_id>
				<author_profile_id><![CDATA[81504688570]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Christoph]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Knopp]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Goethe-Universit&#228;t Frankfurt]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737572</person_id>
				<author_profile_id><![CDATA[81548018659]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Detlef]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kr&#246;mker]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Goethe-Universit&#228;t Frankfurt]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1141916</ref_obj_id>
				<ref_obj_pid>1141911</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Kopf, J., Cohen-Or, D., Deussen, O., and Lischinski, D. 2006. Recursive Wang tiles for real-time blue noise. <i>ACM Transactions on Graphics 25</i>, 3 (July), 509.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1142016</ref_obj_id>
				<ref_obj_pid>1141911</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Luft, T., Colditz, C., and Deussen, O. 2006. Image enhancement by unsharp masking the depth buffer. <i>ACM Transactions on Graphics 25</i>, 3, 1206.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Schmaltz, C., Gwosdek, P., and Bruhn, A. 2010. Electrostatic Halftoning. <i>Computer Graphics</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Interactive generation of (paleontological) scienti.c illustrations from 3D-models Sebastian Sch¨omker 
afer, Christoph Knopp, Detlef Kr¨Goethe-Universit¨at Frankfurt Figure 1: A Leptictidium nesutum tooth 
left upper M2. Layers: illumination, outline, period stippling, irregular stippling and .nal image. 1 
Introduction Scienti.c illustrations play an important role in the research of nat­ural sciences and 
are complex drawings that are manually created. The desired images usually combine several drawing techniques 
such as outlines, distinctive types of stippling or an abstract shaded surface in a single image, as 
is exempli.ed in .gure 2. Each of these elements aim to focus the viewers attention to certain details 
like e.g. shape, curvature or surface structure. As to our knowl­edge no tool exists with which similar 
images can be produced and the creation of such an illustration is a tedious and time-consuming task, 
we examined paleontological scienti.c illustrations together with researchers and artists from Senckenberg 
Research Institute and Natural History Museum with the intention to create an appli­cation to render 
scienti.c illustrations from a 3D-model. Figure 2: A Kopidodon head drawn by Juliane Eberhardt. As a 
result we introduce a novel layer-based approach to in­teractively create scienti.c illustrations from 
a 3D-model with well-proven Non-Photorealistic-Rendering algorithms. The whole pipeline is designed to 
run on the GPU to achieve a real-time ren­dering by relying on screen-space and image-based methods. 
 2 Artist Interaction While it is imperative that the user can freely combine the afore­mentioned techniques, 
such a tool must also offer the user other possibilities to interact with the result. It is quite common 
that the object depicted is an abstraction from the real specimen at hand. The artist should therefor 
be able to sculpt the base model into the idealized form he imagines. Another direct interaction is the 
addition of manually drawn im­ages or the completion and correction of automated results. There are two 
reasons why this interaction is of high importance. First of all the result of any algorithm might not 
fully satisfy the taste of the artist and he or she wants to correct e.g. some outlines or stippling 
points. The other reason is that not all features can be successfully detected by algorithms. This was 
especially true for curvature de­tecting algorithms that did not produce satisfying results. 3 Our approach 
In the .rst step the 3D-model is rendered in a single render-pass into G-Buffer textures as these are 
a good starting point for image space GPU algorithms. Every layer can be freely con.gured by the user 
with respect to it s contents, source and how it is rendered into the .nal image. Once all user chosen 
layers are rendered they are drawn with alpha blending into a .nal texture that is then shown with an 
applied FXAA anti-aliasing .lter. The work.ow is as follows: 1. Load model, adjust lighting and pick 
desired view. 2. Setup layers: select characteristic and how to show it. 3. Manually .ne-tune the parameters, 
curvatures or stippling. 4. Store the setup and export a screenshot.  To render each layer we implemented 
state-of-the-art NPR tech­niques that can be executed on the GPU. Outlines are created by Image Enhancement 
[Luft et al. 2006] or an edge detection .lter applied to normal-and depth-map. Stippling points are set 
regu­larly using Electrostatic Halftoning [Schmaltz et al. 2010] and ir­ regular using Recursive Wang 
tiles for real-time blue noise [Kopf et al. 2006]. The shaper function is realized by painting on a displacement 
map. The 2D-painting is implemented by painting onto additional surface textures. The painting mechanism 
can also be used to explicitly move, add or remove single stippling points. References KOPF, J., COHEN-OR, 
D., DEUSSEN, O., AND LISCHINSKI, D. 2006. Recursive Wang tiles for real-time blue noise. ACM Trans­actions 
on Graphics 25, 3 (July), 509. LUFT, T., COLDITZ, C., AND DEUSSEN, O. 2006. Image en­hancement by unsharp 
masking the depth buffer. ACM Transac­tions on Graphics 25, 3, 1206. SCHMALTZ, C., GWOSDEK, P., AND BRUHN, 
A. 2010. Electro­static Halftoning. Computer Graphics. Copyright is held by the author / owner(s). SIGGRAPH 
2012, Los Angeles, California, August 5 9, 2012. ISBN 978-1-4503-1435-0/12/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2343020</article_id>
		<sort_key>1240</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>107</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>10</seq_no>
		<title><![CDATA[Multi-resolution depth-of-field rendering]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2342896.2343020</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2343020</url>
		<abstract>
			<par><![CDATA[<p>Depth of field (DOF) refers to a distance range in which photographic capture yields acceptable sharp imagery. The DOF effect is crucial in improving the perceptual realism of synthetic images and drawing user's attention. In graphics, object-based approaches served as reference [Cook et al. 1984; Haeberli and Akeley 1990]. However, their low performance, resulting from the repeated rendering for different lens samples, has impeded their real-time use.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3737573</person_id>
				<author_profile_id><![CDATA[81504686115]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Yuna]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Jeong]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Sungkyunkwan University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[jeongyuna@skku.edu]]></email_address>
			</au>
			<au>
				<person_id>P3737574</person_id>
				<author_profile_id><![CDATA[81504686348]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Kangtae]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kim]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Sungkyunkwan University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[sonata@skku.edu]]></email_address>
			</au>
			<au>
				<person_id>P3737575</person_id>
				<author_profile_id><![CDATA[81423594461]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Sungkil]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lee]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Sungkyunkwan University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[sungkil@skku.edu]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>808590</ref_obj_id>
				<ref_obj_pid>800031</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Cook, R., Porter, T., and Carpenter, L. 1984. Distributed ray tracing. In <i>Proc. ACM SIGGRAPH</i>, vol. 18, ACM, 137--145.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>97913</ref_obj_id>
				<ref_obj_pid>97879</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Haeberli, P., and Akeley, K. 1990. The accumulation buffer: Hardware support for high-quality rendering. In <i>Proc. ACM SIGGRAPH</i>, vol. 24, ACM, 309--318.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1778802</ref_obj_id>
				<ref_obj_pid>1778765</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Lee, S., Eisemann, E., and Seidel, H.-P. 2010. Real-Time Lens Blur Effects and Focus Control. <i>ACM Trans. Graphics (Proc. ACM SIGGRAPH'10) 29</i>, 4, 65:1--7.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<article_sponsors>
			<funding_agency>National Research Foundation of Korea</funding_agency>
			<grant_numbers>
				<grant_number>2011-0014015</grant_number>
			</grant_numbers>
		</article_sponsors>
	</article_rec>
	<article_rec>
		<article_id>2343021</article_id>
		<sort_key>1250</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>108</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>11</seq_no>
		<title><![CDATA[Pixelating vector line art]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2342896.2343021</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2343021</url>
		<abstract>
			<par><![CDATA[<p>Pixel art is a style of digital art in which images are edited on the pixel level. This art form emerged in the 1980s in many computer and video games as a way to make the best use of devices with limited graphics capabilities. As digital displays improve in resolution and colour range, digital art is no longer bound by these restrictions. However, pixel art remains popular, particularly in the gaming community, due to its retro charm.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3737576</person_id>
				<author_profile_id><![CDATA[81470655373]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Tiffany]]></first_name>
				<middle_name><![CDATA[C.]]></middle_name>
				<last_name><![CDATA[Inglis]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Waterloo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[piffany@gmail.com]]></email_address>
			</au>
			<au>
				<person_id>P3737577</person_id>
				<author_profile_id><![CDATA[81100155157]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Craig]]></first_name>
				<middle_name><![CDATA[S.]]></middle_name>
				<last_name><![CDATA[Kaplan]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Waterloo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[csk@uwaterloo.ca]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>2330153</ref_obj_id>
				<ref_obj_pid>2330147</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Inglis, T. C., and Kaplan, C. S. 2012. Pixelating vector line art. In <i>Proceedings of the international symposium on Non-photorealistic animation and rendering, to appear</i>, NPAR '12.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Pixelating Vector Line Art Tiffany C. Inglis Craig S. Kaplan University of Waterloo University of Waterloo 
piffany@gmail.com csk@uwaterloo.ca (a) Input vector line art (b) Pixelated output Figure 1: Our algorithm 
converts vector line art into pixel draw­ings. It also allows users to draw and edit the vector components 
while providing real-time feedback on the pixelated result. 1 Introduction Pixel art is a style of digital 
art in which images are edited on the pixel level. This art form emerged in the 1980s in many computer 
and video games as a way to make the best use of devices with limited graphics capabilities. As digital 
displays improve in res­olution and colour range, digital art is no longer bound by these restrictions. 
However, pixel art remains popular, particularly in the gaming community, due to its retro charm. We 
might naturally wish to create pixel art by rasterizing vector illustrations. However, software that 
creates satisfactory results still does not exist, and experts place pixels by hand. Adobe Illustrator 
and Adobe Photoshop are able to rasterize vector objects, but the results often contain jagged boundaries 
or missing pixels. In this work, we introduce a pixelation algorithm that rasterizes a given vector drawing 
consisting of lines and curves into a line drawing with single-pixel-wide paths [Inglis and Kaplan 2012] 
(see Figure 1). Unlike many rasterization techniques that focus on ef.­ ciency and high resolution results, 
our approach focuses on creating more aesthetically pleasing images by allowing the pixelated im­age 
to deviate slightly from input. To evaluate our results, we con­ducted a user study to collect human-generated 
pixel art and then asked the participants to compare computer-and human-generated images. The analysis 
shows that our algorithm outperforms com­mercial software and participants with less pixel art experience. 
 2 Approach To pixelate a curve, it is .rst divided into smooth monotonic curve segments. For each curve 
segment, we use a greedy algorithm to compute a piecewise polygonal approximation in which each line 
segment has slope n or 1/n for a positive integer n. Then each line segment is replaced by either a column 
or a row of pixels to obtain the pixelated curve corresponding to the curve segment. The basic algorithm 
we just described may produce jagged curves (see Figure 2b). To remove jaggies, we shift the curve segments 
so that they are better aligned with the pixel grid, and then the .nal pixelated curve is checked for 
slope monotonicity. For example, in Figure 2, since the vector curve is monotonically increasing in slope, 
the pixels in the corresponding pixelated curve are rearranged to re.ect this property. To evaluate our 
pixelation algorithm, we conducted a two-part user (a) Vector input (b) With jaggies (c) Without jaggies 
 Figure 2: Our algorithm uses slope-sorting to remove jaggies (shown in red) in curve segments with monotonic 
slope. study. In Part 1, participants were given eight vector images, each superimposed on a pixel grid, 
and asked to draw the correspond­ing pixel art. In Part 2, we merged the participants drawings with results 
generated by our algorithm as well as Adobe Illustrator and Adobe Photoshop (see Figures 3 and 4). Participants 
were then pre­ sented with pairs of pixel drawings and asked to choose which they preferred based on 
visual appeal and .delity. We had a total of more than 300 participants, ranging from amateurs to professional 
pixel artists. According to the results, our algorithm signi.cantly outperformed both Adobe Illustrator 
and Photoshop. However, the highest rated drawings came from pixel artists, be­cause they understand 
how to simplify certain features while main­taining readability at low resolutions. For future work, 
we are interested in developing techniques for other aspects of pixel art such as selecting relevant 
features to in­clude based on the resolution, choosing a suitable colour palette, applying artistic dithering 
and anti-aliasing, and creating isometric pixel art from 3D models. References INGLIS, T. C., AND KAPLAN, 
C. S. 2012. Pixelating vector line art. In Proceedings of the international symposium on Non­photorealistic 
animation and rendering, to appear, NPAR 12. (a) Vector input (b) Illustrator (c) Photoshop (d) Our 
algorithm Figure 3: Computer-generated pixel art drawings. Figure 4: Pixel art drawings by user study 
participants. Copyright is held by the author / owner(s). SIGGRAPH 2012, Los Angeles, California, August 
5 9, 2012. ISBN 978-1-4503-1435-0/12/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2343022</article_id>
		<sort_key>1260</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>109</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>12</seq_no>
		<title><![CDATA[Radiance filtering for interactive path tracing]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2342896.2343022</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2343022</url>
		<abstract>
			<par><![CDATA[<p>We present a method for noise reduction that is especially tailored to interactive progressive path tracing (PT). The idea is to exploit spatial coherence in the image and reuse information from neighboring pixels. However, in contrast to image filtering techniques (e.g. [Schwenk et al. 2012]), we do not simply filter pixel values or samples of outgoing radiance. Instead, we only reuse the incident indirect radiance of neighboring pixels in a radiance estimation step with a shrinking kernel similar to stochastic progressive photon mapping (SPPM) [Hachisuka and Jensen 2009]. This novel approach significantly reduces the variance in indirect lighting without blurring details in geometry or texture. In equal time comparisons we often achieve higher quality than previous approaches. The primary use case of our algorithm is to provide fast, reliable previews of global illumination. It is also consistent, retains the conceptual simplicity of PT, is orthogonal to importance and stratified sampling, and is easy to integrate into existing renderers.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3737578</person_id>
				<author_profile_id><![CDATA[81466647527]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Karsten]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Schwenk]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[(Fraunhofer IGD)]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737579</person_id>
				<author_profile_id><![CDATA[81466643467]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Timm]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Drevensek]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[(Fraunhofer IGD)]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1618487</ref_obj_id>
				<ref_obj_pid>1661412</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Hachisuka, T., and Jensen, H. W. 2009. Stochastic progressive photon mapping. In <i>ACM SIGGRAPH Asia 2009 papers</i>, SIGGRAPH Asia '09, 141:1--141:8.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2412524</ref_obj_id>
				<ref_obj_pid>2412363</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Schwenk, K., Kuijper, A., Behr, J., and Fellner, D. 2012. Practical noise reduction for progressive stochastic ray tracing with perceptual control. <i>IEEE CG&A</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Radiance Filtering for Interactive Path Tracing Karsten Schwenk and Timm Drevensek (Fraunhofer IGD) 
 Figure 1: For each sampled path (a) we extract a radiance sample representing the incident indirect 
radiance. These samples consist of position, direction, and radiance (b). When shading a pixel, we apply 
its BRDF also to radiance samples of neighboring pixels (weighted by a Gaussian kernel in world-space). 
Reusing radiance samples reduces variance signi.cantly without losing detail in geometry or texture. 
Our algorithm can provide reliable previews of global illumination with only a few samples per pixel 
(c + d). 1 Introduction We present a method for noise reduction that is especially tailored to interactive 
progressive path tracing (PT). The idea is to exploit spatial coherence in the image and reuse information 
from neigh­boring pixels. However, in contrast to image .ltering techniques (e.g. [Schwenk et al. 2012]), 
we do not simply .lter pixel values or samples of outgoing radiance. Instead, we only reuse the inci­dent 
indirect radiance of neighboring pixels in a radiance estimation step with a shrinking kernel similar 
to stochastic progressive pho­ton mapping (SPPM) [Hachisuka and Jensen 2009]. This novel ap­ proach signi.cantly 
reduces the variance in indirect lighting with­out blurring details in geometry or texture. In equal 
time compar­isons we often achieve higher quality than previous approaches. The primary use case of our 
algorithm is to provide fast, reliable previews of global illumination. It is also consistent, retains 
the conceptual simplicity of PT, is orthogonal to importance and strati­.ed sampling, and is easy to 
integrate into existing renderers. 2 Our Method The algorithm is sketched in Fig.1 and described in 
greater detail in the accompanying poster, which also shows some preliminary re­sults. In this document, 
we will focus on the unique properties of our method and how it compares to related work. The basic concept 
of our method is to treat radiance samples of neighboring pixels as independent realizations of the same 
random variable. The variance of this variable (and thus the noise in the im­age) can be reduced by taking 
a weighted average of independent realizations (i.e. by .ltering). In general neighboring samples rep­resent 
different variables, so this assumption is only approximately true and .ltering means trading noise for 
bias. Many approaches .lter pixel values and try to limit bias by using some variant of (cross) bilateral 
.ltering, which reduces the in.uence of strongly biasing pixels. The biggest problem of these approaches 
is that in most practical situations the bilateral .lter will either be too broad and blur .ne details 
in geometry or texture, or it will be too sharp and will not reduce variance enough. Another problem 
is that the bilateral .lter does not handle sharp antialiased edges correctly, be­cause the pixel value 
is a linear combination of the regions adjacent to the edge and not present in the undiscretized signal 
itself. Ad­dressing these two issues was the primary motivation for our work. The key observation is 
that the unwanted portion of the variance in the outgoing radiance is due to the incident radiance, not 
the BRDF. So instead of .ltering pixel values, we try to reduce vari­ance by only averaging samples of 
the incident radiance Li. In the usual notation our radiance estimate is not LLo(x, .o)=wj fr(xj ,.i,j 
,.o,j )Li(xj ,.i,j )(nj · .i,j ), j but LLo(x, .o)=wj fr(x, .i,j ,.o)Li(xj ,.i,j )(n · .i,j ), j where 
the wj are normalized weights for the samples inside the kernel. Note that with our estimate x, n, and 
.o are taken from the current shading point, not the neighbors, which reduces blur in the factors fr 
and (n·.i,j ). However, shadows and sharp glossy re.ec­tions are still blurred, as they are included 
in Li. Usually we only .lter indirect illumination, which is the primary source of noise and can be expected 
to be reasonably smooth. Our method aver­ages before the pixel reconstruction .lter is applied, so we 
handle antialiased pixels correctly, which is another advantage over image .ltering. A disadvantage is 
the slightly higher overhead due to re­peated BRDF evaluations. The main difference to SPPM is that we 
distribute radiance samples in image-space during PT, not by a separate photon tracing path. The advantage 
is that we can expect a sample density of 1 sample per pixel and can adapt the kernel to aim to collect 
as many sam­ples as needed to reach a user-de.ned threshold on variance. The assumption that neighboring 
pixels contain relevant radiance sam­ples breaks down in the presence of geometric edges and complex 
perfect specular objects, but in most practical cases there will be at least some spatial coherence in 
the image. Early in the rendering process kernel sizes will be relatively large and will produce the 
typical artifacts known from PM (e.g. light leaks). However, as the path traced image converges, the 
variance will decrease and even­tually the image-space size of the kernel will drop below the pixel size, 
at which point our algorithm reduces to standard PT (the sum will only include the sample for the current 
pixel). This makes our algorithm consistent. A disadvantage in comparison with SPPM is that we inherit 
the weaknesses of PT with respect to SDS paths, where PM is clearly the superior algorithm. What was 
presented here is still work in progress. With future work we plan to improve the performance with complex 
perfect specular objects. We also want to evaluate our algorithm with bidirectional path tracing, metropolis 
light transport, depth of .eld, and motion blur. The last two effects are a further weakness of image 
.ltering. References HACHISUKA, T., AND JENSEN, H. W. 2009. Stochastic progressive photon mapping. In 
ACM SIGGRAPH Asia 2009 papers, SIGGRAPH Asia 09, 141:1 141:8. SCHWENK, K., KUIJPER, A., BEHR, J., AND 
FELLNER, D. 2012. Practical noise re­duction for progressive stochastic ray tracing with perceptual control. 
IEEE CG&#38;A. Copyright is held by the author / owner(s). SIGGRAPH 2012, Los Angeles, California, August 
5 9, 2012. ISBN 978-1-4503-1435-0/12/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2343023</article_id>
		<sort_key>1270</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>110</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>13</seq_no>
		<title><![CDATA[Randomized coherent sampling for reducing perceptual rendering error]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2342896.2343023</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2343023</url>
		<abstract>
			<par><![CDATA[<p>Realistic image synthesis using path tracing needs many samples to achieve noise-free images. The noise is due to the use of Monte Carlo integration in path tracing. Since Monte Carlo integration evaluates each pixel using random sampling, we obtain noisy pixels at low sample counts. Due to the random nature of Monte Carlo integration, pixel values with finite numbers of samples can be significantly different, even if their correct solutions to the rendering equation are the same.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3737580</person_id>
				<author_profile_id><![CDATA[81504687411]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Lasse]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Staal]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Aarhus University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737581</person_id>
				<author_profile_id><![CDATA[81320490282]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Toshiya]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hachisuka]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Aarhus University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Sadeghi, I., Chen, B., and Jensen, H. W. 2009. Coherent path tracing. <i>journal of graphics, gpu, and game tools 14</i>, 2, 33--43.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Yee, H. 2004. A perceptual metric for production testing. <i>journal of graphics, gpu, and game tools 9</i>, 4, 33--40.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Randomized Coherent Sampling for Reducing Perceptual Rendering Error Lasse Staal Toshiya Hachisuka 
Aarhus University  Figure 1: Left: Reference image of the killeroo model. Center top: Close-ups of the 
image after 8 minutes of rendering using our sampling method (RMSE 573.115). Center bottom: Equal-time 
comparison with random sampling (RMSE 618.509). Right: Graph of the perceptual difference of the image 
compared to the reference using pdiff [Yee 2004] with default settings. 1 Introduction Realistic image 
synthesis using path tracing needs many samples to achieve noise-free images. The noise is due to the 
use of Monte Carlo integration in path tracing. Since Monte Carlo integration evaluates each pixel using 
random sampling, we obtain noisy pixels at low sample counts. Due to the random nature of Monte Carlo 
integration, pixel values with .nite numbers of samples can be signi.cantly different, even if their 
correct solutions to the rendering equation are the same. We propose a novel sampling method which exploits 
the coherency among integrals of pixels without .ltering. Our method does not aim to reduce RMS error 
of rendered images. Instead, our method makes pixels with similar integrals converge towards the correct 
solutions with similar trend of error. In other words, our method makes the distribution of error coherent. 
Figure 1 demonstrates our results. If integrals are completely the same, the image rendered by our method 
would be just a constant scaling from the reference solution. As the human visual system is not sensitive 
to absolute differences [2004], our method yields perceptually better images compared to tradiditional 
random sampling. The graph shows the perceptual error of our method compared to strati.ed sampling. 
2 Main Idea Instead of sampling every pixel individually, we use the same se­quences of random numbers 
for pixels with similar integrals. This technique has already been proposed by Sadeghi et al. in coherent 
path tracing (CPT) [2009]. However, their main focus is to increase performance by utilizing the coherence 
of rays to do packet tracing. CPT suffers from structured noise in regions where the integral of the 
rendering equation is changing. Sadeghi et al. proposed one heuristic solution to this issue by introducing 
a small tile of different random number sequences over neighboring pixels. We formalize this heuristic 
by incorporating differences among integrals of pixels into the sampling procedure. The key difference 
is that we aim at ren­dering images with reduced perceptual error, rather than improving performance 
of ray tracing. Our method generates two images; ImageR and ImageC . ImageR is generated by using regular 
random sampling. Using the same nota­tion of Sadeghi et al., random sampling can be seen as use of a 
dif­ferent random number sequence uu =(u1,...,un) per pixel[2009]. The vector u i,k is used to render 
the kth sample for pixel xi. CPT uses the same vector u k for every pixel xi and sample k. Our method 
also uses the same vector over the image, but offsets it by another weighted random number sequence for 
each sample. The weight indicates how different the current pixel is from its neighbors. The addition 
of another random number sequence reduces structured noise in regions with varying pixel integrals. The 
map w, containing the weights, is constructed from ImageR, by .ltering the input image of path tracing. 
After .ltering, the current pixel is compared to its neighbors in order to determine how different its 
pixel value is from the neighbors. The .nal vector used to generate the second image ImageC is: u(1) 
uci,k =(u k + wiu i,k) mod 1, where u uci,k is the random sequence for the kth sample for pixel xi and 
wi is the weight associated with pixel xi. If the pixel is similar to its neighbors, wi will be 0, and 
u uci,k reduces to the coherent sequence u k. However, if the pixel is completely different from its 
neighbors, the resulting sequence will be incoherent from the neigh­boring pixels since the incoherent 
random number sequence, uui,k, will dominate the resulting random number sequence by Equation 1. 3 Algorithm 
Our method is a two pass algorithm. In the .rst pass, we render ImageR using incoherent random number 
sequences. We then gen­erate map w as described in the previous section. In the second pass, we utilize 
the resulting map to render ImageC using the vector u uci,k in Equation 1. The two images are combined 
either by addition, or a weighted function based on w, which interpolates the two images according to 
the differences of the integrals. One interesting feature of our method is that the method remains unbiased 
since it is non-adaptive uniform sampling. This feature is in contrast to existing adaptive sampling 
methods where unbiasedness is usually not guaranteed due to their adaptation to previous samples. Currently 
the method is only explored for direct lighting. However, since we use random numbers as the basis of 
our method, the same approach can potentially be used for other rendering techniques such as .nal gathering 
in photon mapping. As the error of similar pixels is the same, at low sample counts, our method may still 
introduce structured noise. However, we see consistent reduction in perceptual error at a reasonable 
amount of samples. References SADEGHI, I., CHEN, B., AND JENSEN, H. W. 2009. Coherent path tracing. 
journal of graphics, gpu, and game tools 14, 2, 33 43. YEE, H. 2004. A perceptual metric for production 
testing. journal of graphics, gpu, and game tools 9, 4, 33 40. Copyright is held by the author / owner(s). 
SIGGRAPH 2012, Los Angeles, California, August 5 9, 2012. ISBN 978-1-4503-1435-0/12/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2343024</article_id>
		<sort_key>1280</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>111</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>14</seq_no>
		<title><![CDATA[Rendering of human skin during physical exercise]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2342896.2343024</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2343024</url>
		<abstract>
			<par><![CDATA[<p>Many researchers have shown interest in the realistic rendering of a human face since it is crucial to the success of many applications in computer graphics, games and animation. However, it is difficult to represent the realistic appearance of a facial skin since it has complex physical properties and its appearance tends to vary as the situation changes. For example, the glossiness of a facial skin increases after exercise and the color of it becomes red after drink. In this paper, we introduce a method to simulate the appearance change of a human skin during physical exercise. Our key idea is that the appearance of a skin under exercise can be divided into a surface reflectance component and a subsurface scattering component so that they are approximated by a BRDF model and a BSSRDF model, respectively. The Torrance-Sparrow BRDF model describes the glossiness and the roughness that are affected by sweat and oil on the skin surface. The Multipole model [Donner and Jensen 2005] with absorption &sigma;<sub>a</sub> and scattering &sigma;'<sub>s</sub> parameters represents subsurface scattering effects that occur between cells and pigments under the surface. In this paper, the parameters of each reflectance model are acquired by measurement.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[appearance rendering]]></kw>
			<kw><![CDATA[exercise]]></kw>
			<kw><![CDATA[human skin]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3737582</person_id>
				<author_profile_id><![CDATA[81421594378]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Myoung]]></first_name>
				<middle_name><![CDATA[Kook]]></middle_name>
				<last_name><![CDATA[Seo]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Gwangju Institute of Science and Technology, Korea]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[escmk@gist.ac.kr]]></email_address>
			</au>
			<au>
				<person_id>P3737583</person_id>
				<author_profile_id><![CDATA[81504686653]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Hyuk]]></first_name>
				<middle_name><![CDATA[Jin]]></middle_name>
				<last_name><![CDATA[Kwon]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Gwangju Institute of Science and Technology, Korea]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737584</person_id>
				<author_profile_id><![CDATA[82459340457]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Bilal]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ahmed]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Gwangju Institute of Science and Technology, Korea]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737585</person_id>
				<author_profile_id><![CDATA[81504686612]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Young]]></first_name>
				<middle_name><![CDATA[Yi]]></middle_name>
				<last_name><![CDATA[Lee]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Gwangju Institute of Science and Technology, Korea]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737586</person_id>
				<author_profile_id><![CDATA[81335500061]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Jae]]></first_name>
				<middle_name><![CDATA[Doug]]></middle_name>
				<last_name><![CDATA[Yoo]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Gwangju Institute of Science and Technology, Korea]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737587</person_id>
				<author_profile_id><![CDATA[81421600485]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[In]]></first_name>
				<middle_name><![CDATA[Yeop]]></middle_name>
				<last_name><![CDATA[Jang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Gwangju Institute of Science and Technology, Korea]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737588</person_id>
				<author_profile_id><![CDATA[81452602365]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>7</seq_no>
				<first_name><![CDATA[Seung]]></first_name>
				<middle_name><![CDATA[Joo]]></middle_name>
				<last_name><![CDATA[Lee]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Gwangju Institute of Science and Technology, Korea]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737589</person_id>
				<author_profile_id><![CDATA[81448592690]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>8</seq_no>
				<first_name><![CDATA[Min]]></first_name>
				<middle_name><![CDATA[ki]]></middle_name>
				<last_name><![CDATA[Park]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Gwangju Institute of Science and Technology, Korea]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737590</person_id>
				<author_profile_id><![CDATA[81448598973]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>9</seq_no>
				<first_name><![CDATA[Kwan]]></first_name>
				<middle_name><![CDATA[H.]]></middle_name>
				<last_name><![CDATA[Lee]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Gwangju Institute of Science and Technology, Korea]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1073308</ref_obj_id>
				<ref_obj_pid>1073204</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Donner, C., and Jensen, H. W. 2005. Light diffusion in multi-layered translucent materials. <i>ACM Trans. Graph. 24</i>, 3 (July), 1032--1039.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Rendering of human skin during physical exercise Myoung Kook Seo, Hyuk Jin Kwon, Bilal Ahmed ,Young 
Yi Lee Jae Doug Yoo , In Yeop Jang, Seung Joo Lee, Min ki Park, Kwan H. Lee Gwangju Institute of Science 
and Technology, Korea*  Figure 1: (a) Real image before exercise (b) Rendering result before exercise 
(c) Real image after exercise (d) Rendering result after exercise Keywords: Human skin, exercise, appearance 
rendering 1 Introduction Many researchers have shown interest in the realistic rendering of a human face 
since it is crucial to the success of many applications in computer graphics, games and animation. However, 
it is dif.­cult to represent the realistic appearance of a facial skin since it has complex physical 
properties and its appearance tends to vary as the situation changes. For example, the glossiness of 
a facial skin increases after exercise and the color of it becomes red after drink. In this paper, we 
introduce a method to simulate the ap­pearance change of a human skin during physical exercise. Our key 
idea is that the appearance of a skin under exercise can be di­vided into a surface re.ectance component 
and a subsurface scat­tering component so that they are approximated by a BRDF model and a BSSRDF model, 
respectively. The Torrance-Sparrow BRDF model describes the glossiness and the roughness that are affected 
by sweat and oil on the skin surface. The Multipole model [Donner and Jensen 2005] with absorption sa 
and scattering s1s parameters represents subsurface scattering effects that occur between cells and pigments 
under the surface. In this paper, the parameters of each re­.ectance model are acquired by measurement. 
 2 Our approach For the skin rendering, we need the geometry, the face albedo, and the face re.ectance 
data of the face. The face albedo is used to represent the spatial details of the facial skin. The geometry 
of the face is acquired by using a commercial scanning device. The face albedo is computed by using a 
single image that is captured by illuminating from the front by a projector. This method uses an assumption 
of Lambertian re.ectance of a skin. We acquire the specular surface re.ectance component and the diffuse 
subsurface scattering component by an image based measurement setup. This setup consists of a camera, 
a projector, and a material holder. In measuring the surface re.ectance and the subsurface scattering, 
the project emits a uniform area light and multiple beams into the face *e-mail:escmk@gist.ac.kr from 
the front, respectively. The multiple beams are uniformly dis­tributed with the distance of 20mm to avoid 
the interference with neighboring beams. Before and after the exercise of running 5 min­utes at the running 
machine, we take six-pictures with a series of different exposure times, ranging from 1/30 second to 
2 seconds for each measurement. These images are used to construct high dynamic range images of a facial 
skin. The total measurement time of our experiment is about 20 seconds. In our experiments, we .nd that 
the exercise changes the appearance of the facial skin such as specular re.ectance, roughness, and translucency 
(see Fig­ure 2). Compared to the normal face before exercise, the specular re.ectance is increased from 
0.1817 to 0.2434 and the roughness is slightly increased from 0.2244 to 0.2284. The average translucency 
l str = 3sa(sa+s1s) for each of three colors (red, green and blue) is reduced 31.4%, 16.6%, 35.8%, respectively. 
Figure 1 show the rendering results of our experiment. Figure 2: The variation of skin appearance after 
exercise. Acknowledgements This work was supported by the National Research Foundation of Korea (NRF) 
grant funded by the Korea government (MEST) (No. 20110020452). References DONNER, C., AND JENSEN, H. 
W. 2005. Light diffusion in multi­layered translucent materials. ACM Trans. Graph. 24, 3 (July), 1032 
1039. Copyright is held by the author / owner(s). SIGGRAPH 2012, Los Angeles, California, August 5 9, 
2012. ISBN 978-1-4503-1435-0/12/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<article_sponsors>
			<funding_agency>Korea government (MEST)</funding_agency>
			<grant_numbers>
				<grant_number>20110020452</grant_number>
			</grant_numbers>
		</article_sponsors>
	</article_rec>
	<article_rec>
		<article_id>2343025</article_id>
		<sort_key>1290</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>112</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>15</seq_no>
		<title><![CDATA[Rich intrinsic image decomposition of outdoor scenes from multiple views]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2342896.2343025</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2343025</url>
		<abstract>
			<par><![CDATA[<p>Intrinsic images aim at separating an image into reflectance and illumination layers to facilitate analysis or manipulation. Most successful methods rely on user indications [Bousseau et al. 2009], precise geometry, or need multiple images from the same viewpoint and varying lighting to solve this severely ill-posed problem. We propose a method to estimate intrinsic images from multiple views of an outdoor scene at a single time of day without the need for precise geometry and with only a simple manual calibration step.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3737591</person_id>
				<author_profile_id><![CDATA[81467641347]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Pierre-Yves]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Laffont]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[REVES/Inria Sophia-Antipolis]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[pierre-yves.laffont@inria.fr]]></email_address>
			</au>
			<au>
				<person_id>P3737592</person_id>
				<author_profile_id><![CDATA[81314487615]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Adrien]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bousseau]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[REVES/Inria Sophia-Antipolis]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[adrien.bousseau@inria.fr]]></email_address>
			</au>
			<au>
				<person_id>P3737593</person_id>
				<author_profile_id><![CDATA[81100408270]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[George]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Drettakis]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[REVES/Inria Sophia-Antipolis]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[george.drettakis@inria.fr]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1618476</ref_obj_id>
				<ref_obj_pid>1618452</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Bousseau, A., Paris, S., and Durand, F. 2009. User-assisted intrinsic images. <i>ACM Trans. Graph. 28</i>, 5.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Rich Intrinsic Image Decomposition of Outdoor Scenes from Multiple Views Pierre-Yves Laffont, Adrien 
Bousseau, George Drettakis REVES / Inria Sophia-Antipolis*  Figure 1: Starting from multiple views of 
the scene (a), our method decomposes photographs into four intrinsic components the re.ectance (e), 
the illumination due to sun (f), the illumination due to sky (g) and the indirect illumination (h). Each 
intrinsic component can then be manipulated independently for advanced image editing applications (b-d). 
Introduction Intrinsic images aim at separating an image into re.ectance and illumination layers to facilitate 
analysis or manipulation. Most suc­cessful methods rely on user indications [Bousseau et al. 2009], precise 
geometry, or need multiple images from the same viewpoint and varying lighting to solve this severely 
ill-posed problem. We propose a method to estimate intrinsic images from multiple views of an outdoor 
scene at a single time of day without the need for precise geometry and with only a simple manual calibration 
step. We use multi-view stereo to automatically reconstruct a 3D point cloud of the scene. Although this 
point cloud is sparse and incom­plete, we show it provides the necessary information to compute plausible 
sky and indirect illumination at each 3D point. We then introduce an optimization method to estimate 
sun visibility over the point cloud. This algorithm compensates for the lack of accurate geometry and 
allows extraction of precise cast shadows. We .nally propagate the information computed over the sparse 
point cloud to every pixel in the photograph using image-guided propagation. Our method not only separates 
re.ectance and illumination, but also decomposes the illumination into sun, sky and indirect lay­ers. 
This rich decomposition allows novel image manipulations. Our Approach Our method relies on a relatively 
lightweight capture setup composed of a digital camera, a photographer s gray card for calibration, and 
a simple re.ective sphere to capture an environ­ment map. We capture 10-30 pictures from dif­ ferent 
viewpoints in addition to the images to decompose. Geometry-based computation We use structure-from-motion 
and multi-view stereo to reconstruct a cloud of oriented 3D points, and surface reconstruction to obtain 
a proxy geometric model of the scene. The user speci.es the orientation and color of sun and sky through 
a simple calibration step. The geometric proxy is approximate and incomplete, and cannot be directly 
used to estimate the illumination at each pixel. In particu­ *e-mail: {pierre-yves.laffont, adrien.bousseau, 
george.drettakis}@inria.fr lar, it produces inaccurate or even missing cast shadows. However, it can 
give a reasonable approximation for low-frequency lighting components: we estimate sky and indirect illumination 
at recon­structed 3D points by casting rays towards all directions. Rays that intersect the proxy contribute 
to indirect lighting (we use the cap­tured photographs to lookup the outgoing radiance at intersected 
points), while rays that hit the environment map above the horizon contribute to sky lighting. We also 
use the proxy to compute an ini­tial estimate of sun visibility (cast shadows), which we later re.ne. 
Sun visibility estimation We introduce an algorithm to reliably identify points in shadow based on a 
new parameterization of re­.ectance with respect to sun visibility. Our algorithm compensates for the 
lack of accurately reconstructed and complete geometry. We show that the re.ectance of each 3D point 
lies on a candidate curve in color space, once sky and indirect illuminations are estimated. Multiple 
points which share a similar re.ectance generate intersect­ing candidate curves. We use an iterative 
optimization method that reliably estimates the re.ectance and sun visibility at reconstructed points, 
by .nding regions where multiple candidate curves inter­sect. We illustrate this process in the supplementary 
document. Image-based propagation and lighting separation At this stage, the total illumination has been 
estimated at sparse recon­structed 3D points. We propagate it to all pixels of the image to de­compose 
by using an image-guided propagation method [Bousseau et al. 2009], which yields a decomposition into 
re.ectance and total illumination. We further decompose the total illumination into sun, sky and indirect 
illumination, by casting this as a Matting problem and enforcing the illumination values estimated at 
3D points. Results Our algorithm decomposes an image into four layers, which can be then modi.ed independently. 
Fig. 1 shows exam­ ples of changes made possible by our approach, while the video demonstrates how to 
use the decomposition in image editing soft­ware. More results and comparisons with single-image approaches 
are shown in the supplementary document. References BOUSSEAU, A., PARIS, S., AND DURAND, F. 2009. User-assisted 
intrinsic images. ACM Trans. Graph. 28, 5. Copyright is held by the author / owner(s). SIGGRAPH 2012, 
Los Angeles, California, August 5 9, 2012. ISBN 978-1-4503-1435-0/12/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2343026</article_id>
		<sort_key>1300</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>113</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>16</seq_no>
		<title><![CDATA[Stereoscopic line drawing using depth maps]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2342896.2343026</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2343026</url>
		<abstract>
			<par><![CDATA[<p>Motivated by the success of the recent stereoscopic 3D films, there is a growing demand for techniques for creating and editing 3D contents. Several researchers have attempted to improve existing 2D image processing methods in order to apply them to the stereoscopic 3D images.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[non-photorealistic rendering]]></kw>
			<kw><![CDATA[stereoscopic 3D]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3737594</person_id>
				<author_profile_id><![CDATA[81492646184]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Yeong-Seok]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kim]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Yonsei University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[kys71015@gmail.com]]></email_address>
			</au>
			<au>
				<person_id>P3737595</person_id>
				<author_profile_id><![CDATA[81414613561]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Ji-Yong]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kwon]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Suwon]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[rinthel@gmail.com]]></email_address>
			</au>
			<au>
				<person_id>P3737596</person_id>
				<author_profile_id><![CDATA[81409592301]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[In-Kwon]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lee]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Yonsei University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[iklee@yonsei.ac.kr]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1274878</ref_obj_id>
				<ref_obj_pid>1274871</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Kang, H., Lee, S., and Chui, C. K. 2007. Coherent line drawing. In <i>ACM Symposium on Non-Photorealistic Animation and Rendering (NPAR)</i>, 43--50.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2191908</ref_obj_id>
				<ref_obj_pid>2191740</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Rhemann, C., Hosni, A., Bleyer, M., Rother, C., and Gelautz, M. 2011. Fast cost-volume filtering for visual correspondence and beyond. In <i>IEEE Computer Vision and Pattern Recognition (CVPR)</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Stereoscopic Line Drawing using Depth Maps Yeong-Seok Kim* Ji-Yong Kwon In-Kwon Lee Yonsei University 
The University of Suwon Yonsei University  Figure 1: (a) Original Image (Left) (b) Stylized edge using 
Coherent Line Drawing (Left) (c) Warped edge (Right) (d) Final Result (Red-Cyan Stereoscopic Image) Keywords: 
Stereoscopic 3D, Non-Photorealistic Rendering 1 Introduction Motivated by the success of the recent stereoscopic 
3D .lms, there is a growing demand for techniques for creating and editing 3D contents. Several researchers 
have attempted to improve existing 2D image processing methods in order to apply them to the stereo­scopic 
3D images. A naive extension of the 2D image stylization technique to 3D imagery is to apply it to each 
left and right image independently. When watching the resulting images using Stereoscopic Device, the 
fused 3D image is too poor to be perceived as a 3D image, because the resulting left and right images 
usually differ from each other. Specically, if the left and right images have different lines, the binocular 
rivalry phenomenon occurs, which disturbs the fusion of the left and right images and causes visual fatigue. 
Therefore, in order to draw consistent lines, we should take into consideration the stereo correspondence 
of the left and right images. In this work, we propose a method that can stylize a stereoscopic image 
by drawing lines and obtaining cartoon-stylized results. The key idea is to generate the vectorized lines 
from left image, warp it into right image, and manipulate line width by depth. Using the new method, 
we can obtain well-fused 3D image, so there is no binocular rivalry phenomenon. Also, with line width 
control, we can get more viewing exprience. 2 Our Approach For a given stereoscopic pair of left and 
right images(Figure 1(a)), which are recti.ed in order to remove vertical disparities, we .rst obtain 
dense correspondences between the two images [Rhemann et al. 2011]. We can apply image warping to the 
stylized left image in order to generate the stylized right image that have consistent lines by using 
the disparity map. Then, we perform image stylization by using the ETF .eld [Kang et al. 2007](Figure 
1(b)). Although this paper uses the image styl­ ization method based on the ETF .eld, other methods can 
be used, such as Bilateral Filtering, DoG .ltering, canny edge detectors. *e-mail:kys71015@gmail.com 
e-mail:rinthel@gmail.com e-mail:iklee@yonsei.ac.kr To warp left image into right one without distortion, 
our method extracts the vectorized lines from the detected edges, which have its own orientation and 
magnitude on each edge point. Then the vectorized lines are warped to right image using computed disparity 
map without occluded region(Figure 1(c)). Because line is vectorized, line width can be manipulated. 
There­fore, we can apply additional effects to the stylized result. Cartoon artists usually express the 
distance from the camera to an object as a line width and intensity; objects near the camera are drawn 
with thick black lines, while objects far from the camera are drawn with narrow gray lines. Thus, we 
use the disparity map to control the line width and intensity. Finally, we draw our warped edge on original 
right image(Figure 1(d)). 3 Conclusion Using our approach, two images can be rendered consistently. 
Ad­ditionally, our line manipulation method can express the distance of objects effectively. The user 
survey shows that our stylized im­ages are perceived more comfortably in 3D than those generated by stylizing 
left and right individually. In future work, we would like to extend our method to other NPR techniques, 
such as painterly rendering or mosaics render­ing. While the proposed method does not make stylized edge 
in occluded region, we should consider it when we attempt to render images with large occlusion. Acknowledgement 
This work was supported by the National Research Foundation of Korea (NRF) grant funded by the Korea 
government (MEST) (No. 2011-0028568). References KANG, H., LEE, S., AND CHUI, C. K. 2007. Coherent line 
draw­ing. In ACM Symposium on Non-Photorealistic Animation and Rendering (NPAR), 43 50. RHEMANN, C., 
HOSNI, A., BLEYER, M., ROTHER, C., AND GELAUTZ, M. 2011. Fast cost-volume .ltering for visual cor­respondence 
and beyond. In IEEE Computer Vision and Pattern Recognition (CVPR). Copyright is held by the author / 
owner(s). SIGGRAPH 2012, Los Angeles, California, August 5 9, 2012. ISBN 978-1-4503-1435-0/12/0008 
 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<article_sponsors>
			<funding_agency>Korea government (MEST)</funding_agency>
			<grant_numbers>
				<grant_number>2011-0028568</grant_number>
			</grant_numbers>
		</article_sponsors>
	</article_rec>
	</section>
	<section>
		<section_id>2343027</section_id>
		<sort_key>1310</sort_key>
		<section_seq_no>18</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Virtual/augmented reality]]></section_title>
		<section_page_from>18</section_page_from>
	<article_rec>
		<article_id>2343028</article_id>
		<sort_key>1320</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>114</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[A dynamic system for controlling the head movement and gaze of virtual characters]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2342896.2343028</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2343028</url>
		<abstract>
			<par><![CDATA[<p>Creating virtual characters with natural movements is a challenging problem. Users are very aware of how people in general m<b>o</b>ve when interacting based on a lifetime of experience. Unintentionally unnatural movements can adversely affect the experience of the participant. Computationally intensive techniques for calculating dynamic movements can cause the system to run slowly, reducing the frame rate or the performance of the avatar within the simulation. Our system provides a simple implementation that enables precise individual control of multiple avatars.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3737597</person_id>
				<author_profile_id><![CDATA[81504684705]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Timothy]]></first_name>
				<middle_name><![CDATA[D.]]></middle_name>
				<last_name><![CDATA[Gifford]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Connecticut]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[timothy.gifford@uconn.edu]]></email_address>
			</au>
			<au>
				<person_id>P3737598</person_id>
				<author_profile_id><![CDATA[81504682175]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Zhenxiang]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Zhang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Connecticut]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737599</person_id>
				<author_profile_id><![CDATA[81453627170]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Kerry]]></first_name>
				<middle_name><![CDATA[L.]]></middle_name>
				<last_name><![CDATA[Marsh]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Connecticut]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Bailenson, J. N., Beall, A. C. and Blascovich, J. 2002. Gaze and task performance in shared virtual environments. <i>The Journal of Visualization and Computer Animation, 13</i>, 313--320.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Guerrero, L. K., DeVito, J. A., &amp; Hecht, M. L. 1999. <i>The nonverbal communication reader: Classic and contemporary readings</i>. Prospect Hills, Il: Waveland Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>365121</ref_obj_id>
				<ref_obj_pid>365024</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[M. Garau, M. Slater, S. Bee, and M. A. Sasse. 2001. The impact of eye gaze on communication using humanoid avatars. <i>In Proceedings SIGCHI Conference on Human Factors in Computing Systems (CHI-01)</i>, 309--316.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 A Dynamic System for Controlling the Head Movement and Gaze of Virtual Characters heads.jpg Timothy 
D. Gifford*, Zhenxiang Zhang, Kerry L. Marsh University of Connecticut  Figure 1. Images of the avatar 
looking directly at the user, looking at another object of interest while facing the user and facing 
away to look at another object of interest. 1. Introduction Creating virtual characters with natural 
movements is a challenging problem. Users are very aware of how people in general move when interacting 
based on a lifetime of experience. Unintentionally unnatural movements can adversely affect the experience 
of the participant. Computationally intensive techniques for calculating dynamic movements can cause 
the system to run slowly, reducing the frame rate or the performance of the avatar within the simulation. 
Our system provides a simple implementation that enables precise individual control of multiple avatars. 
Studies have shown that contextually accurate gaze results in better communication with participants 
than random gaze (Garau, Slater, Bee, Sasse 2001). Different types of eye contact can also have an effect 
on the conversation. Along with eye contact, appropriate head orientation can better engage a person 
in a virtual conversation, much more so than with random or no head movements (Garau, et al 2001; Bailenson, 
Beall, Blascovich 2002). With proper context appropriate look at's, one can create a conversation environment 
that is both engaging and realistic. 2. Approach To simplify the problem we first separated the movement 
of the head and eyes and allowed them to combine dynamically. Head movement is slower and directed more 
by the object of primary attention. Eye movements are quicker and allowed to move with higher variability 
to more objects. In our implementation, objects of interest are defined for each avatar. These are other 
avatars, objects in the environment, and other points in the scene that would naturally draw interest. 
Head movement parameters consist of the set of objects in the environment, each of which has a time to 
be looked (with a random variance) at and a weight to determine how often it is looked at. When the time 
is up, a decision is made about what to look at next based off the weights of the objects in the list. 
This list is changed during different points in the scene. By adjusting the weights dynamically the avatar 
is directed to look at a specific other avatar or other object. The model is based on the assumption 
that the avatar will look at the person who is talking as the primary interest holder. Past research 
has established that gaze is higher when one is listening than speaking (Guerrero et al., 1999). Thus, 
when the user is talking, their weight is increased, making the avatar look at the user. This provides 
a natural behavior of the avatar looking at the user and other objects in a motivated way. The avatar 
s eyes shift between looking along the direction that the head is facing and looking at one of the look 
away points. When the avatar looks away, the time to look away, position to look at, and traversal speed 
are all calculated. The eyes then traverse to look at the point chosen. After the calculated time has 
passed, the time to look ahead and traversal time are chosen and the eyes return to look towards the 
user. The cycle then repeats. A recent psychology experiment conducted by our group provided a test of 
the role of eye gaze in determining reactions to an avatar. The study involved a brief conversation 
between an avatar and a participant. Eye contact was manipulated with either high gaze or low gaze (15% 
above or below established norms of gaze during speaking and during listening). Results showed that participants 
in the high gaze condition felt more connected to the avatar than in the low gaze conditions. We have 
created a simple system for creating dynamic head and eye movements for virtual characters. This system 
allows the programmer to change the objects of interest and the likelihood that the avatar will look 
at the objects throughout the scene. This system is easy to use and computationally light. References 
BAILENSON, J. N., BEALL, A. C. AND BLASCOVICH, J. 2002. Gaze and task performance in shared virtual environments. 
The Journal of Visualization and Computer Animation, 13, 313 320. GUERRERO, L. K., DEVITO, J.A., &#38; 
HECHT, M.L. 1999. The nonverbal communication reader: Classic and contemporary readings. Prospect Hills, 
Il: Waveland Press. M. GARAU, M. SLATER, S. BEE, AND M. A. SASSE. 2001. The impact of eye gaze on communication 
using humanoid avatars. In Proceedings SIGCHI Conference on Human Factors in Computing Systems (CHI-01), 
309 316. 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2343029</article_id>
		<sort_key>1330</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>115</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[A video see-through face mounted display for view sharing]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2342896.2343029</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2343029</url>
		<abstract>
			<par><![CDATA[<p>If the feeling of the presence can be transferred to a different place from rather than the place where we actually exist, our life style will change drastically. By extending robot-human telexistence [1] technology to human-human situations, we are developing an environment where a skilled person, who actually exists at a different place, can work with high efficacy on the ground instead of non-skilled person. In order to realize such a telexistence environment in human interactions, we are developing remote communication technologies exploiting sensemotion sharing. In this project, we have developed a view sharing system to share first person perspectives between remote two people [2]. The system consists of a head mounted display and cameras, which make possible a video see though (VST-HMD). The user wearing the HMD can see his own view and the partner's view, and also send his own view to the partner. Our aim is to share experience and to transmit the skills from one to another by sharing vision and motions [3]. We developed a new view sharing system to improve effectiveness and expand its applications.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3737600</person_id>
				<author_profile_id><![CDATA[81311484645]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Yuki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hashimoto]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Osaka University, Suita, Osaka, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[y.hashimoto@ist.osaka-u.ac.jp]]></email_address>
			</au>
			<au>
				<person_id>P3737601</person_id>
				<author_profile_id><![CDATA[81435600383]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Daisuke]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kondo]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Osaka University, Suita, Osaka, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[kondo@ist.osaka-u.ac.jp]]></email_address>
			</au>
			<au>
				<person_id>P3737602</person_id>
				<author_profile_id><![CDATA[81482658346]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Tomoko]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yonemura]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Osaka University, Suita, Osaka, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[yonemura@ist.osaka-u.ac.jp]]></email_address>
			</au>
			<au>
				<person_id>P3737603</person_id>
				<author_profile_id><![CDATA[81100005260]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Hiroyuki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Iizuka]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Osaka University, Suita, Osaka, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[iizuka@ist.osaka-u.ac.jp]]></email_address>
			</au>
			<au>
				<person_id>P3737604</person_id>
				<author_profile_id><![CDATA[81100486051]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Hideyuki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ando]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Osaka University, Suita, Osaka, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[hide@ist.osaka-u.ac.jp]]></email_address>
			</au>
			<au>
				<person_id>P3737605</person_id>
				<author_profile_id><![CDATA[81100065999]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Taro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Maeda]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Osaka University, Suita, Osaka, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[t_maeda@ist.osaka-u.ac.jp]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Tachi, S., Tanie, K., Komoriya, K. &amp; Kaneko, M., "Tele-Existence (1):Design and Evaluation of a Visual Display with Sensation of Presence," Proc. of the 5th Symposium on Theory and Practice of Robots and Manipulators, pp.245--254, 1984.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1959844</ref_obj_id>
				<ref_obj_pid>1959826</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Maeda, T., Ando, H., Iizuka, H., Yonemura, T., Kondo, D. &amp; Niwa, M., "Parasitic Humanoid: The Wearable Robotics as a Behavioral Assist Interface like Oneness between Horse and Rider," Proc. of the 2nd Augmented Human International Conference, pp.1--4, 2011.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Kawasaki, H., Iizuka, H., Okamoto, S., Ando, H. &amp; Maeda, T., "Collaboration and Skill Transmission by First-person Perspective View Sharing System," Proc. of the 19th IEEE International Symposium in Robot and Human Interactive Communication, pp.125--131, 2010.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 A Video See-Through Face Mounted Display for View Sharing Yuki Hashimoto, Daisuke Kondo, Tomoko Yonemura, 
Hiroyuki Iizuka, Hideyuki Ando and Taro Maeda Osaka University, 2-1 yamadaoka, Suita, Osaka, JAPAN {y.hashimoto, 
kondo, yonemura, iizuka, hide, t_maeda}@ist.osaka-u.ac.jp 1. Introduction If the feeling of the presence 
can be transferred to a different place from rather than the place where we actually exist, our life 
style will change drastically. By extending robot-human telexistence [1] technology to human-human situations, 
we are developing an environment where a skilled person, who actually exists at a different place, can 
work with high efficacy on the ground instead of non-skilled person. In order to realize such a telexistence 
environment in human interactions, we are developing remote communication technologies exploiting sense­motion 
sharing. In this project, we have developed a view sharing system to share first person perspectives 
between remote two people [2]. The system consists of a head mounted display and cameras, which make 
possible a video see though (VST-HMD). The user wearing the HMD can see his own view and the partner 
s view, and also send his own view to the partner. Our aim is to share experience and to transmit the 
skills from one to another by sharing vision and motions [3]. We developed a new view sharing system 
to improve effectiveness and expand its applications. 2. View sharing system Our view sharing system 
is composed of two cameras (PointGrey FireFly MV), two displays (Daeyang FX603), two mirrors, two lamps 
and a motion sensor (NEC/Tokin MDP-A3U9S). The layout of the devices for vision is precisely designed 
to be able to share the first person perspectives. The cameras are placed at the optically same place 
at the eyes. Therefore the cameras capture what subjects would see if they do not wear the system. The 
captured images are once sent to PC and are showed in the display. They can see binocularly. By adjusting 
blending parameters, the views are blended differently. The new system has three advantages. One is light 
weight. Weight of the system (about1.5kg) is drastically lighter than established one (about 32kg). The 
second is stand-alone system. In this system, the user can wear all components. There are only two components; 
the video see though face mounted display (VST-FMD) and a vest including the battery, circuits, and laptop 
PC. Battery life is approximately three hours. Moreover, we mainly use wireless networking so that the 
area of user s actions is not limited by wires. In addition, there is a battery to operate without external 
power supply. Therefore, it is possible to extend user s behavior range. The third is simplicity in wearing 
the system. The display is bonded to goggles. The edges of the goggles are covered with soft rubber so 
that the eyes and goggles maintain close contact. There is no need to adjust the distance between the 
two displays to a user s interocular distance. Therefore, a user can wear the system easily, and expand 
his/her range with a View Sharing System. These features can make more efficient of skill training and 
expand sphere of activity. 3. Demonstration By wearing our view sharing system, the users can experience 
to learn skills such as how to play theremin, do a basic trick of Diabolo (rotating a spool with two 
sticks tied by a string), paper folding, balloon twisting, the way of first aid and so on. The users 
can learn those skills while watching the skilled person s view from the person s perspective and imitating 
the person s motions as if the displayed motion is their own. The users do not have to pay attention 
to how to do it but just follow the skilled person s motion. The users can intuitively learn the skills. 
 View sharing system (top) and skill training (bottom)  REFERENCES [1] Tachi, S., Tanie, K., Komoriya, 
K. &#38; Kaneko, M., Tele-Existence (1):Design and Evaluation of a Visual Display with Sensation of Presence, 
Proc. of the 5th Symposium on Theory and Practice of Robots and Manipulators, pp.245-254, 1984. [2] 
Maeda, T., Ando, H., Iizuka, H., Yonemura, T., Kondo, D. &#38; Niwa, M., Parasitic Humanoid: The Wearable 
Robotics as a Behavioral Assist Interface like Oneness between Horse and Rider, Proc. of the 2nd Augmented 
Human International Conference, pp.1-4, 2011. [3] Kawasaki, H., Iizuka, H., Okamoto, S., Ando, H. &#38; 
Maeda, T., Collaboration and Skill Transmission by First-person Perspective View Sharing System, Proc. 
of the 19th IEEE International Symposium in Robot and Human Interactive Communication, pp.125-131, 2010. 
Copyright is held by the author / owner(s). SIGGRAPH 2012, Los Angeles, California, August 5 9, 2012. 
ISBN 978-1-4503-1435-0/12/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2343030</article_id>
		<sort_key>1340</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>116</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Hand-rewriting]]></title>
		<subtitle><![CDATA[automatic rewriting like natural handwriting]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2342896.2343030</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2343030</url>
		<abstract>
			<par><![CDATA[<p>Approaches for combining handwriting acts and control by computer are gaining force. As two types of general classifications of these approaches, the "pen tablet" and the "digital pen" are often cited. As for the former approach, in addition to being able to display handwritten information, it can display drawing effects and additional information related to that information. Data output by this approach, however, can only be handled by digital-type displays. As for the latter approach, although written information acquired as paper input can be digitized, it is not possible to express additional information as output on paper (i.e., a paper display). In contrast to those two approaches, the system proposed by the authors---called "Hand-rewriting"---automatically performs "rewrite" processing on paper in correspondence with handwriting using pen and paper. More specifically, with the proposed system, when the user writes on a piece of paper with a pen, for example, the information written on the paper is automatically erased as required, and additional information is displayed on the paper in natural print-like colors.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3737606</person_id>
				<author_profile_id><![CDATA[81100411762]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Tomoko]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hashida]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737607</person_id>
				<author_profile_id><![CDATA[81504685158]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Kohei]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nishimura]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737608</person_id>
				<author_profile_id><![CDATA[81100095332]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Takeshi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Naemura]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Nishimura, K., Kim, J., and Naemura, T. 2012. AR-eLaser: Erasable Handwriting Interface with Paper and Pen, <i>Submitted to SIGGRAPH2012 Talks</i>
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2048270</ref_obj_id>
				<ref_obj_pid>2048259</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Hashida, T., Kakehi, Y., and Naemura, T. 2011. Photochromic Sculpture: Volumetric Color-forming Pixels, <i>In Proceedings of SIGGRAPH2011 Emerging Technologies</i>
]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Hand-rewriting: Automatic Rewriting like Natural Handwriting Tomoko Hashida KoheiNishimuraTakeshiNaemura 
The University of Tokyo    The University of Tokyo The University of Tokyo   Fig.1: Decorating characters 
by erasing effect Fig.2:Automatic painting by color-forming effect Fig.3:Automatic correction by combination 
of both effect 1. Introduction Approaches for combining handwriting acts and control by computer are 
gaining force. As two types of general classifications of these approaches, the pen tablet and the digital 
pen are often cited. As for the former approach, inaddition to being able to display handwritten information, 
it candisplay drawing effects and additional information related to thatinformation. Data output by this 
approach, however, can only behandled by digital-type displays. As for the latter approach, although 
written information acquired as paper input can be digitized, it is not possible to express additional 
information asoutput on paper (i.e., a paper display). In contrast to those two approaches, the system 
proposed by the authors called Hand­rewriting automatically performs rewrite processing on paperin correspondence 
with handwriting using pen and paper. More specifically, with the proposed system, when the user writes 
on a piece of paper with a pen, for example, the information written on the paper is automatically erased 
as required, and additional information is displayed on the paper in natural print-like colors.  2. 
Hand-rewriting The Hand-rewriting system combines two technical innovations. The first one is a function 
that automatically erasesspecific areas of the paper, without the need to use an eraser, whenthe user 
writes letters or draws pictures on the paper [Nishimura etal., 2012]. Local erasure on the paper in 
this manner can express letters and pictures written in thermochromic ink by means ofthermal conversion 
by laser heating. As a thermochromic ink pen, a commercial pen ( FRIXION, Pilot Pen Co., Ltd.) with 24colors 
(including black, blue, purple, and pink) is used. This pencan erase the color of written letters (i.e., 
make them colorless andtransparent) by means of frictional heat generated by rubbing witha special rubber. 
In detail, local colors are erased by irradiating ablack film coated on the underside of the paper with 
laser lightfrom a galvanometer scanner controlled by a captured image. When it hits the film, the light 
is converted to heat, and when thetemperature reaches about140°F, colors of local areas are erased. The 
galvanometer scanner can control a laser beam to a distanceof 0.068 mm (in the case of letter-size paper). 
The second innovation is a function that can repeatedly displayadditional related information on the 
paper in color when the userwrites letters or sketches pictures on the paper. The local colors onthe 
paper are created by projecting an ultraviolet (UV) pattern from a UV projector onto paper coated with 
photochromic material (PM). As the PM, spiropyran (which produces colorunder UV light and returns gradually 
to colorless and transparentwhen the UV light is blocked) is used. In detail, four types of PM were used: 
blue, purple, pink, and yellow purchased fromKIROKUSOZAI Co., Ltd. Each PM requires a different time 
forproducing and loosing its respective color. For the projecting theUV pattern, a UV projector fitted 
with a digital micromirror (which can illuminate in units of pixels) and an invisible (i.e.,365-nm wavelength) 
light source is used [Hashida et al., 2011]. Applying the above-described control technologies, three 
types ofinteractive applications of the Hand-rewriting system have been developed. The first application 
is focused on controlling erasureof colors. For example, as shown in Fig. 1, when the user writes something 
by hand in the Roman alphabet, this applicationautomatically erases parts of the characters, and the 
effect is totransform the letters into ornamental writing. The second application is controlling generation 
of colors. As shown in Figure 2, when the user sketches something by hand, this applicationautomatically 
colors the interior of the outline sketch, and the effect is to replicate the sketch itself in the manner 
of a stamp. The third application utilizes controlling generation and erasure ofcolors at the same time. 
When the user makes a mistake while writing something, this application automatically erases the incorrect 
part and displays a guide giving the correct entry in color. Using these applications, the user could 
enjoy performing variouscreative activities on the paper by freely using both kinds ofcontrol (i.e., 
generation and erasure of colors).  3.Concluding remarks A system by which a computer performs automatic 
rewrite processing on paper by means of temperature control and ultraviolet-light control in correspondence 
with handwritingusing a pen and paper was developed. From now onwards, it isplanned to experimentally 
investigate the possibility of linking this system with other electronic pen devices. References NISHIMURA, 
K., KIM, J., AND NAEMURA, T. 2012. AR-eLaser: Erasable Handwriting Interface with Paper and Pen, Submitted 
to SIGGRAPH2012 Talks HASHIDA, T., KAKEHI, Y., AND NAEMURA, T. 2011. Photochromic Sculpture: Volumetric 
Color-forming Pixels, In Proceedings of SIGGRAPH2011 Emerging Technologies Copyright is held by the author 
/ owner(s). SIGGRAPH 2012, Los Angeles, California, August 5 9, 2012. ISBN 978-1-4503-1435-0/12/0008 
  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2343031</article_id>
		<sort_key>1350</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>117</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[Haptic editor]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2342896.2343031</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2343031</url>
		<abstract>
			<par><![CDATA[<p>We propose an interactive editing system for creating haptic-enabled 3D content by drawing shapes in air and copying and pasting surface textures. To realize realistic haptic interaction, we construct the data structure of haptic content composed of kinesthetic layers and a tactile layer. The user can create touchable 3D models by drawing geometries through aerial sketching, setting values of compliance and friction of the geometries, and copying and pasting vibrotactile textures of real objects by using the proposed easy-to-use haptic interface (Figure 1).</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3737609</person_id>
				<author_profile_id><![CDATA[81421597185]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Sho]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kamuro]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[kamuro@tachilab.org]]></email_address>
			</au>
			<au>
				<person_id>P3737610</person_id>
				<author_profile_id><![CDATA[81503684704]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Yuta]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Takeuchi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[yuppon@tachilab.org]]></email_address>
			</au>
			<au>
				<person_id>P3737611</person_id>
				<author_profile_id><![CDATA[81331499667]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Kouta]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Minamizawa]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[kouta@tachilab.org]]></email_address>
			</au>
			<au>
				<person_id>P3737612</person_id>
				<author_profile_id><![CDATA[81365592882]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Susumu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tachi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[tachi@tachilab.org]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>985731</ref_obj_id>
				<ref_obj_pid>985692</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Ryokai, K., Marti, S., and Ishii, H. 2004. I/O brush: drawing with everyday objects as ink. In <i>Proceedings of CHI 2005</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1936510</ref_obj_id>
				<ref_obj_pid>1936324</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Lang, J. and Andrews, S. 2011. Measurement-Based Modeling of Contact Forces and Textures for Haptic Rendering. <i>IEEE Transactions on Visualization and Computer Graphics, Vol. 17, Issue 3</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1597973</ref_obj_id>
				<ref_obj_pid>1597956</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Kamuro, S., Minamizawa, K., Kawakami, N., and Tachi, S. 2009. Pen de Touch. <i>ACM SIGGRAPH 2009 Emerging Technologies</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2196366</ref_obj_id>
				<ref_obj_pid>2195921</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Kamuro, S., Minamizawa, K., Kawakami, N., and Tachi, S. 2011. 3D Haptic Modeling System using Ungrounded Pen-shaped Kinesthetic Display. In <i>Proceedings of IEEE VR 2011</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Minamizawa, K., Kakehi, Y., Nakatani, M., Mihara, S., and Tachi, S. 2012. TECHTILE Toolkit. <i>Laval Virtual 2012 Revolution</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Haptic Editor Sho Kamuro Yuta Takeuchi Kouta Minamizawa Susumu Tachi The University of Tokyo Keio University 
Keio University Keio University   We propose an interactive editing system for creating haptic-enabled 
3D content by drawing shapes in air and copying and pasting surface textures. To realize realistic haptic 
interaction, we construct the data structure of haptic content composed of kinesthetic layers and a tactile 
layer. The user can create touchable 3D models by drawing geometries through aerial sketching, setting 
values of compliance and friction of the geometries, and copying and pasting vibrotactile textures of 
real objects by using the proposed easy-to-use haptic interface (Figure 1). 1. Introduction A number 
of haptic interfaces are proposed thus far. As a next step for the popularity of haptic technologies, 
a creation system for haptic-enabled content is required. At present, in the market, there are a number 
of commercially available visual editors for creating and editing images or 3D models. In creation of 
visual content, it is usual to copy and paste colors or visual textures from one place to another as 
the eye dropper tool in Photoshop [Adobe Systems, Inc.]. I/O brush [Ryokai et al., 2005] has realized 
copying and pasting of various visual images from real world to virtual world. Haptic sensation is an 
integration of a considerable amount of sensory information, and therefore it is difficult to design 
haptic­enabled content. In order to realize the construction of realistic haptic content, some research 
has been conducted on the haptic scanning of real objects as WHaT [Lang and Andrews, 2011]; it enables 
the user to obtain textures by interactively scanning the objects surfaces. However, information gained 
by scanning reveals the minimal bumps of surfaces, and the user cannot directly design and check the 
sensations of touching the content. In this project, we propose a data structure of haptic contents for 
realistic haptic interactions and an editing system for creating haptic-enabled 3D content. To enable 
the user to design haptic content with the use of various and realistic haptic textures, we realize copying 
and pasting of haptic textures from real world to virtual world. 2. Method We have focused on the human 
haptic perception to generate realistic haptic experiences. Our proposed data structure of haptic content 
is composed of 4 layers. Geometry layer represents geometries of haptic content. Compliance layer and 
friction layer represent distributions of the values of compliance and friction respectively. These three 
layers are used for rendering of kinesthetic feedback. And tactile layer represent a distribution of 
vibrotactile textures, which is used for rendering of tactile feedback. We construct a simple and easy-to-use 
haptic interface for the creation and experience of the haptic content (see Figure 1) by integrating 
the force-display method we proposed in [Kamuro et al., 2009] and vibrotactile feedback with a voice 
coil motor. In the * e-mail: {kamuro, yuppon, kouta, tachi}@tachilab.org proposed haptic editor system, 
to construct 3D haptic content, the user first draws its geometry by aerial sketching with the device 
(Figure 2). Shapes are generated based on our sketch-based shape creation method [Kamuro et al., 2011] 
and displayed with 3D images. During this time, the user can set values of compliance and friction for 
kinesthetic feedback. Then, the user picks up some real objects and scans their surfaces with the device 
for copying their texture. A microphone placed at the tip of the device is used for recording the stroking 
vibrations, which represent the haptic textures of the surface (Figure 3) on the basis of the technology 
in [Minamizawa et. al., 2012]. The user can now paste the copied textures on the tactile layer. When 
the user directly touches the content displayed with 3D images, the contact and friction forces are calculated 
on the basis of the physical simulation and are provided to fingers by translating the grip of the device. 
Moreover, textural vibrations are provided by the voice coil on the basis of the stroking velocity toward 
the surfaces. These separated feedback methods result in the realistic sensations of touching textured 
objects efficiently. 3. Application Our simple editing system provides an efficient and intuitive development 
environment for designers of 3D haptic content. They can design the desired content while editing and 
experiencing the effects of such content by using the simple haptic interface. Further, the users can 
enjoy creating content with their own hands in their living rooms. Such user-generated-content will serve 
as a foundation for the popularization of haptic technologies in the future. One possible example of 
the practical uses of the proposed system is haptic-enabled online shopping. The designers prepare various 
types of textures that can be used for customizable products. The users sitting in their home can select 
and test their favorite textures by touching the products virtually via the haptic interface (Figure 
4). This project is supported by JST-CREST Haptic Media Project and JSPS Fellowships. References Ryokai, 
K., Marti, S., and Ishii, H. 2004. I/O brush: drawing with everyday objects as ink. In Proceedings of 
CHI 2005. Lang, J. and Andrews, S. 2011. Measurement-Based Modeling of Contact Forces and Textures for 
Haptic Rendering. IEEE Transactions on Visualization and Computer Graphics, Vol. 17, Issue 3. Kamuro, 
S., Minamizawa, K., Kawakami, N., and Tachi, S. 2009. Pen de Touch. ACM SIGGRAPH 2009 Emerging Technologies. 
Kamuro, S., Minamizawa, K., Kawakami, N., and Tachi, S. 2011. 3D Haptic Modeling System using Ungrounded 
Pen-shaped Kinesthetic Display. In Proceedings of IEEE VR 2011. Minamizawa, K., Kakehi, Y., Nakatani, 
M., Mihara, S., and Tachi, S. 2012. TECHTILE Toolkit. Laval Virtual 2012 Revolution. Copyright is held 
by the author / owner(s). SIGGRAPH 2012, Los Angeles, California, August 5 9, 2012. ISBN 978-1-4503-1435-0/12/0008 
  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2343032</article_id>
		<sort_key>1360</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>118</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>5</seq_no>
		<title><![CDATA[Magic pot]]></title>
		<subtitle><![CDATA[interactive metamorphosis of the perceived shape]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2342896.2343032</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2343032</url>
		<abstract>
			<par><![CDATA[<p>"Magic Pot" is an interactive system which changes the perceived shape of a physical object by using haptic illusion. Haptics is one of the most important sensations in our life, and many researches have been conducted to realize a device to present virtual haptic sensations. However, because these devices are mainly focus on active haptics which aim to reproduce physical force feedback, they become physically too complicated when we try to reproduce complex haptic sensations to use them easily.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3737613</person_id>
				<author_profile_id><![CDATA[81488658104]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Yuki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ban]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[ban@cyber.t.u-tokyo.ac.jp]]></email_address>
			</au>
			<au>
				<person_id>P3737614</person_id>
				<author_profile_id><![CDATA[81331500452]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Takuji]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Narumi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[narumi@cyber.t.u-tokyo.ac.jp]]></email_address>
			</au>
			<au>
				<person_id>P3737615</person_id>
				<author_profile_id><![CDATA[81100172183]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Tomohiro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tanikawa]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[tani@cyber.t.u-tokyo.ac.jp]]></email_address>
			</au>
			<au>
				<person_id>P3737616</person_id>
				<author_profile_id><![CDATA[81496668464]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Michitaka]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hirose]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[hirose@cyber.t.u-tokyo.ac.jp]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1502413</ref_obj_id>
				<ref_obj_pid>1502409</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[A. Lecuyer: Simulating Haptic Feedback Using Vision: a Survey of Research and Applications of Pseudo-Haptic Feedback, Presence: Teleoperators and Virtual Environments, MIT Press, Volume 8, Issuel, pp. 39--Si53, 2009.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1141920</ref_obj_id>
				<ref_obj_pid>1179352</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[S. Schaefer and T. McPhail and J. Warren: Image Deformation Using Moving Least Squares, ACM SIGGRAPH 2006 Papers (SIGGRAPH '06), ACM, New York, NY, USA, pp.533--540, 2006]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Magic Pot: Interactive Metamorphosis of the Perceived Shape Yuki Ban , Takuji Narumi , Tomohiro Tanikawa 
, Michitaka Hirose Graduate School of Information Science and Technology, the University of Tokyo  
 Fig.1 Video See-through System Fig.2 Fitting Hand based on Distortion Fig.3 Interactive Metamorphosis 
of Shape  1. Introduction1  email: {ban, narumi, tani, hirose}@cyber.t.u-tokyo.ac.jp "Magic Pot" is 
an interactive system which changes the perceived shape of a physical object by using haptic illusion. 
Haptics is one of the most important sensations in our life, and many researches have been conducted 
to realize a device to present virtual haptic sensations. However, because these devices are mainly focus 
on active haptics which aim to reproduce physical force feedback, they become physically too complicated 
when we try to reproduce complex haptic sensations to use them easily. On the other hand, there are increasing 
numbers of works which focus on alternative approaches such as passive haptics, which include pseudo-haptics. 
Pseudo-haptics is a kind of cross modal effect between our visual and haptic sense [1], which indicates 
an illusional perception in our haptic sensation evoked by vision. This illusion is evoked when we work 
under an inconsistent situation between the physical behavior of our body and the one observed in our 
vision. For example, it is revealed that the mismatch between a speed of physical computer mouse and 
the one of corresponding cursor in display evokes illusional force feedback on our hands. Pseudo-haptic 
approach is a potential solution for exploiting boundaries and capabilities of the human sensory system 
to simulate haptic information without physical force feedback. In our system, we compose a rendering 
algorithm of visual feedback to evoke pseudo-haptic effects, which affects our haptic perception about 
shape. This algorithm compose the visual feedback in which we can observe as if we were touching the 
virtual shape of the object, and evoke a pseudo-haptic illusion which make us to perceive as if the shape, 
especially the curvature and the size of the static object changed. By using this algorithm, we realize 
an interactive metamorphosis in the shape of "Magic Pot" using simple interface to shapen it. 2. Rendering 
algorithm for Visual Feedback  To make up an inconsistent situation between our vision and haptic sensation 
to evoke effective pseudo-haptic illusion, we composed a video see-through system (Fig. 1). Two cameras 
and a mirror are placed to capture the images around the physical object placed behind the display from 
the position corresponding to the user's eyes. These images are processed to change the shape of an object 
to the one of virtual objects based on the rendering algorithm. Using these composed images for each 
user's eye, we realize stereoscopic video see-through, in which the user can touch an object observed 
in display. Our rendering algorithm is composed of three processes. First, we detect the point on which 
a user is touching on the physical object. The area of the user's hand is extracted from captured images 
based on color to find finger tips of the user's pointing finger and thumb as first and second height 
of the area. Second process is a generation of a distortion map based on difference between the shape 
of the physical object and the one of virtual object. The area of the physical object is extracted from 
captured images as green colored area to detect its outline. Then we compute the distortion which fits 
the outline of the physical object to the virtual one. Finally, we translate and deform the shape of 
the user's hand and fit it to the virtual shape. To deform user's hand, we use the algorithm based on 
moving least squares [2], which can generate the natural deformation considering the rigidity of the 
object, based on the displacement of control points. We displace the user's hand according to the distortion 
map computed in previous step. Then we deform its shape based on the displacement of two control points, 
pointing finger and thumb (Fig. 2). These processes make up the inconsistency between the position and 
shape of user's hand and the one in the visual feedback, and make up pseudo-haptic effect, which makes 
the user to perceive virtual shape in visual feedback. We conducted some studies using this system, and 
revealed 85 percent of people felt the virtual curvature and the size of shape in visual feedback, although 
they were only touching on a statics cylinder. 3. Interactive System to "Change" Shape  With the rendering 
algorithm previously described, we implemented an interactive system which displays a variety of shapes 
of virtual magic pots (Fig. 3). In this experience, a user draws an outline of the magic pot on touch 
panel. The shape of virtual magic pot is generated based on this drawing as the rotating shape. Then 
we compare the shape of a physical cylinder placed behind the display and the one of magic pot in the 
display, and render the visual feedback to evoke pseudo-haptic sensations, which make users feel as if 
the curvature of the cylinder changed according to user input. In addition, the scale of magic pot also 
changes as user touches on it. The distortion map is regenerated according to the change in scale, and 
also makes up pseudo-haptic illusion on the perception about the scale of the object. References [1] 
A. Lecuyer: Simulating Haptic Feedback Using Vision: a Survey of Research and Applications of Pseudo-Haptic 
Feedback, Presence: Teleoperators and Virtual Environments, MIT Press , Volume 8, Issuel, pp. 39-Si53, 
2009. [2] S. Schaefer and T. McPhail and J. Warren: Image Deformation Using Moving Least Squares, ACM 
SIGGRAPH 2006 Papers (SIGGRAPH '06), ACM, New York, NY, USA, pp.533-540, 2006 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2343033</article_id>
		<sort_key>1370</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>119</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>6</seq_no>
		<title><![CDATA[Shadow++]]></title>
		<subtitle><![CDATA[a system for generating artificial shadows based on object movement]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2342896.2343033</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2343033</url>
		<abstract>
			<par><![CDATA[<p>This paper proposes a novel system that generates an artificial shadow by focusing on the position and angle of an object that is used to create a shadow. Our proposed system employs the metaphor of shadow-picture playing. By placing an object in front of a screen, its shadow is projected onto the screen. At the beginning, only the "real shadow" is displayed. After a short while, the shape of the shadow changes, and some additional effects appear on the screen. Our system utilizes the movement of the object as a trigger; it generates a kinetic artificial shadow that corresponds to the movement patterns of the object. When the user moves the object, additional effects change along with the location and position of the object.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3737617</person_id>
				<author_profile_id><![CDATA[81504683102]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Saki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sakaguchi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Kansai University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737618</person_id>
				<author_profile_id><![CDATA[81504685909]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Takuma]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tanaka]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Kansai University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737619</person_id>
				<author_profile_id><![CDATA[81504687861]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Ryo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Shinoki]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Kansai University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737620</person_id>
				<author_profile_id><![CDATA[81504682307]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Mitsunori]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Matsushita]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Kansai University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Shadow++:ASystemfor Generating Arti.cial Shadows Based on Object Movement Saki Sakaguchi TakumaTanaka 
Ryo Shinoki Mitsunori Matsushita Kansai University 1 Introduction This paper proposes a novel system 
that generates an ar­ti.cial shadow by focusing on the position and angle of an object thatis usedto 
createa shadow. Our proposed system employs the metaphor of shadow-picture playing. By plac­ing an object 
in front of a screen, its shadow is projected onto the screen. At the beginning, only the real shadow 
is displayed. After a short while, the shape of the shadow changes, and some additional effects appear 
on the screen. Our system utilizesthemovementofthe objectasa trigger; it generates a kinetic arti.cial 
shadow that corresponds to the movement patterns of the object. When the user moves the object, additional 
effects change along with the location and position of the object. 2 System Components Figure1shows an 
outline of the proposed system. This system consists of two infrared light sources emitting dif­ferentwavelengths, 
two cameras with an infrared pass .lter (IR.lter)tocaptureshadowsmadebythe infraredlights,a beam splitter, 
a projector, and a screen. An object covered with an IR .lter is used to create an arti.cial shadow. 
The system utilizes infrared light as the light source to avoid the mixture of real shadows that are 
created by the light sources and arti.cial shadows that are projected onto the screen from the back by 
the projector. This system uses two CCD cameras with an IR .lter to capture images of the near-infrared 
spectral region. Each camera has an IR .lter (one is IR-80, and the other is IR­92) attached to the lens 
that allows light over a speci.c wavelength (i.e., 800 and 920 nm, respectively) to pene­trate. This 
lets the cameras capture only shadows generated by the light sources. This system uses infrared LEDs 
with two wavelengths as the light sources to create the shadows: 870 and 940 nm. Figure 1: System component 
Figure 2: object Figure 3: application 3 Handheld Object This system uses markers to recognize the location 
and position of the object held by the user. To recognize the object, a two-dimensional black-and-white 
marker is used. The marker is made of black drawing paper. To transmit light, white regions in the marker 
are removed from black regions. When the object is irradiated with infrared light, only the black region 
of the marker is projected onto the screen as an invisible shadow. Because both sides of an object, which 
has a marker attached, are covered with IR­76 .lters, as shown in Figure 2, the user can use the object 
withouthavingtoworry aboutthe marker; thus, turnoffcan be avoided. Figure3shows an application of this 
system. In this ap­plication,byplacingtheobjectinfrontofthe screen,theob­ject s shadow changestoa tree; 
then,the groundandfalling snow appear. When the user changes the object s angle, the angle of thefalling 
snow changes along with it. When the user moves the object up, the root of the tree appears. 4 Concluding 
Remarks This paper proposed a system for generating arti.cial shadows based on the movement of objects. 
With this sys­tem,ausercanhaveanamazingexperiencebyparticipating in a shadow world in which shadows behave 
differently from those in our ordinary lives. Copyright is held by the author / owner(s). SIGGRAPH 2012, 
Los Angeles, California, August 5 9, 2012. ISBN 978-1-4503-1435-0/12/0008 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2343034</article_id>
		<sort_key>1380</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>120</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>7</seq_no>
		<title><![CDATA[Virtual Yamahoko parade with vibration]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2342896.2343034</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2343034</url>
		<abstract>
			<par><![CDATA[<p>With the development of computer graphics and virtual reality technologies, extensive researches have been carried out on digital archiving of cultural properties. For decades, tangible cultural heritage contents including historical crafts, archaeological sites, and historical buildings have been digitally archived. Recently, digital archiving of intangible culture heritage contents, such as traditional festivals and behaviors of participants in cultural events have attracted more and more attention. Yamahoko Parade of Gion Festival, one of the most famous festivals in Japan, becomes a symbolic landscape in Kyoto. During the festival, 32 floats (<i>yama</i> and <i>hoko</i>) representing each local neighborhood parade in the city center. We conduct researches to restore and represent this exciting event, full of Kyoto's tangible and intangible culture and tradition, with cutting-edge technologies such as laser scanning, CG modeling, motion capture system, 4D-GIS, high fidelity sound recording, and vibration system.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3737621</person_id>
				<author_profile_id><![CDATA[81502704325]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Liang]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Li]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Ritsumeikan University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[liliang@fc.ritsumei.ac.jp]]></email_address>
			</au>
			<au>
				<person_id>P3737622</person_id>
				<author_profile_id><![CDATA[81453635348]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Woong]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Choi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Gumma National College of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737623</person_id>
				<author_profile_id><![CDATA[81100078311]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Kozaburo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hachimura]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Ritsumeikan University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737624</person_id>
				<author_profile_id><![CDATA[81320496720]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Keiji]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yano]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Ritsumeikan University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737625</person_id>
				<author_profile_id><![CDATA[81100066602]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Takanobu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nishiura]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Ritsumeikan University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737626</person_id>
				<author_profile_id><![CDATA[81502809423]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Kazuyuki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Izuno]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Ritsumeikan University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1837001</ref_obj_id>
				<ref_obj_pid>1836845</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Choi, W., Fukumori, T., Furukawa, K., Hachimura, K., Nishiura, T., and Yano, K. Virtual yamahoko parade in virtual kyoto. In <i>Proceedings of SIGGRAPH 2010 Posters</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Yano, K., Nakaya, T., and Isoda, Y. 2007. <i>Virtual Kyoto: exploring the past, present and future of Kyoto</i>. Nakanishiya.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Virtual Yamahoko Parade with Vibration Liang Li1 *, Woong Choi2 , Kozaburo Hachimura1, Keiji Yano1, 
Takanobu Nishiura1, Kazuyuki Izuno1 1 Ritsumeikan University 2 Gumma National College of Technology 
(a) (b) (c) Figure 1: (a) Virtual Yamahoko Parade. (b) Acceleration variation illustrated on Google Earth. 
(c) Experimental parade experiencing system with a vibration system. 1 Introduction and Motivation With 
the development of computer graphics and virtual reality tech­nologies, extensive researches have been 
carried out on digital archiving of cultural properties. For decades, tangible cultural her­itage contents 
including historical crafts, archaeological sites, and historical buildings have been digitally archived. 
Recently, digital archiving of intangible culture heritage contents, such as traditional festivals and 
behaviors of participants in cultural events have at­tracted more and more attention. Yamahoko Parade 
of Gion Fes­tival, one of the most famous festivals in Japan, becomes a sym­bolic landscape in Kyoto. 
During the festival, 32 .oats (yama and hoko) representing each local neighborhood parade in the city 
cen­ter. We conduct researches to restore and represent this exciting event, full of Kyoto s tangible 
and intangible culture and tradition, with cutting-edge technologies such as laser scanning, CG model­ing, 
motion capture system, 4D-GIS, high .delity sound recording, and vibration system. In this research we 
built an interactive 3D virtual environment based on our previous work [Choi et al. ]. As a new step, 
we are trying to build a virtual parade experiencing system with a vibration system. During the parade, 
the general publics are not allowed to get on the .oats. However, such a special experience on a special 
occasion can be experienced by anyone via the proposed system. 2 Virtual Yamahoko Parade We construct 
the virtual parade and combine the motion acoustics of the .oats, crews, and spectators using Vizard 
(cf. .gure 1a). (1) The street model of Virtual Kyoto [Yano et al. 2007] is devel­ oped using various 
technologies and materials, such as GIS data, cadastral maps, aerial photos, street photos, and landscape 
paint­ings. (2) Four CG .oats (naginata-hoko, kanko-hoko, fune-hoko, and kitakannon-yama) are created 
by laser-scanning detailed minia­tures of the real .oats, as well as surveying the .oats drawings. (3) 
Four kinds of CG parade crews are created: hikikata who pul­l the .oat; ondotori who lead the .oat; kurumakata 
who control the .oat s directions; and hayashikata who play instruments on the platform of the .oat. 
Since character animation of these crews is crucial for regenerating realistic movements of the parade, 
we use * e-mail:liliang@fc.ritsumei.ac.jp motion capture technique to reproduce the unique motions of 
the crews. (4) We arranged 730 CG models of spectators on both side of the street to regenerate the atmosphere 
of the event. (5) We record­ed the music of the parade played with the traditional instruments of drum, 
.ute, and bell, as well as the sounds of ambient noises made by the .oats, crews, and crowds, using multi-point 
measurement technique. The content of Virtual Yamahoko Parade can be op­erated in 3D vision along with 
high .delity 3D sound environment. Users can interactively control the viewing position and angle in 
the virtual world at real time with a control device such as a gamepad. 3 Vibration System We are trying 
to build a virtual parade experiencing system in an immersive virtual environment that allows the users 
to experience the atmosphere of the parade from the view point of the parade crews. We collected route 
data and acceleration data of fane-hoko using a GPS logger and acceleration sensors during the parade. 
We also captured the scene on the performance stage by a 3D front view camera during the rehearsal parade. 
The acceleration vari­ation along the parade route can be visualized on Google Earth (cf. .gure 1b). 
The acceleration data is transformed into displace­ment data and is employed to reproduce the rolling 
and vibration using a vibration system with 3 degrees of freedom. We built an ex­perimental system by 
integrating vibration, sounds, and 3D videos (cf. .gure 1c). In the evaluation experiments, we received 
positive feedbacks from 6 crews of fune-hoko who had the experiences of riding on the performing stage 
of the .oat. As future work, we are going to build a virtual experiencing system with real-time interactive 
control by integrating the CG content of Virtual Yamahoko Parade and the vibration system. References 
CHOI, W., FUKUMORI, T., FURUKAWA, K., HACHIMURA, K., NISHIURA, T., AND YANO, K. Virtual yamahoko parade 
in virtual kyoto. In Proceedings of SIGGRAPH 2010 Posters. YANO, K., NAKAYA, T., AND ISODA, Y. 2007. 
Virtual Kyoto: exploring the past, present and future of Kyoto. Nakanishiya. Copyright is held by the 
author / owner(s). SIGGRAPH 2012, Los Angeles, California, August 5 9, 2012. ISBN 978-1-4503-1435-0/12/0008 
  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	</section>
	<section>
		<section_id>2343035</section_id>
		<sort_key>1390</sort_key>
		<section_seq_no>19</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Visualization]]></section_title>
		<section_page_from>19</section_page_from>
	<article_rec>
		<article_id>2343036</article_id>
		<sort_key>1400</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>121</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[A collaborative real time previsualization tool for video games and film]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2342896.2343036</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2343036</url>
		<abstract>
			<par><![CDATA[<p>The video game and film industries use previsualization (previs) to produce preliminary animations, walk-throughs and still images of scenes, levels and other assets. Using these collaborative planning tools early in the production pipeline reduces costs later on by providing a consistent and detailed cinematographic vision. Most of the previs literature addresses the problem of camera calibration and shot planning [Nitsche 2008]. Gouchet et al. [2007] developed a natural interface for filming a virtual scene using a real camera. Mori et al. [2011] presented an on-set stereoscopic previs method that employs 3D models of characters and sets to calibrate and plan 3D shots.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[game design]]></kw>
			<kw><![CDATA[previsualization]]></kw>
			<kw><![CDATA[virtual production]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Virtual reality</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Animation</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010866</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Virtual reality</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P3737627</person_id>
				<author_profile_id><![CDATA[81466647327]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Lesley]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Northam]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[lanortha@uwaterloo.ca]]></email_address>
			</au>
			<au>
				<person_id>P3737628</person_id>
				<author_profile_id><![CDATA[81466647282]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Joe]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Istead]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[jistead@uwo.ca]]></email_address>
			</au>
			<au>
				<person_id>P3737629</person_id>
				<author_profile_id><![CDATA[81100155157]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Craig]]></first_name>
				<middle_name><![CDATA[S.]]></middle_name>
				<last_name><![CDATA[Kaplan]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[csk@uwaterloo.ca]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1278297</ref_obj_id>
				<ref_obj_pid>1278280</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Gouchet, X., Quittard, R., and Serikoff, N. 2007. Scp camera. In <i>ACM SIGGRAPH 2007 emerging technologies</i>, ACM, New York, NY, USA, SIGGRAPH '07.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2176654</ref_obj_id>
				<ref_obj_pid>2176584</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Lesley Northam, J. I., and Kaplan, C. S. 2011. Rtfx: Onset previs with unrealengine3. In <i>International Conference on Entertainment Computing</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2077396</ref_obj_id>
				<ref_obj_pid>2077378</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Mori, S., Ichikari, R., Shibata, F., Kimura, A., and Tamura, H. 2011. Enabling on-set stereoscopic mr-based previsualization for 3d filmmaking. In <i>SIGGRAPH Asia 2011 Sketches</i>, ACM, New York, NY, USA, SA '11, 14:1--14:2.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1497011</ref_obj_id>
				<ref_obj_pid>1496984</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Nitsche, M. 2008. Experiments in the use of game technology for pre-visualization. In <i>Proceedings of the 2008 Conference on Future Play: Research, Play, Share</i>, ACM, New York, NY, USA, Future Play '08, 160--165.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 A Collaborative Real Time Previsualization Tool for Video Games and Film LesleyNortham:, Joe Istead 
, Craig S. Kaplan Figure 1: RTFX previs scenario combining real time motion capture (left) with light 
assets from Houdini (middle) into a UDK set. Keywords: CRCategories:I.3.7 [Computer Graphics]: Three-Dimensional 
Graphics and Realism Virtual Reality; I.3.7 [Computer Graph-ics]: Three-Dimensional Graphics and Realism 
Animation previsualization, ADL APDF virtual production, game de­ signLinks: 1 Introduction The video 
game and .lm industries use previsualization (previs) to produce preliminary animations, walk-throughs 
and still images of scenes, levels and other assets. Using these collaborative plan­ ning tools early 
in the production pipeline reduces costs later on by providing a consistent and detailed cinematographic 
vision. Most of the previs literature addresses the problem of camera calibration andshot planning[Nitsche2008]. 
Gouchetetal.[2007]developed a natural interface for .lming a virtual scene using a real camera. Mori 
et al.[2011]presented an on-set stereoscopic previs method that employs 3D models of characters and sets 
to calibrate and plan 3D shots. However, previs is a valuable tool for several other tasks in .lm and 
gaming: planning complex motion and character animation; ef.cient construction and compositing of special 
effects; designing engaging videogame levels. These tasks require cooperation and cohesion amongst the 
existing previs tools; currently, these tools do not incorporate camera work with digital asset construction 
and planning. We present a chat-like client-server architecture for con­ necting previs tools. Our demo 
implementation connects several graphics animation packages with mocap data feeds andgame en­ gines for 
real-time rendering. 2 Approach Existingprevistoolstendto specializeonasingle, speci.cdatapro­ ducing 
and rendering task. Consequently, the production pipeline uses several different tools and packages (e.g., 
Autodesk s Motion-Builder for camera visualization and Maya for mesh construction; Side Effect s Houdini 
for special effects). Our design focuses on connecting any set of graphics or capture applications together 
to produce a single, uni.ed previs scene. This connection provides a collaborative environment where 
many individuals can contribute assets and enhance the shared vision. *e-mail:lanortha@uwaterloo.ca e-mail:jistead@uwo.ca 
e-mail:csk@uwaterloo.ca Our solution,RTFX (real-time specialeffects)[LesleyNorthamand Kaplan 2011], uses 
chat-like communication betweena centralRT-FXServeranda numberofRTFXClients. Eachprevis tool usesan RTFXClient 
plug-in that sends messages consisting of assets and scene data (e.g. camera position and meshes) to 
theRTFXServer. In return, each client receives messages from the server that contain assets and data 
from the other clients in the network. This frame­ workis genericin thatRTFXClient plug-ins canbe written 
for any animation application orgame engine. 3 Discussion Wehave testedRTFXinavarietyofprevisand virtual 
production scenarios, including one that connecteda 16-cameraVicon mocap volume (with both live actors 
and pre-recorded data), Side Effect s Houdini animation package, the Unreal Development Kit (UDK) and 
the Unitygame engine. Another demo procedurally generated a3Dgamelevel inside SideEffect s Houdini and 
then visualizedit live in the Unitygame engine, so when the artist manipulated the level in Houdini, 
theyreceived immediate visual feedback through Unity. This completely bypassed the traditional and costly 
export­ import pipeline for constructinggamelevels. Ourexperiments concludedthatRTFXcanbeusedfor real-timevi­ 
sualization of motion capture (including actors from multiple cap­ ture volumes within a single video 
feed), special effects (including particle systemsandexplosions)andgamelevel construction. We also demonstrated 
compatibility with existing previs tools, includ­ ing real-time visualization of camera work including 
depth of .eld. References GOUCHET, X., QUITTARD, R., AND SERIKOFF, N. 2007. Scp camera. In ACM SIGGRAPH 
2007 emerging technologies, ACM,NewYork,NY, USA, SIGGRAPH 07. LESLEY NORTHAM,J.I., AND KAPLAN,C.S. 2011. 
Rtfx: On­ set previs with unrealengine3. In International Conference on Entertainment Computing. MORI, 
S., ICHIKARI, R., SHIBATA, F., KIMURA, A., AND TAMURA, H. 2011. Enabling on-set stereoscopic mr-based 
previsualization for 3d .lmmaking. In SIGGRAPH Asia 2011 Sketches,ACM,NewYork,NY, USA,SA 11, 14:1 14:2. 
NITSCHE,M. 2008. Experimentsin the useofgame technology for pre-visualization. In Proceedings of the 
2008 Conference on Future Play: Research, Play, Share,ACM,NewYork,NY,USA, Future Play 08, 160 165. Copyright 
is held by the author / owner(s). SIGGRAPH 2012, Los Angeles, California, August 5 9, 2012. ISBN 978-1-4503-1435-0/12/0008 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2343037</article_id>
		<sort_key>1410</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>122</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[A motion sensor interactive interface for viewing and manipulating protein structural data in 3D]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2342896.2343037</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2343037</url>
		<abstract>
			<par><![CDATA[<p>We propose a fun, interactive way to view and alter protein structural data by hand via motion sensors and voice activation. This will enable users to freely zoom, rotate, and view a protein through a friendly hands-on experience. Multiple view and rotation options are given to better understand the structure. The user will also be able to separate the protein by its properties, such as atomic make up, amino acids, backbone, alpha helices and beta sheets, and bond information (Figure 2) and rotation on a user defined pivot point with extreme zooming.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3737630</person_id>
				<author_profile_id><![CDATA[81504687692]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Robyn]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Moncrief]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Oklahoma State University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737631</person_id>
				<author_profile_id><![CDATA[81504687853]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[William]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gobber]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[DePaul University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Krug, Steve, 2000, 2006. Don't Make Me Think. 31--37]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[PyMOL, The PyMOL Molecular Graphics System, Version 1.5.0.1 Schr&#246;dinger, LLC.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 A Motion Sensor Interactive Interface for Viewing and Manipulating Protein Structural Data in 3D Robyn 
Moncrief - Oklahoma State University William Gobber - DePaul University 1 Introduction We propose a 
fun, interactive way to view and alter protein structu­ral data by hand via motion sensors and voice 
activation. This will enable users to freely zoom, rotate, and view a protein through afriendly hands-on 
experience. Multiple view and rotation options are given to better understand the structure. The user 
will also be able to separate the protein by its properties, such as atomic makeup, amino acids, backbone, 
alpha helices and beta sheets, and bondinformation (Figure 2) and rotation on a user defined pivot point 
with extreme zooming.  Figure 2. Menu to isolate structural properties. Figure 1. Using motion sensors, 
the user manipulates the protein.  2 Our Approach Figure 3. View after user specified pivot point 
using keyword. Our program is a Windows-based application written in C#. It is controlled exclusively 
with Microsoft's Kinect sensor.  It offers controller-free, immediate, and easy use. Upon launching the 
program, the Kinect control is made apparentby an immediate appearance of two hand icons (one right,one 
left).The user is presented with a main menu of options, including someintroductory educational materials 
and the option to select a protein to interact with. In order to select an option, the user must navigate 
the right hand icon and hold it over a selection, clearly defined by a large box-like border. This makes 
it obvious what is select-able, increasing usability [KRUG 2006]. A ring around the hand icon appears 
to signify the user must hold on that selection to execute it. The user is presented with a selection 
of pre-loaded proteins, eachwith unique properties. This Protein View section of the program is the core 
function, Choosing a protein will display the protein as a visualization of color-coded spheres (Figure 
1). While two hands are actively posed in front of the Kinect sensor, the user can zoom in or out on 
the 3D model. Moving one's handsapart from each other zooms in and closer together zooms out(Figure 1). 
If the user drops one hand to their side, rotate functiona­ lity is activated. The lone hand can rotate 
the model in whatever direction it goes on-screen. Finally, an additional zoom function is available 
where the user places their hand over a specific part of the model and speaks a trigger word, utilizing 
Kinect's voice recogni­ tion. This results in the change in the pivot point and allows the user to zoom 
and rotate on the specified area (Figure 3). Several different options exist for viewing. One such option, 
"Atoms" will display a sub-menu of the various atoms that make upthe protein (Figure 2). By selecting 
an atom from this sub-menu, the user can alter the model to display just the isolated atoms of that type. 
There is also an option for viewing just the various atomic bonds in the protein and there is also an 
option to pull up statisticaldata. Other view options include surface and mesh, lines and sticks,and 
ribbon and cartoon, each allowing for it s own structural isolation. There is also an option for viewing 
just the various atomic bonds inthe protein, an option to pull up statistical data on the screen about 
theprotein, and an option to pull up a different protein model to be ghosted in the background for shape 
comparison. The 3D model is created using a file-loading/reading code that is specifically equipped to 
handle the data files from the Protein Data Bank (PDB). The data tells us local x, y, and z coordinates 
which are applied to the 3D environment. Each atom of the protein is drawn asa sphere, and scales it 
based on atomic weight. We have applied an algorithm to tone down the drastic difference in atomic weights 
so that the larger atoms do not completely overshadow the smaller ones. 3 Future Work Other work includes 
a quicker way to model the protein in the envi­ronment by eliminating hidden atoms and creating a more 
fluid real­ time experience, helping speed up the display of the ghosted com­parison models and the in-progress 
3D stereoscopic experience.Linking the visualization tool directly to the PDB database will allow users 
to choose a protein and have it created in real-time. Also, a multi-user option will be applied to allow 
for comparative proteinstructures for two proteins. This has the potential to allow for a better learning 
experience with pre-set proteins that users will be asked to find structural differences in and answer 
questions. References KRUG, STEVE, 2000, 2006. Don't Make Me Think.  31-37 PyMOL, The PyMOL Molecular 
Graphics System, Version 1.5.0.1 Schrödinger, LLC. Copyright is held by the author / owner(s). SIGGRAPH 
2012, Los Angeles, California, August 5 9, 2012. ISBN 978-1-4503-1435-0/12/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2343038</article_id>
		<sort_key>1420</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>123</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Advanced GPU-based ray casting for bricked datasets]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2342896.2343038</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2343038</url>
		<abstract>
			<par><![CDATA[<p>Since the 90s, the Direct Volume Rendering shows itself as an efficient tool for the visual analysis of volumetric datasets. There are approaches that allow for the real-time Ray Casting for visualization of the datasets that can be divided into bricks and entirely loaded into the GPU memory. We describe particular details, implemented in our volume rendering engine, that significantly improve rendering quality and performance.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3737632</person_id>
				<author_profile_id><![CDATA[81504687690]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Nikolay]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gavrilov]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Lobachevsky State University of Nizhni Novgorod, Russia]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737633</person_id>
				<author_profile_id><![CDATA[81388600686]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Vadim]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Turlapov]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Lobachevsky State University of Nizhni Novgorod, Russia]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Daniel R. et al, 2008. Efficient GPU-Based Texture Interpolation using Uniform B-Splines, <i>In IEEE Transactions on Journal of Graphics, GPU, &amp; Game Tools</i>, Vol. 13, No. 4, pp 61--69.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Daniel R. et al, 2006, Optimizing GPU Volume Rendering, <i>Li Journal of WSCG'06 14(1-3)</i>, pp. 9--16.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>602106</ref_obj_id>
				<ref_obj_pid>602099</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Guthe, S. et al, 2002, Efficient Interactive rendering of large volume data sets, <i>VIS 2002</i>. IEEE, pp. 53--60]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2386437</ref_obj_id>
				<ref_obj_pid>2386410</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Johanna B. et al, Smooth Mixed-Resolution GPU Volume Rendering, <i>IEEE International Symposium on Volume and Point-Based Graphics (VG '08);</i> 2008. pp. 163--170.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Vincent V. et al, 2008, Simple Empty-Space Removal for Interactive Volume Rendering, <i>Journal of Graphics, GPU, and Game Tools</i>, Volume 13, Issue 2, pp. 21--36]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Advanced GPU-based Ray Casting for Bricked Datasets Nikolay Gavrilov, Vadim Turlapov Lobachevsky State 
University of Nizhni Novgorod, Russia   (a)             (b)              
    (b) Figure 1: a) bounding boxes fitting; b) empty space skipping and early ray termination; c) 
tri-linear (left) &#38; tri-cubic (right) interpolation modes side-by-side comparison (MANIX test dataset). 
 1. Introduction Since the 90s, the Direct Volume Rendering shows itself as an efficient tool for the 
visual analysis of volumetric datasets. There are approaches that allow for the real-time Ray Casting 
for visualization of the datasets that can be divided into bricks and entirely loaded into the GPU memory. 
We describe particular details, implemented in our volume rendering engine, that significantly improve 
rendering quality and performance. 2. Implementation Aspects In order to be able to visualize the datasets, 
that do not fit into a single 3D texture memory, we make the data decomposition into bricks. In order 
to perform correct splicing without any artifacts on the bricks bounds we perform the bricks 2-voxel 
overlapping (3-voxel in tri-cubic filtering case). To render the dataset we render each brick one after 
another. The bricking allows us to ignore fully empty bricks and do not upload them to the GPU. Using 
small 3D textures augments the sampling speed from them during the ray casting process. The front-to-back 
rendering order of the bricks allows us to ignore those image regions of the bricks that are occluded 
from the observer by the previously rendered bricks, so that we obtain a high-level early ray termination 
(Figure 1b). The empty space skipping strategy is also well applicable for the decomposed data because 
we can bound each single brick separately, which is much easier than to bound the whole dataset (Vincent 
V. et al, 2008). For each brick we use a simple bounding box, which is fitted automatically to the visible 
data while the user is applying the transfer function (Figure 1a). Our engine also provides high rendering 
quality by reducing certain artifacts. Because of the finite number of the steps the ray may skip meaningful 
features in the dataset. As the result wood-like image artifacts may appear. This wood-likeness appears 
because rays start from the same plane (i.e. bounding box face). This artifacts regularity can be removed 
by randomization of ray start positions. Then these obtained randomized images can be accumulated so 
that a user will see the average image without noise. The visualization quality can be improved by the 
tri-cubic filtering instead of the common tri-linear one. In order to make a single tri-cubic sampling 
it is necessary to make 8 tri-linear samplings from the same dataset (Daniel R. et al, 2008). The optimal 
brick sizes appeared to be 64³ and 128³ cubes depending on the dataset. High performance is mostly caused 
by the empty space skipping and the small bricks size. However, the choice of a too small brick size 
(e.g. of size 32³) involves the enormous amount of the bricks to draw, which obviously causes the lack 
of rendering performance. The number of samplings per step is not so crucial aspect for the rendering 
performance, e.g. the simple DVR is 3 times faster than DVR with tri-cubic filtering, but not 8 times. 
The rendering time is also wasted when switching to next brick, because we are to copy the rendered result 
to the OpenGL Frame Buffer for reading. We use this buffer to merge the rendered bricks and to perform 
possible early ray termination before casting a ray. References Daniel R. et al, 2008. Efficient GPU-Based 
Texture Interpolation using Uniform B-Splines, In IEEE Transactions on Journal of Graphics, GPU, &#38; 
Game Tools, Vol. 13, No. 4, pp 61-69. Daniel R. et al, 2006, Optimizing GPU Volume Rendering, Li Journal 
of WSCG'06 14(1-3), pp. 9-16. Guthe, S. et al, 2002, Efficient Interactive rendering of large volume 
data sets, VIS 2002. IEEE, pp. 53 - 60 Johanna B. et al, Smooth Mixed-Resolution GPU Volume Rendering, 
IEEE International Symposium on Volume and Point-Based Graphics (VG 08); 2008. pp. 163 170. Vincent V. 
et al, 2008, Simple Empty-Space Removal for Interactive Volume Rendering, Journal of Graphics, GPU, and 
Game Tools, Volume 13, Issue 2, pp. 21-36 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2343039</article_id>
		<sort_key>1430</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>124</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[CrowDiffuse]]></title>
		<subtitle><![CDATA[information diffusion over crowds with social network]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2342896.2343039</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2343039</url>
		<abstract>
			<par><![CDATA[<p>Crowd simulation is to produce collective behaviors through simulating the movement process of a number of agents. Information diffusion is to investigate how information propagates among people, especially on social network. This work considers both the spatial and social perspectives to simulate the dynamics of information diffusion over crowds with underlying social relationships. Existing works [Chao 2009; Neumann 2000] model the communications between agents so that elicited emotions can be spread in the space while others (e.g. [Kempe 2003]) study how information diffuses over a social network. However, the former considers only spatial aspects and the latter utilizes only the social relationships. To combine both spatial and social aspects for diffusion, we propose a framework, <i>CrowDiffuse</i>, which simulates the crowd with an underlying social network and allows information to propagate over agents. In <i>CrowDiffuse</i>, agents can affect each other if they satisfy both spatial and social condition. The spatial condition is satisfied if two agents come close enough in the space while the social condition is met if two agents possess a relationship in the social network. In addition, we further investigate the <i>targeted diffusion</i> problem, which will be described in the following.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3737634</person_id>
				<author_profile_id><![CDATA[81413602545]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Cheng-Te]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Li]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[National Taiwan University, Taipei, Taiwan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[d98944005@csie.ntu.edu.tw]]></email_address>
			</au>
			<au>
				<person_id>P3737635</person_id>
				<author_profile_id><![CDATA[81466643657]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Hsun-Ping]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hsieh]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[National Taiwan University, Taipei, Taiwan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737636</person_id>
				<author_profile_id><![CDATA[81430596514]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Shou-De]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[National Taiwan University, Taipei, Taiwan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Chao, W. M. and Li, T. Y. Simulating Crowd Behaviors with a Communication Model. <i>CASA</i> 2008.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>956769</ref_obj_id>
				<ref_obj_pid>956750</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Kempe, D., Kleinberg, J., and Tardos, E. Maximizing the Spread of Influence through a Social Network. <i>ACM SIGKDD</i> 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Neumann, R. and Strack, F. Mood Contagion: The Automatic Transfer of Mood between Persons. Journal of Personality and Social Psychology, 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Newman, M. E. J. Fast Algorithm for Detecting Community Structure in Networks. Physical Review, E 69, 066133, 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>37406</ref_obj_id>
				<ref_obj_pid>37401</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Reynolds, C. W. 1987. Flocks, Herds and Schools: A Distributed Behavior Model. <i>ACM SIGGRAPH</i> 1987.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Wasserman, S. and Faust, K. Social Network Analysis. 1994.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 CrowDiffuse: Information Diffusion over Crowds with Social Network Cheng-Te Li * Hsun-Ping Hsieh Shou-De 
Lin Graduate Institute of Networking and Multimedia, National Taiwan University, Taipei, Taiwan   Figure 
1: Spatio-social information diffusion with spatial and link-based social condition (purple indicates 
being informed). 1. Introduction Crowd simulation is to produce collective behaviors through simu­lating 
the movement process of a number of agents. Information diffusion is to investigate how information propagates 
among peo­ple, especially on social network. This work considers both the spatial and social perspectives 
to simulate the dynamics of infor­mation diffusion over crowds with underlying social relationships. 
Existing works [Chao 2009; Neumann 2000] model the communi­cations between agents so that elicited emotions 
can be spread in the space while others (e.g. [Kempe 2003]) study how information diffuses over a social 
network. However, the former considers only spatial aspects and the latter utilizes only the social relationships. 
To combine both spatial and social aspects for diffusion, we pro­pose a framework, CrowDiffuse, which 
simulates the crowd with an underlying social network and allows information to propagate over agents. 
In CrowDiffuse, agents can affect each other if they satisfy both spatial and social condition. The spatial 
condition is satisfied if two agents come close enough in the space while the social condition is met 
if two agents possess a relationship in the social network. In addition, we further investigate the targeted 
diffusion problem, which will be described in the following. 2. Spatio-Social Information Diffusion 
We use Reynold s flocking model [1987] for crowd simulation. The DBLP1 database is used to construct 
the social network, in which each node (i.e., agent) stands for an author and each link is co-authorship. 
Diffusion Method. The diffusion process starts from specifying a set of agents as initial informed ones. 
As the simulation proceeds, more and more agents will be informed. Finally all agents in the environment 
are informed. The diffusion from one agent to another is determined by the spatial and social conditions. 
One agent x can successfully affect y if and only if both conditions are satisfied. For the spatial condition, 
when an agent x is exposed to the local visible range of y who has been informed, the spatial condition 
is satisfied. The social condition is divided into link-based and com­munity-based cases. We consider 
Independent Cascade model [Kempe 2003] as the link-based case: an informed agent v is given a single 
chance to affect each of its neighbor u who is not informed yet in the network. And it succeeds with 
a probability pv,u inde­pendently of the spreading history so far. If v succeeds, u will be informed. 
That says, when v and u must have a relationship in the social network and meet the probability pv,u, 
the social condition will be satisfied. Figure 1 shows the diffusion example with one initial informed 
agent and pv,u=0.5. We can find the information is propagated from few to most agents. For community-based 
case, it is looser: if u and v meet and belong to the same community [Newman 2004] and satisfy pu,v, 
it meets the social condition. Targeted Diffusion. We use our spatio-social diffusion model to answer 
if someone want to distribute an idea or certain infor­mation, how to select initial informed agents 
to diffuse the idea efficiently? That is, how to pick the initial informed agents to finally affect all 
agents with as less time as possible? To study the targeted diffusion, we consider spatial and social 
factors to devise six strategies of selecting initial informed agents: (1) Random Se­lection, (2) select 
agents located in Center Location, (3) select agents with Densest Neighborhood in the space. The following 
consider centrality measures in social network area [Wasserman 1994]: select agents with highest (4) 
Degree (more relationships to others), (5) Closeness (averagely closer to others based on graph distances), 
and (6) Betweenness (higher chances to be bridges between communities). Experimental Results. Under some 
rounds, if a strategy can in­forms more agents, comparing to other strategies, it is regarded as a more 
effective strategy. We set the number of initially-informed agents to 4, the probability of independent 
cascade pv,u=0.5, and #community=5. For each strategy, we simulate the diffusion 100 times and compute 
the average number of informed agents. Figure 2 shows the results. We can find the social strategies 
(i.e., degree, closeness, betweenness) generally have better performance. We think it is because agents 
are harder to satisfy the social condition than the spatial condition. The closeness is best since agents 
with higher closeness tend to reach others with fewer steps in the net­work. For the other three, the 
dense-neighbor is better since agents with higher density in the neighborhood have higher potential to 
 Figure 2: Experimental results of targeted diffusion under the link-based (left) and the community-based 
(right) social conditions.  References Chao, W. M. and Li, T.Y. Simulating Crowd Behaviors with a Communica­tion 
Model. CASA 2008. Kempe, D., Kleinberg , J., and Tardos, E. Maximizing the Spread of Influ­ence through 
a Social Network. ACM SIGKDD 2003. Neumann, R. and Strack, F. Mood Contagion: The Automatic Transfer 
of Mood between Persons. Journal of Personality and Social Psychology, 2000. Newman, M. E. J. Fast Algorithm 
for Detecting Community Structure in Networks. Physical Review, E 69, 066133, 2004. Reynolds, C.W. 1987. 
Flocks, Herds and Schools: A Distributed Behavior Model. ACM SIGGRAPH 1987. Wasserman, S. and Faust, 
K. Social Network Analysis. 1994.  Copyright is held by the author / owner(s). SIGGRAPH 2012, Los Angeles, 
California, August 5 9, 2012.* Email: d98944005@csie.ntu.edu.tw ISBN 978-1-4503-1435-0/12/0008 1 http://www.informatik.uni-trier.de/~ley/db/ 
 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2343040</article_id>
		<sort_key>1440</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>125</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>5</seq_no>
		<title><![CDATA[Light-field supported fast volume rendering]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2342896.2343040</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2343040</url>
		<abstract>
			<par><![CDATA[<p>We present a combination of light-field rendering and volume rendering to enable the interactive exploration of large volumetric data sets. We recycle previously rendered images and use the idle times of the volume renderer for filling a cached-managed light field. The final images are then composed from both: light-field rendering and volume rendering -- depending on the state of the light-field cache. Our method is never slower than the stand-alone volume render -- but it accelerates significantly over time.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[image stacks]]></kw>
			<kw><![CDATA[light fields]]></kw>
			<kw><![CDATA[real-time]]></kw>
			<kw><![CDATA[volume rendering]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.3</cat_node>
				<descriptor>Display algorithms</descriptor>
				<type>S</type>
			</primary_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P3737637</person_id>
				<author_profile_id><![CDATA[81503672096]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Clemens]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Birklbauer]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Johannes Kepler University Linz, Austria]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[clemens.birklbauer@jku.at]]></email_address>
			</au>
			<au>
				<person_id>P3737638</person_id>
				<author_profile_id><![CDATA[81100622976]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Oliver]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bimber]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Johannes Kepler University Linz, Austria]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[oliver.bimber@jku.at]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Light-Field Supported Fast Volume Rendering Clemens Birklbauer and Oliver Bimber* Institute of Computer 
Graphics, Johannes Kepler University Linz, Austria  Figure 1: Visualization of a drosophila: full-resolution 
volume rendering at 0.4 fps (a), volume rendering preview at 25 fps (b), our method at 25 fps (c). Color-coded 
contributions of different rendering sources during rotation (d-g): full-resolution volume rendering 
(green), volume rendering preview (red), full-resolution light-.eld rendering (gray), and low-resolution 
light-.eld rendering (blue). The visible seams are the result of the microscope s scanning process not 
of visualization. Abstract We present a combination of light-.eld rendering and volume ren­dering to 
enable the interactive exploration of large volumetric data sets. We recycle previously rendered images 
and use the idle times of the volume renderer for .lling a cached-managed light .eld. The .nal images 
are then composed from both: light-.eld rendering and volume rendering depending on the state of the 
light-.eld cache. Our method is never slower than the stand-alone volume render but it accelerates signi.cantly 
over time. CR Categories: I.3.3 [COMPUTER GRAPHICS]: Picture/Image Generation Display algorithms; Keywords: 
light .elds, volume rendering, image stacks, real-time 1 Introduction Advances in imaging technology 
leads to a continues increase of image data sets. Modern scanning microscopes, for instance, pro­duce 
image stacks with a megapixel lateral resolution and many hundreds to thousands slices in axial direction. 
This trend will con­tinue resulting in very large volumetric data sets that are dif.cult to explore 
interactively, since the complexity of volume rendering is proportional to spatial and lateral resolution 
of the data. Light-.eld rendering is a fast and simple image-based rendering method that requires pre-computed 
image data. For volume ren­dering, each costly computed image is discarded after changing the *e-mail: 
{clemens.birklbauer,oliver.bimber}@jku.at viewing parameters, while it becomes idle if the viewing parameters 
are not changed and the visualization does not need to be updated. We combine light-.eld rendering and 
volume rendering with two goals: We recycle previously rendered images and use the idle times for .lling 
a cached-managed light .eld. The .nal images are then composed from light-.eld rendering and from volume 
rendering depending on the state of the light-.eld cache. This leads to a signi.cant increase in rendering 
performance and to the ability of exploring large volumetric datasets interactively. 2 Our Approach 
For light-.eld rendering, we support a spherical light-.eld param­eterization to enable surround navigation, 
as well as changing the .eld-of-view, aperture and focus. When not interacting with the volume, the volume 
renderer .rst computes, displays and caches a full-resolution image for the current viewing parameters, 
while it will then compute portions of the cached light-.eld data structure in the background. We apply 
dead reckoning to the users interaction pattern to determine the priority-order of these portions. The 
vol­ume renderer always computes the image portions at the necessary level of detail (LOD) depending 
on the adjusted viewing param­eters. They are cached and remain valid for light-.eld rendering until 
a higher LOD is required. In this case, the cache has to be updated by the volume renderer. For rendering 
the .nal image, the light-.eld renderer is used as much as possible. If the required por­tions of the 
light-.eld are cached, this results in a high frame-rate. Only those portions that cannot be provided 
by the light-.eld ren­derer have to be produced by the slow volume renderer. They will be cached as soon 
as they are available. In addition, we integrate the volume renderer s fast preview mode. Every time, 
the LOD produced by this mode is higher than the LOD in the light-.eld cache, we use the preview to deliver 
the corresponding parts of the .nal image. Furthermore, we manage a second, lower-resolution light-.eld 
cache that can be .lled quicker than the full-resolution cache, and allow for a dynamic, on-demand increase 
or decrease of the light-.elds angular resolution. Thus, the .nal image is a piece-wise composition of 
image portions coming from four differ­ent sources: the full-resolution light-.eld, the low-resolution 
light­.eld, the volume renderer s preview and the volume rendered at full resolution. Copyright is held 
by the author / owner(s). SIGGRAPH 2012, Los Angeles, California, August 5 9, 2012. ISBN 978-1-4503-1435-0/12/0008 
  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2343041</article_id>
		<sort_key>1450</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>126</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>6</seq_no>
		<title><![CDATA[Omnistereo images from ground based lidar]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2342896.2343041</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2343041</url>
		<abstract>
			<par><![CDATA[<p>Street level panoramic images provide users with an immersive means to remotely explore street level features. Stereoscopic panoramic images, also referred to as omnistereo panoramics provide an even more compelling view. As 3D display technology becomes less expensive and more ubiquitous; the ability to view these types of images is far easier. In addition to providing a <i>wow</i> factor, 3D stereo enhances the sense of scale and distance, factors which are useful when planing a visit a new location. Omnistereo panoramas cannot be photographed by a pair of full 360&#176; panoramic cameras as each would capture the other when the panoramic image is taken. Typically they are taken by a pair of cameras mounted on either end of a bracket which is then rotated [Peleg et al. 2001]. Each image is broken into a set of successive strips. These strips are then mosaiced together to form the left and right panoramic image. This, however, requires the camera remains stationary while the pictures are taken, and unless the rotation rate and shutter speeds are quite high, that the scene remain static as well. These two restrictions prevent the live capture of panoramic data from a vehicle driving down a busy street. The technique described here uses very dense Lidar and calibrated panoramic images captured by a ground mobile collection vehicle equipped with a Lidar unit and a single panoramic camera, driven at posed speed limits, to automatically create street level omnistereo panoramas. These stereo images are higher quality and therefore more realistic than any other method which uses mobile collection.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3737639</person_id>
				<author_profile_id><![CDATA[81504685441]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Craig]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Barnes]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[L&C Research, Nokia]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[craig.barnes@nokia.com]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>370557</ref_obj_id>
				<ref_obj_pid>370550</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Peleg, S., Ben-Ezra, M., and Pritch, Y. 2001. Omnistereo: panoramic stereo imaging. <i>IEEE Transactions on Pattern Analysis and Machine Intelligence 23</i>, 3, 279--290.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Yang, R., and Wang, L. 2006. View-Dependent Textured Splatting. <i>Environment 22</i>, 7, 456--467.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Omnistereo images from ground based Lidar Craig Barnes * L&#38;C Research, Nokia  Figure 1: Red/Blue 
analygraph of a generated omnistereo image using texture splatting of very dense Lidar and calibrated 
images. 1 Introduction Street level panoramic images provide users with an immersive means to remotely 
explore street level features. Stereoscopic panoramic images, also referred to as omnistereo panoramics 
pro­vide an even more compelling view. As 3D display technology be­comes less expensive and more ubiquitous; 
the ability to view these types of images is far easier. In addition to providing a wow factor, 3D stereo 
enhances the sense of scale and distance, factors which are useful when planing a visit a new location. 
Omnistereo panora­mas cannot be photographed by a pair of full 360 . panoramic cam­eras as each would 
capture the other when the panoramic image is taken. Typically they are taken by a pair of cameras mounted 
on ei­ther end of a bracket which is then rotated [Peleg et al. 2001]. Each image is broken into a set 
of successive strips. These strips are then mosaiced together to form the left and right panoramic image. 
This, however, requires the camera remains stationary while the pictures are taken, and unless the rotation 
rate and shutter speeds are quite high, that the scene remain static as well. These two re­strictions 
prevent the live capture of panoramic data from a vehicle driving down a busy street. The technique described 
here uses very dense Lidar and calibrated panoramic images captured by a ground mobile collection vehicle 
equipped with a Lidar unit and a single panoramic camera, driven at posed speed limits, to automatically 
create street level omnistereo panoramas. These stereo images are higher quality and therefore more realistic 
than any other method which uses mobile collection. 2 Process and Results The basis for this method 
is textured point splatting [Yang and Wang 2006], which uses point cloud data along with calibrated panoramic 
images to texture map point splats. This generates much richer imagery than that achieved by point cloud 
colorization. In textured point splatting, each point is expanded into a small texture mapped camera 
facing billboard called a splat. A 2D panoramic image is used to texture each splat using spherical texture 
projec­ *e-mail:craig.barnes@nokia.com tion. This is done by projecting the vertices of every splat onto 
a unit sphere centered at the panoramic image s origin. The resulting spherical coordinates are mapped 
to the corresponding image coor­dinate which is used to texture each splat. As this can be done using 
the GPU the scene cane be rendered about 25 -30 frame/second de­pending on the point density. When rendered 
from the panoramic image s location the result of the splatting is a recreation of the original image 
for all areas where Lidar data exists. This technique also provides viewpoint interpolation which allows 
the the scene to be viewed from a different viewpoint other than the one collected. Like standard stereoscopic 
images this is the key to creating omnistereo panoramic with street level panoramic images. Each omnistereo 
image is created by offsetting the camera by a dis­placement value eyeSep . Next the full 360 . around 
the center of 2 the rotation axis (i.e center of the panoramic image) is sub-divided into N slices. This 
becomes the horizontal .eld of view. For each slice, the camera is rotated by 360 . . When all N slices 
have been N rendered each slice is concatenated together to product a full 360 . panoramic image. This 
process is repeated for the other eye with the camera displaced by- eyeSep . The results of these steps 
is a pair 2 of panoramic images, one for each eye, which can be processed to produce the red/blue analygraph 
shown in the .gure above. It should be noted that areas without point data can be addressed by merging 
multiple point clouds from separate drives which would .ll in areas not sampled by a single drive, such 
as areas behind the collection vehicle and parked cars. References PELEG, S., BEN-EZRA, M., AND PRITCH, 
Y. 2001. Omnistereo: panoramic stereo imaging. IEEE Transactions on Pattern Anal­ysis and Machine Intelligence 
23, 3, 279 290. YANG, R., AND WANG, L. 2006. View-Dependent Textured Splat­ting. Environment 22, 7, 456 
467. Copyright is held by the author / owner(s). SIGGRAPH 2012, Los Angeles, California, August 5 9, 
2012. ISBN 978-1-4503-1435-0/12/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2343042</article_id>
		<sort_key>1460</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>127</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>7</seq_no>
		<title><![CDATA[Rupture simulation of a bubble with MPS]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2342896.2343042</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2343042</url>
		<abstract>
			<par><![CDATA[<p>One of the most difficult tasks with computer graphics is visualization of liquid, especially bubble that is constructed with air and water. Many scientists have struggled to represent bubble behavior. [Hong et al. 2008] proposed a hybrid method of Eulerian grids and Lagrangian particles in order to visualize small-scale bubbles in large-scale water body. Bubble particles are seeded randomly at the bottom and disappear when they reach the surface. In addition, [Ihmsen et al. 2011] proposed another two-way coupling method for water and air because of the large density ratio of water to air. Each phase is treated separately and combined together. Bubbles are seeded on the fly, treated as foam on the surface and deleted after a user defined time. Both researches use SPH (Smoothed Particle Hydrodynamics) as the particle method and succeeded to visualize the bubble behavior; however, bubbles are generated in calm water and the water is incompressible. They also did not treat the behavior of bubble rupture. On the other hand, [Bird et al. 2010] revealed that numerous small bubbles (daughter bubbles) are generated when a bubble ruptures, and the small bubbles create a ring. Therefore, we propose a method to simulate the rupturing behavior of a bubble with MPS (Moving Particle Semi-implicit), which is another particle method that can treat incompressible fluid.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3737640</person_id>
				<author_profile_id><![CDATA[81543608056]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Nobuhiko]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Mukai]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tokyo City University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[mukai@cs.tcu.ac.jp]]></email_address>
			</au>
			<au>
				<person_id>P3737641</person_id>
				<author_profile_id><![CDATA[81504687666]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Noburo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kagatsume]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tokyo City University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737642</person_id>
				<author_profile_id><![CDATA[81466645409]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Masashi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nakagawa]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tokyo City University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Bird, J., Ruiter, R., Courbin, L., and Stone, H. 2010. Daughter bubble cascades produced by folding of ruptured thin films. <i>Nature 465</i>, 10, 759--762.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1360647</ref_obj_id>
				<ref_obj_pid>1360612</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Hong, J., Lee, H., Yoon, J., and Kim, C. 2008. Bubbles alive. <i>ACM Trans. on Graphics 27</i>, 3, 48:1--48:4.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Ihmsen, M., Bader, J., Akinci, G., and Teschner, M. 2011. Animation of air bubbles with SPH. <i>International Conference on Graphics Theory and Application</i>, 225--234.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Rupture Simulation of a Bubble with MPS Nobuhiko Mukai * , Noburo Kagatsume, and Masashi Nakagawa Tokyo 
City University Figure 1: An image sequence showing that a bubble is rupturing. (a) a bubble is .oating 
up from the bottom of the water to the surface with changing its shape by buoyancy. Red spheres show 
air particles in the bubble. (b) The bubble is on the surface of the water. Green spheres show .lm particles 
constructing the surface of the bubble. (c) The bubble is rupturing. Air particles (red particles), which 
was inside the bubble, are dispersing out of the bubble. (d) The bubble has ruptured and a ring is generated 
with some .lm particles (green particles). 1 Introduction One of the most dif.cult tasks with computer 
graphics is visual­ization of liquid, especially bubble that is constructed with air and water. Many 
scientists have struggled to represent bubble behav­ior. [Hong et al. 2008] proposed a hybrid method 
of Eulerian grids and Lagrangian particles in order to visualize small-scale bubbles in large-scale water 
body. Bubble particles are seeded randomly at the bottom and disappear when they reach the surface. In 
addition, [Ihmsen et al. 2011] proposed another two-way coupling method for water and air because of 
the large density ratio of water to air. Each phase is treated separately and combined together. Bubbles 
are seeded on the .y, treated as foam on the surface and deleted after a user de.ned time. Both researches 
use SPH (Smoothed Par­ticle Hydrodynamics) as the particle method and succeeded to visu­alize the bubble 
behavior; however, bubbles are generated in calm water and the water is incompressible. They also did 
not treat the behavior of bubble rupture. On the other hand, [Bird et al. 2010] revealed that numerous 
small bubbles (daughter bubbles) are gen­erated when a bubble ruptures, and the small bubbles create 
a ring. Therefore, we propose a method to simulate the rupturing behavior of a bubble with MPS (Moving 
Particle Semi-implicit), which is another particle method that can treat incompressible .uid. 2 Method 
Figure 2: Bubble Model. Fig. 2 shows our model of a bubble, which is constructed with 3 kinds of particles: 
water, air and .lm particles. In the water, a bubble is constructed with water and air particles and 
is .oating up to the surface with changing its shape by buoyancy. Buoyancy is calculated with the sum 
of the force that works on each particle. The particle position is de.ned by solving equation of continuity 
*e-mail: mukai@cs.tcu.ac.jp and Navier-Stokes equation with surface tension shown as Eq.(1) and Eq.(2), 
respectively. d. =0 (1) dt Du 11 = -VP + .V2 u + G + ..dn (2) Dt . . where, . is density, t is time, 
u is velocity, P is pressure, . is kinematic coef.cient of viscosity, G is gravity, . is curvature, . 
is surface tension coef.cient, d is delta function, and n is normal vector of the surface. Once the bubble 
is on the surface, .lm particle is detected. Film particle is de.ned as a particle that has free surface 
and air particle within the radius of in.uence. Film particle also has surface ten­sion; however, it 
is not calculated with the last term of Eq.(2) but with the following Eq.(3) according to the idea of 
[Bird et al. 2010], which is based on MSM (Mass Spring Model). .2 ri 2R. rj - ri 1 = - {d(|rj -ri|-lij 
) }+ .aV 2 ni .t2 m0|rj - ri| 2m0 a i =j (3) where, ri and rj are the positions of particle i and j, 
R is .lm radius of bubble, m0 is the initial particle number of density, lij is the length between particles 
of i and j when the .lm is made, .a and Va are the density and velocity of air, and ni is the normal 
vector of particle i. References BIRD, J., RUITER, R., COURBIN, L., AND STONE, H. 2010. Daughter bubble 
cascades produced by folding of ruptured thin .lms. Nature 465, 10, 759 762. HONG, J., LEE, H., YOON, 
J., AND KIM, C. 2008. Bubbles alive. ACM Trans. on Graphics 27, 3, 48:1 48:4. IHMSEN, M., BADER, J., 
AKINCI, G., AND TESCHNER, M. 2011. Animation of air bubbles with SPH. International Conference on Graphics 
Theory and Application, 225 234. Copyright is held by the author / owner(s). SIGGRAPH 2012, Los Angeles, 
California, August 5 9, 2012. ISBN 978-1-4503-1435-0/12/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2343043</article_id>
		<sort_key>1470</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>128</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>8</seq_no>
		<title><![CDATA[Tongue visualization for specified speech task]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2342896.2343043</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2343043</url>
		<abstract>
			<par><![CDATA[<p>Tongue is a muscular organ which is particularly important for phonetic articulation during speech task. In many real cases, a visualized movement of the tongue could be of help, for example in the rehabilitation of speech disorders. The challenge lies in the fact that the tongue motion is subtle, swift and hardly visible during speech production. Many imaging modalities (<i>e.g</i>., MRI/CT/ultrasound) have been adopted for the acquisition of tongue shape. Unfortunately, these methodologies are expensive and with fundamental limitations especially for real-time 3D imaging reconstruction of the tongue.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3737643</person_id>
				<author_profile_id><![CDATA[81504686899]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Yin]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[yinyang@utdallas.edu]]></email_address>
			</au>
			<au>
				<person_id>P3737644</person_id>
				<author_profile_id><![CDATA[81451594661]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Xiaohu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Guo]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[xguo@utdallas.edu]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1032541</ref_obj_id>
				<ref_obj_pid>1032290</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Choi, M. G., and Ko, H.-S. 2005. Modal warping: Real-time simulation of large rotational deformation and manipulation. <i>IEEE TVCG 11</i>, 1, 91--101.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Stavness, I., Lloyd, J. E., Payan, Y., and Fels, S. 2011. Coupled hard-soft tissue simulation with contact and constraints applied to jaw-tongue-hyoid dynamics. <i>Int. J. for Numerical Methods in Biomedical Engineering 27</i>, 3, 367--390.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Tongue Visualization For Speci.ed Speech Task Yin Yang Xiaohu Guo* Figure 1: An overview of the general 
work .ow of the proposed framework.(a) 4 mocap sensors placed on the tongue. (b) The corresponding 4 
nodes on the mesh. (c) A comprehensive visualization interface that provides enriched/abstract visual 
information to the user. 1 Introduction Tongue is a muscular organ which is particularly important for 
pho­netic articulation during speech task. In many real cases, a visual­ized movement of the tongue could 
be of help, for example in the rehabilitation of speech disorders. The challenge lies in the fact that 
the tongue motion is subtle, swift and hardly visible during speech production. Many imaging modalities 
(e.g., MRI/CT/ultrasound) have been adopted for the acquisition of tongue shape. Unfortu­nately, these 
methodologies are expensive and with fundamental limitations especially for real-time 3D imaging reconstruction 
of the tongue. We propose a novel real-time visualization framework based on the motion capture (mocap) 
technique and constraint-driven elastic model that circumvents the aforementioned problems and produces 
the most physically reasonable estimate of the tongue shape. We al­so provide a deformation segmentation 
tool for the domain expert to fast identify the tendency of the deformation. Additional features including 
energy-based low dimension visualization and morphing are also available in this framework. 2 Method 
 Data gathering has 7 subjects including native/non-native English as well as an adult patient suffering 
with communication disor­der involved. 5 mocap sensors were used in the experiment. 4 of them were directly 
attached to the tongue (Fig. 1 (a)) and 1 sensor was placed at mandibular symphysis so that the contribution 
of jaw movement to the tongue s position can be removed. The elicited tasks were consonant-vowel (CV) 
syllable trains that were 5 sylla­bles in length and the audio was also recorded by a head-mounted microphone. 
An optimized transformation that maps the gathered mocap signals to the .nite element mesh was applied 
to minimize the geometrical deviation between the mesh and the tongue (Fig. 1 (b)). Shape modeling is 
based on the theory of linear elasticity and modal warping [Choi and Ko 2005]. The constraint-driven 
dynam­ ics utilizes the normalized mocap data and computes the appropri­ate constraint forces which, 
instead of muscle articulation [Stavness *e-mail:{yinyang|xguo}@utdallas.edu  Figure 2: (a) Sequence 
of snapshots of the tongue shapes during producing CV syllables /ka/ and /la/. (b) Segmentation results 
for a native English speaker and a patient with communication disorder. et al. 2011], forward the movement 
of the tongue (Fig. 2 (a)). Modal analysis signi.cantly reduces the computation intensity and makes the 
visualization real-time. Deformation segmentation targets on providing the visualizaion of the general 
pattern of the 3D deformation. Like using a piecewise linear line segment to approximate a curve, we 
use a low-ranked lo­cal subspace deformation to .t the given displacement .eld. The hidden sematic information 
could be unveiled with this technique. For example, an asymmetrical tip deformation can be found at dis­ordered 
speaker while the normal speaker has mostly symmetric shape (Fig. 2 (b)). References CHOI, M. G., AND 
KO, H.-S. 2005. Modal warping: Real-time simulation of large rotational deformation and manipulation. 
IEEE TVCG 11, 1, 91 101. STAVNESS, I., LLOYD, J. E., PAYAN, Y., AND FELS, S. 2011. Coupled hard-soft 
tis­ sue simulation with contact and constraints applied to jaw-tongue-hyoid dynamics. Int. J. for Numerical 
Methods in Biomedical Engineering 27, 3, 367 390. Copyright is held by the author / owner(s). SIGGRAPH 
2012, Los Angeles, California, August 5 9, 2012. ISBN 978-1-4503-1435-0/12/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2343044</article_id>
		<sort_key>1480</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>129</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>9</seq_no>
		<title><![CDATA[Using motion capture to manipulate and edit meshes]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2342896.2343044</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2343044</url>
		<abstract>
			<par><![CDATA[<p>We are in the initial stages of the design of a system that uses Microsoft's Kinect interface to support gesture-based interaction with segmented 3D medical imagery. By harnessing the skeletal information provided by the Kinect motion capture device, our application enables spatial interaction between the user avatar and meshes using hand motions and gestures. In particular, the user selects vertices nearest their hands and then scales, translates and rotates the mesh with natural, intuitive gestures. This system of interaction enables the user to view and manipulate complex graphical objects in real-time, a tool that we envision will allow medical students to interact with 3D biomedical scans, and share 3D collaborative spaces in order to plan and reason about surgical procedures.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[hand gestures]]></kw>
			<kw><![CDATA[mesh manipulation and editing]]></kw>
			<kw><![CDATA[motion capture]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.4</cat_node>
				<descriptor>Graphics editors</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.4</cat_node>
				<descriptor>Virtual device interfaces</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Virtual reality</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010866</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Virtual reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10011007.10010940.10010941.10010969</concept_id>
				<concept_desc>CCS->Software and its engineering->Software organization and properties->Contextual software domains->Virtual worlds software</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P3737645</person_id>
				<author_profile_id><![CDATA[81466647282]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Joe]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Istead]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[jistead@uwo.ca]]></email_address>
			</au>
			<au>
				<person_id>P3737646</person_id>
				<author_profile_id><![CDATA[81504688587]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Roy]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Eaglesonan]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[eagleson@uwo.ca]]></email_address>
			</au>
			<au>
				<person_id>P3737647</person_id>
				<author_profile_id><![CDATA[81502796136]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Sandrine]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[de Ribaupierre]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[sderibau@uwo.ca]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Using Motion Capture to Manipulate and Edit Meshes Joe Istead* , Roy Eagleson and Sandrine de Ribaupierre 
 Figure 1: Two sequences of gestures for mesh rotation: Grab and Release (top); Pin and Manipulate (bottom). 
CR Categories: I.3.4 [Computer Graphics]: Graphics Utilities Graphics editors; I.3.4 [Computer Graphics]: 
Graphics Utilities Virtual device interfaces; I.3.7 [Computer Graphics]: Three-Dimensional Graphics and 
Realism Virtual reality; Keywords: hand gestures, motion capture, mesh manipulation and editing Links: 
DL PDF 1 Introduction We are in the initial stages of the design of a system that uses Mi­crosoft s 
Kinect interface to support gesture-based interaction with segmented 3D medical imagery. By harnessing 
the skeletal infor­mation provided by the Kinect motion capture device, our applica­tion enables spatial 
interaction between the user avatar and meshes using hand motions and gestures. In particular, the user 
selects ver­tices nearest their hands and then scales, translates and rotates the mesh with natural, 
intuitive gestures. This system of interaction en­ables the user to view and manipulate complex graphical 
objects in real-time, a tool that we envision will allow medical students to in­teract with 3D biomedical 
scans, and share 3D collaborative spaces in order to plan and reason about surgical procedures. More 
speci.cally, we demonstrate two different gesture systems for selecting vertices and transforming the 
mesh that only rely on the position of the user s hands. The .rst system employs an intuitive Grab and 
Release metaphor that distinguishes between open and closed hands a task limited by the lack of hand 
con.guration data. The second system employs a less intuitive Pin and Manipulate metaphor that is better 
suited to hand position-only motion capture data. Our initial analysis focuses on speed and accuracy 
tradeoffs; however, later analysis demonstrates a Fitts-like Index of Perfor­mance for selecting vertex 
positions in 3D. *e-mail: jistead@uwo.ca e-mail:eagleson@uwo.ca e-mail:sderibau@uwo.ca  2 Grab and Release 
The user touches their hands against the surface of the mesh, clos­ing their hands to grasp it. Then, 
while still grasping the mesh with closed hands, the user repositions their hands to scale, rotate and 
translate the mesh. The operation completes when the user opens their hands, releasing the transformed 
mesh. While this gesture system represents a very natural and intuitive approach to manipulating meshes, 
it must distinguish between open and closed hands, an impossible task given only the position of each 
hand. Instead of registering the hand con.guration from the Kinect camera or using voice recognition, 
our grab and release system used a sequence of hold times: after moving into vertex proximity, the user 
holds their hands still for a short duration to close the hands; after performing the transformation, 
the user holds the hands still for another duration to open the hands, releasing the mesh and end­ing 
the operation. Even after .ne-tuning the radius of proximity and hold times, we found this gesture system 
unsuitable for rel­atively small transformations due to spatial jitter during the hold times. The approach 
also requires more physical and mental effort compared to the pin and manipulate gesture system. 3 Pin 
and Manipulate The Pin and Manipulate gesture system employs a pin the tail on the donkey metaphor where 
the user .xes an initial vertex in place and then manipulates a second vertex about that .xed point. 
The system requires the user to hold the pinned hand in place until they wish to terminate the transformation 
by moving the hand away from the pinned vertex. This system is suitable for smaller transformations since 
there are no hold times; furthermore, such an approach requires less phys­ical and mental effort. Unfortunately, 
.xing one vertex reduces the transformation space, so this requires the user to perform ex­tra transformations. 
As well, combining translations and rotations is taxing since the rotation must maintain a .xed distance; 
similarly, translating and scaling in one transformation is impossible. Copyright is held by the author 
/ owner(s). SIGGRAPH 2012, Los Angeles, California, August 5 9, 2012. ISBN 978-1-4503-1435-0/12/0008 
 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	</section>
</content>
</proceeding>
